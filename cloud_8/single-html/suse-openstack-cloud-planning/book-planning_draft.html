<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Planning an Installation with Cloud Lifecycle Manager | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" />
<meta name="title" content="Planning an Installation with Cloud Lifecycle Manager …" />
<meta name="description" content="" />
<meta name="product-name" content="SUSE OpenStack Cloud" />
<meta name="product-number" content="8" />
<meta name="book-title" content="Planning an Installation with Cloud Lifecycle Manager" />
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" />
<meta name="tracker-type" content="bsc" />
<meta name="tracker-bsc-component" content="Documentation" />
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" />
<meta property="og:title" content="Planning an Installation with Cloud Lifecycle Manager …" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Planning an Installation with Cloud Lifecycle Manager …" />
<meta name="twitter:description" content="" />

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#book-planning" accesskey="c"><span class="single-contents-icon"></span>Planning an Installation with Cloud Lifecycle Manager</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#book-planning" accesskey="c"><span class="single-contents-icon"></span>Show Contents: Planning an Installation with Cloud Lifecycle Manager</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#planning-index"><span class="number">I </span><span class="name">Planning</span></a><ol><li class="inactive"><a href="#register-suse-overview"><span class="number">1 </span><span class="name">Registering SLES</span></a></li><li class="inactive"><a href="#min-hardware"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></li><li class="inactive"><a href="#idg-planning-planning-recommended-hardware-minimums-xml-1"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></li><li class="inactive"><a href="#HP3-0HA"><span class="number">4 </span><span class="name">High Availability</span></a></li></ol></li><li class="inactive"><a href="#architecture"><span class="number">II </span><span class="name">Cloud Lifecycle Manager Overview</span></a><ol><li class="inactive"><a href="#cha-input-model-intro-concept"><span class="number">5 </span><span class="name">Input Model</span></a></li><li class="inactive"><a href="#configurationobjects"><span class="number">6 </span><span class="name">Configuration Objects</span></a></li><li class="inactive"><a href="#othertopics"><span class="number">7 </span><span class="name">Other Topics</span></a></li><li class="inactive"><a href="#cpinfofiles"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></li><li class="inactive"><a href="#example-configurations"><span class="number">9 </span><span class="name">Example Configurations</span></a></li><li class="inactive"><a href="#modify-compute-input-model"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></li><li class="inactive"><a href="#modify-input-model"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></li><li class="inactive"><a href="#alternative-configurations"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></li></ol></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="book" id="book-planning" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></h6><div><h1 class="title"><em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em> <a title="Permalink" class="permalink" href="#book-planning">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/book.planning.xml" title="Edit the source file for this section">Edit source</a></h1></div><div class="date"><span class="imprint-label">Publication Date: </span>04/06/2022</div></div></div><div class="toc"><dl><dt><span class="part"><a href="#planning-index"><span class="number">I </span><span class="name">Planning</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#register-suse-overview"><span class="number">1 </span><span class="name">Registering SLES</span></a></span></dt><dd><dl><dt><span class="section"><a href="#register-suse-installation"><span class="number">1.1 </span><span class="name">Registering SLES during the Installation</span></a></span></dt><dt><span class="section"><a href="#register-suse-already-installed"><span class="number">1.2 </span><span class="name">Registering SLES from the Installed System</span></a></span></dt><dt><span class="section"><a href="#register-suse-automated"><span class="number">1.3 </span><span class="name">Registering SLES during Automated Deployment</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#min-hardware"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></span></dt><dd><dl><dt><span class="section"><a href="#hw-support-openstackvers"><span class="number">2.1 </span><span class="name">OpenStack Version Information</span></a></span></dt><dt><span class="section"><a href="#hw-support-hardwareconfig"><span class="number">2.2 </span><span class="name">Supported Hardware Configurations</span></a></span></dt><dt><span class="section"><a href="#core-noncore-openstack"><span class="number">2.3 </span><span class="name">Support for Core and Non-Core OpenStack Features</span></a></span></dt><dt><span class="section"><a href="#hw-support-scaling"><span class="number">2.4 </span><span class="name">Cloud Scaling</span></a></span></dt><dt><span class="section"><a href="#hw-support-software"><span class="number">2.5 </span><span class="name">Supported Software</span></a></span></dt><dt><span class="section"><a href="#hw-support-perfnotes"><span class="number">2.6 </span><span class="name">Notes About Performance</span></a></span></dt><dt><span class="section"><a href="#hw-support-kvmguestos"><span class="number">2.7 </span><span class="name">KVM Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-esxguestos"><span class="number">2.8 </span><span class="name">ESX Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-ironicguestos"><span class="number">2.9 </span><span class="name">Ironic Guest OS Support</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#idg-planning-planning-recommended-hardware-minimums-xml-1"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#rec-min-entryscale-kvm"><span class="number">3.1 </span><span class="name">Recommended Hardware Minimums for an Entry-scale KVM</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm"><span class="number">3.2 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX KVM Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm-mml"><span class="number">3.3 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX, KVM with Dedicated Cluster for Metering, Monitoring, and Logging</span></a></span></dt><dt><span class="section"><a href="#rec-min-ironic"><span class="number">3.4 </span><span class="name">Recommended Hardware Minimums for an Ironic Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-swift"><span class="number">3.5 </span><span class="name">Recommended Hardware Minimums for an Entry-scale Swift Model</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#HP3-0HA"><span class="number">4 </span><span class="name">High Availability</span></a></span></dt><dd><dl><dt><span class="section"><a href="#concepts-overview"><span class="number">4.1 </span><span class="name">High Availability Concepts Overview</span></a></span></dt><dt><span class="section"><a href="#highly-available-cloud-infrastructure"><span class="number">4.2 </span><span class="name">Highly Available Cloud Infrastructure</span></a></span></dt><dt><span class="section"><a href="#high-availablity-controllers"><span class="number">4.3 </span><span class="name">High Availability of Controllers</span></a></span></dt><dt><span class="section"><a href="#CVR"><span class="number">4.4 </span><span class="name">High Availability Routing - Centralized</span></a></span></dt><dt><span class="section"><a href="#DVR"><span class="number">4.5 </span><span class="name">High Availability Routing - Distributed</span></a></span></dt><dt><span class="section"><a href="#availability-zones"><span class="number">4.6 </span><span class="name">Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-kvm"><span class="number">4.7 </span><span class="name">Compute with KVM</span></a></span></dt><dt><span class="section"><a href="#nova-availability-zones"><span class="number">4.8 </span><span class="name">Nova Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-esx"><span class="number">4.9 </span><span class="name">Compute with ESX Hypervisor</span></a></span></dt><dt><span class="section"><a href="#cinder-availability-zones"><span class="number">4.10 </span><span class="name">Cinder Availability Zones</span></a></span></dt><dt><span class="section"><a href="#object-storage-swift"><span class="number">4.11 </span><span class="name">Object Storage with Swift</span></a></span></dt><dt><span class="section"><a href="#highly-available-app-workloads"><span class="number">4.12 </span><span class="name">Highly Available Cloud Applications and Workloads</span></a></span></dt><dt><span class="section"><a href="#what-not-ha"><span class="number">4.13 </span><span class="name">What is not Highly Available?</span></a></span></dt><dt><span class="section"><a href="#more-information"><span class="number">4.14 </span><span class="name">More Information</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#architecture"><span class="number">II </span><span class="name">Cloud Lifecycle Manager Overview</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-input-model-intro-concept"><span class="number">5 </span><span class="name">Input Model</span></a></span></dt><dd><dl><dt><span class="section"><a href="#input-model-introduction"><span class="number">5.1 </span><span class="name">Introduction to the Input Model</span></a></span></dt><dt><span class="section"><a href="#concepts"><span class="number">5.2 </span><span class="name">Concepts</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#configurationobjects"><span class="number">6 </span><span class="name">Configuration Objects</span></a></span></dt><dd><dl><dt><span class="section"><a href="#configobj-cloud"><span class="number">6.1 </span><span class="name">Cloud Configuration</span></a></span></dt><dt><span class="section"><a href="#configobj-controlplane"><span class="number">6.2 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#configobj-load-balancers"><span class="number">6.3 </span><span class="name">Load Balancers</span></a></span></dt><dt><span class="section"><a href="#configobj-regions"><span class="number">6.4 </span><span class="name">Regions</span></a></span></dt><dt><span class="section"><a href="#configobj-servers"><span class="number">6.5 </span><span class="name">Servers</span></a></span></dt><dt><span class="section"><a href="#configobj-servergroups"><span class="number">6.6 </span><span class="name">Server Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-serverroles"><span class="number">6.7 </span><span class="name">Server Roles</span></a></span></dt><dt><span class="section"><a href="#configobj-diskmodels"><span class="number">6.8 </span><span class="name">
  Disk Models</span></a></span></dt><dt><span class="section"><a href="#configobj-memorymodels"><span class="number">6.9 </span><span class="name">Memory Models</span></a></span></dt><dt><span class="section"><a href="#configobj-cpumodels"><span class="number">6.10 </span><span class="name">
  CPU Models</span></a></span></dt><dt><span class="section"><a href="#configobj-interfacemodels"><span class="number">6.11 </span><span class="name">Interface Models</span></a></span></dt><dt><span class="section"><a href="#configobj-nicmappings"><span class="number">6.12 </span><span class="name">NIC Mappings</span></a></span></dt><dt><span class="section"><a href="#configobj-networkgroups"><span class="number">6.13 </span><span class="name">Network Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-networks"><span class="number">6.14 </span><span class="name">Networks</span></a></span></dt><dt><span class="section"><a href="#configobj-firewallrules"><span class="number">6.15 </span><span class="name">Firewall Rules</span></a></span></dt><dt><span class="section"><a href="#configobj-configurationdata"><span class="number">6.16 </span><span class="name">Configuration Data</span></a></span></dt><dt><span class="section"><a href="#passthrough"><span class="number">6.17 </span><span class="name">Pass Through</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#othertopics"><span class="number">7 </span><span class="name">Other Topics</span></a></span></dt><dd><dl><dt><span class="section"><a href="#services-components"><span class="number">7.1 </span><span class="name">Services and Service Components</span></a></span></dt><dt><span class="section"><a href="#namegeneration"><span class="number">7.2 </span><span class="name">Name Generation</span></a></span></dt><dt><span class="section"><a href="#persisteddata"><span class="number">7.3 </span><span class="name">Persisted Data</span></a></span></dt><dt><span class="section"><a href="#serverallocation"><span class="number">7.4 </span><span class="name">Server Allocation</span></a></span></dt><dt><span class="section"><a href="#servernetworkselection"><span class="number">7.5 </span><span class="name">Server Network Selection</span></a></span></dt><dt><span class="section"><a href="#networkroutevalidation"><span class="number">7.6 </span><span class="name">Network Route Validation</span></a></span></dt><dt><span class="section"><a href="#configneutronprovidervlans"><span class="number">7.7 </span><span class="name">Configuring Neutron Provider VLANs</span></a></span></dt><dt><span class="section"><a href="#standalonedeployer"><span class="number">7.8 </span><span class="name">Standalone Cloud Lifecycle Manager</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cpinfofiles"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></span></dt><dd><dl><dt><span class="section"><a href="#address-info-yml"><span class="number">8.1 </span><span class="name">address_info.yml</span></a></span></dt><dt><span class="section"><a href="#firewall-info-yml"><span class="number">8.2 </span><span class="name">firewall_info.yml</span></a></span></dt><dt><span class="section"><a href="#route-info-yml"><span class="number">8.3 </span><span class="name">route_info.yml</span></a></span></dt><dt><span class="section"><a href="#server-info-yml"><span class="number">8.4 </span><span class="name">server_info.yml</span></a></span></dt><dt><span class="section"><a href="#service-info-yml"><span class="number">8.5 </span><span class="name">service_info.yml</span></a></span></dt><dt><span class="section"><a href="#control-plane-topology-yml"><span class="number">8.6 </span><span class="name">control_plane_topology.yml</span></a></span></dt><dt><span class="section"><a href="#network-topology-yml"><span class="number">8.7 </span><span class="name">network_topology.yml</span></a></span></dt><dt><span class="section"><a href="#region-topology-yml"><span class="number">8.8 </span><span class="name">region_topology.yml</span></a></span></dt><dt><span class="section"><a href="#service-topology-yml"><span class="number">8.9 </span><span class="name">service_topology.yml</span></a></span></dt><dt><span class="section"><a href="#private-data-metadata-ccp-yml"><span class="number">8.10 </span><span class="name">private_data_metadata_ccp.yml</span></a></span></dt><dt><span class="section"><a href="#password-change-yml"><span class="number">8.11 </span><span class="name">password_change.yml</span></a></span></dt><dt><span class="section"><a href="#explain-txt"><span class="number">8.12 </span><span class="name">explain.txt</span></a></span></dt><dt><span class="section"><a href="#clouddiagram-txt"><span class="number">8.13 </span><span class="name">CloudDiagram.txt</span></a></span></dt><dt><span class="section"><a href="#html-representation"><span class="number">8.14 </span><span class="name">HTML Representation</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#example-configurations"><span class="number">9 </span><span class="name">Example Configurations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#example-configs"><span class="number">9.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span></a></span></dt><dt><span class="section"><a href="#alternative"><span class="number">9.2 </span><span class="name">Alternative Configurations</span></a></span></dt><dt><span class="section"><a href="#kvm-examples"><span class="number">9.3 </span><span class="name">KVM Examples</span></a></span></dt><dt><span class="section"><a href="#esx-examples"><span class="number">9.4 </span><span class="name">ESX Examples</span></a></span></dt><dt><span class="section"><a href="#swift-examples"><span class="number">9.5 </span><span class="name">Swift Examples</span></a></span></dt><dt><span class="section"><a href="#ironic-examples"><span class="number">9.6 </span><span class="name">Ironic Examples</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#modify-compute-input-model"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sles-compute-model"><span class="number">10.1 </span><span class="name">SLES Compute Nodes</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#modify-input-model"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></span></dt><dd><dl><dt><span class="section"><a href="#objectstorage-overview"><span class="number">11.1 </span><span class="name">Object Storage using Swift Overview</span></a></span></dt><dt><span class="section"><a href="#topic-r3k-v2c-jt"><span class="number">11.2 </span><span class="name">Allocating Proxy, Account, and Container (PAC) Servers for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-tq1-xt5-dt"><span class="number">11.3 </span><span class="name">Allocating Object Servers</span></a></span></dt><dt><span class="section"><a href="#topic-uh2-td1-kt"><span class="number">11.4 </span><span class="name">Creating Roles for Swift Nodes</span></a></span></dt><dt><span class="section"><a href="#allocating-disk-drives"><span class="number">11.5 </span><span class="name">Allocating Disk Drives for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-d1s-hht-tt"><span class="number">11.6 </span><span class="name">Swift Requirements for Device Group Drives</span></a></span></dt><dt><span class="section"><a href="#topic-rvj-21c-jt"><span class="number">11.7 </span><span class="name">Creating a Swift Proxy, Account, and Container (PAC) Cluster</span></a></span></dt><dt><span class="section"><a href="#topic-jzk-q1c-jt"><span class="number">11.8 </span><span class="name">Creating Object Server Resource Nodes</span></a></span></dt><dt><span class="section"><a href="#topic-pcj-hzv-dt"><span class="number">11.9 </span><span class="name">Understanding Swift Network and Service Requirements</span></a></span></dt><dt><span class="section"><a href="#ring-specification"><span class="number">11.10 </span><span class="name">Understanding Swift Ring Specifications</span></a></span></dt><dt><span class="section"><a href="#swift-storage-policies"><span class="number">11.11 </span><span class="name">Designing Storage Policies</span></a></span></dt><dt><span class="section"><a href="#designing-swift-zones"><span class="number">11.12 </span><span class="name">Designing Swift Zones</span></a></span></dt><dt><span class="section"><a href="#topic-rdf-hkp-rt"><span class="number">11.13 </span><span class="name">Customizing Swift Service Configuration Files</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#alternative-configurations"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#standalone-deployer"><span class="number">12.1 </span><span class="name">Using a Dedicated Cloud Lifecycle Manager Node</span></a></span></dt><dt><span class="section"><a href="#without-dvr"><span class="number">12.2 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR</span></a></span></dt><dt><span class="section"><a href="#without-l3agent"><span class="number">12.3 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only</span></a></span></dt><dt><span class="section"><a href="#twosystems"><span class="number">12.4 </span><span class="name">Considerations When Installing Two Systems on One Subnet</span></a></span></dt></dl></dd></dl></dd></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#ControlPlane1"><span class="number">4.1 </span><span class="name">HA Architecture</span></a></span></dt><dt><span class="figure"><a href="#Layer3HA"><span class="number">4.2 </span><span class="name">Layer-3 HA</span></a></span></dt><dt><span class="figure"><a href="#DeploymentZones"><span class="number">4.3 </span><span class="name">Availability Zones</span></a></span></dt><dt><span class="figure"><a href="#multi-tenancy"><span class="number">9.1 </span><span class="name">Entry-scale Cloud with Ironic Muti-Tenancy</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#neutron-networks-vxlan"><span class="number">6.1 </span><span class="name">neutron.networks.vxlan</span></a></span></dt><dt><span class="table"><a href="#neutron-networks-vlan"><span class="number">6.2 </span><span class="name">neutron.networks.vlan</span></a></span></dt><dt><span class="table"><a href="#neutron-networks-flat"><span class="number">6.3 </span><span class="name">neutron.networks.flat</span></a></span></dt><dt><span class="table"><a href="#neutron-l3-agent"><span class="number">6.4 </span><span class="name">neutron.l3_agent.external_network_bridge</span></a></span></dt></dl></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><dl><dt><span class="example"><a href="#id-1.3.4.9.10.3.12"><span class="number">11.1 </span><span class="name">
    <span class="bold">PACO</span> - proxy, account, container,
    and object run on the same node type.
   </span></a></span></dt><dt><span class="example"><a href="#id-1.3.4.9.10.3.13"><span class="number">11.2 </span><span class="name">
    <span class="bold">PAC</span> - proxy, account, and
    container run on the same node type.
   </span></a></span></dt><dt><span class="example"><a href="#id-1.3.4.9.10.3.14"><span class="number">11.3 </span><span class="name"><span class="bold">OBJ</span> - Dedicated object
    server</span></a></span></dt></dl></div><div><div class="legalnotice" id="id-1.3.2.1"><p>
  Copyright © 2006–
2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under
  <span class="bold"><strong>Creative Commons Attribution 3.0 License
  </strong></span>:
  <a class="link" href="http://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">


  </a>
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be held
  liable for possible errors or the consequences thereof.
 </p></div></div><div class="part" id="planning-index"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part I </span><span class="name">Planning </span><a title="Permalink" class="permalink" href="#planning-index">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-planning_index.xml" title="Edit the source file for this section">Edit source</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#register-suse-overview"><span class="number">1 </span><span class="name">Registering SLES</span></a></span></dt><dd class="toc-abstract"><p>To get technical support and product updates, you need to register and activate your SUSE product with the SUSE Customer Center. It is recommended to register during the installation, since this will enable you to install the system with the latest updates and patches available. However, if you are …</p></dd><dt><span class="chapter"><a href="#min-hardware"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></span></dt><dd class="toc-abstract"><p>
  This document lists the details about the supported hardware and software for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
 </p></dd><dt><span class="chapter"><a href="#idg-planning-planning-recommended-hardware-minimums-xml-1"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></span></dt><dd class="toc-abstract"><p>These recommended minimums are based on example configurations included with the installation models (see Chapter 9, Example Configurations). They are suitable only for demo environments. For production systems you will want to consider your capacity and performance requirements when making decision…</p></dd><dt><span class="chapter"><a href="#HP3-0HA"><span class="number">4 </span><span class="name">High Availability</span></a></span></dt><dd class="toc-abstract"><p>
    This chapter covers High Availability concepts overview and cloud
    infrastructure.
   </p></dd></dl></div><div class="chapter " id="register-suse-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES</span> <a title="Permalink" class="permalink" href="#register-suse-overview">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-suse-register_suse_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_overview.xml</li><li><span class="ds-label">ID: </span>register-suse-overview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#register-suse-installation"><span class="number">1.1 </span><span class="name">Registering SLES during the Installation</span></a></span></dt><dt><span class="section"><a href="#register-suse-already-installed"><span class="number">1.2 </span><span class="name">Registering SLES from the Installed System</span></a></span></dt><dt><span class="section"><a href="#register-suse-automated"><span class="number">1.3 </span><span class="name">Registering SLES during Automated Deployment</span></a></span></dt></dl></div></div><p>
  To get technical support and product updates, you need to register and
  activate your SUSE product with the SUSE Customer Center. It is recommended to register
  during the installation, since this will enable you to install the system
  with the latest updates and patches available. However, if you are offline or
  want to skip the registration step, you can register at any time later from
  the installed system.
 </p><div id="id-1.3.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   In case your organization does not provide a local registration server,
   registering SLES requires a SUSE account. In case you do not have a
   SUSE account yet, go to the SUSE Customer Center home page
   (<a class="link" href="https://scc.suse.com/" target="_blank">https://scc.suse.com/</a>) to
   create one.
  </p></div><div class="sect1" id="register-suse-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES during the Installation</span> <a title="Permalink" class="permalink" href="#register-suse-installation">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-suse-register_suse_install.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_install.xml</li><li><span class="ds-label">ID: </span>register-suse-installation</li></ul></div></div></div></div><p>
  To register your system, provide the E-mail address associated with the
  SUSE account you or your organization uses to manage subscriptions. In case
  you do not have a SUSE account yet, go to the SUSE Customer Center home page
  (<a class="link" href="https://scc.suse.com/" target="_blank">https://scc.suse.com/</a>) to
  create one.
 </p><p>
  Enter the Registration Code you received with your copy of SUSE Linux Enterprise Server. Proceed
  with <span class="guimenu ">Next</span> to start the registration process.
 </p><p>
  By default the system is registered with the SUSE Customer Center. However, if your
  organization provides local registration servers you can either choose one
  form the list of auto-detected servers or provide the URL at
  <code class="literal">Register System via local SMT Server</code>. Proceed with
  <span class="guimenu ">Next</span>.
 </p><p>
  During the registration, the online update repositories will be added to your
  installation setup. When finished, you can choose whether to install the
  latest available package versions from the update repositories. This ensures
  that SUSE Linux Enterprise Server is installed with the latest security updates available. If you
  choose No, all packages will be installed from the installation media.
  Proceed with Next.
 </p><p>
  If the system was successfully registered during installation, YaST will
  disable repositories from local installation media such as CD/DVD or flash
  disks when the installation has been completed. This prevents problems if the
  installation source is no longer available and ensures that you always get
  the latest updates from the online repositories.
 </p></div><div class="sect1" id="register-suse-already-installed"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES from the Installed System</span> <a title="Permalink" class="permalink" href="#register-suse-already-installed">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-suse-register_suse_already_installed.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_already_installed.xml</li><li><span class="ds-label">ID: </span>register-suse-already-installed</li></ul></div></div></div></div><div class="sect2" id="id-1.3.3.2.5.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering from the Installed System</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.5.2">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-suse-register_suse_already_installed.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_already_installed.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you have skipped the registration during the installation or want to
   re-register your system, you can register the system at any time using the
   YaST module <span class="guimenu ">Product Registration</span> or the command line
   tool <code class="command">SUSEConnect</code>.
  </p><p>
   <span class="bold"><strong>Registering with YaST</strong></span>
  </p><p>
   To register the system start
   <span class="guimenu ">YaST</span> › <span class="guimenu ">Software</span> › <span class="guimenu ">Product
   Registration</span>. Provide the E-mail address associated
   with the SUSE account you or your organization uses to manage
   subscriptions. In case you do not have a SUSE account yet, go to the SUSE Customer Center
   homepage (<a class="link" href="https://scc.suse.com/" target="_blank">https://scc.suse.com/</a>) to create one.
  </p><p>
   Enter the Registration Code you received with your copy of SUSE Linux Enterprise Server. Proceed
   with <span class="guimenu ">Next</span> to start the registration process.
  </p><p>
   By default the system is registered with the SUSE Customer Center. However, if your
   organization provides local registration servers you can either choose one
   form the list of auto-detected servers or provide the URl at
   <span class="guimenu ">Register System via local SMT Server</span>. Proceed with
   <span class="guimenu ">Next</span>.
  </p><p>
   <span class="bold"><strong>Registering with SUSEConnect</strong></span>
  </p><p>
   To register from the command line, use the command
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -r <em class="replaceable ">REGISTRATION_CODE</em> -e <em class="replaceable ">EMAIL_ADDRESS</em></pre></div><p>
   Replace <em class="replaceable ">REGISTRATION_CODE</em> with the Registration
   Code you received with your copy of SUSE Linux Enterprise Server. Replace
   <em class="replaceable ">EMAIL_ADDRESS</em> with the E-mail address associated
   with the SUSE account you or your organization uses to manage
   subscriptions. To register with a local registration server, also provide
   the URL to the server:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -r <em class="replaceable ">REGISTRATION_CODE</em> -e <em class="replaceable ">EMAIL_ADDRESS</em> \
--url "https://suse_register.example.com/"</pre></div></div></div><div class="sect1" id="register-suse-automated"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES during Automated Deployment</span> <a title="Permalink" class="permalink" href="#register-suse-automated">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-suse-register_suse_automated.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_automated.xml</li><li><span class="ds-label">ID: </span>register-suse-automated</li></ul></div></div></div></div><p>
  If you deploy your instances automatically using AutoYaST, you can register
  the system during the installation by providing the respective information in
  the AutoYaST control file. Refer to
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-autoyast/#CreateProfile-Register" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-autoyast/#CreateProfile-Register</a>
  for details.
 </p></div></div><div class="chapter " id="min-hardware"><div class="titlepage"><div><div><h2 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware and Software Support Matrix</span> <a title="Permalink" class="permalink" href="#min-hardware">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_matrix.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_matrix.xml</li><li><span class="ds-label">ID: </span>min-hardware</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#hw-support-openstackvers"><span class="number">2.1 </span><span class="name">OpenStack Version Information</span></a></span></dt><dt><span class="section"><a href="#hw-support-hardwareconfig"><span class="number">2.2 </span><span class="name">Supported Hardware Configurations</span></a></span></dt><dt><span class="section"><a href="#core-noncore-openstack"><span class="number">2.3 </span><span class="name">Support for Core and Non-Core OpenStack Features</span></a></span></dt><dt><span class="section"><a href="#hw-support-scaling"><span class="number">2.4 </span><span class="name">Cloud Scaling</span></a></span></dt><dt><span class="section"><a href="#hw-support-software"><span class="number">2.5 </span><span class="name">Supported Software</span></a></span></dt><dt><span class="section"><a href="#hw-support-perfnotes"><span class="number">2.6 </span><span class="name">Notes About Performance</span></a></span></dt><dt><span class="section"><a href="#hw-support-kvmguestos"><span class="number">2.7 </span><span class="name">KVM Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-esxguestos"><span class="number">2.8 </span><span class="name">ESX Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-ironicguestos"><span class="number">2.9 </span><span class="name">Ironic Guest OS Support</span></a></span></dt></dl></div></div><p>
  This document lists the details about the supported hardware and software for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
 </p><div class="sect1" id="hw-support-openstackvers"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack Version Information</span> <a title="Permalink" class="permalink" href="#hw-support-openstackvers">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_openstackvers.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_openstackvers.xml</li><li><span class="ds-label">ID: </span>hw-support-openstackvers</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> services have been updated to the
  <a class="link" href="https://www.openstack.org/software/pike" target="_blank">OpenStack
  Pike</a> release.
 </p></div><div class="sect1" id="hw-support-hardwareconfig"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Hardware Configurations</span> <a title="Permalink" class="permalink" href="#hw-support-hardwareconfig">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_hardwareconfig.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_hardwareconfig.xml</li><li><span class="ds-label">ID: </span>hw-support-hardwareconfig</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports hardware that is certified for SLES through the YES
  certification program. You will find a database of certified hardware at <a class="link" href="https://www.suse.com/yessearch/" target="_blank">https://www.suse.com/yessearch/</a>.
 </p></div><div class="sect1" id="core-noncore-openstack"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support for Core and Non-Core OpenStack Features</span> <a title="Permalink" class="permalink" href="#core-noncore-openstack">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-core_non-core_openstack_support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-core_non-core_openstack_support.xml</li><li><span class="ds-label">ID: </span>core-noncore-openstack</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /><col class="col7" /></colgroup><tbody><tr><td><span class="bold"><strong><span class="productname">OpenStack</span> Service</strong></span></td><td><span class="bold"><strong>Packages</strong></span></td><td><span class="bold"><strong>Supported</strong></span></td><td>  </td><td><span class="bold"><strong><span class="productname">OpenStack</span> Service</strong></span></td><td><span class="bold"><strong>Packages</strong></span></td><td><span class="bold"><strong>Supported</strong></span></td></tr><tr><td>Aodh</td><td>No</td><td>No</td><td>  </td><td>Barbican</td><td>Yes</td><td>Yes</td></tr><tr><td>Ceilometer</td><td>Yes</td><td>Yes</td><td>  </td><td>Cinder</td><td>Yes</td><td>Yes</td></tr><tr><td>Designate</td><td>Yes</td><td>Yes</td><td>  </td><td>Freezer</td><td>Yes</td><td>Yes</td></tr><tr><td>Glance</td><td>Yes</td><td>Yes</td><td>  </td><td>Heat</td><td>Yes</td><td>Yes</td></tr><tr><td>Horizon</td><td>Yes</td><td>Yes</td><td>  </td><td>Ironic</td><td>Yes</td><td>Yes</td></tr><tr><td>Keystone</td><td>Yes</td><td>Yes</td><td>  </td><td>Magnum</td><td>Yes</td><td>Yes</td></tr><tr><td>Manila</td><td>Yes</td><td>Yes</td><td>  </td><td>Monasca</td><td>Yes</td><td>Yes</td></tr><tr><td>Monasca-Ceilometer</td><td>Yes</td><td>Yes</td><td>  </td><td>Neutron</td><td>Yes</td><td>Yes</td></tr><tr><td>Neutron(LBaaSv2)</td><td>Yes</td><td>Yes</td><td>  </td><td>Neutron(VPNaaS)</td><td>Yes</td><td>Yes</td></tr><tr><td>Neutron(FWaaS)</td><td>Yes</td><td>Yes</td><td>  </td><td>Nova</td><td>Yes</td><td>Yes</td></tr><tr><td>Octavia</td><td>Yes</td><td>Yes</td><td>  </td><td>Swift</td><td>Yes</td><td>Yes</td></tr></tbody></table></div><p><span class="bold"><strong>Nova</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>SLES KVM Hypervisor</p></td><td><p>Xen hypervisor</p></td></tr><tr><td><p>VMware ESX Hypervisor</p></td><td><p>Hyper-V</p></td></tr><tr><td></td><td><p>Non-x86 Architectures</p></td></tr></tbody></table></div><p><span class="bold"><strong>Neutron</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td>
        <p>Tenant networks</p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>IPv6</p></li><li class="listitem "><p>SR-IOV</p></li><li class="listitem "><p>PCI-PT</p></li><li class="listitem "><p>DPDK</p></li></ul></div>
       </td><td>
        <p>
         Distributed Virtual Router (DVR) with any of the following:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>IPv6</p></li><li class="listitem "><p>BGP/Fast Path Exit</p></li><li class="listitem "><p>L2 gateway</p></li><li class="listitem "><p>SNAT HA</p></li></ul></div>
       </td></tr><tr><td><p>VMware ESX Hypervisor</p></td><td><p>QoS</p></td></tr><tr><td></td><td></td></tr></tbody></table></div><p><span class="bold"><strong>Glance Supported Features</strong></span></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Swift and Ceph backends</p></li></ul></div><p><span class="bold"><strong>Cinder</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Encrypted &amp; private volumes</p></td><td><p>VSA</p></td></tr><tr><td><p>Incremental backup, backup attached volume, encrypted volume backup, backup
      snapshots</p></td><td></td></tr></tbody></table></div><p><span class="bold"><strong>Swift</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Erasure coding</p></td><td><p>Geographically distributed clusters</p></td></tr><tr><td><p>Dispersion report</p></td><td></td></tr><tr><td><p>Swift zones</p></td><td></td></tr></tbody></table></div><p><span class="bold"><strong>Keystone</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Domains</p></td><td><p>Web SSO</p></td></tr><tr><td><p>Fernet tokens</p></td><td><p>Multi-Factor authentication</p></td></tr><tr><td>LDAP integration</td><td><p>Federation Keystone to Keystone</p></td></tr><tr><td></td><td><p>Hierarchical multi-tenancy</p></td></tr></tbody></table></div><p><span class="bold"><strong>Barbican Supported Features</strong></span></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Encryption for the following:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Cinder</p></li><li class="listitem "><p>Hardware security model</p></li><li class="listitem "><p>Encrypted data volumes</p></li><li class="listitem "><p>LBaaS</p></li><li class="listitem "><p>Symmetric keys</p></li><li class="listitem "><p>Storage keys</p></li></ul></div></li><li class="listitem "><p>CADF format auditing events</p></li></ul></div><p><span class="bold"><strong>Ceilometer</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Keystone v3 support</p></td><td><p>Gnocchi</p></td></tr><tr><td><p>Glance v2 API</p></td><td><p>IPMI and SNMP</p></td></tr><tr><td></td><td><p>Ceilometer Event APIs</p></td></tr><tr><td></td><td><p>Ceilometer Compute Agent</p></td></tr></tbody></table></div><p><span class="bold"><strong>Heat Features Not Supported</strong></span></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Multi-region stack</p></li></ul></div><p><span class="bold"><strong>Ironic</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td>
        <p>
         Drivers
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Agent_ilo</p></li><li class="listitem "><p>Agent_ipmitool</p></li><li class="listitem "><p>PXE_ilo</p></li><li class="listitem "><p>PXE_ipmitool</p></li></ul></div>
         </td><td><p>UEFI secure</p></td></tr><tr><td><p>Booting methods</p></td><td></td></tr><tr><td><p>Power</p></td><td></td></tr><tr><td><p>Compute</p></td><td></td></tr><tr><td><p>Networking</p></td><td></td></tr><tr><td><p>Images</p></td><td></td></tr></tbody></table></div><p><span class="bold"><strong>Freezer</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Backup of Control Plane data</p></td><td><p>Backup of tenant VMs, volumes, files, directories and databases</p></td></tr><tr><td><p>Backup Lifecycle Manager</p></td><td></td></tr><tr><td><p>Backup of audit logs</p></td><td></td></tr><tr><td><p>Backup of centralized logging files</p></td><td></td></tr></tbody></table></div></div><div class="sect1" id="hw-support-scaling"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Scaling</span> <a title="Permalink" class="permalink" href="#hw-support-scaling">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_scaling.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_scaling.xml</li><li><span class="ds-label">ID: </span>hw-support-scaling</li></ul></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> a total of 200 total compute nodes in a single region
  (Region0) across any of the following hypervisors is supported:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    VMware ESX
   </p></li><li class="listitem "><p>
    SLES/KVM
   </p></li></ul></div><p>
  You can distribute the compute nodes in any number of deployments as long as
  the total is no more than 200. Example: 100 ESX + 100 KVM or 50 ESX + 150
  KVM.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports a total of 8000 virtual machines across a total of
  200 compute nodes.
 </p></div><div class="sect1" id="hw-support-software"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Software</span> <a title="Permalink" class="permalink" href="#hw-support-software">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_software.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_software.xml</li><li><span class="ds-label">ID: </span>hw-support-software</li></ul></div></div></div></div><p>
  <span class="bold"><strong>Supported ESXi versions</strong></span>
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> currently supports the following ESXi versions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    ESXi version 6.0
   </p></li><li class="listitem "><p>
    ESXi version 6.0 (Update 1b)
   </p></li><li class="listitem "><p>
    ESXi version 6.5
   </p></li></ul></div><p>
  The following are the requirements for your vCenter server:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Software: vCenter (It is recommended to run the same server version as the
    ESXi hosts.)
   </p></li><li class="listitem "><p>
    License Requirements: vSphere Enterprise Plus license
   </p></li></ul></div></div><div class="sect1" id="hw-support-perfnotes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes About Performance</span> <a title="Permalink" class="permalink" href="#hw-support-perfnotes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_perfnotes.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_perfnotes.xml</li><li><span class="ds-label">ID: </span>hw-support-perfnotes</li></ul></div></div></div></div><p>
  We have the following recommendations to ensure good performance of your
  cloud environment:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    On the control plane nodes, you will want good I/O performance. Your array
    controllers must have cache controllers and we advise against the use of
    RAID-5.
   </p></li><li class="listitem "><p>
    On compute nodes, the I/O performance will influence the virtual machine
    start-up performance. We also recommend the use of cache controllers in
    your storage arrays.
   </p></li><li class="listitem "><p>
    If you are using dedicated object storage (Swift) nodes, in particular the
    account, container, and object servers, we recommend that your storage
    arrays have cache controllers.
   </p></li><li class="listitem "><p>
    For best performance on, set the servers power management
    setting in the iLO to OS Control Mode. This power mode setting is only
    available on servers that include the HP Power Regulator.
   </p></li></ul></div></div><div class="sect1" id="hw-support-kvmguestos"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM Guest OS Support</span> <a title="Permalink" class="permalink" href="#hw-support-kvmguestos">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_kvmguestos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_kvmguestos.xml</li><li><span class="ds-label">ID: </span>hw-support-kvmguestos</li></ul></div></div></div></div><p>
  For a list of the supported VM guests, see
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#virt-support-guests" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#virt-support-guests</a>
 </p></div><div class="sect1" id="hw-support-esxguestos"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX Guest OS Support</span> <a title="Permalink" class="permalink" href="#hw-support-esxguestos">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_esxguestos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_esxguestos.xml</li><li><span class="ds-label">ID: </span>hw-support-esxguestos</li></ul></div></div></div></div><p>
  For ESX, refer to the <a class="link" href="https://www.vmware.com/resources/compatibility/search.php?deviceCategory=software&amp;details=1&amp;releases=273,274,338&amp;productNames=15&amp;page=1&amp;display_interval=500&amp;sortColumn=Partner&amp;sortOrder=Asc&amp;testConfig=16" target="_blank">VMware
  Compatibility Guide</a>. The information for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is below the
  search form.
 </p></div><div class="sect1" id="hw-support-ironicguestos"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Guest OS Support</span> <a title="Permalink" class="permalink" href="#hw-support-ironicguestos">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-hw_support_ironicguestos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_ironicguestos.xml</li><li><span class="ds-label">ID: </span>hw-support-ironicguestos</li></ul></div></div></div></div><p>
  A <span class="bold"><strong>Verified</strong></span> Guest OS has been tested by
  <span class="phrase"><span class="phrase">SUSE</span></span> and appears to function properly as a bare metal instance on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
 </p><p>
  A <span class="bold"><strong>Certified</strong></span> Guest OS has been officially
  tested by the operating system vendor, or by <span class="phrase"><span class="phrase">SUSE</span></span> under the vendor's
  authorized program, and will be supported by the operating system vendor as a
  bare metal instance on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Ironic Guest Operating System</th><th>Verified</th><th>Certified</th></tr></thead><tbody><tr><td>SUSE Linux Enterprise Server 12 SP3</td><td>Yes</td><td>Yes</td></tr></tbody></table></div></div></div><div class="chapter " id="idg-planning-planning-recommended-hardware-minimums-xml-1"><div class="titlepage"><div><div><h2 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for the Example Configurations</span> <a title="Permalink" class="permalink" href="#idg-planning-planning-recommended-hardware-minimums-xml-1">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-recommended_hardware_minimums.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-recommended_hardware_minimums.xml</li><li><span class="ds-label">ID: </span>idg-planning-planning-recommended-hardware-minimums-xml-1</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#rec-min-entryscale-kvm"><span class="number">3.1 </span><span class="name">Recommended Hardware Minimums for an Entry-scale KVM</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm"><span class="number">3.2 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX KVM Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm-mml"><span class="number">3.3 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX, KVM with Dedicated Cluster for Metering, Monitoring, and Logging</span></a></span></dt><dt><span class="section"><a href="#rec-min-ironic"><span class="number">3.4 </span><span class="name">Recommended Hardware Minimums for an Ironic Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-swift"><span class="number">3.5 </span><span class="name">Recommended Hardware Minimums for an Entry-scale Swift Model</span></a></span></dt></dl></div></div><div class="sect1" id="rec-min-entryscale-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale KVM</span> <a title="Permalink" class="permalink" href="#rec-min-entryscale-kvm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-rec_min_entryscale_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_entryscale_kvm.xml</li><li><span class="ds-label">ID: </span>rec-min-entryscale-kvm</li></ul></div></div></div></div><p>
  These recommended minimums are based on example configurations included with
  the installation models (see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>). They
  are suitable only for demo environments. For production systems you will
  want to consider your capacity and performance requirements when making
  decisions about your hardware.
 </p><div id="id-1.3.3.4.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The disk requirements detailed below can be met with logical drives, logical
   volumes, or external storage such as a 3PAR array.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">
      Server Hardware - Minimum Requirements and Recommendations
     </th></tr><tr><th>Disk</th><th>Memory</th><th>Network</th><th>CPU</th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute</td><td>Compute</td><td>1-3</td><td>2 x 600 GB (minimum)</td><td>32 GB (memory must be sized based on the virtual machine instances hosted on the
            Compute node)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64) with hardware virtualization support. The
            CPU cores must be sized based on the VM instances hosted by the Compute node.</td></tr></tbody></table></div><p>
  For more details about the supported network requirements, see
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
 </p></div><div class="sect1" id="rec-min-entryscale-esx-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale ESX KVM Model</span> <a title="Permalink" class="permalink" href="#rec-min-entryscale-esx-kvm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-rec_min_entryscale_esx_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_entryscale_esx_kvm.xml</li><li><span class="ds-label">ID: </span>rec-min-entryscale-esx-kvm</li></ul></div></div></div></div><p>
  These recommended minimums are based on example configurations included with
  the installation models (see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>). They
  are suitable only for demo environments. For production systems you will want
  to consider your capacity and performance requirements when making decisions
  about your hardware.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> currently supports the following ESXi versions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    ESXi version 6.0
   </p></li><li class="listitem "><p>
    ESXi version 6.0 (Update 1b)
   </p></li><li class="listitem "><p>
    ESXi version 6.5
   </p></li></ul></div><p>
  The following are the requirements for your vCenter server:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Software: vCenter (It is recommended to run the same
    server version as the ESXi hosts.)
   </p></li><li class="listitem "><p>
    License Requirements: vSphere Enterprise Plus license
   </p></li></ul></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">Server Hardware - Minimum Requirements and
            Recommendations</th></tr><tr><th>Disk</th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (ESXi hypervisor)</td><td> </td><td>2</td><td>2 x 1 TB (minimum, shared across all nodes)</td><td>128 GB (minimum)</td><td>2 x 10 Gbit/s +1 NIC (for DC access)</td><td>16 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (KVM hypervisor)</td><td>kvm-compute</td><td>1-3</td><td>2 x 600 GB (minimum)</td><td>
      32 GB (memory must be sized based on the virtual machine instances
      hosted on the Compute node)
     </td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>
      8 CPU (64-bit) cores total (Intel x86_64) with hardware virtualization
      support. The CPU cores must be sized based on the VM instances hosted
      by the Compute node.
     </td></tr></tbody></table></div></div><div class="sect1" id="rec-min-entryscale-esx-kvm-mml"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale ESX, KVM with Dedicated Cluster for Metering, Monitoring, and Logging</span> <a title="Permalink" class="permalink" href="#rec-min-entryscale-esx-kvm-mml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-rec_min_entryscale_esx_kvm_mml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_entryscale_esx_kvm_mml.xml</li><li><span class="ds-label">ID: </span>rec-min-entryscale-esx-kvm-mml</li></ul></div></div></div></div><p>
  These recommended minimums are based on example configurations included with
  the installation models (see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>). They
  are suitable only for demo environments. For production systems you will want
  to consider your capacity and performance requirements when making decisions
  about your hardware.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> currently supports the following ESXi versions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    ESXi version 6.0
   </p></li><li class="listitem "><p>
    ESXi version 6.0 (Update 1b)
   </p></li><li class="listitem "><p>
    ESXi version 6.5
   </p></li></ul></div><p>
  The following are the requirements for your vCenter server:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Software: vCenter (It is recommended to run the same
    server version as the ESXi hosts.)
   </p></li><li class="listitem "><p>
    License Requirements: vSphere Enterprise Plus license
   </p></li></ul></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">
      Server Hardware - Minimum Requirements and Recommendations
     </th></tr><tr><th>Disk</th><th>Memory</th><th>Network</th><th>CPU</th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td rowspan="3">Control Plane</td><td>Core-API Controller</td><td>2</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 300 GB (minimum) - Swift drive
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with PXE Support</td><td>24 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>DBMQ Cluster</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         1 x 300 GB (minimum) - MariaDB drive
        </p></li></ul></div>
     </td><td>96 GB</td><td>2 x 10 Gbit/s with PXE Support</td><td>24 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Metering Mon/Log Cluster</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>24 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (ESXi hypervisor)</td><td> </td><td>2 (minimum)</td><td>2 X 1 TB (minimum, shared across all nodes)</td><td>64 GB (memory must be sized based on the virtual machine instances hosted on the
            Compute node)</td><td>2 x 10 Gbit/s +1 NIC (for Data Center access)</td><td>16 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (KVM hypervisor)</td><td>kvm-compute</td><td>1-3</td><td>2 X 600 GB (minimum)</td><td>32 GB (memory must be sized based on the virtual machine instances hosted on the
            Compute node)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64) with hardware virtualization support. The
            CPU cores must be sized based on the VM instances hosted by the Compute node.</td></tr></tbody></table></div></div><div class="sect1" id="rec-min-ironic"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Ironic Flat Network Model</span> <a title="Permalink" class="permalink" href="#rec-min-ironic">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-rec_min_ironic.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_ironic.xml</li><li><span class="ds-label">ID: </span>rec-min-ironic</li></ul></div></div></div></div><p>
  When using the <code class="literal">agent_ilo</code> driver, you should ensure that
  the most recent iLO controller firmware is installed. A recommended minimum
  for the iLO4 controller is version 2.30.
 </p><p>
  The recommended minimum hardware requirements are based on the
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a> included with the base installation
  and are suitable only for demo environments. For production systems you will
  want to consider your capacity and performance requirements when making
  decisions about your hardware.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">
      Server Hardware - Minimum Requirements and Recommendations</th></tr><tr><th>Disk </th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute</td><td>Compute</td><td>1</td><td>1 x 600 GB (minimum)</td><td>16 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>16 CPU (64-bit) cores total (Intel x86_64)</td></tr></tbody></table></div><p>
  For more details about the supported network requirements, see
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
 </p></div><div class="sect1" id="rec-min-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale Swift Model</span> <a title="Permalink" class="permalink" href="#rec-min-swift">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-rec_min_swift.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_swift.xml</li><li><span class="ds-label">ID: </span>rec-min-swift</li></ul></div></div></div></div><p>
  These recommended minimums are based on the included
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a> included with the base installation
  and are suitable only for demo environments. For production systems you will
  want to consider your capacity and performance requirements when making
  decisions about your hardware.
 </p><p>
  The <code class="literal">entry-scale-swift</code> example runs the Swift proxy,
  account and container services on the three controller servers. However, it
  is possible to extend the model to include the Swift proxy, account and
  container services on dedicated servers (typically referred to as the Swift
  proxy servers). If you are using this model, we have included the recommended
  Swift proxy servers specs in the table below.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">Server Hardware - Minimum Requirements and
            Recommendations</th></tr><tr><th>Disk </th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Swift account/container data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Swift Object</td><td>swobj</td><td>3</td><td>
      <p>
       If using x3 replication only:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
      <p>
       If using Erasure Codes only or a mix of x3 replication and Erasure
       Codes:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         6 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
     </td><td>32 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Swift Proxy, Account, and Container</td><td>swpac</td><td>3</td><td>2 x 600 GB (minimum, see considerations at bottom of page for more details)</td><td>64 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr></tbody></table></div><div id="id-1.3.3.4.6.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The disk speeds (RPM) chosen should be consistent within the same ring or
   storage policy. It is best to not use disks with mixed disk speeds within the
   same Swift ring.
  </p></div><p>
  <span class="bold"><strong>Considerations for your Swift object and proxy,
  account, container servers RAM and disk capacity needs</strong></span>
 </p><p>
  Swift can have a diverse number of hardware configurations. For example, a
  Swift object server may have just a few disks (minimum of 6 for erasure
  codes) or up to 70 and beyond. The memory requirement needs to be increased
  as more disks are added. The general rule of thumb for memory needed is 0.5
  GB per TB of storage. For example, a system with 24 hard drives at 8TB each,
  giving a total capacity of 192TB, should use 96GB of RAM. However, this does
  not work well for a system with a small number of small hard drives or a very
  large number of very large drives. So, if after calculating the memory given
  this guideline, if the answer is less than 32GB then go with 32GB of memory
  minimum and if the answer is over 256GB then use 256GB maximum, no need to
  use more memory than that.
 </p><p>
  When considering the capacity needs for the Swift proxy, account, and
  container (PAC) servers, you should calculate 2% of the total raw storage
  size of your object servers to specify the storage required for the PAC
  servers. So, for example, if you were using the example we provided earlier
  and you had an object server setup of 24 hard drives with 8TB each for a
  total of 192TB and you had a total of 6 object servers, that would give a raw
  total of 1152TB. So you would take 2% of that, which is 23TB, and ensure that
  much storage capacity was available on your Swift proxy, account, and
  container (PAC) server cluster. If you had a cluster of three Swift PAC
  servers, that would be ~8TB each.
 </p><p>
  Another general rule of thumb is that if you are expecting to have more than
  a million objects in a container then you should consider using SSDs on the
  Swift PAC servers rather than HDDs.
 </p></div></div><div class="chapter " id="HP3-0HA"><div class="titlepage"><div><div><h2 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability</span> <a title="Permalink" class="permalink" href="#HP3-0HA">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>HP3-0HA</li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.3.5.2.1">#</a></h6></div><p>
    This chapter covers High Availability concepts overview and cloud
    infrastructure.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#concepts-overview"><span class="number">4.1 </span><span class="name">High Availability Concepts Overview</span></a></span></dt><dt><span class="section"><a href="#highly-available-cloud-infrastructure"><span class="number">4.2 </span><span class="name">Highly Available Cloud Infrastructure</span></a></span></dt><dt><span class="section"><a href="#high-availablity-controllers"><span class="number">4.3 </span><span class="name">High Availability of Controllers</span></a></span></dt><dt><span class="section"><a href="#CVR"><span class="number">4.4 </span><span class="name">High Availability Routing - Centralized</span></a></span></dt><dt><span class="section"><a href="#DVR"><span class="number">4.5 </span><span class="name">High Availability Routing - Distributed</span></a></span></dt><dt><span class="section"><a href="#availability-zones"><span class="number">4.6 </span><span class="name">Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-kvm"><span class="number">4.7 </span><span class="name">Compute with KVM</span></a></span></dt><dt><span class="section"><a href="#nova-availability-zones"><span class="number">4.8 </span><span class="name">Nova Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-esx"><span class="number">4.9 </span><span class="name">Compute with ESX Hypervisor</span></a></span></dt><dt><span class="section"><a href="#cinder-availability-zones"><span class="number">4.10 </span><span class="name">Cinder Availability Zones</span></a></span></dt><dt><span class="section"><a href="#object-storage-swift"><span class="number">4.11 </span><span class="name">Object Storage with Swift</span></a></span></dt><dt><span class="section"><a href="#highly-available-app-workloads"><span class="number">4.12 </span><span class="name">Highly Available Cloud Applications and Workloads</span></a></span></dt><dt><span class="section"><a href="#what-not-ha"><span class="number">4.13 </span><span class="name">What is not Highly Available?</span></a></span></dt><dt><span class="section"><a href="#more-information"><span class="number">4.14 </span><span class="name">More Information</span></a></span></dt></dl></div></div><div class="sect1" id="concepts-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Concepts Overview</span> <a title="Permalink" class="permalink" href="#concepts-overview">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>concepts-overview</li></ul></div></div></div></div><p>
   A highly available (HA) cloud ensures that a minimum level of cloud
   resources are always available on request, which results in uninterrupted
   operations for users.
  </p><p>
   In order to achieve this high availability of infrastructure and workloads,
   we define the scope of HA to be limited to protecting these only against
   single points of failure (SPOF). Single points of failure include:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Hardware SPOFs</strong></span>: Hardware failures can
     take the form of server failures, memory going bad, power failures,
     hypervisors crashing, hard disks dying, NIC cards breaking, switch ports
     failing, network cables loosening, and so forth.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Software SPOFs</strong></span>: Server processes can
     crash due to software defects, out-of-memory conditions, operating system
     kernel panic, and so forth.
    </p></li></ul></div><p>
   By design, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> strives to create a system architecture resilient to
   SPOFs, and does not attempt to automatically protect the system against
   multiple cascading levels of failures; such cascading failures will result
   in an unpredictable state. The cloud operator is encouraged to recover and
   restore any failed component as soon as the first level of failure occurs.
  </p></div><div class="sect1" id="highly-available-cloud-infrastructure"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Highly Available Cloud Infrastructure</span> <a title="Permalink" class="permalink" href="#highly-available-cloud-infrastructure">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>highly-available-cloud-infrastructure</li></ul></div></div></div></div><p>
   The highly available cloud infrastructure consists of the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     High Availability of Controllers
    </p></li><li class="listitem "><p>
     Availability Zones
    </p></li><li class="listitem "><p>
     Compute with KVM
    </p></li><li class="listitem "><p>
     Nova Availability Zones
    </p></li><li class="listitem "><p>
     Compute with ESX
    </p></li><li class="listitem "><p>
     Object Storage with Swift
    </p></li></ul></div></div><div class="sect1" id="high-availablity-controllers"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability of Controllers</span> <a title="Permalink" class="permalink" href="#high-availablity-controllers">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>high-availablity-controllers</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer deploys highly available configurations of OpenStack
   cloud services, resilient against single points of failure.
  </p><p>
   The high availability of the controller components comes in two main forms.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Many services are stateless and multiple instances are run across the
     control plane in active-active mode. The API services (nova-api,
     cinder-api, etc.) are accessed through the HA proxy load balancer whereas
     the internal services (nova-scheduler, cinder-scheduler, etc.), are
     accessed through the message broker. These services use the database
     cluster to persist any data.
    </p><div id="id-1.3.3.5.5.4.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The HA proxy load balancer is also run in active-active mode and
      keepalived (used for Virtual IP (VIP) Management) is run in active-active
      mode, with only one keepalived instance holding the VIP at any one point
      in time.
     </p></div></li><li class="listitem "><p>
     The high availability of the message queue service and the database
     service is achieved by running these in a clustered mode across the three
     nodes of the control plane: RabbitMQ cluster with Mirrored Queues and
     MariaDB Galera cluster.
    </p></li></ul></div><div class="figure" id="ControlPlane1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HPE_HA_Flow.png" target="_blank"><img src="images/media-ha30-HPE_HA_Flow.png" width="" alt="HA Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.1: </span><span class="name">HA Architecture </span><a title="Permalink" class="permalink" href="#ControlPlane1">#</a></h6></div></div><p>
   The above diagram illustrates the HA architecture with the focus on VIP
   management and load balancing. It only shows a subset of active-active API
   instances and does not show examples of other services such as
   nova-scheduler, cinder-scheduler, etc.
  </p><p>
   In the above diagram, requests from an OpenStack client to the API services
   are sent to VIP and port combination; for example, 192.0.2.26:8774 for a
   Nova request. The load balancer listens for requests on that VIP and port.
   When it receives a request, it selects one of the controller nodes
   configured for handling Nova requests, in this particular case, and then
   forwards the request to the IP of the selected controller node on the same
   port.
  </p><p>
   The nova-api service, which is listening for requests on the IP of its host
   machine, then receives the request and deals with it accordingly. The
   database service is also accessed through the load balancer. RabbitMQ, on
   the other hand, is not currently accessed through VIP/HA proxy as the
   clients are configured with the set of nodes in the RabbitMQ cluster and
   failover between cluster nodes is automatically handled by the clients.
  </p></div><div class="sect1" id="CVR"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Routing - Centralized</span> <a title="Permalink" class="permalink" href="#CVR">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>CVR</li></ul></div></div></div></div><p>
   Incorporating High Availability into a system involves implementing
   redundancies in the component that is being made highly available. In
   Centralized Virtual Router (CVR), that element is the Layer 3 agent (L3
   agent). By making L3 agent highly available, upon failure all HA routers are
   migrated from the primary L3 agent to a secondary L3 agent. The
   implementation efficiency of an HA subsystem is measured by the number of
   packets that are lost when the secondary L3 agent is made the master.
  </p><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the primary and secondary L3 agents run continuously, and
   failover involves a rapid switchover of mastership to the secondary agent
   (IEFT RFC 5798). The failover essentially involves a switchover from an
   already running master to an already running slave. This substantially
   reduces the latency of the HA. The mechanism used by the master and the
   slave to implement a failover is implemented using Linux’s pacemaker HA
   resource manager. This CRM (Cluster resource manager) uses VRRP (Virtual
   Router Redundancy Protocol) to implement the HA mechanism. VRRP is a
   industry standard protocol and defined in RFC 5798.
  </p><div class="figure" id="Layer3HA"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HPE_HA_Layer-3HA.png" target="_blank"><img src="images/media-ha30-HPE_HA_Layer-3HA.png" width="" alt="Layer-3 HA" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.2: </span><span class="name">Layer-3 HA </span><a title="Permalink" class="permalink" href="#Layer3HA">#</a></h6></div></div><p>
   L3 HA uses of VRRP comes with several benefits.
  </p><p>
   The primary benefit is that the failover mechanism does not involve
   interprocess communication overhead. Such overhead would be in the order of
   10s of seconds. By not using an RPC mechanism to invoke the secondary agent
   to assume the primary agents role enables VRRP to achieve failover within
   1-2 seconds.
  </p><p>
   In VRRP, the primary and secondary routers are all active. As the routers
   are running, it is a matter of making the router aware of its primary/master
   status. This switchover takes less than 2 seconds instead of 60+ seconds it
   would have taken to start a backup router and failover.
  </p><p>
   The failover depends upon a heartbeat link between the primary and
   secondary. That link in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses keepalived package of the
   pacemaker resource manager. The heartbeats are sent at a 2 second intervals
   between the primary and secondary. As per the VRRP protocol, if the
   secondary does not hear from the master after 3 intervals, it assumes the
   function of the primary.
  </p><p>
   Further, all the routable IP addresses, that is the VIPs (virtual IPs) are
   assigned to the primary agent.
  </p></div><div class="sect1" id="DVR"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Routing - Distributed</span> <a title="Permalink" class="permalink" href="#DVR">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>DVR</li></ul></div></div></div></div><p>
   The OpenStack Distributed Virtual Router (DVR) function delivers HA through
   its distributed architecture. The one centralized function remaining is
   source network address translation (SNAT), where high availability is
   provided by DVR SNAT HA.
  </p><p>
   DVR SNAT HA is enabled on a per router basis and requires that two or more
   L3 agents capable of providing SNAT services be running on the system. If a
   minimum number of L3 agents is configured to 1 or lower, the neutron server
   will fail to start and a log message will be created. The L3 Agents must be
   running on a control-plane node, L3 agents running on a compute node do not
   provide SNAT services.
  </p></div><div class="sect1" id="availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Availability Zones</span> <a title="Permalink" class="permalink" href="#availability-zones">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>availability-zones</li></ul></div></div></div></div><div class="figure" id="DeploymentZones"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HA_AvailabilityZones_3.png" target="_blank"><img src="images/media-ha30-HA_AvailabilityZones_3.png" width="" alt="Availability Zones" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.3: </span><span class="name">Availability Zones </span><a title="Permalink" class="permalink" href="#DeploymentZones">#</a></h6></div></div><p>
   While planning your OpenStack deployment, you should decide on how to zone
   various types of nodes - such as compute, block storage, and object storage.
   For example, you may decide to place all servers in the same rack in the
   same zone. For larger deployments, you may plan more elaborate redundancy
   schemes for redundant power, network ISP connection, and even physical
   firewalling between zones (<span class="emphasis"><em>this aspect is outside the scope of
   this document</em></span>).
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> offers APIs, CLIs and Horizon UIs for the administrator to define
   and user to consume, availability zones for Nova, Cinder and Swift services.
   This section outlines the process to deploy specific types of nodes to
   specific physical servers, and makes a statement of available support for
   these types of availability zones in the current release.
  </p><div id="id-1.3.3.5.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is deployed in a single availability zone upon
    installation. Multiple availability zones can be configured by an
    administrator post-install, if required. Refer to <a class="link" href="https://docs.openstack.org/openstack-ansible/pike/admin/maintenance-tasks/scale-environment.html" target="_blank">OpenStack
    Docs:Scaling your environment</a>
   </p></div></div><div class="sect1" id="compute-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute with KVM</span> <a title="Permalink" class="permalink" href="#compute-kvm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>compute-kvm</li></ul></div></div></div></div><p>
   You can deploy your KVM nova-compute nodes either during initial
   installation or by adding compute nodes post initial installation.
  </p><p>
   While adding compute nodes post initial installation, you can specify the
   target physical servers for deploying the compute nodes.
  </p><p>
   Learn more about adding compute nodes in
   <span class="intraxref">Book “Operations Guide”, Chapter 13 “System Maintenance”, Section 13.1 “Planned System Maintenance”, Section 13.1.3 “Planned Compute Maintenance”, Section 13.1.3.4 “Adding Compute Node”</span>.
  </p></div><div class="sect1" id="nova-availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nova Availability Zones</span> <a title="Permalink" class="permalink" href="#nova-availability-zones">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>nova-availability-zones</li></ul></div></div></div></div><p>
   Nova host aggregates and Nova availability zones can be used to segregate
   Nova compute nodes across different failure zones.
  </p></div><div class="sect1" id="compute-esx"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute with ESX Hypervisor</span> <a title="Permalink" class="permalink" href="#compute-esx">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>compute-esx</li></ul></div></div></div></div><p>
   Compute nodes deployed on ESX Hypervisor can be made highly available using
   the HA feature of VMware ESX Clusters. For more information on VMware
   HA, please refer to your VMware ESX documentation.
  </p></div><div class="sect1" id="cinder-availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cinder Availability Zones</span> <a title="Permalink" class="permalink" href="#cinder-availability-zones">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>cinder-availability-zones</li></ul></div></div></div></div><p>
   Cinder availability zones are not supported for general consumption in the
   current release.
  </p></div><div class="sect1" id="object-storage-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage with Swift</span> <a title="Permalink" class="permalink" href="#object-storage-swift">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>object-storage-swift</li></ul></div></div></div></div><p>
   High availability in Swift is achieved at two levels.
  </p><p>
   <span class="bold"><strong>Control Plane</strong></span>
  </p><p>
   The Swift API is served by multiple Swift proxy nodes. Client requests are
   directed to all Swift proxy nodes by the HA Proxy load balancer in
   round-robin fashion. The HA Proxy load balancer regularly checks the node is
   responding, so that if it fails, traffic is directed to the remaining nodes.
   The Swift service will continue to operate and respond to client requests as
   long as at least one Swift proxy server is running.
  </p><p>
   If a Swift proxy node fails in the middle of a transaction, the transaction
   fails. However it is standard practice for Swift clients to retry
   operations. This is transparent to applications that use the
   python-swiftclient library.
  </p><p>
   The entry-scale example cloud models contain three Swift proxy nodes.
   However, it is possible to add additional clusters with additional Swift
   proxy nodes to handle a larger workload or to provide additional resiliency.
  </p><p>
   <span class="bold"><strong>Data</strong></span>
  </p><p>
   Multiple replicas of all data is stored. This happens for account, container
   and object data. The example cloud models recommend a replica count of
   three. However, you may change this to a higher value if needed.
  </p><p>
   When Swift stores different replicas of the same item on disk, it ensures
   that as far as possible, each replica is stored in a different zone, server
   or drive. This means that if a single server of disk drives fails, there
   should be two copies of the item on other servers or disk drives.
  </p><p>
   If a disk drive is failed, Swift will continue to store three replicas. The
   replicas that would normally be stored on the failed drive are “handed
   off” to another drive on the system. When the failed drive is replaced,
   the data on that drive is reconstructed by the replication process. The
   replication process re-creates the <span class="quote">“<span class="quote ">missing</span>”</span> replicas by
   copying them to the drive using one of the other remaining replicas. While
   this is happening, Swift can continue to store and retrieve data.
  </p></div><div class="sect1" id="highly-available-app-workloads"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Highly Available Cloud Applications and Workloads</span> <a title="Permalink" class="permalink" href="#highly-available-app-workloads">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>highly-available-app-workloads</li></ul></div></div></div></div><p>
   Projects writing applications to be deployed in the cloud must be aware of
   the cloud architecture and potential points of failure and architect their
   applications accordingly for high availability.
  </p><p>
   Some guidelines for consideration:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Assume intermittent failures and plan for retries
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>OpenStack Service APIs</strong></span>: invocations can
       fail - you should carefully evaluate the response of each invocation,
       and retry in case of failures.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span>: VMs can die - monitor and
       restart them
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Network</strong></span>: Network calls can fail - retry
       should be successful
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Storage</strong></span>: Storage connection can hiccup
       - retry should be successful
      </p></li></ul></div></li><li class="listitem "><p>
     Build redundancy into your application tiers
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Replicate VMs containing stateless services such as Web application tier
       or Web service API tier and put them behind load balancers (you must
       implement your own HA Proxy type load balancer in your application VMs
       until <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> delivers the LBaaS service).
      </p></li><li class="listitem "><p>
       Boot the replicated VMs into different Nova availability zones.
      </p></li><li class="listitem "><p>
       If your VM stores state information on its local disk (Ephemeral
       Storage), and you cannot afford to lose it, then boot the VM off a
       Cinder volume.
      </p></li><li class="listitem "><p>
       Take periodic snapshots of the VM which will back it up to Swift through
       Glance.
      </p></li><li class="listitem "><p>
       Your data on ephemeral may get corrupted (but not your backup data in
       Swift and not your data on Cinder volumes).
      </p></li><li class="listitem "><p>
       Take regular snapshots of Cinder volumes and also back up Cinder volumes
       or your data exports into Swift.
      </p></li></ul></div></li><li class="listitem "><p>
     Instead of rolling your own highly available stateful services, use
     readily available <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> platform services such as Designate, the DNS
     service.
    </p></li></ol></div></div><div class="sect1" id="what-not-ha"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is not Highly Available?</span> <a title="Permalink" class="permalink" href="#what-not-ha">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>what-not-ha</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.3.5.15.2.1"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
      The Cloud Lifecycle Manager in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is not highly available. The Cloud Lifecycle Manager state/data are
      all maintained in a filesystem and are backed up by the Freezer service.
      In case of Cloud Lifecycle Manager failure, the state/data can be recovered from the
      backup.
     </p></dd><dt id="id-1.3.3.5.15.2.2"><span class="term ">Control Plane</span></dt><dd><p>
      High availability (HA) is supported for the Network Services LBaaS and
      FWaaS. HA is <span class="bold"><strong>not</strong></span> supported for VPNaaS.
     </p></dd><dt id="id-1.3.3.5.15.2.3"><span class="term ">Nova-consoleauth</span></dt><dd><p>
      Nova-consoleauth is a singleton service, it can only run on a single node
      at a time. While nova-consoleauth is not high availability, some work has
      been done to provide the ability to switch nova-consoleauth to another
      controller node in case of a failure.
      
     </p></dd><dt id="id-1.3.3.5.15.2.4"><span class="term ">Cinder Volume and Backup Services</span></dt><dd><p>
      Cinder Volume and Backup Services are not high availability and started
      on one controller node at a time.
      More information on Cinder Volume and Backup Services can
      be found in <span class="intraxref">Book “Operations Guide”, Chapter 7 “Managing Block Storage”, Section 7.1 “Managing Block Storage using Cinder”, Section 7.1.3 “Managing Cinder Volume and Backup Services”</span>.
     </p></dd><dt id="id-1.3.3.5.15.2.5"><span class="term ">Keystone Cron Jobs</span></dt><dd><p>
      The Keystone cron job is a singleton service, which can only run on a
      single node at a time. A manual setup process for this job will be
      required in case of a node failure.
      More information on enabling the cron job for Keystone on
      the other nodes can be found in <span class="intraxref">Book “Operations Guide”, Chapter 4 “Managing Identity”, Section 4.12 “Identity Service Notes and Limitations”, Section 4.12.4 “System cron jobs need setup”</span>.
     </p></dd></dl></div></div><div class="sect1" id="more-information"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#more-information">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>more-information</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="link" href="https://docs.openstack.org/ha-guide/" target="_blank">OpenStack
     High-availability Guide</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://12factor.net/" target="_blank">12-Factor Apps</a>
    </p></li></ul></div></div></div></div><div class="part" id="architecture"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part II </span><span class="name">Cloud Lifecycle Manager Overview </span><a title="Permalink" class="permalink" href="#architecture">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-architecture_index.xml" title="Edit the source file for this section">Edit source</a></h1></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.2.1">#</a></h6></div><p>
    This section contains information on the Input Model and the Example
    Configurations.
   </p></div></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-input-model-intro-concept"><span class="number">5 </span><span class="name">Input Model</span></a></span></dt><dd class="toc-abstract"><p>
  This document describes how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input models can be used to define and
  configure the cloud.
 </p></dd><dt><span class="chapter"><a href="#configurationobjects"><span class="number">6 </span><span class="name">Configuration Objects</span></a></span></dt><dd class="toc-abstract"><p>
  The top-level cloud configuration file, <code class="filename">cloudConfig.yml</code>,
  defines some global values for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, as described in the table below.
 </p></dd><dt><span class="chapter"><a href="#othertopics"><span class="number">7 </span><span class="name">Other Topics</span></a></span></dt><dd class="toc-abstract"><p>Names are generated by the configuration processor for all allocated IP addresses. A server connected to multiple networks will have multiple names associated with it. One of these may be assigned as the hostname for a server via the network-group configuration (see Section 6.12, “NIC Mappings”). Na…</p></dd><dt><span class="chapter"><a href="#cpinfofiles"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></span></dt><dd class="toc-abstract"><p>
  In addition to producing all of the data needed to deploy and configure the
  cloud, the configuration processor also creates a number of information files
  that provide details of the resulting configuration.
 </p></dd><dt><span class="chapter"><a href="#example-configurations"><span class="number">9 </span><span class="name">Example Configurations</span></a></span></dt><dd class="toc-abstract"><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> system ships with a collection of pre-qualified example
  configurations. These are designed to help you to get up and running quickly
  with a minimum number of configuration changes.
 </p></dd><dt><span class="chapter"><a href="#modify-compute-input-model"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></span></dt><dd class="toc-abstract"><p>
  This section contains detailed information about the Compute Node parts of
  the input model. For example input models, see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>. For general descriptions of the input
  model, see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>.
 </p></dd><dt><span class="chapter"><a href="#modify-input-model"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></span></dt><dd class="toc-abstract"><p>This section contains detailed descriptions about the Swift-specific parts of the input model. For example input models, see Chapter 9, Example Configurations. For general descriptions of the input model, see Section 6.14, “Networks”. In addition, the Swift ring specifications are available in the ~…</p></dd><dt><span class="chapter"><a href="#alternative-configurations"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></span></dt><dd class="toc-abstract"><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> there are alternative configurations that we recommend
    for specific purposes.
   </p></dd></dl></div><div class="chapter " id="cha-input-model-intro-concept"><div class="titlepage"><div><div><h2 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input Model</span> <a title="Permalink" class="permalink" href="#cha-input-model-intro-concept">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-input_model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-input_model.xml</li><li><span class="ds-label">ID: </span>cha-input-model-intro-concept</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#input-model-introduction"><span class="number">5.1 </span><span class="name">Introduction to the Input Model</span></a></span></dt><dt><span class="section"><a href="#concepts"><span class="number">5.2 </span><span class="name">Concepts</span></a></span></dt></dl></div></div><div class="sect1" id="input-model-introduction"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction to the Input Model</span> <a title="Permalink" class="permalink" href="#input-model-introduction">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-input_model_introduction.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-input_model_introduction.xml</li><li><span class="ds-label">ID: </span>input-model-introduction</li></ul></div></div></div></div><p>
  This document describes how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input models can be used to define and
  configure the cloud.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with a set of example input models that can be used as
  starting points for defining a custom cloud. An input model allows you, the
  cloud administrator, to describe the cloud configuration in terms of:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Which OpenStack services run on which server nodes
   </p></li><li class="listitem "><p>
    How individual servers are configured in terms of disk and network adapters
   </p></li><li class="listitem "><p>
    The overall network configuration of the cloud
   </p></li><li class="listitem "><p>
    Network traffic separation
   </p></li><li class="listitem "><p>
    CIDR and VLAN assignments
   </p></li></ul></div><p>
  The input model is consumed by the configuration processor which parses and
  validates the input model and outputs the effective configuration that will
  be deployed to each server that makes up your cloud.
 </p><p>
  The document is structured as follows:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="guimenu ">Concepts</span> - This explains the ideas behind the
    declarative model approach used in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> and the core concepts
    used in describing that model
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Input Model</span> - This section provides a description of
    each of the configuration entities in the input model
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Core Examples</span> - In this section we provide samples and
    definitions of some of the more important configuration entities
   </p></li></ul></div></div><div class="sect1" id="concepts"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Concepts</span> <a title="Permalink" class="permalink" href="#concepts">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-concepts.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-concepts.xml</li><li><span class="ds-label">ID: </span>concepts</li></ul></div></div></div></div><p>
  An <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> cloud is defined by a declarative model that is described
  in a series of configuration objects. These configuration objects are
  represented in YAML files which together constitute the various example
  configurations provided as templates with this release. These examples can be
  used nearly unchanged, with the exception of necessary changes to IP
  addresses and other site and hardware-specific identifiers. Alternatively,
  the examples may be customized to meet site requirements.
 </p><p>
  The following diagram shows the set of configuration objects and their
  relationships. All objects have a name that you may set to be something
  meaningful for your context. In the examples these names are provided in
  capital letters as a convention. These names have no significance to
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, rather it is the relationships between them that define the
  configuration.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-HPE_InputModel_Flow_40.png" target="_blank"><img src="images/media-inputmodel-HPE_InputModel_Flow_40.png" width="" /></a></div></div><p>
  The configuration processor reads and validates the input model described in
  the YAML files discussed above, combines it with the service definitions
  provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and any persisted state information about the current
  deployment to produce a set of Ansible variables that can be used to deploy
  the cloud. It also produces a set of information files that provide details
  about the configuration.
 </p><p>
  The relationship between the file systems on the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment server
  and the configuration processor is shown in the following diagram. Below the
  line are the directories that you, the cloud administrator, edit to declare
  the cloud configuration. Above the line are the directories that are
  internal to the Cloud Lifecycle Manager such as Ansible playbooks and variables.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_directories.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_directories.png" width="" /></a></div></div><p>
  The input model is read from the
  <code class="filename">~/openstack/my_cloud/definition</code> directory. Although the
  supplied examples use separate files for each type of object in the model,
  the names and layout of the files have no significance to the configuration
  processor, it simply reads all of the .yml files in this directory. Cloud
  administrators are therefore free to use whatever structure is best for their
  context. For example, you may decide to maintain separate files or
  sub-directories for each physical rack of servers.
 </p><p>
  As mentioned, the examples use the conventional upper casing for object
  names, but these strings are used only to define the relationship between
  objects. They have no specific significance to the configuration processor.
 </p><div class="sect2" id="concept-cloud"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud</span> <a title="Permalink" class="permalink" href="#concept-cloud">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-cloud.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-cloud.xml</li><li><span class="ds-label">ID: </span>concept-cloud</li></ul></div></div></div></div><p>
  The Cloud definition includes a few top-level configuration values such as
  the name of the cloud, the host prefix, details of external services (NTP,
  DNS, SMTP) and the firewall settings.
 </p><p>
  The location of the cloud configuration file also tells the configuration
  processor where to look for the files that define all of the other objects in
  the input model.
 </p></div><div class="sect2" id="concept-controlplanes"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Planes</span> <a title="Permalink" class="permalink" href="#concept-controlplanes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-controlplanes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-controlplanes.xml</li><li><span class="ds-label">ID: </span>concept-controlplanes</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A control-plane runs one or more <span class="guimenu ">services</span>
  distributed across <span class="guimenu ">clusters</span> and <span class="guimenu ">resource
  groups</span></em></span>.
 </p><p>
  <span class="emphasis"><em>A control-plane uses servers with a particular
  <span class="guimenu ">server-role</span></em></span>.
 </p><p>
  A <span class="guimenu ">control-plane</span> provides the operating environment for a
  set of <span class="guimenu ">services</span>; normally consisting of a set of shared
  services (MariaDB, RabbitMQ, HA Proxy, Apache, etc.), OpenStack control
  services (API, schedulers, etc.) and the <span class="guimenu ">resources</span> they
  are managing (compute, storage, etc.).
 </p><p>
  A simple cloud may have a single <span class="guimenu ">control-plane</span> which runs
  all of the <span class="guimenu ">services</span>. A more complex cloud may have
  multiple <span class="guimenu ">control-planes</span> to allow for more than one
  instance of some services.
  
   Services that need to consume (use) another service
  (such as Neutron consuming MariaDB, Nova consuming Neutron) always use the
  service within the same <span class="guimenu ">control-plane</span>.
  In addition a control-plane can describe which services can be consumed
  from other control-planes. It is one of the functions of the configuration
  processor to resolve these relationships and make sure that each
  consumer/service is provided with the configuration details to connect to the
  appropriate provider/service.
 </p><p>
  Each <span class="guimenu ">control-plane</span> is structured as
  <span class="guimenu ">clusters</span> and <span class="guimenu ">resources</span>. The
  <span class="guimenu ">clusters</span> are typically used to host the OpenStack services
  that manage the cloud such as API servers, database servers, Neutron agents,
  and Swift proxies, while the <span class="guimenu ">resources</span> are used to host
  the scale-out OpenStack services such as Nova-Compute or Swift-Object
  services. This is a representation convenience rather than a strict rule, for
  example it is possible to run the Swift-Object service in the management
  cluster in a smaller-scale cloud that is not designed for scale-out object
  serving.
 </p><p>
  A cluster can contain one or more <span class="guimenu ">servers</span> and you can have
  one or more <span class="guimenu ">clusters</span> depending on the capacity and
  scalability needs of the cloud that you are building. Spreading services
  across multiple <span class="guimenu ">clusters</span> provides greater scalability, but
  it requires a greater number of physical servers. A common pattern for a
  large cloud is to run high data volume services such as monitoring and
  logging in a separate cluster. A cloud with a high object storage requirement
  will typically also run the Swift service in its own cluster.
 </p><p>
  Clusters in this context are a mechanism for grouping service components in
  physical servers, but all instances of a component in a
  <span class="guimenu ">control-plane</span> work collectively. For example, if HA Proxy
  is configured to run on multiple clusters within the same
  <span class="guimenu ">control-plane</span> then all of those instances will work as a
  single instance of the ha-proxy service.
 </p><p>
  Both <span class="guimenu ">clusters</span> and <span class="guimenu ">resources</span> define the
  type (via a list of <span class="guimenu ">server-roles</span>) and number of servers
  (min and max or count) they require.
 </p><p>
  The <span class="guimenu ">control-plane</span> can also define a list of failure-zones
  (<span class="guimenu ">server-groups</span>) from which to allocate servers.
 </p><div class="sect3" id="concept-controlplanes-regions"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Planes and Regions</span> <a title="Permalink" class="permalink" href="#concept-controlplanes-regions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-controlplanes_regions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-controlplanes_regions.xml</li><li><span class="ds-label">ID: </span>concept-controlplanes-regions</li></ul></div></div></div></div><p>
  A region in OpenStack terms is a collection of URLs that together provide a
  consistent set of services (Nova, Neutron, Swift, etc). Regions are
  represented in the Keystone identity service catalog. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>,
  multiple regions are not supported. Only <code class="literal">Region0</code> is valid.
 </p><p>
  In a simple single control-plane cloud, there is no need for a separate
  region definition and the control-plane itself can define the region name.
 </p></div></div><div class="sect2" id="concept-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Services</span> <a title="Permalink" class="permalink" href="#concept-services">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-services.xml</li><li><span class="ds-label">ID: </span>concept-services</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">control-plane</span> runs one or more
  <span class="guimenu ">services</span>.</em></span>
 </p><p>
  A service is the collection of <span class="guimenu ">service-components</span> that
  provide a particular feature; for example, Nova provides the compute service
  and consists of the following service-components: nova-api, nova-scheduler,
  nova-conductor, nova-novncproxy, and nova-compute. Some services, like the
  authentication/identity service Keystone, only consist of a single
  service-component.
 </p><p>
  To define your cloud, all you need to know about a service are the names of
  the <span class="guimenu ">service-components</span>. The details of the services
  themselves and how they interact with each other is captured in service
  definition files provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  When specifying your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud you have to decide where components will
  run and how they connect to the networks. For example, should they all run in
  one <span class="guimenu ">control-plane</span> sharing common services or be
  distributed across multiple <span class="guimenu ">control-planes</span> to provide
  separate instances of some services? The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supplied examples provide
  solutions for some typical configurations.
 </p><p>
  Where services run is defined in the <span class="guimenu ">control-plane</span>. How
  they connect to networks is defined in the <span class="guimenu ">network-groups</span>.
 </p></div><div class="sect2" id="concept-serverroles"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Roles</span> <a title="Permalink" class="permalink" href="#concept-serverroles">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-serverroles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-serverroles.xml</li><li><span class="ds-label">ID: </span>concept-serverroles</li></ul></div></div></div></div><p>
  <span class="emphasis"><em><span class="guimenu ">Clusters</span> and <span class="guimenu ">resources</span> use
  <span class="guimenu ">servers</span> with a particular set of
  <span class="guimenu ">server-role</span>s.</em></span>
 </p><p>
  You are going to be running the services on physical
  <span class="guimenu ">servers</span>, and you are going to need a way to specify
  which type of servers you want to use where. This is defined via the
  <span class="guimenu ">server-role</span>. Each <span class="guimenu ">server-role</span> describes
  how to configure the physical aspects of a server to fulfill the needs of a
  particular role. You will generally use a different role whenever the servers
  are physically different (have different disks or network interfaces) or if
  you want to use some specific servers in a particular role (for example to
  choose which of a set of identical servers are to be used in the control
  plane).
 </p><p>
  Each <span class="guimenu ">server-role</span> has a relationship to four other
  entities:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The <span class="guimenu ">disk-model</span> specifies how to configure and use a
    server's local storage and it specifies disk sizing information for
    virtual machine servers. The disk model is described in the next section.
   </p></li><li class="listitem "><p>
    The <span class="guimenu ">interface-model</span> describes how a server's network
    interfaces are to be configured and used. This is covered in more details
    in the networking section.
   </p></li><li class="listitem "><p>
    An optional <span class="guimenu ">memory-model</span> specifies how to configure
    and use huge pages. The memory-model specifies memory sizing information
    for virtual machine servers.
   </p></li><li class="listitem "><p>
    An optional <span class="bold"><strong> <span class="guimenu ">cpu-model</span> </strong></span>
    specifies how the CPUs will be used by Nova and by DPDK.
    The cpu-model specifies CPU sizing information for virtual machine
    servers.
   </p></li></ul></div></div><div class="sect2" id="concept-diskmodel"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disk Model</span> <a title="Permalink" class="permalink" href="#concept-diskmodel">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-diskmodel.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-diskmodel.xml</li><li><span class="ds-label">ID: </span>concept-diskmodel</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>Each physical disk device is associated with a
  <span class="guimenu ">device-group</span> or a
  <span class="guimenu ">volume-group</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Device-groups</span> are consumed by
  <span class="guimenu ">services</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Volume-groups</span> are divided into
  <span class="guimenu ">logical-volumes</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Logical-volumes</span> are mounted as file systems or
  consumed by services.</em></span>
 </p><p>
  Disk-models define how local storage is to be configured and presented to
  <span class="guimenu ">services</span>. Disk-models are identified by a name, which you
  will specify. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> examples provide some typical configurations. As
  this is an area that varies with respect to the services that are hosted on a
  server and the number of disks available, it is impossible to cover all
  possible permutations you may need to express via modifications to the
  examples.
 </p><p>
  Within a <span class="guimenu ">disk-model</span>, disk devices are assigned to either a
  <span class="guimenu ">device-group</span> or a <span class="guimenu ">volume-group</span>.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_diskmodels.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_diskmodels.png" width="" /></a></div></div><p>
  A <span class="guimenu ">device-group</span> is a set of one or more disks that are to
  be consumed directly by a service. For example, a set of disks to be used by
  Swift. The device-group identifies the list of disk devices, the service, and
  a few service-specific attributes that tell the service about the intended
  use (for example, in the case of Swift this is the ring names). When a device
  is assigned to a device-group, the associated service is responsible for the
  management of the disks. This management includes the creation and mounting
  of file systems. (Swift can provide additional data integrity when it has
  full control over the file systems and mount points.)
 </p><p>
  A <span class="guimenu ">volume-group</span> is used to present disk devices in a LVM
  volume group. It also contains details of the logical volumes to be created
  including the file system type and mount point. Logical volume sizes are
  expressed as a percentage of the total capacity of the volume group. A
  <span class="guimenu ">logical-volume</span> can also be consumed by a service in the
  same way as a <span class="guimenu ">device-group</span>. This allows services to manage
  their own devices on configurations that have limited numbers of disk drives.
 </p><p>
  Disk models also provide disk sizing information for virtual machine servers.
 </p></div><div class="sect2" id="concept-memorymodel"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Memory Model</span> <a title="Permalink" class="permalink" href="#concept-memorymodel">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-memorymodel.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-memorymodel.xml</li><li><span class="ds-label">ID: </span>concept-memorymodel</li></ul></div></div></div></div><p>
  Memory models define how the memory of a server should be configured to meet
  the needs of a particular role. It allows a number of HugePages to be defined
  at both the server and numa-node level.
 </p><p>
  Memory models also provide memory sizing information for virtual machine
  servers.
 </p><p>
  Memory models are optional - it is valid to have a server role without a
  memory model.
 </p></div><div class="sect2" id="concept-cpumodel"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CPU Model</span> <a title="Permalink" class="permalink" href="#concept-cpumodel">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-cpumodel.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-cpumodel.xml</li><li><span class="ds-label">ID: </span>concept-cpumodel</li></ul></div></div></div></div><p>
  CPU models define how CPUs of a server will be used. The model allows CPUs to
  be assigned for use by components such as Nova (for VMs) and Open vSwitch
  (for DPDK). It also allows those CPUs to be isolated from the general kernel
  SMP balancing and scheduling algorithms.
 </p><p>
  CPU models also provide CPU sizing information for virtual machine servers.
 </p><p>
  CPU models are optional - it is valid to have a server role without a cpu
  model.
 </p></div><div class="sect2" id="concept-servers"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Servers</span> <a title="Permalink" class="permalink" href="#concept-servers">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-servers.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servers.xml</li><li><span class="ds-label">ID: </span>concept-servers</li></ul></div></div></div></div><p>
  <span class="emphasis"><em><span class="guimenu ">Servers</span> have a <span class="guimenu ">server-role</span>
  which determines how they will be used in the cloud.</em></span>
 </p><p>
  <span class="guimenu ">Servers</span> (in the input model) enumerate the resources
  available for your cloud. In addition, in this definition file you can either
  provide <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with all of the details it needs to PXE boot and install an
  operating system onto the server, or, if you prefer to use your own operating
  system installation tooling you can simply provide the details needed to be
  able to SSH into the servers and start the deployment.
 </p><p>
  The address specified for the server will be the one used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> for
  lifecycle management and must be part of a network which is in the input
  model. If you are using <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to install the operating system this network
  must be an untagged VLAN. The first server must be installed manually from
  the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO and this server must be included in the input model as well.
 </p><p>
  In addition to the network details used to install or connect to the server,
  each server defines what its <span class="guimenu ">server-role</span> is and to which
  <span class="guimenu ">server-group</span> it belongs.
 </p></div><div class="sect2" id="concept-servergroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups</span> <a title="Permalink" class="permalink" href="#concept-servergroups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-servergroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servergroups.xml</li><li><span class="ds-label">ID: </span>concept-servergroups</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">server</span> is associated with a
  <span class="guimenu ">server-group</span>.</em></span>
 </p><p>
  <span class="emphasis"><em>A <span class="guimenu ">control-plane</span> can use
  <span class="guimenu ">server-groups</span> as failure zones for server
  allocation.</em></span>
 </p><p>
  <span class="emphasis"><em>A <span class="guimenu ">server-group</span> may be associated with a list of
  <span class="guimenu ">networks</span>.</em></span>
 </p><p>
  <span class="emphasis"><em>A <span class="guimenu ">server-group</span> can contain other
  <span class="guimenu ">server-groups</span>.</em></span>
 </p><p>
  The practice of locating physical servers in a number of racks or enclosures
  in a data center is common. Such racks generally provide a degree of physical
  isolation that allows for separate power and/or network connectivity.
 </p><p>
  In the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> model we support this configuration by allowing you to define
  a hierarchy of <span class="guimenu ">server-groups</span>. Each
  <span class="guimenu ">server</span> is associated with one
  <span class="guimenu ">server-group</span>, normally at the bottom of the hierarchy.
 </p><p>
  <span class="guimenu ">Server-groups</span> are an optional part of the input model - if
  you do not define any, then all <span class="guimenu ">servers</span> and
  <span class="guimenu ">networks</span> will be allocated as if they are part of the same
  <span class="guimenu ">server-group</span>.
 </p><div class="sect3" id="concept-servergroups-failurezones"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups and Failure Zones</span> <a title="Permalink" class="permalink" href="#concept-servergroups-failurezones">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-servergroups_failurezones.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servergroups_failurezones.xml</li><li><span class="ds-label">ID: </span>concept-servergroups-failurezones</li></ul></div></div></div></div><p>
  A <span class="guimenu ">control-plane</span> defines a list of
  <span class="guimenu ">server-groups</span> as the failure zones from which it wants to
  use servers. All servers in a <span class="guimenu ">server-group</span> listed as a
  failure zone in the <span class="guimenu ">control-plane</span> and any
  <span class="guimenu ">server-groups</span> they contain are considered part of that
  failure zone for allocation purposes. The following example shows how three
  levels of <span class="guimenu ">server-groups</span> can be used to model a failure
  zone consisting of multiple racks, each of which in turn contains a number of
  <span class="guimenu ">servers</span>.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_servergroups.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_servergroups.png" width="" /></a></div></div><p>
  When allocating <span class="guimenu ">servers</span>, the configuration processor will
  traverse down the hierarchy of <span class="guimenu ">server-groups</span> listed as
  failure zones until it can find an available server with the required
  <span class="guimenu ">server-role</span>. If the allocation policy is defined to be
  strict, it will allocate <span class="guimenu ">servers</span> equally across each of
  the failure zones. A <span class="guimenu ">cluster</span> or
  <span class="guimenu ">resource-group</span> can also independently specify the failure
  zones it wants to use if needed.
 </p></div><div class="sect3" id="concept-servergroups-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups and Networks</span> <a title="Permalink" class="permalink" href="#concept-servergroups-networks">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-servergroups_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servergroups_networks.xml</li><li><span class="ds-label">ID: </span>concept-servergroups-networks</li></ul></div></div></div></div><p>
  Each L3 <span class="guimenu ">network</span> in a cloud must be associated with all or
  some of the <span class="guimenu ">servers</span>, typically following a physical
  pattern (such as having separate networks for each rack or set of racks).
  This is also represented in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> model via
  <span class="guimenu ">server-groups</span>, each group lists zero or more networks to
  which <span class="guimenu ">servers</span> associated with
  <span class="guimenu ">server-groups</span> at or below this point in the hierarchy are
  connected.
 </p><p>
  When the configuration processor needs to resolve the specific
  <span class="guimenu ">network</span> a <span class="guimenu ">server</span> should be configured
  to use, it traverses up the hierarchy of <span class="guimenu ">server-groups</span>,
  starting with the group the server is directly associated with, until it
  finds a server-group that lists a network in the required network group.
 </p><p>
  The level in the <span class="guimenu ">server-group</span> hierarchy at which a
  <span class="guimenu ">network</span> is associated will depend on the span of
  connectivity it must provide. In the above example there might be networks in
  some <span class="guimenu ">network-groups</span> which are per rack (that is Rack 1 and
  Rack 2 list different networks from the same
  <span class="guimenu ">network-group</span>) and <span class="guimenu ">networks</span> in a
  different <span class="guimenu ">network-group</span> that span failure zones (the
  network used to provide floating IP addresses to virtual machines for
  example).
 </p></div></div><div class="sect2" id="concept-networking"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking</span> <a title="Permalink" class="permalink" href="#concept-networking">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-networking.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networking.xml</li><li><span class="ds-label">ID: </span>concept-networking</li></ul></div></div></div></div><p>
  In addition to the mapping of <span class="guimenu ">services</span> to specific
  <span class="guimenu ">clusters</span> and <span class="guimenu ">resources</span> we must also be
  able to define how the <span class="guimenu ">services</span> connect to one or more
  <span class="guimenu ">networks</span>.
 </p><p>
  In a simple cloud there may be a single L3 network but more typically there
  are functional and physical layers of network separation that need to be
  expressed.
 </p><p>
  Functional network separation provides different networks for different types
  of traffic; for example, it is common practice in even small clouds to
  separate the External APIs that users will use to access the cloud and the
  external IP addresses that users will use to access their virtual machines.
  In more complex clouds it is common to also separate out virtual networking
  between virtual machines, block storage traffic, and volume traffic onto
  their own sets of networks. In the input model, this level of separation is
  represented by <span class="guimenu ">network-groups</span>.
 </p><p>
  Physical separation is required when there are separate L3 network segments
  providing the same type of traffic; for example, where each rack uses a
  different subnet. This level of separation is represented in the input model
  by the <span class="guimenu ">networks</span> within each
  <span class="guimenu ">network-group</span>.
 </p><div class="sect3" id="concept-networkgroups"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Groups</span> <a title="Permalink" class="permalink" href="#concept-networkgroups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-networkgroups.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networkgroups.xml</li><li><span class="ds-label">ID: </span>concept-networkgroups</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>Service endpoints attach to <span class="guimenu ">networks</span> in a
  specific <span class="guimenu ">network-group</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Network-groups</span> can define routes to other
  <span class="guimenu ">networks</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Network-groups</span> encapsulate the configuration for
  <span class="guimenu ">services</span> via <span class="guimenu ">network-tags</span></em></span>
 </p><p>
  A <span class="guimenu ">network-group</span> defines the traffic separation model and
  all of the properties that are common to the set of L3 networks that carry
  each type of traffic. They define where services are attached to the network
  model and the routing within that model.
 </p><p>
  In terms of <span class="guimenu ">service</span> connectivity, all that has to be
  captured in the <span class="guimenu ">network-groups</span> definition are the same
  service-component names that are used when defining
  <span class="guimenu ">control-planes</span>. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> also allows a default attachment
  to be used to specify "all service-components" that are not explicitly
  connected to another <span class="guimenu ">network-group</span>. So, for example, to
  isolate Swift traffic, the swift-account, swift-container, and swift-object
  service components are attached to an "Object"
  <span class="guimenu ">network-group</span> and all other services are connected to
  "MANAGEMENT" <span class="guimenu ">network-group</span> via the default relationship.
 </p><div id="id-1.3.4.3.3.19.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The name of the "MANAGEMENT" <span class="guimenu ">network-group</span> cannot be
   changed. It must be upper case. Every SUSE <span class="productname">OpenStack</span> Cloud requires this network group in
   order to be valid.
  </p></div><p>
  The details of how each service connects, such as what port it uses, if it
  should be behind a load balancer, if and how it should be registered in
  Keystone, and so forth, are defined in the service definition files provided
  by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  In any configuration with multiple networks, controlling the routing is a
  major consideration. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, routing is controlled at the
  <span class="guimenu ">network-group</span> level. First, all
  <span class="guimenu ">networks</span> are configured to provide the route to any other
  <span class="guimenu ">networks</span> in the same <span class="guimenu ">network-group</span>. In
  addition, a <span class="guimenu ">network-group</span> may be configured to provide the
  route any other <span class="guimenu ">networks</span> in the same
  <span class="guimenu ">network-group</span>; for example, if the internal APIs are in a
  dedicated <span class="guimenu ">network-group</span> (a common configuration in a
  complex network because a network group with load balancers cannot be
  segmented) then other <span class="guimenu ">network-groups</span> may need to include a
  route to the internal API <span class="guimenu ">network-group</span> so that services
  can access the internal API endpoints. Routes may also be required to define
  how to access an external storage network or to define a general default
  route.
 </p><p>
  As part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment, networks are configured to act as the
  default route for all traffic that was received via that network (so that
  response packets always return via the network the request came from).
 </p><p>
  Note that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will configure the routing rules on the servers it deploys
  and will validate that the routes between services exist in the model, but
  ensuring that gateways can provide the required routes is the responsibility
  of your network configuration. The configuration processor provides
  information about the routes it is expecting to be configured.
 </p><p>
  For a detailed description of how the configuration processor validates
  routes, refer to <a class="xref" href="#networkroutevalidation" title="7.6. Network Route Validation">Section 7.6, “Network Route Validation”</a>.
 </p><div class="sect4" id="concept-loadbalancers"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.2.10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancers</span> <a title="Permalink" class="permalink" href="#concept-loadbalancers">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-loadbalancers.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-loadbalancers.xml</li><li><span class="ds-label">ID: </span>concept-loadbalancers</li></ul></div></div></div></div><p>
  <span class="guimenu ">Load-balancers</span> provide a specific type of routing and are
  defined

  as a relationship between the virtual IP address (VIP) on a network in one
  <span class="guimenu ">network group</span> and a set of service endpoints (which may be
  on <span class="guimenu ">networks</span> in the same or a different
  <span class="guimenu ">network-group</span>).
 </p><p>
  As each <span class="guimenu ">load-balancer</span> is defined providing a virtual IP on
  a <span class="guimenu ">network-group</span>, it follows that those
  <span class="guimenu ">network-group</span>s
  
  can each only have one <span class="guimenu ">network</span> associated to them.
 </p><p>
  The <span class="guimenu ">load-balancer</span> definition includes a list of
  <span class="guimenu ">service-components</span> and endpoint roles it will provide a
  virtual IP for. This model allows service-specific
  <span class="guimenu ">load-balancers</span> to be defined on different
  <span class="guimenu ">network-groups</span>. A "default" value is used to express "all
  service-components" which require a virtual IP address and are not explicitly
  configured in another <span class="guimenu ">load-balancer</span> configuration. The
  details of how the <span class="guimenu ">load-balancer</span> should be configured for
  each service, such as which ports to use, how to check for service liveness,
  etc., are provided in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supplied service definition files.
 </p><p>
  Where there are multiple instances of a service (for example, in a cloud with multiple
  control-planes), each control-plane needs its own set of virtual IP address
  and different values for some properties such as the external name and
  security certificate. To accommodate this in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, load-balancers
  are defined as part of the control-plane, with the network groups defining
  just which load-balancers are attached to them.
 </p><p>
  Load balancers are always implemented by an ha-proxy service in the same
  control-plane as the services.
 </p></div><div class="sect4" id="concept-endpoints"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.2.10.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Separation of Public, Admin, and Internal Endpoints</span> <a title="Permalink" class="permalink" href="#concept-endpoints">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-endpoints.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-endpoints.xml</li><li><span class="ds-label">ID: </span>concept-endpoints</li></ul></div></div></div></div><p>
  The list of endpoint roles for a <span class="guimenu ">load-balancer</span> make it
  possible to configure separate <span class="guimenu ">load-balancers</span> for public
  and internal access to services, and the configuration processor uses this
  information to both ensure the correct registrations in Keystone and to make
  sure the internal traffic is routed to the correct endpoint. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  services are configured to only connect to other services via internal
  virtual IP addresses and endpoints, allowing the name and security
  certificate of public endpoints to be controlled by the customer and set to
  values that may not be resolvable/accessible from the servers making up the
  cloud.
 </p><p>
  Note that each <span class="guimenu ">load-balancer</span> defined in the input model
  will be allocated a separate virtual IP address even when the load-balancers
  are part of the same <span class="guimenu ">network-group</span>. Because of the need to
  be able to separate both public and internal access, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will not allow
  a single <span class="guimenu ">load-balancer</span> to provide both public and internal
  access. <span class="guimenu ">Load-balancers</span> in this context are logical
  entities (sets of rules to transfer traffic from a virtual IP address to one
  or more endpoints).
  
 </p><p>
  The following diagram shows a possible configuration in which the hostname
  associated with the public URL has been configured to resolve to a firewall
  controlling external access to the cloud. Within the cloud, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services
  are configured to use the internal URL to access a separate virtual IP
  address.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_loadbalancers.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_loadbalancers.png" width="" /></a></div></div></div><div class="sect4" id="concept-networktags"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.2.10.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Tags</span> <a title="Permalink" class="permalink" href="#concept-networktags">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-networktags.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networktags.xml</li><li><span class="ds-label">ID: </span>concept-networktags</li></ul></div></div></div></div><p>
  Network tags are defined by some <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  <span class="guimenu ">service-components</span> and are used to convey information
  between the network model and the service, allowing the dependent aspects of
  the service to be automatically configured.
  
 </p><p>
  Network tags also convey requirements a service may have for aspects of the
  server network configuration, for example, that a bridge is required on the
  corresponding network device on a server where that service-component is
  installed.
 </p><p>
  See <a class="xref" href="#configobj-networktags" title="6.13.2. Network Tags">Section 6.13.2, “Network Tags”</a> for more information on specific
  tags and their usage.
 </p></div></div><div class="sect3" id="concept-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networks</span> <a title="Permalink" class="permalink" href="#concept-networks">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networks.xml</li><li><span class="ds-label">ID: </span>concept-networks</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">network</span> is part of a
  <span class="guimenu ">network-group</span>.</em></span>
 </p><p>
  <span class="guimenu ">Networks</span> are fairly simple definitions. Each
  <span class="guimenu ">network</span> defines the details of its VLAN, optional address
  details (CIDR, start and end address, gateway address), and which
  <span class="guimenu ">network-group</span> it is a member of.
 </p></div><div class="sect3" id="concept-interfacemodel"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interface Model</span> <a title="Permalink" class="permalink" href="#concept-interfacemodel">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-interfacemodel.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-interfacemodel.xml</li><li><span class="ds-label">ID: </span>concept-interfacemodel</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">server-role</span> identifies an
  <span class="guimenu ">interface-model</span> that describes how its network interfaces
  are to be configured and used.</em></span>
 </p><p>
  Network groups are mapped onto specific network interfaces via an
  <span class="guimenu ">interface-model</span>, which describes the network devices that
  need to be created (bonds, ovs-bridges, etc.) and their properties.
 </p><p>
  An <span class="guimenu ">interface-model</span> acts like a template; it can define how
  some or all of the <span class="guimenu ">network-groups</span> are to be mapped for a
  particular combination of physical NICs. However, it is the
  <span class="guimenu ">service-components</span> on each server that determine which
  <span class="guimenu ">network-groups</span> are required and hence which interfaces and
  <span class="guimenu ">networks</span> will be configured. This means that
  <span class="guimenu ">interface-models</span> can be shared between different
  <span class="guimenu ">server-roles</span>. For example, an API role and a database role
  may share an interface model even though they may have different disk models
  and they will require a different subset of the
  <span class="guimenu ">network-groups</span>.
 </p><p>
  Within an <span class="guimenu ">interface-model</span>, physical ports are identified
  by a device name, which in turn is resolved to a physical port on a server
  basis via a <span class="guimenu ">nic-mapping</span>. To allow different physical
  servers to share an <span class="guimenu ">interface-model</span>, the
  <span class="guimenu ">nic-mapping</span> is defined as a property of each
  <span class="guimenu ">server</span>.
 </p><p>
  The <code class="literal">interface-model</code> can also used to describe how network
  devices are to be configured for use with DPDK, SR-IOV, and PCI Passthrough.
 </p></div><div class="sect3" id="concept-nicmapping"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NIC Mapping</span> <a title="Permalink" class="permalink" href="#concept-nicmapping">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-nicmapping.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-nicmapping.xml</li><li><span class="ds-label">ID: </span>concept-nicmapping</li></ul></div></div></div></div><p>
  When a <span class="guimenu ">server</span> has more than a single physical network
  port, a <span class="guimenu ">nic-mapping</span> is required to unambiguously identify
  each port. Standard Linux mapping of ports to interface names at the time of
  initial discovery (for example, <code class="literal">eth0</code>,
  <code class="literal">eth1</code>, <code class="literal">eth2</code>, ...) is not uniformly
  consistent
  from server to server, so a mapping of PCI bus address to interface name is
  instead.
 </p><p>
  NIC mappings are also used to specify the device type for interfaces that are
  to be used for SR-IOV or PCI Passthrough. Each <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> release includes the
  data for the supported device types.
 </p></div><div class="sect3" id="concept-firewallconfiguration"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall Configuration</span> <a title="Permalink" class="permalink" href="#concept-firewallconfiguration">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-firewallconfiguration.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-firewallconfiguration.xml</li><li><span class="ds-label">ID: </span>concept-firewallconfiguration</li></ul></div></div></div></div><p>
  The configuration processor uses the details it has about which networks and
  ports <span class="guimenu ">service-components</span> use to create a set of firewall
  rules for each server. The model allows additional user-defined rules on a
  per <span class="guimenu ">network-group</span> basis.
 </p></div></div><div class="sect2" id="concept-configuration-data"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Data</span> <a title="Permalink" class="permalink" href="#concept-configuration-data">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-concepts-configuration_data.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-configuration_data.xml</li><li><span class="ds-label">ID: </span>concept-configuration-data</li></ul></div></div></div></div><p>
  Configuration Data is used to provide settings which have to be applied in a
  specific context, or where the data needs to be verified against or merged
  with other values in the input model.
 </p><p>
  For example, when defining a Neutron provider network to be used by Octavia,
  the network needs to be included in the routing configuration generated by
  the Configuration Processor.
 </p></div></div></div><div class="chapter " id="configurationobjects"><div class="titlepage"><div><div><h2 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Objects</span> <a title="Permalink" class="permalink" href="#configurationobjects">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-configurationobjects.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationobjects.xml</li><li><span class="ds-label">ID: </span>configurationobjects</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#configobj-cloud"><span class="number">6.1 </span><span class="name">Cloud Configuration</span></a></span></dt><dt><span class="section"><a href="#configobj-controlplane"><span class="number">6.2 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#configobj-load-balancers"><span class="number">6.3 </span><span class="name">Load Balancers</span></a></span></dt><dt><span class="section"><a href="#configobj-regions"><span class="number">6.4 </span><span class="name">Regions</span></a></span></dt><dt><span class="section"><a href="#configobj-servers"><span class="number">6.5 </span><span class="name">Servers</span></a></span></dt><dt><span class="section"><a href="#configobj-servergroups"><span class="number">6.6 </span><span class="name">Server Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-serverroles"><span class="number">6.7 </span><span class="name">Server Roles</span></a></span></dt><dt><span class="section"><a href="#configobj-diskmodels"><span class="number">6.8 </span><span class="name">
  Disk Models</span></a></span></dt><dt><span class="section"><a href="#configobj-memorymodels"><span class="number">6.9 </span><span class="name">Memory Models</span></a></span></dt><dt><span class="section"><a href="#configobj-cpumodels"><span class="number">6.10 </span><span class="name">
  CPU Models</span></a></span></dt><dt><span class="section"><a href="#configobj-interfacemodels"><span class="number">6.11 </span><span class="name">Interface Models</span></a></span></dt><dt><span class="section"><a href="#configobj-nicmappings"><span class="number">6.12 </span><span class="name">NIC Mappings</span></a></span></dt><dt><span class="section"><a href="#configobj-networkgroups"><span class="number">6.13 </span><span class="name">Network Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-networks"><span class="number">6.14 </span><span class="name">Networks</span></a></span></dt><dt><span class="section"><a href="#configobj-firewallrules"><span class="number">6.15 </span><span class="name">Firewall Rules</span></a></span></dt><dt><span class="section"><a href="#configobj-configurationdata"><span class="number">6.16 </span><span class="name">Configuration Data</span></a></span></dt><dt><span class="section"><a href="#passthrough"><span class="number">6.17 </span><span class="name">Pass Through</span></a></span></dt></dl></div></div><div class="sect1" id="configobj-cloud"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Configuration</span> <a title="Permalink" class="permalink" href="#configobj-cloud">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-cloud.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cloud.xml</li><li><span class="ds-label">ID: </span>configobj-cloud</li></ul></div></div></div></div><p>
  The top-level cloud configuration file, <code class="filename">cloudConfig.yml</code>,
  defines some global values for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, as described in the table below.
 </p><p>
  The snippet below shows the start of the control plane definition file.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  cloud:
    name: entry-scale-kvm

    hostname-data:
        host-prefix: ardana
        member-prefix: -m

    ntp-servers:
        - "ntp-server1"

    # dns resolving configuration for your site
    dns-settings:
      nameservers:
        - name-server1

    firewall-settings:
        enable: true
        # log dropped packets
        logging: true

    audit-settings:
       audit-dir: /var/audit
       default: disabled
       enabled-services:
         - keystone</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the cloud</td></tr><tr><td>hostname-data (optional)</td><td>
      <p>
       Provides control over some parts of the generated names (see )
      </p>
      <p>
       Consists of two values:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         host-prefix - default is to use the cloud name (above)
        </p></li><li class="listitem "><p>
         member-prefix - default is "-m"
        </p></li></ul></div>
     </td></tr><tr><td>ntp-servers (optional)</td><td>
      <p>
       A list of external NTP servers your cloud has access to. If specified
       by name then the names need to be resolvable via the external DNS
       nameservers you specify in the next section. All servers running the
       "ntp-server" component will be configured to use these external NTP
       servers.
      </p>
     </td></tr><tr><td>dns-settings (optional)</td><td>
      <p>
       DNS configuration data that will be applied to all servers. See
       example configuration for a full list of values.
      </p>
     </td></tr><tr><td>smtp-settings (optional)</td><td>
      <p>
       SMTP client configuration data that will be applied to all servers.
       See example configurations for a full list of values.
      </p>
     </td></tr><tr><td>firewall-settings (optional)</td><td>
      <p>
       Used to enable/disable the firewall feature and to enable/disable
       logging of dropped packets.
      </p>
      <p>
       The default is to have the firewall enabled.
      </p>
     </td></tr><tr><td>audit-settings (optional)</td><td>
      <p>
       Used to enable/disable the production of audit data from services.
      </p>
      <p>
       The default is to have audit disabled for all services.
      </p>
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-controlplane"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane</span> <a title="Permalink" class="permalink" href="#configobj-controlplane">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-controlplane.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-controlplane.xml</li><li><span class="ds-label">ID: </span>configobj-controlplane</li></ul></div></div></div></div><p>
  The snippet below shows the start of the control plane definition file.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  control-planes:
     - name: control-plane-1
       control-plane-prefix: cp1
       region-name: region0
       failure-zones:
         - AZ1
         - AZ2
         - AZ3
       configuration-data:
         - NEUTRON-CONFIG-CP1
         - OCTAVIA-CONFIG-CP1
       common-service-components:
         - logging-producer
         - monasca-agent
         - freezer-agent
         - stunnel
         - lifecycle-manager-target
       clusters:
         - name: cluster1
           cluster-prefix: c1
           server-role: CONTROLLER-ROLE
           member-count: 3
           allocation-policy: strict
           service-components:
             - lifecycle-manager
             - ntp-server
             - swift-ring-builder
             - mysql
             - ip-cluster
             ...

       resources:
         - name: compute
           resource-prefix: comp
           server-role: COMPUTE-ROLE
           allocation-policy: any
           min-count: 0
           service-components:
              - ntp-client
              - nova-compute
              - nova-compute-kvm
              - neutron-l3-agent
              ...</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       This name identifies the control plane. This value is used to persist
       server allocations <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a> and cannot be
       changed once servers have been allocated.
      </p>
     </td></tr><tr><td>control-plane-prefix (optional)</td><td>
      <p>
       The control-plane-prefix is used as part of the hostname (see
       <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>). If not specified, the control plane
       name is used.
      </p>
     </td></tr><tr><td>region-name</td><td>
      <p>
       This name identifies the Keystone region within which services in the
       control plane will be registered. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, multiple regions are
       not supported. Only <code class="literal">Region0</code> is valid.
      </p>
      <p>
       For clouds consisting of multiple control planes, this attribute should
       be omitted and the regions object should be used to set the region
       name (<code class="literal">Region0</code>).
      </p>
     </td></tr><tr><td>uses (optional)</td><td>
      <p>
       Identifies the services this control will consume from other control
       planes (see <a class="xref" href="#configobj-multiple-control-planes" title="6.2.3. Multiple Control Planes">Section 6.2.3, “Multiple Control Planes”</a>).
      </p>
     </td></tr><tr><td>load-balancers (optional)</td><td>
      <p>
       A list of load balancer definitions for this control plane (see
       <a class="xref" href="#configobj-load-balancer-definitions" title="6.2.4. Load Balancer Definitions in Control Planes">Section 6.2.4, “Load Balancer Definitions in Control Planes”</a>).
      </p>
      <p>
       For a multi control-plane cloud load balancers must be defined in each
       control-plane. For a single control-plane cloud they may be defined
       either in the control plane or as part of a network group.
      </p>
     </td></tr><tr><td>common-service-components (optional)</td><td>
      <p>
       This lists a set of service components that run on all servers in the
       control plane (clusters and resource pools).
      </p>
     </td></tr><tr><td>failure-zones (optional)</td><td>
      <p>
       A list of <span class="guimenu ">server-group</span> names that servers for this
       control plane will be allocated from. If no failure-zones are
       specified, only servers not associated with a
       <span class="guimenu ">server-group</span> will be used. (See
       <a class="xref" href="#concept-servergroups-failurezones" title="5.2.9.1. Server Groups and Failure Zones">Section 5.2.9.1, “Server Groups and Failure Zones”</a> for a description
       of server-groups as failure zones.)
      </p>
     </td></tr><tr><td>configuration-data (optional)</td><td>
      <p>
       A list of configuration data settings to be used for services in this
       control plane (see <a class="xref" href="#concept-configuration-data" title="5.2.11. Configuration Data">Section 5.2.11, “Configuration Data”</a>).
      </p>
     </td></tr><tr><td>clusters</td><td>
      <p>
       A list of clusters for this control plane (see
       <a class="xref" href="#configobj-clusters" title="6.2.1.  Clusters">Section 6.2.1, “
  Clusters”</a>).
      </p>
     </td></tr><tr><td>resources</td><td>
      <p>
       A list of resource groups for this control plane (see
       <a class="xref" href="#configobj-resources" title="6.2.2. Resources">Section 6.2.2, “Resources”</a>).
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="configobj-clusters"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  Clusters</span> <a title="Permalink" class="permalink" href="#configobj-clusters">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-clusters.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-clusters.xml</li><li><span class="ds-label">ID: </span>configobj-clusters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       Cluster and resource names must be unique within a control plane. This
       value is used to persist server allocations (see
       <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a>) and cannot be changed once servers
       have been allocated.
      </p>
     </td></tr><tr><td>cluster-prefix (optional)</td><td>
      <p>
       The cluster prefix is used in the hostname (see
       <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>). If not supplied then the cluster
       name is used.
      </p>
     </td></tr><tr><td>server-role</td><td>
      <p>
       This can either be a string (for a single role) or a list of roles.
       Only servers matching one of the specified
       <span class="guimenu ">server-roles</span> will be allocated to this cluster.
       (see <a class="xref" href="#concept-serverroles" title="5.2.4. Server Roles">Section 5.2.4, “Server Roles”</a> for a description of server
       roles)
      </p>
     </td></tr><tr><td>service-components</td><td>
      <p>
       The list of <span class="guimenu ">service-components</span> to be deployed on
       the servers allocated for the cluster. (The common-service-components
       for the control plane are also deployed.)
      </p>
     </td></tr><tr><td>
      <p>
       member-count
      </p>
      <p>
       min-count
      </p>
      <p>
       max-count
      </p>
      <p>
       (all optional)
      </p>
     </td><td>
      <p>
       Defines the number of servers to add to the cluster.
      </p>
      <p>
       The number of servers that can be supported in a cluster depends on the
       services it is running. For example MariaDB and RabbitMQ can only be
       deployed on clusters on 1 (non-HA) or 3 (HA) servers. Other services may
       support different sizes of cluster.
      </p>
      <p>
       If min-count is specified, then at least that number of servers will be
       allocated to the cluster. If min-count is not specified it defaults to a
       value of 1.
      </p>
      <p>
       If max-count is specified, then the cluster will be limited to that
       number of servers. If max-count is not specified then all servers
       matching the required role and failure-zones will be allocated to the
       cluster.
      </p>
      <p>
       Specifying member-count is equivalent to specifying min-count and
       max-count with the same value.
      </p>
     </td></tr><tr><td>failure-zones (optional)</td><td>
      <p>
       A list of <span class="guimenu ">server-groups</span> that servers will be
       allocated from. If specified, it overrides the list of values
       specified for the control-plane. If not specified, the control-plane
       value is used. (see
       <a class="xref" href="#concept-servergroups-failurezones" title="5.2.9.1. Server Groups and Failure Zones">Section 5.2.9.1, “Server Groups and Failure Zones”</a> for a description
       of server groups as failure zones).
      </p>
     </td></tr><tr><td>allocation-policy (optional)</td><td>
      <p>
       Defines how failure zones will be used when allocating servers.
      </p>
      <p>
       <span class="bold"><strong>strict</strong></span>: Server allocations will be
       distributed across all specified failure zones. (if max-count is not a
       whole number, an exact multiple of the number of zones, then some zones
       may provide one more server than other zones)
      </p>
      <p>
       <span class="bold"><strong>any</strong></span>: Server allocations will be made
       from any combination of failure zones.
      </p>
      <p>
       The default allocation-policy for a cluster is
       <span class="emphasis"><em>strict</em></span>.
      </p>
     </td></tr><tr><td>configuration-data (optional)</td><td>
      <p>
       A list of configuration-data settings that will be applied to the
       services in this cluster. The values for each service will be combined
       with any values defined as part of the configuration-data list for the
       control-plane. If a value is specified by settings in both lists, the
       value defined here takes precedence.
      </p>
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-resources"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Resources</span> <a title="Permalink" class="permalink" href="#configobj-resources">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-resources.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-resources.xml</li><li><span class="ds-label">ID: </span>configobj-resources</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       The name of this group of resources. Cluster names and resource-node
       names must be unique within a control plane. Additionally, clusters and
       resources cannot share names within a control-plane.
      </p>
      <p>
       This value is used to persist server allocations (see
       <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a>) and cannot be changed once servers
       have been allocated.
      </p>
     </td></tr><tr><td>resource-prefix</td><td>
      The resource-prefix is used in the name generation. (see
      <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>)
     </td></tr><tr><td>server-role</td><td>
      This can either be a string (for a single role) or a list of roles.
      Only servers matching one of the specified
      <span class="guimenu ">server-roles</span> will be allocated to this resource
      group. (see <a class="xref" href="#concept-serverroles" title="5.2.4. Server Roles">Section 5.2.4, “Server Roles”</a> for a description of
      server roles).
     </td></tr><tr><td>service-components</td><td>
      The list of <span class="guimenu ">service-components</span> to be deployed on the
      servers in this resource group. (The common-service-components for the
      control plane are also deployed.)
     </td></tr><tr><td>
      <p>
       member-count
      </p>
      <p>
       min-count
      </p>
      <p>
       max-count
      </p>
      <p>
       (all optional)
      </p>
     </td><td>
      <p>
       Defines the number of servers to add to the cluster.
      </p>
      <p>
       The number of servers that can be supported in a cluster depends on the
       services it is running. For example MariaDB and RabbitMQ can only be
       deployed on clusters on 1 (non-HA) or 3 (HA) servers. Other services may
       support different sizes of cluster.
      </p>
      <p>
       If min-count is specified, then at least that number of servers will be
       allocated to the cluster. If min-count is not specified it defaults to a
       value of 1.
      </p>
      <p>
       If max-count is specified, then the cluster will be limited to that
       number of servers. If max-count is not specified then all servers
       matching the required role and failure-zones will be allocated to the
       cluster.
      </p>
      <p>
       Specifying member-count is equivalent to specifying min-count and
       max-count with the same value.
      </p>
     </td></tr><tr><td>failure-zones (optional)</td><td>
      A list of <span class="guimenu ">server-groups</span> that servers will be
      allocated from. If specified, it overrides the list of values specified
      for the control-plane. If not specified, the control-plane value is
      used. (see <a class="xref" href="#concept-servergroups-failurezones" title="5.2.9.1. Server Groups and Failure Zones">Section 5.2.9.1, “Server Groups and Failure Zones”</a> for a
      description of server groups as failure zones).
     </td></tr><tr><td>allocation-policy (optional)</td><td>
      <p>
       Defines how failure zones will be used when allocating servers.
      </p>
      <p>
       <span class="bold"><strong>strict</strong></span>: Server allocations will be
       distributed across all specified failure zones. (if max-count is not a
       whole number, an exact multiple of the number of zones, then some zones
       may provide one more server than other zones)
      </p>
      <p>
       <span class="bold"><strong>any</strong></span>: Server allocations will be made
       from any combination of failure zones.
      </p>
      <p>
       The default allocation-policy for resources is <span class="emphasis"><em>any</em></span>.
      </p>
     </td></tr><tr><td>configuration-data (optional)</td><td>
      A list of configuration-data settings that will be applied to the
      services in this cluster. The values for each service will be combined
      with any values defined as part of the configuration-data list for the
      control-plane. If a value is specified by settings in both lists, the
      value defined here takes precedence.
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-multiple-control-planes"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Control Planes</span> <a title="Permalink" class="permalink" href="#configobj-multiple-control-planes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-multiple_control_planes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-multiple_control_planes.xml</li><li><span class="ds-label">ID: </span>configobj-multiple-control-planes</li></ul></div></div></div></div><p>
  The dependencies between service components (for example, Nova needs
  MariaDB and Keystone API) is defined as part of the service definitions
  provide by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the control-planes define how those dependencies will
  be met. For clouds consisting of multiple control-planes, the relationship
  between services in different control planes is defined by a
  <code class="literal">uses</code> attribute in its control-plane object. Services
  will always use other services in the same control-plane before looking to
  see if the required service can be provided from another control-plane.
  For example, a service component in control-plane
  <code class="literal">cp-2</code> (for example, nova-api) might use service
  components from control-plane <code class="literal">cp-shared</code> (for example,
  keystone-api).
 </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: cp-2
      uses:
        - from: cp-shared
          service-components:
            - any</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>from</td><td>
      The name of the control-plane providing services which may be consumed
      by this control-plane.
     </td></tr><tr><td>service-components</td><td>
      A list of service components from the specified control-plane which may
      be consumed by services in this control-plane. The reserved keyword
      <code class="literal">any</code> indicates that any service component from the
      specified control-plane may be consumed by services in this
      control-plane.
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-load-balancer-definitions"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer Definitions in Control Planes</span> <a title="Permalink" class="permalink" href="#configobj-load-balancer-definitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-load_balancer_definitions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-load_balancer_definitions.xml</li><li><span class="ds-label">ID: </span>configobj-load-balancer-definitions</li></ul></div></div></div></div><p>
  Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, a load-balancer may be defined within a
  control-plane object, and referenced by name from a network-groups object.
  The following example shows load balancer <code class="literal">extlb</code> defined in
  control-plane <code class="literal">cp1</code> and referenced from the EXTERNAL-API
  network group. See section Load balancers for a complete description of load
  balance attributes.
 </p><div class="verbatim-wrap"><pre class="screen">network-groups:
    - name: EXTERNAL-API
      load-balancers:
        - extlb

  control-planes:
    - name: cp1
      load-balancers:
        - provider: ip-cluster
          name: extlb
          external-name:
          tls-components:
            - default
          roles:
            - public
          cert-file: cp1-extlb-cert</pre></div></div></div><div class="sect1" id="configobj-load-balancers"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancers</span> <a title="Permalink" class="permalink" href="#configobj-load-balancers">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-load_balancers.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-load_balancers.xml</li><li><span class="ds-label">ID: </span>configobj-load-balancers</li></ul></div></div></div></div><p>
  Load balancers may be defined as part of a network-group object, or as part
  of a control-plane object. When a load-balancer is defined in a
  control-plane, it must be referenced by name only from the associated
  network-group object.
 </p><p>
  For clouds consisting of multiple control planes, load balancers must be
  defined as part of a control-plane object. This allows different load
  balancer configurations for each control plane.
 </p><p>
  In either case, a load-balancer definition has the following attributes:
 </p><div class="verbatim-wrap"><pre class="screen">load-balancers:
        - provider: ip-cluster
          name: extlb
          external-name:

          tls-components:
            - default
          roles:
            - public
          cert-file: cp1-extlb-cert</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator defined name for the load balancer. This name is used
      to make the association from a network-group.
     </td></tr><tr><td>provider</td><td>
      The service component that implements the load balancer. Currently only
      <code class="literal">ip-cluster</code> (ha-proxy) is supported. Future releases
      will provide support for external load balancers.
     </td></tr><tr><td>roles</td><td>
      The list of endpoint roles that this load balancer provides (see
      below). 
      Valid roles are
      <code class="literal">public</code>, <code class="literal">internal</code>, and
      <code class="literal">admin</code>. To ensure separation of concerns, the role
      <code class="literal">public</code> cannot be combined with any other role. See
      Load Balancers for an example of how the role provides endpoint
      separation.
     </td></tr><tr><td>components (optional)</td><td>
      The list of service-components for which the load balancer provides a
      non-encrypted virtual IP address for.
     </td></tr><tr><td>tls-components (optional)</td><td>
      The list of service-components for which the load balancer provides
      TLS-terminated virtual IP addresses for.
     </td></tr><tr><td>external-name (optional)</td><td>
      The name to be registered in Keystone for the publicURL. If not
      specified, the virtual IP address will be registered. Note that this
      value cannot be changed after the initial deployment.
     </td></tr><tr><td>cert-file (optional)</td><td>
      The name of the certificate file to be used for tls endpoints. If not
      specified, a file name will be constructed using the format
      <code class="literal"><em class="replaceable ">CP-NAME</em>-<em class="replaceable ">LB-NAME</em>-cert</code>,
      where <code class="literal"><em class="replaceable ">CP-NAME</em></code> is the
      control-plane name and
      <code class="literal"><em class="replaceable ">LB-NAME</em></code> is the
      load-balancer name.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-regions"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Regions</span> <a title="Permalink" class="permalink" href="#configobj-regions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-regions.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-regions.xml</li><li><span class="ds-label">ID: </span>configobj-regions</li></ul></div></div></div></div><p>
  The regions configuration object is used to define how a set of services from
  one or more control-planes are mapped into Openstack regions (entries within
  the Keystone catalog). In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, multiple regions are not
  supported. Only <code class="literal">Region0</code> is valid.
 </p><p>
  Within each region a given service is provided by one control plane, but the
  set of services in the region may be provided by multiple control planes.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>The name of the region in the Keystone service catalog. </td></tr><tr><td>includes</td><td>
      A list of services to include in this region, broken down by the
      control planes providing the services.
     </td></tr></tbody></table></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>control-plane</td><td>A control-plane name.</td></tr><tr><td>services</td><td>
      A list of service names. This list specifies the services from this
      control-plane to be included in this region. The reserved keyword
      <code class="literal">all</code> may be used when all services from the
      control-plane are to be included.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-servers"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Servers</span> <a title="Permalink" class="permalink" href="#configobj-servers">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-servers.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-servers.xml</li><li><span class="ds-label">ID: </span>configobj-servers</li></ul></div></div></div></div><p>
  The <span class="guimenu ">servers</span> configuration object is used to list the
  available servers for deploying the cloud.
 </p><p>
  Optionally, it can be used as an input file to the operating system
  installation process, in which case some additional fields (identified below)
  will be necessary.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  baremetal:
    subnet: 192.168.10.0
    netmask: 255.255.255.0

  servers:
    - id: controller1
      ip-addr: 192.168.10.3
      role: CONTROLLER-ROLE
      server-group: RACK1
      nic-mapping: HP-DL360-4PORT
      mac-addr: b2:72:8d:ac:7c:6f
      ilo-ip: 192.168.9.3
      ilo-password: password
      ilo-user: admin

    - id: controller2
      ip-addr: 192.168.10.4
      role: CONTROLLER-ROLE
      server-group: RACK2
      nic-mapping: HP-DL360-4PORT
      mac-addr: 8a:8e:64:55:43:76
      ilo-ip: 192.168.9.4
      ilo-password: password
      ilo-user: admin</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>id</td><td>
      An administrator-defined identifier for the server. IDs must be unique
      and are used to track server allocations. (see
      <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a>).
     </td></tr><tr><td>ip-addr</td><td>
      <p>
       The IP address is used by the configuration processor to install and
       configure the service components on this server.
      </p>
      <p>
       This IP address must be within the range of a <span class="guimenu ">network</span>
       defined in this model.
      </p>
      <p>
       When the servers file is being used for operating system installation,
       this IP address will be assigned to the node by the installation
       process, and the associated <span class="guimenu ">network</span> must be an
       untagged VLAN.
      </p>
     </td></tr><tr><td>hostname (optional)</td><td>
      The value to use for the hostname of the server. If specified this will
      be used to set the hostname value of the server which will in turn be
      reflected in systems such as Nova, Monasca, etc. If not specified the
      hostname will be derived based on where the server is used and the
      network defined to provide hostnames.
     </td></tr><tr><td>role</td><td>
      Identifies the <span class="guimenu ">server-role</span> of the server.
      
     </td></tr><tr><td>nic-mapping</td><td>
      Name of the <span class="guimenu ">nic-mappings</span> entry to apply to this
      server. (See <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>.)
     </td></tr><tr><td>server-group (optional)</td><td>
      Identifies the <span class="guimenu ">server-groups</span> entry that this server
      belongs to. (see <a class="xref" href="#concept-servergroups" title="5.2.9. Server Groups">Section 5.2.9, “Server Groups”</a>)
     </td></tr><tr><td>boot-from-san (optional)</td><td>
      Must be set to true is the server needs to be configured to boot from
      SAN storage. Default is False
     </td></tr><tr><td>fcoe-interfaces (optional)</td><td>
      A list of network devices that will be used for accessing FCoE storage.
      This is only needed for devices that present as native FCoE, not
      devices such as Emulex which present as a FC device.
     </td></tr><tr><td>ansible-options (optional)</td><td>
      A string of additional variables to be set when defining the server as
      a host in Ansible. For example, <code class="literal">ansible_ssh_port=5986</code>
     </td></tr><tr><td>mac-addr (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This identifies the MAC address on the server that will
      be used to network install the operating system.
     </td></tr><tr><td>kopt-extras (optional)</td><td>
      Provides additional command line arguments to be passed to the booting
      network kernel. For example, <code class="literal">vga=769</code> sets the video
      mode for the install to low resolution which can be useful for remote
      console users.
     </td></tr><tr><td>ilo-ip (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This provides the IP address of the power management
      (for example, IPMI, iLO) subsystem.
     </td></tr><tr><td>ilo-user (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This provides the user name of the power management (for
      example, IPMI, iLO) subsystem.
     </td></tr><tr><td>ilo-password (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This provides the user password of the power management
      (for example, IPMI, iLO) subsystem.
     </td></tr><tr><td>ilo-extras (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. Additional options to pass to ipmitool. For example, this
      may be required if the servers require additional IPMI addressing
      parameters.
     </td></tr><tr><td>moonshot (optional)</td><td>
      Provides the node identifier for HPE Moonshot servers, for example,
      <code class="literal">c4n1</code> where c4 is the cartridge and n1 is node.
     </td></tr><tr><td>hypervisor-id (optional)</td><td>
      This attribute serves two purposes: it indicates that this server is a
      virtual machine (VM), and it specifies the server id of the Cloud Lifecycle Manager
      hypervisor that will host the VM.
     </td></tr><tr><td>ardana-hypervisor (optional)</td><td>
      When set to True, this attribute identifies a server as a Cloud Lifecycle Manager
      hypervisor. A Cloud Lifecycle Manager hypervisor is a server that may be used to host
      other servers that are themselves virtual machines. Default value is
      <code class="literal">False</code>.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-servergroups"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups</span> <a title="Permalink" class="permalink" href="#configobj-servergroups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-servergroups.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-servergroups.xml</li><li><span class="ds-label">ID: </span>configobj-servergroups</li></ul></div></div></div></div><p>
  The server-groups configuration object provides a mechanism for organizing
  servers and networks into a hierarchy that can be used for allocation and
  network resolution.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

     - name: CLOUD
        server-groups:
         - AZ1
         - AZ2
         - AZ3
        networks:
         - EXTERNAL-API-NET
         - EXTERNAL-VM-NET
         - GUEST-NET
         - MANAGEMENT-NET

     #
     # Create a group for each failure zone
     #
     - name: AZ1
       server-groups:
         - RACK1

     - name: AZ2
       server-groups:
         - RACK2

     - name: AZ3
       server-groups:
         - RACK3

     #
     # Create a group for each rack
     #
     - name: RACK1
     - name: RACK2
     - name: RACK3</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator-defined name for the server group. The name is used to
      link server-groups together and to identify server-groups to be used as
      failure zones in a <span class="guimenu ">control-plane</span>. (see
      <a class="xref" href="#configobj-controlplane" title="6.2. Control Plane">Section 6.2, “Control Plane”</a>)
     </td></tr><tr><td>server-groups (optional)</td><td>
      A list of server-group names that are nested below this group in the
      hierarchy. Each server group can only be listed in one other server
      group (that is in a strict tree topology).
     </td></tr><tr><td>networks (optional)</td><td>
      A list of network names (see <a class="xref" href="#concept-networks" title="5.2.10.2. Networks">Section 5.2.10.2, “Networks”</a>). See
      <a class="xref" href="#concept-servergroups-networks" title="5.2.9.2. Server Groups and Networks">Section 5.2.9.2, “Server Groups and Networks”</a> for a description of
      how networks are matched to servers via server groups.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-serverroles"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Roles</span> <a title="Permalink" class="permalink" href="#configobj-serverroles">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-serverroles.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-serverroles.xml</li><li><span class="ds-label">ID: </span>configobj-serverroles</li></ul></div></div></div></div><p>
  The server-roles configuration object is a list of the various server roles
  that you can use in your cloud. Each server role is linked to other
  configuration objects:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Disk model (<a class="xref" href="#configobj-diskmodels" title="6.8.  Disk Models">Section 6.8, “
  Disk Models”</a>)
   </p></li><li class="listitem "><p>
    Interface model (<a class="xref" href="#configobj-interfacemodels" title="6.11. Interface Models">Section 6.11, “Interface Models”</a>)
   </p></li><li class="listitem "><p>
    Memory model (<a class="xref" href="#configobj-memorymodels" title="6.9. Memory Models">Section 6.9, “Memory Models”</a>)
   </p></li><li class="listitem "><p>
    CPU model (<a class="xref" href="#configobj-cpumodels" title="6.10.  CPU Models">Section 6.10, “
  CPU Models”</a>)
   </p></li></ul></div><p>
  Server roles are referenced in the servers (see
  <a class="xref" href="#configobj-serverroles" title="6.7. Server Roles">Section 6.7, “Server Roles”</a>) configuration object above.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  server-roles:

     - name: CONTROLLER-ROLE
       interface-model: CONTROLLER-INTERFACES
       disk-model: CONTROLLER-DISKS

     - name: COMPUTE-ROLE
       interface-model: COMPUTE-INTERFACES
       disk-model: COMPUTE-DISKS
       memory-model: COMPUTE-MEMORY
       cpu-model: COMPUTE-CPU</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the role.</td></tr><tr><td>interface-model</td><td>
      <p>
       The name of the <span class="guimenu ">interface-model</span> to be used for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same interface-model.
      </p>
     </td></tr><tr><td>disk-model</td><td>
      <p>
       The name of the <span class="guimenu ">disk-model</span> to use for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same disk-model.
      </p>
     </td></tr><tr><td>memory-model (optional)</td><td>
      <p>
       The name of the <span class="guimenu ">memory-model</span> to use for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same memory-model.
      </p>
     </td></tr><tr><td>cpu-model (optional)</td><td>
      <p>
       The name of the <span class="guimenu ">cpu-model</span> to use for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same cpu-model.
      </p>
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-diskmodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  Disk Models</span> <a title="Permalink" class="permalink" href="#configobj-diskmodels">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-diskmodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-diskmodels.xml</li><li><span class="ds-label">ID: </span>configobj-diskmodels</li></ul></div></div></div></div><p>
  The disk-models configuration object is used to specify how the directly
  attached disks on the server should be configured. It can also identify which
  service or service component consumes the disk, for example, Swift object
  server, and provide service-specific information associated with the disk.
  It is also used to specify disk sizing information for virtual machine
  servers.
 </p><p>
  Disks can be used as raw devices or as logical volumes and the disk model
  provides a configuration item for each.
 </p><p>
  If the operating system has been installed by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
  process then the root disk will already have been set up as a volume-group
  with a single logical-volume. This logical-volume will have been created on a
  partition identified, symbolically, in the configuration files as
  <code class="filename">/dev/sda_root</code>. This is due to the fact that different
  BIOS systems (UEFI, Legacy) will result in different partition numbers on the
  root disk.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  disk-models:
  - name: SES-DISKS

    volume-groups:
       - ...
    device-groups:
       - ...
    vm-size:
       ...</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      The name of the disk-model that is referenced from one or more
      server-roles.
     </td></tr><tr><td>volume-groups</td><td>
      A list of volume-groups to be configured (see below). There must be at
      least one volume-group describing the root file system.
      
     </td></tr><tr><td>device-groups (optional)</td><td>A list of device-groups (see below)</td></tr></tbody></table></div><div class="sect2" id="configobj-volumegroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Volume Groups</span> <a title="Permalink" class="permalink" href="#configobj-volumegroups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-volumegroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-volumegroups.xml</li><li><span class="ds-label">ID: </span>configobj-volumegroups</li></ul></div></div></div></div><p>
  The <span class="guimenu ">volume-groups</span> configuration object is used to define
  volume groups and their constituent logical volumes.
 </p><p>
  Note that volume-groups are not exact analogs of device-groups. A
  volume-group specifies a set of physical volumes used to make up a
  volume-group that is then subdivided into multiple logical volumes.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> operating system installation automatically creates a
  volume-group name "ardana-vg" on the first drive in the system. It creates a
  "root" logical volume there. The volume-group can be expanded by adding more
  physical-volumes (see examples). In addition, it is possible to create more
  logical-volumes on this volume-group to provide dedicated capacity for
  different services or file system mounts.
 </p><div class="verbatim-wrap"><pre class="screen">   volume-groups:
     - name: ardana-vg
       physical-volumes:
         - /dev/sda_root

       logical-volumes:
         - name: root
           size: 35%
           fstype: ext4
           mount: /

         - name: log
           size: 50%
           mount: /var/log
           fstype: ext4
           mkfs-opts: -O large_file

         - ...

     - name: vg-comp
       physical-volumes:
         - /dev/sdb
       logical-volumes:
         - name: compute
           size: 95%
           mount: /var/lib/nova
           fstype: ext4
           mkfs-opts: -O large_file</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>name</td><td>The name that will be assigned to the volume-group</td></tr><tr><td>physical-volumes</td><td>
      <p>
       A list of physical disks that make up the volume group.
      </p>
      <p>
       As installed by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> operating system install process, the
       volume group "ardana-vg" will use a large partition (sda_root) on the
       first disk. This can be expanded by adding additional disk(s).
      </p>
     </td></tr><tr><td>logical-volumes</td><td>
      A list of logical volume devices to create from the above named volume
      group.
     </td></tr><tr><td>name</td><td>The name to assign to the logical volume.</td></tr><tr><td>size</td><td>
      The size, expressed as a percentage of the entire volume group
      capacity, to assign to the logical volume.
     </td></tr><tr><td>fstype (optional)</td><td>
      The file system type to create on the logical volume. If none
      specified, the volume is not formatted.
     </td></tr><tr><td>mkfs-opts (optional)</td><td>
      Options, for example, <code class="literal">-O large_file</code> to pass to the
      mkfs command.
     </td></tr><tr><td>mode (optional)</td><td>
      The <code class="literal">mode</code> changes the root file system mode bits,
      which can be either a symbolic representation or an octal number
      representing the bit pattern for the new mode bits.
     </td></tr><tr><td>mount (optional)</td><td>Mount point for the file system.</td></tr><tr><td>consumer attributes (optional, consumer dependent)</td><td>
      <p>
       These will vary according to the service consuming the device group. The
       examples section provides sample content for the different services.
      </p>
     </td></tr></tbody></table></div><div id="id-1.3.4.4.9.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   Multipath storage should be listed as the corresponding
   <code class="filename">/dev/mapper/mpath<em class="replaceable ">X</em></code>
  </p></div></div><div class="sect2" id="configobj-devicegroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Device Groups</span> <a title="Permalink" class="permalink" href="#configobj-devicegroups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-devicegroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-devicegroups.xml</li><li><span class="ds-label">ID: </span>configobj-devicegroups</li></ul></div></div></div></div><p>
  The device-groups configuration object provides the mechanism to make the
  whole of a physical disk available to a service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the device group.</td></tr><tr><td>devices</td><td>
      <p>
       A list of named devices to be assigned to this group. There must be at
       least one device in the group.
      </p>
      <p>
       Multipath storage should be listed as the corresponding
       <code class="filename">/dev/mapper/mpath<em class="replaceable ">X</em>f</code>
      </p>
     </td></tr><tr><td>consumer</td><td>
      <p>
       Identifies the name of one of the storage services (for example, one
       of the following: Swift, Cinder, etc.) that will consume the disks in
       this device group.
      </p>
     </td></tr><tr><td>consumer attributes</td><td>
      <p>
       These will vary according to the service consuming the device group.
       The examples section provides sample content for the different
       services.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-memorymodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Memory Models</span> <a title="Permalink" class="permalink" href="#configobj-memorymodels">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-memorymodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-memorymodels.xml</li><li><span class="ds-label">ID: </span>configobj-memorymodels</li></ul></div></div></div></div><p>
  The memory-models configuration object describes details of the optional
  configuration of Huge Pages. It also describes the amount of memory to be
  allocated for virtual machine servers.
 </p><p>
  The memory-model allows the number of pages of a particular size to be
  configured at the server level or at the numa-node level.
 </p><p>
  The following example would configure:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    five 2 MB pages in each of numa nodes 0 and 1
   </p></li><li class="listitem "><p>
    three 1 GB pages (distributed across all numa nodes)
   </p></li><li class="listitem "><p>
    six 2 MB pages (distributed across all numa nodes)
   </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 2M
      huge-pages:
        - size: 2M
          count: 5
          numa-node: 0
        - size: 2M
          count: 5
          numa-node: 1
        - size: 1G
          count: 3
        - size: 2M
          count: 6
    - name: VIRTUAL-CONTROLLER-MEMORY
      vm-size:
        ram: 6G</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>The name of the memory-model that is referenced from one or more
              server-roles.</td></tr><tr><td>default-huge-page-size
              (optional)
            </td><td>
      <p>
       The default page size that will be used is specified when allocating
       huge pages.
      </p>
      <p>
       If not specified, the default is set by the operating system.
      </p>
     </td></tr><tr><td>huge-pages</td><td>A list of huge page definitions (see below).</td></tr></tbody></table></div><div class="sect2" id="configobj-huge-pages"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Huge Pages</span> <a title="Permalink" class="permalink" href="#configobj-huge-pages">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-huge_pages.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-huge_pages.xml</li><li><span class="ds-label">ID: </span>configobj-huge-pages</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>size</td><td>
      <p>
       The page size in kilobytes, megabytes, or gigabytes specified as
       <span class="emphasis"><em>n</em></span>X where:
      </p>
      <div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.4.10.8.2.1.4.1.2.2.1"><span class="term "><span class="emphasis"><em>n</em></span>
        </span></dt><dd><p>
          is an integer greater than zero
         </p></dd><dt id="id-1.3.4.4.10.8.2.1.4.1.2.2.2"><span class="term ">X</span></dt><dd><p>
          is one of "K", "M" or "G"
         </p></dd></dl></div>
     </td></tr><tr><td>count</td><td>The number of pages of this size to create (must be greater than zero).</td></tr><tr><td>numa-node (optional) </td><td>
      <p>
       If specified the pages will be created in the memory associated with
       this numa node.
      </p>
      <p>
       If not specified the pages are distributed across numa nodes by the
       operating system.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-cpumodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  CPU Models</span> <a title="Permalink" class="permalink" href="#configobj-cpumodels">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-cpumodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpumodels.xml</li><li><span class="ds-label">ID: </span>configobj-cpumodels</li></ul></div></div></div></div><p>
  The <code class="literal">cpu-models</code> configuration object describes how CPUs are
  assigned for use by service components such as Nova (for VMs) and Open
  vSwitch (for DPDK), and whether or not those CPUs are isolated from the
  general kernel SMP balancing and scheduling algorithms.
  It also describes the number of vCPUs for virtual machine servers.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  cpu-models:
    - name: COMPUTE-CPU
      assignments:
        - components:
            - nova-compute-kvm
          cpu:
            - processor-ids: 0-1,3,5-7
              role: vm
        - components:
            - openvswitch
          cpu:
            - processor-ids: 4,12
              isolate: False
              role: eal
            - processor-ids: 2,10
              role: pmd
    - name: VIRTUAL-CONTROLLER-CPU
      vm-size:
         vcpus: 4</pre></div><p>
  <span class="bold"><strong>cpu-models</strong></span>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the cpu model.</td></tr><tr><td>assignments</td><td>A list of CPU assignments .</td></tr></tbody></table></div><div class="sect2" id="configobj-cpu-assignments"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CPU Assignments</span> <a title="Permalink" class="permalink" href="#configobj-cpu-assignments">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-cpu_assignments.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpu_assignments.xml</li><li><span class="ds-label">ID: </span>configobj-cpu-assignments</li></ul></div></div></div></div><p>
  <span class="bold"><strong>assignments</strong></span>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>components</td><td>A list of components to which the CPUs will be assigned.</td></tr><tr><td>cpu</td><td>
      A list of CPU usage objects (see <a class="xref" href="#configobj-cpu-usage" title="6.10.2. CPU Usage">Section 6.10.2, “CPU Usage”</a>
      below).
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-cpu-usage"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CPU Usage</span> <a title="Permalink" class="permalink" href="#configobj-cpu-usage">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-cpu_usage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpu_usage.xml</li><li><span class="ds-label">ID: </span>configobj-cpu-usage</li></ul></div></div></div></div><p>
  <span class="bold"><strong>cpu</strong></span>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>processor-ids</td><td>A list of CPU IDs as seen by the operating system.</td></tr><tr><td>isolate (optional) </td><td>
      <p>
       A Boolean value which indicates if the CPUs are to be isolated from the
       general kernel SMP balancing and scheduling algorithms. The specified
       processor IDs will be configured in the Linux kernel isolcpus parameter.
      </p>
      <p>
       The default value is True.
      </p>
     </td></tr><tr><td>role</td><td>A role within the component for which the CPUs will be used.</td></tr></tbody></table></div></div><div class="sect2" id="configobj-cpu-components-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Components and Roles in the CPU Model</span> <a title="Permalink" class="permalink" href="#configobj-cpu-components-roles">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-cpu_components_roles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpu_components_roles.xml</li><li><span class="ds-label">ID: </span>configobj-cpu-components-roles</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component</th><th>Role</th><th>Description</th></tr></thead><tbody><tr><td>nova-compute-kvm</td><td>vm</td><td>
      <p>
       The specified processor IDs will be configured in the Nova
       vcpu_pin_set option.
      </p>
     </td></tr><tr><td rowspan="2">openvswitch</td><td>eal</td><td>
      <p>
       The specified processor IDs will be configured in the Open vSwitch
       DPDK EAL -c (coremask) option. Refer to the DPDK documentation for
       details.
      </p>
     </td></tr><tr><td>pmd</td><td>
      <p>
       The specified processor IDs will be configured in the <span class="productname">Open vSwitch</span>
       pmd-cpu-mask option. Refer to the <span class="productname">Open vSwitch</span> documentation and the
       ovs-vswitchd.conf.db man page for details.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-interfacemodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interface Models</span> <a title="Permalink" class="permalink" href="#configobj-interfacemodels">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-interfacemodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-interfacemodels.xml</li><li><span class="ds-label">ID: </span>configobj-interfacemodels</li></ul></div></div></div></div><p>
  The interface-models configuration object describes how network interfaces
  are bonded and the mapping of network groups onto interfaces. Interface
  devices are identified by name and mapped to a particular physical port by
  the <span class="guimenu ">nic-mapping</span> (see <a class="xref" href="#concept-nicmapping" title="5.2.10.4. NIC Mapping">Section 5.2.10.4, “NIC Mapping”</a>).
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  interface-models:
     - name: INTERFACE_SET_CONTROLLER
       network-interfaces:
          - name: BONDED_INTERFACE
            device:
              name: bond0
            bond-data:
              provider: linux
              devices:
                - name: hed3
                - name: hed4
              options:
                mode: active-backup
                miimon: 200
                primary: hed3
            network-groups:
               - EXTERNAL_API
               - EXTERNAL_VM
               - GUEST

          - name: UNBONDED_INTERFACE
            device:
               name: hed0
            network-groups:
               - MGMT


       fcoe-interfaces:
          - name: FCOE_DEVICES
            devices:
              - eth7
              - eth8


     - name: INTERFACE_SET_DPDK
       network-interfaces:
          - name: BONDED_DPDK_INTERFACE
            device:
              name: bond0
            bond-data:
              provider: openvswitch
              devices:
                - name: dpdk0
                - name: dpdk1
              options:
                mode: active-backup
            network-groups:
               - GUEST
          - name: UNBONDED_DPDK_INTERFACE
            device:
               name: dpdk2
            network-groups:
               - PHYSNET2
       dpdk-devices:
         - devices:
             - name: dpdk0
             - name: dpdk1
             - name: dpdk2
               driver: igb_uio
           components:
             - openvswitch
           eal-options:
             - name: socket-mem
               value: 1024,0
             - name: n
               value: 2
           component-options:
             - name: n-dpdk-rxqs
               value: 64</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the interface model.</td></tr><tr><td>network-interfaces</td><td>A list of network interface definitions.</td></tr><tr><td>
      fcoe-interfaces (optional): <a class="xref" href="#configobj-fcoeinterfaces" title="6.11.2. fcoe-interfaces">Section 6.11.2, “fcoe-interfaces”</a>
      </td><td>
      <p>
       A list of network interfaces that will be used for Fibre Channel over
       Ethernet (FCoE). This is only needed for devices that present as a
       native FCoE device, not cards such as Emulex which present FCoE as a FC
       device.
      </p>
     </td></tr><tr><td>dpdk-devices (optional)</td><td>A list of DPDK device definitions.</td></tr></tbody></table></div><div id="id-1.3.4.4.12.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   The devices must be <span class="quote">“<span class="quote ">raw</span>”</span> device names, not names controlled
   via a nic-mapping.
  </p></div><div class="sect2" id="configobj-network-interfaces"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  network-interfaces</span> <a title="Permalink" class="permalink" href="#configobj-network-interfaces">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-network_interfaces.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-network_interfaces.xml</li><li><span class="ds-label">ID: </span>configobj-network-interfaces</li></ul></div></div></div></div><p>
  The network-interfaces configuration object has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the interface</td></tr><tr><td>device</td><td>
      <p>
       A dictionary containing the network device name (as seen on the
       associated server) and associated properties (see
       <a class="xref" href="#configobj-network-interfaces-device" title="6.11.1.1. network-interfaces device">Section 6.11.1.1, “network-interfaces device”</a> for details).
      </p>
      
     </td></tr><tr><td>network-groups (optional if forced-network-groups is defined)</td><td>
      A list of one or more <span class="guimenu ">network-groups</span> (see
      <a class="xref" href="#configobj-networkgroups" title="6.13. Network Groups">Section 6.13, “Network Groups”</a>) containing
      <span class="guimenu ">networks</span> (see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>)
      that can be accessed via this interface. Networks in these groups will
      only be configured if there is at least one
      <span class="guimenu ">service-component</span> on the server which matches the
      list of component-endpoints defined in the
      <span class="guimenu ">network-group</span>.
     </td></tr><tr><td>forced-network-groups (optional if network-groups is defined)</td><td>
      A list of one or more <span class="guimenu ">network-groups</span> (see
      <a class="xref" href="#configobj-networkgroups" title="6.13. Network Groups">Section 6.13, “Network Groups”</a>) containing
      <span class="guimenu ">networks</span> (see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>)
      that can be accessed via this interface. Networks in these groups are
      always configured on the server.
     </td></tr><tr><td>passthrough-network-groups (optional)</td><td>
      A list of one or more network-groups (see
      <a class="xref" href="#configobj-networkgroups" title="6.13. Network Groups">Section 6.13, “Network Groups”</a>) containing networks (see
      <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>) that can be accessed by servers
      running as virtual machines on an Cloud Lifecycle Manager hypervisor server. Networks in
      these groups are not configured on the Cloud Lifecycle Manager hypervisor server unless
      they also are specified in the <code class="literal">network-groups</code> or
      <code class="literal">forced-network-groups</code> attributes.
     </td></tr></tbody></table></div><div class="sect3" id="configobj-network-interfaces-device"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.11.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">network-interfaces device</span> <a title="Permalink" class="permalink" href="#configobj-network-interfaces-device">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-network_interfaces_device.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-network_interfaces_device.xml</li><li><span class="ds-label">ID: </span>configobj-network-interfaces-device</li></ul></div></div></div></div><p>
  <span class="bold"><strong>network-interfaces device</strong></span>
 </p><p>
  The network-interfaces device configuration object has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       When configuring a bond, this is used as the bond device name - the
       names of the devices to be bonded are specified in the bond-data
       section.
      </p>
      <p>
       If the interface is not bonded, this must be the name of the device
       specified by the nic-mapping (see NIC Mapping).
       
      </p>
     </td></tr><tr><td>vf-count (optional)</td><td>
      <p>
       Indicates that the interface is to be used for SR-IOV. The value is the
       number of virtual functions to be created. The associated device
       specified by the nic-mapping must have a valid nice-device-type.
      </p>
      <p>
       vf-count cannot be specified on bonded interfaces
      </p>
      <p>
       Interfaces used for SR-IOV must be associated with a network with
       <code class="literal">tagged-vlan: false</code>.
      </p>
     </td></tr><tr><td>sriov-only (optional) </td><td>
      <p>
       Only valid when vf-count is specified. If set to true then the interface
       is to be used for virtual functions only and the physical function will
       not be used.
      </p>
      <p>
       The default value is False.
      </p>
     </td></tr><tr><td>pci-pt (optional) </td><td>
      <p>
       If set to true then the interface is used for PCI passthrough.
      </p>
      <p>
       The default value is False.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect2" id="configobj-fcoeinterfaces"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">fcoe-interfaces</span> <a title="Permalink" class="permalink" href="#configobj-fcoeinterfaces">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-fcoeinterfaces.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-fcoeinterfaces.xml</li><li><span class="ds-label">ID: </span>configobj-fcoeinterfaces</li></ul></div></div></div></div><p>
  The fcoe-interfaces configuration object has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the group of FCOE interfaces </td></tr><tr><td>devices</td><td>
      <p>
       A list of network devices that will be configured for FCOE
      </p>
      <p>
       Entries in this must be the name of a device specified by the
       nic-mapping (see <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>).
      </p>
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-dpdkdevices"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">dpdk-devices</span> <a title="Permalink" class="permalink" href="#configobj-dpdkdevices">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-dpdkdevices.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-dpdkdevices.xml</li><li><span class="ds-label">ID: </span>configobj-dpdkdevices</li></ul></div></div></div></div><p>
  The dpdk-devices configuration object has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>devices</td><td>
      <p>
       A list of network devices to be configured for DPDK. See
       <a class="xref" href="#configobj-dpdkdevices-devices" title="6.11.3.1.  dpdk-devices devices">Section 6.11.3.1, “
  dpdk-devices devices”</a>.
      </p>
     </td></tr><tr><td>eal-options</td><td>
      <p>
       A list of key-value pairs that may be used to set DPDK Environmental
       Abstraction Layer (EAL) options. Refer to the DPDK documentation for
       details.
      </p>
      <p>
       Note that the cpu-model should be used to specify the processor IDs to
       be used by EAL for this component. The EAL coremask
       (<code class="literal">-c</code>) option will be set automatically based on the
       information in the cpu-model, and so should not be specified here. See
       <a class="xref" href="#configobj-cpumodels" title="6.10.  CPU Models">Section 6.10, “
  CPU Models”</a>.
      </p>
     </td></tr><tr><td>component-options</td><td>
      <p>
       A list of key-value pairs that may be used to set component-specific
       configuration options.
      </p>
     </td></tr></tbody></table></div><div class="sect3" id="configobj-dpdkdevices-devices"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.11.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  dpdk-devices devices</span> <a title="Permalink" class="permalink" href="#configobj-dpdkdevices-devices">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-dpdkdevices_devices.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-dpdkdevices_devices.xml</li><li><span class="ds-label">ID: </span>configobj-dpdkdevices-devices</li></ul></div></div></div></div><p>
  The devices configuration object within dpdk-devices has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>name</td><td>The name of a network device to be used with DPDK. The device names must be the
            logical-name specified by the nic-mapping
            (see <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>).</td></tr><tr><td>driver (optional) </td><td>
      <p>
       Defines the userspace I/O driver to be used for network devices where
       the native device driver does not provide userspace I/O capabilities.
      </p>
      <p>
       The default value is <code class="literal">igb_uio</code>.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="configobj-dpdk-componentoptions"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.11.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK component-options for the openvswitch component</span> <a title="Permalink" class="permalink" href="#configobj-dpdk-componentoptions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-dpdk_componentoptions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-dpdk_componentoptions.xml</li><li><span class="ds-label">ID: </span>configobj-dpdk-componentoptions</li></ul></div></div></div></div><p>
  The following options are supported for use with the openvswitch component:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>n-dpdk-rxqs</td><td>
      <p>
       Number of rx queues for each DPDK interface. Refer to the Open vSwitch
       documentation and the <code class="literal">ovs-vswitchd.conf.db</code> man page
       for details.
      </p>
     </td></tr></tbody></table></div><p>
  Note that the cpu-model should be used to define the CPU affinity of the <span class="productname">Open vSwitch</span>
  PMD (Poll Mode Driver) threads. The <span class="productname">Open vSwitch</span>
  <code class="literal">pmd-cpu-mask</code> option will be set automatically based on the
  information in the cpu-model. See <a class="xref" href="#configobj-cpumodels" title="6.10.  CPU Models">Section 6.10, “
  CPU Models”</a>.
 </p></div></div></div><div class="sect1" id="configobj-nicmappings"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NIC Mappings</span> <a title="Permalink" class="permalink" href="#configobj-nicmappings">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-nicmappings.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-nicmappings.xml</li><li><span class="ds-label">ID: </span>configobj-nicmappings</li></ul></div></div></div></div><p>
  The <span class="guimenu ">nic-mappings</span> configuration object is used to ensure
  that the network device name used by the operating system always maps to the
  same physical device. A <span class="guimenu ">nic-mapping</span> is associated to a
  <span class="guimenu ">server</span> in the server definition file.  Devices should be named <code class="literal">hedN</code> to
  avoid name clashes with any other devices configured during the operating
  system install as well as any interfaces that are not being managed by
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, ensuring that all devices on a baremetal machine are specified in
  the file. An excerpt from <code class="filename">nic_mappings.yml</code> illustrates:
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  nic-mappings:

    - name: HP-DL360-4PORT
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:07:00.0"

        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"
          nic-device-type: '8086:10fb'

        - logical-name: hed3
          type: multi-port
          bus-address: "0000:09:00.0"
          port-attributes:
              port-num: 0

        - logical-name: hed4
          type: multi-port
          bus-address: "0000:09:00.0"
          port-attributes:
              port-num: 1</pre></div><p>
  Each entry in the <span class="guimenu ">nic-mappings</span> list has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator-defined name for the mapping. This name may be used in
      a server definition (see <a class="xref" href="#configobj-servers" title="6.5. Servers">Section 6.5, “Servers”</a>) to apply
      the mapping to that server.
     </td></tr><tr><td>physical-ports</td><td>A list containing device name to address mapping information.</td></tr></tbody></table></div><p>
  Each entry in the <span class="guimenu ">physical-ports</span> list has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>logical-name</td><td>
      The network device name that will be associated with the device at the
      specified <span class="emphasis"><em>bus-address</em></span>. The logical-name specified
      here can be used as a device name in network interface model
      definitions. (See <a class="xref" href="#configobj-interfacemodels" title="6.11. Interface Models">Section 6.11, “Interface Models”</a>.)
     </td></tr><tr><td>type</td><td>
      <p>
       The type of port. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports "simple-port" and
       "multi-port". Use "simple-port" if your device has a unique bus-address.
       Use "multi-port" if your hardware requires a "port-num" attribute to
       identify a single port on a multi-port device. An examples of such a
       device is:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Mellanox Technologies MT26438 [ConnectX VPI PCIe 2.0 5GT/s - IB QDR /
         10GigE Virtualization+]
        </p></li></ul></div>
     </td></tr><tr><td>bus-address</td><td>
      PCI bus address of the port. Enclose the bus address in quotation marks
      so yaml does not misinterpret the embedded colon
      (<code class="literal">:</code>) characters. See
      <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 2 “Pre-Installation Checklist”</span> for details on how to determine
      this value. </td></tr><tr><td>
      port-attributes (required if type is
      <code class="literal">multi-port</code>)
     </td><td>
      Provides a list of attributes for the physical port. The current
      implementation supports only one attribute, "port-num". Multi-port
      devices share a bus-address. Use the "port-num" attribute to identify
      which physical port on the multi-port device to map. See
      <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 2 “Pre-Installation Checklist”</span> for details on how to determine
      this value.</td></tr><tr><td>nic-device-type (optional) </td><td>
      Specifies the PCI vendor ID and device ID of the port in the format of
      <code class="literal"><em class="replaceable ">VENDOR_ID</em>:<em class="replaceable ">DEVICE_ID</em></code>, for example,
      <code class="literal">8086:10fb</code>.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-networkgroups"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Groups</span> <a title="Permalink" class="permalink" href="#configobj-networkgroups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-networkgroups.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-networkgroups.xml</li><li><span class="ds-label">ID: </span>configobj-networkgroups</li></ul></div></div></div></div><p>
  Network-groups define the overall network topology, including where
  service-components connect, what load balancers are to be deployed, which
  connections use TLS, and network routing. They also provide the data needed
  to map Neutron's network configuration to the physical networking.
 </p><div id="id-1.3.4.4.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The name of the "MANAGEMENT" <span class="guimenu ">network-group</span> cannot be
   changed. It must be upper case. Every SUSE <span class="productname">OpenStack</span> Cloud requires this network group in
   order to be valid.
  </p></div><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  network-groups:

     - name: EXTERNAL-API
       hostname-suffix: extapi

       load-balancers:
         - provider: ip-cluster
           name: extlb
           external-name:

           tls-components:
             - default
           roles:
            - public
           cert-file: my-public-entry-scale-kvm-cert

      - name: EXTERNAL-VM
        tags:
          - neutron.l3_agent.external_network_bridge

      - name: GUEST
        hostname-suffix: guest
        tags:
          - neutron.networks.vxlan

      - name: MANAGEMENT
        hostname-suffix: mgmt
        hostname: true

        component-endpoints:
          - default

        routes:
          - default

        load-balancers:
          - provider: ip-cluster
            name: lb
            components:
              - default
            roles:
              - internal
              - admin

        tags:
          - neutron.networks.vlan:
              provider-physical-network: physnet1</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator-defined name for the network group. The name is used
      to make references from other parts of the input model.
     </td></tr><tr><td>component-endpoints (optional)</td><td>
      The list of <span class="guimenu ">service-components</span> that will bind to or
      need direct access to networks in this network-group.</td></tr><tr><td>hostname (optional)</td><td>
      <p>
       If set to true, the name of the address associated with a network in
       this group will be used to set the hostname of the server.
      </p>
     </td></tr><tr><td>hostname-suffix (optional)</td><td>
      If supplied, this string will be used in the name generation (see
      <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>). If not specified, the name of the
      network-group will be used.
     </td></tr><tr><td>load-balancers (optional)</td><td>
      <p>
       A list of load balancers to be configured on networks in this
       network-group. Because load balances need a virtual IP address, any
       network group that contains a load balancer can only have one network
       associated with it.
      </p>
      <p>
       For clouds consisting of a single control plane, a load balancer may be
       fully defined within a <code class="literal">network-group</code> object. See Load
       balancer definitions in network groups.
      </p>
      <p>
       Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, a load balancer may be defined within a
       <code class="literal">control-plane</code> object and referenced by name from a
       <code class="literal">network-group</code> object. See
       <a class="xref" href="#configobj-lb-defs-networkgroups" title="6.13.1. Load Balancer Definitions in Network Groups">Section 6.13.1, “Load Balancer Definitions in Network Groups”</a>
       in control planes.
      </p>
     </td></tr><tr><td>routes (optional)</td><td>
      <p>
       A list of <span class="guimenu ">network-groups</span> that networks in this group
       provide access to via their gateway. This can include the value
       <code class="literal">default</code> to define the default route.
      </p>
      <p>
       A network group with no services attached to it can be used to define
       routes to external networks.
      </p>
      <p>
       The name of a Neutron provide network defined via configuration-data
       (see <a class="xref" href="#configobj-neutron-provider-networks" title="6.16.2.1. neutron-provider-networks">Section 6.16.2.1, “neutron-provider-networks”</a>) can also be
       included in this list.
      </p>
     </td></tr><tr><td>tags (optional)</td><td>
      <p>
       A list of network tags. Tags provide the linkage between the physical
       network configuration and the Neutron network configuration.
      </p>
      <p>
       Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, network tags may be defined as part of a
       Neutron <code class="literal">configuration-data</code> object rather than as part
       of a <code class="literal">network-group</code> object (see
       <a class="xref" href="#configobj-configurationdata-neutron" title="6.16.2. Neutron Configuration Data">Section 6.16.2, “Neutron Configuration Data”</a>).
      </p>
     </td></tr><tr><td>mtu (optional)</td><td>
      <p>
       Specifies the MTU value required for networks in this network group If
       not specified a default value of 1500 is used.
      </p>
      <p>
       See <a class="xref" href="#configobj-mtu" title="6.13.3. MTU (Maximum Transmission Unit)">Section 6.13.3, “MTU (Maximum Transmission Unit)”</a> on how MTU settings are applied to
       interfaces when there are multiple tagged networks on the same
       interface.
      </p>
     </td></tr></tbody></table></div><div id="id-1.3.4.4.14.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   <code class="literal">hostname</code><span class="bold"><strong>must</strong></span> be set to
   <code class="literal">true</code> for one, and only one, of your network groups.
  </p></div><p>
  A load balancer definition has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the load balancer.</td></tr><tr><td>provider</td><td>
      The service component that implements the load balancer. Currently only
      <code class="literal">ip-cluster</code> (ha-proxy) is supported. Future releases
      will provide support for external load balancers.
     </td></tr><tr><td>roles</td><td>
      The list of endpoint roles that this load balancer provides (see
      below). Valid roles are "public", "internal", and "admin'. To ensure
      separation of concerns, the role "public" cannot be combined with any
      other role. See <a class="xref" href="#concept-loadbalancers" title="5.2.10.1.1. Load Balancers">Section 5.2.10.1.1, “Load Balancers”</a> for an example
      of how the role provides endpoint separation.
     </td></tr><tr><td>components (optional)</td><td>The list of <span class="guimenu ">service-components</span> for which the load
                        balancer provides a non-encrypted virtual IP address for.</td></tr><tr><td>tls-components (optional)</td><td>
      The list of <span class="guimenu ">service-components</span> for which the load
      balancer provides TLS-terminated virtual IP addresses for. In
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, TLS is supported both for internal and public endpoints.
     </td></tr><tr><td>external-name (optional)</td><td>
      The name to be registered in Keystone for the publicURL. If not
      specified, the virtual IP address will be registered. Note that this
      value cannot be changed after the initial deployment.
     </td></tr><tr><td>cert-file (optional)</td><td>
      The name of the certificate file to be used for TLS endpoints.
     </td></tr></tbody></table></div><div class="sect2" id="configobj-lb-defs-networkgroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer Definitions in Network Groups</span> <a title="Permalink" class="permalink" href="#configobj-lb-defs-networkgroups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-lb_defs_networkgroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-lb_defs_networkgroups.xml</li><li><span class="ds-label">ID: </span>configobj-lb-defs-networkgroups</li></ul></div></div></div></div><p>
  In a cloud consisting of a single control-plane, a
  <code class="literal">load-balancer</code> may be fully defined within a
  <code class="literal">network-groups</code> object as shown in the examples above. See
  section <a class="xref" href="#configobj-load-balancers" title="6.3. Load Balancers">Section 6.3, “Load Balancers”</a> for a complete description
  of load balancer attributes.
 </p><p>
  Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, a <code class="literal">load-balancer</code> may be
  defined within a <code class="literal">control-plane</code> object in which case the
  network-group provides just a list of load balancer names as shown below. See
  section <a class="xref" href="#configobj-load-balancers" title="6.3. Load Balancers">Section 6.3, “Load Balancers”</a> definitions in control
  planes.
 </p><div class="verbatim-wrap"><pre class="screen">network-groups:

     - name: EXTERNAL-API
       hostname-suffix: extapi

       load-balancers:
         - lb-cp1
         - lb-cp2</pre></div><p>
  The same load balancer name can be used in multiple control-planes to make
  the above list simpler.
 </p></div><div class="sect2" id="configobj-networktags"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Tags</span> <a title="Permalink" class="permalink" href="#configobj-networktags">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-networktags.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-networktags.xml</li><li><span class="ds-label">ID: </span>configobj-networktags</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports a small number of network tags which may be used to convey
  information between the input model and the service components (currently
  only Neutron uses network tags). A network tag consists minimally of a tag
  name; but some network tags have additional attributes.
 </p><div class="table" id="neutron-networks-vxlan"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.1: </span><span class="name">neutron.networks.vxlan </span><a title="Permalink" class="permalink" href="#neutron-networks-vxlan">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.networks.vxlan" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.networks.vxlan</td><td>This tag causes Neutron to be configured to use VxLAN as the underlay for
                        tenant networks. The associated network group will carry the VxLAN
                        traffic.</td></tr><tr><td>tenant-vxlan-id-range (optional)</td><td>Used to specify the VxLAN identifier range in the format
     <span class="quote">“<span class="quote "><em class="replaceable ">MIN-ID</em>:<em class="replaceable ">MAX-ID</em></span>”</span>. The
     default range is <span class="quote">“<span class="quote ">1001:65535</span>”</span>. Enclose the range in quotation
     marks. Multiple ranges can be specified as a comma-separated
     list. </td></tr></tbody></table></div></div><p>
  Example using the default ID range:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vxlan</pre></div><p>
  Example using a user-defined ID range:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vxlan:
        tenant-vxlan-id-range: “1:20000”</pre></div><p>
  Example using multiple user-defined ID range:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vxlan:
        tenant-vxlan-id-range: “1:2000,3000:4000,5000:6000”</pre></div><div class="table" id="neutron-networks-vlan"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.2: </span><span class="name">neutron.networks.vlan </span><a title="Permalink" class="permalink" href="#neutron-networks-vlan">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.networks.vlan" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.networks.vlan</td><td>
      <p>
       This tag causes Neutron to be configured for provider VLAN networks, and
       optionally to use VLAN as the underlay for tenant networks. The
       associated network group will carry the VLAN traffic. This tag can be
       specified on multiple network groups. However, this tag does not cause
       any Neutron networks to be created, that must be done in Neutron after
       the cloud is deployed.
      </p>
     </td></tr><tr><td>provider-physical-network</td><td>The provider network name. This is the name to be used in the Neutron API
                        for the <span class="emphasis"><em>provider:physical_network</em></span> parameter of network
                        objects.</td></tr><tr><td>tenant-vlan-id-range (optional)</td><td>This attribute causes Neutron to use VLAN for tenant networks; omit
     this attribute if you are using provider VLANs only. It specifies the VLAN
     ID range for tenant networks, in the format
     <span class="quote">“<span class="quote "><em class="replaceable ">MIN-ID</em>:<em class="replaceable ">MAX-ID</em></span>”</span>. Enclose
     the range in quotation marks. Multiple ranges can be specified as a
     comma-separated list.</td></tr></tbody></table></div></div><p>
  Example using a provider vlan only (may be used with tenant VxLAN):
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vlan:
        provider-physical-network: physnet1</pre></div><p>
  Example using a tenant and provider VLAN:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vlan:
        provider-physical-network: physnet1
        tenant-vlan-id-range: “30:50,100:200”</pre></div><div class="table" id="neutron-networks-flat"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.3: </span><span class="name">neutron.networks.flat </span><a title="Permalink" class="permalink" href="#neutron-networks-flat">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.networks.flat" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.networks.flat</td><td>
      <p>
       This tag causes Neutron to be configured for provider flat networks. The
       associated network group will carry the traffic. This tag can be
       specified on multiple network groups. However, this tag does not cause
       any Neutron networks to be created, that must be done in Neutron after
       the cloud is deployed.
      </p>
     </td></tr><tr><td>provider-physical-network</td><td>The provider network name. This is the name to be used in the Neutron API
                        for the <span class="emphasis"><em>provider:physical_network</em></span> parameter of network
                        objects. When specified on multiple network groups, the name must be unique
                        for each network group.</td></tr></tbody></table></div></div><p>
  Example using a provider flat network:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.flat:
        provider-physical-network: flatnet1</pre></div><div class="table" id="neutron-l3-agent"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.4: </span><span class="name">neutron.l3_agent.external_network_bridge </span><a title="Permalink" class="permalink" href="#neutron-l3-agent">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.l3_agent.external_network_bridge" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.l3_agent.external_network_bridge</td><td>
      <p>
       This tag causes the Neutron L3 Agent to be configured to use the
       associated network group as the Neutron external network for floating IP
       addresses. A CIDR <span class="bold"><strong>should not</strong></span> be defined
       for the associated physical network, as that will cause addresses from
       that network to be configured in the hypervisor. When this tag is used,
       provider networks cannot be used as external networks. However, this tag
       does not cause a Neutron external networks to be created, that must be
       done in Neutron after the cloud is deployed.
      </p>
     </td></tr></tbody></table></div></div><p>
  Example using neutron.l3_agent.external_network_bridge:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.l3_agent.external_network_bridge</pre></div></div><div class="sect2" id="configobj-mtu"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MTU (Maximum Transmission Unit)</span> <a title="Permalink" class="permalink" href="#configobj-mtu">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-mtu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-mtu.xml</li><li><span class="ds-label">ID: </span>configobj-mtu</li></ul></div></div></div></div><p>
  A network group may optionally specify an MTU for its networks to use.
  Because a network-interface in the interface-model may have a mix of one
  untagged-vlan network group and one or more tagged-vlan network groups, there
  are some special requirements when specifying an MTU on a network group.
 </p><p>
  If the network group consists of untagged-vlan network(s) then its specified
  MTU must be greater than or equal to the MTU of any tagged-vlan network
  groups which are co-located on the same network-interface.
 </p><p>
  For example consider a network group with untagged VLANs, NET-GROUP-1, which
  is going to share (via a Network Interface definition) a device (eth0) with
  two network groups with tagged VLANs: NET-GROUP-2 (ID=201, MTU=1550) and
  NET-GROUP-3 (ID=301, MTU=9000).
 </p><p>
  The device (eth0) must have an MTU which is large enough to accommodate the
  VLAN in NET-GROUP-3. Since NET-GROUP-1 has untagged VLANS it will also be
  using this device and so it must also have an MTU of 9000, which results in
  the following configuration.
 </p><div class="verbatim-wrap"><pre class="screen">    +eth0 (9000)   &lt;------ this MTU comes from NET-GROUP-1
    | |
    | |----+ vlan201@eth0 (1550)
    \------+ vlan301@eth0 (9000)</pre></div><p>
  Where an interface is used only by network groups with tagged VLANs the MTU
  of the device or bond will be set to the highest MTU value in those groups.
 </p><p>
  For example if bond0 is configured to be used by three network groups:
  NET-GROUP-1 (ID=101, MTU=3000), NET-GROUP-2 (ID=201, MTU=1550) and
  NET-GROUP-3 (ID=301, MTU=9000).
 </p><p>
  Then the resulting configuration would be:
 </p><div class="verbatim-wrap"><pre class="screen">    +bond0 (9000)   &lt;------ because of NET-GROUP-3
    | | |
    | | |--+vlan101@bond0 (3000)
    | |----+vlan201@bond0 (1550)
    |------+vlan301@bond0 (9000)</pre></div></div></div><div class="sect1" id="configobj-networks"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networks</span> <a title="Permalink" class="permalink" href="#configobj-networks">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-networks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-networks.xml</li><li><span class="ds-label">ID: </span>configobj-networks</li></ul></div></div></div></div><p>
  A network definition represents a physical L3 network used by the cloud
  infrastructure. Note that these are different from the network definitions
  that are created/configured in Neutron, although some of the networks may be
  used by Neutron.
 </p><div class="verbatim-wrap"><pre class="screen">---
   product:
     version: 2

   networks:
     - name: NET_EXTERNAL_VM
       vlanid: 102
       tagged-vlan: true
       network-group: EXTERNAL_VM

     - name: NET_GUEST
       vlanid: 103
       tagged-vlan: true
       cidr: 10.1.1.0/24
       gateway-ip: 10.1.1.1
       network-group: GUEST

     - name: NET_MGMT
       vlanid: 100
       tagged-vlan: false
       cidr: 10.2.1.0/24
       addresses:
       - 10.2.1.10-10.2.1.20
       - 10.2.1.24
       - 10.2.1.30-10.2.1.36
       gateway-ip: 10.2.1.1
       network-group: MGMT</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      The name of this network. The network <span class="emphasis"><em>name</em></span> may be
      used in a server-group definition (see
      <a class="xref" href="#configobj-servergroups" title="6.6. Server Groups">Section 6.6, “Server Groups”</a>) to specify a particular
      network from within a network-group to be associated with a set of
      servers.
     </td></tr><tr><td>network-group</td><td>The name of the associated network group.</td></tr><tr><td>vlanid (optional)</td><td>
      The IEEE 802.1Q VLAN Identifier, a value in the range 1 through 4094. A
      <span class="emphasis"><em>vlanid</em></span> must be specified when
      <span class="emphasis"><em>tagged-vlan</em></span> is true.
     </td></tr><tr><td>tagged-vlan (optional)</td><td>
      May be set to <code class="literal">true</code> or <code class="literal">false</code>. If
      true, packets for this network carry the
      <span class="emphasis"><em>vlanid </em></span>in the packet header; such packets are
      referred to as VLAN-tagged frames in IEEE 1Q.
     </td></tr><tr><td>cidr (optional)</td><td>The IP subnet associated with this network.</td></tr><tr><td>addresses (optional)</td><td>
      <p>
       A list of IP addresses or IP address ranges (specified as
       <code class="literal"><em class="replaceable ">START_ADDRESS_RANGE</em>-<em class="replaceable ">END_ADDRESS_RANGE</em></code>
       from which server addresses may be allocated. The default value is the
       first host address within the CIDR (for example, the
       <code class="literal">.1</code> address).
      </p>
      <p>
       The <code class="literal">addresses</code> parameter provides more flexibility
       than the <code class="literal">start-address</code> and
       <code class="literal">end-address</code> parameters and so is the preferred means
       of specifying this data.
      </p>
     </td></tr><tr><td>start-address (optional) (deprecated)</td><td>
      <p>
       An IP address within the <span class="emphasis"><em>CIDR</em></span> which will be used as
       the start of the range of IP addresses from which server addresses may
       be allocated. The default value is the first host address within the
       <span class="emphasis"><em>CIDR</em></span> (for example, the .1 address).
      </p>
     </td></tr><tr><td>end-address (optional) (deprecated)</td><td>
      <p>
       An IP address within the <span class="emphasis"><em>CIDR</em></span> which will be used as
       the end of the range of IP addresses from which server addresses may be
       allocated. The default value is the last host address within the
       <span class="emphasis"><em>CIDR</em></span> (for example, the .254 address of a /24). This
       parameter is deprecated in favor of the new <code class="literal">addresses</code>
       parameter. This parameter may be removed in a future release.
      </p>
     </td></tr><tr><td>gateway-ip (optional)</td><td>
      The IP address of the gateway for this network. Gateway addresses must
      be specified if the associated
      <span class="guimenu ">network-group</span> provides routes.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-firewallrules"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall Rules</span> <a title="Permalink" class="permalink" href="#configobj-firewallrules">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-firewallrules.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-firewallrules.xml</li><li><span class="ds-label">ID: </span>configobj-firewallrules</li></ul></div></div></div></div><p>
  The configuration processor will automatically generate "allow" firewall
  rules for each server based on the services deployed and block all other
  ports. The firewall rules in the input model allow the customer to define
  additional rules for each network group.
 </p><p>
  Administrator-defined rules are applied after all rules generated by the
  Configuration Processor.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  firewall-rules:

     - name: PING
       network-groups:
       - MANAGEMENT
       - GUEST
       - EXTERNAL-API
       rules:
       # open ICMP echo request (ping)
       - type: allow
         remote-ip-prefix:  0.0.0.0/0
         # icmp type
         port-range-min: 8
         # icmp code
         port-range-max: 0
         protocol: icmp</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the group of rules.</td></tr><tr><td>network-groups</td><td>
      <p>
       A list of <span class="guimenu ">network-group</span> names that the rules apply
       to. A value of "all" matches all network-groups.
      </p>
     </td></tr><tr><td>rules</td><td>
      <p>
       A list of rules. Rules are applied in the order in which they appear
       in the list, apart from the control provided by the "final" option
       (see above). The order between sets of rules is indeterminate.
       
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="configobj-rule"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rule</span> <a title="Permalink" class="permalink" href="#configobj-rule">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-rule.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-rule.xml</li><li><span class="ds-label">ID: </span>configobj-rule</li></ul></div></div></div></div><p>
  Each rule in the list takes the following parameters (which match the
  parameters of a Neutron security group rule):
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>type</td><td>Must <code class="literal">allow</code>
     </td></tr><tr><td>remote-ip-prefix</td><td>
      Range of remote addresses in CIDR format that this rule applies
      to.
     </td></tr><tr><td>
      <p>
       port-range-min
      </p>
      <p>
       port-range-max
      </p>
     </td><td>
      Defines the range of ports covered by the rule. Note that if the
      protocol is <code class="literal">icmp</code> then port-range-min is the ICMP
      type and port-range-max is the ICMP code.
     </td></tr><tr><td>protocol</td><td>
      Must be one of <code class="literal">tcp</code>, <code class="literal">udp</code>, or
      <code class="literal">icmp</code>. </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-configurationdata"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-configurationdata.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata</li></ul></div></div></div></div><p>
  Configuration data allows values to be passed into the model to be used in
  the context of a specific control plane or cluster. The content and format of
  the data is service specific.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  NEUTRON-CONFIG-CP1
      services:
        - neutron
      data:
        neutron_provider_networks:
        - name: OCTAVIA-MGMT-NET
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 106
          cidr: 172.30.1.0/24
          no_gateway:  True
          enable_dhcp: True
          allocation_pools:
            - start: 172.30.1.10
              end: 172.30.1.250
          host_routes:
            # route to MANAGEMENT-NET-1
            - destination: 192.168.245.0/24
              nexthop:  172.30.1.1

        neutron_external_networks:
        - name: ext-net
          cidr: 172.31.0.0/24
          gateway: 172.31.0.1
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 107
          allocation_pools:
            - start: 172.31.0.2
              end: 172.31.0.254

      network-tags:
        - network-group: MANAGEMENT
          tags:
            - neutron.networks.vxlan
            - neutron.networks.vlan:
                provider-physical-network: physnet1
        - network-group: EXTERNAL-VM
          tags:
            - neutron.l3_agent.external_network_bridge</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the set of configuration data.</td></tr><tr><td>services</td><td>
      <p>
       A list of services that the data applies to. Note that these are
       service names (for example,
       <code class="literal">neutron</code>, <code class="literal">octavia</code>, etc.) not
       service-component names
       (<code class="literal">neutron-server</code>, <code class="literal">octavia-api</code>,
       etc.).
      </p>
     </td></tr><tr><td>data</td><td>A service specific data structure (see below). </td></tr><tr><td>network-tags (optional, Neutron-only)</td><td>
      <p>
       A list of network tags. Tags provide the linkage between the physical
       network configuration and the Neutron network configuration.
      </p>
      <p>
       Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, network tags may be defined as part of a
       Neutron <code class="literal">configuration-data</code> object rather than as part
       of a <code class="literal">network-group</code> object.
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="configobj-neutron-network-tags"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron network-tags</span> <a title="Permalink" class="permalink" href="#configobj-neutron-network-tags">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-neutron_network_tags.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-neutron_network_tags.xml</li><li><span class="ds-label">ID: </span>configobj-neutron-network-tags</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>network-group</td><td>The name of the network-group with which the tags are associated.</td></tr><tr><td>tags</td><td>A list of network tags. Tags provide the linkage between the physical
                        network configuration and the Neutron network configuration. See section
                        Network Tags. </td></tr></tbody></table></div></div><div class="sect2" id="configobj-configurationdata-neutron"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-neutron">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-configurationdata_neutron.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_neutron.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-neutron</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron-provider-networks</td><td>A list of provider networks that will be created in Neutron.</td></tr><tr><td>neutron-external-networks</td><td>
      A list of external networks that will be created in Neutron. These
      networks will have the “router:external” attribute set to True.
     </td></tr></tbody></table></div><div class="sect3" id="configobj-neutron-provider-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.16.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">neutron-provider-networks</span> <a title="Permalink" class="permalink" href="#configobj-neutron-provider-networks">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>configobj-neutron-provider-networks</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       The name for this network in Neutron.
      </p>
      <p>
       This name must be distinct from the names of any Network Groups in the
       model to enable it to be included in the “routes” value of a network
       group.
      </p>
     </td></tr><tr><td>provider</td><td>
      <p>
       Details of network to be created
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         network_type
        </p></li><li class="listitem "><p>
         physical_network
        </p></li><li class="listitem "><p>
         segmentation_id
        </p></li></ul></div>
      <p>
       These values are passed as <code class="literal">--provider:</code> options to the
       Neutron <code class="literal">net-create</code> command
      </p>
     </td></tr><tr><td>cidr</td><td>
      <p>
       The CIDR to use for the network. This is passed to the Neutron
       <code class="literal">subnet-create</code> command.
      </p>
     </td></tr><tr><td>shared (optional)</td><td>
      <p>
       A Boolean value that specifies if the network can be shared.
      </p>
      <p>
       This value is passed to the Neutron <code class="literal">net-create</code>
       command.
      </p>
     </td></tr><tr><td>allocation_pools (optional)</td><td>
      <p>
       A list of start and end address pairs that limit the set of IP addresses
       that can be allocated for this network.
      </p>
      <p>
       These values are passed to the Neutron <code class="literal">subnet-create</code>
       command.
      </p>
     </td></tr><tr><td>host_routes (optional)</td><td>
      <p>
       A list of routes to be defined for the network. Each route consists of a
       <code class="literal">destination</code> in cidr format and a
       <code class="literal">nexthop</code> address.
      </p>
      <p>
       These values are passed to the Neutron <code class="literal">subnet-create</code>
       command.
      </p>
     </td></tr><tr><td>gateway_ip (optional)</td><td>
      <p>
       A gateway address for the network.
      </p>
      <p>
       This value is passed to the Neutron <code class="literal">subnet-create</code>
       command.
      </p>
     </td></tr><tr><td>no_gateway (optional)</td><td>
      <p>
       A Boolean value indicating that the gateway should not be distributed on
       this network.
      </p>
      <p>
       This is translated into the <code class="literal">no-gateway</code> option to the
       Neutron <code class="literal">subnet-create</code> command
      </p>
     </td></tr><tr><td>enable_dhcp (optional)</td><td>
      <p>
       A Boolean value indicating that DHCP should be enabled. The default if
       not specified is to not enable DHCP.
      </p>
      <p>
       This value is passed to the Neutron <code class="literal">subnet-create</code>
       command.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="configobj-neutron-external-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.16.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">neutron-external-networks</span> <a title="Permalink" class="permalink" href="#configobj-neutron-external-networks">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>configobj-neutron-external-networks</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       The name for this network in Neutron.
      </p>
      <p>
       This name must be distinct from the names of any Network Groups in the
       model to enable it to be included in the “routes” value of a network
       group.
      </p>
     </td></tr><tr><td>provider (optional)</td><td>
      <p>
       The provider attributes are specified when using Neutron provider
       networks as external networks. Provider attributes should not be
       specified when the external network is configured with the
       <code class="literal">neutron.l3_agent.external_network_bridge</code>.
      </p>
      <p>
       Standard provider network attributes may be specified:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         network_type
        </p></li><li class="listitem "><p>
         physical_network
        </p></li><li class="listitem "><p>
         segmentation_id
        </p></li></ul></div>
      <p>
       These values are passed as <code class="literal">--provider:</code> options to the
       Neutron <code class="literal">net-create</code> command
      </p>
     </td></tr><tr><td>cidr</td><td>
      <p>
       The CIDR to use for the network. This is passed to the Neutron
       <code class="literal">subnet-create</code> command.
      </p>
     </td></tr><tr><td>allocation_pools (optional)</td><td>
      <p>
       A list of start and end address pairs that limit the set of IP addresses
       that can be allocated for this network.
      </p>
      <p>
       These values are passed to the Neutron <code class="literal">subnet-create</code>
       command.
      </p>
     </td></tr><tr><td>gateway (optional)</td><td>
      <p>
       A gateway address for the network.
      </p>
      <p>
       This value is passed to the Neutron <code class="literal">subnet-create</code>
       command.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect2" id="configobj-configurationdata-octavia"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Octavia Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-octavia">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-configurationdata_octavia.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_octavia.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-octavia</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name: OCTAVIA-CONFIG-CP1
      services:
        - octavia
      data:
        amp_network_name: OCTAVIA-MGMT-NET</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>amp_network_name</td><td>
      The name of the Neutron provider network that Octavia will use for
      management access to load balancers.
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-configurationdata-ironic"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-ironic">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-configurationdata_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_ironic.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-ironic</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  IRONIC-CONFIG-CP1
      services:
        - ironic
      data:
        cleaning_network: guest-network
        enable_node_cleaning: true
        enable_oneview: false

        oneview_manager_url:
        oneview_username:
        oneview_encrypted_password:
        oneview_allow_insecure_connections:
        tls_cacert_file:
        enable_agent_drivers: true</pre></div><p>
  Refer to the documentation on configuring Ironic for details of the above
  attributes.
 </p></div><div class="sect2" id="configobj-configurationdata-swift"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-swift">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-configurationdata_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_swift.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-swift</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
  - name: SWIFT-CONFIG-CP1
    services:
      - swift
    data:
      control_plane_rings:
        swift-zones:
          - id: 1
            server-groups:
              - AZ1
          - id: 2
            server-groups:
              - AZ2
          - id: 3
            server-groups:
              - AZ3
        rings:
          - name: account
            display-name: Account Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: container
            display-name: Container Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: object-0
            display-name: General
            default: yes
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3</pre></div><p>
  Refer to the documentation on <a class="xref" href="#ring-specification" title="11.10. Understanding Swift Ring Specifications">Section 11.10, “Understanding Swift Ring Specifications”</a> for
  details of the above attributes.
 </p></div></div><div class="sect1" id="passthrough"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pass Through</span> <a title="Permalink" class="permalink" href="#passthrough">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-configobj-passthrough.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-passthrough.xml</li><li><span class="ds-label">ID: </span>passthrough</li></ul></div></div></div></div><p>
  Through pass_through definitions, certain configuration values can be
  assigned and used.
 </p><div class="verbatim-wrap"><pre class="screen">product:
  version: 2

pass-through:
  global:
    esx_cloud: true
  servers:
      data:
        vmware:
          cert_check: false
          vcenter_cluster: Cluster1
          vcenter_id: BC9DED4E-1639-481D-B190-2B54A2BF5674
          vcenter_ip: 10.1.200.41
          vcenter_port: 443
          vcenter_username: administrator@vsphere.local
          id: 7d8c415b541ca9ecf9608b35b32261e6c0bf275a</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>global</td><td>These values will be used at the cloud level.</td></tr><tr><td>servers </td><td>
      These values will be assigned to a specific server(s) using the
      server-id.
     </td></tr></tbody></table></div></div></div><div class="chapter " id="othertopics"><div class="titlepage"><div><div><h2 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Topics</span> <a title="Permalink" class="permalink" href="#othertopics">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-othertopics.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-othertopics.xml</li><li><span class="ds-label">ID: </span>othertopics</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#services-components"><span class="number">7.1 </span><span class="name">Services and Service Components</span></a></span></dt><dt><span class="section"><a href="#namegeneration"><span class="number">7.2 </span><span class="name">Name Generation</span></a></span></dt><dt><span class="section"><a href="#persisteddata"><span class="number">7.3 </span><span class="name">Persisted Data</span></a></span></dt><dt><span class="section"><a href="#serverallocation"><span class="number">7.4 </span><span class="name">Server Allocation</span></a></span></dt><dt><span class="section"><a href="#servernetworkselection"><span class="number">7.5 </span><span class="name">Server Network Selection</span></a></span></dt><dt><span class="section"><a href="#networkroutevalidation"><span class="number">7.6 </span><span class="name">Network Route Validation</span></a></span></dt><dt><span class="section"><a href="#configneutronprovidervlans"><span class="number">7.7 </span><span class="name">Configuring Neutron Provider VLANs</span></a></span></dt><dt><span class="section"><a href="#standalonedeployer"><span class="number">7.8 </span><span class="name">Standalone Cloud Lifecycle Manager</span></a></span></dt></dl></div></div><div class="sect1" id="services-components"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Services and Service Components</span> <a title="Permalink" class="permalink" href="#services-components">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-services_components.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-services_components.xml</li><li><span class="ds-label">ID: </span>services-components</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Type</th><th>Service</th><th>Service Components</th></tr></thead><tbody><tr><td colspan="3"><span class="bold"><strong>Compute</strong></span></td></tr><tr><td>Virtual Machine Provisioning</td><td>nova</td><td>
<div class="verbatim-wrap"><pre class="screen">nova-api
nova-compute
nova-compute-hyperv
nova-compute-ironic
nova-compute-kvm
nova-conductor
nova-console-auth
nova-esx-compute-proxy
nova-metadata
nova-novncproxy
nova-scheduler
nova-scheduler-ironic
nova-placement-api</pre></div>
     </td></tr><tr><td>Bare Metal Provisioning</td><td>ironic</td><td>
<div class="verbatim-wrap"><pre class="screen">ironic-api
ironic-conductor</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Networking</strong></span></td></tr><tr><td>Networking</td><td>neutron</td><td>
<div class="verbatim-wrap"><pre class="screen">infoblox-ipam-agent
neutron-dhcp-agent
neutron-l2gateway-agent
neutron-l3-agent
neutron-lbaas-agent
neutron-lbaasv2-agent
neutron-metadata-agent
neutron-ml2-plugin
neutron-openvswitch-agent
neutron-ovsvapp-agent
neutron-server
neutron-sriov-nic-agent
neutron-vpn-agent</pre></div>
     </td></tr><tr><td>Network Load Balancer</td><td>octavia</td><td>
<div class="verbatim-wrap"><pre class="screen">octavia-api
octavia-health-manager</pre></div>
     </td></tr><tr><td>Domain Name Service (DNS)</td><td>designate</td><td>
<div class="verbatim-wrap"><pre class="screen">designate-api
designate-central
designate-mdns
designate-mdns-external
designate-pool-manager
designate-zone-manager</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Storage</strong></span></td></tr><tr><td>Block Storage</td><td>cinder</td><td>
<div class="verbatim-wrap"><pre class="screen">cinder-api
cinder-backup
cinder-scheduler
cinder-volume</pre></div>
     </td></tr><tr><td>Object Storage</td><td>swift</td><td>
<div class="verbatim-wrap"><pre class="screen">swift-account
swift-common
swift-container
swift-object
swift-proxy
swift-ring-builder
swift-rsync</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Image</strong></span></td></tr><tr><td>Image Management</td><td>glance</td><td>
<div class="verbatim-wrap"><pre class="screen">glance-api
glance-registry</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Security</strong></span></td></tr><tr><td>Key Management</td><td>barbican</td><td>
<div class="verbatim-wrap"><pre class="screen">barbican-api
barbican-worker</pre></div>
     </td></tr><tr><td>Identity and Authentication</td><td>keystone</td><td>
<div class="verbatim-wrap"><pre class="screen">keystone-api</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Orchestration</strong></span></td></tr><tr><td>Orchestration</td><td>heat</td><td>
<div class="verbatim-wrap"><pre class="screen">heat-api
heat-api-cfn
heat-api-cloudwatch
heat-engine</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Operations</strong></span></td></tr><tr><td>Telemetry</td><td>ceilometer</td><td>
<div class="verbatim-wrap"><pre class="screen">ceilometer-agent-notification
ceilometer-api
ceilometer-common
ceilometer-polling</pre></div>
     </td></tr><tr><td>Backup and Recovery</td><td>freezer</td><td>
<div class="verbatim-wrap"><pre class="screen">freezer-agent
freezer-api</pre></div>
     </td></tr><tr><td>Cloud Lifecycle Manager</td><td>ardana</td><td>
<div class="verbatim-wrap"><pre class="screen">ardana-ux-services
lifecycle-manager
lifecycle-manager-target</pre></div>
     </td></tr><tr><td>Dashboard</td><td>horizon</td><td>
<div class="verbatim-wrap"><pre class="screen">horizon</pre></div>
     </td></tr><tr><td>Centralized Logging</td><td>logging</td><td>
<div class="verbatim-wrap"><pre class="screen">logging-api
logging-producer
logging-rotate
logging-server</pre></div>
     </td></tr><tr><td>Monitoring</td><td>monasca</td><td>
<div class="verbatim-wrap"><pre class="screen">monasca-agent
monasca-api
monasca-dashboard
monasca-liveness-check
monasca-notifier
monasca-persister
monasca-threshold
monasca-transform</pre></div>
     </td></tr><tr><td>Operations Console</td><td>operations</td><td>
<div class="verbatim-wrap"><pre class="screen">ops-console-web</pre></div>
     </td></tr><tr><td>Openstack Functional Test Suite</td><td>tempest</td><td>
<div class="verbatim-wrap"><pre class="screen">tempest</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Foundation</strong></span></td></tr><tr><td>OpenStack Clients</td><td>clients</td><td>
<div class="verbatim-wrap"><pre class="screen">barbican-client
ceilometer-client
cinder-client
designate-client
glance-client
heat-client
ironic-client
keystone-client
monasca-client
neutron-client
nova-client
openstack-client
swift-client</pre></div>
     </td></tr><tr><td>Supporting Services</td><td>foundation</td><td>
<div class="verbatim-wrap"><pre class="screen">apache2
bind
bind-ext
influxdb
ip-cluster
kafka
memcached
mysql
ntp-client
ntp-server
openvswitch
powerdns
powerdns-ext
rabbitmq
spark
storm
cassandra
zookeeper</pre></div>
     </td></tr></tbody></table></div></div><div class="sect1" id="namegeneration"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Name Generation</span> <a title="Permalink" class="permalink" href="#namegeneration">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-namegeneration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-namegeneration.xml</li><li><span class="ds-label">ID: </span>namegeneration</li></ul></div></div></div></div><p>
  Names are generated by the configuration processor for all allocated IP
  addresses. A server connected to multiple networks will have multiple names
  associated with it. One of these may be assigned as the hostname for a server
  via the network-group configuration (see
  <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>). Names are generated from data taken
  from various parts of the input model as described in the following sections.
 </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.5.3.3"><span class="name">Clusters</span><a title="Permalink" class="permalink" href="#id-1.3.4.5.3.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-namegeneration.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
  Names generated for servers in a cluster have the following form:
 </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em>-<em class="replaceable ">CONTROL-PLANE</em>-<em class="replaceable ">CLUSTER</em><em class="replaceable ">MEMBER-PREFIX</em><em class="replaceable ">MEMBER_ID</em>-<em class="replaceable ">NETWORK</em></pre></div><p>
  Example: <code class="literal">ardana-cp1-core-m1-mgmt</code>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><em class="replaceable ">CLOUD</em></td><td>
      Comes from the hostname-data section of the
      <span class="guimenu ">cloud</span> object (see
      <a class="xref" href="#configobj-cloud" title="6.1. Cloud Configuration">Section 6.1, “Cloud Configuration”</a>)
     </td></tr><tr><td><em class="replaceable ">CONTROL-PLANE</em></td><td>
      is the <span class="guimenu ">control-plane</span> prefix or name (see
      <a class="xref" href="#configobj-controlplane" title="6.2. Control Plane">Section 6.2, “Control Plane”</a>)
     </td></tr><tr><td><em class="replaceable ">CLUSTER</em></td><td>
      is the <span class="guimenu ">cluster-prefix</span> name (see
      <a class="xref" href="#configobj-clusters" title="6.2.1.  Clusters">Section 6.2.1, “
  Clusters”</a>)
     </td></tr><tr><td><em class="replaceable ">member-prefix</em></td><td>
      comes from the hostname-data section of the
      <span class="guimenu ">cloud</span> object (see
      <a class="xref" href="#configobj-cloud" title="6.1. Cloud Configuration">Section 6.1, “Cloud Configuration”</a>)
     </td></tr><tr><td><em class="replaceable ">member_id</em></td><td>
      is the ordinal within the cluster, generated by the configuration
      processor as servers are allocated to the cluster</td></tr><tr><td><em class="replaceable ">network</em></td><td>
      comes from the <span class="guimenu ">hostname-suffix</span> of the network group
      to which the network belongs (see
      <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>).
     </td></tr></tbody></table></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.5.3.8"><span class="name">Resource Nodes</span><a title="Permalink" class="permalink" href="#id-1.3.4.5.3.8">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-namegeneration.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
  Names generated for servers in a resource group have the following form:
 </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em>-<em class="replaceable ">CONTROL-PLANE</em>-<em class="replaceable ">RESOURCE-PREFIX</em><em class="replaceable ">MEMBER_ID</em>-<em class="replaceable ">NETWORK</em></pre></div><p>
  Example: <code class="literal">ardana-cp1-comp0001-mgmt</code>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><em class="replaceable ">CLOUD</em></td><td>
      comes from the hostname-data section of the
      <span class="guimenu ">cloud</span> object (see
      <a class="xref" href="#configobj-cloud" title="6.1. Cloud Configuration">Section 6.1, “Cloud Configuration”</a>).
     </td></tr><tr><td><em class="replaceable ">CONTROL-PLANE</em></td><td>
      is the <span class="guimenu ">control-plane</span> prefix or name (see
      <a class="xref" href="#configobj-controlplane" title="6.2. Control Plane">Section 6.2, “Control Plane”</a>).
     </td></tr><tr><td><em class="replaceable ">RESOURCE-PREFIX</em></td><td>
      is the <span class="guimenu ">resource-prefix</span> value name (see
      <a class="xref" href="#configobj-resources" title="6.2.2. Resources">Section 6.2.2, “Resources”</a>).
     </td></tr><tr><td><em class="replaceable ">MEMBER_ID</em></td><td>
      is the ordinal within the cluster, generated by the configuration
      processor as servers are allocated to the cluster, padded with leading
      zeroes to four digits.
     </td></tr><tr><td><em class="replaceable ">NETWORK</em></td><td>
      comes from the <span class="guimenu ">hostname-suffix</span> of the network group
      to which the network belongs to (see
      <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>)
     </td></tr></tbody></table></div></div><div class="sect1" id="persisteddata"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persisted Data</span> <a title="Permalink" class="permalink" href="#persisteddata">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-persisteddata.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-persisteddata.xml</li><li><span class="ds-label">ID: </span>persisteddata</li></ul></div></div></div></div><p>
  The configuration processor makes allocation decisions on servers and IP
  addresses which it needs to remember between successive runs so that if new
  servers are added to the input model they do not disrupt the previously
  deployed allocations.
 </p><p>
  To allow users to make multiple iterations of the input model before
  deployment <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will only persist data when the administrator confirms
  that they are about to deploy the results via the "ready-deployment"
  operation. To understand this better, consider the following example:
 </p><p>
  Imagine you have completed your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment with servers A, B, and C
  and you want to add two new compute nodes by adding servers D and E to the
  input model.
 </p><p>
  When you add these to the input model and re-run the configuration processor
  it will read the persisted data for A, B, and C and allocate D and E as new
  servers. The configuration processor now has allocation data for A, B, C, D,
  and E -- which it keeps in a staging area (actually a special branch in Git)
  until we get confirmation that the configuration processor has done what you
  intended and you are ready to deploy the revised configuration.
 </p><p>
  If you notice that the role of E is wrong and it became a Swift node instead
  of a Nova node you need to be able to change the input model and re-run the
  configuration processor. This is fine because the allocations of D and E have
  not been confirmed, and so the configuration processor will re-read the data
  about A, B, C and re-allocate D and E now to the correct clusters, updating
  the persisted data in the staging area.
 </p><p>
  You can loop though this as many times as needed. Each time, the
  configuration processor is processing the deltas to what is deployed, not the
  results of the previous run. When you are ready to use the results of the
  configuration processor, you run <code class="literal">ready-deployment.yml</code>
  which commits the data in the staging area into the persisted data. The next
  run of the configuration processor will then start from the persisted data
  for A, B, C, D, and E.
 </p><div class="sect2" id="persistedserverallocations"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persisted Server Allocations</span> <a title="Permalink" class="permalink" href="#persistedserverallocations">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-persisteddata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-persisteddata.xml</li><li><span class="ds-label">ID: </span>persistedserverallocations</li></ul></div></div></div></div><p>
   Server allocations are persisted by the administrator-defined server ID (see
   <a class="xref" href="#configobj-servers" title="6.5. Servers">Section 6.5, “Servers”</a>), and include the
   control plane, cluster/resource name, and ordinal within the cluster or
   resource group.
  </p><p>
   To guard against data loss, the configuration processor persists server
   allocations even when the server ID no longer exists in the input model --
   for example, if a server was removed accidentally and the configuration
   processor allocated a new server to the same ordinal, then it would be very
   difficult to recover from that situation.
  </p><p>
   The following example illustrates the behavior:
  </p><p>
   A cloud is deployed with four servers with IDs of A, B, C, and D that can
   all be used in a resource group with <code class="literal">min-size=0</code> and
   <code class="literal">max-size=3</code>. At the end of this deployment they persisted
   state is as follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>Control Plane</th><th>Resource Group</th><th>Ordinal</th><th>State</th><th>Deployed As</th></tr></thead><tbody><tr><td>A</td><td>ccp</td><td>compute</td><td>1</td><td>Allocated</td><td>mycloud-ccp-comp0001</td></tr><tr><td>B</td><td>ccp</td><td>compute</td><td>2</td><td>Allocated</td><td>mycloud-ccp-comp0002</td></tr><tr><td>C</td><td>ccp</td><td>compute</td><td>3</td><td>Allocated</td><td>mycloud-ccp-comp0003</td></tr><tr><td>D</td><td> </td><td> </td><td> </td><td>Available</td><td> </td></tr></tbody></table></div><p>
   (In this example server D has not been allocated because the group is at its
   max size, and there are no other groups that required this server)
  </p><p>
   If server B is removed from the input model and the configuration processor
   is re-run, the state is changed to:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>Control Plane</th><th>Resource Group</th><th>Ordinal</th><th>State</th><th>Deployed As</th></tr></thead><tbody><tr><td>A</td><td>ccp</td><td>compute</td><td>1</td><td>Allocated</td><td>mycloud-ccp-comp0001</td></tr><tr><td>B</td><td>ccp</td><td>compute</td><td>2</td><td>Deleted</td><td> </td></tr><tr><td>C</td><td>ccp</td><td>compute</td><td>3</td><td>Allocated</td><td>mycloud-ccp-comp0003</td></tr><tr><td>D</td><td>ccp</td><td>compute</td><td>4</td><td>Allocated</td><td>mycloud-ccp-comp0004</td></tr></tbody></table></div><p>
   The details associated with server B are still retained, but the
   configuration processor will not generate any deployment data for this
   server. Server D has been added to the group to meet the minimum size
   requirement but has been given a different ordinal and hence will get
   different names and IP addresses than were given to server B.
  </p><p>
   If server B is added back into the input model the resulting state will be:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>Control Plane</th><th>Resource Group</th><th>Ordinal</th><th>State</th><th>Deployed As</th></tr></thead><tbody><tr><td>A</td><td>ccp</td><td>compute</td><td>1</td><td>Allocated</td><td>mycloud-ccp-comp0001</td></tr><tr><td>B</td><td>ccp</td><td>compute</td><td>2</td><td>Deleted</td><td> </td></tr><tr><td>C</td><td>ccp</td><td>compute</td><td>3</td><td>Allocated</td><td>mycloud-ccp-comp0003</td></tr><tr><td>D</td><td>ccp</td><td>compute</td><td>4</td><td>Allocated</td><td>mycloud-ccp-comp0004</td></tr></tbody></table></div><p>
   The configuration processor will issue a warning that server B cannot be
   returned to the compute group because it would exceed the max-size
   constraint. However, because the configuration processor knows that server B
   is associated with this group it will not allocate it to any other group that
   could use it, since that might lead to data loss on that server.
  </p><p>
   If the max-size value of the group was increased, then server B would be
   allocated back to the group, with its previous name and addresses
   (<code class="literal">mycloud-cp1-compute0002</code>).
  </p><p>
   Note that the configuration processor relies on the server ID to identify a
   physical server. If the ID value of a server is changed the configuration
   processor will treat it as a new server. Conversely, if a different physical
   server is added with the same ID as a deleted server the configuration
   processor will assume that it is the original server being returned to the
   model.
  </p><p>
   You can force the removal of persisted data for servers that are no longer
   in the input model by running the configuration processor with the
   <code class="literal">remove_deleted_servers</code> option, like below:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
-e remove_deleted_servers="y"</pre></div></div><div class="sect2" id="persistedaddressallocations"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persisted Address Allocations</span> <a title="Permalink" class="permalink" href="#persistedaddressallocations">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-persisteddata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-persisteddata.xml</li><li><span class="ds-label">ID: </span>persistedaddressallocations</li></ul></div></div></div></div><p>
   The configuration processor persists IP address allocations by the generated
   name (see <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a> for how names are generated). As
   with servers. once an address has been allocated that address will remain
   allocated until the configuration processor is explicitly told that it is no
   longer required. The configuration processor will generate warnings for
   addresses that are persisted but no longer used.
  </p><p>
   You can remove persisted address allocations that are no longer used in the
   input model by running the configuration processor with the
   <code class="literal">free_unused_addresses</code> option, like below:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
-e free_unused_addresses="y"</pre></div></div></div><div class="sect1" id="serverallocation"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Allocation</span> <a title="Permalink" class="permalink" href="#serverallocation">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-serverallocation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-serverallocation.xml</li><li><span class="ds-label">ID: </span>serverallocation</li></ul></div></div></div></div><p>
  The configuration processor allocates servers to a cluster or resource group
  in the following sequence:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Any <span class="guimenu ">servers</span> that are persisted with a state of
    "allocated" are first returned to the <span class="guimenu ">cluster</span> or
    <span class="guimenu ">resource group</span>. Such servers are always allocated even
    if this contradicts the cluster size, failure-zones, or list of server
    roles since it is assumed that these servers are actively deployed.
   </p></li><li class="listitem "><p>
    If the <span class="guimenu ">cluster</span> or <span class="guimenu ">resource group</span> is
    still below its minimum size, then any <span class="guimenu ">servers</span> that are
    persisted with a state of "deleted", but where the server is now listed in
    the input model (that is, the server was removed but is now back), are added
    to the group providing they meet the <span class="guimenu ">failure-zone</span> and
    <span class="guimenu ">server-role</span> criteria. If they do not meet the criteria
    then a warning is given and the <span class="guimenu ">server</span> remains in a
    deleted state (that is, it is still not allocated to any other cluster or
    group). These <span class="guimenu ">servers</span> are not part of the current
    deployment, and so you must resolve any conflicts before they can be
    redeployed.
   </p></li><li class="listitem "><p>
    If the <span class="guimenu ">cluster</span> or <span class="guimenu ">resource group</span> is
    still below its minimum size, the configuration processor will allocate
    additional <span class="guimenu ">servers</span> that meet the
    <span class="guimenu ">failure-zone</span> and <span class="guimenu ">server-role</span>
    criteria. If the allocation policy is set to "strict" then the failure
    zones of servers already in the cluster or resource group are not
    considered until an equal number of servers has been allocated from each
    zone.
   </p></li></ol></div></div><div class="sect1" id="servernetworkselection"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Network Selection</span> <a title="Permalink" class="permalink" href="#servernetworkselection">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-servernetworkselection.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-servernetworkselection.xml</li><li><span class="ds-label">ID: </span>servernetworkselection</li></ul></div></div></div></div><p>
  Once the configuration processor has allocated a <span class="guimenu ">server</span> to
  a <span class="guimenu ">cluster</span> or <span class="guimenu ">resource group</span> it uses the
  information in the associated <span class="guimenu ">interface-model</span> to determine
  which <span class="guimenu ">networks</span> need to be configured. It does this by:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Looking at the <span class="guimenu ">service-components</span> that are to run on the
    server (from the <span class="guimenu ">control-plane</span> definition)
   </p></li><li class="step "><p>
    Looking to see which <span class="guimenu ">network-group</span> each of those
    components is attached to (from the <span class="guimenu ">network-groups</span>
    definition)
   </p></li><li class="step "><p>
    Looking to see if there are any <span class="guimenu ">network-tags</span> related to
    a <span class="guimenu ">service-component</span> running on this server, and if so,
    adding those <span class="guimenu ">network-groups</span> to the list (also from the
    <span class="guimenu ">network-groups</span> definition)
   </p></li><li class="step "><p>
    Looking to see if there are any <span class="guimenu ">network-groups</span> that the
    <span class="guimenu ">interface-model</span> says should be forced onto the server
   </p></li><li class="step "><p>
    It then searches the <span class="guimenu ">server-group</span> hierarchy (as
    described in <a class="xref" href="#concept-servergroups-networks" title="5.2.9.2. Server Groups and Networks">Section 5.2.9.2, “Server Groups and Networks”</a>) to find a
    <span class="guimenu ">network</span> in each of the <span class="guimenu ">network-groups</span>
    it needs to attach to
   </p></li></ol></div></div><p>
  If there is no <span class="guimenu ">network</span> available to a server, either
  because the <span class="guimenu ">interface-model</span> does not include the required
  <span class="guimenu ">network-group</span>, or there is no <span class="guimenu ">network</span>
  from that group in the appropriate part of the
  <span class="guimenu ">server-groups</span> hierarchy, then the configuration processor
  will generate an error.
 </p><p>
  The configuration processor will also generate an error if the
  <span class="guimenu ">server</span> address does not match any of the networks it will
  be connected to.
 </p></div><div class="sect1" id="networkroutevalidation"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Route Validation</span> <a title="Permalink" class="permalink" href="#networkroutevalidation">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-networkroutevalidation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-networkroutevalidation.xml</li><li><span class="ds-label">ID: </span>networkroutevalidation</li></ul></div></div></div></div><p>
  Once the configuration processor has allocated all of the required
  <span class="guimenu ">servers</span> and matched them to the appropriate
  <span class="guimenu ">networks</span>, it validates that all
  <span class="guimenu ">service-components</span> have the required network routes to
  other <span class="guimenu ">service-components</span>.
 </p><p>
  It does this by using the data in the services section of the input model
  which provides details of which <span class="guimenu ">service-components</span> need to
  connect to each other. This data is not configurable by the administrator;
  however, it is provided as part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> release.
 </p><p>
  For each <span class="guimenu ">server</span>, the configuration processor looks at the
  list of <span class="guimenu ">service-components</span> it runs and determines the
  network addresses of every other <span class="guimenu ">service-component</span> it
  needs to connect to (depending on the service, this might be a virtual IP
  address on a load balancer or a set of addresses for the service).
 </p><p>
  If the target address is on a <span class="guimenu ">network</span> that this
  <span class="guimenu ">server</span> is connected to, then there is no routing required.
  If the target address is on a different <span class="guimenu ">network</span>, then the
  Configuration Processor looks at each <span class="guimenu ">network</span> the server
  is connected to and looks at the routes defined in the corresponding
  <span class="guimenu ">network-group</span>. If the <span class="guimenu ">network-group</span>
  provides a route to the <span class="guimenu ">network-group</span> of the target
  address, then that route is considered valid.
 </p><p>
  <span class="guimenu ">Networks</span> within the same <span class="guimenu ">network-group</span>
  are always considered as routed to each other; <span class="guimenu ">networks</span>
  from different <span class="guimenu ">network-groups</span> must have an explicit entry
  in the <code class="literal">routes</code> stanza of the
  <span class="guimenu ">network-group</span> definition. Routes to a named
  <span class="guimenu ">network-group</span> are always considered before a "default"
  route.
 </p><p>
  A warning is given for any routes which are using the "default" route since
  it is possible that the user did not intend to route this traffic. Such
  warning can be removed by adding the appropriate
  <span class="guimenu ">network-group</span> to the list of routes.
 </p><p>
  The configuration processor provides details of all routes between networks
  that it is expecting to be configured in the
  <code class="literal">info/route_info.yml</code> file.
 </p><p>
  To illustrate how network routing is defined in the input model, consider the
  following example:
 </p><p>
  A compute server is configured to run nova-compute which requires access to
  the Neutron API servers and a block storage service. The Neutron API
  servers have a virtual IP address provided by a load balancer in the
  INTERNAL-API network-group and the storage service is connected to the ISCSI
  network-group. Nova-compute itself is part of the set of components attached
  by default to the MANAGEMENT network-group. The intention is to have virtual
  machines on the compute server connect to the block storage via the ISCSI
  network.
 </p><p>
  The physical network is shown below:
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_networkroutevalidation.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_networkroutevalidation.png" width="" /></a></div></div><p>
  The corresponding entries in the <span class="guimenu ">network-groups</span> are:
 </p><div class="verbatim-wrap"><pre class="screen">  - name: INTERNAL-API
    hostname-suffix: intapi

    load-balancers:
       - provider: ip-cluster
         name: lb
         components:
           - default
         roles:
           - internal
           - admin

       - name: MANAGEMENT
         hostname-suffix: mgmt
         hostname: true

         component-endpoints:
           - default

         routes:
           - INTERNAL-API
           - default

       - name: ISCSI
         hostname-suffix: iscsi

         component-endpoints:
            - storage service</pre></div><p>
  And the <span class="guimenu ">interface-model</span> for the compute server looks like
  this:
 </p><div class="verbatim-wrap"><pre class="screen">  - name: INTERFACE_SET_COMPUTE
    network-interfaces:
      - name: BOND0
        device:
           name: bond0
        bond-data:
           options:
              mode: active-backup
              miimon: 200
              primary: hed5
           provider: linux
           devices:
              - name: hed4
              - name: hed5
        network-groups:
          - MANAGEMENT
          - ISCSI</pre></div><p>
  When validating the route from nova-compute to the Neutron API, the
  configuration processor will detect that the target address is on a network
  in the INTERNAL-API network group, and that the MANAGEMENT network (which is
  connected to the compute server) provides a route to this network, and thus
  considers this route valid.
 </p><p>
  When validating the route from nova-compute to a storage service, the
  configuration processor will detect that the target address is on a network
  in the ISCSInetwork group. However, because there is no service component on
  the compute server connected to the ISCSI network (according to the
  network-group definition) the ISCSI network will not have been configured on
  the compute server (see <a class="xref" href="#servernetworkselection" title="7.5. Server Network Selection">Section 7.5, “Server Network Selection”</a>. The
  configuration processor will detect that the MANAGEMENT network-group provides
  a "default" route and thus considers the route as valid (it is, of course,
  valid to route ISCSI traffic). However, because this is using the default
  route, a warning will be issued:
 </p><div class="verbatim-wrap"><pre class="screen">#   route-generator-2.0       WRN: Default routing used between networks
The following networks are using a 'default' route rule. To remove this warning
either add an explicit route in the source network group or force the network to
attach in the interface model used by the servers.
  MANAGEMENT-NET-RACK1 to ISCSI-NET
    ardana-ccp-comp0001
  MANAGEMENT-NET-RACK 2 to ISCSI-NET
    ardana-ccp-comp0002
  MANAGEMENT-NET-RACK 3 to SCSI-NET
    ardana-ccp-comp0003</pre></div><p>
  To remove this warning, you can either add ISCSI to the list of routes in the
  MANAGEMENT network group (routed ISCSI traffic is still a valid
  configuration) or force the compute server to attach to the ISCSI
  network-group by adding it as a forced-network-group in the interface-model,
  like this:
 </p><div class="verbatim-wrap"><pre class="screen">  - name: INTERFACE_SET_COMPUTE
      network-interfaces:
        - name: BOND0
          device:
            name: bond0
          bond-data:
            options:
               mode: active-backup
               miimon: 200
               primary: hed5
            provider: linux
            devices:
               - name: hed4
               - name: hed5
          network-groups:
             - MANAGEMENT
          forced-network-groups:
             - ISCSI</pre></div><p>
  With the attachment to the ISCSI network group forced, the configuration
  processor will attach the compute server to a network in that group and
  validate the route as either being direct or between networks in the same
  network-group.
 </p><p>
  The generated <code class="literal">route_info.yml</code> file will include entries
  such as the following, showing the routes that are still expected to be
  configured between networks in the MANAGEMENT network group and the
  INTERNAL-API network group.
 </p><div class="verbatim-wrap"><pre class="screen">  MANAGEMENT-NET-RACK1:
     INTERNAL-API-NET:
        default: false
        used_by:
           nova-compute:
              neutron-server:
              - ardana-ccp-comp0001
   MANAGEMENT-NET-RACK2:
     INTERNAL-API-NET:
        default: false
        used_by:
          nova-compute:
            neutron-server:
            - ardana-ccp-comp0003</pre></div></div><div class="sect1" id="configneutronprovidervlans"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Neutron Provider VLANs</span> <a title="Permalink" class="permalink" href="#configneutronprovidervlans">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-configneutronprovidervlans.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-configneutronprovidervlans.xml</li><li><span class="ds-label">ID: </span>configneutronprovidervlans</li></ul></div></div></div></div><p>
  Neutron provider VLANs are networks that map directly to an 802.1Q VLAN in
  the cloud provider’s physical network infrastructure. There are four
  aspects to a provider VLAN configuration:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Network infrastructure configuration (for example, the top-of-rack switch)
   </p></li><li class="listitem "><p>
    Server networking configuration (for compute nodes and Neutron network
    nodes)
   </p></li><li class="listitem "><p>
    Neutron configuration file settings
   </p></li><li class="listitem "><p>
    Creation of the corresponding network objects in Neutron
   </p></li></ul></div><p>
  The physical network infrastructure must be configured to convey the provider
  VLAN traffic as tagged VLANs to the cloud compute nodes and Neutron network
  nodes. Configuration of the physical network infrastructure is outside the
  scope of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> software.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> automates the server networking configuration and the Neutron
  configuration based on information in the cloud definition. To configure the
  system for provider VLANs, specify the
  <code class="literal">neutron.networks.vlan</code> tag with a
  <code class="literal">provider-physical-network</code> attribute on one or more
  <span class="guimenu ">network-groups</span> as described in
  <a class="xref" href="#configobj-networktags" title="6.13.2. Network Tags">Section 6.13.2, “Network Tags”</a>. For example (some
  attributes omitted for brevity):
 </p><div class="verbatim-wrap"><pre class="screen">  network-groups:

    - name: NET_GROUP_A
      tags:
        - neutron.networks.vlan:
              provider-physical-network: physnet1

    - name: NET_GROUP_B
      tags:
        - neutron.networks.vlan:
              provider-physical-network: physnet2</pre></div><p>
  A <span class="guimenu ">network-group</span> is associated with a server network
  interface via an <span class="guimenu ">interface-model</span> as described in
  <a class="xref" href="#configobj-interfacemodels" title="6.11. Interface Models">Section 6.11, “Interface Models”</a>. For example (some
  attributes omitted for brevity):
 </p><div class="verbatim-wrap"><pre class="screen">  interface-models:
     - name: INTERFACE_SET_X
       network-interfaces:
        - device:
              name: bond0
          network-groups:
            - NET_GROUP_A
        - device:
              name: hed3
          network-groups:
            - NET_GROUP_B</pre></div><p>
  A <span class="guimenu ">network-group</span> used for provider VLANs may contain only a
  single <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="guimenu ">network</span>, because that VLAN must span all
  compute nodes and any Neutron network nodes/controllers (that is, it is a single
  L2 segment). The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="guimenu ">network</span> must be defined with
  <code class="literal">tagged-vlan: false</code>, otherwise a Linux VLAN network
  interface will be created. For example:
 </p><div class="verbatim-wrap"><pre class="screen">  networks:
     - name: NET_A
       tagged-vlan: false
       network-group: NET_GROUP_A
     - name: NET_B
       tagged-vlan: false
       network-group: NET_GROUP_B</pre></div><p>
  When the cloud is deployed, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> will create the appropriate
  bridges on the servers, and set the appropriate attributes in the Neutron
  configuration files (for example, bridge_mappings).
 </p><p>
  After the cloud has been deployed, create Neutron network objects for each
  provider VLAN using the Neutron CLI:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo neutron net-create --provider:network_type vlan \
--provider:physical_network <em class="replaceable ">PHYSNET1</em> --provider:segmentation_id <em class="replaceable ">101</em> <em class="replaceable ">MYNET101</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo neutron net-create --provider:network_type vlan \
--provider:physical_network <em class="replaceable ">PHYSNET2</em> --provider:segmentation_id <em class="replaceable ">234</em> <em class="replaceable ">MYNET234</em></pre></div></div><div class="sect1" id="standalonedeployer"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Standalone Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#standalonedeployer">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-other_topics-standalonedeployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-standalonedeployer.xml</li><li><span class="ds-label">ID: </span>standalonedeployer</li></ul></div></div></div></div><p>
  All the example configurations use a <span class="quote">“<span class="quote ">deployer-in-the-cloud</span>”</span>
  scenario where the first controller is also the deployer/Cloud Lifecycle Manager. If you want
  to use a standalone Cloud Lifecycle Manager, you need to add the relevant details in
  <code class="literal">control_plane.yml</code>, <code class="literal">servers.yml</code> and
  related configuration files. Detailed instructions are available at <a class="xref" href="#standalone-deployer" title="12.1. Using a Dedicated Cloud Lifecycle Manager Node">Section 12.1, “Using a Dedicated Cloud Lifecycle Manager Node”</a>.
 </p></div></div><div class="chapter " id="cpinfofiles"><div class="titlepage"><div><div><h2 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Processor Information Files</span> <a title="Permalink" class="permalink" href="#cpinfofiles">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-cpinfofiles.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-cpinfofiles.xml</li><li><span class="ds-label">ID: </span>cpinfofiles</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#address-info-yml"><span class="number">8.1 </span><span class="name">address_info.yml</span></a></span></dt><dt><span class="section"><a href="#firewall-info-yml"><span class="number">8.2 </span><span class="name">firewall_info.yml</span></a></span></dt><dt><span class="section"><a href="#route-info-yml"><span class="number">8.3 </span><span class="name">route_info.yml</span></a></span></dt><dt><span class="section"><a href="#server-info-yml"><span class="number">8.4 </span><span class="name">server_info.yml</span></a></span></dt><dt><span class="section"><a href="#service-info-yml"><span class="number">8.5 </span><span class="name">service_info.yml</span></a></span></dt><dt><span class="section"><a href="#control-plane-topology-yml"><span class="number">8.6 </span><span class="name">control_plane_topology.yml</span></a></span></dt><dt><span class="section"><a href="#network-topology-yml"><span class="number">8.7 </span><span class="name">network_topology.yml</span></a></span></dt><dt><span class="section"><a href="#region-topology-yml"><span class="number">8.8 </span><span class="name">region_topology.yml</span></a></span></dt><dt><span class="section"><a href="#service-topology-yml"><span class="number">8.9 </span><span class="name">service_topology.yml</span></a></span></dt><dt><span class="section"><a href="#private-data-metadata-ccp-yml"><span class="number">8.10 </span><span class="name">private_data_metadata_ccp.yml</span></a></span></dt><dt><span class="section"><a href="#password-change-yml"><span class="number">8.11 </span><span class="name">password_change.yml</span></a></span></dt><dt><span class="section"><a href="#explain-txt"><span class="number">8.12 </span><span class="name">explain.txt</span></a></span></dt><dt><span class="section"><a href="#clouddiagram-txt"><span class="number">8.13 </span><span class="name">CloudDiagram.txt</span></a></span></dt><dt><span class="section"><a href="#html-representation"><span class="number">8.14 </span><span class="name">HTML Representation</span></a></span></dt></dl></div></div><p>
  In addition to producing all of the data needed to deploy and configure the
  cloud, the configuration processor also creates a number of information files
  that provide details of the resulting configuration.
 </p><p>
  These files can be found in <code class="filename">~/openstack/my_cloud/info</code>
  after the first configuration processor run. This directory is also rebuilt
  each time the Configuration Processor is run.
 </p><p>
  Most of the files are in YAML format, allowing them to be used in further
  automation tasks if required.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>File</th><th>Provides details of</th></tr></thead><tbody><tr><td><code class="filename">address_info.yml</code>
     </td><td>
      IP address assignments on each network. See
      <a class="xref" href="#address-info-yml" title="8.1. address_info.yml">Section 8.1, “address_info.yml”</a>
     </td></tr><tr><td><code class="filename">firewall_info.yml</code>
     </td><td>
      All ports that are open on each network by the firewall configuration.
      Can be used if you want to configure an additional firewall in front of
      the API network, for example. See <a class="xref" href="#firewall-info-yml" title="8.2. firewall_info.yml">Section 8.2, “firewall_info.yml”</a>
     </td></tr><tr><td><code class="filename">route_info.yml</code>
     </td><td>
      Routes that need to be configured between networks. See
      <a class="xref" href="#route-info-yml" title="8.3. route_info.yml">Section 8.3, “route_info.yml”</a>
     </td></tr><tr><td><code class="filename">server_info.yml</code>
     </td><td>
      How servers have been allocated, including their network configuration.
      Allows details of a server to be found from its ID. See
      <a class="xref" href="#server-info-yml" title="8.4. server_info.yml">Section 8.4, “server_info.yml”</a>
     </td></tr><tr><td><code class="filename">service_info.yml</code>
     </td><td>
      Details of where components of each service are deployed. See
      <a class="xref" href="#service-info-yml" title="8.5. service_info.yml">Section 8.5, “service_info.yml”</a>
     </td></tr><tr><td><code class="filename">control_plane_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each
      control-plane. See <a class="xref" href="#control-plane-topology-yml" title="8.6. control_plane_topology.yml">Section 8.6, “control_plane_topology.yml”</a>
     </td></tr><tr><td><code class="filename">network_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each
      control-plane. See <a class="xref" href="#network-topology-yml" title="8.7. network_topology.yml">Section 8.7, “network_topology.yml”</a>
     </td></tr><tr><td><code class="filename">region_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each region.
      See <a class="xref" href="#region-topology-yml" title="8.8. region_topology.yml">Section 8.8, “region_topology.yml”</a>
     </td></tr><tr><td><code class="filename">service_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each
      service. See <a class="xref" href="#service-topology-yml" title="8.9. service_topology.yml">Section 8.9, “service_topology.yml”</a>
     </td></tr><tr><td><code class="filename">private_data_metadata_ccp.yml</code>
     </td><td>
      Details the secrets that are generated by the configuration processor –
      the names of the secrets, along with the service(s) that use each
      secret and a list of the clusters on which the service that consumes
      the secret is deployed. See <a class="xref" href="#private-data-metadata-ccp-yml" title="8.10. private_data_metadata_ccp.yml">Section 8.10, “private_data_metadata_ccp.yml”</a>
     </td></tr><tr><td><code class="filename">password_change.yml</code>
     </td><td>
      Details the secrets that have been changed by the configuration
      processor – information for each secret is the same as for
      <code class="literal">private_data_metadata_ccp.yml</code>. See
      <a class="xref" href="#password-change-yml" title="8.11. password_change.yml">Section 8.11, “password_change.yml”</a>
     </td></tr><tr><td><code class="filename">explain.txt</code>
     </td><td>
      An explanation of the decisions the configuration processor has made
      when allocating servers and networks. See <a class="xref" href="#explain-txt" title="8.12. explain.txt">Section 8.12, “explain.txt”</a>
     </td></tr><tr><td><code class="filename">CloudDiagram.txt</code>
     </td><td>A pictorial representation of the cloud. See <a class="xref" href="#clouddiagram-txt" title="8.13. CloudDiagram.txt">Section 8.13, “CloudDiagram.txt”</a>
     </td></tr></tbody></table></div><p>
  The examples are taken from the <code class="literal">entry-scale-kvm</code>
  example configuration.
 </p><div class="sect1" id="address-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">address_info.yml</span> <a title="Permalink" class="permalink" href="#address-info-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-address_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-address_info_yml.xml</li><li><span class="ds-label">ID: </span>address-info-yml</li></ul></div></div></div></div><p>
  This file provides details of all the IP addresses allocated by the
  Configuration Processor:
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">NETWORK GROUPS</em>
     <em class="replaceable ">LIST OF NETWORKS</em>
        <em class="replaceable ">IP ADDRESS</em>
           <em class="replaceable ">LIST OF ALIASES</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">  EXTERNAL-API:
     EXTERNAL-API-NET:
        10.0.1.2:
           - ardana-cp1-c1-m1-extapi
        10.0.1.3:
           - ardana-cp1-c1-m2-extapi
        10.0.1.4:
           - ardana-cp1-c1-m3-extapi
        10.0.1.5:
           - ardana-cp1-vip-public-SWF-PRX-extapi
           - ardana-cp1-vip-public-FRE-API-extapi
           - ardana-cp1-vip-public-GLA-API-extapi
           - ardana-cp1-vip-public-HEA-ACW-extapi
           - ardana-cp1-vip-public-HEA-ACF-extapi
           - ardana-cp1-vip-public-NEU-SVR-extapi
           - ardana-cp1-vip-public-KEY-API-extapi
           - ardana-cp1-vip-public-MON-API-extapi
           - ardana-cp1-vip-public-HEA-API-extapi
           - ardana-cp1-vip-public-NOV-API-extapi
           - ardana-cp1-vip-public-CND-API-extapi
           - ardana-cp1-vip-public-CEI-API-extapi
           - ardana-cp1-vip-public-SHP-API-extapi
           - ardana-cp1-vip-public-OPS-WEB-extapi
           - ardana-cp1-vip-public-HZN-WEB-extapi
           - ardana-cp1-vip-public-NOV-VNC-extapi
  EXTERNAL-VM:
     EXTERNAL-VM-NET: {}
  GUEST:
     GUEST-NET:
        10.1.1.2:
           - ardana-cp1-c1-m1-guest
        10.1.1.3:
           - ardana-cp1-c1-m2-guest
        10.1.1.4:
           - ardana-cp1-c1-m3-guest
        10.1.1.5:
           - ardana-cp1-comp0001-guest
  MANAGEMENT:
  ...</pre></div></div><div class="sect1" id="firewall-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">firewall_info.yml</span> <a title="Permalink" class="permalink" href="#firewall-info-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-firewall_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-firewall_info_yml.xml</li><li><span class="ds-label">ID: </span>firewall-info-yml</li></ul></div></div></div></div><p>
  This file provides details of all the network ports that will be opened on
  the deployed cloud. Data is ordered by network. If you want to configure an
  external firewall in front of the External API network, then you would need
  to open the ports listed in that section.
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">NETWORK NAME</em>
     List of:
        <em class="replaceable ">PORT</em>
        <em class="replaceable ">PROTOCOL</em>
        <em class="replaceable ">LIST OF IP ADDRESSES</em>
        <em class="replaceable ">LIST OF COMPONENTS</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">  EXTERNAL-API:
  -   addresses:
      - 10.0.1.5
      components:
      - horizon
      port: '443'
      protocol: tcp
  -   addresses:
      - 10.0.1.5
      components:
      - keystone-api
      port: '5000'
      protocol: tcp</pre></div><p>
  <span class="emphasis"><em>Port 443 (tcp) is open on network EXTERNAL-API for address 10.0.1.5
  because it is used by Horizon</em></span>
 </p><p>
  <span class="emphasis"><em>Port 5000 (tcp) is open on network EXTERNAL-API for address
  10.0.1.5 because it is used by Keystone API</em></span>
 </p></div><div class="sect1" id="route-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">route_info.yml</span> <a title="Permalink" class="permalink" href="#route-info-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-route_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-route_info_yml.xml</li><li><span class="ds-label">ID: </span>route-info-yml</li></ul></div></div></div></div><p>
  This file provides details of routes between networks that need to be
  configured. Available routes are defined in the input model as part of the
  <span class="guimenu ">network-groups</span> data; this file shows which routes will
  actually be used. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will reconfigure routing rules on the servers, you
  must configure the corresponding routes within your physical network. Routes
  must be configured to be symmetrical -- only the direction in which a
  connection is initiated is captured in this file.
 </p><p>
  Note that simple models may not require any routes, with all servers being
  attached to common L3 networks. The following example is taken from the
  <code class="literal">tech-preview/mid-scale-kvm</code> example.
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">SOURCE-NETWORK-NAME</em>
      <em class="replaceable ">TARGET-NETWORK-NAME</em>
           default:   <em class="replaceable ">TRUE IF THIS IS THIS THE RESULT OF A "DEFAULT" ROUTE RULE</em>
           used_by:
                <em class="replaceable ">SOURCE-SERVICE</em>
                     <em class="replaceable ">TARGET-SERVICE</em>
                     <em class="replaceable ">LIST OF HOSTS USING THIS ROUTE</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">MANAGEMENT-NET-RACK1:
    INTERNAL-API-NET:
         default: false
         used_by:
            ceilometer-client:
            ceilometer-api:
            - ardana-cp1-mtrmon-m1
            keystone-api:
            - ardana-cp1-mtrmon-m1
     MANAGEMENT-NET-RACK2:
         default: false
         used_by:
            cinder-backup:
            rabbitmq:
            - ardana-cp1-core-m1</pre></div><p>
  A route is required from network
  <span class="bold"><strong>MANAGEMENT-NET-RACK1</strong></span> to network
  <span class="bold"><strong>INTERNAL-API-NET</strong></span> so that
  <span class="bold"><strong>ceilometer-client</strong></span> can connect to
  <span class="bold"><strong>ceilometer-api</strong></span> from server
  <span class="bold"><strong>ardana-cp1-mtrmon-m1</strong></span> and to
  <span class="bold"><strong>keystone-api</strong></span> from the same server.
 </p><p>
  A route is required from network
  <span class="bold"><strong>MANAGEMENT-NET-RACK1</strong></span> to network
  <span class="bold"><strong>MANAGEMENT-NET-RACK2</strong></span> so that
  <span class="bold"><strong>cinder-backup</strong></span> can connect to
  <span class="bold"><strong>rabbitmq</strong></span> from server
  <span class="bold"><strong>ardana-cp1-core-m1</strong></span>
 </p></div><div class="sect1" id="server-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">server_info.yml</span> <a title="Permalink" class="permalink" href="#server-info-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-server_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-server_info_yml.xml</li><li><span class="ds-label">ID: </span>server-info-yml</li></ul></div></div></div></div><p>
  This file provides details of how servers have been allocated by the
  Configuration Processor. This provides the easiest way to find where a
  specific physical server (identified by <code class="literal">server-id</code>) is
  being used.
 </p><div class="verbatim-wrap"><pre class="screen">   <em class="replaceable ">SERVER-ID</em>
         failure-zone: <em class="replaceable ">FAILURE ZONE THAT THE SERVER WAS ALLOCATED FROM</em>
         hostname: <em class="replaceable ">HOSTNAME OF THE SERVER</em>
         net_data: <em class="replaceable ">NETWORK CONFIGURATION</em>
         state: <em class="replaceable "> "allocated" | "available" </em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">   controller1:
         failure-zone: AZ1
         hostname: ardana-cp1-c1-m1-mgmt
         net_data:
              BOND0:
                   EXTERNAL-API-NET:
                       addr: 10.0.1.2
                       tagged-vlan: true
                       vlan-id: 101
                   EXTERNAL-VM-NET:
                       addr: null
                       tagged-vlan: true
                       vlan-id: 102
                   GUEST-NET:
                       addr: 10.1.1.2
                       tagged-vlan: true
                       vlan-id: 103
                   MANAGEMENT-NET:
                       addr: 192.168.10.3
                       tagged-vlan: false
                       vlan-id: 100
         state: allocated</pre></div></div><div class="sect1" id="service-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">service_info.yml</span> <a title="Permalink" class="permalink" href="#service-info-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-service_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-service_info_yml.xml</li><li><span class="ds-label">ID: </span>service-info-yml</li></ul></div></div></div></div><p>
  This file provides details of how services are distributed across the cloud.
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">CONTROL-PLANE</em>
      <em class="replaceable ">SERVICE</em>
          <em class="replaceable ">SERVICE COMPONENT</em>
               <em class="replaceable ">LIST OF HOSTS</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">  control-plane-1:
        neutron:
             neutron-client:
                - ardana-cp1-c1-m1-mgmt
                - ardana-cp1-c1-m2-mgmt
                - ardana-cp1-c1-m3-mgmt
             neutron-dhcp-agent:
                - ardana-cp1-c1-m1-mgmt
                - ardana-cp1-c1-m2-mgmt
                - ardana-cp1-c1-m3-mgmt
             neutron-l3-agent:
                 - ardana-cp1-comp0001-mgmt
             neutron-lbaasv2-agent:
                 - ardana-cp1-comp0001-mgmt
        ...</pre></div></div><div class="sect1" id="control-plane-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">control_plane_topology.yml</span> <a title="Permalink" class="permalink" href="#control-plane-topology-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-control_plane_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-control_plane_topology_yml.xml</li><li><span class="ds-label">ID: </span>control-plane-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each control plane:
 </p><div class="verbatim-wrap"><pre class="screen">control_planes:
  <em class="replaceable ">CONTROL-PLANE-NAME</em>
      load-balancers:
         <em class="replaceable ">LOAD-BALANCER-NAME</em>:
             address:  <em class="replaceable ">IP ADDRESS OF VIP</em>
             cert-file:  <em class="replaceable ">NAME OF CERT FILE</em>
             external-name: <em class="replaceable ">NAME TO USED FOR ENDPOINTS</em>
             network: <em class="replaceable ">NAME OF THE NETWORK THIS LB IS CONNECTED TO</em>
             network_group: <em class="replaceable ">NAME OF THE NETWORK GROUP THIS LB IS CONNECT TO</em>
             provider: <em class="replaceable ">SERVICE COMPONENT PROVIDING THE LB</em>
             roles:  <em class="replaceable ">LIST OF ROLES OF THIS LB</em>
             services:
                <em class="replaceable ">SERVICE-NAME</em>:
                    <em class="replaceable ">COMPONENT-NAME</em>:
                        aliases:
                           <em class="replaceable ">ROLE</em>:  <em class="replaceable ">NAME IN /etc/hosts</em>
                        host-tls:  <em class="replaceable ">BOOLEAN, TRUE IF CONNECTION FROM LB USES TLS</em>
                        hosts:  <em class="replaceable ">LIST OF HOSTS FOR THIS SERVICE</em>
                        port:  <em class="replaceable ">PORT USED FOR THIS COMPONENT</em>
                        vip-tls: <em class="replaceable ">BOOLEAN, TRUE IF THE VIP TERMINATES TLS</em>
      clusters:
          <em class="replaceable ">CLUSTER-NAME</em>
              failure-zones:
                 <em class="replaceable ">FAILURE-ZONE-NAME</em>:
                    <em class="replaceable ">LIST OF HOSTS</em>
              services:
                 <em class="replaceable ">SERVICE NAME</em>:
                     components:
                        <em class="replaceable ">LIST OF SERVICE COMPONENTS</em>
                     regions:
                        <em class="replaceable ">LIST OF REGION NAMES</em>
      resources:
         <em class="replaceable ">RESOURCE-NAME</em>:
             <em class="replaceable ">AS FOR CLUSTERS ABOVE</em></pre></div><p>
  <span class="bold"><strong>Example:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">    control_planes:
    control-plane-1:
        clusters:
            cluster1:
                failure_zones:
                    AZ1:
                    - ardana-cp1-c1-m1-mgmt
                    AZ2:
                    - ardana-cp1-c1-m2-mgmt
                    AZ3:
                    - ardana-cp1-c1-m3-mgmt
                services:
                    barbican:
                        components:
                        - barbican-api
                        - barbican-worker
                        regions:
                        - region1
                                               …
        load-balancers:
            extlb:
                address: 10.0.1.5
                cert-file: my-public-entry-scale-kvm-cert
                external-name: ''
                network: EXTERNAL-API-NET
                network-group: EXTERNAL-API
                provider: ip-cluster
                roles:
                - public
                services:
                    barbican:
                        barbican-api:
                            aliases:
                                public: ardana-cp1-vip-public-KEYMGR-API-extapi
                            host-tls: true
                            hosts:
                            - ardana-cp1-c1-m1-mgmt
                            - ardana-cp1-c1-m2-mgmt
                            - ardana-cp1-c1-m3-mgmt
                            port: '9311'
                            vip-tls: true</pre></div></div><div class="sect1" id="network-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">network_topology.yml</span> <a title="Permalink" class="permalink" href="#network-topology-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-network_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-network_topology_yml.xml</li><li><span class="ds-label">ID: </span>network-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each network_group:
 </p><div class="verbatim-wrap"><pre class="screen">network-groups:
  <em class="replaceable ">NETWORK-GROUP-NAME</em>:
      <em class="replaceable ">NETWORK-NAME</em>:
          control-planes:
              <em class="replaceable ">CONTROL-PLANE-NAME</em>:
                  clusters:
                     <em class="replaceable ">CLUSTER-NAME</em>:
                         servers:
                            <em class="replaceable ">ARDANA-SERVER-NAME</em>: <em class="replaceable ">ip address</em>
                         vips:
                            <em class="replaceable ">IP ADDRESS</em>: <em class="replaceable ">load balancer name</em>
                  resources:
                     <em class="replaceable ">RESOURCE-GROUP-NAME</em>:
                         servers:
                            <em class="replaceable ">ARDANA-SERVER-NAME</em>: <em class="replaceable ">ip address</em></pre></div><p>
  <span class="bold"><strong>Example:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">   network_groups:
    EXTERNAL-API:
        EXTERNAL-API-NET:
            control_planes:
                control-plane-1:
                    clusters:
                        cluster1:
                            servers:
                                ardana-cp1-c1-m1: 10.0.1.2
                                ardana-cp1-c1-m2: 10.0.1.3
                                ardana-cp1-c1-m3: 10.0.1.4
                            vips:
                                10.0.1.5: extlb
    EXTERNAL-VM:
        EXTERNAL-VM-NET:
            control_planes:
                control-plane-1:
                    clusters:
                        cluster1:
                            servers:
                                ardana-cp1-c1-m1: null
                                ardana-cp1-c1-m2: null
                                ardana-cp1-c1-m3: null
                    resources:
                        compute:
                            servers:
                                ardana-cp1-comp0001: null</pre></div></div><div class="sect1" id="region-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">region_topology.yml</span> <a title="Permalink" class="permalink" href="#region-topology-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-region_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-region_topology_yml.xml</li><li><span class="ds-label">ID: </span>region-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each region.  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, multiple regions are not supported. Only
  <code class="literal">Region0</code> is valid.
 </p><div class="verbatim-wrap"><pre class="screen">regions:
  <em class="replaceable ">REGION-NAME</em>:
      control-planes:
          <em class="replaceable ">CONTROL-PLANE-NAME</em>:
              services:
                 <em class="replaceable ">SERVICE-NAME</em>:
                     <em class="replaceable ">LIST OF SERVICE COMPONENTS</em></pre></div><p>
  <span class="bold"><strong>Example:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">regions:
    region0:
        control-planes:
            control-plane-1:
                services:
                    barbican:
                    - barbican-api
                    - barbican-worker
                    ceilometer:
                    - ceilometer-common
                    - ceilometer-agent-notification
                    - ceilometer-api
                    - ceilometer-polling
                    cinder:
                    - cinder-api
                    - cinder-volume
                    - cinder-scheduler
                    - cinder-backup</pre></div></div><div class="sect1" id="service-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">service_topology.yml</span> <a title="Permalink" class="permalink" href="#service-topology-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-service_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-service_topology_yml.xml</li><li><span class="ds-label">ID: </span>service-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each service:
 </p><div class="verbatim-wrap"><pre class="screen">services:
    <em class="replaceable ">SERVICE-NAME</em>:
        components:
            <em class="replaceable ">COMPONENT-NAME</em>:
                control-planes:
                    <em class="replaceable ">CONTROL-PLANE-NAME</em>:
                        clusters:
                            <em class="replaceable ">CLUSTER-NAME</em>:
                                <em class="replaceable ">LIST OF SERVERS</em>
                        resources:
                            <em class="replaceable ">RESOURCE-GROUP-NAME</em>:
                                <em class="replaceable ">LIST OF SERVERS</em>
                        regions:
                            <em class="replaceable ">LIST OF REGIONS</em></pre></div><p>
  <span class="bold"><strong>Example:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">services:
    freezer:
        components:
            freezer-agent:
                control_planes:
                    control-plane-1:
                        clusters:
                            cluster1:
                            - ardana-cp1-c1-m1-mgmt
                            - ardana-cp1-c1-m2-mgmt
                            - ardana-cp1-c1-m3-mgmt
                        regions:
                        - region1
                        resources:
                            compute:
                            - ardana-cp1-comp0001-mgmt
                        regions:
                        - region1</pre></div></div><div class="sect1" id="private-data-metadata-ccp-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">private_data_metadata_ccp.yml</span> <a title="Permalink" class="permalink" href="#private-data-metadata-ccp-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-private_data_metadata_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-private_data_metadata_yml.xml</li><li><span class="ds-label">ID: </span>private-data-metadata-ccp-yml</li></ul></div></div></div></div><p>
  This file provide details of the secrets that are generated by the
  configuration processor. The details include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The names of each secret
   </p></li><li class="listitem "><p>
    Metadata about each secret. This is a list where each element contains
    details about each <code class="literal">component</code> service that uses the
    secret.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The <code class="literal">component</code> service that uses the secret, and if
      applicable the service that this component "consumes" when using the
      secret
     </p></li><li class="listitem "><p>
      The list of clusters on which the <code class="literal">component</code> service is
      deployed
     </p></li><li class="listitem "><p>
      The control plane <code class="literal">cp</code> on which the services are
      deployed
     </p></li></ul></div></li><li class="listitem "><p>
    A version number (the model version number)
   </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">SECRET</em>
      <em class="replaceable ">METADATA</em>
           <em class="replaceable ">LIST OF METADATA</em>
               <em class="replaceable ">CLUSTERS</em>
                   <em class="replaceable ">LIST OF CLUSTERS</em>
               <em class="replaceable ">COMPONENT</em>
               <em class="replaceable ">CONSUMES</em>
               <em class="replaceable ">CONTROL-PLANE</em>
       <em class="replaceable ">VERSION</em></pre></div><p>
  For example:
 </p><div class="verbatim-wrap"><pre class="screen">barbican_admin_password:
    metadata:
    -   clusters:
        - cluster1
        component: barbican-api
        cp: ccp
    version: '2.0'
keystone_swift_password:
    metadata:
    -   clusters:
        - cluster1
        component: swift-proxy
        consumes: keystone-api
        cp: ccp
    version: '2.0'
metadata_proxy_shared_secret:
    metadata:
    -   clusters:
        - cluster1
        component: nova-metadata
        cp: ccp
    -   clusters:
        - cluster1
        - compute
        component: neutron-metadata-agent
        cp: ccp
    version: '2.0'
    …</pre></div></div><div class="sect1" id="password-change-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">password_change.yml</span> <a title="Permalink" class="permalink" href="#password-change-yml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-password_change_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-password_change_yml.xml</li><li><span class="ds-label">ID: </span>password-change-yml</li></ul></div></div></div></div><p>
  This file provides details equivalent to those in private_data_metadata_ccp.yml
  for passwords which have been changed from their original values, using the
  procedure outlined in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> documentation
 </p></div><div class="sect1" id="explain-txt"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">explain.txt</span> <a title="Permalink" class="permalink" href="#explain-txt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-explain_txt.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-explain_txt.xml</li><li><span class="ds-label">ID: </span>explain-txt</li></ul></div></div></div></div><p>
  This file provides details of the server allocation and network configuration
  decisions the configuration processor has made. The sequence of information
  recorded is:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Any service components that are automatically added
   </p></li><li class="listitem "><p>
    Allocation of servers to clusters and resource groups
   </p></li><li class="listitem "><p>
    Resolution of the network configuration for each server
   </p></li><li class="listitem "><p>
    Resolution of the network configuration of each load balancer
   </p></li></ul></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">        Add required services to control plane control-plane-1
        ======================================================
        control-plane-1: Added nova-metadata required by nova-api
        control-plane-1: Added swift-common required by swift-proxy
        control-plane-1: Added swift-rsync required by swift-account

        Allocate Servers for control plane control-plane-1
        ==================================================

        cluster: cluster1
        -----------------
          Persisted allocation for server 'controller1' (AZ1)
          Persisted allocation for server 'controller2' (AZ2)
          Searching for server with role ['CONTROLLER-ROLE'] in zones: set(['AZ3'])
          Allocated server 'controller3' (AZ3)

        resource: compute
        -----------------
          Persisted allocation for server 'compute1' (AZ1)
          Searching for server with role ['COMPUTE-ROLE'] in zones: set(['AZ1', 'AZ2', 'AZ3'])

        Resolve Networks for Servers
        ============================
        server: ardana-cp1-c1-m1
        ------------------------
          add EXTERNAL-API for component ip-cluster
          add MANAGEMENT for component ip-cluster
          add MANAGEMENT for lifecycle-manager (default)
          add MANAGEMENT for ntp-server (default)
          ...
          add MANAGEMENT for swift-rsync (default)
          add GUEST for tag neutron.networks.vxlan (neutron-openvswitch-agent)
          add EXTERNAL-VM for tag neutron.l3_agent.external_network_bridge (neutron-vpn-agent)
          Using persisted address 10.0.1.2 for server ardana-cp1-c1-m1 on network EXTERNAL-API-NET
          Using address 192.168.10.3 for server ardana-cp1-c1-m1 on network MANAGEMENT-NET
          Using persisted address 10.1.1.2 for server ardana-cp1-c1-m1 on network GUEST-NET

        …
        Define load balancers
        =====================

        Load balancer: extlb
        --------------------
          Using persisted address 10.0.1.5 for vip extlb ardana-cp1-vip-extlb-extapi on network EXTERNAL-API-NET
          Add nova-api for roles ['public'] due to 'default'
          Add glance-api for roles ['public'] due to 'default'
          ...

        Map load balancers to providers
        ===============================

        Network EXTERNAL-API-NET
        ------------------------
          10.0.1.5: ip-cluster nova-api roles: ['public'] vip-port: 8774 host-port: 8774
          10.0.1.5: ip-cluster glance-api roles: ['public'] vip-port: 9292 host-port: 9292
          10.0.1.5: ip-cluster keystone-api roles: ['public'] vip-port: 5000 host-port: 5000
          10.0.1.5: ip-cluster swift-proxy roles: ['public'] vip-port: 8080 host-port: 8080
          10.0.1.5: ip-cluster monasca-api roles: ['public'] vip-port: 8070 host-port: 8070
          10.0.1.5: ip-cluster heat-api-cfn roles: ['public'] vip-port: 8000 host-port: 8000
          10.0.1.5: ip-cluster ops-console-web roles: ['public'] vip-port: 9095 host-port: 9095
          10.0.1.5: ip-cluster heat-api roles: ['public'] vip-port: 8004 host-port: 8004
          10.0.1.5: ip-cluster nova-novncproxy roles: ['public'] vip-port: 6080 host-port: 6080
          10.0.1.5: ip-cluster neutron-server roles: ['public'] vip-port: 9696 host-port: 9696
          10.0.1.5: ip-cluster heat-api-cloudwatch roles: ['public'] vip-port: 8003 host-port: 8003
          10.0.1.5: ip-cluster ceilometer-api roles: ['public'] vip-port: 8777 host-port: 8777
          10.0.1.5: ip-cluster freezer-api roles: ['public'] vip-port: 9090 host-port: 9090
          10.0.1.5: ip-cluster horizon roles: ['public'] vip-port: 443 host-port: 80
          10.0.1.5: ip-cluster cinder-api roles: ['public'] vip-port: 8776 host-port: 8776</pre></div></div><div class="sect1" id="clouddiagram-txt"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CloudDiagram.txt</span> <a title="Permalink" class="permalink" href="#clouddiagram-txt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-clouddiagram_txt.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-clouddiagram_txt.xml</li><li><span class="ds-label">ID: </span>clouddiagram-txt</li></ul></div></div></div></div><p>
  This file provides a pictorial representation of the cloud. Although this
  file is still produced, it is superseded by the HTML output described in the
  following section.
 </p></div><div class="sect1" id="html-representation"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HTML Representation</span> <a title="Permalink" class="permalink" href="#html-representation">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-input_model-cpinfofiles-html_representation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-html_representation.xml</li><li><span class="ds-label">ID: </span>html-representation</li></ul></div></div></div></div><p>
  An HTML representation of the cloud can be found in
  <code class="filename">~/openstack/my_cloud/html</code> after the first Configuration
  Processor run. This directory is also rebuilt each time the Configuration
  Processor is run. These files combine the data in the input model with
  allocation decisions made by the Configuration processor to allow the
  configured cloud to be viewed from a number of different perspectives.
 </p><p>
  Most of the entries on the HTML pages provide either links to other parts of
  the HTML output or additional details via hover text.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-html_representation_1.png" target="_blank"><img src="images/media-inputmodel-html_representation_1.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-html_representation_2.png" target="_blank"><img src="images/media-inputmodel-html_representation_2.png" width="" /></a></div></div></div></div><div class="chapter " id="example-configurations"><div class="titlepage"><div><div><h2 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Configurations</span> <a title="Permalink" class="permalink" href="#example-configurations">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>example-configurations</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#example-configs"><span class="number">9.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span></a></span></dt><dt><span class="section"><a href="#alternative"><span class="number">9.2 </span><span class="name">Alternative Configurations</span></a></span></dt><dt><span class="section"><a href="#kvm-examples"><span class="number">9.3 </span><span class="name">KVM Examples</span></a></span></dt><dt><span class="section"><a href="#esx-examples"><span class="number">9.4 </span><span class="name">ESX Examples</span></a></span></dt><dt><span class="section"><a href="#swift-examples"><span class="number">9.5 </span><span class="name">Swift Examples</span></a></span></dt><dt><span class="section"><a href="#ironic-examples"><span class="number">9.6 </span><span class="name">Ironic Examples</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> system ships with a collection of pre-qualified example
  configurations. These are designed to help you to get up and running quickly
  with a minimum number of configuration changes.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input model allows a wide variety of configuration parameters
  that can, at first glance, appear daunting. The example configurations are
  designed to simplify this process by providing pre-built and pre-qualified
  examples that need only a minimum number of modifications to get started.
 </p><div class="sect1" id="example-configs"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span> <a title="Permalink" class="permalink" href="#example-configs">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>example-configs</li></ul></div></div></div></div><p>
   This section briefly describes the various example configurations and their
   capabilities. It also describes in detail, for the entry-scale-kvm
   example, how you can adapt the input model to work in your environment.
  </p><p>
   The following pre-qualified examples are shipped with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Location</th></tr></thead><tbody><tr><td>
      <a class="xref" href="#entry-scale-kvm" title="9.3.1. Entry-Scale Cloud">Section 9.3.1, “Entry-Scale Cloud”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td>
      <a class="xref" href="#entry-scale-kvm-mml" title="9.3.2. Entry Scale Cloud with Metering and Monitoring Services">Section 9.3.2, “Entry Scale Cloud with Metering and Monitoring Services”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td><a class="xref" href="#entry-scale-kvm-esx" title="9.4.1. Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors">Section 9.4.1, “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</a>
     </td><td><code class="filename">~/openstack/examples/entry-scale-esx-kvm</code>
     </td></tr><tr><td>
      <a class="xref" href="#entry-scale-kvm-esx-mml" title="9.4.2. Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors">Section 9.4.2, “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-esx-kvm-mml</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-swift" title="9.5.1. Entry-scale Swift Model">Section 9.5.1, “Entry-scale Swift Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-ironic" title="9.6.1. Entry-Scale Cloud with Ironic Flat Network">Section 9.6.1, “Entry-Scale Cloud with Ironic Flat Network”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-ironic-multi-tenancy" title="9.6.2. Entry-Scale Cloud with Ironic Multi-Tenancy">Section 9.6.2, “Entry-Scale Cloud with Ironic Multi-Tenancy”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td><a class="xref" href="#mid-scale-kvm" title="9.3.3. Single-Region Mid-Size Model">Section 9.3.3, “Single-Region Mid-Size Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div><p>
   The entry-scale systems are designed to provide an entry-level solution that
   can be scaled from a small number of nodes to a moderately high node count
   (approximately 100 compute nodes, for example).
  </p><p>
   In the mid-scale model, the cloud control plane is subdivided into a number
   of dedicated service clusters to provide more processing power for
   individual control plane elements. This enables a greater number of
   resources to be supported (compute nodes, Swift object servers). This model
   also shows how a segmented network can be expressed in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> model.
  </p></div><div class="sect1" id="alternative"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Configurations</span> <a title="Permalink" class="permalink" href="#alternative">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>alternative</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> there are alternative configurations that we recommend
   for specific purposes and this section we will outline them.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#standalone-deployer" title="12.1. Using a Dedicated Cloud Lifecycle Manager Node">Section 12.1, “Using a Dedicated Cloud Lifecycle Manager Node”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#without-dvr" title="12.2. Configuring SUSE OpenStack Cloud without DVR">Section 12.2, “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#without-l3agent" title="12.3. Configuring SUSE OpenStack Cloud with Provider VLANs and Physical Routers Only">Section 12.3, “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#twosystems" title="12.4. Considerations When Installing Two Systems on One Subnet">Section 12.4, “Considerations When Installing Two Systems on One Subnet”</a>
    </p></li></ul></div><p>
   The Ironic multi-tenancy feature uses Neutron to manage the tenant
   networks. The interaction between Neutron and the physical switch is
   facilitated by Neutron's Modular Layer 2 (ML2) plugin. The Neutron ML2
   plugin supports drivers to interact with various networks, as each vendor
   may have their own extensions. Those drivers are referred to as <span class="emphasis"><em>Neutron ML2
   mechanism drivers</em></span>, or simply <span class="emphasis"><em>mechanism drivers</em></span>.
  </p><p>
   The Ironic multi-tenancy feature has been validated using <span class="productname">OpenStack</span>
   genericswitch mechanism driver. However, if the given physical switch
   requires a different mechanism driver, you must update the input model
   accordingly. To update the input model with a custom ML2 mechanism driver,
   specify the relevant information in the
   <code class="literal">multi_tenancy_switch_config:</code> section of the
   <code class="filename">data/ironic/ironic_config.yml</code> file.
  </p></div><div class="sect1" id="kvm-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM Examples</span> <a title="Permalink" class="permalink" href="#kvm-examples">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-kvm_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-kvm_examples.xml</li><li><span class="ds-label">ID: </span>kvm-examples</li></ul></div></div></div></div><div class="sect2" id="entry-scale-kvm"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-entry-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm</li></ul></div></div></div></div><p>
  This example deploys an entry-scale cloud.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.2.3.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     Keystone, Nova API, Glance API, Neutron API, Horizon, and Heat
     API.
    </p></dd><dt id="id-1.3.4.7.6.2.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.6.2.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> One node of type
       <code class="literal">COMPUTE-ROLE</code> runs Nova Compute and associated
       services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       resources are provided by the control plane.
      </p></li></ul></div><p>
     Additional resource nodes can be added to the configuration.
    </p></dd><dt id="id-1.3.4.7.6.2.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> This network provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This network is used
       for all internal traffic between the cloud services. It is also used to
       install and configure the nodes. The network needs to be on an untagged
       VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.6.2.3.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by Swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file
    </p></dd></dl></div></div><div class="sect2" id="entry-scale-kvm-mml"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry Scale Cloud with Metering and Monitoring Services</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm-mml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-entry-scale-kvm-mml.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-mml.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-mml</li></ul></div></div></div></div><p>
  This example deploys an entry-scale cloud that provides metering and
  monitoring services and runs the database and messaging services in their own
  cluster.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.3.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Cluster1</strong></span> 2 nodes of type
       <code class="literal">CONTROLLER-ROLE</code> run the core OpenStack services, such
       as Keystone, Nova API, Glance API, Neutron API, Horizon, and
       Heat API.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster2</strong></span> 3 nodes of type
       <code class="literal">MTRMON-ROLE</code>, run the OpenStack services for metering
       and monitoring (for example, Ceilometer, Monasca and Logging).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster3</strong></span> 3 nodes of type
       <code class="literal">DBMQ-ROLE</code> that run clustered database and RabbitMQ
       services to support the cloud infrastructure. 3 nodes are required for
       high availability.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.6.3.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code>file.
    </p></dd><dt id="id-1.3.4.7.6.3.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> 1 node of type
       <code class="literal">COMPUTE-ROLE</code> runs Nova Compute and associated
       services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       resources are provided by the control plane.
      </p></li></ul></div><p>
     Additional resource nodes can be added to the configuration.
    </p></dd><dt id="id-1.3.4.7.6.3.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that is used for all internal traffic between the cloud services. It is
       also used to install and configure the nodes. The network needs to be on
       an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.6.3.3.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB of capacity. In addition,
     the example configures one additional disk depending on the role of
     the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Core Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> is
       configured to be used by Swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>DBMQ Controllers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used by the database and RabbitMQ.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage.
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="mid-scale-kvm"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Mid-Size Model</span> <a title="Permalink" class="permalink" href="#mid-scale-kvm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-mid-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-mid-scale-kvm.xml</li><li><span class="ds-label">ID: </span>mid-scale-kvm</li></ul></div></div></div></div><p>
  The mid-size model is intended as a template for a moderate sized cloud. The
  Control plane is made up of multiple server clusters to provide sufficient
  computational, network and IOPS capacity for a mid-size production style
  cloud.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.4.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Core Cluster</strong></span> runs core OpenStack
       Services, such as Keystone, Nova API, Glance API, Neutron API,
       Horizon, and Heat API. Default configuration is two nodes of role
       type <code class="literal">CORE-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Metering and Monitoring Cluster</strong></span> runs
       the OpenStack Services for metering and monitoring (for example,
       Ceilometer, Monasca and logging). Default configuration is three
       nodes of role type <code class="literal">MTRMON-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Database and Message Queue Cluster</strong></span> runs
       clustered MariaDB and RabbitMQ services to support the Ardana cloud
       infrastructure. Default configuration is three nodes of role type
       <code class="literal">DBMQ-ROLE</code>. Three nodes are required for high
       availability.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Swift PAC Cluster</strong></span> runs the Swift
       Proxy, Account and Container services. Default configuration is three
       nodes of role type <code class="literal">SWPAC-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Neutron Agent Cluster</strong></span> Runs Neutron
       VPN (L3), DHCP, Metadata and OpenVswitch agents. Default configuration
       is two nodes of role type <code class="literal">NEUTRON-ROLE</code>.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.6.4.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.6.4.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> runs Nova Compute and
       associated services. Runs on nodes of role type
       <code class="literal">COMPUTE-ROLE</code>. This model lists 3 nodes. 1 node is the
       minimum requirement.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> 3 nodes of type
       <code class="literal">SOWBJ-ROLE</code> run the Swift Object service. The
       minimum node count should match your Swift replica count.
      </p></li></ul></div><p>
     The minimum node count required to run this model unmodified is 19 nodes.
     This can be reduced by consolidating services on the control plane
     clusters.
    </p></dd><dt id="id-1.3.4.7.6.4.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Internal API</strong></span> This network is used
       within the cloud for API access between services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> This network provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This network is used
       for all internal traffic between the cloud services. It is also used to
       install and configure the nodes. The network needs to be on an untagged
       VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>SWIFT</strong></span> This network is used for internal
       Swift communications between the Swift nodes.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd></dl></div><div class="sect3" id="id-1.3.4.7.6.4.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adapting the Mid-Size Model to Fit Your Environment</span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.6.4.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-mid-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-mid-scale-kvm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The minimum set of changes you need to make to adapt the model for your
   environment are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Update <code class="filename">servers.yml</code> to list the details of your
     baremetal servers.
    </p></li><li class="listitem "><p>
     Update the <code class="filename">networks.yml</code> file to replace network CIDRs
     and VLANs with site specific values.
    </p></li><li class="listitem "><p>
     Update the <code class="filename">nic_mappings.yml</code> file to ensure that
     network devices are mapped to the correct physical port(s).
    </p></li><li class="listitem "><p>
     Review the disk models (<code class="filename">disks_*.yml</code>) and confirm that
     the associated servers have the number of disks required by the disk
     model. The device names in the disk models might need to be adjusted to
     match the probe order of your servers. The default number of disks for the
     Swift nodes (3 disks) is set low on purpose to facilitate deployment on
     generic hardware. For production scale Swift the servers should have
     more disks. For example, 6 on SWPAC nodes and 12 on SWOBJ nodes. If you
     allocate more Swift disks then you should review the ring power in the
     Swift ring configuration. This is documented in the Swift section.
     Disk models are provided as follows:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       DISK SET CONTROLLER: Minimum 1 disk
      </p></li><li class="listitem "><p>
       DISK SET DBMQ: Minimum 3 disks
      </p></li><li class="listitem "><p>
       DISK SET COMPUTE: Minimum 2 disks
      </p></li><li class="listitem "><p>
       DISK SET SWPAC: Minimum 3 disks
      </p></li><li class="listitem "><p>
       DISK SET SWOBJ: Minimum 3 disks
      </p></li></ul></div></li><li class="listitem "><p>
     Update the <code class="filename">netinterfaces.yml</code> file to match the server
     NICs used in your configuration. This file has a separate interface model
     definition for each of the following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       INTERFACE SET CONTROLLER
      </p></li><li class="listitem "><p>
       INTERFACE SET DBMQ
      </p></li><li class="listitem "><p>
       INTERFACE SET SWPAC
      </p></li><li class="listitem "><p>
       INTERFACE SET SWOBJ
      </p></li><li class="listitem "><p>
       INTERFACE SET COMPUTE
      </p></li></ul></div></li></ul></div></div></div></div><div class="sect1" id="esx-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX Examples</span> <a title="Permalink" class="permalink" href="#esx-examples">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-esx_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-esx_examples.xml</li><li><span class="ds-label">ID: </span>esx-examples</li></ul></div></div></div></div><div class="sect2" id="entry-scale-kvm-esx"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm-esx">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-entry-scale-kvm-esx.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-esx.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-esx</li></ul></div></div></div></div><p>
  This example deploys a cloud which mixes KVM and ESX hypervisors.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.7.2.3.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     Keystone, Nova API, Glance API, Neutron API, Horizon, and Heat
     API.
    </p></dd><dt id="id-1.3.4.7.7.2.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.7.2.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Compute:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>KVM</strong></span> runs Nova Computes and
         associated services. It runs on nodes of role type
         <code class="literal">COMPUTE-ROLE</code>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>ESX</strong></span> provides ESX Compute services. OS
         and software on this node is installed by user.
        </p></li></ul></div></li></ul></div></dd><dt id="id-1.3.4.7.7.2.3.4"><span class="term ">ESX Resource Requirements</span></dt><dd><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       User needs to supply vSphere server
      </p></li><li class="listitem "><p>
       User needs to deploy the ovsvapp network resources using the
       vSphere GUI (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 15 “Installing ESX Computes and OVSvAPP”, Section 15.8 “Configuring the Required Distributed vSwitches and Port Groups”, Section 15.8.2 “Creating ESXi MGMT DVS and Required Portgroup”</span>) by running the
       <code class="literal">neutron-create-ovsvapp-resources.yml</code> playbook
       (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 15 “Installing ESX Computes and OVSvAPP”, Section 15.8 “Configuring the Required Distributed vSwitches and Port Groups”, Section 15.8.3 “Configuring OVSvApp Network Resources Using Ansible-Playbook”</span>) or via Python-Networking-vSphere
       (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 15 “Installing ESX Computes and OVSvAPP”, Section 15.8 “Configuring the Required Distributed vSwitches and Port Groups”, Section 15.8.4 “Configuring OVSVAPP Using Python-Networking-vSphere”</span>)
      </p><p>
       The following DVS and DVPGs need to be created and configured for each
       cluster in each ESX hypervisor that will host an OvsVapp appliance. The
       settings for each DVS and DVPG are specific to your system and network
       policies. A JSON file example is provided in the documentation, but it
       needs to be edited to match your requirements.
      </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><tbody><tr><td><span class="bold"><strong>DVS</strong></span></td><td><span class="bold"><strong>Port Groups assigned to DVS</strong></span></td></tr><tr><td>MGMT</td><td>MGMT-PG, ESX-CONF-PG, GUEST-PG</td></tr><tr><td>TRUNK</td><td>TRUNK-PG</td></tr></tbody></table></div></li><li class="listitem "><p>
       User needs to deploy ovsvapp appliance (<code class="literal">OVSVAPP-ROLE</code>)
       and nova-proxy appliance (<code class="literal">ESX-COMPUTE-ROLE</code>)
      </p></li><li class="listitem "><p>
       User needs to add required information related to compute proxy and
       OVSvApp Nodes
      </p></li></ol></div></dd><dt id="id-1.3.4.7.7.2.3.5"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span>network connected to the
       lifecycle-manager and the IPMI ports of all nodes, except the ESX
       hypervisors.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for
       making requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that
       provides access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> The network
       used for all internal traffic between the cloud services. It is also
       used to install and configure the nodes. The network needs to be on an
       untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This network carries
       traffic between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>SES</strong></span> This is the network that
       control-plane and compute-node clients use to talk to the external SUSE Enterprise Storage.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>TRUNK</strong></span> is the network that is used
       to apply security group rules on tenant traffic. It is managed by the
       cloud admin and is restricted to the vCenter environment.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>ESX-CONF-NET</strong></span> network is used
       only to configure the ESX compute nodes in the cloud. This network
       should be different from the network used with PXE to stand up the cloud
       control-plane.
      </p></li></ul></div><p>
     This example's set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.7.2.3.6"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In addition,
     the example configures additional disk depending on the node's role:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by Swift
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="entry-scale-kvm-esx-mml"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm-esx-mml">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-entry-scale-kvm-esx-mml.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-esx-mml.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-esx-mml</li></ul></div></div></div></div><p>
  This example deploys a cloud which mixes KVM and ESX hypervisors, provides
  metering and monitoring services, and runs the database and messaging
  services in their own cluster.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.7.3.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Cluster1</strong></span> 2 nodes of type
       <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such
       as Keystone, Nova API, Glance API, Neutron API, Horizon, and
       Heat API.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster2</strong></span> 3 nodes of type
       <code class="literal">MTRMON-ROLE</code>, run the <span class="productname">OpenStack</span> services for metering
       and monitoring (for example, Ceilometer, Monasca and Logging).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster3</strong></span> 3 nodes of type
       <code class="literal">DBMQ-ROLE</code>, run clustered database and RabbitMQ
       services to support the cloud infrastructure. 3 nodes are required for
       high availability.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.7.3.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.7.3.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Compute:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>KVM</strong></span> runs Nova Computes and
         associated services. It runs on nodes of role type
         <code class="literal">COMPUTE-ROLE</code>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>ESX</strong></span> provides ESX Compute services. OS
         and software on this node is installed by user.
        </p></li></ul></div></li></ul></div></dd><dt id="id-1.3.4.7.7.3.3.4"><span class="term "> ESX Resource Requirements </span></dt><dd><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       User needs to supply vSphere server
      </p></li><li class="listitem "><p>
       User needs to deploy the ovsvapp network resources using the
       vSphere GUI or by running the
       <code class="literal">neutron-create-ovsvapp-resources.yml</code> playbook
      </p><p>
       The following DVS and DVPGs need to be created and configured for each
       cluster in each ESX hypervisor that will host an OvsVapp appliance. The
       settings for each DVS and DVPG are specific to your system and network
       policies. A JSON file example is provided in the documentation, but it
       needs to be edited to match your requirements.
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         ESX-CONF (DVS and DVPG) connected to ovsvapp eth0 and compute-proxy
         eth0
        </p></li><li class="listitem "><p>
         MANAGEMENT (DVS and DVPG) connected to ovsvapp eth1, eth2, eth3 and compute-proxy
         eth1
        </p></li></ul></div></li><li class="listitem "><p>
       User needs to deploy ovsvapp appliance (<code class="literal">OVSVAPP-ROLE</code>)
       and nova-proxy appliance (<code class="literal">ESX-COMPUTE-ROLE</code>)
      </p></li><li class="listitem "><p>
       User needs to add required information related to compute proxy and
       OVSvApp Nodes
      </p></li></ol></div></dd><dt id="id-1.3.4.7.7.3.3.5"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span>network connected to the
       lifecycle-manager and the IPMI ports of all nodes, except the ESX
       hypervisors.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for
       making requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that
       provides access to VMs (via floating IP addresses).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This
       network is used for all internal traffic between the cloud services. It
       is also used to install and configure the nodes. The network needs to be
       on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This is the network that will
       carry traffic between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>TRUNK</strong></span> is the network that will be used
       to apply security group rules on tenant traffic. It is managed by the
       cloud admin and is restricted to the vCenter environment.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>ESX-CONF-NET</strong></span> network is used
       only to configure the ESX compute nodes in the cloud. This network
       should be different from the network used with PXE to stand up the cloud
       control-plane.
      </p></li></ul></div><p>
     This example's set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.7.3.3.6"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition, the example configures additional disk depending on the node's
     role:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by Swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file
    </p></dd></dl></div></div></div><div class="sect1" id="swift-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Examples</span> <a title="Permalink" class="permalink" href="#swift-examples">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-swift_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-swift_examples.xml</li><li><span class="ds-label">ID: </span>swift-examples</li></ul></div></div></div></div><div class="sect2" id="entryscale-swift"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-scale Swift Model</span> <a title="Permalink" class="permalink" href="#entryscale-swift">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-entryscale_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_swift.xml</li><li><span class="ds-label">ID: </span>entryscale-swift</li></ul></div></div></div></div><p>
  This example shows how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can be configured to provide a Swift-only
  configuration, consisting of three controllers and one or more Swift object
  servers.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-examples-entry_scale_swift.png" target="_blank"><img src="images/media-examples-entry_scale_swift.png" width="" /></a></div></div><p>
  The example requires the following networks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>External API</strong></span> - The network for making
    requests to the cloud.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Swift</strong></span> - The network for all data traffic
    between the Swift services.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Management</strong></span> - This network that is used for
    all internal traffic between the cloud services, including node
    provisioning. This network must be on an untagged VLAN.
   </p></li></ul></div><p>
  All of these networks are configured to be presented via a pair of bonded
  NICs. The example also enables provider VLANs to be configured in Neutron on
  this interface.
 </p><p>
  In the diagram "External Routing" refers to whatever routing you want to
  provide to allow users to access the External API. "Internal Routing" refers
  to whatever routing you want to provide to allow administrators to access the
  Management network.
 </p><p>
  If you are using <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to install the operating system, then an IPMI
  network connected to the IPMI ports of all servers and routable from the
  Cloud Lifecycle Manager is also required for BIOS and power management of the node during the
  operating system installation process.
 </p><p>
  In the example the controllers use one disk for the operating system and two
  disks for Swift proxy and account storage. The Swift object servers use one
  disk for the operating system and four disks for Swift storage. These values
  can be modified to suit your environment.
 </p><p>
  These recommended minimums are based on the included with the base
  installation and are suitable only for demo environments. For production
  systems you will want to consider your capacity and performance requirements
  when making decisions about your hardware.
 </p><p>
  The <code class="literal">entry-scale-swift</code> example runs the Swift proxy,
  account and container services on the three controller servers. However, it
  is possible to extend the model to include the Swift proxy, account and
  container services on dedicated servers (typically referred to as the Swift
  proxy servers). If you are using this model, we have included the recommended
  Swift proxy servers specs in the table below.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">Server Hardware - Minimum Requirements and
            Recommendations</th></tr><tr><th>Disk </th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Swift account/container data drive
        </p></li></ul></div>
     </td><td>64 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Swift Object</td><td>swobj</td><td>3</td><td>
      <p>
       If using x3 replication only:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
      <p>
       If using Erasure Codes only or a mix of x3 replication and Erasure
       Codes:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         6 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
     </td><td>32 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Swift Proxy, Account, and Container</td><td>swpac</td><td>3</td><td>2 x 600 GB (minimum, see considerations at bottom of page for more details)</td><td>64 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr></tbody></table></div><div id="id-1.3.4.7.8.2.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The disk speeds (RPM) chosen should be consistent within the same ring or
   storage policy. It is best to not use disks with mixed disk speeds within the
   same Swift ring.
  </p></div><p>
  <span class="bold"><strong>Considerations for your Swift object and proxy,
  account, container servers RAM and disk capacity needs</strong></span>
 </p><p>
  Swift can have a diverse number of hardware configurations. For example, a
  Swift object server may have just a few disks (minimum of 6 for erasure
  codes) or up to 70 and beyond. The memory requirement needs to be increased
  as more disks are added. The general rule of thumb for memory needed is 0.5
  GB per TB of storage. For example, a system with 24 hard drives at 8TB each,
  giving a total capacity of 192TB, should use 96GB of RAM. However, this does
  not work well for a system with a small number of small hard drives or a very
  large number of very large drives. So, if after calculating the memory given
  this guideline, if the answer is less than 32GB then go with 32GB of memory
  minimum and if the answer is over 256GB then use 256GB maximum, no need to
  use more memory than that.
 </p><p>
  When considering the capacity needs for the Swift proxy, account, and
  container (PAC) servers, you should calculate 2% of the total raw storage
  size of your object servers to specify the storage required for the PAC
  servers. So, for example, if you were using the example we provided earlier
  and you had an object server setup of 24 hard drives with 8TB each for a
  total of 192TB and you had a total of 6 object servers, that would give a raw
  total of 1152TB. So you would take 2% of that, which is 23TB, and ensure that
  much storage capacity was available on your Swift proxy, account, and
  container (PAC) server cluster. If you had a cluster of three Swift PAC
  servers, that would be ~8TB each.
 </p><p>
  Another general rule of thumb is that if you are expecting to have more than
  a million objects in a container then you should consider using SSDs on the
  Swift PAC servers rather than HDDs.
 </p></div></div><div class="sect1" id="ironic-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Examples</span> <a title="Permalink" class="permalink" href="#ironic-examples">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-ironic_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-ironic_examples.xml</li><li><span class="ds-label">ID: </span>ironic-examples</li></ul></div></div></div></div><div class="sect2" id="entryscale-ironic"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud with Ironic Flat Network</span> <a title="Permalink" class="permalink" href="#entryscale-ironic">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_ironic.xml</li><li><span class="ds-label">ID: </span>entryscale-ironic</li></ul></div></div></div></div><p>
  This example deploys an entry scale cloud that uses the Ironic service to
  provision physical machines through the Compute services API.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-exampleconfigs-entry_scale_ironic.png" target="_blank"><img src="images/media-hos.docs-exampleconfigs-entry_scale_ironic.png" width="" /></a></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.9.2.4.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core OpenStack services, such
     as Keystone, Nova API, Glance API, Neutron API, Horizon, and
     Heat API.
    </p></dd><dt id="id-1.3.4.7.9.2.4.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.9.2.4.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Ironic Compute</strong></span> One node of type
       <code class="literal">IRONIC-COMPUTE-ROLE</code> runs nova-compute,
       nova-compute-ironic, and other supporting services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       resources are provided by the control plane.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.9.2.4.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> This is the network that
       users will use to make requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that will be used for all internal traffic between the cloud services.
       This network is also used to install and configure the nodes. The
       network needs to be on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This is the flat network that
       will carry traffic between bare metal instances within the cloud. It is
       also used to PXE boot said bare metal instances and install the
       operating system selected by tenants.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">GUEST</code> network for the bare metal instances to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     modified to match your system.
    </p></dd><dt id="id-1.3.4.7.9.2.4.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code>
       configured to be used by Swift.
      </p></li></ul></div><p>
     Additional discs can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="entryscale-ironic-multi-tenancy"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud with Ironic Multi-Tenancy</span> <a title="Permalink" class="permalink" href="#entryscale-ironic-multi-tenancy">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-examples-entryscale_ironic_multi_tenancy.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_ironic_multi_tenancy.xml</li><li><span class="ds-label">ID: </span>entryscale-ironic-multi-tenancy</li></ul></div></div></div></div><p>
  This example deploys an entry scale cloud that uses the Ironic service to
  provision physical machines through the Compute services API and supports
  multi tenancy.
 </p><div class="figure" id="multi-tenancy"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ironic-Entry-ScaleIronicMultiTenancy.png" target="_blank"><img src="images/media-ironic-Entry-ScaleIronicMultiTenancy.png" width="" alt="Entry-scale Cloud with Ironic Muti-Tenancy" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 9.1: </span><span class="name">Entry-scale Cloud with Ironic Muti-Tenancy </span><a title="Permalink" class="permalink" href="#multi-tenancy">#</a></h6></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.9.3.4.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     Keystone, Nova API, Glance API, Neutron API, Horizon, and Heat
     API.
    </p></dd><dt id="id-1.3.4.7.9.3.4.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code>file.
    </p></dd><dt id="id-1.3.4.7.9.3.4.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Ironic Compute</strong></span> One node of type
       <code class="literal">IRONIC-COMPUTE-ROLE</code> runs nova-compute,
       nova-compute-ironic, and other supporting services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       Resources are provided by the control plane.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.9.3.4.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       deployer and the IPMI ports of all nodes.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> network is used to make
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that will be used for all internal traffic between the cloud services.
       This network is also used to install and configure the controller nodes.
       The network needs to be on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Provisioning</strong></span> is the network used to PXE
       boot the Ironic nodes and install the operating system selected by
       tenants. This network needs to be tagged on the switch for control
       plane/Ironic compute nodes. For Ironic bare metal nodes, VLAN
       configuration on the switch will be set by Neutron driver.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Tenant VLANs</strong></span> The range of VLAN IDs
       should be reserved for use by Ironic and set in the cloud configuration.
       It is configured as untagged on control plane nodes, therefore it cannot
       be combined with management network on the same network interface.
      </p></li></ul></div><p>
     The following access should be allowed by routing/firewall:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Access from Management network to IPMI. Used during cloud
       installation and during Ironic bare metal node provisioning.
      </p></li><li class="listitem "><p>
       Access from Management network to switch management network. Used by
       neutron driver.
      </p></li><li class="listitem "><p>
       The <code class="literal">EXTERNAL API</code> network must be reachable from the
       tenant networks if you want bare metal nodes to be able to make API
       calls to the cloud.
      </p></li></ul></div><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses <code class="filename">hed3</code> for Management and External API
     traffic, and <code class="filename">hed4</code> for provisioning and tenant network
     traffic. If you need to modify these assignments for your environment,
     they are defined in <code class="filename">data/net_interfaces.yml</code>.
    </p></dd><dt id="id-1.3.4.7.9.3.4.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code>
       configured to be used by Swift.
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div></div></div><div class="chapter " id="modify-compute-input-model"><div class="titlepage"><div><div><h2 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying Example Configurations for Compute Nodes</span> <a title="Permalink" class="permalink" href="#modify-compute-input-model">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-compute-model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-compute-model.xml</li><li><span class="ds-label">ID: </span>modify-compute-input-model</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sles-compute-model"><span class="number">10.1 </span><span class="name">SLES Compute Nodes</span></a></span></dt></dl></div></div><p>
  This section contains detailed information about the Compute Node parts of
  the input model. For example input models, see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>. For general descriptions of the input
  model, see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>.
 </p><p>
  Usually, the example models provide most of the data that is required to
  create a valid input model. However, before you start to deploy, you may want
  to customize an input model using the following information about Compute
  Nodes.
 </p><div class="sect1" id="sles-compute-model"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SLES Compute Nodes</span> <a title="Permalink" class="permalink" href="#sles-compute-model">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-alternative-sles_compute_model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-sles_compute_model.xml</li><li><span class="ds-label">ID: </span>sles-compute-model</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.8.4.2.1"><span class="term "><code class="filename">net_interfaces.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">- name: <span class="bold"><strong>SLES-COMPUTE-INTERFACES</strong></span>
 network-interfaces:
   - name: BOND0
     device:
         name: bond0
     bond-data:
         options:
             mode: active-backup
             miimon: 200
             primary: hed1
         provider: linux
         devices:
             - name: hed1
             - name: hed2
     network-groups:
       - EXTERNAL-VM
       - GUEST
       - MANAGEMENT</pre></div></dd><dt id="id-1.3.4.8.4.2.2"><span class="term "><code class="filename">servers.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">    - id: compute1
      ip-addr: 10.13.111.15
      <span class="bold"><strong>role: SLES-COMPUTE-ROLE</strong></span>
      server-group: RACK1
      nic-mapping: DL360p_G8_2Port
      mac-addr: ec:b1:d7:77:d0:b0
      ilo-ip: 10.12.13.14
      ilo-password: *********
      ilo-user: Administrator
      <span class="bold"><strong>distro-id: sles12sp3-x86_64</strong></span></pre></div></dd><dt id="id-1.3.4.8.4.2.3"><span class="term "><code class="filename">server_roles.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">- name: <span class="bold"><strong>SLES-COMPUTE-ROLE</strong></span>
  interface-model: <span class="bold"><strong>SLES-COMPUTE-INTERFACES</strong></span>
  disk-model: <span class="bold"><strong>SLES-COMPUTE-DISKS</strong></span></pre></div></dd><dt id="id-1.3.4.8.4.2.4"><span class="term "><code class="filename">disk_compute.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">  - name: <span class="bold"><strong>SLES-COMPUTE-DISKS</strong></span>
    volume-groups:
      - name: ardana-vg
        physical-volumes:
         - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file

      - name: vg-comp
        # this VG is dedicated to Nova Compute to keep VM IOPS off the OS disk
        physical-volumes:
          - /dev/sdb
        logical-volumes:
          - name: compute
            size: 95%
            mount: /var/lib/nova
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></dd><dt id="id-1.3.4.8.4.2.5"><span class="term "><code class="filename">control_plane.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">  control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1
      region-name: region0
....
      resources:
        - name: <span class="bold"><strong>sles-compute</strong></span>
          resource-prefix: <span class="bold"><strong>sles-comp</strong></span>
          server-role: <span class="bold"><strong>SLES-COMPUTE-ROLE</strong></span>
          allocation-policy: any
          min-count: 1
          service-components:
            - ntp-client
            - nova-compute
            - nova-compute-kvm
            - neutron-l3-agent
            - neutron-metadata-agent
            - neutron-openvswitch-agent
            - neutron-lbaasv2-agent</pre></div></dd></dl></div></div></div><div class="chapter " id="modify-input-model"><div class="titlepage"><div><div><h2 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying Example Configurations for Object Storage using Swift</span> <a title="Permalink" class="permalink" href="#modify-input-model">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-swift_input_model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_input_model.xml</li><li><span class="ds-label">ID: </span>modify-input-model</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#objectstorage-overview"><span class="number">11.1 </span><span class="name">Object Storage using Swift Overview</span></a></span></dt><dt><span class="section"><a href="#topic-r3k-v2c-jt"><span class="number">11.2 </span><span class="name">Allocating Proxy, Account, and Container (PAC) Servers for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-tq1-xt5-dt"><span class="number">11.3 </span><span class="name">Allocating Object Servers</span></a></span></dt><dt><span class="section"><a href="#topic-uh2-td1-kt"><span class="number">11.4 </span><span class="name">Creating Roles for Swift Nodes</span></a></span></dt><dt><span class="section"><a href="#allocating-disk-drives"><span class="number">11.5 </span><span class="name">Allocating Disk Drives for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-d1s-hht-tt"><span class="number">11.6 </span><span class="name">Swift Requirements for Device Group Drives</span></a></span></dt><dt><span class="section"><a href="#topic-rvj-21c-jt"><span class="number">11.7 </span><span class="name">Creating a Swift Proxy, Account, and Container (PAC) Cluster</span></a></span></dt><dt><span class="section"><a href="#topic-jzk-q1c-jt"><span class="number">11.8 </span><span class="name">Creating Object Server Resource Nodes</span></a></span></dt><dt><span class="section"><a href="#topic-pcj-hzv-dt"><span class="number">11.9 </span><span class="name">Understanding Swift Network and Service Requirements</span></a></span></dt><dt><span class="section"><a href="#ring-specification"><span class="number">11.10 </span><span class="name">Understanding Swift Ring Specifications</span></a></span></dt><dt><span class="section"><a href="#swift-storage-policies"><span class="number">11.11 </span><span class="name">Designing Storage Policies</span></a></span></dt><dt><span class="section"><a href="#designing-swift-zones"><span class="number">11.12 </span><span class="name">Designing Swift Zones</span></a></span></dt><dt><span class="section"><a href="#topic-rdf-hkp-rt"><span class="number">11.13 </span><span class="name">Customizing Swift Service Configuration Files</span></a></span></dt></dl></div></div><p>
  This section contains detailed descriptions about the Swift-specific parts of
  the input model. For example input models, see
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>. For general descriptions of the
  input model, see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>. In addition, the Swift
  ring specifications are available in the
  <code class="filename">~/openstack/my_cloud/definition/data/swift/rings.yml</code>
  file.
 </p><p>
  Usually, the example models provide most of the data that is required to
  create a valid input model. However, before you start to deploy, you must do
  the following:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Check the disk model used by your nodes and that all disk drives are
    correctly named and used as described in
    <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.
   </p></li><li class="listitem "><p>
    Select an appropriate partition power for your rings. For more information,
    see <a class="xref" href="#ring-specification" title="11.10. Understanding Swift Ring Specifications">Section 11.10, “Understanding Swift Ring Specifications”</a>.
   </p></li></ul></div><p>
  For further information, read these related pages:
 </p><div class="sect1" id="objectstorage-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage using Swift Overview</span> <a title="Permalink" class="permalink" href="#objectstorage-overview">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-objectstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-objectstorage_overview.xml</li><li><span class="ds-label">ID: </span>objectstorage-overview</li></ul></div></div></div></div><div class="sect2" id="about"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is the Object Storage (Swift) Service?</span> <a title="Permalink" class="permalink" href="#about">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-objectstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-objectstorage_overview.xml</li><li><span class="ds-label">ID: </span>about</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Object Storage using Swift service leverages Swift which uses
   software-defined storage (SDS) layered on top of industry-standard servers
   using native storage devices. Swift presents an object paradigm, using an
   underlying set of disk drives. The disk drives are managed by a data
   structure called a "ring" and you can store, retrieve, and delete objects in
   containers using RESTful APIs.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Object Storage using Swift provides a highly-available, resilient,
   and scalable storage pool for unstructured data. It has a highly-durable
   architecture, with no single point of failure. In addition, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   includes the concept of cloud models, where the user can modify the cloud
   input model to provide the configuration required for their environment.
  </p></div><div class="sect2" id="services"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage (Swift) Services</span> <a title="Permalink" class="permalink" href="#services">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-objectstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-objectstorage_overview.xml</li><li><span class="ds-label">ID: </span>services</li></ul></div></div></div></div><p>
   A Swift system consists of a number of services:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Swift-proxy provides the API for all requests to the Swift system.
    </p></li><li class="listitem "><p>
     Account and container services provide storage management of the accounts
     and containers.
    </p></li><li class="listitem "><p>
     Object services provide storage management for object storage.
    </p></li></ul></div><p>
   These services can be co-located in a number of ways. The following general
   pattern exists in the example cloud models distributed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The swift-proxy, account, container, and object services run on the same
     (PACO) node type in the control plane. This is used for smaller clouds or
     where Swift is a minor element in a larger cloud. This is the model seen
     in most of the entry-scale models.
    </p></li><li class="listitem "><p>
     The swift-proxy, account, and container services run on one (PAC) node
     type in a cluster in a control plane and the object services run on
     another (OBJ) node type in a resource pool. This deployment model, known
     as the Entry-Scale Swift model, is used in larger clouds or where a larger
     Swift system is in use or planned. See <a class="xref" href="#entryscale-swift" title="9.5.1. Entry-scale Swift Model">Section 9.5.1, “Entry-scale Swift Model”</a>
     for more details.
    </p></li></ul></div><p>
   The Swift storage service can be scaled both vertically (nodes with larger
   or more disks) and horizontally (more Swift storage nodes) to handle an
   increased number of simultaneous user connections and provide larger storage
   space.
  </p><p>
   Swift is configured through a number of YAML files in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   implementation of the OpenStack Object Storage (Swift) service. For more
   details on the configuration of the YAML files, see
   <a class="xref" href="#modify-input-model" title="Chapter 11. Modifying Example Configurations for Object Storage using Swift">Chapter 11, <em>Modifying Example Configurations for Object Storage using Swift</em></a>.
  </p></div></div><div class="sect1" id="topic-r3k-v2c-jt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allocating Proxy, Account, and Container (PAC) Servers for Object Storage</span> <a title="Permalink" class="permalink" href="#topic-r3k-v2c-jt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocate_pac.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocate_pac.xml</li><li><span class="ds-label">ID: </span>topic-r3k-v2c-jt</li></ul></div></div></div></div><p>
  A Swift proxy, account, and container (PAC) server is a node that runs the
  swift-proxy, swift-account and swift-container services. It is used to
  respond to API requests and to store account and container data. The PAC node
  does not store object data.
 </p><p>
  This section describes the procedure to allocate PAC servers during the
  <span class="bold"><strong>initial</strong></span> deployment of the system.
 </p><div class="sect2" id="id-1.3.4.9.7.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Allocate Swift PAC servers</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.7.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocate_pac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocate_pac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to allocate PAC servers:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Verify if the example input model already contains a suitable server role.
     The server roles are usually described in the
     <code class="literal">data/server_roles.yml</code> file. If the server role is not
     described, you must add a suitable server role and allocate drives to
     store object data. For instructions, see
     <a class="xref" href="#topic-uh2-td1-kt" title="11.4. Creating Roles for Swift Nodes">Section 11.4, “Creating Roles for Swift Nodes”</a> and
     <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
    </p></li><li class="listitem "><p>
     Verify if the example input model has assigned a cluster to Swift proxy,
     account, container servers. It is usually mentioned in the
     <code class="literal">data/control_plane.yml</code> file. If the cluster is not
     assigned, then add a suitable cluster. For instructions, see
     <a class="xref" href="#topic-rvj-21c-jt" title="11.7. Creating a Swift Proxy, Account, and Container (PAC) Cluster">Section 11.7, “Creating a Swift Proxy, Account, and Container (PAC) Cluster”</a>.
    </p></li><li class="listitem "><p>
     Identify the physical servers and their IP address and other detailed
     information.
    </p><div class="itemizedlist " id="ul-tzf-pqb-jt"><ul class="itemizedlist"><li class="listitem "><p>
       You add these details to the servers list (usually in the
       <code class="literal">data/servers.yml</code> file).
      </p></li><li class="listitem "><p>
       As with all servers, you must also verify and/or modify the
       server-groups information (usually in
       <code class="literal">data/server_groups.yml</code>)
      </p></li></ul></div></li></ul></div><p>
   The only part of this process that is unique to Swift is the allocation of
   disk drives for use by the account and container rings. For instructions,
   see <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
  </p></div></div><div class="sect1" id="topic-tq1-xt5-dt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allocating Object Servers</span> <a title="Permalink" class="permalink" href="#topic-tq1-xt5-dt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_server.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_server.xml</li><li><span class="ds-label">ID: </span>topic-tq1-xt5-dt</li></ul></div></div></div></div><p>
  A Swift object server is a node that runs the swift-object service
  (<span class="bold"><strong>only</strong></span>) and is used to store object
  data. It does not run the swift-proxy, swift-account, or swift-container
  services.
  
 </p><p>
  This section describes the procedure to allocate a Swift object server during
  the <span class="bold"><strong>initial</strong></span> deployment of the system.
 </p><div class="sect2" id="procedure"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Allocate a Swift Object Server</span> <a title="Permalink" class="permalink" href="#procedure">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_server.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_server.xml</li><li><span class="ds-label">ID: </span>procedure</li></ul></div></div></div></div><p>
   Perform the following steps to allocate one or more Swift object servers:
  </p><div class="itemizedlist " id="ul-fbk-fj3-kt"><ul class="itemizedlist"><li class="listitem "><p>
     Verify if the example input model already contains a suitable server role.
     The server roles are usually described in the
     <code class="literal">data/server_roles.yml</code> file. If the server role is not
     described, you must add a suitable server role. For instructions, see
     <a class="xref" href="#topic-uh2-td1-kt" title="11.4. Creating Roles for Swift Nodes">Section 11.4, “Creating Roles for Swift Nodes”</a>. While adding a server role for the
     Swift object server, you will also allocate drives to store object data.
     For instructions, see <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
    </p></li><li class="listitem "><p>
     Verify if the example input model has a resource node assigned to Swift
     object servers. The resource nodes are usually assigned in the
     <code class="literal">data/control_plane.yml</code> file. If it is not assigned, you
     must add a suitable resource node. For instructions, see
     <a class="xref" href="#topic-jzk-q1c-jt" title="11.8. Creating Object Server Resource Nodes">Section 11.8, “Creating Object Server Resource Nodes”</a>.
    </p></li><li class="listitem "><p>
     Identify the physical servers and their IP address and other detailed
     information. Add the details for the servers in either of the following
     YAML files and verify the server-groups information:
    </p><div class="itemizedlist " id="ul-gmj-ng3-kt"><ul class="itemizedlist"><li class="listitem "><p>
       Add details in the servers list (usually in the
       <code class="literal">data/servers.yml</code> file).
      </p></li><li class="listitem "><p>
       As with all servers, you must also verify and/or modify the
       server-groups information (usually in the
       <code class="literal">data/server_groups.yml</code> file).
      </p></li></ul></div><p>
     The only part of this process that is unique to Swift is the allocation of
     disk drives for use by the object ring. For instructions, see
     <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
    </p></li></ul></div></div></div><div class="sect1" id="topic-uh2-td1-kt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Roles for Swift Nodes</span> <a title="Permalink" class="permalink" href="#topic-uh2-td1-kt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-creating_roles_swift_nodes.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_roles_swift_nodes.xml</li><li><span class="ds-label">ID: </span>topic-uh2-td1-kt</li></ul></div></div></div></div><p>
  To create roles for Swift nodes, you must edit the
  <code class="literal">data/server_roles.yml</code> file and add an entry to the
  server-roles list using the following syntax:
 </p><div class="verbatim-wrap"><pre class="screen">server-roles:
- name: <em class="replaceable ">PICK-A-NAME</em>
  interface-model: <em class="replaceable ">SPECIFY-A-NAME</em>
  disk-model: <em class="replaceable ">SPECIFY-A-NAME</em></pre></div><p>
  The fields for server roles are defined as follows:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong><code class="literal">name</code></strong></span>
     </td><td>
      Specifies a name assigned for the role. In the following example,
      <span class="bold"><strong>SWOBJ-ROLE</strong></span> is the role name.
     </td></tr><tr><td><span class="bold"><strong><code class="literal">interface-model</code></strong></span>
     </td><td>
      You can either select an existing interface model or create one
      specifically for Swift object servers. In the following example
      <span class="bold"><strong>SWOBJ-INTERFACES</strong></span> is used. For more
      information, see <a class="xref" href="#topic-pcj-hzv-dt" title="11.9. Understanding Swift Network and Service Requirements">Section 11.9, “Understanding Swift Network and Service Requirements”</a>.
     </td></tr><tr><td><span class="bold"><strong><code class="literal">disk-model</code></strong></span>
     </td><td>
      You can either select an existing model or create one specifically for
      Swift object servers. In the following example
      <span class="bold"><strong>SWOBJ-DISKS</strong></span> is used. For more
      information, see <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
     </td></tr></tbody></table></div><div class="verbatim-wrap"><pre class="screen">server-roles:
- name: <span class="bold"><strong>SWOBJ-ROLE</strong></span>
  interface-model: <span class="bold"><strong>SWOBJ-INTERFACES</strong></span>
  disk-model: <span class="bold"><strong>SWOBJ-DISKS</strong></span></pre></div></div><div class="sect1" id="allocating-disk-drives"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allocating Disk Drives for Object Storage</span> <a title="Permalink" class="permalink" href="#allocating-disk-drives">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_disk_drives.xml</li><li><span class="ds-label">ID: </span>allocating-disk-drives</li></ul></div></div></div></div><p>
  The disk model describes the configuration of disk drives
  
  and their usage. The examples include several disk models.
  
  You must always review the disk devices before making any changes to the
  existing the disk model.
 </p><div class="sect2" id="making-changes-disk-model"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Changes to a Swift Disk Model</span> <a title="Permalink" class="permalink" href="#making-changes-disk-model">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_disk_drives.xml</li><li><span class="ds-label">ID: </span>making-changes-disk-model</li></ul></div></div></div></div><p>
   There are several reasons for changing the disk model:
  </p><div class="itemizedlist " id="ul-d5y-255-dt"><ul class="itemizedlist"><li class="listitem "><p>
     If you have additional drives available, you can add them to the devices
     list.
    </p></li><li class="listitem "><p>
     If the disk devices listed in the example disk model have different names
     on your servers. This may be due to different hardware drives. Edit the
     disk model and change the device names to the correct names.
    </p></li><li class="listitem "><p>
     If you prefer a different disk drive than the one listed in the model. For
     example, if <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code>
     are slow hard drives and you have SDD drives available in
     <code class="literal">/dev/sdd</code> and <code class="literal">/dev/sde</code>. In this case,
     delete <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code> and
     replace them with <code class="literal">/dev/sdd</code> and
     <code class="literal">/dev/sde</code>.
    </p><div id="id-1.3.4.9.10.3.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Disk drives must not contain labels or file systems from a prior usage.
      For more information, see <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.
     </p></div><div id="id-1.3.4.9.10.3.3.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip</h6><p>
      The terms <span class="bold"><strong>add</strong></span> and
      <span class="bold"><strong>delete</strong></span> in the document
      means editing the respective YAML files to add or delete the
      configurations/values.
     </p></div></li></ul></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.9.10.3.4"><span class="name">Swift Consumer Syntax</span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   The consumer field determines the usage of a disk drive or logical volume by
   Swift. The syntax of the consumer field is as follows:
  </p><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: <em class="replaceable ">RING-NAME</em>
        - name: <em class="replaceable ">RING-NAME</em>
        - etc...</pre></div><p>
   The fields for consumer are defined as follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td>
       <code class="literal">name</code>
      </td><td>
       Specifies the service that uses the device group. A
       <code class="literal">name</code> field containing
       <span class="bold"><strong>swift</strong></span> indicates that the drives or
       logical volumes are used by Swift.
      </td></tr><tr><td>
       <code class="literal">attrs</code>
      </td><td>
       Lists the rings that the devices are allocated to. It must contain a
       <code class="literal">rings</code> item.
      </td></tr><tr><td>
       <code class="literal">rings</code>
      </td><td>
       Contains a list of ring names. In the <code class="literal">rings</code> list,
       the <code class="literal">name</code> field is optional.
      </td></tr></tbody></table></div><p>
   The following are the different configurations (patterns) of the proxy,
   account, container, and object services:
  </p><div class="itemizedlist " id="ul-dvq-kyb-jt"><ul class="itemizedlist"><li class="listitem "><p>
     Proxy, account, container, and object (PACO) run on same node type.
    </p></li><li class="listitem "><p>
     Proxy, account, and container run on a node type (PAC) and the object
     services run on a dedicated object server (OBJ).
    </p></li></ul></div><div id="id-1.3.4.9.10.3.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    The proxy service does not have any rings associated with it.
   </p></div><div class="example" id="id-1.3.4.9.10.3.12"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 11.1: </span><span class="name">
    <span class="bold">PACO</span> - proxy, account, container,
    and object run on the same node type.
    </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.12">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: account
        - name: container
        - name: object-0</pre></div></div></div><div class="example" id="id-1.3.4.9.10.3.13"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 11.2: </span><span class="name">
    <span class="bold">PAC</span> - proxy, account, and
    container run on the same node type.
    </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.13">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: account
        - name: container</pre></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.9.10.3.14"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 11.3: </span><span class="name"><span class="bold">OBJ</span> - Dedicated object
    server </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.14">#</a></h6></div><div class="example-contents"><p>
    The following example shows two Storage Policies (object-0 and object-1).
    For more information, see
    <a class="xref" href="#swift-storage-policies" title="11.11. Designing Storage Policies">Section 11.11, “Designing Storage Policies”</a>.
   </p><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: object-0
        - name: object-1</pre></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.9.10.3.15"><span class="name">Swift Device Groups</span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.15">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   You may have several device groups if you have several different uses for
   different sets of drives.
  </p><p>
   The following example shows a configuration where one drive is used for
   account and container rings and the other drives are used by the object-0
   ring:
  </p><div class="verbatim-wrap"><pre class="screen">device-groups:

- name: swiftpac
  devices:
  - name: /dev/sdb
  consumer:
      name: swift
      attrs:
      - name: account
      - name: container
  - name: swiftobj
    devices:
    - name: /dev/sdc
    - name: /dev/sde
    - name: /dev/sdf
    consumer:
       name: swift
       attrs:
           rings:
              - name: object-0</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.9.10.3.19"><span class="name">Swift Logical Volumes</span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.19">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h5></div><div id="id-1.3.4.9.10.3.20" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Be careful while using logical volumes to store Swift data. The data
    remains intact during an upgrade, but will be lost if the server is
    reimaged. If you use logical volumes you must ensure that you only reimage
    one server at a time. This is to allow the data from the other replicas to
    be replicated back to the logical volume once the reimage is complete.
   </p></div><p>
   Swift can use a logical volume. To do this, ensure you meet the requirements
   listed in the table below:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">mount</code>
         </p></li><li class="listitem "><p>
          <code class="literal">mkfs-opts</code>
         </p></li><li class="listitem "><p>
          <code class="literal">fstype</code>
         </p></li></ul></div>
      </td><td>Do not specify these attributes.</td></tr><tr><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">name</code>
         </p></li><li class="listitem "><p>
          <code class="literal">size</code>
         </p></li></ul></div>
      </td><td>Specify both of these attributes.</td></tr><tr><td>
       <div class="itemizedlist " id="ul-z5d-s3n-pt"><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">consumer</code>
         </p></li></ul></div>
      </td><td>
       This attribute must have a <code class="literal">name</code> field set to
       <span class="bold"><strong>swift</strong></span>.
      </td></tr></tbody></table></div><div id="id-1.3.4.9.10.3.23" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    When setting up Swift as a logical volume, the configuration processor
    will give a warning. This warning is normal and does not affect the
    configuration.
   </p></div><p>
   Following is an example of Swift logical volumes:
  </p><div class="verbatim-wrap"><pre class="screen">...
   - name: swift
     size: 50%
     consumer:
         name: swift
         attrs:
             rings:
             - name: object-0
             - name: object-1</pre></div></div></div><div class="sect1" id="topic-d1s-hht-tt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Requirements for Device Group Drives</span> <a title="Permalink" class="permalink" href="#topic-d1s-hht-tt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-swift_device_groups.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_device_groups.xml</li><li><span class="ds-label">ID: </span>topic-d1s-hht-tt</li></ul></div></div></div></div><p>
  To install and deploy, Swift requires that the disk drives listed in the
  devices list of the device-groups item in a disk model meet the following
  criteria (if not, the deployment will fail):
 </p><div class="itemizedlist " id="ul-fsy-255-dt"><ul class="itemizedlist"><li class="listitem "><p>
    The disk device must exist on the server. For example, if you add
    <code class="filename">/dev/sd<em class="replaceable ">X</em></code> to a server with
    only three devices, then the deploy process will fail.
   </p></li><li class="listitem "><p>
    The disk device must be unpartitioned or have a single partition that uses
    the whole drive.
   </p></li><li class="listitem "><p>
    The partition must not be labeled.

   </p></li><li class="listitem "><p>
    The XFS file system must not contain a file system label.

   </p></li><li class="listitem "><p>
    If the disk drive is already labeled as described above, the
    <code class="literal">swiftlm-drive-provision</code> process will assume that the
    drive has valuable data and will not use or modify the drive.
   </p></li></ul></div></div><div class="sect1" id="topic-rvj-21c-jt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Swift Proxy, Account, and Container (PAC) Cluster</span> <a title="Permalink" class="permalink" href="#topic-rvj-21c-jt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-creating_pac_cluster.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_pac_cluster.xml</li><li><span class="ds-label">ID: </span>topic-rvj-21c-jt</li></ul></div></div></div></div><p>
  If you already have a cluster with the server-role
  <code class="literal">SWPAC-ROLE</code> there is no need to proceed through these
  steps.
 </p><div class="sect2" id="steps"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps to Create a Swift Proxy, Account, and Container (PAC) Cluster</span> <a title="Permalink" class="permalink" href="#steps">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-creating_pac_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_pac_cluster.xml</li><li><span class="ds-label">ID: </span>steps</li></ul></div></div></div></div><p>
   To create a cluster for Swift proxy, account, and container (PAC) servers,
   you must identify the control plane and node type/role:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In the
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file, identify the control plane that the PAC servers are associated with.
    </p></li><li class="step "><p>
     Next, identify the node type/role used by the Swift PAC servers. In the
     following example, <code class="literal">server-role</code> is set to
     <span class="bold"><strong>SWPAC-ROLE</strong></span>.
    </p><p>
     Add an entry to the <code class="literal">clusters</code> item in the
     <code class="literal">control-plane</code> section.
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1

  . . .
  clusters:
  . . .
     - name: swpac1
       cluster-prefix: c2
       server-role: SWPAC-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - ntp-client
         - swift-ring-builder
         - swift-proxy
         - swift-account
         - swift-container
         - swift-client</pre></div><div id="id-1.3.4.9.12.3.3.2.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      Do not change the name of the cluster <code class="literal">swpac</code> as it may
      conflict with an existing cluster. Use a name such as
      <code class="literal">swpac1</code>, <code class="literal">swpac2</code>, or
      <code class="literal">swpac3</code>.
     </p></div></li><li class="step "><p>
     If you have more than three servers available that have the
     <code class="literal">SWPAC-ROLE</code> assigned to them, you must change
     <code class="literal">member-count</code> to match the number of servers.
    </p><p>
     For example, if you have four servers with a role of
     <code class="literal">SWPAC-ROLE</code>, then the <code class="literal">member-count</code>
     should be 4.
    </p></li></ol></div></div></div><div class="sect2" id="components"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service Components</span> <a title="Permalink" class="permalink" href="#components">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-creating_pac_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_pac_cluster.xml</li><li><span class="ds-label">ID: </span>components</li></ul></div></div></div></div><p>
   A Swift PAC server requires the following service components:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     ntp-client
    </p></li><li class="listitem "><p>
     swift-proxy
    </p></li><li class="listitem "><p>
     swift-account
    </p></li><li class="listitem "><p>
     swift-container
    </p></li><li class="listitem "><p>
     swift-ring-builder
    </p></li><li class="listitem "><p>
     swift-client
    </p></li></ul></div></div></div><div class="sect1" id="topic-jzk-q1c-jt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Object Server Resource Nodes</span> <a title="Permalink" class="permalink" href="#topic-jzk-q1c-jt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-creating_object_server_resource.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_object_server_resource.xml</li><li><span class="ds-label">ID: </span>topic-jzk-q1c-jt</li></ul></div></div></div></div><p>
  To create a resource node for Swift object servers, you must identify the
  control plane and node type/role:
 </p><div class="itemizedlist " id="ul-r4r-r1c-jt"><ul class="itemizedlist"><li class="listitem "><p>
    In the <code class="literal">data/control_plane.yml</code> file, identify the control
    plane that the object servers are associated with.
   </p></li><li class="listitem "><p>
    Next, identify the node type/role used by the Swift object servers. In the
    following example, <code class="literal">server-role</code> is set to
    <span class="bold"><strong>SWOBJ-ROLE</strong></span>:
   </p><p>
    Add an entry to the <code class="literal">resources</code> item in the
    <span class="bold"><strong>control-plane</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1
      region-name: region1
  . . .
  resources:
  . . .
  - name: swobj
    resource-prefix: swobj
    server-role: SWOBJ-ROLE
    allocation-policy: strict
    min-count: 0
    service-components:
    - ntp-client
    - swift-object</pre></div></li></ul></div><p>
  <span class="bold"><strong>Service Components</strong></span>
 </p><p>
  A Swift object server requires the following service components:
 </p><div class="itemizedlist " id="ul-fyb-51c-jt"><ul class="itemizedlist"><li class="listitem "><p>
    <code class="literal">ntp-client</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift-object</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift-client</code> is optional; installs the
    <code class="literal">python-swiftclient</code> package on the server.
   </p></li></ul></div><p>
  Resource nodes do not have a member count attribute. So the number of servers
  allocated with the <span class="bold"><strong>SWOBJ-ROLE</strong></span> is the number
  of servers in the <code class="literal">data/servers.yml</code> file with a server role
  of <span class="bold"><strong>SWOBJ-ROLE</strong></span>.
 </p></div><div class="sect1" id="topic-pcj-hzv-dt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding Swift Network and Service Requirements</span> <a title="Permalink" class="permalink" href="#topic-pcj-hzv-dt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-allocating_network.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_network.xml</li><li><span class="ds-label">ID: </span>topic-pcj-hzv-dt</li></ul></div></div></div></div><p>
  This topic describes Swift’s requirements for which service components must
  exist in the input model and how these relate to the network model. This
  information is useful if you are creating a cluster or resource node, or when
  defining the networks used by Swift. The network model allows many options
  and configurations. For smooth Swift operation, the following must be
  <span class="bold"><strong>true</strong></span>:
 </p><div class="itemizedlist " id="ul-ggm-rkc-jt"><ul class="itemizedlist"><li class="listitem "><p>
    The following services must have a
    <span class="bold"><strong>direct</strong></span>
    connection to the same network:
   </p><div class="itemizedlist " id="ul-x1c-5kc-jt"><ul class="itemizedlist"><li class="listitem "><p>
      <code class="literal">swift-proxy</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-account</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-container</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-object</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-ring-builder</code>
     </p></li></ul></div></li><li class="listitem "><p>
    The <code class="literal">swift-proxy</code> service must have a
    <span class="bold"><strong>direct</strong></span> connection to the same network as
    the <code class="literal">cluster-ip</code> service.
   </p></li><li class="listitem "><p>
    The memcached service must be configured on a cluster of the control plane.
    In small deployments, it is convenient to run it on the same cluster as the
    horizon service. For larger deployments, with many nodes running the
    <code class="literal">swift-proxy</code> service, it is better to
    <span class="bold"><strong>co-locate</strong></span>
    the <code class="literal">swift-proxy</code> and <code class="literal">memcached</code>
    services. The <code class="literal">swift-proxy</code> and
    <code class="literal">swift-container</code> services must have a
    <span class="bold"><strong>direct</strong></span> connection to the same network as
    the <code class="literal">memcached</code> service.
   </p></li><li class="listitem "><p>
    The <code class="literal"> swift-proxy</code> and
    <code class="literal">swift-ring-builder</code> service must be
    <span class="bold"><strong>co-located</strong></span> in the same cluster of the
    control plane.
   </p></li><li class="listitem "><p>
    The <code class="literal">ntp-client</code> service must be
    <span class="bold"><strong>present</strong></span> on all Swift nodes.
   </p></li></ul></div></div><div class="sect1" id="ring-specification"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding Swift Ring Specifications</span> <a title="Permalink" class="permalink" href="#ring-specification">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>ring-specification</li></ul></div></div></div></div><p>
  In Swift, the ring is responsible for mapping data on particular disks. There
  is a separate ring for account databases, container databases, and each
  object storage policy, but each ring works similarly. The
  <code class="literal">swift-ring-builder</code> utility is used to build and manage
  rings. This utility uses a builder file to contain ring information and
  additional data required to build future rings. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you will
  use the cloud model to specify how the rings are configured and used. This
  model is used to automatically invoke the
  <code class="literal">swift-ring-builder</code> utility as part of the deploy process.
  (Normally, you will not run the <code class="literal">swift-ring-builder</code> utility
  directly.)
 </p><p>
  The rings are specified in the input model using the
  <span class="bold"><strong>configuration-data</strong></span> key. The
  <code class="literal">configuration-data</code> in the
  <code class="literal">control-planes</code> definition is given a name that you will
  then use in the <code class="literal">swift_config.yml</code> file. If you have several
  control planes hosting Swift services, the ring specifications can use a
  shared <code class="literal">configuration-data</code> object, however it is considered
  best practice to give each Swift instance its own
  <code class="literal">configuration-data</code> object.
 </p><div class="sect2" id="input-model"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ring Specifications in the Input Model</span> <a title="Permalink" class="permalink" href="#input-model">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>input-model</li></ul></div></div></div></div><p>
   In most models, the ring-specification is mentioned in the
   <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code>
   file. For example:
  </p><div class="verbatim-wrap"><pre class="screen">configuration-data:
  - name: SWIFT-CONFIG-CP1
    services:
      - swift
    data:
      control_plane_rings:
        swift-zones:
          - id: 1
            server-groups:
              - AZ1
          - id: 2
            server-groups:
              - AZ2
          - id: 3
            server-groups:
              - AZ3
        rings:
          - name: account
            display-name: Account Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: container
            display-name: Container Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: object-0
            display-name: General
            default: yes
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3</pre></div><p>
   The above sample file shows that the rings are specified using the
   <code class="literal">configuration-data</code> object
   <span class="bold"><strong>SWIFT-CONFIG-CP1</strong></span> and has three
   rings as follows:
  </p><div class="itemizedlist " id="ul-hl3-q55-dt"><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Account ring</strong></span>: You must always specify a
     ring called <span class="bold"><strong>account</strong></span>. The account ring is
     used by Swift to store metadata about the projects in your system. In
     Swift, a Keystone project maps to a Swift account. The
     <code class="literal">display-name</code> is informational and not used.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Container ring</strong></span>:You must always specify a
     ring called <span class="bold"><strong>container</strong></span>. The
     <code class="literal">display-name</code> is informational and not used.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Object ring</strong></span>: This ring is also known as a
     storage policy. You must always specify a ring called
     <span class="bold"><strong>object-0</strong></span>. It is possible to have multiple
     object rings, which is known as <span class="emphasis"><em>storage policies</em></span>. The
     <code class="literal">display-name</code> is the name of the storage policy and can
     be used by users of the Swift system when they create containers. It
     allows them to specify the storage policy that the container uses. In the
     example, the storage policy is called
     <span class="bold"><strong>General</strong></span>. There are also two aliases for
     the storage policy name: <code class="literal">GeneralPolicy</code> and
     <code class="literal">AnotherAliasForGeneral</code>. In this example, you can use
     <code class="literal">General</code>, <code class="literal">GeneralPolicy</code>, or
     <code class="literal">AnotherAliasForGeneral</code> to refer to this storage policy.
     The aliases item is optional. The <code class="literal">display-name</code> is
     required.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Min-part-hours, partition-power,
     replication-policy</strong></span> and
     <span class="bold"><strong>replica-count</strong></span> are described in the
     following section.
    </p></li></ul></div></div><div class="sect2" id="ring-parameters"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replication Ring Parameters</span> <a title="Permalink" class="permalink" href="#ring-parameters">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>ring-parameters</li></ul></div></div></div></div><p>
   The ring parameters for traditional replication rings are defined as
   follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td><code class="literal">replica-count</code>
      </td><td>
       <p>
        Defines the number of copies of object created.
       </p>
       <p>
        Use this to control the degree of resiliency or availability. The
        <code class="literal">replica-count</code> is normally set to
        <code class="literal">3</code> (that means Swift will
        keep three copies of accounts, containers, or objects). As a best
        practice, do not set the value below <code class="literal">3</code>. To achieve
        higher resiliency, increase the value.
       </p>
      </td></tr><tr><td><code class="literal">min-part-hours</code>
      </td><td>
       <p>
        Changes the value used to decide when a given partition can be moved.
        This is the number of hours that the
        <code class="command">swift-ring-builder</code> tool will enforce between ring
        rebuilds. On a small system, this can be as low as
        <code class="literal">1</code> (one hour). The
        value can be different for each ring.
       </p>
       <p>
        In the example above, the <code class="literal">swift-ring-builder</code> will
        enforce a minimum of 16 hours between ring rebuilds. However, this time
        is system-dependent so you will be unable to determine the appropriate
        value for <code class="literal">min-part-hours</code> until you have more
        experience with your system.
       </p>
       <p>
        A value of <code class="literal">0</code> (zero) is not allowed.
       </p>
       <p>
        In prior releases, this parameter was called
        <code class="literal">min-part-time</code>. The older name is still supported,
        however do not specify both <code class="literal">min-part-hours</code> and
        <code class="literal">min-part-time</code> in the same files.
       </p>
      </td></tr><tr><td><code class="literal">partition-power</code>
      </td><td>The optimal value for this parameter is related to the number of disk drives that
              you allocate to Swift storage. As a best practice, you should use the same drives for
              both the account and container rings. In this case, the
                <code class="literal">partition-power</code> value should be the same. For more information,
              see <a class="xref" href="#selecting-partition-power" title="11.10.4. Selecting a Partition Power">Section 11.10.4, “Selecting a Partition Power”</a>.</td></tr><tr><td><code class="literal">replication-policy</code>
      </td><td>Specifies that a ring uses replicated storage. The duplicate copies of the object
              are created and stored on different disk drives. All replicas are identical. If one is
              lost or corrupted, the system automatically copies one of the remaining replicas to
              restore the missing replica.</td></tr><tr><td><code class="literal">default</code>
      </td><td>The default value in the above sample file of ring-specification is set to
                <span class="bold"><strong>yes</strong></span>, which means that the storage policy is enabled
              to store objects. For more information, see
              <a class="xref" href="#swift-storage-policies" title="11.11. Designing Storage Policies">Section 11.11, “Designing Storage Policies”</a>.</td></tr></tbody></table></div></div><div class="sect2" id="erasure-coded"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Erasure Coded Rings</span> <a title="Permalink" class="permalink" href="#erasure-coded">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>erasure-coded</li></ul></div></div></div></div><p>
   In the cloud model, a <code class="literal">ring-specification</code> is mentioned in
   the <code class="filename">~/openstack/my_cloud/definition/data/swift/rings.yml</code>
   file. A typical erasure coded ring in this file looks like this:
  </p><div class="verbatim-wrap"><pre class="screen">- name: object-1
  display-name: EC_ring
  default: no
  min-part-hours: 16
  partition-power: 12
  erasure-coding-policy:
    ec-type: jerasure_rs_vand
    ec-num-data-fragments: 10
    ec-num-parity-fragments: 4
    ec-object-segment-size: 1048576</pre></div><p>
   The additional parameters are defined as follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td>ec-type</td><td>
       <p>
        This is the particular erasure policy scheme that is being used. The
        supported ec_types in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> are:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">jerasure_rs_vand</code> =&gt; Vandermonde Reed-Solomon
          encoding, based on Jerasure
         </p></li></ul></div>
      </td></tr><tr><td>erasure-coding-policy</td><td>This line indicates that the object ring will be of type "erasure coding"</td></tr><tr><td>ec-num-data-fragments</td><td>This indicated the number of data fragments for an object in the ring.</td></tr><tr><td>ec-num-parity-fragments</td><td>This indicated the number of parity fragments for an object in the ring.</td></tr><tr><td>ec-object-segment-size</td><td>The amount of data that will be buffered up before feeding a segment into the
              encoder/decoder. The default value is 1048576.</td></tr></tbody></table></div><p>
   When using an erasure coded ring, the number of devices in the ring must be
   greater than or equal to the total number of fragments of an object. For
   example, if you define an erasure coded ring with 10 data fragments and 4
   parity fragments, there must be at least 14 (10+4) devices added to the
   ring.
  </p><p>
   When using erasure codes, for a PUT object to be successful it must store
   <code class="literal">ec_ndata + 1</code> fragment to achieve quorum. Where the number
   of data fragments (<code class="literal">ec_ndata</code>) is 10 then at least 11
   fragments must be saved for the object PUT to be successful. The 11
   fragments must be saved to different drives. To tolerate a single object
   server going down, say in a system with 3 object servers, each object server
   must have at least 6 drives assigned to the erasure coded storage policy. So
   with a single object server down, 12 drives are available between the
   remaining object servers. This allows an object PUT to save 12 fragments,
   one more than the minimum to achieve quorum.
  </p><p>
   Unlike replication rings, none of the erasure coded parameters may be edited
   after the initial creation. Otherwise there is potential for permanent loss
   of access to the data.
  </p><p>
   On the face of it, you would expect that an erasure coded configuration that
   uses a data to parity ratio of 10:4, that the data consumed storing the
   object is 1.4 times the size of the object just like the x3 replication
   takes x3 times the size of the data when storing the object. However, for
   erasure coding, this 10:4 ratio is not correct. The efficiency (that is how
   much storage is needed to store the object) is very poor for small objects
   and improves as the object size grows. However, the improvement is not
   linear. If all of your files are less than 32K in size, erasure coding will
   take more space to store than the x3 replication.
  </p></div><div class="sect2" id="selecting-partition-power"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting a Partition Power</span> <a title="Permalink" class="permalink" href="#selecting-partition-power">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>selecting-partition-power</li></ul></div></div></div></div><p>
   When storing an object, the object storage system hashes the name. This hash
   results in a hit on a partition (so a number of different object names
   result in the same partition number). Generally, the partition is mapped to
   available disk drives. With a replica count of 3, each partition is mapped
   to three different disk drives. The hashing algorithm used hashes over a
   fixed number of partitions. The partition-power attribute determines the
   number of partitions you have.
  </p><p>
   Partition power is used to distribute the data uniformly across drives in a
   Swift nodes. It also defines the storage cluster capacity. You must set the
   partition power value based on the total amount of storage you expect your
   entire ring to use.
  </p><p>
   You should select a partition power for a given ring that is appropriate to
   the number of disk drives you allocate to the ring for the following
   reasons:
  </p><div class="itemizedlist " id="ul-i4b-gv5-dt"><ul class="itemizedlist"><li class="listitem "><p>
     If you use a high partition power and have a few disk drives, each disk
     drive will have thousands of partitions. With too many partitions, audit
     and other processes in the Object Storage system cannot walk the
     partitions in a reasonable time and updates will not occur in a timely
     manner.
    </p></li><li class="listitem "><p>
     If you use a low partition power and have many disk drives, you will have
     tens (or maybe only one) partition on a drive. The Object Storage system
     does not use size when hashing to a partition - it hashes the name.
    </p><p>
     With many partitions on a drive, a large partition is cancelled out by a
     smaller partition so the overall drive usage is similar. However, with
     very small numbers of partitions, the uneven distribution of sizes can be
     reflected in uneven disk drive usage (so one drive becomes full while a
     neighboring drive is empty).
    </p></li></ul></div><p>
   An ideal number of partitions per drive is 100. If you know the number of
   drives, select a partition power that will give you approximately 100
   partitions per drive. Usually, you install a system with a specific number
   of drives and add drives as needed. However, you cannot change the value of
   the partition power. Hence you must select a value that is a compromise
   between current and planned capacity.
  </p><div id="id-1.3.4.9.15.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If you are installing a small capacity system and you need to grow to a
    very large capacity but you cannot fit within any of the ranges in the
    table, please seek help from <span class="phrase"><span class="phrase">Sales Engineering</span></span> to plan your system.
   </p></div><p>
   There are additional factors that can help mitigate the fixed nature of the
   partition power:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Account and container storage represents a small fraction (typically 1
     percent) of your object storage needs. Hence, you can select a smaller
     partition power (relative to object ring partition power) for the account
     and container rings.
    </p></li></ul></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For object storage, you can add additional storage policies (that is, another
     object ring). When you have reached capacity in an existing storage
     policy, you can add a new storage policy with a higher partition power
     (because you now have more disk drives in your system). This means that
     you can install your system using a small partition power appropriate to a
     small number of initial disk drives. Later, when you have many disk
     drives, the new storage policy can have a higher value appropriate to the
     larger number of drives.
    </p></li></ul></div><p>
   However, when you continue to add storage capacity, existing containers will
   continue to use their original storage policy. Hence, the additional objects
   must be added to new containers to take advantage of the new storage policy.
  </p><p>
   Use the following table to select an appropriate partition power for each
   ring. The partition power of a ring cannot be changed, so it is important to
   select an appropriate value. This table is based on a replica count of 3. If
   your replica count is different, or you are unable to find your system in
   the table, then see <a class="xref" href="#selecting-partition-power" title="11.10.4. Selecting a Partition Power">Section 11.10.4, “Selecting a Partition Power”</a> for
   information of selecting a partition power.
  </p><p>
   The table assumes that when you first deploy Swift, you have a small number
   of drives (the minimum column in the table), and later you add drives.
  </p><div id="id-1.3.4.9.15.7.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Use the total number of drives. For example, if you have three servers,
      each with two drives, the total number of drives is six.
     </p></li><li class="listitem "><p>
      The lookup should be done separately for each of the account, container
      and object rings. Since account and containers represent approximately 1
      to 2 percent of object storage, you will probably use fewer drives for
      the account and container rings (that is, you will have fewer proxy,
      account, and container (PAC) servers) so that your object rings may have
      a higher partition power.
     </p></li><li class="listitem "><p>
      The largest anticipated number of drives imposes a limit in the minimum
      drives you can have. (For more information, see
      <a class="xref" href="#selecting-partition-power" title="11.10.4. Selecting a Partition Power">Section 11.10.4, “Selecting a Partition Power”</a>.) This means that, if you
      anticipate significant growth, your initial system can be small, but
      under a certain limit. For example, if you determine that the maximum
      number of drives the system will grow to is 40,000, then use a partition
      power of 17 as listed in the table below. In addition, a minimum of 36
      drives is required to build the smallest system with this partition
      power.
     </p></li><li class="listitem "><p>
      The table assumes that disk drives are the same size. The actual size of
      a drive is not significant.
     </p></li></ul></div></div></div></div><div class="sect1" id="swift-storage-policies"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designing Storage Policies</span> <a title="Permalink" class="permalink" href="#swift-storage-policies">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-storage_policies.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-storage_policies.xml</li><li><span class="ds-label">ID: </span>swift-storage-policies</li></ul></div></div></div></div><p>
  Storage policies enable you to differentiate the way objects are stored.
 </p><p>
  Reasons to use storage policies include the following:
 </p><div class="itemizedlist " id="ul-q4t-cxd-jt"><ul class="itemizedlist"><li class="listitem "><p>
    Different types or classes of disk drive
   </p><p>
    You can use different drives to store various type of data. For example,
    you can use 7.5K RPM high-capacity drives for one type of data and fast SSD
    drives for another type of data.
   </p></li><li class="listitem "><p>
    Different redundancy or availability needs
   </p><p>
    You can define the redundancy and availability based on your requirement.
    You can use a replica count of 3 for "normal" data and a replica count of 4
    for "critical" data.
   </p></li><li class="listitem "><p>
    Growing of cluster capacity
   </p><p>
    If the storage cluster capacity grows beyond the recommended partition
    power as described in <a class="xref" href="#ring-specification" title="11.10. Understanding Swift Ring Specifications">Section 11.10, “Understanding Swift Ring Specifications”</a>.
   </p></li><li class="listitem "><p>
    Erasure-coded storage and replicated storage
   </p><p>
    If you use erasure-coded storage for some objects and replicated storage
    for other objects.
   </p></li></ul></div><p>
  Storage policies are implemented on a per-container basis. If you want a
  non-default storage policy to be used for a new container, you can explicitly
  specify the storage policy to use when you create the container. You can
  change which storage policy is the default. However, this does not affect
  existing containers. Once the storage policy of a container is set, the
  policy for that container cannot be changed.
 </p><p>
  The disk drives used by storage policies can overlap or be distinct. If the
  storage policies overlap (that is, have disks in common between two storage
  policies), it is recommended to use the same set of disk drives for both
  policies. But in the case where there is a partial overlap in disk drives,
  because one storage policy receives many objects, the drives that are common
  to both policies must store more objects than drives that are only allocated
  to one storage policy. This can be appropriate for a situation where the
  overlapped disk drives are larger than the non-overlapped drives.

 </p><div class="sect2" id="id-1.3.4.9.16.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying Storage Policies</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.16.7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-storage_policies.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-storage_policies.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There are two places where storage policies are specified in the input
   model:
  </p><div class="itemizedlist " id="ul-psf-lg2-jt"><ul class="itemizedlist"><li class="listitem "><p>
     The attribute of the storage policy is specified in ring-specification in
     the <code class="literal">data/swift/rings.yml</code> file.
    </p></li><li class="listitem "><p>
     When associating disk drives with specific rings in a disk model. This
     specifies which drives and nodes use the storage policy. In other word
     words, where data associated with a storage policy is stored.
    </p></li></ul></div><p>
   A storage policy is specified similar to other rings. However, the following
   features are unique to storage policies:
  </p><div class="itemizedlist " id="ul-k3j-ng2-jt"><ul class="itemizedlist"><li class="listitem "><p>
     Storage policies are applicable to object rings only. The account or
     container rings cannot have storage policies.
    </p></li><li class="listitem "><p>
     There is a format for the ring name:
     object-<em class="replaceable ">index</em>, where index is a number in the
     range 0 to 9 (in this release). For example: object-0.
    </p></li><li class="listitem "><p>
     The object-0 ring must always be specified.
    </p></li><li class="listitem "><p>
     Once a storage policy is deployed, it should never be deleted. You can
     remove all disk drives for the storage policy, however the ring
     specification itself cannot be deleted.
    </p></li><li class="listitem "><p>
     You can use the <code class="literal">display-name</code> attribute when creating a
     container to indicate which storage policy you want to use for that
     container.
    </p></li><li class="listitem "><p>
     One of the storage policies can be the default policy. If you do not
     specify the storage policy then the object created in new container uses
     the default storage policy.
    </p></li><li class="listitem "><p>
     If you change the default, only containers created later will have that
     changed default policy.
    </p></li></ul></div><p>
   The following example shows three storage policies in use. Note that the
   third storage policy example is an erasure coded ring.
  </p><div class="verbatim-wrap"><pre class="screen">rings:
. . .
- name: object-0
  display-name: General
  default: no
  min-part-hours: 16
  partition-power: 12
  replication-policy:
      replica-count: 3
- name: object-1
  display-name: Data
  default: yes
  min-part-hours: 16
  partition-power: 20
  replication-policy:
      replica-count: 3
- name: object-2
  display-name: Archive
  default: no
  min-part-hours: 16
  partition-power: 20
  erasure-coded-policy:
    ec-type: jerasure_rs_vand
    ec-num-data-fragments: 10
    ec-num-parity-fragments: 4
    ec-object-segment-size: 1048576</pre></div></div></div><div class="sect1" id="designing-swift-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designing Swift Zones</span> <a title="Permalink" class="permalink" href="#designing-swift-zones">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-swift_zones.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_zones.xml</li><li><span class="ds-label">ID: </span>designing-swift-zones</li></ul></div></div></div></div><p>
  The concept of Swift zones allows you to control the placement of replicas on
  different groups of servers. When constructing rings and allocating replicas
  to specific disk drives, Swift will, where possible, allocate replicas using
  the following hierarchy so that the greatest amount of resiliency is achieved
  by avoiding single points of failure:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Swift will place each replica on a different disk drive within the same
    server.
   </p></li><li class="listitem "><p>
    Swift will place each replica on a different server.
   </p></li><li class="listitem "><p>
    Swift will place each replica in a different Swift zone.
   </p></li></ul></div><p>
  If you have three servers and a replica count of three, it is easy for Swift
  to place each replica on a different server. If you only have two servers
  though, Swift will place two replicas on one server (different drives on the
  server) and one copy on the other server.
 </p><p>
  With only three servers there is no need to use the Swift zone concept.
  However, if you have more servers than your replica count, the Swift zone
  concept can be used to control the degree of resiliency. The following table
  shows how data is placed and explains what happens under various failure
  scenarios. In all cases, a replica count of three is assumed and that there
  are a total of six servers.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Number of Swift Zones</th><th>Replica Placement</th><th>Failure Scenarios</th><th>Details</th></tr></thead><tbody><tr><td rowspan="3">
      One (all servers in the same zone)
     </td><td rowspan="3">
      Replicas are placed on different servers. For any given object, you
      have no control over which servers the replicas are placed on.
     </td><td>
      One server fails
     </td><td>
      You are guaranteed that there are two other replicas.
     </td></tr><tr><td>Two servers fail</td><td>You are guaranteed that there is one remaining replica.</td></tr><tr><td>Three servers fail</td><td>
      1/3 of the objects cannot be accessed. 2/3 of the objects have three
      replicas.
     </td></tr><tr><td>Two (three servers in each Swift zone)</td><td>
      Half the objects have two replicas in Swift zone 1 with one replica in
      Swift zone The other objects are reversed, with one replica in Swift
      zone 1 and two replicas in Swift zone 2.
     </td><td>One Swift zone fails</td><td>
      You are guaranteed to have at least one replica. Half the objects have
      two remaining replicas and the other half have a single replica.
     </td></tr><tr><td rowspan="2">Three (two servers in each Swift zone)</td><td rowspan="2">
      Each zone contains a replica. For any given object, there is a replica
      in each Swift zone.
     </td><td>One Swift zone fails</td><td>You are guaranteed to have two replicas of every object.</td></tr><tr><td>Two Swift zones fail</td><td>You are guaranteed to have one replica of every object.</td></tr></tbody></table></div><p>
  The following sections show examples of how to specify the Swift zones in
  your input model.
 </p><div class="sect2" id="server-groups"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Server Groups to Specify Swift Zones</span> <a title="Permalink" class="permalink" href="#server-groups">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-swift_zones.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_zones.xml</li><li><span class="ds-label">ID: </span>server-groups</li></ul></div></div></div></div><p>
   Swift zones are specified in the ring specifications using the server group
   concept. To define a Swift zone, you specify:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     An id - this is the Swift zone number
    </p></li><li class="listitem "><p>
     A list of associated server groups
    </p></li></ul></div><p>
   Server groups are defined in your input model. The example input models
   typically define a number of server groups. You can use these pre-defined
   server groups or create your own.
  </p><p>
   For example, the following three models use the example server groups
   <code class="literal">CLOUD</code>, <code class="literal">AZ1</code>, <code class="literal">AZ2</code> and
   <code class="literal">AZ3</code>. Each of these examples achieves the same effect –
   creating a single Swift zone.
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
              - region: region1
              swift-zones:
              - id: 1
              server-groups:
              - CLOUD
              rings:
              …</pre></div><div class="verbatim-wrap"><pre class="screen">ring-specifications:
              - region: region1
              swift-zones:
              - id: 1
              server-groups:
              - AZ1
              - AZ2
              - AZ3
              rings:
              …</pre></div><div class="verbatim-wrap"><pre class="screen">server-groups:
              - name: ZONE_ONE
              server-groups:
              - AZ1
              - AZ2
              - AZ3
              ring-specifications:
              - region: region1
              swift-zones:
              - id: 1
              server-groups:
              - ZONE_ONE
              rings:
              …</pre></div><p>
   Alternatively, if you omit the <code class="literal">swift-zones</code> specification,
   a single Swift zone is used by default for all servers.
  </p><p>
   In the following example, three Swift zones are specified and mapped to the
   same availability zones that Nova uses (assuming you are using one of the
   example input models):
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
      - region: region1
      swift-zones:
      - id: 1
      server-groups:
      - AZ1
      - id: 2
      server-groups:
      - AZ2
      - id: 3
      server-groups:
      - AZ3</pre></div><p>
   In this example, it shows a datacenter with four availability zones which
   are mapped to two Swift zones. This type of setup may be used if you had two
   buildings where each building has a duplicated network infrastructure:
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
      - region: region1
      swift-zones:
      - id: 1
      server-groups:
      - AZ1
      - AZ2
      - id: 2
      server-groups:
      - AZ3
      - AZ4</pre></div></div><div class="sect2" id="zones-ring-level"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying Swift Zones at Ring Level</span> <a title="Permalink" class="permalink" href="#zones-ring-level">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-swift_zones.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_zones.xml</li><li><span class="ds-label">ID: </span>zones-ring-level</li></ul></div></div></div></div><p>
   Usually, you would use the same Swift zone layout for all rings in your
   system. However, it is possible to specify a different layout for a given
   ring. The following example shows that the account, container and object-0
   rings have two zones, but the object-1 ring has a single zone.
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
        - region: region1
        swift-zones:
        - id: 1
        server-groups:
        - AZ1
        - id: 2
        server-groups:
        - AZ2
        rings
        - name: account
        …
        - name: container
        …
        - name: object-0
        …
        - name: object-1
        swift-zones:
        - id: 1
        server-groups:
        - CLOUD
        …</pre></div></div></div><div class="sect1" id="topic-rdf-hkp-rt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing Swift Service Configuration Files</span> <a title="Permalink" class="permalink" href="#topic-rdf-hkp-rt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span>topic-rdf-hkp-rt</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> enables you to modify various Swift service configuration
  files. The following Swift service configuration files are located on the
  Cloud Lifecycle Manager in the <code class="filename">~/openstack/my_cloud/config/swift/</code> directory:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <code class="literal">account-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">container-reconciler.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">container-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">container-sync-realms.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">object-expirer.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">object-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">proxy-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">rsyncd.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift-recon.j2</code>
   </p></li></ul></div><p>
  There are many configuration options that can be set or changed, including
  <span class="bold"><strong>container rate limit</strong></span>
  and <span class="bold"><strong>logging level</strong></span>:
 </p><div class="sect2" id="configuring-swift-contianer-rate-limit"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Swift Container Rate Limit</span> <a title="Permalink" class="permalink" href="#configuring-swift-contianer-rate-limit">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span>configuring-swift-contianer-rate-limit</li></ul></div></div></div></div><p>
   The Swift container rate limit allows you to limit the number of
   <code class="literal">PUT</code> and <code class="literal">DELETE</code> requests of an object
   based on the number of objects in a container. For example, suppose the
   <code class="literal">container_ratelimit_x = r </code>. It means that for containers
   of size <code class="literal">x</code>, limit requests per second to
   <code class="literal">r</code>.
  </p><p>
   To enable container rate limiting:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the <code class="literal">DEFAULT</code> section of
     <code class="filename">~/openstack/my_cloud/config/swift/proxy-server.conf.j2</code>:
    </p><div class="verbatim-wrap"><pre class="screen">container_ratelimit_0 = 100
container_ratelimit_1000000 = 100
container_ratelimit_5000000 = 50</pre></div><p>
     This will set the <code class="literal">PUT</code> and <code class="literal">DELETE</code>
     object rate limit to 100 requests per second for containers with up to
     1,000,000 objects. Also, the <code class="literal">PUT</code> and
     <code class="literal">DELETE</code> rate for containers with between 1,000,000 and
     5,000,000 objects will vary linearly from between 100 and 50 requests per
     second as the container object count increases.
    </p></li><li class="step "><p>
     Commit your changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git commit -m "<em class="replaceable ">COMMIT_MESSAGE</em>" \
~/openstack/my_cloud/config/swift/proxy-server.conf.j2</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">swift-reconfigure.yml</code> playbook to reconfigure
     the Swift servers:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="modifying-swift-account-server-logging-level"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Swift Account Server Logging Level</span> <a title="Permalink" class="permalink" href="#modifying-swift-account-server-logging-level">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span>modifying-swift-account-server-logging-level</li></ul></div></div></div></div><p>
   By default the Swift logging level is set to <code class="literal">INFO</code>. As a
   best practice, do not set the log level to DEBUG for a long period of time.
   Use it for troubleshooting issues and then change it back to INFO.
  </p><p>
   Perform the following steps to set the logging level of the
   <code class="literal">account-server</code> to <code class="literal">DEBUG</code>:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the <code class="literal">DEFAULT</code> section of
     <code class="filename">~/openstack/my_cloud/config/swift/account-server.conf.j2</code>:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT] . . log_level = DEBUG</pre></div></li><li class="step "><p>
     Commit your changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git commit -m "<em class="replaceable ">COMMIT_MESSAGE</em>" \
~/openstack/my_cloud/config/swift/account-server.conf.j2</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">swift-reconfigure.yml</code> playbook to reconfigure
     the Swift servers:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.4.9.18.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.18.7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For more information, see:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 12 “Managing Monitoring, Logging, and Usage Reporting”, Section 12.2 “Centralized Logging Service”, Section 12.2.5 “Configuring Centralized Logging”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 12 “Managing Monitoring, Logging, and Usage Reporting”, Section 12.2 “Centralized Logging Service”</span>
    </p></li></ul></div></div></div></div><div class="chapter " id="alternative-configurations"><div class="titlepage"><div><div><h2 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Configurations</span> <a title="Permalink" class="permalink" href="#alternative-configurations">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-alternative-alternative_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-alternative_configurations.xml</li><li><span class="ds-label">ID: </span>alternative-configurations</li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.10.2.1">#</a></h6></div><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> there are alternative configurations that we recommend
    for specific purposes.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#standalone-deployer"><span class="number">12.1 </span><span class="name">Using a Dedicated Cloud Lifecycle Manager Node</span></a></span></dt><dt><span class="section"><a href="#without-dvr"><span class="number">12.2 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR</span></a></span></dt><dt><span class="section"><a href="#without-l3agent"><span class="number">12.3 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only</span></a></span></dt><dt><span class="section"><a href="#twosystems"><span class="number">12.4 </span><span class="name">Considerations When Installing Two Systems on One Subnet</span></a></span></dt></dl></div></div><div class="sect1" id="standalone-deployer"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using a Dedicated Cloud Lifecycle Manager Node</span> <a title="Permalink" class="permalink" href="#standalone-deployer">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-alternative-standalone_deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-standalone_deployer.xml</li><li><span class="ds-label">ID: </span>standalone-deployer</li></ul></div></div></div></div><p>
  All of the example configurations included host the Cloud Lifecycle Manager on the first
  Control Node. It is also possible to deploy this service on a dedicated
  node. One use case for wanting to run the dedicated Cloud Lifecycle Manager is to be able to
  test the deployment of different configurations without having to re-install
  the first server. Some administrators prefer the additional security of
  keeping all of the configuration data on a separate server from those that
  users of the cloud connect to (although all of the data can be encrypted and
  SSH keys can be password protected).
 </p><p>
  Here is a graphical representation of this setup:
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-examples-entry_scale_kvm.png" target="_blank"><img src="images/media-examples-entry_scale_kvm.png" width="" /></a></div></div><div class="sect2" id="sec-specify-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying a dedicated Cloud Lifecycle Manager in your input model</span> <a title="Permalink" class="permalink" href="#sec-specify-lifecycle-manager">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-alternative-standalone_deployer.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-standalone_deployer.xml</li><li><span class="ds-label">ID: </span>sec-specify-lifecycle-manager</li></ul></div></div></div></div><p>
   To specify a dedicated Cloud Lifecycle Manager in your input model, make the following edits
   to your configuration files.
  </p><div id="id-1.3.4.10.3.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    The indentation of each of the input files is important and will cause
    errors if not done correctly. Use the existing content in each of these
    files as a reference when adding additional content for your Cloud Lifecycle Manager.
   </p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Update <code class="filename">control_plane.yml</code> to add the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update <code class="filename">server_roles.yml</code> to add the Cloud Lifecycle Manager role.
    </p></li><li class="listitem "><p>
     Update <code class="filename">net_interfaces.yml</code> to add the interface
     definition for the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Create a <code class="filename">disks_lifecycle_manager.yml</code> file to define
     the disk layout for the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update <code class="filename">servers.yml</code> to add the dedicated Cloud Lifecycle Manager node.
    </p></li></ul></div><p>
   <code class="filename">Control_plane.yml</code>: The snippet below shows the addition
   of a single node cluster into the control plane to host the Cloud Lifecycle Manager service.
   Note that, in addition to adding the new cluster, you also have to remove
   the Cloud Lifecycle Manager component from the <code class="literal">cluster1</code> in the examples:
  </p><div class="verbatim-wrap"><pre class="screen">  clusters:
<span class="bold"><strong>     - name: cluster0
       cluster-prefix: c0
       server-role: LIFECYCLE-MANAGER-ROLE
       member-count: 1
       allocation-policy: strict
       service-components:
         - lifecycle-manager</strong></span>
         - ntp-client
     - name: cluster1
       cluster-prefix: c1
       server-role: CONTROLLER-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - lifecycle-manager
         - ntp-server
         - tempest</pre></div><p>
   This specifies a single node of role
   <code class="literal">LIFECYCLE-MANAGER-ROLE</code> hosting the Cloud Lifecycle Manager.
  </p><p>
   <code class="filename">Server_roles.yml</code>: The snippet below shows the insertion
   of the new server roles definition:
  </p><div class="verbatim-wrap"><pre class="screen">   server-roles:

<span class="bold"><strong>      - name: LIFECYCLE-MANAGER-ROLE
        interface-model: LIFECYCLE-MANAGER-INTERFACES
        disk-model: LIFECYCLE-MANAGER-DISKS</strong></span>

      - name: CONTROLLER-ROLE</pre></div><p>
   This defines a new server role which references a new interface-model and
   disk-model to be used when configuring the server.
  </p><p>
   <code class="filename">net-interfaces.yml</code>: The snippet below shows the
   insertion of the network-interface info:
  </p><div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>    - name: LIFECYCLE-MANAGER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                 mode: active-backup
                 miimon: 200
                 primary: hed3
             provider: linux
             devices:
                 - name: hed3
                 - name: hed4
          network-groups:
             - MANAGEMENT</strong></span></pre></div><p>
   This assumes that the server uses the same physical networking layout as the
   other servers in the example.
   
   
  </p><p>
   <code class="filename">disks_lifecycle_manager.yml</code>: In the examples,
   disk-models are provided as separate files (this is just a convention, not a
   limitation) so the following should be added as a new file named
   <code class="filename">disks_lifecycle_manager.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">---
   product:
      version: 2

   disk-models:
<span class="bold"><strong>   - name: LIFECYCLE-MANAGER-DISKS
     # Disk model to be used for Cloud Lifecycle Managers nodes
     # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5

     volume-groups:
       - name: ardana-vg
         physical-volumes:
           - /dev/sda_root

       logical-volumes:
       # The policy is not to consume 100% of the space of each volume group.
       # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 80%
            fstype: ext4
            mount: /
          - name: crash
            size: 15%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
              name: os</strong></span></pre></div><p>
   <code class="filename">Servers.yml</code>: The snippet below shows the insertion of an
   additional server used for hosting the Cloud Lifecycle Manager. Provide the address
   information here for the server you are running on, that is, the node where
   you have installed the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO.
  </p><div class="verbatim-wrap"><pre class="screen">  servers:
     # NOTE: Addresses of servers need to be changed to match your environment.
     #
     #       Add additional servers as required

<span class="bold"><strong>     #Lifecycle-manager
     - id: lifecycle-manager
       ip-addr: <em class="replaceable ">YOUR IP ADDRESS HERE</em>
       role: LIFECYCLE-MANAGER-ROLE
       server-group: RACK1
       nic-mapping: HP-SL230-4PORT
       mac-addr: 8c:dc:d4:b5:c9:e0
       # ipmi information is not needed </strong></span>

     # Controllers
     - id: controller1
       ip-addr: 192.168.10.3
       role: CONTROLLER-ROLE</pre></div><div id="id-1.3.4.10.3.5.18" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    With a stand-alone deployer, the OpenStack CLI and other clients will not
    be installed automatically. You need to install <span class="productname">OpenStack</span> clients to get the
    desired <span class="productname">OpenStack</span> capabilities. For more information and installation
    instructions, consult <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 28 “Installing OpenStack Clients”</span>.
   </p></div></div></div><div class="sect1" id="without-dvr"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR</span> <a title="Permalink" class="permalink" href="#without-dvr">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-alternative-without_dvr.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-without_dvr.xml</li><li><span class="ds-label">ID: </span>without-dvr</li></ul></div></div></div></div><p>
  By default in the KVM model, the Neutron service utilizes distributed routing
  (DVR). This is the recommended setup because it allows for high availability.
  However, if you would like to disable this feature, here are the steps to
  achieve this.
 </p><p>
  On your Cloud Lifecycle Manager, make the following changes:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = {{ router_distributed }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = True</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/l3_agent.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">agent_mode = {{ neutron_l3_agent_mode }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">agent_mode = legacy</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    file, remove the following values from the Compute resource
    <code class="literal">service-components</code> list:
   </p><div class="verbatim-wrap"><pre class="screen">- neutron-l3-agent
   - neutron-metadata-agent</pre></div><div id="id-1.3.4.10.4.4.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
     If you fail to remove the above values from the Compute resource
     service-components list from file
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>,
     you will end up with routers (non_DVR routers) being deployed in the
     compute host, even though the lifecycle manager is configured for
     non_distributed routers.
    </p></div></li><li class="step "><p>
    Commit your changes to your local git repository:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Run the ready deployment playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Continue installation. More information on cloud deployments are available
    in the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 7 “Overview”</span>
   </p></li></ol></div></div></div><div class="sect1" id="without-l3agent"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only</span> <a title="Permalink" class="permalink" href="#without-l3agent">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-alternative-without_l3agent.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-without_l3agent.xml</li><li><span class="ds-label">ID: </span>without-l3agent</li></ul></div></div></div></div><p>
  Another option for configuring Neutron is to use provider VLANs and physical
  routers only, here are the steps to achieve this.
 </p><p>
  On your Cloud Lifecycle Manager, make the following changes:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = {{ router_distributed }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = True</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/dhcp_agent.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = True</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    file, remove the following values from the Compute resource
    <code class="literal">service-components</code> list:
   </p><div class="verbatim-wrap"><pre class="screen">- neutron-l3-agent
  - neutron-metadata-agent</pre></div></li></ol></div></div></div><div class="sect1" id="twosystems"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Considerations When Installing Two Systems on One Subnet</span> <a title="Permalink" class="permalink" href="#twosystems">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/planning-architecture-alternative-twosystems.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-twosystems.xml</li><li><span class="ds-label">ID: </span>twosystems</li></ul></div></div></div></div><p>
  If you wish to install two separate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> systems using a single
  subnet, you will need to consider the following notes.
 </p><p>
  The <code class="literal">ip_cluster</code> service includes the
  <code class="literal">keepalived</code> daemon which maintains virtual IPs (VIPs) on
  cluster nodes. In order to maintain VIPs, it communicates between cluster
  nodes over the VRRP protocol.
 </p><p>
  A VRRP virtual routerid identifies a particular VRRP cluster and must be
  unique for a subnet. If you have two VRRP clusters with the same virtual
  routerid, causing a clash of VRRP traffic, the VIPs are unlikely to be up or
  pingable and you are likely to get the following signature in your
  <code class="literal">/etc/keepalived/keepalived.log</code>:
 </p><div class="verbatim-wrap"><pre class="screen">Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: ip address
  associated with VRID not present in received packet : 10.2.1.11
Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: one or more VIP
  associated with VRID mismatch actual MASTER advert
Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: bogus VRRP packet
  received on br-bond0 !!!
Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: VRRP_Instance(VI_2)
  ignoring received advertisment...</pre></div><p>
  To resolve this issue, our recommendation is to install your separate
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> systems with VRRP traffic on different subnets.
 </p><p>
  If this is not possible, you may also assign a unique routerid to your
  separate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> system by changing the
  <code class="literal">keepalived_vrrp_offset</code> service configurable. The routerid
  is currently derived using the <code class="literal">keepalived_vrrp_index</code> which
  comes from a configuration processor variable and the
  <code class="literal">keepalived_vrrp_offset</code>.
 </p><p>
  For example,
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to your Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Edit your
    <code class="filename">~/openstack/my_cloud/config/keepalived/defaults.yml</code>
    file and change the value of the following line:
   </p><div class="verbatim-wrap"><pre class="screen">keepalived_vrrp_offset: 0</pre></div><p>
    Change the off value to a number that uniquely identifies a separate vrrp
    cluster. For example:
   </p><p>
    <code class="literal">keepalived_vrrp_offset: 0</code> for the 1st vrrp cluster on
    this subnet.
   </p><p>
    <code class="literal">keepalived_vrrp_offset: 1</code> for the 2nd vrrp cluster on
    this subnet.
   </p><p>
    <code class="literal">keepalived_vrrp_offset: 2</code> for the 3rd vrrp cluster on
    this subnet.
   </p><div id="symlinks" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     You should be aware that the files in the
     <code class="filename">~/openstack/my_cloud/config/</code> directory are symlinks
     to the <code class="filename">~/openstack/ardana/ansible/</code> directory. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -al ~/openstack/my_cloud/config/keepalived/defaults.yml
lrwxrwxrwx 1 stack stack 55 May 24 20:38 /var/lib/ardana/openstack/my_cloud/config/keepalived/defaults.yml -&gt;
    ../../../ardana/ansible/roles/keepalived/defaults/main.yml</pre></div><p>
     If you are using a tool like <code class="literal">sed</code> to make edits to files
     in this directory, you might break the symbolic link and create a new copy
     of the file. To maintain the link, you will need to force
     <code class="literal">sed</code> to follow the link:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sed -i <span class="bold"><strong>--follow-symlinks</strong></span> \
  's$keepalived_vrrp_offset: 0$keepalived_vrrp_offset: 2$' \
  ~/openstack/my_cloud/config/keepalived/defaults.yml</pre></div><p>
     Alternatively, directly edit the target of the link
     <code class="filename">~/openstack/ardana/ansible/roles/keepalived/defaults/main.yml</code>.
    </p></div></li><li class="step "><p>
    Commit your configuration to the Git repository (see
    <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "changing Admin password"</pre></div></li><li class="step "><p>
    Run the configuration processor with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Use the playbook below to create a deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    If you are making this change after your initial install, run the following
    reconfigure playbook to make this change in your environment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-reconfigure.yml</pre></div></li></ol></div></div></div></div></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2022 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>