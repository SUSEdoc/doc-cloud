<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Deploying With Crowbar | SUSE OpenStack Cloud Crowbar 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" />
<meta name="title" content="Deploying With Crowbar | SUSE OpenStack Cloud Crowbar 8" />
<meta name="description" content="SUSE® OpenStack Cloud Crowbar is an open source software solution that provides the fundamental capabilities to deploy and manage a cloud infrastructure based …" />
<meta name="product-name" content="SUSE OpenStack Cloud Crowbar" />
<meta name="product-number" content="8" />
<meta name="book-title" content="Deploying With Crowbar" />
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" />
<meta name="tracker-type" content="bsc" />
<meta name="tracker-bsc-component" content="Documentation" />
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" />
<meta property="og:title" content="Deploying With Crowbar | SUSE OpenStack Cloud Crowbar 8" />
<meta property="og:description" content="SUSE® OpenStack Cloud Crowbar is an open source software solution that provides the fundamental capabilities to deploy and manage a cloud infrastructure based …" />
<meta property="og:type" content="article" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Deploying With Crowbar | SUSE OpenStack Cloud Crowbar 8" />
<meta name="twitter:description" content="SUSE® OpenStack Cloud Crowbar is an open source software solution that provides the fundamental capabilities to deploy and manage a cloud infrastructure based …" />

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#book-deployment" accesskey="c"><span class="single-contents-icon"></span>Deploying With Crowbar</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#book-deployment" accesskey="c"><span class="single-contents-icon"></span>Show Contents: Deploying With Crowbar</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#pre-cloud-deploy"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="#part-depl-intro"><span class="number">I </span><span class="name">Architecture and Requirements</span></a><ol><li class="inactive"><a href="#cha-depl-arch"><span class="number">1 </span><span class="name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></a></li><li class="inactive"><a href="#cha-depl-req"><span class="number">2 </span><span class="name">Considerations and Requirements</span></a></li></ol></li><li class="inactive"><a href="#part-depl-admserv"><span class="number">II </span><span class="name">Setting Up the Administration Server</span></a><ol><li class="inactive"><a href="#cha-depl-adm-inst"><span class="number">3 </span><span class="name">Installing the Administration Server</span></a></li><li class="inactive"><a href="#app-deploy-smt"><span class="number">4 </span><span class="name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></a></li><li class="inactive"><a href="#cha-depl-repo-conf"><span class="number">5 </span><span class="name">Software Repository Setup</span></a></li><li class="inactive"><a href="#sec-depl-adm-inst-network"><span class="number">6 </span><span class="name">Service Configuration:  Administration Server Network Configuration</span></a></li><li class="inactive"><a href="#sec-depl-adm-inst-crowbar"><span class="number">7 </span><span class="name">Crowbar Setup</span></a></li><li class="inactive"><a href="#sec-depl-adm-start-crowbar"><span class="number">8 </span><span class="name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></a></li><li class="inactive"><a href="#sec-depl-adm-crowbar-extra-features"><span class="number">9 </span><span class="name">Customizing Crowbar</span></a></li></ol></li><li class="inactive"><a href="#part-depl-ostack"><span class="number">III </span><span class="name">Setting Up <span class="productname">OpenStack</span> Nodes and Services</span></a><ol><li class="inactive"><a href="#cha-depl-crowbar"><span class="number">10 </span><span class="name">The Crowbar Web Interface</span></a></li><li class="inactive"><a href="#cha-depl-inst-nodes"><span class="number">11 </span><span class="name">Installing the <span class="productname">OpenStack</span> Nodes</span></a></li><li class="inactive"><a href="#cha-depl-ostack"><span class="number">12 </span><span class="name">Deploying the <span class="productname">OpenStack</span> Services</span></a></li><li class="inactive"><a href="#sec-deploy-policy-json"><span class="number">13 </span><span class="name">Limiting Users' Access Rights</span></a></li><li class="inactive"><a href="#cha-depl-ostack-configs"><span class="number">14 </span><span class="name">Configuration Files for <span class="productname">OpenStack</span> Services</span></a></li><li class="inactive"><a href="#install-heat-templates"><span class="number">15 </span><span class="name">Installing SUSE CaaS Platform Heat Templates</span></a></li></ol></li><li class="inactive"><a href="#part-depl-nostack"><span class="number">IV </span><span class="name">Setting Up Non-<span class="productname">OpenStack</span> Services</span></a><ol><li class="inactive"><a href="#cha-depl-nostack"><span class="number">16 </span><span class="name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></a></li></ol></li><li class="inactive"><a href="#part-depl-maintenance"><span class="number">V </span><span class="name">Maintenance and Support</span></a><ol><li class="inactive"><a href="#cha-depl-maintenance"><span class="number">17 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Maintenance</span></a></li><li class="inactive"><a href="#self-assign-certs"><span class="number">18 </span><span class="name">Generate SUSE <span class="productname">OpenStack</span> Cloud Self Signed Certificate</span></a></li><li class="inactive"><a href="#cha-deploy-logs"><span class="number">19 </span><span class="name">Log Files</span></a></li><li class="inactive"><a href="#cha-depl-trouble"><span class="number">20 </span><span class="name">Troubleshooting and Support</span></a></li></ol></li><li class="inactive"><a href="#part-depl-poc"><span class="number">VI </span><span class="name">Proof of Concepts Deployments</span></a><ol><li class="inactive"><a href="#cha-deploy-poc"><span class="number">21 </span><span class="name">Building a SUSE <span class="productname">OpenStack</span> Cloud Test lab</span></a></li></ol></li><li class="inactive"><a href="#app-deploy-vmware"><span class="number">A </span><span class="name">VMware vSphere Installation Instructions</span></a></li><li class="inactive"><a href="#app-deploy-cisco"><span class="number">B </span><span class="name">Using Cisco Nexus Switches with Neutron</span></a></li><li class="inactive"><a href="#app-deploy-docupdates"><span class="number">C </span><span class="name">Documentation Updates</span></a></li><li class="inactive"><a href="#gl-cloud"><span class="number"> </span><span class="name">Glossary of Terminology and Product Names</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="book" id="book-deployment" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></h6><div><h1 class="title"><em class="citetitle ">Deploying With Crowbar</em></h1></div><div class="date"><span class="imprint-label">Publication Date: </span>
April 04, 2022
</div></div></div><div class="toc"><dl><dt><span class="preface"><a href="#pre-cloud-deploy"><span class="name">About This Guide</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#id-1.3.2.9"><span class="name">Available Documentation</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.2.10"><span class="name">Feedback</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.2.11"><span class="name">Documentation Conventions</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.2.12"><span class="name">About the Making of This Manual</span></a></span></dt></dl></dd><dt><span class="part"><a href="#part-depl-intro"><span class="number">I </span><span class="name">Architecture and Requirements</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-depl-arch"><span class="number">1 </span><span class="name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-arch-components-admin"><span class="number">1.1 </span><span class="name">The Administration Server</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-control"><span class="number">1.2 </span><span class="name">The Control Node(s)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-compute"><span class="number">1.3 </span><span class="name">The Compute Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-storage"><span class="number">1.4 </span><span class="name">The Storage Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-monitoring"><span class="number">1.5 </span><span class="name">The Monitoring Node</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-ha"><span class="number">1.6 </span><span class="name">HA Setup</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-req"><span class="number">2 </span><span class="name">Considerations and Requirements</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-req-network"><span class="number">2.1 </span><span class="name">Network</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-storage"><span class="number">2.2 </span><span class="name">Persistent Storage</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-ssl"><span class="number">2.3 </span><span class="name">SSL Encryption</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-hardware"><span class="number">2.4 </span><span class="name">Hardware Requirements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-software"><span class="number">2.5 </span><span class="name">Software Requirements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-ha"><span class="number">2.6 </span><span class="name">High Availability</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-summary"><span class="number">2.7 </span><span class="name">Summary: Considerations and Requirements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-installation"><span class="number">2.8 </span><span class="name">Overview of the SUSE <span class="productname">OpenStack</span> Cloud Installation</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#part-depl-admserv"><span class="number">II </span><span class="name">Setting Up the Administration Server</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-depl-adm-inst"><span class="number">3 </span><span class="name">Installing the Administration Server</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-adm-inst-os"><span class="number">3.1 </span><span class="name">Starting the Operating System Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-online-update"><span class="number">3.2 </span><span class="name">Registration and Online Updates</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-add-on"><span class="number">3.3 </span><span class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Extension</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-partition"><span class="number">3.4 </span><span class="name">Partitioning</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-settings"><span class="number">3.5 </span><span class="name">Installation Settings</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#app-deploy-smt"><span class="number">4 </span><span class="name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#app-deploy-smt-install"><span class="number">4.1 </span><span class="name">SMT Installation</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-config"><span class="number">4.2 </span><span class="name">SMT Configuration</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-repos"><span class="number">4.3 </span><span class="name">Setting up Repository Mirroring on the SMT Server</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-info"><span class="number">4.4 </span><span class="name">For More Information</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-repo-conf"><span class="number">5 </span><span class="name">Software Repository Setup</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-product"><span class="number">5.1 </span><span class="name">Copying the Product Media Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-scc"><span class="number">5.2 </span><span class="name">Update and Pool Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-admserv-post-adm-repos"><span class="number">5.3 </span><span class="name">
    Software Repository Sources for the Administration Server Operating System
   </span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-repo-locations"><span class="number">5.4 </span><span class="name">Repository Locations</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#sec-depl-adm-inst-network"><span class="number">6 </span><span class="name">Service Configuration:  Administration Server Network Configuration</span></a></span></dt><dt><span class="chapter"><a href="#sec-depl-adm-inst-crowbar"><span class="number">7 </span><span class="name">Crowbar Setup</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-user"><span class="number">7.1 </span><span class="name"><span class="guimenu ">User Settings</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-network"><span class="number">7.2 </span><span class="name"><span class="guimenu ">Networks</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-mode"><span class="number">7.3 </span><span class="name"><span class="guimenu ">Network Mode</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-repos"><span class="number">7.4 </span><span class="name"><span class="guimenu ">Repositories</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-admserv-post-network"><span class="number">7.5 </span><span class="name">Custom Network Configuration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#sec-depl-adm-start-crowbar"><span class="number">8 </span><span class="name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></a></span></dt><dt><span class="chapter"><a href="#sec-depl-adm-crowbar-extra-features"><span class="number">9 </span><span class="name">Customizing Crowbar</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.4.8.2"><span class="number">9.1 </span><span class="name">Skip Unready Nodes</span></a></span></dt><dt><span class="section"><a href="#id-1.3.4.8.3"><span class="number">9.2 </span><span class="name">Skip Unchanged Nodes</span></a></span></dt><dt><span class="section"><a href="#id-1.3.4.8.4"><span class="number">9.3 </span><span class="name">Controlling Chef Restarts Manually</span></a></span></dt><dt><span class="section"><a href="#id-1.3.4.8.5"><span class="number">9.4 </span><span class="name">Prevent Automatic Restart</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#part-depl-ostack"><span class="number">III </span><span class="name">Setting Up <span class="productname">OpenStack</span> Nodes and Services</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-depl-crowbar"><span class="number">10 </span><span class="name">The Crowbar Web Interface</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-crow-login"><span class="number">10.1 </span><span class="name">Logging In</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-crow-overview"><span class="number">10.2 </span><span class="name">Overview: Main Elements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-barclamps"><span class="number">10.3 </span><span class="name">Deploying Barclamp Proposals</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-inst-nodes"><span class="number">11 </span><span class="name">Installing the <span class="productname">OpenStack</span> Nodes</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-inst-nodes-prep"><span class="number">11.1 </span><span class="name">Preparations</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-install"><span class="number">11.2 </span><span class="name">Node Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-install-external"><span class="number">11.3 </span><span class="name">Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-post"><span class="number">11.4 </span><span class="name">Post-Installation Configuration</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-edit"><span class="number">11.5 </span><span class="name">Editing Allocated Nodes</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-ostack"><span class="number">12 </span><span class="name">Deploying the <span class="productname">OpenStack</span> Services</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-ostack-designate"><span class="number">12.1 </span><span class="name">Deploying Designate</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-pacemaker"><span class="number">12.2 </span><span class="name">Deploying Pacemaker (Optional, HA Setup Only)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-db"><span class="number">12.3 </span><span class="name">Deploying the Database</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-rabbit"><span class="number">12.4 </span><span class="name">Deploying RabbitMQ</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-keystone"><span class="number">12.5 </span><span class="name">Deploying Keystone</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-monasca"><span class="number">12.6 </span><span class="name">Deploying Monasca (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-swift"><span class="number">12.7 </span><span class="name">Deploying Swift (optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-glance"><span class="number">12.8 </span><span class="name">Deploying Glance</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-cinder"><span class="number">12.9 </span><span class="name">Deploying Cinder</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-quantum"><span class="number">12.10 </span><span class="name">Deploying Neutron</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-nova"><span class="number">12.11 </span><span class="name">Deploying Nova</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-dash"><span class="number">12.12 </span><span class="name">Deploying Horizon (<span class="productname">OpenStack</span> Dashboard)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-heat"><span class="number">12.13 </span><span class="name">Deploying Heat (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-ceilometer"><span class="number">12.14 </span><span class="name">Deploying Ceilometer (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-manila"><span class="number">12.15 </span><span class="name">Deploying Manila</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-tempest"><span class="number">12.16 </span><span class="name">Deploying Tempest (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-magnum"><span class="number">12.17 </span><span class="name">Deploying Magnum (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-barbican"><span class="number">12.18 </span><span class="name">Deploying Barbican (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-sahara"><span class="number">12.19 </span><span class="name">Deploying Sahara</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-ironic"><span class="number">12.20 </span><span class="name">Deploying Ironic (optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-final"><span class="number">12.21 </span><span class="name">How to Proceed</span></a></span></dt><dt><span class="sect1"><a href="#crow-ses-integration"><span class="number">12.22 </span><span class="name">SUSE Enterprise Storage integration</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-services"><span class="number">12.23 </span><span class="name">Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-crowbatch-description"><span class="number">12.24 </span><span class="name">Crowbar Batch Command</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#sec-deploy-policy-json"><span class="number">13 </span><span class="name">Limiting Users' Access Rights</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-deploy-policy-json-edit"><span class="number">13.1 </span><span class="name">Editing <code class="filename">policy.json</code></span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-keystone-policy-json-edit"><span class="number">13.2 </span><span class="name">Editing <code class="filename">keystone_policy.json</code></span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-policy-json-keystone"><span class="number">13.3 </span><span class="name">Adjusting the <span class="guimenu ">Keystone</span> Barclamp
   Proposal</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-policy-json-horizon"><span class="number">13.4 </span><span class="name">Adjusting the <span class="guimenu ">Horizon</span> Barclamp
   Proposal</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-policy-json-admin"><span class="number">13.5 </span><span class="name">Pre-Installed Service Admin Role Components</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-ostack-configs"><span class="number">14 </span><span class="name">Configuration Files for <span class="productname">OpenStack</span> Services</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#id-1.3.5.6.2"><span class="number">14.1 </span><span class="name">Default Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.5.6.3"><span class="number">14.2 </span><span class="name">Custom Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-configs-custom-naming"><span class="number">14.3 </span><span class="name">Naming Conventions for Custom Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-configs-custom-order"><span class="number">14.4 </span><span class="name">Processing Order of Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-configs-custom-more"><span class="number">14.5 </span><span class="name">For More Information</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-heat-templates"><span class="number">15 </span><span class="name">Installing SUSE CaaS Platform Heat Templates</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-heat-templates-install"><span class="number">15.1 </span><span class="name">SUSE CaaS Platform Heat Installation Procedure</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.4"><span class="number">15.2 </span><span class="name">Installing SUSE CaaS Platform with Multiple Masters</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.5"><span class="number">15.3 </span><span class="name">Enabling the Cloud Provider Integration (CPI) Feature</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.6"><span class="number">15.4 </span><span class="name">More Information about SUSE CaaS Platform</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#part-depl-nostack"><span class="number">IV </span><span class="name">Setting Up Non-<span class="productname">OpenStack</span> Services</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-depl-nostack"><span class="number">16 </span><span class="name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-nostack-crowbar-tuning"><span class="number">16.1 </span><span class="name">Tuning the Crowbar Service</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-nostack-ntp"><span class="number">16.2 </span><span class="name">Configuring the NTP Service</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#part-depl-maintenance"><span class="number">V </span><span class="name">Maintenance and Support</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-depl-maintenance"><span class="number">17 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Maintenance</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-depl-maintenance-updates"><span class="number">17.1 </span><span class="name">Keeping the Nodes Up-To-Date</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-service-order"><span class="number">17.2 </span><span class="name">Service Order on SUSE <span class="productname">OpenStack</span> Cloud Start-up or Shutdown</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-upgrade"><span class="number">17.3 </span><span class="name">Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-recover-compute-node-failure"><span class="number">17.4 </span><span class="name">Recovering from Compute Node Failure</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-bootstrap-compute-plane"><span class="number">17.5 </span><span class="name">Bootstrapping Compute Plane</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.2.9"><span class="number">17.6 </span><span class="name">Updating MariaDB with Galera</span></a></span></dt><dt><span class="section"><a href="#database-maintenance"><span class="number">17.7 </span><span class="name">Periodic OpenStack Maintenance Tasks</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-fernet-tokens"><span class="number">17.8 </span><span class="name">Rotating Fernet Tokens</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#self-assign-certs"><span class="number">18 </span><span class="name">Generate SUSE <span class="productname">OpenStack</span> Cloud Self Signed Certificate</span></a></span></dt><dd><dl><dt><span class="section"><a href="#create-root-pair"><span class="number">18.1 </span><span class="name">Create the CA Root Pair</span></a></span></dt><dt><span class="section"><a href="#sign-server-client-cert"><span class="number">18.2 </span><span class="name">Sign server and client certificates</span></a></span></dt><dt><span class="section"><a href="#deploy-cert"><span class="number">18.3 </span><span class="name">Deploying the certificate</span></a></span></dt><dt><span class="section"><a href="#lets-encrypt-cert"><span class="number">18.4 </span><span class="name">Generate Public Certificate using Let’s Encrypt</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-deploy-logs"><span class="number">19 </span><span class="name">Log Files</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-deploy-logs-adminserv"><span class="number">19.1 </span><span class="name">On the Administration Server</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-logs-crownodes"><span class="number">19.2 </span><span class="name">On All Other Crowbar Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-logs-contrnode"><span class="number">19.3 </span><span class="name">On the Control Node(s)</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-logs-compnode"><span class="number">19.4 </span><span class="name">On Compute Nodes</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-trouble"><span class="number">20 </span><span class="name">Troubleshooting and Support</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-trouble-faq"><span class="number">20.1 </span><span class="name">FAQ</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-trouble-support"><span class="number">20.2 </span><span class="name">Support</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#part-depl-poc"><span class="number">VI </span><span class="name">Proof of Concepts Deployments</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-deploy-poc"><span class="number">21 </span><span class="name">Building a SUSE <span class="productname">OpenStack</span> Cloud Test lab</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-poc-scope"><span class="number">21.1 </span><span class="name">Document Scope</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-features"><span class="number">21.2 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Key Features</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-components"><span class="number">21.3 </span><span class="name">Main Components</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-objectives"><span class="number">21.4 </span><span class="name">Objectives and Preparations</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-matrix"><span class="number">21.5 </span><span class="name">Hardware and Software Matrix</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-topology"><span class="number">21.6 </span><span class="name">Network Topology</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-architecture"><span class="number">21.7 </span><span class="name">Network Architecture</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-services"><span class="number">21.8 </span><span class="name">Services Architecture</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-testcases"><span class="number">21.9 </span><span class="name">Proof of Concept Test Cases</span></a></span></dt></dl></dd></dl></dd><dt><span class="appendix"><a href="#app-deploy-vmware"><span class="number">A </span><span class="name">VMware vSphere Installation Instructions</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#app-deploy-vmware-requirements"><span class="number">A.1 </span><span class="name">Requirements</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-vmware-vcenter"><span class="number">A.2 </span><span class="name">Preparing the VMware vCenter Server</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-vmware-compnode"><span class="number">A.3 </span><span class="name">Finishing the Nova Compute VMware Node Installation</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-vmware-ha"><span class="number">A.4 </span><span class="name">Making the Nova Compute VMware Node Highly Available</span></a></span></dt></dl></dd><dt><span class="appendix"><a href="#app-deploy-cisco"><span class="number">B </span><span class="name">Using Cisco Nexus Switches with Neutron</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#app-deploy-cisco-requirements"><span class="number">B.1 </span><span class="name">Requirements</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-cisco-deploy"><span class="number">B.2 </span><span class="name">Deploying Neutron with the Cisco Plugin</span></a></span></dt></dl></dd><dt><span class="appendix"><a href="#app-deploy-docupdates"><span class="number">C </span><span class="name">Documentation Updates</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-deploy-docupdates-c8-gm"><span class="number">C.1 </span><span class="name">April 2018 (Initial Release <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8)</span></a></span></dt></dl></dd><dt><span class="glossary"><a href="#gl-cloud"><span class="name">Glossary of Terminology and Product Names</span></a></span></dt></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#id-1.3.3.2.6"><span class="number">1.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Infrastructure</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.3.3.2.4"><span class="number">2.1 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Network: Overview</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.3.3.2.11"><span class="number">2.2 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Network: Details</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.4.6.2.3"><span class="number">7.1 </span><span class="name">YaST Crowbar Setup: User Settings</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.4.6.3.3"><span class="number">7.2 </span><span class="name">YaST Crowbar Setup: Network Settings</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.4.6.3.7.4"><span class="number">7.3 </span><span class="name">YaST Crowbar Setup: Network Settings for the BMC Network</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.4.6.4.6.6"><span class="number">7.4 </span><span class="name">YaST Crowbar Setup: Network Settings for the Bastion Network</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.4.6.5.4"><span class="number">7.5 </span><span class="name">YaST Crowbar Setup: Repository Settings</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.4.7.11"><span class="number">8.1 </span><span class="name">The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation Web interface</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.4.7.13"><span class="number">8.2 </span><span class="name">Crowbar Web Interface: The Dashboard</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.2.5.3"><span class="number">10.1 </span><span class="name">Crowbar UI—Dashboard (Main Screen)</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.6.3.3.2"><span class="number">11.1 </span><span class="name">Discovered Nodes</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.6.3.4.2.2.2"><span class="number">11.2 </span><span class="name">Grouping Nodes</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.6.3.5.2"><span class="number">11.3 </span><span class="name">Editing a Single Node</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.6.3.7.5"><span class="number">11.4 </span><span class="name">Bulk Editing Nodes</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.6.3.9.2"><span class="number">11.5 </span><span class="name">All Nodes Have Been Installed</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.8.3.6.3.3"><span class="number">11.6 </span><span class="name">SUSE Updater barclamp: Configuration</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.8.3.6.4.2"><span class="number">11.7 </span><span class="name">SUSE Updater barclamp: Node Deployment</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.8.4.6.6.2"><span class="number">11.8 </span><span class="name">SUSE Manager barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.8.5.4.4.2"><span class="number">11.9 </span><span class="name">NFS barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.8.5.4.5.3"><span class="number">11.10 </span><span class="name">Editing an NFS barclamp Proposal</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.3.9.4"><span class="number">11.11 </span><span class="name">Node Information</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.9.10"><span class="number">12.1 </span><span class="name">The Pacemaker Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.9.13"><span class="number">12.2 </span><span class="name">The Pacemaker Barclamp: Node Deployment Example</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.10.5"><span class="number">12.3 </span><span class="name">The Database Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.10.6.5.2"><span class="number">12.4 </span><span class="name">MariaDB Configuration</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.11.4"><span class="number">12.5 </span><span class="name">The RabbitMQ Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.11.6.7.1.2"><span class="number">12.6 </span><span class="name">SSL Settings for RabbitMQ Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.12.3.6.2.2"><span class="number">12.7 </span><span class="name">The Keystone Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.12.3.7.2.2.4.2.3"><span class="number">12.8 </span><span class="name">The SSL Dialog</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.13.4"><span class="number">12.9 </span><span class="name">The Monasca barclamp Raw Mode</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.13.25"><span class="number">12.10 </span><span class="name">The Monasca Barclamp: Node Deployment Example</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.14.7"><span class="number">12.11 </span><span class="name">The Swift Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.14.12"><span class="number">12.12 </span><span class="name">The Swift Barclamp: Node Deployment Example</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.15.6"><span class="number">12.13 </span><span class="name">The Glance Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.16.33"><span class="number">12.14 </span><span class="name">The Cinder Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.16.36"><span class="number">12.15 </span><span class="name">The Cinder Barclamp: Node Deployment Example</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.17.11"><span class="number">12.16 </span><span class="name">The Neutron Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.17.14"><span class="number">12.17 </span><span class="name">The Neutron barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.18.5"><span class="number">12.18 </span><span class="name">The Nova Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.18.8"><span class="number">12.19 </span><span class="name">The Nova Barclamp: Node Deployment Example with Two KVM Nodes</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.19.5"><span class="number">12.20 </span><span class="name">The Horizon Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.20.6"><span class="number">12.21 </span><span class="name">The Heat Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.20.7.6"><span class="number">12.22 </span><span class="name">the Heat barclamp: Raw Mode</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.21.7"><span class="number">12.23 </span><span class="name">The Ceilometer Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.21.11"><span class="number">12.24 </span><span class="name">The Ceilometer Barclamp: Node Deployment</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.22.17"><span class="number">12.25 </span><span class="name">The Manila Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.22.20"><span class="number">12.26 </span><span class="name">The Manila Barclamp: Node Deployment Example</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.23.7"><span class="number">12.27 </span><span class="name">The Tempest Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.24.7"><span class="number">12.28 </span><span class="name">The Magnum Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.25.4"><span class="number">12.29 </span><span class="name">The Barbican Barclamp: Raw Mode</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.25.7.1.2.2.4.2.3"><span class="number">12.30 </span><span class="name">The SSL Dialog</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.26.4"><span class="number">12.31 </span><span class="name">The Sahara Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.5.4.27.5.4"><span class="number">12.32 </span><span class="name">The Ironic barclamp Custom view</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.6.2.4.3"><span class="number">16.1 </span><span class="name">The Crowbar barclamp: Raw Mode</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.7.3.6.8.2.2"><span class="number">18.1 </span><span class="name">Database Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.7.3.6.8.4.2"><span class="number">18.2 </span><span class="name">RabbitMQ Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.7.3.6.8.6.2"><span class="number">18.3 </span><span class="name">Keystone Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.7.3.6.8.8.2"><span class="number">18.4 </span><span class="name">Glance Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.7.3.6.8.10.2"><span class="number">18.5 </span><span class="name">Cinder Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.7.3.6.8.12.2"><span class="number">18.6 </span><span class="name">Neutron Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.7.3.6.8.14.2"><span class="number">18.7 </span><span class="name">Nova Barclamp</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.8.2.8.4.5"><span class="number">21.1 </span><span class="name">Network Modes</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.8.2.9.11.4"><span class="number">21.2 </span><span class="name">Network Architecture</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.8.2.10.3"><span class="number">21.3 </span><span class="name">Services Architecture</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.9.6.4"><span class="number">A.1 </span><span class="name">The Nova barclamp: VMware Configuration</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.10.5.2.5.2"><span class="number">B.1 </span><span class="name">The Neutron barclamp: Cisco Plugin</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#tab-netw-range-ip-min"><span class="number">2.1 </span><span class="name">Minimum Number of IP Addresses for Network Ranges</span></a></span></dt><dt><span class="table"><a href="#id-1.3.3.3.2.12.6"><span class="number">2.2 </span><span class="name"><code class="systemitem">192.168.124.0/24</code> (Admin/BMC) Network Address Allocation</span></a></span></dt><dt><span class="table"><a href="#id-1.3.3.3.2.12.7"><span class="number">2.3 </span><span class="name"><code class="systemitem">192.168.125/24</code> (Storage) Network Address Allocation</span></a></span></dt><dt><span class="table"><a href="#id-1.3.3.3.2.12.8"><span class="number">2.4 </span><span class="name"><code class="systemitem">192.168.123/24</code> (Private Network/nova-fixed) Network Address Allocation</span></a></span></dt><dt><span class="table"><a href="#id-1.3.3.3.2.12.9"><span class="number">2.5 </span><span class="name"><code class="systemitem">192.168.126/24</code> (Public Network nova-floating, public) Network Address Allocation</span></a></span></dt><dt><span class="table"><a href="#id-1.3.3.3.2.12.10"><span class="number">2.6 </span><span class="name"><code class="systemitem">192.168.130/24</code> (Software Defined Network) Network Address Allocation</span></a></span></dt><dt><span class="table"><a href="#id-1.3.4.4.2.6"><span class="number">5.1 </span><span class="name">Local Product Repositories for SUSE <span class="productname">OpenStack</span> Cloud</span></a></span></dt><dt><span class="table"><a href="#tab-smt-repos-local"><span class="number">5.2 </span><span class="name">SMT Repositories Hosted on the Administration Server</span></a></span></dt><dt><span class="table"><a href="#tab-smt-repos-remote"><span class="number">5.3 </span><span class="name">SMT Repositories hosted on a Remote Server</span></a></span></dt><dt><span class="table"><a href="#tab-depl-adm-conf-susemgr-repos"><span class="number">5.4 </span><span class="name">SUSE Manager Repositories (Channels)</span></a></span></dt><dt><span class="table"><a href="#tab-depl-adm-conf-local-repos"><span class="number">5.5 </span><span class="name">Default Repository Locations on the Administration Server</span></a></span></dt><dt><span class="table"><a href="#id-1.3.4.6.3.7.3"><span class="number">7.1 </span><span class="name">Separate BMC Network Example Configuration</span></a></span></dt><dt><span class="table"><a href="#id-1.3.4.6.4.6.4"><span class="number">7.2 </span><span class="name">Example Addresses for a Bastion Network</span></a></span></dt><dt><span class="table"><a href="#id-1.3.4.6.6.12.5"><span class="number">7.3 </span><span class="name">VLANs used by the SUSE <span class="productname">OpenStack</span> Cloud Default Network Setup</span></a></span></dt><dt><span class="table"><a href="#id-1.3.8.2.7.6"><span class="number">21.1 </span><span class="name">BoM/SUSE <span class="productname">OpenStack</span> Cloud Services</span></a></span></dt><dt><span class="table"><a href="#id-1.3.8.2.8.5.3"><span class="number">21.2 </span><span class="name">Administrator Network Layout</span></a></span></dt><dt><span class="table"><a href="#id-1.3.8.2.8.5.4"><span class="number">21.3 </span><span class="name">Private Network Layout</span></a></span></dt><dt><span class="table"><a href="#id-1.3.8.2.8.5.5"><span class="number">21.4 </span><span class="name">Public/Nova Floating Network Layout/Externally Provided</span></a></span></dt><dt><span class="table"><a href="#id-1.3.8.2.8.5.6"><span class="number">21.5 </span><span class="name">Storage Network Layout</span></a></span></dt></dl></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><dl><dt><span class="example"><a href="#interface-map-example"><span class="number">7.1 </span><span class="name">Changing the Network Interface Order on a Machine with four NICs</span></a></span></dt><dt><span class="example"><a href="#ex-conduits-nic-number"><span class="number">7.2 </span><span class="name">Network Modes for Different NIC Numbers</span></a></span></dt><dt><span class="example"><a href="#ex-conduits-role"><span class="number">7.3 </span><span class="name">Network Modes for Certain Roles</span></a></span></dt><dt><span class="example"><a href="#ex-conduits-machine"><span class="number">7.4 </span><span class="name">Network Modes for Certain Machines</span></a></span></dt><dt><span class="example"><a href="#id-1.3.4.6.6.12.3"><span class="number">7.5 </span><span class="name">Example Network Definition for the External Network 192.168.150.0/16</span></a></span></dt><dt><span class="example"><a href="#ex-ironic-network-json"><span class="number">12.1 </span><span class="name">Example network.json</span></a></span></dt><dt><span class="example"><a href="#ex-ironic-network-json-diff"><span class="number">12.2 </span><span class="name">Diff of Ironic Configuration</span></a></span></dt><dt><span class="example"><a href="#id-1.3.10.4.3.6.2"><span class="number">B.1 </span><span class="name">Exclusively Mapping <span class="guimenu ">nova-fixed</span> to conduit <span class="guimenu ">intf1</span> in dual mode</span></a></span></dt></dl></div><div><div class="legalnotice" id="id-1.3.1.5"><p>
  Copyright © 2006–
2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under
  <span class="bold"><strong>Creative Commons Attribution 3.0 License
  </strong></span>:
  <a class="link" href="http://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">


  </a>
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be held
  liable for possible errors or the consequences thereof.
 </p></div></div><div class="preface " id="pre-cloud-deploy"><div class="titlepage"><div><div><h1 class="title"><span class="number"> </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">About This Guide</span> <a title="Permalink" class="permalink" href="#pre-cloud-deploy">#</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_intro.xml</li><li><span class="ds-label">ID: </span>pre-cloud-deploy</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#id-1.3.2.9"><span class="name">Available Documentation</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.2.10"><span class="name">Feedback</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.2.11"><span class="name">Documentation Conventions</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.2.12"><span class="name">About the Making of This Manual</span></a></span></dt></dl></div></div><p>
   <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is an open source software solution that provides the
  fundamental capabilities to deploy and manage a cloud infrastructure
  based on SUSE Linux Enterprise. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is powered by <span class="productname">OpenStack</span>, the leading
  community-driven, open source cloud infrastructure project. It seamlessly
  manages and provisions workloads across a heterogeneous cloud environment
  in a secure, compliant, and fully-supported manner. The product tightly
  integrates with other SUSE technologies and with the SUSE maintenance
  and support infrastructure.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, there are several different high-level user roles:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.2.5.1"><span class="term ">SUSE <span class="productname">OpenStack</span> Cloud Operator</span></dt><dd><p>
     Installs and deploys <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> on bare-metal, then
     installs the operating system and the <span class="productname">OpenStack</span> components. For detailed
     information about the operator's tasks and how to solve them, refer
     to SUSE <span class="productname">OpenStack</span> Cloud <em class="citetitle ">Deploying With Crowbar</em>.
    </p></dd><dt id="id-1.3.2.5.2"><span class="term ">SUSE <span class="productname">OpenStack</span> Cloud Administrator</span></dt><dd><p>
     Manages projects, users, images, flavors, and quotas within
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.  For detailed information about the administrator's
     tasks and how to solve them, refer to the <span class="productname">OpenStack</span> <em class="citetitle ">Administrator Guide</em> and the
     SUSE <span class="productname">OpenStack</span> Cloud <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">End User Guide</em></em>.
    </p></dd><dt id="id-1.3.2.5.3"><span class="term ">SUSE <span class="productname">OpenStack</span> Cloud User</span></dt><dd><p>
     End user who launches and manages instances, creates snapshots, and
     uses volumes for persistent storage within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. For detailed
     information about the user's tasks and how to solve them, refer to
     <span class="productname">OpenStack</span> <em class="citetitle ">End User Guide</em> and the SUSE <span class="productname">OpenStack</span> Cloud <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">End User Guide</em></em>.
    </p></dd></dl></div><p>
  This guide provides cloud operators with the information needed to deploy
  and maintain SUSE <span class="productname">OpenStack</span> Cloud administrative units, the Administration Server, the
  Control Nodes, and the Compute and Storage Nodes. The Administration Server
  provides all services needed to manage and deploy all other nodes in the
  cloud. The Control Node hosts all <span class="productname">OpenStack</span> components needed to operate
  virtual machines deployed on the Compute Nodes in the SUSE <span class="productname">OpenStack</span> Cloud. Each
  virtual machine (instance) started in the cloud will be hosted on one
  of the Compute Nodes. Object storage is managed by the Storage Nodes.

 </p><p>
  Many chapters in this manual contain links to additional documentation
  resources. These include additional documentation that is available on the
  system, and documentation available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
 </p><div class="sect1 " id="id-1.3.2.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Available Documentation</span> <a title="Permalink" class="permalink" href="#id-1.3.2.9">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>common_intro_available_doc_i.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.2.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Online Documentation and Latest Updates</h6><p>
  Documentation for our products is available at
  <a class="link" href="http://documentation.suse.com" target="_blank">http://documentation.suse.com</a>, where you can also
  find the latest updates, and browse or download the documentation in various formats.
 
 </p></div><p>
  In addition, the product documentation
  is usually available in your installed system under
  <code class="filename">/usr/share/doc/manual</code>. You can also access the
  product-specific manuals and the upstream documentation from
  the <span class="guimenu ">Help</span> links in the graphical Web interfaces.
 </p><p>The following documentation is available for this product:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.2.9.6.1"><span class="term "><em class="citetitle ">Deploying With Crowbar</em></span></dt><dd><p>
     Gives an introduction to the <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> architecture, lists
     the requirements, and describes how to set up, deploy, and maintain the
     individual components. Also contains information about troubleshooting,
     support, and a glossary listing the most important terms and concepts
     for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
    </p></dd><dt id="id-1.3.2.9.6.2"><span class="term "><em class="citetitle ">Administrator Guide</em>
   </span></dt><dd><p>
     Introduces the <span class="productname">OpenStack</span> services and their components.
    </p><p>
     Also guides you through tasks like managing images, roles, instances, flavors,
     volumes, shares, quotas, host aggregates, and viewing cloud resources. To
     complete these tasks, use either the graphical Web interface (<span class="productname">OpenStack</span> Dashboard,
     code name <code class="literal">Horizon</code>) or the <span class="productname">OpenStack</span> command line clients.
    </p></dd><dt id="id-1.3.2.9.6.3"><span class="term "><em class="citetitle ">End User Guide</em>
   </span></dt><dd><p>
     Describes how to manage images, instances, networks, object containers,
     volumes, shares, stacks, and databases. To complete these tasks, use either the graphical Web interface (<span class="productname">OpenStack</span> Dashboard, code name
     <code class="literal">Horizon</code>) or the <span class="productname">OpenStack</span> command line clients.
    </p></dd><dt id="id-1.3.2.9.6.4"><span class="term "><em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">End User Guide</em></em></span></dt><dd><p>
     A supplement to the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <em class="citetitle ">Administrator Guide</em> and
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <em class="citetitle ">End User Guide</em>. It contains additional information for
     admins and end users that is specific to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
    </p></dd><dt id="id-1.3.2.9.6.5"><span class="term "><em class="citetitle ">Overview</em></span></dt><dd><p>
     A manual introducing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring. It is written for
     everybody interested in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
    </p></dd><dt id="id-1.3.2.9.6.6"><span class="term "><em class="citetitle "><span class="productname">OpenStack</span> Operator's Guide</em></span></dt><dd><p>A manual for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> operators describing how to prepare their
     <span class="productname">OpenStack</span> platform for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring. The manual also describes
     how the operators use <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring for monitoring their <span class="productname">OpenStack</span>
     services.
     </p></dd><dt id="id-1.3.2.9.6.7"><span class="term "><em class="citetitle ">Monitoring Service Operator's Guide</em></span></dt><dd><p>A manual for system operators describing how to operate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
     Monitoring. The manual also describes how the operators can use
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring for monitoring their environment.
    </p></dd></dl></div></div><div class="sect1 " id="id-1.3.2.10"><div class="titlepage"><div><div><h2 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Feedback</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>common_intro_feedback_i.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
  Several feedback channels are available:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.2.10.4.1"><span class="term ">Services and Support Options</span></dt><dd><p>
     For services and support options available for your product, refer to
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p></dd><dt id="id-1.3.2.10.4.2"><span class="term ">User Comments/Bug Reports</span></dt><dd><p>
     We want to hear your comments about and suggestions for this manual and
     the other documentation included with this product. If you are reading the
     HTML version of this guide, use the Comments feature at the bottom of each page
     in the online documentation
     at <a class="link" href="http://documentation.suse.com" target="_blank">http://documentation.suse.com</a>.
    </p><p>If you are reading the single-page HTML version of this guide, you can
     use the <span class="guimenu ">Report Bug</span> link next to each section to open
     a bug report at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>. A user
    account is needed for this.</p></dd><dt id="id-1.3.2.10.4.3"><span class="term ">Mail</span></dt><dd><p>
     For feedback on the documentation of this product, you can also send a
     mail to <code class="literal">doc-team@suse.com</code>. Make sure to include the
     document title, the product version, and the publication date of the
     documentation. To report errors or suggest enhancements, provide a
     concise description of the problem and refer to the respective section
     number and page (or URL).
    </p></dd></dl></div></div><div class="sect1 " id="id-1.3.2.11"><div class="titlepage"><div><div><h2 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Documentation Conventions</span> <a title="Permalink" class="permalink" href="#id-1.3.2.11">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>common_intro_typografie_i.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
  The following notices and typographical conventions are used
  in this documentation:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><div id="id-1.3.2.11.4.1.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Vital information you must be aware of before proceeding. Warns you about
    security issues, potential loss of data, damage to hardware, or physical
    hazards.
   </p></div><div id="id-1.3.2.11.4.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Important information you should be aware of before proceeding.
   </p></div><div id="id-1.3.2.11.4.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Additional information, for example about differences in software
    versions.
   </p></div><div id="id-1.3.2.11.4.1.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip</h6><p>
    Helpful information, like a guideline or a piece of practical advice.
   </p></div></li><li class="listitem "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">command</code></pre></div><p>
    Commands that can be run by any user, including the <code class="systemitem">root</code> user.
   </p></li><li class="listitem "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">command</code></pre></div><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. Often you
    can also prefix these commands with the <code class="command">sudo</code> command to
    run them.
   </p></li><li class="listitem "><p>
    <code class="filename">/etc/passwd</code>: directory names and file names
   </p></li><li class="listitem "><p>
    <em class="replaceable ">PLACEHOLDER</em>: replace
    <em class="replaceable ">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem "><p>
    <code class="envar">PATH</code>: the environment variable PATH
   </p></li><li class="listitem "><p>
    <code class="command">ls</code>, <code class="option">--help</code>: commands, options, and
    parameters
   </p></li><li class="listitem "><p>
    <code class="systemitem">user</code>: users or groups
   </p></li><li class="listitem "><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: a key to press or a key combination;
    keys are shown in uppercase as on a keyboard
   </p></li><li class="listitem "><p>
    <span class="guimenu ">File</span>, <span class="guimenu ">File</span> › <span class="guimenu ">Save As</span>: menu items, buttons
   </p></li><li class="listitem "><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architecture. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"></strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">z Systems</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"></strong></p></li><li class="listitem "><p>
    <span class="emphasis"><em>Dancing Penguins</em></span> (Chapter
    <span class="emphasis"><em>Penguins</em></span>, ↑Another Manual): This is a
    reference to a chapter in another manual.
   </p></li></ul></div></div><div class="sect1 " id="id-1.3.2.12"><div class="titlepage"><div><div><h2 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">About the Making of This Manual</span> <a title="Permalink" class="permalink" href="#id-1.3.2.12">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>common_intro_making_i.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This documentation is written in SUSEDoc, a subset of <a class="link" href="http://www.docbook.org" target="_blank">DocBook 5</a>. The XML source
   files were validated by <code class="command">jing</code>, processed by
   <code class="command">xsltproc</code>, and converted into XSL-FO using a customized
   version of Norman Walsh's stylesheets. The final PDF is formatted through
   <span class="phrase">FOP</span> from Apache Software Foundation. The open source tools
   and the environment used to build this documentation are provided by the
   DocBook Authoring and Publishing Suite (DAPS). The project's home page can
   be found at <a class="link" href="https://github.com/openSUSE/daps" target="_blank">https://github.com/openSUSE/daps</a>.
  </p><p>
   The XML source code of this documentation can be found at <a class="link" href="https://github.com/SUSE-Cloud/doc-cloud" target="_blank">https://github.com/SUSE-Cloud/doc-cloud</a>.
  </p></div></div><div class="part" id="part-depl-intro"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part I </span><span class="name">Architecture and Requirements </span><a title="Permalink" class="permalink" href="#part-depl-intro">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-depl-arch"><span class="number">1 </span><span class="name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></a></span></dt><dd class="toc-abstract"><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is a managed cloud infrastructure solution that provides a
    full stack of cloud deployment and management services.
   </p></dd><dt><span class="chapter"><a href="#cha-depl-req"><span class="number">2 </span><span class="name">Considerations and Requirements</span></a></span></dt><dd class="toc-abstract"><p>
    Before deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, there are some requirements to meet and architectural decisions to make. Read this
    chapter thoroughly first, as some decisions need to be made <span class="emphasis"><em>before</em></span>
    deploying SUSE <span class="productname">OpenStack</span> Cloud, and you cannot change them afterward.
   </p></dd></dl></div><div class="chapter " id="cha-depl-arch"><div class="titlepage"><div><div><h2 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span> <a title="Permalink" class="permalink" href="#cha-depl-arch">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_overview.xml</li><li><span class="ds-label">ID: </span>cha-depl-arch</li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.3.2.2.2">#</a></h6></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is a managed cloud infrastructure solution that provides a
    full stack of cloud deployment and management services.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-arch-components-admin"><span class="number">1.1 </span><span class="name">The Administration Server</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-control"><span class="number">1.2 </span><span class="name">The Control Node(s)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-compute"><span class="number">1.3 </span><span class="name">The Compute Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-storage"><span class="number">1.4 </span><span class="name">The Storage Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-monitoring"><span class="number">1.5 </span><span class="name">The Monitoring Node</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-arch-components-ha"><span class="number">1.6 </span><span class="name">HA Setup</span></a></span></dt></dl></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>  <span class="phrase"><span class="phrase">8</span></span> provides the following features:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Open source software that is based on the <span class="productname">OpenStack</span> Pike
    release.
   </p></li><li class="listitem "><p>
    Centralized resource tracking providing insight into activities and
    capacity of the cloud infrastructure for optimized automated deployment
    of services.
   </p></li><li class="listitem "><p>
    A self-service portal enabling end users to configure and deploy
    services as necessary, and to track resource
    consumption (Horizon).
   </p></li><li class="listitem "><p>
    An image repository from which standardized, pre-configured virtual
    machines are published (Glance).
   </p></li><li class="listitem "><p>
    Automated installation processes via Crowbar using pre-defined scripts
    for configuring and deploying the Control Node(s) and Compute
    and Storage Nodes.
   </p></li><li class="listitem "><p>
    Multi-tenant, role-based provisioning and access control for multiple
    departments and users within your organization.
   </p></li><li class="listitem "><p>
    APIs enabling the integration of third-party software, such as identity
    management and billing solutions.
   </p></li><li class="listitem "><p>
    Heterogeneous hypervisor support (Xen and KVM).
   </p></li><li class="listitem "><p>
    An optional monitoring as a service solution, that allows to manage, track,
    and optimize the cloud infrastructure and the services provided to end
    users (SUSE OpenStack Cloud Monitoring, Monasca).
   </p></li></ul></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is based on SUSE Linux Enterprise Server, <span class="productname">OpenStack</span>, Crowbar, and
  Chef. SUSE Linux Enterprise Server is the underlying operating system for all
  cloud infrastructure machines (also called nodes). The cloud management layer,
  <span class="productname">OpenStack</span>, works as the <span class="quote">“<span class="quote ">Cloud Operating
  System</span>”</span>. Crowbar and Chef automatically deploy
  and manage the <span class="productname">OpenStack</span> nodes from a central Administration Server.
 </p><div class="figure" id="id-1.3.3.2.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/cloud_node_structure.png" target="_blank"><img src="images/cloud_node_structure.png" width="" alt="SUSE OpenStack Cloud Crowbar Infrastructure" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 1.1: </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Infrastructure </span><a title="Permalink" class="permalink" href="#id-1.3.3.2.6">#</a></h6></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is deployed to four different types of machines:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    one Administration Server for node deployment and management
   </p></li><li class="listitem "><p>
    one or more Control Nodes hosting the cloud management services
   </p></li><li class="listitem "><p>
    several Compute Nodes on which the instances are started
   </p></li><li class="listitem "><p>
    several Monitoring Node for monitoring services and servers.
   </p></li></ul></div><div class="sect1 " id="sec-depl-arch-components-admin"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Administration Server</span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-admin">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_overview.xml</li><li><span class="ds-label">ID: </span>sec-depl-arch-components-admin</li></ul></div></div></div></div><p>
   The Administration Server provides all services needed to manage and deploy all
   other nodes in the cloud. Most of these services are provided by the
   Crowbar tool that—together with Chef—automates all the
   required installation and configuration tasks. Among the services
   provided by the server are DHCP, DNS, NTP, PXE, and TFTP.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/cloud_admin-node-crowbar.png" target="_blank"><img src="images/cloud_admin-node-crowbar.png" width="" alt="Administration Server Diagram" /></a></div></div><p>
   The Administration Server also hosts the software repositories for SUSE Linux Enterprise Server and
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, which are needed for node deployment. If no other
   sources for the software repositories are available, it can host the Subscription Management Tool (SMT), providing up-to-date repositories
   with updates and patches for all nodes.
  </p></div><div class="sect1 " id="sec-depl-arch-components-control"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Control Node(s)</span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-control">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_overview.xml</li><li><span class="ds-label">ID: </span>sec-depl-arch-components-control</li></ul></div></div></div></div><p>
   The Control Node(s) hosts all <span class="productname">OpenStack</span> components needed to
   orchestrate virtual machines deployed on the Compute Nodes in the
   SUSE <span class="productname">OpenStack</span> Cloud. <span class="productname">OpenStack</span> on SUSE <span class="productname">OpenStack</span> Cloud uses a MariaDB database,
   which is hosted on the Control Node(s). The following <span class="productname">OpenStack</span>
   components—if deployed—run on the Control Node(s):
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     PostgreSQL database.
    </p></li><li class="listitem "><p>
     Image (Glance) for managing virtual images.
    </p></li><li class="listitem "><p>
     Identity (Keystone), providing authentication and authorization
     for all <span class="productname">OpenStack</span> components.
    </p></li><li class="listitem "><p>
     Networking (Neutron), providing <span class="quote">“<span class="quote ">networking as a
     service</span>”</span> between interface devices managed by other <span class="productname">OpenStack</span>
     services.
    </p></li><li class="listitem "><p>
     Block Storage (Cinder), providing block storage.
    </p></li><li class="listitem "><p>
     <span class="productname">OpenStack</span> Dashboard (Horizon), providing the Dashboard,
     a user Web interface for the <span class="productname">OpenStack</span> components.
    </p></li><li class="listitem "><p>
     Compute (Nova) management (Nova controller) including API and
     scheduler.
    </p></li><li class="listitem "><p>
     Message broker (RabbitMQ).
    </p></li><li class="listitem "><p>
     Swift proxy server plus dispersion tools (health monitor) and
     Swift ring (index of objects, replicas, and
     devices). Swift provides object storage.
    </p></li><li class="listitem "><p>
     Hawk, a monitor for a pacemaker cluster in a High Availability (HA)
     setup.
    </p></li><li class="listitem "><p>
     Heat, an orchestration engine.
    </p></li><li class="listitem "><p>
     Designate provides DNS as a Service (DNSaaS)
    </p></li><li class="listitem "><p>
     Ceilometer server and agents. Ceilometer collects CPU and networking data
     for billing purposes.
    </p></li><li class="listitem "><p>
     Ironic, the OpenStack bare metal service for provisioning physical machines.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> requires a three-node cluster for any production deployment
   since it leverages a MariaDB Galera Cluster for high availability.
  </p><p>
   We recommend deploying certain parts of Networking (Neutron) on separate nodes for production clouds. See
   <a class="xref" href="#sec-depl-ostack-quantum" title="12.10. Deploying Neutron">Section 12.10, “Deploying Neutron”</a> for details.
  </p><p>
   You can separate authentication and authorization services from other cloud
   services, for stronger security, by hosting Identity (Keystone) on a
   separate node. Hosting Block Storage (Cinder, particularly the
   cinder-volume role) on a separate node when using local disks for storage
   enables you to customize your storage and network hardware to best meet your
   requirements.
  </p><div id="id-1.3.3.2.10.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Moving Services in an Existing Setup</h6><p>
    If you plan to move a service from one Control Node to another, we strongly recommended shutting down or saving <span class="emphasis"><em>all</em></span> instances before doing so. Restart
    them after having successfully re-deployed the services. Moving
    services also requires stopping them manually on the original
    Control Node.


   </p></div></div><div class="sect1 " id="sec-depl-arch-components-compute"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Compute Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-compute">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_overview.xml</li><li><span class="ds-label">ID: </span>sec-depl-arch-components-compute</li></ul></div></div></div></div><p>
   The Compute Nodes are the pool of machines on which your instances
   are running. These machines need to be equipped with a sufficient number
   of CPUs and enough RAM to start several instances. They also need to
   provide sufficient hard disk space, see
   <a class="xref" href="#sec-depl-req-storage-hardware-compute" title="2.2.2.3. Compute Nodes">Section 2.2.2.3, “Compute Nodes”</a> for details. The
   Control Node distributes instances within the pool of
   Compute Nodes and provides them with the necessary network resources. The
   <span class="productname">OpenStack</span> component Compute (Nova) runs on the Compute Nodes and
   provides the means for setting up, starting, and stopping virtual machines.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports several hypervisors, including KVM, VMware
   vSphere, and Xen. Each image that is started with an instance is
   bound to one hypervisor. Each Compute Node can only run one hypervisor
   at a time. You will choose which hypervisor to run on each Compute Node
   when deploying the Nova barclamp.
  </p></div><div class="sect1 " id="sec-depl-arch-components-storage"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Storage Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-storage">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_overview.xml</li><li><span class="ds-label">ID: </span>sec-depl-arch-components-storage</li></ul></div></div></div></div><p>
   The Storage Nodes are the pool of machines providing object or block
   storage. Object storage is provided by the <span class="productname">OpenStack</span> Swift component,
   while block storage is provided by Cinder. The latter supports several
   back-ends, including Ceph, that are deployed during the
   installation. Deploying Swift and Ceph is optional.
  </p></div><div class="sect1 " id="sec-depl-arch-components-monitoring"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Monitoring Node</span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-monitoring">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_overview.xml</li><li><span class="ds-label">ID: </span>sec-depl-arch-components-monitoring</li></ul></div></div></div></div><p>
The Monitoring Node is the node that has the <code class="literal">monasca-server</code> role assigned.
It hosts most services needed for SUSE <span class="productname">OpenStack</span> Cloud
Monitoring, our Monasca-based monitoring and logging solution. The following
services run on this node:
</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.3.2.13.3.1"><span class="term ">Monitoring API</span></dt><dd><p>
          The Monasca Web API that is used for sending metrics by Monasca agents, and
  retrieving metrics with the Monasca command line client and the Monasca Grafana dashboard.
      </p></dd><dt id="id-1.3.3.2.13.3.2"><span class="term ">Message Queue</span></dt><dd><p>
         A Kafka instance used exclusively by SUSE <span class="productname">OpenStack</span> Cloud Monitoring.
      </p></dd><dt id="id-1.3.3.2.13.3.3"><span class="term ">Persister</span></dt><dd><p>
         Stores metrics and alarms in InfluxDB.
      </p></dd><dt id="id-1.3.3.2.13.3.4"><span class="term ">Notification Engine</span></dt><dd><p>
         Consumes alarms sent by the Threshold Engine and sends
notifications (e.g. via email).
      </p></dd><dt id="id-1.3.3.2.13.3.5"><span class="term ">Threshold Engine</span></dt><dd><p>
         Based on Apache Storm. Computes thresholds on metrics and handles alarming.
      </p></dd><dt id="id-1.3.3.2.13.3.6"><span class="term ">Metrics and Alarms Database</span></dt><dd><p>
         A Cassandra database for storing metrics alarm history.
      </p></dd><dt id="id-1.3.3.2.13.3.7"><span class="term ">Config Database</span></dt><dd><p>
         A dedicated MariaDB instance used only for monitoring related data.
      </p></dd><dt id="id-1.3.3.2.13.3.8"><span class="term ">Log API</span></dt><dd><p>
         The Monasca Web API that is used for sending log entries by Monasca agents, and
  retrieving log entries with the Kibana Server.
      </p></dd><dt id="id-1.3.3.2.13.3.9"><span class="term ">Log Transformer</span></dt><dd><p>
         Transforms raw log entries sent to the Log API into a format
suitable for storage.
      </p></dd><dt id="id-1.3.3.2.13.3.10"><span class="term ">Log Metrics</span></dt><dd><p>
         Sends metrics about high severity log messages to the Monitoring API.
      </p></dd><dt id="id-1.3.3.2.13.3.11"><span class="term ">Log Persister</span></dt><dd><p>
         Stores logs processed by Monasca Log Transformer in the Log Database.
      </p></dd><dt id="id-1.3.3.2.13.3.12"><span class="term ">Kibana Server</span></dt><dd><p>
         A graphical web frontend for querying the Log Database.
      </p></dd><dt id="id-1.3.3.2.13.3.13"><span class="term ">Log Database</span></dt><dd><p>
         An Elasticsearch database for storing logs.
      </p></dd><dt id="id-1.3.3.2.13.3.14"><span class="term ">Zookeeper</span></dt><dd><p>
         Cluster synchronization for Kafka and Storm.
      </p></dd></dl></div><p>
Currently there can only be one Monitoring node. Clustering support is
planned for a future release. We strongly recommend using a dedicated physical
node without any other services as a Monitoring Node.
</p></div><div class="sect1 " id="sec-depl-arch-components-ha"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup</span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-ha">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_overview.xml</li><li><span class="ds-label">ID: </span>sec-depl-arch-components-ha</li></ul></div></div></div></div><p>
   A failure of components in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> can lead to system downtime
   and data loss. To prevent this, set up a High Availability (HA) cluster
   consisting of several nodes. You can assign certain roles to this cluster
   instead of assigning them to individual nodes. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   <span class="phrase"><span class="phrase">8</span></span>, Control Nodes and Compute Nodes can be made highly available.
  </p><p>
   For all HA-enabled roles, their respective functions are automatically
   handled by the clustering software SUSE Linux Enterprise High Availability Extension. The High Availability Extension uses
   the Pacemaker cluster stack with Pacemaker as cluster resource manager,
   and Corosync as the messaging/infrastructure layer.
  </p><p>
   View the cluster status and configuration with the cluster
   management tools HA Web Console (Hawk) or the
   <code class="command">crm</code> shell.
  </p><div id="id-1.3.3.2.14.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Do Not Change the Configuration</h6><p>
    Use the cluster management tools only for <span class="emphasis"><em>viewing</em></span>.
    All of the clustering configuration is done automatically via Crowbar
    and Chef. If you change anything via the cluster management tools
    you risk breaking the cluster. Changes done there may be reverted by the
    next run of Chef anyway.
   </p></div><p>
   A failure of the <span class="productname">OpenStack</span> infrastructure services (running on the
   Control Nodes) can be critical and may cause downtime within the cloud. For more information on making those services highly-available and avoiding other potential points of
   failure in your cloud setup, refer to <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a>.
  </p></div></div><div class="chapter " id="cha-depl-req"><div class="titlepage"><div><div><h2 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Considerations and Requirements</span> <a title="Permalink" class="permalink" href="#cha-depl-req">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.3.3.1.2">#</a></h6></div><p>
    Before deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, there are some requirements to meet and architectural decisions to make. Read this
    chapter thoroughly first, as some decisions need to be made <span class="emphasis"><em>before</em></span>
    deploying SUSE <span class="productname">OpenStack</span> Cloud, and you cannot change them afterward.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-req-network"><span class="number">2.1 </span><span class="name">Network</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-storage"><span class="number">2.2 </span><span class="name">Persistent Storage</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-ssl"><span class="number">2.3 </span><span class="name">SSL Encryption</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-hardware"><span class="number">2.4 </span><span class="name">Hardware Requirements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-software"><span class="number">2.5 </span><span class="name">Software Requirements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-ha"><span class="number">2.6 </span><span class="name">High Availability</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-summary"><span class="number">2.7 </span><span class="name">Summary: Considerations and Requirements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-req-installation"><span class="number">2.8 </span><span class="name">Overview of the SUSE <span class="productname">OpenStack</span> Cloud Installation</span></a></span></dt></dl></div></div><div class="sect1 " id="sec-depl-req-network"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> requires a complex network setup consisting of several
   networks that are configured during installation. These networks are for
   exclusive cloud usage. You need a router to access them from an existing network.
  </p><p>
   The network configuration on the nodes in the SUSE <span class="productname">OpenStack</span> Cloud network is
   entirely controlled by Crowbar. Any network configuration not created with
   Crowbar (for example, with YaST) will automatically be
   overwritten. After the cloud is deployed, network settings cannot be
   changed.
  </p><div class="figure" id="id-1.3.3.3.2.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/cloud_network_overview.png" target="_blank"><img src="images/cloud_network_overview.png" width="" alt="SUSE OpenStack Cloud Network: Overview" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 2.1: </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Network: Overview </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.2.4">#</a></h6></div></div><p>
   The following networks are pre-defined when setting up <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   The IP addresses listed are the default addresses and can be changed
   using the YaST Crowbar module (see
   <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>). It is also possible to
   customize the network setup by manually editing
   the network barclamp template. See
   <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for detailed instructions.
  </p><div class="variablelist "><dl class="variablelist"><dt id="vle-netw-adm"><span class="term ">
     Admin Network (192.168.124/24)
    </span></dt><dd><p>
      A private network to access the Administration Server and all nodes for
      administration purposes. The default setup also allows access to the
      BMC (Baseboard Management Controller) data via IPMI (Intelligent
      Platform Management Interface) from this network. If required, BMC
      access can be swapped to a separate network.
     </p><p>
      You have the following options for controlling access to this network:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        Do not allow access from the outside and keep the admin network
        completely separated.
       </p></li><li class="listitem "><p>
        Allow access to the Administration Server from a single network (for example,
        your company's administration network) via the <span class="quote">“<span class="quote ">bastion
        network</span>”</span> option configured on an additional network card with
        a fixed IP address.
       </p></li><li class="listitem "><p>
        Allow access from one or more networks via a gateway.
       </p></li></ul></div></dd><dt id="vle-netw-stor"><span class="term ">
     Storage Network (192.168.125/24)
    </span></dt><dd><p>
      Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used by
      Ceph and Swift only. It should not be accessed by
      users.
     </p></dd><dt id="id-1.3.3.3.2.6.3"><span class="term ">
     Private Network (nova-fixed, 192.168.123/24)
    </span></dt><dd><p>
      Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used for
      inter-instance communication and provides access to the outside
      world for the instances. The required gateway is automatically provided by SUSE <span class="productname">OpenStack</span> Cloud.
     </p></dd><dt id="id-1.3.3.3.2.6.4"><span class="term ">
     Public Network (nova-floating, public, 192.168.126/24)
    </span></dt><dd><p>
      The only public network provided by SUSE <span class="productname">OpenStack</span> Cloud. You can access the
      Nova Dashboard and all instances (provided they have been equipped
      with floating IP addresses) on this network. This network can only be accessed
      via a gateway, which must be provided externally. All SUSE <span class="productname">OpenStack</span> Cloud
      users and administrators must have access to the public network.
     </p></dd><dt id="vle-netw-sdn"><span class="term ">
     Software Defined Network (os_sdn, 192.168.130/24)
    </span></dt><dd><p>
      Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used
      when Neutron is configured to use openvswitch with
      <code class="literal">GRE</code> tunneling for the virtual networks. It should
      not be accessible to users.
     </p></dd><dt id="id-1.3.3.3.2.6.6"><span class="term ">The Monasca Monitoring Network</span></dt><dd><p>
      The Monasca monitoring node needs to have an interface on both the
      admin network and the public network. Monasca's backend services will
      listen on the admin network, the API services
      (<code class="systemitem">openstack-monasca-api</code>,
      <code class="systemitem">openstack-monasca-log-api</code>) will listen on all
      interfaces. <code class="systemitem">openstack-monasca-agent</code> and
      <code class="systemitem">openstack-monasca-log-agent</code> will send their logs
      and metrics to the
      <code class="systemitem">monasca-api</code>/<code class="systemitem">monasca-log-api</code>
      services to the monitoring node's public network IP address.
     </p></dd></dl></div><div id="id-1.3.3.3.2.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Protect Networks from External Access</h6><p>
    For security reasons, protect the following networks from external
    access:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <a class="xref" href="#vle-netw-adm">
     Admin Network (192.168.124/24)
    </a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#vle-netw-stor">
     Storage Network (192.168.125/24)
    </a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#vle-netw-sdn">
     Software Defined Network (os_sdn, 192.168.130/24)
    </a>
     </p></li></ul></div><p>
    Especially traffic from the cloud instances must not be able to pass
    through these networks.
   </p></div><div id="id-1.3.3.3.2.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: VLAN Settings</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>, using a VLAN for the admin network is
    only supported on a native/untagged VLAN. If you need VLAN support for the
    admin network, it must be handled at switch level.
   </p><p>
    When changing the network configuration with YaST or by editing
    <code class="filename">/etc/crowbar/network.json</code> you can define VLAN
    settings for each network. For the networks <code class="literal">nova-fixed</code>
    and <code class="literal">nova-floating</code>, however, special rules apply:
   </p><p>
    <span class="bold"><strong>nova-fixed</strong></span>: The <span class="guimenu ">USE
    VLAN</span> setting will be ignored. However, VLANs will automatically
    be used if deploying Neutron with VLAN support (using the plugins
    linuxbridge, openvswitch plus VLAN, or cisco plus VLAN). In this case, you
    need to specify a correct <span class="guimenu ">VLAN ID</span> for this network.
   </p><p>
    <span class="bold"><strong>nova-floating</strong></span>: When using a VLAN for
    <code class="literal">nova-floating</code> (which is the default), the <span class="guimenu ">USE
    VLAN</span> and <span class="guimenu ">VLAN ID</span> settings for
    <span class="guimenu ">nova-floating</span> and <span class="guimenu ">public</span> must be
    the same. When not using a VLAN for <code class="literal">nova-floating</code>, it
    must have a different physical network interface than the
    <code class="literal">nova_fixed</code> network.
   </p></div><div id="id-1.3.3.3.2.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: No IPv6 Support</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, IPv6 is not supported. This applies to the cloud
    internal networks and to the instances. You must use static IPv4 addresses
    for all network interfaces on the Admin Node, and disable IPv6 before
    deploying Crowbar on the Admin Node.
   </p></div><p>
   The following diagram shows the pre-defined SUSE <span class="productname">OpenStack</span> Cloud network in more
   detail. It demonstrates how the <span class="productname">OpenStack</span> nodes and services use the
   different networks.
  </p><div class="figure" id="id-1.3.3.3.2.11"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/cloud_network_detail.png" target="_blank"><img src="images/cloud_network_detail.png" width="" alt="SUSE OpenStack Cloud Network: Details" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 2.2: </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Network: Details </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.2.11">#</a></h6></div></div><div class="sect2 " id="sec-depl-req-network-allocation"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Address Allocation</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-allocation">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network-allocation</li></ul></div></div></div></div><p>
    The default networks set up in SUSE <span class="productname">OpenStack</span> Cloud are class C networks with 256
    IP addresses each. This limits the maximum number of instances that
    can be started simultaneously. Addresses within the networks are
    allocated as outlined in the following table. Use the YaST
    Crowbar module to make customizations (see
    <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>). The last address in the IP address
    range of each network is always reserved as the broadcast address. This
    assignment cannot be changed.
   </p><p>For an overview of the minimum number of IP addresses needed for each
    of the ranges in the network settings, see <a class="xref" href="#tab-netw-range-ip-min" title="Minimum Number of IP Addresses for Network Ranges">Table 2.1, “Minimum Number of IP Addresses for Network Ranges”</a>.
   </p><div class="table" id="tab-netw-range-ip-min"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.1: </span><span class="name">Minimum Number of IP Addresses for Network Ranges </span><a title="Permalink" class="permalink" href="#tab-netw-range-ip-min">#</a></h6></div><div class="table-contents"><table class="table" summary="Minimum Number of IP Addresses for Network Ranges" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
        <p>
         Network
        </p>
       </th><th>
        <p>
         Required Number of IP addresses
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         Admin Network
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 IP address per node (Administration Server, Control Nodes, and
           Compute Nodes)</p></li><li class="listitem "><p>1 VIP address for RabbitMQ</p></li><li class="listitem "><p>1 VIP address per cluster (per Pacemaker barclamp proposal)</p></li></ul></div>
       </td></tr><tr><td>
        <p>
         Public Network
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 IP address per node (Control Nodes and Compute Nodes)</p></li><li class="listitem "><p>1 VIP address per cluster</p></li></ul></div>
       </td></tr><tr><td>
        <p>
         BMC Network
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 IP address per node (Administration Server, Control Nodes, and
           Compute Nodes)</p></li></ul></div>
       </td></tr><tr><td>
        <p>
         Software Defined Network
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>1 IP address per node (Control Nodes and
           Compute Nodes)</p></li></ul></div>
       </td></tr></tbody></table></div></div><div id="id-1.3.3.3.2.12.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Limitations of the Default Network Proposal</h6><p>
     The default network proposal as described below limits the maximum
     number of Compute Nodes to 80, the maximum number of floating IP
     addresses to 61 and the maximum number of addresses in the nova_fixed
     network to 204.
    </p><p>
     To overcome these limitations you need to reconfigure the network setup
     by using appropriate address ranges. Do this by either using the
     YaST Crowbar module as described in
     <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>, or by manually editing the
     network template file as described in
     <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a>.
    </p></div><div class="table" id="id-1.3.3.3.2.12.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.2: </span><span class="name"><code class="systemitem">192.168.124.0/24</code> (Admin/BMC) Network Address Allocation </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.2.12.6">#</a></h6></div><div class="table-contents"><table class="table" summary="192.168.124.0/24 (Admin/BMC) Network Address Allocation" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
        <p>
         Function
        </p>
       </th><th>
        <p>
         Address
        </p>
       </th><th>
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         router
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.124.1</code>
        </p>
       </td><td>
        <p>
         Provided externally.
        </p>
       </td></tr><tr><td>
        <p>
         admin
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.124.10</code> -
         <code class="systemitem">192.168.124.11</code>
        </p>
       </td><td>
        <p>
         Fixed addresses reserved for the Administration Server.
        </p>
       </td></tr><tr><td>
        <p>
         DHCP
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.124.21</code> -
         <code class="systemitem">192.168.124.80</code>
        </p>
       </td><td>
        <p>
         Address range reserved for node allocation/installation. Determines
         the maximum number of parallel allocations/installations.
        </p>
       </td></tr><tr><td>
        <p>
         host
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.124.81</code> -
         <code class="systemitem">192.168.124.160</code>
        </p>
       </td><td>
        <p>
         Fixed addresses for the <span class="productname">OpenStack</span> nodes. Determines the maximum
         number of <span class="productname">OpenStack</span> nodes that can be deployed.
        </p>
       </td></tr><tr><td>
        <p>
         bmc vlan host
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.124.161</code>
        </p>
       </td><td>
        <p>
         Fixed address for the BMC VLAN. Used to generate a VLAN tagged
         interface on the Administration Server that can access the BMC
         network. The BMC VLAN must be in the same ranges as BMC, and
         BMC must have VLAN enabled.
        </p>
       </td></tr><tr><td>
        <p>
         bmc host
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.124.162</code> -
         <code class="systemitem">192.168.124.240</code>
        </p>
       </td><td>
        <p>
         Fixed addresses for the <span class="productname">OpenStack</span> nodes. Determines the maximum
         number of <span class="productname">OpenStack</span> nodes that can be deployed.
        </p>
       </td></tr><tr><td>
        <p>
         switch
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.124.241</code> -
         <code class="systemitem">192.168.124.250</code>
        </p>
       </td><td>
        <p>
         This range is not used in current releases and might be removed in
         the future.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.3.3.3.2.12.7"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.3: </span><span class="name"><code class="systemitem">192.168.125/24</code> (Storage) Network Address Allocation </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.2.12.7">#</a></h6></div><div class="table-contents"><table class="table" summary="192.168.125/24 (Storage) Network Address Allocation" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
        <p>
         Function
        </p>
       </th><th>
        <p>
         Address
        </p>
       </th><th>
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         host
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.125.10</code> -
         <code class="systemitem">192.168.125.239</code>
        </p>
       </td><td>
        <p>
         Each Storage Node will get an address from this range.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.3.3.3.2.12.8"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.4: </span><span class="name"><code class="systemitem">192.168.123/24</code> (Private Network/nova-fixed) Network Address Allocation </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.2.12.8">#</a></h6></div><div class="table-contents"><table class="table" summary="192.168.123/24 (Private Network/nova-fixed) Network Address Allocation" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
        <p>
         Function
        </p>
       </th><th>
        <p>
         Address
        </p>
       </th><th>
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         DHCP
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.123.1</code> -
         <code class="systemitem">192.168.123.254</code>
        </p>
       </td><td>
        <p>
         Address range for instances, routers and DHCP/DNS agents.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.3.3.3.2.12.9"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.5: </span><span class="name"><code class="systemitem">192.168.126/24</code> (Public Network nova-floating, public) Network Address Allocation </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.2.12.9">#</a></h6></div><div class="table-contents"><table class="table" summary="192.168.126/24 (Public Network nova-floating, public) Network Address Allocation" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
        <p>
         Function
        </p>
       </th><th>
        <p>
         Address
        </p>
       </th><th>
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         router
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.126.1</code>
        </p>
       </td><td>
        <p>
         Provided externally.
        </p>
       </td></tr><tr><td>
        <p>
         public host
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.126.2</code> -
         <code class="systemitem">192.168.126.127</code>
        </p>
       </td><td>
        <p>
         Public address range for external SUSE <span class="productname">OpenStack</span> Cloud components such as the
         <span class="productname">OpenStack</span> Dashboard or the API.
        </p>
       </td></tr><tr><td>
        <p>
         floating host
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.126.129</code> -
         <code class="systemitem">192.168.126.254</code>
        </p>
       </td><td>
        <p>
         Floating IP address range. Floating IP addresses can be manually assigned to
         a running instance to allow to access the guest from the
         outside. Determines the maximum number of instances that can
         concurrently be accessed from the outside.
        </p>
        <p>
         The nova_floating network is set up with a netmask of
         255.255.255.192, allowing a maximum number of 61 IP addresses. This
         range is pre-allocated by default and managed by Neutron.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.3.3.3.2.12.10"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.6: </span><span class="name"><code class="systemitem">192.168.130/24</code> (Software Defined Network) Network Address Allocation </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.2.12.10">#</a></h6></div><div class="table-contents"><table class="table" summary="192.168.130/24 (Software Defined Network) Network Address Allocation" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
        <p>
         Function
        </p>
       </th><th>
        <p>
         Address
        </p>
       </th><th>
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         host
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.130.10</code> -
         <code class="systemitem">192.168.130.254</code>
        </p>
       </td><td>
        <p>
         If Neutron is configured with <code class="literal">openvswitch</code>
         and <code class="literal">gre</code>, each network node and all
         Compute Nodes will get an IP address from this range.
        </p>
       </td></tr></tbody></table></div></div><div id="id-1.3.3.3.2.12.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Addresses for Additional Servers</h6><p>
     Addresses not used in the ranges mentioned above can be used to add
     additional servers with static addresses to SUSE <span class="productname">OpenStack</span> Cloud. Such servers
     can be used to provide additional services. A SUSE Manager server
     inside SUSE <span class="productname">OpenStack</span> Cloud, for example, must be configured using one of
     these addresses.
    </p></div></div><div class="sect2 " id="sec-depl-req-network-modes"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Modes</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network-modes</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports different network modes defined in Crowbar: <code class="literal">single</code>, <code class="literal">dual</code>, and
    <code class="literal">team</code>. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>, the networking mode
    is applied to all nodes and the Administration Server. That means that all
    machines need to meet the hardware requirements for the chosen mode. The
    network mode can be configured using the YaST Crowbar module
    (<a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>). The network mode cannot
    be changed after the cloud is deployed.
   </p><p>
    Other, more flexible network mode setups can be configured by manually
    editing the Crowbar network configuration files. See <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for more information. SUSE or a
    partner can assist you in creating a custom setup within the scope of a
    consulting services agreement (see <a class="link" href="http://www.suse.com/consulting/" target="_blank">http://www.suse.com/consulting/</a> for more information on
    SUSE consulting).
   </p><div id="id-1.3.3.3.2.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Network Device Bonding is Required for HA</h6><p>Network device bonding is required for an HA setup of
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. If you are planning to move your cloud to an
     HA setup at a later point in time, make sure to use a network
     mode in the YaST Crowbar that supports network device bonding.</p><p>Otherwise a migration to an HA setup is not supported.</p></div><div class="sect3 " id="sec-depl-req-network-modes-single"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single Network Mode</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes-single">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network-modes-single</li></ul></div></div></div></div><p>
     In single mode you use one Ethernet card for all the traffic:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/cloud_networking_single_mode.png" target="_blank"><img src="images/cloud_networking_single_mode.png" width="" alt="Single Network Mode" /></a></div></div></div><div class="sect3 " id="sec-depl-req-network-modes-dual"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Dual Network Mode</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes-dual">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network-modes-dual</li></ul></div></div></div></div><p>
     Dual mode needs two Ethernet cards (on all nodes but Administration Server) to completely separate traffic between the Admin Network and the public network:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/cloud_networking_dual_mode.png" target="_blank"><img src="images/cloud_networking_dual_mode.png" width="" alt="Dual Network Mode" /></a></div></div></div><div class="sect3 " id="sec-depl-req-network-modes-teaming"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Team Network Mode</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes-teaming">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network-modes-teaming</li></ul></div></div></div></div><p>
     Team mode is similar to single mode, except that you
     combine several Ethernet cards to a <span class="quote">“<span class="quote ">bond</span>”</span> (network device
     bonding). Team mode needs two or more Ethernet cards.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/cloud_networking_team_mode.png" target="_blank"><img src="images/cloud_networking_team_mode.png" width="" alt="Team Network Mode" /></a></div></div><p>
     When using team mode, you must choose a <span class="quote">“<span class="quote ">bonding
     policy</span>”</span> that defines how to use the combined Ethernet cards. You
     can either set them up for fault tolerance, performance (load balancing),
     or a combination of both.
    </p></div></div><div class="sect2 " id="sec-depl-req-network-bastion"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing the Administration Server via a Bastion Network</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-bastion">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network-bastion</li></ul></div></div></div></div><p>
    Enabling access to the Administration Server from another network requires an external gateway. This option offers
    maximum flexibility, but requires additional hardware and may be less
    secure than you require. Therefore SUSE <span class="productname">OpenStack</span> Cloud offers a second option for
    accessing the Administration Server: the bastion network. You only need a
    dedicated Ethernet card and a static IP address from the external
    network to set it up.
   </p><p>
    The bastion network setup (see <a class="xref" href="#sec-depl-adm-inst-crowbar-mode-bastion" title="7.3.1. Setting Up a Bastion Network">Section 7.3.1, “Setting Up a Bastion Network”</a> for setup instructions)
    enables logging in to the Administration Server via SSH from the company network. A
    direct login to other nodes in the cloud is not possible. However, the
    Administration Server can act as a <span class="quote">“<span class="quote ">jump host</span>”</span>: First
    log in to the Administration Server via SSH, then log in via SSH to
    other nodes.
   </p></div><div class="sect2 " id="sec-depl-req-network-dns"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS and Host Names</span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-dns">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-network-dns</li></ul></div></div></div></div><p>
    The Administration Server acts as a name server for all nodes in the cloud. If
    the Administration Server has access to the outside, then you can add additional
    name servers that are automatically used to forward requests. If
    additional name servers are found on your cloud deployment, the name server
    on the Administration Server is automatically configured to forward requests
    for non-local records to these servers.
   </p><p>
    The Administration Server must have a fully qualified host
    name. The domain name you specify is used for the DNS zone. It is
    required to use a sub-domain such as
    <em class="replaceable ">cloud.example.com</em>. The Administration Server
    must have authority over the domain it is on so that it can
    create records for discovered nodes. As a result, it will not forward
    requests for names it cannot resolve in this domain, and thus cannot
    resolve names for the second-level domain, .e.g. <em class="replaceable ">example.com</em>, other
    than for nodes in the cloud.
   </p><p>
    This host name must not be changed after SUSE <span class="productname">OpenStack</span> Cloud has been deployed.
    The <span class="productname">OpenStack</span> nodes are named after their MAC address by default,
    but you can provide aliases, which are easier to remember when
    allocating the nodes. The aliases for the <span class="productname">OpenStack</span> nodes can be
    changed at any time. It is useful to have a list of MAC addresses and
    the intended use of the corresponding host at hand when deploying the
    <span class="productname">OpenStack</span> nodes.
   </p></div></div><div class="sect1 " id="sec-depl-req-storage"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persistent Storage</span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-storage</li></ul></div></div></div></div><p>
   When talking about <span class="quote">“<span class="quote ">persistent storage</span>”</span> on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>,
   there are two completely different aspects to discuss: 1) the block and
   object storage services SUSE <span class="productname">OpenStack</span> Cloud offers, 2) the
   hardware related storage aspects on the different node types.
  </p><div id="id-1.3.3.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Persistent vs. Ephemeral Storage</h6><p>
    Block and object storage are persistent storage models where files or
    images are stored until they are explicitly deleted. SUSE <span class="productname">OpenStack</span> Cloud also
    offers ephemeral storage for images attached to instances. These
    ephemeral images only exist during the life of an instance and are
    deleted when the guest is terminated. See
    <a class="xref" href="#sec-depl-req-storage-hardware-compute" title="2.2.2.3. Compute Nodes">Section 2.2.2.3, “Compute Nodes”</a> for more
    information.
   </p></div><div class="sect2 " id="sec-depl-req-storage-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Storage Services</span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-services">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-storage-services</li></ul></div></div></div></div><p>
    SUSE <span class="productname">OpenStack</span> Cloud offers two different types of services
    for persistent storage: object and block storage. Object storage lets
    you upload and download files (similar to an FTP server), whereas a
    block storage provides mountable devices (similar to a hard disk
    partition). SUSE <span class="productname">OpenStack</span> Cloud provides a repository to store the
    virtual disk images used to start instances.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.3.3.3.4.3.1"><span class="term ">Object Storage with Swift</span></dt><dd><p>
       The <span class="productname">OpenStack</span> object storage service is called Swift. The
       storage component of Swift (swift-storage) must be
       deployed on dedicated nodes where no other cloud services run. Deploy at
       least two Swift nodes to provide redundant storage. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is configured to
       always use all unused disks on a node for storage.
      </p><p>
       Swift can optionally be used by Glance, the service
       that manages the images used to boot the instances. Offering
       object storage with Swift is optional.
      </p></dd><dt id="id-1.3.3.3.3.4.3.2"><span class="term ">Block Storage</span></dt><dd><p>
       Block storage on SUSE <span class="productname">OpenStack</span> Cloud is provided by Cinder.
       Cinder can use a variety of storage back-ends, including
       network storage solutions like NetApp or EMC. It is also possible to
       use local disks for block storage. A list of drivers available for
       Cinder and the features supported for each driver is
       available from the <em class="citetitle ">CinderSupportMatrix</em> at
       <a class="link" href="https://wiki.openstack.org/wiki/CinderSupportMatrix" target="_blank">https://wiki.openstack.org/wiki/CinderSupportMatrix</a>.
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> ships with <span class="productname">OpenStack</span>
       Pike.
      </p><p>
       Alternatively, Cinder can use Ceph RBD as a back-end.  Ceph
       offers data security and speed by storing the the content on a dedicated
       Ceph cluster.
      </p></dd><dt id="id-1.3.3.3.3.4.3.3"><span class="term ">The Glance Image Repository</span></dt><dd><p>
       Glance provides a catalog and repository for virtual disk images
       used to start the instances. Glance is installed on a
       Control Node. It uses Swift, Ceph, or a
       directory on the Control Node to store the images. The image
       directory can either be a local directory or an NFS share.
      </p></dd></dl></div></div><div class="sect2 " id="sec-depl-req-storage-hardware"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Hardware Requirements</span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-storage-hardware</li></ul></div></div></div></div><p>
    Each node in SUSE <span class="productname">OpenStack</span> Cloud needs sufficient disk space to store both the operating system and  additional data.
    Requirements and recommendations for the various node types are listed
    below.
   </p><div id="id-1.3.3.3.3.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Choose a Hard Disk for the Operating System Installation</h6><p>
     The operating system will always be installed on the
     <span class="emphasis"><em>first</em></span> hard disk. This is the disk that is listed
     <span class="emphasis"><em>first</em></span> in the BIOS, the one from which the machine
     will boot. Make sure that the hard disk the operating system is installed on will be recognized as the first disk.
    </p></div><div class="sect3 " id="sec-depl-req-storage-hardware-admin"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Administration Server</span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-admin">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-storage-hardware-admin</li></ul></div></div></div></div><p>
     If you store the update repositories directly on the Administration Server (see
     <a class="xref" href="#sec-depl-req-repos" title="2.5.2. Product and Update Repositories">Section 2.5.2, “Product and Update Repositories”</a>), we recommend mounting <code class="filename">/srv</code> on a separate partition or volume with a minimum of 30 GB space.
    </p><p>
     Log files from all nodes in SUSE <span class="productname">OpenStack</span> Cloud are stored on the Administration Server
     under <code class="filename">/var/log</code> (see
     <a class="xref" href="#sec-deploy-logs-adminserv" title="19.1. On the Administration Server">Section 19.1, “On the Administration Server”</a> for a complete list).
     The message service RabbitMQ requires 1 GB of free space
     in <code class="filename">/var</code>.
    </p></div><div class="sect3 " id="sec-depl-req-storage-hardware-control"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-control">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-storage-hardware-control</li></ul></div></div></div></div><p>
     Depending on how the services are set up, Glance and
     Cinder may require additional disk space on the
     Control Node on which they are running. Glance may be configured
     to use a local directory, whereas Cinder may use a local
     image file for storage. For performance and scalability reasons this is
     only recommended for test setups. Make sure there is sufficient free
     disk space available if you use a local file for storage.
    </p><p>
     Cinder may be configured to use local disks for storage (configuration option <code class="literal">raw</code>). If you choose this setup, we recommend deploying the <span class="guimenu ">cinder-volume</span> role to one or more dedicated Control Nodes. Those should be equipped with several disks providing sufficient storage space. It may also be necessary to equip this node with two or more bonded network cards, since it will generate heavy network traffic. Bonded network cards require a special setup for this node. For details, refer to <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a>.
    </p><p>
     Live migration for Xen instances requires exporting <code class="filename">/var/lib/nova/instances</code> on the Control Node hosting <code class="systemitem">nova-controller</code>. This directory will host a copy of the root disk of <span class="emphasis"><em>all</em></span> Xen instances in the cloud and needs to have sufficient disk space. We strongly recommended using a separate block device for this directory, preferably a RAID device to ensure data security.
    </p></div><div class="sect3 " id="sec-depl-req-storage-hardware-compute"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-compute">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-storage-hardware-compute</li></ul></div></div></div></div><p>
     Unless an instance is started via “Boot from Volume”, it is started with at least one disk, which is a copy of the image from which it has been started. Depending on the flavor you start, the instance may also have a second, so-called <span class="quote">“<span class="quote ">ephemeral</span>”</span>
     disk. The size of the root disk depends on the image itself.
     Ephemeral disks are always created as sparse image files that grow up
     to a defined size as they are <span class="quote">“<span class="quote ">filled</span>”</span>. By default
     ephemeral disks have a size of 10 GB.
    </p><p>
     Both disks, root images and ephemeral disk, are directly bound to the
     instance and are deleted when the instance is terminated.
     These disks are bound to the Compute Node on which the
     instance has been started. The disks are created under
     <code class="filename">/var/lib/nova</code> on the Compute Node. Your
     Compute Nodes should be equipped with enough disk space to store the
     root images and ephemeral disks.
    </p><div id="id-1.3.3.3.3.5.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Ephemeral Disks vs. Block Storage</h6><p>
      Do not confuse ephemeral disks with persistent block storage. In
      addition to an ephemeral disk, which is automatically provided with
      most instance flavors, you can optionally add a persistent storage
      device provided by Cinder. Ephemeral disks are deleted when
      the instance terminates, while persistent storage devices can be
      reused in another instance.
     </p></div><p>
     The maximum disk space required on a compute node depends on the
     available flavors. A flavor specifies the number of CPUs, RAM, and disk
     size of an instance. Several flavors ranging from
     <span class="guimenu ">tiny</span> (1 CPU, 512 MB RAM, no ephemeral disk) to
     <span class="guimenu ">xlarge</span> (8 CPUs, 8 GB RAM, 10 GB ephemeral disk) are
     available by default. Adding custom flavors, and editing and deleting
     existing flavors is also supported.
    </p><p>
     To calculate the minimum disk space needed on a compute node, you need
     to determine the highest disk-space-to-RAM ratio from your flavors.
     For example:
    </p><table border="0" summary="Simple list" class="simplelist "><tr><td>
      Flavor small: 2 GB RAM, 100 GB ephemeral disk =&gt; 50 GB disk /1 GB RAM
     </td></tr><tr><td>
      Flavor large: 8 GB RAM, 200 GB ephemeral disk =&gt; 25 GB disk /1 GB RAM
     </td></tr></table><p>
     So, 50 GB disk /1 GB RAM is the ratio that matters. If you multiply
     that value by the amount of RAM in GB available on your compute node,
     you have the minimum disk space required by ephemeral disks. Pad that
     value with sufficient space for the root disks plus a buffer to leave room for flavors with a higher disk-space-to-RAM ratio in
     the future.
    </p><div id="id-1.3.3.3.3.5.6.9" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Overcommitting Disk Space</h6><p>
      The scheduler that decides in which node an instance is started
      does not check for available disk space. If there is no disk space
      left on a compute node, this will not only cause data loss on the
      instances, but the compute node itself will also stop operating.
      Therefore you must make sure all compute nodes are equipped with
      enough hard disk space.
     </p></div></div><div class="sect3 " id="sec-depl-req-storage-hardware-store"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Nodes (optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-store">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-storage-hardware-store</li></ul></div></div></div></div><p> The block storage service
    Ceph RBD and the object storage service Swift need to be deployed onto
    dedicated nodes—it is not possible to mix these services. The Swift
    component requires at least two machines (more are recommended) to store
    data redundantly. For information on hardware requirements for Ceph, see
    <a class="link" href="https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#storage-bp-hwreq" target="_blank">https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#storage-bp-hwreq</a>
    </p><p>
     Each Ceph/Swift Storage Node needs at least two hard disks.
     The first one will be used for the operating system installation, while
     the others can be used for storage. We recommend equipping
     the storage nodes with as many disks as possible.
    </p><p>
     Using RAID on Swift storage nodes is not supported.
     Swift takes care of redundancy and replication on its own. Using
     RAID with Swift would also result in a huge performance
     penalty.
    </p></div></div></div><div class="sect1 " id="sec-depl-req-ssl"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SSL Encryption</span> <a title="Permalink" class="permalink" href="#sec-depl-req-ssl">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-ssl</li></ul></div></div></div></div><p>
   Whenever non-public data travels over a network it must be encrypted.
   Encryption protects the integrity and confidentiality of data. Therefore
   you should enable SSL support when deploying SUSE <span class="productname">OpenStack</span> Cloud to production. (SSL
   is not enabled by default as it requires you to provide certificates.)
   The following services (and their APIs, if available) can use SSL:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Cinder
    </p></li><li class="listitem "><p>
     Horizon
    </p></li><li class="listitem "><p>
     Glance
    </p></li><li class="listitem "><p>
     Heat
    </p></li><li class="listitem "><p>
     Keystone
    </p></li><li class="listitem "><p>
     Manila
    </p></li><li class="listitem "><p>
     Neutron
    </p></li><li class="listitem "><p>
     Nova
    </p></li><li class="listitem "><p>
     Swift
    </p></li><li class="listitem "><p>
     VNC
    </p></li><li class="listitem "><p>
    RabbitMQ
    </p></li><li class="listitem "><p>
    Ironic
    </p></li><li class="listitem "><p>
    Magnum
    </p></li></ul></div><p>
   You have two options for deploying your SSL certificates. You may use a single shared certificate for all services on each node, or provide individual certificates for each service. The minimum requirement is a single certificate for the Control Node and all services installed on it.
  </p><p>
   Certificates must be signed by a trusted authority. Refer to
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-apache2-ssl" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-apache2-ssl</a>
   for instructions on how to create and sign them.
  </p><div id="id-1.3.3.3.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Host Names</h6><p>
    Each SSL certificate is issued for a certain host name and, optionally,
    for alternative host names (via the <code class="literal">AlternativeName</code>
    option). Each publicly available node in SUSE <span class="productname">OpenStack</span> Cloud has two host
    names—an internal and a public one. The SSL certificate needs
    to be issued for <span class="emphasis"><em>both</em></span> internal and public names.
   </p><p>
    The internal name has the following scheme:
   </p><div class="verbatim-wrap"><pre class="screen">d<em class="replaceable ">MACADDRESS</em>.<em class="replaceable ">FQDN</em></pre></div><p>
    <em class="replaceable ">MACADDRESS</em> is the MAC address of the
    interface used to boot the machine via PXE. All letters are turned
    lowercase and all colons are replaced with dashes. For example,
    <code class="literal">00-00-5E-00-53-00</code>. <em class="replaceable ">FQDN</em> is
    the fully qualified domain name. An example name looks like this:
   </p><div class="verbatim-wrap"><pre class="screen">d00-00-5E-00-53-00.example.com</pre></div><p>
    Unless you have entered a custom <span class="guimenu ">Public Name</span> for a
    client (see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a> for details),
    the public name is the same as the internal name prefixed by
    <code class="literal">public</code>:
   </p><div class="verbatim-wrap"><pre class="screen">public-d00-00-5E-00-53-00.example.com</pre></div><p>
    To look up the node names open the Crowbar Web interface and click the
    name of a node in the <span class="guimenu ">Node Dashboard</span>. The names are
    listed as <span class="guimenu ">Full Name</span> and <span class="guimenu ">Public
    Name</span>.
   </p></div></div><div class="sect1 " id="sec-depl-req-hardware"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware Requirements</span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-hardware</li></ul></div></div></div></div><p>
   Precise hardware requirements can only be listed for the Administration Server and
   the <span class="productname">OpenStack</span> Control Node. The requirements of the <span class="productname">OpenStack</span>
   Compute and Storage Nodes depends on the number of concurrent
   instances and their virtual hardware equipment.
  </p><p>
   A minimum of three machines are required for a SUSE <span class="productname">OpenStack</span> Cloud:
   one Administration Server, one Control Node, and one Compute Node. You also need a gateway providing access to the public network.
   Deploying storage requires additional nodes: at least two nodes for
   Swift and a minimum of four nodes for Ceph.
  </p><div id="id-1.3.3.3.5.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Virtual/Physical Machines and Architecture</h6><p>
    Deploying SUSE <span class="productname">OpenStack</span> Cloud functions to virtual machines is only supported for the
    Administration Server—all other nodes need to be physical hardware. Although the
    Control Node can be virtualized in test environments, this is not
    supported for production systems.
   </p><p>
    SUSE <span class="productname">OpenStack</span> Cloud currently only runs on <code class="literal">x86_64</code> hardware.
   </p></div><div class="sect2 " id="sec-depl-req-hardware-admserv"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Administration Server</span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-admserv">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-hardware-admserv</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Architecture: x86_64.
     </p></li><li class="listitem "><p>
      RAM: at least 4 GB, 8 GB recommended. The demand for memory depends on
      the total number of nodes in SUSE <span class="productname">OpenStack</span> Cloud—the higher the number of
      nodes, the more RAM is needed. A deployment with 50 nodes requires a
      minimum of 24 GB RAM for each Control Node.
     </p></li><li class="listitem "><p>
      Hard disk: at least 50 GB. We recommend putting
      <code class="filename">/srv</code> on a separate partition with at least
      additional 30 GB of space. Alternatively, you can mount the update
      repositories from another server (see <a class="xref" href="#sec-depl-req-repos" title="2.5.2. Product and Update Repositories">Section 2.5.2, “Product and Update Repositories”</a> for details).
     </p></li><li class="listitem "><p>
      Number of network cards: 1 for single and dual mode, 2 or more for
      team mode. Additional networks such as the bastion network and/or a
      separate BMC network each need an additional network card. See
      <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for details.
     </p></li><li class="listitem "><p>
      Can be deployed on physical hardware or a virtual machine.
     </p></li></ul></div></div><div class="sect2 " id="sec-depl-req-hardware-contrnode"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Node</span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-contrnode">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-hardware-contrnode</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Architecture: x86_64.
     </p></li><li class="listitem "><p>
      RAM: at least 8 GB, 12 GB when deploying a single Control Node, and 32 GB
      recommended.
     </p></li><li class="listitem "><p>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. See <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for details.
     </p></li><li class="listitem "><p>
      Hard disk: See
      <a class="xref" href="#sec-depl-req-storage-hardware-control" title="2.2.2.2. Control Nodes">Section 2.2.2.2, “Control Nodes”</a>.
     </p></li></ul></div></div><div class="sect2 " id="sec-depl-req-hardware-compnode"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Node</span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-compnode">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-hardware-compnode</li></ul></div></div></div></div><p>
    The Compute Nodes need to be equipped with a sufficient amount of RAM
    and CPUs, matching the numbers required by the maximum number of
    instances running concurrently. An instance started in
    SUSE <span class="productname">OpenStack</span> Cloud cannot share resources from several physical nodes. It uses
    the resources of the node on which it was started. So if you
    offer a flavor (see <a class="xref" href="#gloss-flavor" title="Flavor">Flavor</a> for a definition)
    with 8 CPUs and 12 GB RAM, at least one of your nodes should be able to
    provide these resources. Add 1 GB RAM for every two nodes
    (including Control Nodes and Storage Nodes) deployed in your cloud.
   </p><p>
    See <a class="xref" href="#sec-depl-req-storage-hardware-compute" title="2.2.2.3. Compute Nodes">Section 2.2.2.3, “Compute Nodes”</a> for storage
    requirements.
   </p></div><div class="sect2 " id="sec-depl-req-hardware-stornode"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Node</span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-stornode">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-hardware-stornode</li></ul></div></div></div></div><p>
    Usually a single CPU and a minimum of 4 GB RAM are sufficient for the Storage Nodes. Memory requirements increase depending on the total number of
    nodes in SUSE <span class="productname">OpenStack</span> Cloud—the higher the number of nodes, the more RAM you need. A deployment with 50 nodes requires a minimum of 20 GB for each
    Storage Node. If you use Ceph as storage, the storage nodes should be
    equipped with an additional 2 GB RAM per OSD (Ceph object storage
    daemon).
   </p><p>
    For storage requirements, see <a class="xref" href="#sec-depl-req-storage-hardware-store" title="2.2.2.4. Storage Nodes (optional)">Section 2.2.2.4, “Storage Nodes (optional)”</a>.
   </p></div><div class="sect2 " id="sec-depl-req-hardware-monnode"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Node</span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-monnode">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-hardware-monnode</li></ul></div></div></div></div><p>
     The Monasca Node is a dedicated physical machine that runs the
     <code class="literal">monasca-server</code> role. This node is used for
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.  Hardware requirements for the Monasca Node are as
     follows:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Architecture: x86_64
     </p></li><li class="listitem "><p>
       RAM: At least 32 GB, 64 GB or more is recommended
     </p></li><li class="listitem "><p>
       CPU: At least 8 cores, 16 cores or more is recommended
     </p></li><li class="listitem "><p>
       Hard Disk: SSD is strongly recommended
     </p></li></ul></div><p>
     The following formula can be used to calculate the required disk space:
   </p><div class="verbatim-wrap"><pre class="screen">200 GB + ["number of nodes" * "retention period" * ("space for log
   data/day" + "space for metrics data/day") ]</pre></div><p>
    The recommended values for the formula are as follows:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Retention period = 60 days for InfluxDB and Elasticsearch
     </p></li><li class="listitem "><p>
       Space for daily log data = 2GB
     </p></li><li class="listitem "><p>
       Space for daily metrics data = 50MB
     </p></li></ul></div><p>
     The formula is based on the following log data assumptions:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Approximately 50 log files per node
     </p></li><li class="listitem "><p>
       Approximately 1 log entry per file per sec
     </p></li><li class="listitem "><p>
       200 bytes in size
     </p></li></ul></div><p>
     The formula is based on the following metrics data assumptions:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       400 metrics per node
     </p></li><li class="listitem "><p>
      Time interval of 30 seconds
     </p></li><li class="listitem "><p>
      20 bytes in size
     </p></li></ul></div><p>
    The formula provides only a rough estimation of the required disk
    space. There are several factors that can affect disk space
    requirements. This includes the exact combination of services that run on
    your <span class="productname">OpenStack</span> node actual cloud usage pattern, and whether any or all
    services have debug logging enabled.
   </p></div></div><div class="sect1 " id="sec-depl-req-software"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Requirements</span> <a title="Permalink" class="permalink" href="#sec-depl-req-software">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-software</li></ul></div></div></div></div><p>
   All nodes and the Administration Server in SUSE <span class="productname">OpenStack</span> Cloud run on SUSE Linux Enterprise Server 12 SP3. Subscriptions for
   the following components are available as one- or three-year subscriptions
   including priority support:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Control Node + <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Administration Server (including
     entitlements for High Availability and SUSE Linux Enterprise Server 12 SP3)
    </p></li><li class="listitem "><p>
     Additional <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Control Node (including
     entitlements for High Availability and SUSE Linux Enterprise Server 12 SP3)
    </p></li><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Compute Node (excluding entitlements for High Availability and
     SUSE Linux Enterprise Server 12 SP3)
    </p></li><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Swift node (excluding entitlements for High Availability
     and SUSE Linux Enterprise Server 12 SP3)
    </p></li></ul></div><p>
   SUSE Linux Enterprise Server 12 SP3, HA entitlements for Compute Nodes and Swift Storage Nodes, and entitlements for guest operating systems need
   to be purchased separately. Refer to
   <a class="link" href="http://www.suse.com/products/suse-openstack-cloud/how-to-buy/" target="_blank">http://www.suse.com/products/suse-openstack-cloud/how-to-buy/</a>
   for more information on licensing and pricing.
  </p><p>
   Running an external Ceph cluster (optional) with SUSE <span class="productname">OpenStack</span> Cloud  requires an additional SUSE Enterprise Storage
   subscription. Refer to <a class="link" href="https://www.suse.com/products/suse-enterprise-storage/" target="_blank">https://www.suse.com/products/suse-enterprise-storage/</a> and
   <a class="link" href="https://www.suse.com/products/suse-openstack-cloud/frequently-asked-questions" target="_blank">https://www.suse.com/products/suse-openstack-cloud/frequently-asked-questions</a>
   for more information.
  </p><div id="id-1.3.3.3.6.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: SUSE Account</h6><p>
    A SUSE account is needed for product registration and access to
    update repositories. If you do not already have one, go to
    <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a> to create it.
   </p></div><div class="sect2 " id="sec-depl-req-software-optional"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional Component: SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#sec-depl-req-software-optional">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-software-optional</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> can be extended by SUSE Enterprise Storage for setting up a Ceph cluster
    providing block storage services. To store virtual disks for instances, SUSE <span class="productname">OpenStack</span> Cloud uses block storage provided by the Cinder
    module. Cinder itself needs a back-end providing storage. In
    production environments this usually is a network storage
    solution. Cinder can use a variety of network storage back-ends, among them solutions from EMC, Fujitsu, or NetApp. In case your
    organization does not provide a network storage solution that can be used
    with SUSE <span class="productname">OpenStack</span> Cloud, you can set up a Ceph cluster with SUSE Enterprise Storage. SUSE Enterprise Storage
    provides a reliable and fast distributed storage architecture using
    commodity hardware platforms.
   </p></div><div class="sect2 " id="sec-depl-req-repos"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Product and Update Repositories</span> <a title="Permalink" class="permalink" href="#sec-depl-req-repos">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-repos</li></ul></div></div></div></div><p>
    You need seven software repositories to deploy SUSE <span class="productname">OpenStack</span> Cloud and to keep a running SUSE <span class="productname">OpenStack</span> Cloud up-to-date. This includes the static product repositories, which do not change over the
    product life cycle, and the update repositories, which constantly change.
    The following repositories are needed:
   </p><div class="variablelist "><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="name">Mandatory Repositories </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.6.8.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.3.3.6.8.3.2"><span class="term ">SUSE Linux Enterprise Server 12 SP3 Product</span></dt><dd><p>
       The SUSE Linux Enterprise Server 12 SP3 product repository is a copy of the installation
       media (DVD #1) for SUSE Linux Enterprise Server. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
       <span class="phrase"><span class="phrase">8</span></span> it is required to have it available locally on the
       Administration Server. This repository requires approximately 3.5 GB of hard
       disk space.
      </p></dd><dt id="id-1.3.3.3.6.8.3.3"><span class="term "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> Product</span></dt><dd><p>
       The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> product repository is a copy
       of the installation media (DVD #1) for SUSE <span class="productname">OpenStack</span> Cloud. It can either be
       made available remotely via HTTP, or locally on the Administration Server. We recommend the latter since it makes the setup of the Administration Server
       easier. This repository requires approximately 500 MB of hard disk
       space.
      </p></dd><dt id="id-1.3.3.3.6.8.3.4"><span class="term ">PTF</span></dt><dd><p>
       This repository is created automatically on the Administration Server when you install the
       SUSE <span class="productname">OpenStack</span> Cloud add-on product. It serves as a repository for
       <span class="quote">“<span class="quote ">Program Temporary Fixes</span>”</span> (PTF), which are part of the
       SUSE support program.
      </p></dd><dt id="id-1.3.3.3.6.8.3.5"><span class="term ">SLES12-SP3-Pool and <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Pool</span></dt><dd><p>
       The SUSE Linux Enterprise Server and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> repositories contain all binary
       RPMs from the installation media, plus pattern information and
       support status metadata. These repositories are served from SUSE Customer Center
       and need to be kept in synchronization with their sources. Make them  available remotely via an existing SMT or SUSE Manager
       server. Alternatively, make them available locally on the Administration Server
       by installing a local SMT server, by mounting or synchronizing a
       remote directory, or by copying them.
      </p></dd><dt id="id-1.3.3.3.6.8.3.6"><span class="term ">SLES12-SP3-Updates and <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Updates</span></dt><dd><p>
       These repositories contain maintenance updates to packages in the
       corresponding Pool repositories. These repositories are served from
       SUSE Customer Center and need to be kept synchronized with their sources. Make them available remotely via an existing SMT or
       SUSE Manager server, or locally on the Administration Server by installing a
       local SMT server, by mounting or synchronizing a remote
       directory, or by regularly copying them.
      </p></dd></dl></div><p>
    As explained in <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a>, Control Nodes in SUSE <span class="productname">OpenStack</span> Cloud
    can optionally be made highly available with the SUSE Linux Enterprise
    High Availability Extension. The following repositories are
    required to deploy SLES High Availability Extension nodes:
   </p><div class="variablelist "><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="name">Optional Repositories </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.6.8.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.3.3.6.8.5.2"><span class="term ">SLE12-HA12-SP3-Pool</span></dt><dd><p>
       The pool repositories contain all binary RPMs from the installation
       media, plus pattern information and support status metadata. These
       repositories are served from SUSE Customer Center and need to be kept in
       synchronization with their sources. Make them available
       remotely via an existing SMT or SUSE Manager server. Alternatively, make
       them available locally on the Administration Server by installing a local SMT server, by
       mounting or synchronizing a remote directory, or by copying them.
      </p></dd><dt id="id-1.3.3.3.6.8.5.3"><span class="term ">SLE12-HA12-SP3-Updates</span></dt><dd><p>
       These repositories contain maintenance updates to packages in the
       corresponding pool repositories. These repositories are served from
       SUSE Customer Center and need to be kept synchronized with their sources. Make them
       available remotely via an existing SMT or SUSE Manager server, or locally
       on the Administration Server by installing a local SMT server, by mounting or
       synchronizing a remote directory, or by regularly copying them.
      </p></dd></dl></div><p>
    The product repositories for SUSE Linux Enterprise Server 12 SP3 and
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> do not change during the life cycle
    of a product. Thus, they can be copied to the destination directory from the
    installation media. However, the pool and update repositories must be
    kept synchronized with their sources on the SUSE Customer Center. SUSE
    offers two products that synchronize repositories and make
    them available within your organization: SUSE Manager
    (<a class="link" href="http://www.suse.com/products/suse-manager/" target="_blank">http://www.suse.com/products/suse-manager/</a>, and
    Subscription Management Tool (which ships with SUSE Linux Enterprise Server 12 SP3).
   </p><p>
    All repositories must be served via HTTP to be
    available for SUSE <span class="productname">OpenStack</span> Cloud deployment. Repositories that are installed on the Administration Server are made available by the Apache Web
    server running on the Administration Server. If your organization already uses
    SUSE Manager or SMT, you can use the repositories provided by these
    servers.
   </p><p>
    Making the repositories locally available on the Administration Server has the
    advantage of a simple network setup within SUSE <span class="productname">OpenStack</span> Cloud, and it allows you to
    seal off the SUSE <span class="productname">OpenStack</span> Cloud network from other networks in your
    organization. Hosting the repositories on a remote server has
    the advantage of using existing resources and services, and it makes
    setting up the Administration Server much easier. However, this requires a custom network setup
    for SUSE <span class="productname">OpenStack</span> Cloud, since the Administration Server needs access to the remote
    server.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.3.3.6.8.9.1"><span class="term ">Installing a Subscription Management Tool (SMT) Server on the Administration Server</span></dt><dd><p>
       The SMT server, shipping with SUSE Linux Enterprise Server 12 SP3, regularly
       synchronizes repository data from SUSE Customer Center with your local host.
       Installing the SMT server on the Administration Server is recommended if
       you do not have access to update repositories from elsewhere within
       your organization. This option requires the Administration Server to have Internet access.
      </p></dd><dt id="id-1.3.3.3.6.8.9.2"><span class="term ">Using a Remote SMT Server</span></dt><dd><p>
       If you already run an SMT server within your organization, you
       can use it within SUSE <span class="productname">OpenStack</span> Cloud. When using a remote SMT server,
       update repositories are served directly from the SMT server.
       Each node is configured with these repositories upon its initial
       setup.
      </p><p>
       The SMT server needs to be accessible from the Administration Server and
       all nodes in SUSE <span class="productname">OpenStack</span> Cloud (via one or more gateways). Resolving the
       server's host name also needs to work.
      </p></dd><dt id="id-1.3.3.3.6.8.9.3"><span class="term ">Using a SUSE Manager Server</span></dt><dd><p>
       Each client that is managed by SUSE Manager needs to register with the
       SUSE Manager server. Therefore the SUSE Manager support can only be installed
       after the nodes have been deployed. SUSE Linux Enterprise Server 12 SP3 must be set up
       for autoinstallation on the SUSE Manager server in order to use repositories
       provided by SUSE Manager during node deployment.
      </p><p>
       The server needs to be accessible from the Administration Server and all nodes
       in SUSE <span class="productname">OpenStack</span> Cloud (via one or more gateways). Resolving the server's host
       name also needs to work.
      </p></dd><dt id="id-1.3.3.3.6.8.9.4"><span class="term ">Using Existing Repositories</span></dt><dd><p>
       If you can access existing repositories from within your company
       network from the Administration Server, you have the following options: mount, synchronize, or
       manually transfer these repositories to the required locations on the
       Administration Server.
      </p></dd></dl></div></div></div><div class="sect1 " id="sec-depl-req-ha"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability</span> <a title="Permalink" class="permalink" href="#sec-depl-req-ha">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-ha</li></ul></div></div></div></div><p>
   Several components and services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> are potentially single
   points of failure that may cause system downtime and data loss if they
   fail.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> provides various mechanisms to ensure that the crucial
   components and services are highly available. The following sections
   provide an overview of components on each node that can be made highly available. For making the Control Node functions and the
   Compute Nodes highly available, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> uses the cluster software SUSE Linux Enterprise
   High Availability Extension. Make sure to thoroughly read <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a> to learn about additional requirements for high availability deployments.
  </p><div class="sect2 " id="sec-depl-req-ha-admin"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability of the Administration Server</span> <a title="Permalink" class="permalink" href="#sec-depl-req-ha-admin">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-ha-admin</li></ul></div></div></div></div><p>
    The Administration Server provides all services needed to manage and deploy all
    other nodes in the cloud. If the Administration Server is not available, new
    cloud nodes cannot be allocated, and you cannot add new roles to cloud
    nodes.
   </p><p>
    However, only two services on the Administration Server are single points of
    failure, without which the cloud cannot continue to run properly: DNS
    and NTP.
   </p><div class="sect3 " id="sec-depl-req-ha-admin-spof"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Administration Server—Avoiding Points of Failure</span> <a title="Permalink" class="permalink" href="#sec-depl-req-ha-admin-spof">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-ha-admin-spof</li></ul></div></div></div></div><p>
     To avoid DNS and NTP as potential points of failure, deploy the roles
     <code class="systemitem">dns-server</code> and
     <code class="systemitem">ntp-server</code> to multiple nodes.
    </p><div id="id-1.3.3.3.7.4.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Access to External Network</h6><p>
      If any configured DNS forwarder or NTP external server is not
      reachable through the admin network from these nodes, allocate an
      address in the public network for each node that has the
      <code class="systemitem">dns-server</code> and
      <code class="systemitem">ntp-server</code> roles:
     </p><div class="verbatim-wrap"><pre class="screen">crowbar network allocate_ip default `hostname -f` public host</pre></div><p>
      Then the nodes can use the public gateway to reach the external
      servers. The change will only become effective after the next run of
      <code class="command">chef-client</code> on the affected nodes.
     </p></div></div></div><div class="sect2 " id="sec-depl-reg-ha-control"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability of the Control Node(s)</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-control">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-control</li></ul></div></div></div></div><p>
    The Control Node(s) usually run a variety of services without which
    the cloud would not be able to run properly.
   </p><div class="sect3 " id="sec-depl-reg-ha-control-spof"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Node(s)—Avoiding Points of Failure</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-control-spof">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-control-spof</li></ul></div></div></div></div><p>
     To prevent the cloud from avoidable downtime if one or more
     Control Nodes fail, you can make the
     following roles highly available: 
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="systemitem">database-server</code>
       (<code class="systemitem">database</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">keystone-server</code>
       (<code class="systemitem">keystone</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">rabbitmq-server</code>
       (<code class="systemitem">rabbitmq</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">swift-proxy</code> (<code class="systemitem">swift</code>
       barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">glance-server</code>
       (<code class="systemitem">glance</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">cinder-controller</code>
       (<code class="systemitem">cinder</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">neutron-server</code>
       (<code class="systemitem">neutron</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">neutron-network</code> (<code class="systemitem">neutron</code>
       barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">nova-controller</code>
       (<code class="systemitem">nova</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">nova_dashboard-server</code>
       (<code class="systemitem">nova_dashboard</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">ceilometer-server</code>
       (<code class="systemitem">ceilometer</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">ceilometer-polling</code>
       (<code class="systemitem">ceilometer</code> barclamp)
      </p></li><li class="listitem "><p>
       <code class="systemitem">heat-server</code> (<code class="systemitem">heat</code>
       barclamp)
      </p></li></ul></div><p>
     Instead of assigning these roles to individual cloud nodes, you can
     assign them to one or several High Availability clusters. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> will
     then use the Pacemaker cluster stack (shipped with the SUSE Linux Enterprise
     High Availability Extension) to manage the services. If one Control Node fails, the services
     will fail over to another
     Control Node. For details on the
     Pacemaker cluster stack and the SUSE Linux Enterprise High Availability Extension, refer to the
     <em class="citetitle ">High Availability Guide</em>, available at
     <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/</a>.
     Note that SUSE Linux Enterprise High Availability Extension includes Linux Virtual Server as the load-balancer, and
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> uses HAProxy for this purpose
     (<a class="link" href="http://haproxy.1wt.eu/" target="_blank">http://haproxy.1wt.eu/</a>).
    </p><div id="id-1.3.3.3.7.5.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Recommended Setup</h6><p>
      Though it is possible to use the same cluster for all of the roles
      above, the recommended setup is to use three clusters and to deploy
      the roles as follows:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">data</code> cluster:
        <code class="systemitem">database-server</code> and
        <code class="systemitem">rabbitmq-server</code>
       </p></li><li class="listitem "><p>
        <code class="literal">network</code> cluster:
        <code class="systemitem">neutron-network</code> (as the
        <code class="systemitem">neutron-network</code> role may result in heavy network
        load and CPU impact)
       </p></li><li class="listitem "><p>
        <code class="systemitem">services</code> cluster: all other roles listed
        above (as they are related to API/schedulers)
       </p></li></ul></div><p><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> does not support High Availability for the LBaaS service plug-in.
      Thus, failover of a neutron load-balancer to another node
      can only be configured manually by editing the database.</p></div><div id="id-1.3.3.3.7.5.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Cluster Requirements and Recommendations</h6><p>
      For setting up the clusters, some special requirements and
      recommendations apply. For details, refer to
      <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a>.
     </p></div></div><div class="sect3 " id="sec-depl-reg-ha-control-recover"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.6.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Node(s)—Recovery</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-control-recover">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-control-recover</li></ul></div></div></div></div><p>
     Recovery of the Control Node(s) is done automatically by the cluster
     software: if one Control Node fails, Pacemaker will fail over the
     services to another Control Node. If a failed Control Node is
     repaired and rebuilt via Crowbar, it will be automatically configured
     to join the cluster. At this point Pacemaker will have the option to
     fail back services if required.
    </p></div></div><div class="sect2 " id="sec-depl-reg-ha-compute"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability of the Compute Node(s)</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-compute">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-compute</li></ul></div></div></div></div><p>
    If a Compute Node fails, all VMs running on that node will go down. While it cannot protect against failures of individual VMs, a
    High Availability setup for Compute Nodes helps to minimize VM downtime caused by
    Compute Node failures.  If the <code class="literal">nova-compute</code> service or
    <code class="literal">libvirtd</code> fail on a Compute Node, Pacemaker will
    try to automatically recover them.  If recovery fails, or the
    node itself should become unreachable, the node will be fenced and the
    VMs will be moved to a different Compute Node.
   </p><p>
    If you decide to use High Availability for Compute Nodes, your Compute Node will
    be run as Pacemaker remote nodes. With the <code class="literal">pacemaker-remote</code>
    service, High Availability clusters can be extended to control remote nodes without any
    impact on scalability, and without having to install the full cluster stack
    (including <code class="literal">corosync</code>) on the remote nodes.  Instead, each
    Compute Node only runs the <code class="literal">pacemaker-remote</code> service. The service
    acts as a proxy, allowing the cluster stack on the <span class="quote">“<span class="quote ">normal</span>”</span>
    cluster nodes to connect to it and to control services remotely. Thus, the
    node is effectively integrated into the cluster as a remote node. In this way,
    the services running on the OpenStack compute nodes can be controlled from the core
    Pacemaker cluster in a lightweight, scalable fashion.
   </p><p> Find more information about the <code class="literal">pacemaker_remote</code>
    service in
    <em class="citetitle ">Pacemaker Remote—Extending High Availability into
    Virtual Nodes</em>,
    available at <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. </p><p>To configure High Availability for Compute Nodes, you need to adjust the following
   barclamp proposals:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Pacemaker—for details, see <a class="xref" href="#sec-depl-ostack-pacemaker" title="12.2. Deploying Pacemaker (Optional, HA Setup Only)">Section 12.2, “Deploying Pacemaker (Optional, HA Setup Only)”</a>.</p></li><li class="listitem "><p>Nova—for details, see <a class="xref" href="#sec-depl-ostack-nova-ha" title="12.11.1. HA Setup for Nova">Section 12.11.1, “HA Setup for Nova”</a>.</p></li></ul></div></div><div class="sect2 " id="sec-depl-reg-ha-storage"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability of the Storage Node(s)</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-storage">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-storage</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> offers two different types of storage that can be used
    for the Storage Nodes: object storage (provided by the <span class="productname">OpenStack</span>
    Swift component) and block storage (provided by Ceph).
   </p><p>
    Both already consider High Availability aspects by design, therefore it does not
    require much effort to make the storage highly available.
   </p><div class="sect3 " id="sec-depl-reg-ha-storage-swift"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.6.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift—Avoiding Points of Failure</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-storage-swift">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-storage-swift</li></ul></div></div></div></div><p>
     The <span class="productname">OpenStack</span> Object Storage replicates the data by design, provided
     the following requirements are met:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       The option <span class="guimenu ">Replicas</span> in the Swift
       barclamp is set to <code class="literal">3</code>, the tested and recommended
       value.
      </p></li><li class="listitem "><p>
       The number of Storage Nodes needs to be greater than the value set in
       the <span class="guimenu ">Replicas</span> option.
      </p></li></ul></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       To avoid single points of failure, assign the
       <code class="systemitem">swift-storage</code> role to multiple nodes.
      </p></li><li class="step "><p>
       To make the API highly available, assign the
       <code class="systemitem">swift-proxy</code> role to a cluster instead of
       assigning it to a single Control Node. See
       <a class="xref" href="#sec-depl-reg-ha-control-spof" title="2.6.2.1. Control Node(s)—Avoiding Points of Failure">Section 2.6.2.1, “Control Node(s)—Avoiding Points of Failure”</a>. Other swift roles
       must not be deployed on a cluster.
      </p></li></ol></div></div></div><div class="sect3 " id="sec-depl-reg-ha-storage-ceph"><div class="titlepage"><div><div><h4 class="title"><span class="number">2.6.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceph—Avoiding Points of Failure</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-storage-ceph">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-storage-ceph</li></ul></div></div></div></div><p>
     Ceph is a distributed storage solution that can provide High Availability.  For High Availability
     redundant storage and monitors need to be configured in the Ceph
     cluster. For more information refer to the SUSE Enterprise Storage documentation at
     <a class="link" href="https://documentation.suse.com/ses/5.5/" target="_blank">https://documentation.suse.com/ses/5.5/</a>.
    </p></div></div><div class="sect2 " id="sec-depl-reg-ha-general"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Requirements and Recommendations</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-general">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-general</li></ul></div></div></div></div><p>
    When considering setting up one or more High Availability clusters, refer to the
    chapter <em class="citetitle ">System Requirements</em> in the
    <em class="citetitle ">High Availability Guide</em> for SUSE Linux Enterprise High Availability Extension. The guide is available at
    <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/</a>.
   </p><p>
    The HA requirements for Control Node also apply to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Note that by
    buying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, you automatically get an entitlement for
    SUSE Linux Enterprise High Availability Extension.
   </p><p>
    Especially note the following requirements:
   </p><div class="variablelist "><dl class="variablelist"><dt id="vle-ha-req-nodes"><span class="term ">Number of Cluster Nodes</span></dt><dd><p>
       Each cluster needs to consist of at least three cluster nodes.
      </p><div id="id-1.3.3.3.7.8.5.1.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Odd Number of Cluster Nodes</h6><p>
        The Galera cluster needs an <span class="emphasis"><em>odd</em></span> number of cluster
        nodes with a <span class="emphasis"><em>minimum</em></span> of three nodes.
       </p><p>
        A cluster needs
        <a class="xref" href="#gloss-quorum" title="Quorum">Quorum</a> to
        keep services running. A three-node cluster can tolerate
         failure of only one node at a time, whereas a five-node cluster can
        tolerate failures of two nodes.
       </p></div></dd><dt id="vle-ha-req-stonith"><span class="term ">STONITH</span></dt><dd><p>
       The cluster software will shut down <span class="quote">“<span class="quote ">misbehaving</span>”</span> nodes
       in a cluster to prevent them from causing trouble. This mechanism is
       called <code class="literal">fencing</code> or
       <a class="xref" href="#gloss-stonith" title="STONITH">STONITH</a>.
      </p><div id="id-1.3.3.3.7.8.5.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: No Support Without STONITH</h6><p>
        A cluster without STONITH is not supported.
       </p></div><p>
       For a supported HA setup, ensure the following:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Each node in the High Availability cluster needs to have at least one
         STONITH device (usually a hardware device). We strongly
         recommend multiple STONITH devices per node, unless STONITH Block Device (SBD) is used.
        </p></li><li class="listitem "><p>
         The global cluster options <code class="systemitem">stonith-enabled</code>
         and <code class="systemitem">startup-fencing</code> must be set to
         <code class="literal">true</code>. These options are set automatically when
         deploying the <code class="systemitem">Pacemaker</code> barclamp. When you change them, you will lose support.
        </p></li><li class="listitem "><p>
         When deploying the <code class="literal">Pacemaker</code> service, select a
         <a class="xref" href="#vle-pacemaker-barcl-stonith">STONITH: Configuration mode for STONITH
    </a>
         that matches your setup. If your STONITH devices support the
         IPMI protocol, choosing the IPMI option is the easiest way to
         configure STONITH. Another alternative is SBD. It provides a way to enable STONITH and fencing
         in clusters without external power switches, but it requires shared
         storage. For SBD requirements, see
         <a class="link" href="http://linux-ha.org/wiki/SBD_Fencing" target="_blank">http://linux-ha.org/wiki/SBD_Fencing</a>, section
         <em class="citetitle ">Requirements</em>.
        </p></li></ul></div><p>
       For more information, refer to the <em class="citetitle ">High Availability Guide</em>, available at
       <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/</a>.
       Especially read the following chapters: <em class="citetitle ">Configuration and
       Administration Basics</em>, and <em class="citetitle ">Fencing and
       STONITH</em>, <em class="citetitle "> Storage Protection</em>.
      </p></dd><dt id="vle-ha-req-communication"><span class="term ">Network Configuration</span></dt><dd><div id="id-1.3.3.3.7.8.5.3.2.1" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Redundant Communication Paths</h6><p>
        For a supported HA setup, it is required to set up cluster
        communication via two or more redundant paths. For this purpose, use
        network device bonding and team network mode in your Crowbar network setup. For details, see
        <a class="xref" href="#sec-depl-req-network-modes-teaming" title="2.1.2.3. Team Network Mode">Section 2.1.2.3, “Team Network Mode”</a>. At least two
        Ethernet cards per cluster node are required for network redundancy.
        We advise using team network mode everywhere (not only
        between the cluster nodes) to ensure redundancy.
       </p></div><p>
       For more information, refer to the <em class="citetitle ">High Availability Guide</em>, available at
       <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/</a>.
       Especially read the following chapter: <em class="citetitle ">Network Device
       Bonding</em>.
      </p><p>
       Using a second communication channel (ring) in Corosync (as an
       alternative to network device bonding) is not supported yet in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
       By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> uses the admin network (typically
       <code class="literal">eth0</code>) for the first Corosync ring.
      </p><div id="id-1.3.3.3.7.8.5.3.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Dedicated Networks</h6><p>
         The <code class="literal">corosync</code> network communication layer is
         crucial to the health of the cluster. <code class="literal">corosync</code> traffic always
         goes over the admin network.
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           Use redundant communication paths for the <code class="literal">corosync</code>
           network communication layer.
          </p></li><li class="listitem "><p>
           Do not place the <code class="literal">corosync</code> network communication layer
           on interfaces shared with any other networks that could experience heavy
           load, such as the <span class="productname">OpenStack</span> public / private / SDN / storage networks.
          </p></li></ul></div><p>
         Similarly, if SBD over iSCSI is used as a STONITH device (see
         <a class="xref" href="#vle-ha-req-stonith">STONITH</a>), do not place the iSCSI traffic on
         interfaces that could experience heavy load, because this might disrupt
         the SBD mechanism.
        </p></div></dd><dt id="vle-ha-req-storage"><span class="term ">Storage Requirements</span></dt><dd><p>
       When using SBD as STONITH device, additional requirements apply
       for the shared storage. For details, see
       <a class="link" href="http://linux-ha.org/wiki/SBD_Fencing" target="_blank">http://linux-ha.org/wiki/SBD_Fencing</a>, section
       <em class="citetitle ">Requirements</em>.
      </p></dd></dl></div></div><div class="sect2 " id="sec-depl-reg-ha-more"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-more">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-reg-ha-more</li></ul></div></div></div></div><p>
    For a basic understanding and detailed information on the SUSE Linux Enterprise
    High Availability Extension (including the Pacemaker cluster stack), read the
    <em class="citetitle ">High Availability Guide</em>. It is available at
    <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/</a>.
   </p><p>
    In addition to the chapters mentioned in
    <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a>, the following
    chapters are especially recommended:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <em class="citetitle ">Product Overview</em>
     </p></li><li class="listitem "><p>
      <em class="citetitle ">Configuration and Administration Basics</em>
     </p></li></ul></div><p>
    The <em class="citetitle ">High Availability Guide</em> also provides comprehensive information about the
    cluster management tools with which you can view and check the cluster status
    in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. They can also be used to look up details like
    configuration of cluster resources or global cluster options. Read the
    following chapters for more information:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      HA Web Console: <em class="citetitle ">Configuring and Managing Cluster Resources (Web
      Interface)</em>
     </p></li><li class="listitem "><p>
      <code class="command">crm.sh</code>: <em class="citetitle "> Configuring and Managing
      Cluster Resources (Command Line)</em>
     </p></li></ul></div></div></div><div class="sect1 " id="sec-depl-req-summary"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Summary: Considerations and Requirements</span> <a title="Permalink" class="permalink" href="#sec-depl-req-summary">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-summary</li></ul></div></div></div></div><p>
   As outlined above, there are some important considerations to be made
   before deploying SUSE <span class="productname">OpenStack</span> Cloud. The following briefly summarizes what was
   discussed in detail in this chapter. Keep in mind that as of
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> it is not possible to change some
   aspects such as the network setup when SUSE <span class="productname">OpenStack</span> Cloud is deployed!
   
  </p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Network </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.8.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
     If you do not want to stick with the default networks and addresses,
     define custom networks and addresses. You need five different networks.
     If you need to separate the admin and the BMC network, a sixth network
     is required. See <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for details.
     Networks that share interfaces need to be configured as VLANs.
    </p></li><li class="listitem "><p>
     The SUSE <span class="productname">OpenStack</span> Cloud networks are completely isolated, therefore it is not
     required to use public IP addresses for them. A class C network as used
     in this documentation may not provide enough addresses for a cloud that
     is supposed to grow. You may alternatively choose addresses from a
     class B or A network.
    </p></li><li class="listitem "><p>
     Determine how to allocate addresses from your network. Make sure not to
     allocate IP addresses twice. See
     <a class="xref" href="#sec-depl-req-network-allocation" title="2.1.1. Network Address Allocation">Section 2.1.1, “Network Address Allocation”</a> for the default
     allocation scheme.
    </p></li><li class="listitem "><p>
     Define which network mode to use. Keep in mind that all machines within
     the cloud (including the Administration Server) will be set up with the chosen
     mode and therefore need to meet the hardware requirements. See
     <a class="xref" href="#sec-depl-req-network-modes" title="2.1.2. Network Modes">Section 2.1.2, “Network Modes”</a> for details.
    </p></li><li class="listitem "><p>
     Define how to access the admin and BMC network(s): no access from the
     outside (no action is required), via an external gateway (gateway needs
     to be provided), or via bastion network. See
     <a class="xref" href="#sec-depl-req-network-bastion" title="2.1.3. Accessing the Administration Server via a Bastion Network">Section 2.1.3, “Accessing the Administration Server via a Bastion Network”</a> for details.
    </p></li><li class="listitem "><p>
     Provide a gateway to access the public network (public, nova-floating).
    </p></li><li class="listitem "><p>
     Make sure the Administration Server's host name is correctly configured
     (<code class="command">hostname</code> <code class="option">-f</code> needs to return a
     fully qualified host name). If this is not the case, run <span class="guimenu ">YaST</span> › <span class="guimenu ">Network Services</span> › <span class="guimenu ">Hostnames</span> and add a fully qualified
     host name.
    </p></li><li class="listitem "><p>
     Prepare a list of MAC addresses and the intended use of the
     corresponding host for all <span class="productname">OpenStack</span> nodes.
    </p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Update Repositories </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.8.4">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
     Depending on your network setup you have different options for
     providing up-to-date update repositories for SUSE Linux Enterprise Server and SUSE <span class="productname">OpenStack</span> Cloud for
     SUSE <span class="productname">OpenStack</span> Cloud deployment: using an existing SMT or SUSE Manager
     server, installing SMT on the Administration Server, synchronizing data
     with an existing repository, mounting remote repositories, or using physical media. Choose the option that best matches your
     needs.
    </p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Storage </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.8.5">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
     Decide whether you want to deploy the object storage service
     Swift. If so, you need to deploy at least two nodes with
     sufficient disk space exclusively dedicated to Swift.
    </p></li><li class="listitem "><p>
     Decide which back-end to use with Cinder. If using the
     <span class="guimenu ">raw</span> back-end (local disks) we strongly
     recommend using a separate node equipped with several hard disks for
     deploying <code class="literal">cinder-volume</code>. Ceph needs a minimum of four exclusive nodes with sufficient disk space.
    </p></li><li class="listitem "><p>
     Make sure all Compute Nodes are equipped with sufficient hard disk
     space.
    </p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">SSL Encryption </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.8.6">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
     Decide whether to use different SSL certificates for the services and
     the API, or whether to use a single certificate.
    </p></li><li class="listitem "><p>
     Get one or more SSL certificates certified by a trusted third party
     source.
    </p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Hardware and Software Requirements </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.8.7">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
     Make sure the hardware requirements for the different node types are
     met.
    </p></li><li class="listitem "><p>
     Make sure to have all required software at hand.
    </p></li></ul></div></div><div class="sect1 " id="sec-depl-req-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview of the SUSE <span class="productname">OpenStack</span> Cloud Installation</span> <a title="Permalink" class="permalink" href="#sec-depl-req-installation">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_require.xml</li><li><span class="ds-label">ID: </span>sec-depl-req-installation</li></ul></div></div></div></div><p>
   Deploying and installing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is a multi-step process.
   Start by deploying a basic SUSE Linux Enterprise Server installation and the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> add-on product to the Administration Server. Then the product and
   update repositories need to be set up and the SUSE <span class="productname">OpenStack</span> Cloud network needs to
   be configured. Next, complete the Administration Server setup. After the
   Administration Server is ready, you can start deploying and configuring the
   <span class="productname">OpenStack</span> nodes. The complete node deployment is done automatically via
   Crowbar and Chef from the Administration Server. All you need to do is to
   boot the nodes using PXE and to deploy the <span class="productname">OpenStack</span> components to them.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Install SUSE Linux Enterprise Server 12 SP3 on the Administration Server with the add-on product
     SUSE <span class="productname">OpenStack</span> Cloud. Optionally select the Subscription Management Tool (SMT) pattern for installation. See
     <a class="xref" href="#cha-depl-adm-inst" title="Chapter 3. Installing the Administration Server">Chapter 3, <em>Installing the Administration Server</em></a>.
    </p></li><li class="step "><p>
     Optionally set up and configure the SMT server on the Administration Server. See
     <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>.
    </p></li><li class="step "><p>
     Make all required software repositories available on the Administration Server. See
     <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
    </p></li><li class="step "><p>
     Set up the network on the Administration Server. See
     <a class="xref" href="#sec-depl-adm-inst-network" title="Chapter 6. Service Configuration: Administration Server Network Configuration">Chapter 6, <em>Service Configuration:  Administration Server Network Configuration</em></a>.
    </p></li><li class="step "><p>
     Perform the Crowbar setup to configure the SUSE <span class="productname">OpenStack</span> Cloud network and to make the
     repository locations known. When the configuration is done, start the
     SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. See <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>.
    </p></li><li class="step "><p>
     Boot all nodes onto which the <span class="productname">OpenStack</span> components should be deployed
     using PXE and allocate them in the Crowbar Web interface to start the
     automatic SUSE Linux Enterprise Server installation. See
     <a class="xref" href="#cha-depl-inst-nodes" title="Chapter 11. Installing the OpenStack Nodes">Chapter 11, <em>Installing the <span class="productname">OpenStack</span> Nodes</em></a>.
    </p></li><li class="step "><p>
     Configure and deploy the <span class="productname">OpenStack</span> components via the Crowbar Web
     interface or command line tools. See <a class="xref" href="#cha-depl-ostack" title="Chapter 12. Deploying the OpenStack Services">Chapter 12, <em>Deploying the <span class="productname">OpenStack</span> Services</em></a>.
    </p></li><li class="step "><p>
     When all <span class="productname">OpenStack</span> components are up and running, SUSE <span class="productname">OpenStack</span> Cloud is ready.
     The cloud administrator can now upload images to enable users to start
     deploying instances. See the <em class="citetitle ">Administrator Guide</em> and the
     <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">End User Guide</em></em>.

    </p></li></ol></div></div></div></div></div><div class="part" id="part-depl-admserv"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part II </span><span class="name">Setting Up the Administration Server </span><a title="Permalink" class="permalink" href="#part-depl-admserv">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-depl-adm-inst"><span class="number">3 </span><span class="name">Installing the Administration Server</span></a></span></dt><dd class="toc-abstract"><p>
    In this chapter you will learn how to install the Administration Server from
    scratch. It will run on SUSE Linux Enterprise Server 12 SP3 and include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
    extension and, optionally, the Subscription Management Tool (SMT) server. Prior to starting
    the installation, refer to <a class="xref" href="#sec-depl-req-hardware" title="2.4. Hardware Requirements">Section 2.4, “Hardware Requirements”</a> and
    <a class="xref" href="#sec-depl-req-software" title="2.5. Software Requirements">Section 2.5, “Software Requirements”</a>.
   </p></dd><dt><span class="chapter"><a href="#app-deploy-smt"><span class="number">4 </span><span class="name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></a></span></dt><dd class="toc-abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    SUSE <span class="productname">OpenStack</span> Cloud is to install a Subscription Management Tool (SMT) server on the Administration Server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Administration Server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Administration Server, skip this step.
   </p></dd><dt><span class="chapter"><a href="#cha-depl-repo-conf"><span class="number">5 </span><span class="name">Software Repository Setup</span></a></span></dt><dd class="toc-abstract"><p>
    Nodes in SUSE <span class="productname">OpenStack</span> Cloud are automatically installed from the Administration Server. For this to happen,
    software repositories containing products, extensions, and the respective
    updates for all software need to be available on or accessible from the
    Administration Server. In this configuration step, these repositories are made
    available. There are two types of repositories:
   </p><p>
    <span class="bold"><strong>Product Media Repositories</strong></span>: Product media
    repositories are copies of the installation media. They need to be
    directly copied to the Administration Server, <span class="quote">“<span class="quote ">loop-mounted</span>”</span> from an iso
    image, or mounted from a remote server via NFS. Affected are SUSE Linux Enterprise Server 12 SP3 and
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>. These are static repositories; they do not change or receive updates. See <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a> for setup
    instructions.
   </p><p>
    <span class="bold"><strong>Update and Pool Repositories</strong></span>: Update and
    Pool repositories are provided by the SUSE Customer Center. They contain all updates and
    patches for the products and extensions. To make them available for
    SUSE <span class="productname">OpenStack</span> Cloud they need to be mirrored from the SUSE Customer Center. Since their content is
    regularly updated, they must be kept in synchronization with SUSE Customer Center. For
    these purposes, SUSE provides either the Subscription Management Tool (SMT) or the
    SUSE Manager.
   </p></dd><dt><span class="chapter"><a href="#sec-depl-adm-inst-network"><span class="number">6 </span><span class="name">Service Configuration:  Administration Server Network Configuration</span></a></span></dt><dd class="toc-abstract"><p>
    Prior to starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, make sure the first network
    interface (<code class="systemitem">eth0</code>) gets a
    fixed IP address from the admin network. A host and domain name
    also need to be provided. Other interfaces will be automatically
    configured during the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
   </p></dd><dt><span class="chapter"><a href="#sec-depl-adm-inst-crowbar"><span class="number">7 </span><span class="name">Crowbar Setup</span></a></span></dt><dd class="toc-abstract"><p>
    The YaST Crowbar module enables you to configure all networks within the
    cloud, to set up additional repositories, and to manage the Crowbar users.
    This module should be launched before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. To start
    this module, either run <code class="command">yast crowbar</code> or <span class="guimenu ">YaST</span> › <span class="guimenu ">Miscellaneous</span> › <span class="guimenu ">Crowbar</span>.
   </p></dd><dt><span class="chapter"><a href="#sec-depl-adm-start-crowbar"><span class="number">8 </span><span class="name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></a></span></dt><dd class="toc-abstract"><p>
    The last step in configuring the Administration Server is starting Crowbar.
   </p></dd><dt><span class="chapter"><a href="#sec-depl-adm-crowbar-extra-features"><span class="number">9 </span><span class="name">Customizing Crowbar</span></a></span></dt><dd class="toc-abstract"><p>In large deployments with many nodes, there are always some nodes that are in a fail or unknown state. New barclamps cannot be applied to them and values cannot be updated in some barclamps that are already deployed. This happens because Crowbar will refuse to apply a barclamp to a list of nodes if …</p></dd></dl></div><div class="chapter " id="cha-depl-adm-inst"><div class="titlepage"><div><div><h2 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Administration Server</span> <a title="Permalink" class="permalink" href="#cha-depl-adm-inst">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.2.1.3">#</a></h6></div><p>
    In this chapter you will learn how to install the Administration Server from
    scratch. It will run on SUSE Linux Enterprise Server 12 SP3 and include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
    extension and, optionally, the Subscription Management Tool (SMT) server. Prior to starting
    the installation, refer to <a class="xref" href="#sec-depl-req-hardware" title="2.4. Hardware Requirements">Section 2.4, “Hardware Requirements”</a> and
    <a class="xref" href="#sec-depl-req-software" title="2.5. Software Requirements">Section 2.5, “Software Requirements”</a>.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-adm-inst-os"><span class="number">3.1 </span><span class="name">Starting the Operating System Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-online-update"><span class="number">3.2 </span><span class="name">Registration and Online Updates</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-add-on"><span class="number">3.3 </span><span class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Extension</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-partition"><span class="number">3.4 </span><span class="name">Partitioning</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-settings"><span class="number">3.5 </span><span class="name">Installation Settings</span></a></span></dt></dl></div></div><div class="sect1 " id="sec-depl-adm-inst-os"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Starting the Operating System Installation</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-os">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-os</li></ul></div></div></div></div><p>
   Start the installation by booting into the SUSE Linux Enterprise Server 12 SP3 installation system.
   For detailed installation instructions for SUSE Linux Enterprise Server, refer to <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#cha-install" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#cha-install</a>.
  </p><p>
   The following sections will only cover the differences from the default
   installation process.
  </p></div><div class="sect1 " id="sec-depl-adm-inst-online-update"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registration and Online Updates</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-online-update">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-online-update</li></ul></div></div></div></div><p>
   Registering SUSE Linux Enterprise Server 12 SP3 during the installation process is required for
   getting product updates and for installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   extension. Refer to <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-conf-manual-cc" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-conf-manual-cc</a>
  </p><p>
   After a successful registration you will be asked whether
   to add the update repositories. If you agree, the latest updates will
   automatically be installed, ensuring that your system is on the latest
   patch level after the initial installation. We strongly recommend adding the update repositories immediately. If you choose to skip this step you need to perform
   an online update later, before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
  </p><div id="id-1.3.4.2.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: SUSE Login Required</h6><p>
    To register a product, you need to have a SUSE login.
    If you do not have such a login, create it at
    <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>.
   </p></div></div><div class="sect1 " id="sec-depl-adm-inst-add-on"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Extension</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-add-on">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-add-on</li></ul></div></div></div></div><p>
   SUSE <span class="productname">OpenStack</span> Cloud is an extension to SUSE Linux Enterprise Server. Installing it during the SUSE Linux Enterprise Server
   installation is the easiest and recommended way to set up the Administration Server. To get access to the extension selection dialog, you need to register
   SUSE Linux Enterprise Server 12 SP3 during the installation. After a successful registration, the
   SUSE Linux Enterprise Server 12 SP3 installation continues with the <span class="guimenu ">Extension &amp; Module
   Selection</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span></span>
   and provide the registration key you obtained by purchasing
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. The registration and the extension installation require an
   Internet connection.
  </p><p>
   Alternatively, install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> after the
   SUSE Linux Enterprise Server 12 SP3 installation via <span class="guimenu ">YaST</span> › <span class="guimenu ">Software</span> › <span class="guimenu ">Add-On Products</span>.
   For details, refer to the section <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-add-ons-extensions" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-add-ons-extensions</a>.
  </p></div><div class="sect1 " id="sec-depl-adm-inst-partition"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Partitioning</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-partition">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-partition</li></ul></div></div></div></div><p>
   Currently, Crowbar requires <code class="filename">/opt</code> to be writable. We recommend creating a separate partition
   or volume formatted with XFS for <code class="filename">/srv</code> with a size of
   at least 30 GB.
  </p><p>
   The default file system on SUSE Linux Enterprise Server 12 SP3 is Btrfs with snapshots enabled.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> installs into <code class="filename">/opt</code>, a directory that is
   excluded from snapshots. Reverting to a snapshot may therefore break the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> installation. We recommend disabling Btrfs snapshots on
   the Administration Server.
  </p><p>
   Help on using the partitioning tool is available at the section <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-yast2-i-y2-part-expert" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-yast2-i-y2-part-expert</a>.
  </p></div><div class="sect1 " id="sec-depl-adm-inst-settings"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation Settings</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-settings</li></ul></div></div></div></div><p>
   In the final installation step, <span class="guimenu ">Installation Settings</span>, you need to adjust
   the software selection and the firewall settings for your Administration Server setup. For more information
   refer to the <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-proposal" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-proposal</a>.
  </p><div class="sect2 " id="sec-depl-adm-inst-settings-software"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Selection</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings-software">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-settings-software</li></ul></div></div></div></div><p>
    Installing a minimal base system is sufficient to set up the
    Administration Server. The following patterns are the minimum required:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <span class="guimenu ">Base System</span>
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Minimal System (Appliances)</span>
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Meta Package for Pattern cloud_admin</span> (in case you have
      chosen to install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Extension)
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Subscription Management Tool</span> (optional, also see <a class="xref" href="#tip-depl-adm-inst-settings-smt" title="Tip: Installing a Local SMT Server (Optional)">Tip: Installing a Local SMT Server (Optional)</a>)
     </p></li></ul></div><div id="tip-depl-adm-inst-settings-smt" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Installing a Local SMT Server (Optional)</h6><p>
     If you do not have a SUSE Manager or SMT server in your
     organization, or are planning to manually update the repositories required for deployment of the SUSE <span class="productname">OpenStack</span> Cloud nodes, you need to set up an SMT server on
     the Administration Server. Choose the pattern <span class="guimenu ">Subscription Management
     Tool</span> in addition to the patterns listed above to install the
     SMT server software.
    </p></div></div><div class="sect2 " id="sec-depl-adm-inst-settings-firewall"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall Settings</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings-firewall">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_admin.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-settings-firewall</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> requires disabling the firewall on the Administration Server. You can
    disable the firewall during installation in the <span class="guimenu ">Firewall and SSH</span>
    section. If your environment requires a firewall to be active at this
    stage of the installation, you can disable the firewall during your final network configuration (see <a class="xref" href="#sec-depl-adm-inst-network" title="Chapter 6. Service Configuration: Administration Server Network Configuration">Chapter 6, <em>Service Configuration:  Administration Server Network Configuration</em></a>). Optionally, you can also enable SSH
    access to the Administration Server in this section.
   </p><div id="warn-depl-adm-inst-settings-proxy" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: HTTP_PROXY and NO_PROXY</h6><p>
      Setting HTTP_PROXY without properly configuring NO_PROXY for the
      Administration Server might result in chef-client failing in non-obvious ways.
     </p></div></div></div></div><div class="chapter " id="app-deploy-smt"><div class="titlepage"><div><div><h2 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span> <a title="Permalink" class="permalink" href="#app-deploy-smt">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt</li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.3.2.2">#</a></h6></div><p>
    One way to provide the repositories needed to set up the nodes in
    SUSE <span class="productname">OpenStack</span> Cloud is to install a Subscription Management Tool (SMT) server on the Administration Server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Administration Server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Administration Server, skip this step.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#app-deploy-smt-install"><span class="number">4.1 </span><span class="name">SMT Installation</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-config"><span class="number">4.2 </span><span class="name">SMT Configuration</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-repos"><span class="number">4.3 </span><span class="name">Setting up Repository Mirroring on the SMT Server</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-info"><span class="number">4.4 </span><span class="name">For More Information</span></a></span></dt></dl></div></div><div id="id-1.3.4.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Use of SMT Server and Ports</h6><p>
   When installing an SMT server on the Administration Server, use it exclusively
   for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. To use the SMT server for other
   products, run it outside of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Make sure it can be accessed
   from the Administration Server for mirroring the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
  </p><p>
   When the SMT server is installed on the Administration Server, Crowbar
   provides the mirrored repositories on port <code class="literal">8091</code>.
  </p></div><div class="sect1 " id="app-deploy-smt-install"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SMT Installation</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-install">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-install</li></ul></div></div></div></div><p>
   If you have not installed the SMT server during the initial Administration Server
   installation as suggested in <a class="xref" href="#sec-depl-adm-inst-settings-software" title="3.5.1. Software Selection">Section 3.5.1, “Software Selection”</a>, run the following command
   to install it:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern smt</pre></div></div><div class="sect1 " id="app-deploy-smt-config"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SMT Configuration</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-config">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-config</li></ul></div></div></div></div><p>
   No matter whether the SMT server was installed during the initial
   installation or in the running system, it needs to be configured with the
   following steps.
  </p><div id="id-1.3.4.3.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Prerequisites</h6><p>
    To configure the SMT server, a SUSE account is required. If you do not
    have such an account, register at <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>. All products and
    extensions for which you want to mirror updates with the SMT
    server should be registered at the SUSE Customer Center (<a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>).
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Configuring the SMT server requires you to have your mirroring
     credentials (user name and password) and your registration e-mail
     address at hand. To access them, proceed as follows:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Open a Web browser and log in to the SUSE Customer Center at
       <a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>.
      </p></li><li class="step "><p>
       Click your name to see the e-mail address which you have registered.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">Organization</span> › <span class="guimenu ">Organization Credentials</span> to obtain
       your mirroring credentials (user name and password).
      </p></li></ol></li><li class="step "><p>
     Start <span class="guimenu ">YaST</span> › <span class="guimenu ">Network
     Services</span> › <span class="guimenu ">SMT Configuration
     Wizard</span>.
    </p></li><li class="step "><p>
     Activate <span class="guimenu ">Enable Subscription Management Tool Service
     (SMT)</span>.
    </p></li><li class="step "><p>
     Enter the <span class="guimenu ">Customer Center Configuration</span> data as
     follows:
    </p><table border="0" summary="Simple list" class="simplelist "><tr><td><span class="guimenu ">Use Custom Server</span>:
     Do <span class="emphasis"><em>not</em></span> activate this option</td></tr><tr><td><span class="guimenu ">User</span>: The user name you retrieved from the
     SUSE Customer Center</td></tr><tr><td><span class="guimenu ">Password</span>: The password you retrieved from the
     SUSE Customer Center</td></tr></table><p>
     Check your input with <span class="guimenu ">Test</span>. If the test does not
     return <code class="literal">success</code>, check the credentials you entered.
    </p></li><li class="step "><p>
     Enter the e-mail address you retrieved from the SUSE Customer Center at
     <span class="guimenu ">SCC E-Mail Used for Registration</span>.
    </p></li><li class="step "><p>
     <span class="guimenu ">Your SMT Server URL</span> shows the HTTP address of your
     server. Usually it should not be necessary to change it.
    </p></li><li class="step "><p>
     Select <span class="guimenu ">Next</span> to proceed to step two of the <span class="guimenu ">SMT Configuration Wizard</span>.
    </p></li><li class="step "><p>
     Enter a <span class="guimenu ">Database Password for SMT User</span> and confirm
     it by entering it once again.
    </p></li><li class="step "><p>
     Enter one or more e-mail addresses to which SMT status reports are
     sent by selecting <span class="guimenu ">Add</span>.
    </p></li><li class="step "><p>
     Select <span class="guimenu ">Next</span> to save your SMT configuration. When
     setting up the database you will be prompted for the MariaDB root
     password. If you have not already created one then create it in this step. Note that this is
     the global MariaDB root password, not the database password for the SMT
     user you specified before.
    </p><p>
     The SMT server requires a server certificate at
     <code class="filename">/etc/pki/trust/anchors/YaST-CA.pem</code>. Choose
     <span class="guimenu ">Run CA Management</span>, provide a password and choose
     <span class="guimenu ">Next</span> to create such a certificate. If your
     organization already provides a CA certificate, <span class="guimenu ">Skip</span>
     this step and import the certificate via <span class="guimenu ">YaST</span> › <span class="guimenu ">Security and Users</span> › <span class="guimenu ">CA Management</span> after the SMT
     configuration is done. See
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-security/#cha-security-yast-ca" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-security/#cha-security-yast-ca</a>
   for more information.
    </p><p>
     After you complete your configuration a synchronization check with the SUSE Customer Center will run, which may take several minutes.
    </p></li></ol></div></div></div><div class="sect1 " id="app-deploy-smt-repos"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up Repository Mirroring on the SMT Server</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-repos</li></ul></div></div></div></div><p>
   The final step in setting up the SMT server is configuring it to
   mirror the repositories needed for SUSE <span class="productname">OpenStack</span> Cloud. The SMT server
   mirrors the repositories from the SUSE Customer Center. Make
   sure to have the appropriate subscriptions registered in SUSE Customer Center with the
   same e-mail address you specified when configuring SMT. For
   details on the required subscriptions refer to
   <a class="xref" href="#sec-depl-req-software" title="2.5. Software Requirements">Section 2.5, “Software Requirements”</a>.
  </p><div class="sect2 " id="app-deploy-smt-repos-mandatory"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Mandatory Repositories</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mandatory">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-repos-mandatory</li></ul></div></div></div></div><p>
    Mirroring the SUSE Linux Enterprise Server 12 SP3 and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>
    repositories is mandatory. Run the following commands as user
    <code class="systemitem">root</code> to add them to the list of mirrored repositories:
   </p><div class="verbatim-wrap"><pre class="screen">for REPO in SLES12-SP3-{Pool,Updates} <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></div><div class="sect2 " id="app-deploy-smt-repos-optional"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Optional Repositories</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-optional">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-repos-optional</li></ul></div></div></div></div><p>
    The following optional repositories provide high availability and storage:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.3.6.4.3.1"><span class="term ">High Availability</span></dt><dd><p>
       For the optional HA setup you need to mirror the SLE12-HA12-SP3
       repositories. Run the following commands as user <code class="systemitem">root</code> to add
       them to the list of mirrored repositories:
      </p><div class="verbatim-wrap"><pre class="screen">for REPO in SLE12-HA12-SP3-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></dd><dt id="id-1.3.4.3.6.4.3.2"><span class="term ">SUSE Enterprise Storage</span></dt><dd><p>
       The SUSE Enterprise Storage repositories are needed if you plan to use an external
       Ceph with SUSE <span class="productname">OpenStack</span> Cloud. Run the following commands as user <code class="systemitem">root</code> to
       add them to the list of mirrored repositories:
      </p><div class="verbatim-wrap"><pre class="screen">for REPO in SUSE-Enterprise-Storage-5-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></dd></dl></div></div><div class="sect2 " id="app-deploy-smt-repos-mirror"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the Repositories</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mirror">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-repos-mirror</li></ul></div></div></div></div><p>
    New repositories added to SMT must be updated immediately by running the following command as user <code class="systemitem">root</code>:
   </p><div class="verbatim-wrap"><pre class="screen">smt-mirror -L /var/log/smt/smt-mirror.log</pre></div><p>
    This command will download several GB of patches. This process may last
    up to several hours. A log file is written to
    <code class="filename">/var/log/smt/smt-mirror.log</code>. After this first manual update the repositories are updated automatically via cron
    job. A list of all
    repositories and their location in the file system on the Administration Server can be
    found at <a class="xref" href="#tab-smt-repos-local" title="SMT Repositories Hosted on the Administration Server">Table 5.2, “SMT Repositories Hosted on the Administration Server”</a>.
   </p></div></div><div class="sect1 " id="app-deploy-smt-info"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-info">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_smt_setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-info</li></ul></div></div></div></div><p>
   For detailed information about SMT refer to the Subscription Management Tool manual at <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/</a>.
  </p></div></div><div class="chapter " id="cha-depl-repo-conf"><div class="titlepage"><div><div><h2 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Repository Setup</span> <a title="Permalink" class="permalink" href="#cha-depl-repo-conf">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.4.1.3">#</a></h6></div><p>
    Nodes in SUSE <span class="productname">OpenStack</span> Cloud are automatically installed from the Administration Server. For this to happen,
    software repositories containing products, extensions, and the respective
    updates for all software need to be available on or accessible from the
    Administration Server. In this configuration step, these repositories are made
    available. There are two types of repositories:
   </p><p>
    <span class="bold"><strong>Product Media Repositories</strong></span>: Product media
    repositories are copies of the installation media. They need to be
    directly copied to the Administration Server, <span class="quote">“<span class="quote ">loop-mounted</span>”</span> from an iso
    image, or mounted from a remote server via NFS. Affected are SUSE Linux Enterprise Server 12 SP3 and
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>. These are static repositories; they do not change or receive updates. See <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a> for setup
    instructions.
   </p><p>
    <span class="bold"><strong>Update and Pool Repositories</strong></span>: Update and
    Pool repositories are provided by the SUSE Customer Center. They contain all updates and
    patches for the products and extensions. To make them available for
    SUSE <span class="productname">OpenStack</span> Cloud they need to be mirrored from the SUSE Customer Center. Since their content is
    regularly updated, they must be kept in synchronization with SUSE Customer Center. For
    these purposes, SUSE provides either the Subscription Management Tool (SMT) or the
    SUSE Manager.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-product"><span class="number">5.1 </span><span class="name">Copying the Product Media Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-scc"><span class="number">5.2 </span><span class="name">Update and Pool Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-admserv-post-adm-repos"><span class="number">5.3 </span><span class="name">
    Software Repository Sources for the Administration Server Operating System
   </span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-repo-locations"><span class="number">5.4 </span><span class="name">Repository Locations</span></a></span></dt></dl></div></div><div class="sect1 " id="sec-depl-adm-conf-repos-product"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Copying the Product Media Repositories</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-product">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-product</li></ul></div></div></div></div><p>
   The files in the product repositories for SUSE Linux Enterprise Server and SUSE <span class="productname">OpenStack</span> Cloud do not
   change, therefore they do not need to be synchronized with a remote
   source. It is sufficient to either copy the data (from a remote host or
   the installation media), to mount the product repository from a remote
    server via <code class="literal">NFS</code>, or to loop mount a copy of the
    installation images.
   </p><div id="id-1.3.4.4.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: No Symbolic Links for the SUSE Linux Enterprise Server Repository</h6><p>
     Note that the SUSE Linux Enterprise Server product repository <span class="emphasis"><em>must</em></span> be
     directly available from the local directory listed below. It is not
     possible to use a symbolic link to a directory located elsewhere, since
     this will cause booting via PXE to fail.
    </p></div><div id="id-1.3.4.4.2.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Providing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Repository via HTTP</h6><p>
     The SUSE Linux Enterprise Server product repositories need to be available locally
     to enable booting via PXE for node deployment. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
     repository may also be served via HTTP from a remote
     host. In this case, enter the URL to the <code class="literal">Cloud</code>
     repository as described in <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a>.
    </p><p>
     We recommend copying the data to the Administration Server as the best solution. It does not require much hard disk space (approximately
     900 MB). Nor does it require the Administration Server to access a remote host from a different network.
    </p></div><p>
    The following product media must be copied to the specified
    directories:
   </p><div class="table" id="id-1.3.4.4.2.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.1: </span><span class="name">Local Product Repositories for SUSE <span class="productname">OpenStack</span> Cloud </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Local Product Repositories for SUSE OpenStack Cloud" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
        <p>
         Repository
        </p>
       </th><th>
        <p>
         Directory
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         SUSE Linux Enterprise Server 12 SP3 DVD #1
        </p>
       </td><td>
        <p>
         <code class="filename">/srv/tftpboot/suse-12.3/x86_64/install</code>
        </p>
       </td></tr><tr><td>
        <p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD #1
        </p>
       </td><td>
        <p>
         <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/Cloud</code>
        </p>
       </td></tr></tbody></table></div></div><p>
    The data can be copied by a variety of methods:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.4.2.8.1"><span class="term ">Copying from the Installation Media</span></dt><dd><p>
      We recommended using <code class="command">rsync</code> for copying. If the
      installation data is located on a removable device, make sure to mount
      it first (for example, after inserting the DVD1 in the Administration Server and
      waiting for the device to become ready):
     </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.1.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP3 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.1.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/install
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/tftpboot/suse-12.3/x86_64/install/
umount /mnt</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.1.2.4"><span class="name">
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
     </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.1.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/repos/Cloud
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/tftpboot/suse-12.3/x86_64/repos/Cloud/
umount /mnt</pre></div></dd><dt id="id-1.3.4.4.2.8.2"><span class="term ">Copying from a Remote Host</span></dt><dd><p>
       If the data is provided by a remote machine, log in to that machine and
       push the data to the Administration Server (which has the IP address <code class="systemitem">192.168.124.10</code> in the following
       example):
      </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.2.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP3 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.2.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/install
rsync -avPz <em class="replaceable ">/data/SLES-12-SP3/DVD1/</em> <em class="replaceable ">192.168.124.10</em>:/srv/tftpboot/suse-12.3/x86_64/install/</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.2.2.4"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.2.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/repos/Cloud
rsync -avPz <em class="replaceable ">/data/SUSE-OPENSTACK-CLOUD//DVD1/</em> <em class="replaceable ">192.168.124.10</em>:/srv/tftpboot/suse-12.3/x86_64/repos/Cloud/</pre></div></dd><dt id="id-1.3.4.4.2.8.3"><span class="term ">Mounting from an NFS Server</span></dt><dd><p>
       If the installation data is provided via NFS by a remote machine, mount
       the respective shares as follows. To automatically mount these
       directories either create entries in <code class="filename">/etc/fstab</code> or
       set up the automounter.
      </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.3.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP3 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.3.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/install
mount -t nfs <em class="replaceable ">nfs.example.com:/exports/SLES-12-SP3/x86_64/DVD1/</em> /srv/tftpboot/suse-12.3/x86_64/install</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.3.2.4"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.3.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/repos/Cloud/
mount -t nfs <em class="replaceable ">nfs.example.com:/exports/SUSE-OPENSTACK-CLOUD/DVD1/</em> /srv/tftpboot/suse-12.3/x86_64/repos/Cloud</pre></div></dd><dt id="id-1.3.4.4.2.8.4"><span class="term ">Mounting the ISO Images</span></dt><dd><p>
       The product repositories can also be made available by copying the
       respective ISO images to the Administration Server and mounting them. To
       automatically mount these directories either create entries in
       <code class="filename">/etc/fstab</code> or set up the automounter.
      </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.4.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP3 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.4.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/install/
mount -o loop <em class="replaceable ">/local/SLES-12-SP3-x86_64-DVD1.iso</em> /srv/tftpboot/suse-12.3/x86_64/install</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.4.2.8.4.2.4"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.2.8.4.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.3/x86_64/repos/Cloud/
mount -o loop <em class="replaceable ">/local/SUSE-OPENSTACK-CLOUD-<span class="phrase"><span class="phrase">8</span></span>-x86_64-DVD1.iso</em> /srv/tftpboot/suse-12.3/x86_64/repos/Cloud</pre></div></dd></dl></div></div><div class="sect1 " id="sec-depl-adm-conf-repos-scc"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update and Pool Repositories</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc</li></ul></div></div></div></div><p>
    Update and Pool Repositories are required on the Administration Server to set up and
    maintain the SUSE <span class="productname">OpenStack</span> Cloud nodes. They are provided by SUSE Customer Center and contain all
    software packages needed to install SUSE Linux Enterprise Server 12 SP3 and the extensions (pool
    repositories). In addition, they contain all updates and patches (update repositories). Update
    repositories are used when deploying the nodes that build
    SUSE <span class="productname">OpenStack</span> Cloud to ensure they are initially equipped with the latest software
    versions available.
   </p><p>
    The repositories can be made available on the Administration Server using one or more of the
    following methods:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-local-smt" title="5.2.1.  Repositories Hosted on an SMT Server Installed on the Administration Server">Section 5.2.1, “
     Repositories Hosted on an SMT Server Installed on the Administration Server
    ”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-remote-smt" title="5.2.2. Repositories Hosted on a Remote SMT Server">Section 5.2.2, “Repositories Hosted on a Remote SMT Server”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-remote-susemgr" title="5.2.3. Repositories Hosted on a SUSE Manager Server">Section 5.2.3, “Repositories Hosted on a SUSE Manager Server”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-alternatives" title="5.2.4. Alternative Ways to Make the Repositories Available">Section 5.2.4, “Alternative Ways to Make the Repositories Available”</a>
     </p></li></ul></div><div class="sect2 " id="sec-depl-adm-conf-repos-scc-local-smt"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
     Repositories Hosted on an SMT Server Installed on the Administration Server
    </span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-local-smt">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc-local-smt</li></ul></div></div></div></div><p>
     When all update and pool repositories are managed by an SMT server
     installed on the Administration Server (see <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>), make
     sure the repository location in YaST Crowbar is set to <span class="guimenu ">Local
     SMT Server</span> (this is the default). For details, see <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a>. No
     further action is required. The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation automatically
     detects all available repositories.
    </p></div><div class="sect2 " id="sec-depl-adm-conf-repos-scc-remote-smt"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Repositories Hosted on a Remote SMT Server</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-remote-smt">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc-remote-smt</li></ul></div></div></div></div><p>
     To use repositories from a remote SMT server, you first need to make
     sure all required repositories are mirrored on the server. Refer to <a class="xref" href="#app-deploy-smt-repos" title="4.3. Setting up Repository Mirroring on the SMT Server">Section 4.3, “Setting up Repository Mirroring on the SMT Server”</a> for more information. When all update
     and pool repositories are managed by a remote SMT server, make sure the
     repository location in YaST Crowbar is set to <span class="guimenu ">Remote SMT
     Server</span>. For details, see <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a>. No further action is
     required. The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation automatically detects all
     available repositories.
    </p><div id="id-1.3.4.4.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Accessing an External SMT Server</h6><p>
        When using an external SMT server, it needs to be reachable by all
        nodes. This means that the SMT server either needs to be part of the admin network
        or it needs to be accessible via the default route of the
        nodes. The latter can be either the gateway of the admin network or the gateway
        of the public network.
     </p></div></div><div class="sect2 " id="sec-depl-adm-conf-repos-scc-remote-susemgr"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Repositories Hosted on a SUSE Manager Server</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-remote-susemgr">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc-remote-susemgr</li></ul></div></div></div></div><p>
     To use repositories from SUSE Manager you first need to make sure all
     required products and extensions are registered, and the corresponding
     channels are mirrored in SUSE Manager (refer to
     <a class="xref" href="#tab-depl-adm-conf-susemgr-repos" title="SUSE Manager Repositories (Channels)">Table 5.4, “SUSE Manager Repositories (Channels)”</a> for a list of
     channels).
    </p><div id="id-1.3.4.4.3.7.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Accessing a SUSE Manager Server</h6><p>
      An external SUSE Manager server needs to be accessible to
      <span class="emphasis"><em>all</em></span> nodes in SUSE <span class="productname">OpenStack</span> Cloud. The network hosting the SUSE Manager server must be added to the
      network definitions as described in
      <a class="xref" href="#sec-depl-inst-admserv-post-network-external" title="7.5.8. Providing Access to External Networks">Section 7.5.8, “Providing Access to External Networks”</a>.
     </p></div><p>
     By default SUSE Manager does not expose repositories for direct access. To
     access them via HTTPS, you need to create a
     <span class="guimenu ">Distribution</span> for auto-installation for the SUSE Linux Enterprise Server 12 SP3
     (x86_64) product. Creating this distribution makes the update
     repositories for this product available, including the repositories
     for all registered add-on products (like <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, SLES High Availability Extension and
     SUSE Enterprise Storage). Instructions for creating a distribution are in the SUSE Manager documentation in <a class="link" href="https://documentation.suse.com/suma/" target="_blank">https://documentation.suse.com/suma/</a>.
    </p><p>
     During the distribution setups you need to provide a
     <span class="guimenu ">Label</span> for each the distribution. This label will be
     part of the URL under which the repositories are available. We
     recommend choosing a name consisting of characters that do not need to
     be URL-encoded. In <a class="xref" href="#tab-depl-adm-conf-susemgr-repos" title="SUSE Manager Repositories (Channels)">Table 5.4, “SUSE Manager Repositories (Channels)”</a> we
     assume the following label has been provided:
     <code class="literal">sles12-sp3-x86_64</code>.
    </p><p>
     When all update and pool repositories are managed by a SUSE Manager server,
     make sure the repository location in YaST Crowbar is set to
     <span class="guimenu ">SUSE Manager Server</span>. For details, see <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a>. No further action is
     required. The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation automatically detects all
     available repositories.
    </p><p>
     The autoinstallation tree provided by SUSE Manager does not provide the
     SLES Pool repository. Although this repository is not used
     for node installation, it needs to be present. To work around this issue,
     it is sufficient to create an empty Pool repository for SUSE Linux Enterprise Server 12 SP3:
    </p><div class="verbatim-wrap"><pre class="screen">mkdir /srv/tftpboot/suse-12.3/x86_64/repos/SLES12-SP3-Pool/
createrepo /srv/tftpboot/suse-12.3/x86_64/repos/SLES12-SP3-Pool/</pre></div></div><div class="sect2 " id="sec-depl-adm-conf-repos-scc-alternatives"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Ways to Make the Repositories Available</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-alternatives">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc-alternatives</li></ul></div></div></div></div><p>
     If you want to keep your SUSE <span class="productname">OpenStack</span> Cloud network as isolated from the company
     network as possible, or your infrastructure does not allow accessing a
     SUSE Manager or an SMT server, you can alternatively provide access to the
     required repositories by one of the following methods:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Mount the repositories from a remote server.
      </p></li><li class="listitem "><p>
       Synchronize the repositories from a remote server (for example via
       <code class="command">rsync</code> and cron).
      </p></li><li class="listitem "><p>
        Manually synchronize the update repositories from removable media.
      </p></li></ul></div><p>
     We strongly recommended making the repositories available at the
     default locations on the Administration Server as listed in <a class="xref" href="#tab-depl-adm-conf-local-repos" title="Default Repository Locations on the Administration Server">Table 5.5, “Default Repository Locations on the Administration Server”</a>. When choosing these locations,
     it is sufficient to set the repository location in YaST Crowbar to
     <span class="guimenu ">Custom</span>. You do not need to specify a detailed location
     for each repository. Refer to <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a> for details. If you prefer to
     use different locations, you need to announce each location with YaST
     Crowbar.
    </p></div></div><div class="sect1 " id="sec-depl-inst-admserv-post-adm-repos"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
    Software Repository Sources for the Administration Server Operating System
   </span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-adm-repos">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-admserv-post-adm-repos</li></ul></div></div></div></div><p>
   During the installation of the Administration Server, repository locations for SUSE Linux Enterprise Server 12 SP3
   are automatically added to the Administration Server. They point to the source
   used to install the Administration Server and to the SUSE Customer Center. These repository locations
   have no influence on the repositories used to set up nodes in the cloud. They
   are solely used to maintain and update the Administration Server itself.
  </p><p>
   However, as the Administration Server and all nodes in the cloud use the same
   operating system—SUSE Linux Enterprise Server 12 SP3—it makes sense to use the same
   repositories for the cloud and the Administration Server. To avoid
   downloading the same patches twice, change this setup so that the repositories set up for SUSE <span class="productname">OpenStack</span> Cloud deployment are also used
   on the Administration Server.
  </p><p>
   To do so, you need to disable or delete all services. In a second step all
   SUSE Linux Enterprise Server and SUSE <span class="productname">OpenStack</span> Cloud repositories need to be edited to point to the
   alternative sources. Use either Zypper or YaST to edit the repository setup. Note that changing the repository setup on the Administration Server
   is optional.
  </p></div><div class="sect1 " id="sec-deploy-repo-locations"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Repository Locations</span> <a title="Permalink" class="permalink" href="#sec-deploy-repo-locations">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_repos.xml</li><li><span class="ds-label">ID: </span>sec-deploy-repo-locations</li></ul></div></div></div></div><p>
The following tables show the locations of all repositories that can be used for SUSE OpenStack Cloud.
 </p><div class="table" id="tab-smt-repos-local"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.2: </span><span class="name">SMT Repositories Hosted on the Administration Server </span><a title="Permalink" class="permalink" href="#tab-smt-repos-local">#</a></h6></div><div class="table-contents"><table class="table" summary="SMT Repositories Hosted on the Administration Server" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Repository
      </p>
     </th><th>
      <p>
       Directory
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/OpenStack-Cloud-Crowbar/8/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/OpenStack-Cloud-Crowbar/8/x86_64/update/</code>
      </p>
     </td></tr><tr><td colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/SLE-HA/12-SP3/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/SLE-HA/12-SP3/x86_64/update/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/Storage/5/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/Storage/5/x86_64/update/</code>
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="tab-smt-repos-remote"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.3: </span><span class="name">SMT Repositories hosted on a Remote Server </span><a title="Permalink" class="permalink" href="#tab-smt-repos-remote">#</a></h6></div><div class="table-contents"><table class="table" summary="SMT Repositories hosted on a Remote Server" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Repository
      </p>
     </th><th>
      <p>
       URl
      </p>
     </th></tr></thead><tbody><tr><td>
      <p>
       Mandatory Repositories
      </p>
     </td><td> </td></tr><tr><td>
      <p>
       SLES12-SP3-Pool
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/</p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/</p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Pool
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Products/OpenStack-Cloud/7/x86_64/product/</p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Updates
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Updates/OpenStack-Cloud/7/x86_64/update/</p>
     </td></tr><tr><td colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Pool
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Products/SLE-HA/12-SP3/x86_64/product/</p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Updates
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Updates/SLE-HA/12-SP3/x86_64/update/</p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Products/Storage/4/x86_64/product/</p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>http://<em class="replaceable ">smt.example.com</em>/repo/SUSE/Updates/Storage/4/x86_64/update/</p>
     </td></tr></tbody></table></div></div><div class="table" id="tab-depl-adm-conf-susemgr-repos"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.4: </span><span class="name">SUSE Manager Repositories (Channels) </span><a title="Permalink" class="permalink" href="#tab-depl-adm-conf-susemgr-repos">#</a></h6></div><div class="table-contents"><table class="table" summary="SUSE Manager Repositories (Channels)" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Repository
      </p>
     </th><th>
      <p>
       URL
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td>
      <p>
       http://manager.example.com/ks/dist/child/sles12-sp3-updates-x86_64/sles12-sp3-x86_64/
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Pool
      </p>
     </td><td>
      <p>
       
       http://manager.example.com/ks/dist/child/suse-openstack-cloud-7-pool-x86_64/sles12-sp3-x86_64/
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>--Updates
      </p>
     </td><td>
      <p>
       
       http://manager.example.com/ks/dist/child/suse-openstack-cloud-7-updates-x86_64/sles12-sp3-x86_64/
      </p>
     </td></tr><tr><td colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Pool
      </p>
     </td><td>
      <p>
       http://manager.example.com/ks/dist/child/sle-ha12-sp3-pool-x86_64/sles12-sp3-x86_64/
      </p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Updates
      </p>
     </td><td>
      <p>
       http://manager.example.com/ks/dist/child/sle-ha12-sp3-updates-x86_64/sles12-sp3-x86_64/
      </p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td>
      <p>
       
       http://manager.example.com/ks/dist/child/suse-enterprise-storage-2.1-pool-x86_64/sles12-sp3-x86_64/
      </p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>
       
       http://manager.example.com/ks/dist/child/suse-enterprise-storage-4-updates-x86_64/sles12-sp3-x86_64/
      </p>
     </td></tr></tbody></table></div></div><p>
  The following table shows the recommended default repository locations  to use when manually copying, synchronizing, or mounting the
  repositories. When choosing these locations, it is sufficient to set the
  repository location in YaST Crowbar to <span class="guimenu ">Custom</span>. You do
  not need to specify a detailed location for each repository. Refer to <a class="xref" href="#sec-depl-adm-conf-repos-scc-alternatives" title="5.2.4. Alternative Ways to Make the Repositories Available">Section 5.2.4, “Alternative Ways to Make the Repositories Available”</a> and <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a> for details.
 </p><div class="table" id="tab-depl-adm-conf-local-repos"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.5: </span><span class="name">Default Repository Locations on the Administration Server </span><a title="Permalink" class="permalink" href="#tab-depl-adm-conf-local-repos">#</a></h6></div><div class="table-contents"><table class="table" summary="Default Repository Locations on the Administration Server" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
      <p>
       Channel
      </p>
     </th><th>
      <p>
       Directory on the Administration Server
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/SLES12-SP3-Pool/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/SLES12-SP3-Updates/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Pool/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Updates</code>
      </p>
     </td></tr><tr><td colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/SLE12-HA12-SP3-Pool</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLE12-HA12-SP3-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/SLE12-HA12-SP3-Updates</code>
      </p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/SUSE-Enterprise-Storage-5-Pool</code>
      </p>
     </td></tr><tr><td>
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/SUSE-Enterprise-Storage-5-Updates</code>
      </p>
     </td></tr></tbody></table></div></div></div></div><div class="chapter " id="sec-depl-adm-inst-network"><div class="titlepage"><div><div><h2 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service Configuration:  Administration Server Network Configuration</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-network">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_network.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.5.1.3">#</a></h6></div><p>
    Prior to starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, make sure the first network
    interface (<code class="systemitem">eth0</code>) gets a
    fixed IP address from the admin network. A host and domain name
    also need to be provided. Other interfaces will be automatically
    configured during the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
   </p></div></div></div></div><div class="line"></div><p>
  To configure the network interface proceed as follows:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Start <span class="guimenu ">YaST</span> › <span class="guimenu ">System</span> › <span class="guimenu ">Network Settings</span>.
   </p></li><li class="step "><p>
    Switch to the <span class="guimenu ">Overview</span> tab, select the interface
    with the <span class="guimenu ">Device</span> identifier, <code class="literal">eth0</code> and
    choose <span class="guimenu ">Edit</span>.
   </p></li><li class="step "><p>
    Switch to the <span class="guimenu ">Address</span> tab and activate
    <span class="guimenu ">Statically Assigned IP Address</span>. Provide an IPv4
    <span class="guimenu ">IP Address</span>, a <span class="guimenu ">Subnet Mask</span>, and a
    fully qualified <span class="guimenu ">Hostname</span>. Examples in this book assume
    the default IP address of <code class="systemitem">192.168.124.10</code> and a network mask of
    <code class="systemitem">255.255.255.0</code>. Using a
    different IP address requires adjusting the Crowbar configuration in a
    later step as described in <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>.
   </p></li><li class="step "><p>
    Check the settings on the <span class="guimenu ">General</span> tab. The device needs
    to be activated <span class="guimenu ">At Boot Time</span>. Confirm your settings
    with <span class="guimenu ">Next</span>.
   </p></li><li class="step "><p>
    Back on the <span class="guimenu ">Network Settings</span> dialog, switch to the
    <span class="guimenu ">Routing</span> tab and enter a <span class="guimenu ">Default IPv4
    Gateway</span>. The address depends on whether you
    have provided an external gateway for the admin network. In that case, use the address
    of that gateway. If not, use <em class="replaceable ">xxx.xxx.xxx</em>.1, for
    example, <code class="systemitem">192.168.124.1</code>. Confirm your settings
    with <span class="guimenu ">OK</span>.
   </p></li><li class="step "><p>
    Choose <span class="guimenu ">Hostname/DNS</span> from the <span class="guimenu ">Network
    Settings</span> dialog and set the <span class="guimenu ">Hostname</span> and
    <span class="guimenu ">Domain Name</span>. Examples in this book assume     <em class="replaceable ">admin.cloud.example.com</em> for the host/domain
    name.
   </p><p>
    If the Administration Server has access to the outside, you can add additional name
    servers here that will automatically be used to forward requests. The
    Administration Server's name server will automatically be configured during the
    SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation to forward requests for non-local records to those
    server(s).
   </p></li><li class="step "><p>
    Last, check if the firewall is disabled. Return to YaST's main menu
    (<span class="guimenu ">YaST Control Center</span>) and start <span class="guimenu ">Security and Users</span> › <span class="guimenu ">Firewall</span>. On <span class="guimenu ">Start-Up</span> › <span class="guimenu ">Service Start</span>, the firewall needs to be
    disabled. Confirm your settings with <span class="guimenu ">Next</span>.
   </p></li></ol></div></div><div id="id-1.3.4.5.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Administration Server Domain Name and Host name</h6><p>
   Setting up the SUSE <span class="productname">OpenStack</span> Cloud will also install a DNS server for all nodes in the
   cloud. The domain name you specify for the Administration Server will be used for the
   DNS zone. It is required to use a sub-domain such as
   <em class="replaceable ">cloud.example.com</em>. See <a class="xref" href="#sec-depl-req-network-dns" title="2.1.4. DNS and Host Names">Section 2.1.4, “DNS and Host Names”</a> for more information.
  </p><p>
   The host name and the FQDN need to be resolvable with
   <code class="command">hostname</code> <code class="option">-f</code>. Double-check whether
   <code class="filename">/etc/hosts</code> contains an appropriate entry for the
   Administration Server. It should look like the following:
  </p><div class="verbatim-wrap"><pre class="screen">192.168.124.10 admin.cloud.example.com admin</pre></div><p>
   It is <span class="emphasis"><em>not</em></span> possible to change the Administration Server host name
   or the FQDN after the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation has been completed.
  </p></div></div><div class="chapter " id="sec-depl-adm-inst-crowbar"><div class="titlepage"><div><div><h2 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Crowbar Setup</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.6.1.3">#</a></h6></div><p>
    The YaST Crowbar module enables you to configure all networks within the
    cloud, to set up additional repositories, and to manage the Crowbar users.
    This module should be launched before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. To start
    this module, either run <code class="command">yast crowbar</code> or <span class="guimenu ">YaST</span> › <span class="guimenu ">Miscellaneous</span> › <span class="guimenu ">Crowbar</span>.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-user"><span class="number">7.1 </span><span class="name"><span class="guimenu ">User Settings</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-network"><span class="number">7.2 </span><span class="name"><span class="guimenu ">Networks</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-mode"><span class="number">7.3 </span><span class="name"><span class="guimenu ">Network Mode</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-repos"><span class="number">7.4 </span><span class="name"><span class="guimenu ">Repositories</span></span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-admserv-post-network"><span class="number">7.5 </span><span class="name">Custom Network Configuration</span></a></span></dt></dl></div></div><div class="sect1 " id="sec-depl-adm-inst-crowbar-user"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="guimenu ">User Settings</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-user">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-crowbar-user</li></ul></div></div></div></div><p>
   In this section, you can manage users for the Crowbar Web interface. The user
   <code class="systemitem">crowbar</code> (password
   <code class="literal">crowbar</code>) is preconfigured. Use the
   <span class="guimenu ">Add</span>, <span class="guimenu ">Edit</span>, and
   <span class="guimenu ">Delete</span> buttons to manage user accounts. Users configured
   here have no relation to existing system users on the Administration Server.
  </p><div class="figure" id="id-1.3.4.6.2.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast_crowbar_user.png" target="_blank"><img src="images/yast_crowbar_user.png" width="" alt="YaST Crowbar Setup: User Settings" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7.1: </span><span class="name">YaST Crowbar Setup: User Settings </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.2.3">#</a></h6></div></div></div><div class="sect1 " id="sec-depl-adm-inst-crowbar-network"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="guimenu ">Networks</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-network">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-crowbar-network</li></ul></div></div></div></div><p>
   Use the <span class="guimenu ">Networks</span> tab to change the default network setup
   (described in <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a>). Change the IP address
   assignment for each network under <span class="guimenu ">Edit Ranges</span>. You may
   also add a bridge (<span class="guimenu ">Add Bridge</span>) or a VLAN (<span class="guimenu ">Use
   VLAN</span>, <span class="guimenu ">VLAN ID</span>) to a network. Only change the
   latter two settings if you really know what you require; we recommend
   sticking with the defaults.
  </p><div class="figure" id="id-1.3.4.6.3.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast_crowbar_networks.png" target="_blank"><img src="images/yast_crowbar_networks.png" width="" alt="YaST Crowbar Setup: Network Settings" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7.2: </span><span class="name">YaST Crowbar Setup: Network Settings </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.3.3">#</a></h6></div></div><div id="id-1.3.4.6.3.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</h6><p>
    After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, you cannot change the network
    setup. If you do need to change it, you must completely set up the
    Administration Server again.
   </p></div><div id="id-1.3.4.6.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: VLAN Settings</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>, using a VLAN for the admin network is
    only supported on a native/untagged VLAN. If you need VLAN support for the
    admin network, it must be handled at switch level.
   </p><p>
    When changing the network configuration with YaST or by editing
    <code class="filename">/etc/crowbar/network.json</code>, you can define VLAN
    settings for each network. For the networks <code class="literal">nova-fixed</code>
    and <code class="literal">nova-floating</code>, however, special rules apply:
   </p><p>
    <span class="bold"><strong>nova-fixed</strong></span>: The <span class="guimenu ">USE
    VLAN</span> setting will be ignored. However, VLANs will automatically
    be used if deploying Neutron with VLAN support (using the drivers
    linuxbridge, openvswitch plus VLAN, or cisco_nexus). In this case, you need
    to specify a correct <span class="guimenu ">VLAN ID</span> for this network.
   </p><p>
    <span class="bold"><strong>nova-floating</strong></span>: When using a VLAN for
    <code class="literal">nova-floating</code> (which is the default), the <span class="guimenu ">USE
    VLAN</span> and <span class="guimenu ">VLAN ID</span> settings for
    <span class="guimenu ">nova-floating</span> and <span class="guimenu ">public</span> default to
    the same.
   </p><p>
    You have the option of separating public and floating networks with a
    custom configuration. Configure your own separate floating network (not as
    a subnet of the public network), and give the floating network its own
    router. For example, define <code class="literal">nova-floating</code> as part of an
    external network with a custom <code class="literal">bridge-name</code>. When you are
    using different networks and OpenVSwitch is configured, the pre-defined
    <code class="literal">bridge-name</code> won't work.
   </p></div><p>
   Other, more flexible network mode setups, can be configured by manually
   editing the Crowbar network configuration files. See
   <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for more information.
   SUSE or a partner can assist you in creating a custom setup within the
   scope of a consulting services agreement. See
   <a class="link" href="http://www.suse.com/consulting/" target="_blank">http://www.suse.com/consulting/</a> for more information on
   SUSE consulting.
  </p><div class="sect2 " id="sec-depl-adm-inst-crowbar-network-bmc"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Separating the Admin and the BMC Network</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-network-bmc">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-crowbar-network-bmc</li></ul></div></div></div></div><p>
    If you want to separate the admin and the BMC network, you must change the
    settings for the networks <span class="guimenu ">bmc</span> and
    <span class="guimenu ">bmc_vlan</span>. The <span class="guimenu ">bmc_vlan</span> is used to
    generate a VLAN tagged interface on the Administration Server that can access the
    <span class="guimenu ">bmc</span> network. The <span class="guimenu ">bmc_vlan</span> needs to be
    in the same ranges as <span class="guimenu ">bmc</span>, and <span class="guimenu ">bmc</span>
    needs to have <span class="guimenu ">VLAN</span> enabled.
   </p><div class="table" id="id-1.3.4.6.3.7.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 7.1: </span><span class="name">Separate BMC Network Example Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.3.7.3">#</a></h6></div><div class="table-contents"><table class="table" summary="Separate BMC Network Example Configuration" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
        
       </th><th align="center">
        <p>
         bmc
        </p>
       </th><th align="center">
        <p>
         bmc_vlan
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         Subnet
        </p>
       </td><td colspan="2" align="center">
        <p>
         <code class="systemitem">192.168.128.0</code>
        </p>
       </td></tr><tr><td>
        <p>
         Netmask
        </p>
       </td><td colspan="2" align="center">
        <p>
         <code class="systemitem">255.255.255.0</code>
        </p>
       </td></tr><tr><td>
        <p>
         Router
        </p>
       </td><td colspan="2" align="center">
        <p>
         <code class="systemitem">192.168.128.1</code>
        </p>
       </td></tr><tr><td>
        <p>
         Broadcast
        </p>
       </td><td colspan="2" align="center">
        <p>
         <code class="systemitem">192.168.128.255</code>
        </p>
       </td></tr><tr><td>
        <p>
         Host Range
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.128.10</code> -
         <code class="systemitem">192.168.128.100</code>
        </p>
       </td><td>
        <p>
         <code class="systemitem">192.168.128.101</code> -
         <code class="systemitem">192.168.128.101</code>
        </p>
       </td></tr><tr><td>
        <p>
         VLAN
        </p>
       </td><td colspan="2" align="center">
        <p>
         yes
        </p>
       </td></tr><tr><td>
        <p>
         VLAN ID
        </p>
       </td><td colspan="2" align="center">
        <p>
         100
        </p>
       </td></tr><tr><td>
        <p>
         Bridge
        </p>
       </td><td colspan="2" align="center">
        <p>
         no
        </p>
       </td></tr></tbody></table></div></div><div class="figure" id="id-1.3.4.6.3.7.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast_crowbar_networks_bmc.png" target="_blank"><img src="images/yast_crowbar_networks_bmc.png" width="" alt="YaST Crowbar Setup: Network Settings for the BMC Network" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7.3: </span><span class="name">YaST Crowbar Setup: Network Settings for the BMC Network </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.3.7.4">#</a></h6></div></div></div></div><div class="sect1 " id="sec-depl-adm-inst-crowbar-mode"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="guimenu ">Network Mode</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-mode">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-crowbar-mode</li></ul></div></div></div></div><p>
   On the <span class="guimenu ">Network Mode</span> tab you can choose between
   <span class="guimenu ">single</span>, <span class="guimenu ">dual</span>, and
   <span class="guimenu ">team</span>. In single mode, all traffic is handled by a single
   Ethernet card. Dual mode requires two Ethernet cards and separates traffic
   for private and public networks. See
   <a class="xref" href="#sec-depl-req-network-modes" title="2.1.2. Network Modes">Section 2.1.2, “Network Modes”</a> for details.
  </p><p>
   Team mode is similar to single mode, except that you combine several
   Ethernet cards to a “bond”. It is required for an HA setup of SUSE <span class="productname">OpenStack</span> Cloud.
   When choosing this mode, you also need to specify a <span class="guimenu ">Bonding
   Policy</span>. This option lets you define whether to focus on
   reliability (fault tolerance), performance (load balancing), or a
   combination of both. You can choose from the following modes:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.6.4.4.1"><span class="term "><span class="guimenu ">0</span> (balance-rr)</span></dt><dd><p>
      Default mode in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Packets are transmitted in round-robin
      fashion from the first to the last available interface. Provides fault
      tolerance and load balancing.
     </p></dd><dt id="id-1.3.4.6.4.4.2"><span class="term "><span class="guimenu ">1</span> (active-backup)</span></dt><dd><p>
      Only one network interface is active. If it fails, a different interface
      becomes active. This setting is the default for SUSE <span class="productname">OpenStack</span> Cloud. Provides fault
      tolerance.
     </p></dd><dt id="id-1.3.4.6.4.4.3"><span class="term "><span class="guimenu ">2</span> (balance-xor)</span></dt><dd><p>
      Traffic is split between all available interfaces based on the following
      policy: <code class="literal">[(source MAC address XOR'd with destination MAC address
      XOR packet type ID) modulo slave count]</code> Requires support from
      the switch. Provides fault tolerance and load balancing.
     </p></dd><dt id="id-1.3.4.6.4.4.4"><span class="term "><span class="guimenu ">3</span> (broadcast)</span></dt><dd><p>
      All traffic is broadcast on all interfaces. Requires support from the
      switch. Provides fault tolerance.
     </p></dd><dt id="id-1.3.4.6.4.4.5"><span class="term "><span class="guimenu ">4</span> (802.3ad)</span></dt><dd><p>
      Aggregates interfaces into groups that share the same speed and duplex
      settings. Requires <code class="command">ethtool</code> support in the interface
      drivers, and a switch that supports and is configured for IEEE 802.3ad
      Dynamic link aggregation. Provides fault tolerance and load balancing.
     </p></dd><dt id="id-1.3.4.6.4.4.6"><span class="term "><span class="guimenu ">5</span> (balance-tlb)</span></dt><dd><p>
      Adaptive transmit load balancing. Requires <code class="command">ethtool</code>
      support in the interface drivers but no switch support. Provides fault
      tolerance and load balancing.
     </p></dd><dt id="id-1.3.4.6.4.4.7"><span class="term "><span class="guimenu ">6</span> (balance-alb)</span></dt><dd><p>
      Adaptive load balancing. Requires <code class="command">ethtool</code> support in
      the interface drivers but no switch support. Provides fault tolerance and
      load balancing.
     </p></dd></dl></div><p>
   For a more detailed description of the modes, see
   <a class="link" href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" target="_blank">https://www.kernel.org/doc/Documentation/networking/bonding.txt</a>.
  </p><div class="sect2 " id="sec-depl-adm-inst-crowbar-mode-bastion"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up a Bastion Network</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-mode-bastion">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-crowbar-mode-bastion</li></ul></div></div></div></div><p>
    The <span class="guimenu ">Network Mode</span> tab of the YaST Crowbar module also
    lets you set up a Bastion network. As outlined in
    <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a>, one way to access the Administration Server
    from a defined external network is via a Bastion network and a second
    network card (as opposed to providing an external gateway).
   </p><p>
    To set up the Bastion network, you need to have a static IP address for the
    Administration Server from the external network. The example configuration used below
    assumes that the external network from which to access the admin network
    has the following addresses. Adjust them according to your needs.
   </p><div class="table" id="id-1.3.4.6.4.6.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 7.2: </span><span class="name">Example Addresses for a Bastion Network </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.4.6.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Example Addresses for a Bastion Network" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><tbody><tr><td>
        <p>
         Subnet
        </p>
       </td><td>
        <p>
         <code class="systemitem">10.10.1.0</code>
        </p>
       </td></tr><tr><td>
        <p>
         Netmask
        </p>
       </td><td>
        <p>
         <code class="systemitem">255.255.255.0</code>
        </p>
       </td></tr><tr><td>
        <p>
         Broadcast
        </p>
       </td><td>
        <p>
         <code class="systemitem">10.10.1.255</code>
        </p>
       </td></tr><tr><td>
        <p>
         Gateway
        </p>
       </td><td>
        <p>
         <code class="systemitem">10.10.1.1</code>
        </p>
       </td></tr><tr><td>
        <p>
         Static Administration Server address
        </p>
       </td><td>
        <p>
         <code class="systemitem">10.10.1.125</code>
        </p>
       </td></tr></tbody></table></div></div><p>
    In addition to the values above, you need to enter the <span class="guimenu ">Physical
    Interface Mapping</span>. With this value you specify the Ethernet card
    that is used for the bastion network. See
    <a class="xref" href="#sec-deploy-network-json-conduits" title="7.5.5. Network Conduits">Section 7.5.5, “Network Conduits”</a> for details on the
    syntax. The default value <code class="literal">?1g2</code> matches the second
    interface (<span class="quote">“<span class="quote ">eth1</span>”</span>) of the system.
   </p><div class="figure" id="id-1.3.4.6.4.6.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast_crowbar_networks_bastion.png" target="_blank"><img src="images/yast_crowbar_networks_bastion.png" width="" alt="YaST Crowbar Setup: Network Settings for the Bastion Network" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7.4: </span><span class="name">YaST Crowbar Setup: Network Settings for the Bastion Network </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.4.6.6">#</a></h6></div></div><div id="id-1.3.4.6.4.6.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</h6><p>
     After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, you cannot change the network
     setup. If you do need to change it, you must completely set up the
     Administration Server again.
    </p></div><div id="id-1.3.4.6.4.6.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Accessing Nodes From Outside the Bastion Network</h6><p>
     The example configuration from above allows access to the SUSE <span class="productname">OpenStack</span> Cloud nodes
     from <span class="emphasis"><em>within</em></span> the bastion network. If you want to
     access nodes from outside the bastion network, make the router for the
     bastion network the default router for the Administration Server. This is achieved by
     setting the value for the bastion network's <span class="guimenu ">Router
     preference</span> entry to a lower value than the corresponding entry
     for the admin network. By default no router preference is set for the
     Administration Server—in this case, set the preference for the bastion network
     to <code class="literal">5</code>.
    </p><p>
     If you use a Linux gateway between the outside and the bastion network,
     you also need to disable route verification (rp_filter) on the Administration Server.
     Do so by running the following command on the Administration Server:
    </p><div class="verbatim-wrap"><pre class="screen">echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</pre></div><p>
     That command disables route verification for the current session, so the
     setting will not survive a reboot. Make it permanent by editing
     <code class="filename">/etc/sysctl.conf</code> and setting the value for
     <span class="guimenu ">net.ipv4.conf.all.rp_filter</span> to <code class="literal">0</code>.
    </p></div></div></div><div class="sect1 " id="sec-depl-adm-inst-crowbar-repos"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="guimenu ">Repositories</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-repos">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-crowbar-repos</li></ul></div></div></div></div><p>
   This dialog lets you announce the locations of the product, pool, and update
   repositories (see <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a> for details). You can
   choose between four alternatives:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.6.5.3.1"><span class="term "><span class="guimenu ">Local SMT Server</span>
    </span></dt><dd><p>
      If you have an SMT server installed on the Administration Server as explained in
      <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>, choose this option. The repository
      details do not need to be provided as they will be configured
      automatically. This option will be applied by default if the repository
      configuration has not been changed manually.
     </p></dd><dt id="id-1.3.4.6.5.3.2"><span class="term "><span class="guimenu ">Remote SMT Server</span>
    </span></dt><dd><p>
      If you use a remote SMT for <span class="emphasis"><em>all</em></span> repositories,
      choose this option and provide the <span class="guimenu ">Sever URL</span> (in the
      form of <code class="literal">http://smt.example.com</code>). The repository
      details do not need to be provided, they will be configured
      automatically.
     </p></dd><dt id="id-1.3.4.6.5.3.3"><span class="term "><span class="guimenu ">SUSE Manager Server</span>
    </span></dt><dd><p>
      If you use a remote SUSE Manager server for <span class="emphasis"><em>all</em></span>
      repositories, choose this option and provide the <span class="guimenu ">Sever
      URL</span> (in the form of
      <code class="literal">http://manager.example.com</code>).
     </p></dd><dt id="id-1.3.4.6.5.3.4"><span class="term ">Custom</span></dt><dd><p>
      If you use different sources for your repositories or are using
      non-standard locations, choose this option and manually provide a
      location for each repository. This can either be a local directory
      (<code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/SLES12-SP3-Pool/</code>)
      or a remote location
      (<code class="literal">http://manager.example.com/ks/dist/child/sles12-sp3-updates-x86_64/sles12-sp3-x86_64/</code>).
      Activating <span class="guimenu ">Ask On Error</span> ensures that you will be
      informed if a repository is not available during node deployment,
      otherwise errors will be silently ignored.
     </p><p>
      The <span class="guimenu ">Add Repository</span> dialog allows adding additional
      repositories. See
      <a class="xref" href="#q-depl-trouble-faq-admin-custom-repos" title="Q:"><em>How to make custom software repositories from an external server (for example a remote SMT or SUSE M..?</em></a> for
      instructions.
     </p><div id="id-1.3.4.6.5.3.4.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Default Locations</h6><p>
       If you have made the repositories available in the default locations on
       the Administration Server (see
       <a class="xref" href="#tab-depl-adm-conf-local-repos" title="Default Repository Locations on the Administration Server">Table 5.5, “Default Repository Locations on the Administration Server”</a> for a list),
       choose <span class="guimenu ">Custom</span> and leave the <span class="guimenu ">Repository
       URL</span> empty (default). The repositories will automatically be
       detected.
      </p></div></dd></dl></div><div class="figure" id="id-1.3.4.6.5.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/yast_crowbar_repos.png" target="_blank"><img src="images/yast_crowbar_repos.png" width="" alt="YaST Crowbar Setup: Repository Settings" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7.5: </span><span class="name">YaST Crowbar Setup: Repository Settings </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.5.4">#</a></h6></div></div></div><div class="sect1 " id="sec-depl-inst-admserv-post-network"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Custom Network Configuration</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-admserv-post-network</li></ul></div></div></div></div><p>
   To adjust the pre-defined network setup of SUSE <span class="productname">OpenStack</span> Cloud beyond the scope of
   changing IP address assignments (as described in
   <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>), modify the network barclamp
   template.
  </p><p>
   The Crowbar network barclamp provides two functions for the system. The first
   is a common role to instantiate network interfaces on the Crowbar managed
   systems. The other function is address pool management. While the addresses
   can be managed with the YaST Crowbar module, complex network setups require
   to manually edit the network barclamp template file
   <code class="filename">/etc/crowbar/network.json</code>. This section explains the
   file in detail. Settings in this file are applied to all nodes in SUSE <span class="productname">OpenStack</span> Cloud.
   (See <a class="xref" href="#sec-network-json-resolve" title="7.5.11. Matching Logical and Physical Interface Names with network-json-resolve">Section 7.5.11, “Matching Logical and Physical Interface Names with network-json-resolve”</a> to learn how to verify
   your correct network interface names.)
  </p><div id="id-1.3.4.6.6.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</h6><p>
    After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation installation, you cannot change
    the network setup. If you do need to change it, you must completely set up
    the Administration Server again.
   </p><p>
    The only exception to this rule is the interface map, which can be changed
    after setup. See <a class="xref" href="#sec-deploy-network-json-interface-map" title="7.5.3. Interface Map">Section 7.5.3, “Interface Map”</a>
    for details.
   </p></div><div class="sect2 " id="sec-deploy-network-json-edit"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Editing <code class="filename">network.json</code></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-edit">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-deploy-network-json-edit</li></ul></div></div></div></div><p>
    The <code class="filename">network.json</code> file is located in
    <code class="filename">/etc/crowbar/</code>. The template has the following general
    structure:
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "<em class="replaceable ">VALUE</em>",
         "start_up_delay" : <em class="replaceable ">VALUE</em>,
         "teaming" : { "mode": <em class="replaceable ">VALUE</em> },<span id="structure-general"></span><span class="callout">1</span>
         "enable_tx_offloading" : <em class="replaceable ">VALUE</em>,
         "enable_rx_offloading" : <em class="replaceable ">VALUE</em>,
         "interface_map"<span id="structure-interface-map"></span><span class="callout">2</span> : [
            ...
         ],
         "conduit_map"<span id="structure-conduit"></span><span class="callout">3</span> : [
            ...
         ],
         "networks"<span id="structure-networks"></span><span class="callout">4</span> : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#structure-general"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
      General attributes. Refer to
      <a class="xref" href="#sec-deploy-network-json-global" title="7.5.2. Global Attributes">Section 7.5.2, “Global Attributes”</a> for details.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#structure-interface-map"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
      Interface map section. Defines the order in which the physical network
      interfaces are to be used. Refer to
      <a class="xref" href="#sec-deploy-network-json-interface-map" title="7.5.3. Interface Map">Section 7.5.3, “Interface Map”</a> for details.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#structure-conduit"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
      Network conduit section defining the network modes and the network
      interface usage. Refer to
      <a class="xref" href="#sec-deploy-network-json-conduits" title="7.5.5. Network Conduits">Section 7.5.5, “Network Conduits”</a> for details.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#structure-networks"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
      Network definition section. Refer to
      <a class="xref" href="#sec-deploy-network-json-networks" title="7.5.7. Network Definitions">Section 7.5.7, “Network Definitions”</a> for details.
     </p></td></tr></table></div><div id="id-1.3.4.6.6.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Order of Elements</h6><p>
     The order in which the entries in the <code class="filename">network.json</code>
     file appear may differ from the one listed above. Use your editor's search
     function to find certain entries.
    </p></div></div><div class="sect2 " id="sec-deploy-network-json-global"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Global Attributes</span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-global">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-deploy-network-json-global</li></ul></div></div></div></div><p>
    The most important options to define in the global attributes section are
    the default values for the network and bonding modes. The following global
    attributes exist:
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",<span id="global-mode"></span><span class="callout">1</span>
         "start_up_delay" : 30,<span id="global-startup"></span><span class="callout">2</span>
         "teaming" : { "mode": 5 },<span id="global-bonding"></span><span class="callout">3</span>
         "enable_tx_offloading" : true, <span id="global-tx"></span><span class="callout">4</span>
         "enable_rx_offloading" : true, <a class="xref" href="#global-tx"><span class="callout">4</span></a>
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#global-mode"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
      Network mode. Defines the configuration name (or name space) to be used
      from the conduit_map (see
      <a class="xref" href="#sec-deploy-network-json-conduits" title="7.5.5. Network Conduits">Section 7.5.5, “Network Conduits”</a>). Your choices are
      single, dual, or team.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#global-startup"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
      Time (in seconds) the Chef-client waits for the network interfaces to
      come online before timing out.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#global-bonding"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
      Default bonding mode. For a list of available modes, see
      <a class="xref" href="#sec-depl-adm-inst-crowbar-mode" title="7.3. Network Mode">Section 7.3, “<span class="guimenu ">Network Mode</span>”</a>.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#global-tx"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
      Turn on/off TX and RX checksum offloading. If set to
      <code class="literal">false</code>, disable offloading by running <code class="command">ethtool
      -K</code> and adding the setting to the respective ifcfg configuration
      file. If set to <code class="literal">true</code>, use the defaults of the network
      driver. If the network driver supports TX and/or RX checksum offloading
      and enables it by default, it will be used.
     </p><p>
      Checksum offloading is set to <code class="literal">true</code> in
      <code class="filename">network.json</code> by default. It is recommended to keep
      this setting. If you experience problems, such as package losses, try
      disabling this feature by setting the value to <code class="literal">false</code>.
     </p><div id="id-1.3.4.6.6.6.4.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Change of the Default Value</h6><p>
       Starting with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, the default value for TX and RX checksum
       offloading changed from <code class="literal">false</code> to
       <code class="literal">true</code>.
      </p></div><p>
      To check which defaults a network driver uses, run <code class="command">ethtool
      -k</code>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ethtool -k eth0 | grep checksumming
rx-checksumming: on
tx-checksumming: on</pre></div><p>
      Note that if the output shows a value marked as
      <code class="literal">[fixed]</code>, this value cannot be changed. For more
      information on TX and RX checksum offloading refer to your hardware
      vendor's documentation. Detailed technical information can also be
      obtained from
      <a class="link" href="https://www.kernel.org/doc/Documentation/networking/checksum-offloads.txt" target="_blank">https://www.kernel.org/doc/Documentation/networking/checksum-offloads.txt</a>.
     </p></td></tr></table></div></div><div class="sect2 " id="sec-deploy-network-json-interface-map"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interface Map</span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-interface-map">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-deploy-network-json-interface-map</li></ul></div></div></div></div><p>
    By default, physical network interfaces are used in the order they appear
    under <code class="filename">/sys/class/net/</code>. If you want to apply a
    different order, you need to create an interface map where you can specify
    a custom order of the bus IDs. Interface maps are created for specific
    hardware configurations and are applied to all machines matching this
    configuration.
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true ,
         "enable_rx_offloading" : true ,
         "interface_map" : [
            {
              "pattern" : "PowerEdge R610"<span id="interface-pattern"></span><span class="callout">1</span>,
              "serial_number" : "0x02159F8E"<span id="interface-serial"></span><span class="callout">2</span>,
              "bus_order" : [<span id="interface-bus"></span><span class="callout">3</span>
                "0000:00/0000:00:01",
                "0000:00/0000:00:03"
              ]
            }
            ...
         ],
         "conduit_map" : [
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#interface-pattern"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
      Hardware specific identifier. This identifier can be obtained by running
      the command <code class="command">dmidecode</code> <code class="option">-s
      system-product-name</code> on the machine you want to identify. You can
      log in to a node during the hardware discovery phase (when booting the
      SLEShammer image) via the Administration Server.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#interface-serial"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
      Additional hardware specific identifier. This identifier can be used in
      case two machines have the same value for <span class="guimenu ">pattern</span>, but
      different interface maps are needed. Specifying this parameter is
      optional (it is not included in the default
      <code class="filename">network.json</code> file). The serial number of a machine
      can be obtained by running the command <code class="command">dmidecode</code>
      <code class="option">-s system-serial-number</code> on the machine you want to
      identify.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#interface-bus"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
      Bus IDs of the interfaces. The order in which they are listed here
      defines the order in which Chef addresses the interfaces. The IDs can
      be obtained by listing the contents of
      <code class="filename">/sys/class/net/</code>.
     </p></td></tr></table></div><div id="id-1.3.4.6.6.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: PXE Boot Interface Must be Listed First</h6><p>
     The physical interface used to boot the node via PXE must always be listed
     first.
    </p></div><div id="id-1.3.4.6.6.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Interface Map Changes Allowed After Having Completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar Installation</h6><p>
     Contrary to all other sections in <code class="filename">network.json</code>, you
     can change interface maps after completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. However, nodes
     that are already deployed and affected by these changes must be deployed
     again. Therefore, we do not recommend making changes to the interface map
     that affect active nodes.
    </p><p>
     If you change the interface mappings after completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation you
     <span class="emphasis"><em>must not</em></span> make your changes by editing
     <code class="filename">network.json</code>. You must rather use the Crowbar Web
     interface and open <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span> › <span class="guimenu ">Network</span> › <span class="guimenu ">Edit</span>. Activate your changes by clicking
     <span class="guimenu ">Apply</span>.
    </p></div></div><div class="sect2 " id="sec-deploy-network-json-interface-map-examples"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interface Map Example</span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-interface-map-examples">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-deploy-network-json-interface-map-examples</li></ul></div></div></div></div><div class="complex-example"><div class="example" id="interface-map-example"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 7.1: </span><span class="name">Changing the Network Interface Order on a Machine with four NICs </span><a title="Permalink" class="permalink" href="#interface-map-example">#</a></h6></div><div class="example-contents"><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       Get the machine identifier by running the following command on the
       machine to which the map should be applied:
      </p><div class="verbatim-wrap"><pre class="screen">~ # dmidecode -s system-product-name
AS 2003R</pre></div><p>
       The resulting string needs to be entered on the
       <span class="guimenu ">pattern</span> line of the map. It is interpreted as a Ruby
       regular expression (see
       <a class="link" href="http://www.ruby-doc.org/core-2.0/Regexp.html" target="_blank">http://www.ruby-doc.org/core-2.0/Regexp.html</a> for a
       reference). Unless the pattern starts with <code class="literal">^</code> and ends
       with <code class="literal">$</code>, a substring match is performed against the
       name returned from the above commands.
      </p></li><li class="listitem "><p>
       List the interface devices in <code class="filename">/sys/class/net</code> to get
       the current order and the bus ID of each interface:
      </p><div class="verbatim-wrap"><pre class="screen">~ # ls -lgG /sys/class/net/ | grep eth
lrwxrwxrwx 1 0 Jun 19 08:43 eth0 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.0/net/eth0
lrwxrwxrwx 1 0 Jun 19 08:43 eth1 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.1/net/eth1
lrwxrwxrwx 1 0 Jun 19 08:43 eth2 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.2/net/eth2
lrwxrwxrwx 1 0 Jun 19 08:43 eth3 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.3/net/eth3</pre></div><p>
       The bus ID is included in the path of the link target—it is the
       following string: <code class="filename">../../devices/pci<em class="replaceable ">BUS
       ID</em>/net/eth0</code>
      </p></li><li class="listitem "><p>
       Create an interface map with the bus ID listed in the order the
       interfaces should be used. Keep in mind that the interface from which
       the node is booted using PXE must be listed first. In the following
       example the default interface order has been changed to
       <code class="systemitem">eth0</code>,
       <code class="systemitem">eth2</code>,
       <code class="systemitem">eth1</code> and
       <code class="systemitem">eth3</code>.
      </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            {
              "pattern" : "AS 2003R",
              "bus_order" : [
                "0000:00/0000:00:1c.0/0000:09:00.0",
                "0000:00/0000:00:1c.0/0000:09:00.2",
                "0000:00/0000:00:1c.0/0000:09:00.1",
                "0000:00/0000:00:1c.0/0000:09:00.3"
              ]
            }
            ...
         ],
         "conduit_map" : [
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div></li></ol></div></div></div></div></div><div class="sect2 " id="sec-deploy-network-json-conduits"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Conduits</span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-conduits">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-deploy-network-json-conduits</li></ul></div></div></div></div><p>
    Network conduits define mappings for logical interfaces—one or more
    physical interfaces bonded together. Each conduit can be identified by a
    unique name, the <span class="guimenu ">pattern</span>. This pattern is also called
    <span class="quote">“<span class="quote ">Network Mode</span>”</span> in this document.
   </p><p>
    Three network modes are available:
   </p><table border="0" summary="Simple list" class="simplelist "><tr><td><span class="bold"><strong>single</strong></span>: Only use the first interface for
    all networks. VLANs will be added on top of this single interface.
   </td></tr><tr><td><span class="bold"><strong>dual</strong></span>: Use the first interface as the admin
    interface and the second one for all other networks. VLANs will be added
    on top of the second interface.
   </td></tr><tr><td><span class="bold"><strong>team</strong></span>: Bond the first two or more
    interfaces. VLANs will be added on top of the bond.
   </td></tr></table><p>
    See <a class="xref" href="#sec-depl-req-network-modes" title="2.1.2. Network Modes">Section 2.1.2, “Network Modes”</a> for detailed descriptions.
    Apart from these modes a fallback mode <code class="literal">".*/.*/.*"</code> is
    also pre-defined—it is applied in case no other mode matches the one
    specified in the global attributes section. These modes can be adjusted
    according to your needs. It is also possible to customize modes, but mode
    names must be either <code class="literal">single</code>, <code class="literal">dual</code>, or
    <code class="literal">team</code>. 
   </p><p>
    The mode name that is specified with <code class="literal">mode</code> in the global
    attributes section is deployed on all nodes in SUSE <span class="productname">OpenStack</span> Cloud. It is not possible
    to use a different mode for a certain node. However, you can define
    <span class="quote">“<span class="quote ">sub</span>”</span> modes with the same name that only match the following
    machines:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Machines with a certain number of physical network interfaces.
     </p></li><li class="listitem "><p>
      Machines with certain roles (all Compute Nodes for example).
     </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
            {
              "pattern" : "single/.*/.*"<span id="conduit-pattern"></span><span class="callout">1</span>,
              "conduit_list" : {
                "intf2"<span id="conduit-name"></span><span class="callout">2</span> : {
                  "if_list" : ["1g1","1g2"]<span id="conduit-interface"></span><span class="callout">3</span>,
                  "team_mode" : 5<span id="conduit-bonding"></span><span class="callout">4</span>
                },
                "intf1" : {
                  "if_list" : ["1g1","1g2"],
                  "team_mode" : 5
                },
                "intf0" : {
                  "if_list" : ["1g1","1g2"],
                  "team_mode" : 5
                }
              }
            },
         ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#conduit-pattern"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
      This line contains the pattern definition for the
      <code class="systemitem">conduit_map</code>. The value for pattern must have the
      following form:
     </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">MODE_NAME</em>/<em class="replaceable ">NUMBER_OF_NICS</em>/<em class="replaceable ">NODE_ROLE</em></pre></div><p>
      Each field in the pattern is interpreted as a Ruby regular expression
      (see <a class="link" href="http://www.ruby-doc.org/core-2.0/Regexp.html" target="_blank">http://www.ruby-doc.org/core-2.0/Regexp.html</a>
      for a reference).
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.6.6.9.9.1.4.1"><span class="term ">mode_name
      </span></dt><dd><p>
         Name of the network mode. This string is used to reference the mode
         from the general attributes section.
        </p></dd><dt id="id-1.3.4.6.6.9.9.1.4.2"><span class="term ">number_of_nics
      </span></dt><dd><p>
         Normally it is not possible to apply different network modes to
         different roles—you can only specify one mode in the global
         attributes section. However, it does not make sense to apply a network
         mode that bonds three interfaces on a machine with only two physical
         network interfaces. This option enables you to create modes for nodes
         with a given number of interfaces.
        </p></dd><dt id="id-1.3.4.6.6.9.9.1.4.3"><span class="term ">node_role
      </span></dt><dd><p>
         This part of the pattern lets you create matches for a certain node
         role. This enables you to create network modes for certain roles, for
         example the Compute Nodes (role: <span class="guimenu ">nova-compute</span>) or the
         Swift nodes (role: <span class="guimenu ">swift-storage</span>). See
         <a class="xref" href="#ex-conduits-role" title="Network Modes for Certain Roles">Example 7.3, “Network Modes for Certain Roles”</a> for the full list of roles.
        </p></dd></dl></div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#conduit-name"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
      The logical network interface definition. Each conduit list must contain
      at least one such definition. This line defines the name of the logical
      interface. This identifier must be unique and will also be referenced in
      the network definition section. We recommend sticking with the
      pre-defined naming scheme: <code class="literal">intf0</code> for <span class="quote">“<span class="quote ">Interface
      0</span>”</span>, <code class="literal">intf1</code> for <span class="quote">“<span class="quote ">Interface 1</span>”</span>, etc.
      If you change the name (not recommended), you also need to change all
      references in the network definition section.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#conduit-interface"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
      This line maps one or more <span class="emphasis"><em>physical</em></span> interfaces to
      the logical interface. Each entry represents a physical interface. If
      more than one entry exists, the interfaces are bonded—either with
      the mode defined in the <span class="guimenu ">team_mode</span> attribute of this
      conduit section. Or, if that is not present, by the globally defined
      <span class="guimenu ">teaming</span> attribute.
     </p><p>
      The physical interfaces definition needs to fit the following pattern:
     </p><div class="verbatim-wrap"><pre class="screen">[Quantifier][Speed][Order]</pre></div><p>
      Valid examples are <code class="literal">+1g2</code>, <code class="literal">10g1</code> or
      <code class="literal">?1g2</code>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.6.6.9.9.3.5.1"><span class="term ">Quantifier</span></dt><dd><p>
         Specifying the quantifier is optional. The following values may be
         entered:
        </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">+</code>: at least the speed specified afterwards
        (specified value or higher)</td></tr><tr><td><code class="literal">-</code>: at most the speed specified afterwards
        (specified value or lower)</td></tr><tr><td><code class="literal">?</code>: any speed (speed specified afterwards is
        ignored)</td></tr></table><p>
         If no quantifier is specified, the exact speed specified is used.
        </p></dd><dt id="id-1.3.4.6.6.9.9.3.5.2"><span class="term ">Speed</span></dt><dd><p>
         Specifying the interface speed is mandatory (even if using the
         <code class="literal">?</code> quantifier). The following values may be entered:
        </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">10m</code>: 10 Mbit</td></tr><tr><td><code class="literal">100m</code>: 100 Mbit</td></tr><tr><td><code class="literal">1g</code>: 1 Gbit</td></tr><tr><td><code class="literal">10g</code>: 10 Gbit</td></tr><tr><td><code class="literal">20g</code>: 20 Gbit</td></tr><tr><td><code class="literal">40g</code>: 40 Gbit</td></tr><tr><td><code class="literal">56g</code>: 56 Gbit</td></tr></table></dd><dt id="id-1.3.4.6.6.9.9.3.5.3"><span class="term ">Order</span></dt><dd><p>
         Position in the interface order. Specifying this value is mandatory.
         The interface order is defined by the order in which the interfaces
         appear in <code class="filename">/sys/class/net</code> (default) or, if it
         exists, by an interface map. The order is also linked to the speed in
         this context:
        </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">1g1</code>: the first 1Gbit interface</td></tr><tr><td><code class="literal">+1g1</code>: the first 1Gbit or 10Gbit
        interface. Crowbar will take the first 1Gbit interface. Only if such an
        interface does not exist, it will take the first 10Gbit interface
        available.</td></tr><tr><td><code class="literal">?1g3</code>: the third 1Gbit, 10Gbit, 100Mbit or
        10Mbit interface. Crowbar will take the third 1Gbit interface. Only if
        such an interface does not exist, it will take the third 10Gbit
        interface, then the third 100Mbit or 10Mbit interface.</td></tr></table><div id="id-1.3.4.6.6.9.9.3.5.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Ordering Numbers</h6><p>
          Ordering numbers start with <code class="literal">1</code> rather than with
          <code class="literal">0</code>.
         </p><p>
          Each interfaces that supports multiple speeds is referenced by
          multiple names—one for each speed it supports. A 10Gbit
          interface is therefore represented by four names:
          <code class="literal">10g<em class="replaceable ">X</em></code>,
          <code class="literal">1g<em class="replaceable ">X</em></code>,
          <code class="literal">100m<em class="replaceable ">X</em></code>,
          <code class="literal">10m<em class="replaceable ">X</em></code>, where
          <em class="replaceable ">X</em> is the ordering number.
         </p><p>
          Ordering numbers always start with <code class="literal">1</code> and are
          assigned ascending for each speed, for example
          <code class="literal">1g1</code>, <code class="literal">1g2</code>, and
          <code class="literal">1g3</code>. Numbering starts with the first physical
          interface. On systems with network interfaces supporting different
          maximum speeds, ordering numbers for the individual speeds differ, as
          the following example shows:
         </p><table border="0" summary="Simple list" class="simplelist "><tr><td>
          100Mbit (first interface): <code class="literal">100m1</code>,
          <code class="literal">10m1</code>
          </td></tr><tr><td>
          1Gbit (second interface): <code class="literal">1g1</code>,
          <code class="literal">100m2</code>, <code class="literal">10m2</code>
          </td></tr><tr><td>
          10Gbit (third interface): <code class="literal">10g1</code>,
          <code class="literal">1g2</code>, <code class="literal">100m3</code>,
          <code class="literal">10m3</code>
          </td></tr></table><p>
          In this example the pattern <code class="literal">?1g3</code> would match
          <code class="literal">100m3</code>, since no third 1Gbit or 10Gbit interface
          exist.
         </p></div></dd></dl></div></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#conduit-bonding"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
      The bonding mode to be used for this logical interface. Overwrites the
      default set in the global attributes section <span class="emphasis"><em>for this
      interface</em></span>. See
      <a class="link" href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" target="_blank">https://www.kernel.org/doc/Documentation/networking/bonding.txt</a>
      for a list of available modes. Specifying this option is
      optional—if not specified here, the global setting applies.
     </p></td></tr></table></div></div><div class="sect2 " id="sec-deploy-network-json-conduits-examples"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Conduit Examples</span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-conduits-examples">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-deploy-network-json-conduits-examples</li></ul></div></div></div></div><div class="complex-example"><div class="example" id="ex-conduits-nic-number"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 7.2: </span><span class="name">Network Modes for Different NIC Numbers </span><a title="Permalink" class="permalink" href="#ex-conduits-nic-number">#</a></h6></div><div class="example-contents"><p>
     The following example defines a team network mode for nodes with 6, 3, and
     an arbitrary number of network interfaces. Since the first mode that
     matches is applied, it is important that the specific modes (for 6 and 3
     NICs) are listed before the general mode:
    </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
           {
              "pattern" : "single/6/.*",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "single/3/.*",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "single/.*/.*",
              "conduit_list" : {
                ...
              }
            },
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-conduits-role"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 7.3: </span><span class="name">Network Modes for Certain Roles </span><a title="Permalink" class="permalink" href="#ex-conduits-role">#</a></h6></div><div class="example-contents"><p>
     The following example defines network modes for Compute Nodes with four
     physical interfaces, the Administration Server (role <code class="literal">crowbar</code>), the
     Control Node, and a general mode applying to all other nodes.
    </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "team",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
          {
            "pattern" : "team/4/nova-compute",
            "conduit_list" : {
              ...
            }
            },
            {
              "pattern" : "team/.*/^crowbar$",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "team/.*/nova-controller",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "team/.*/.*",
              "conduit_list" : {
                ...
              }
            },
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><p>
     The following values for <code class="literal">node_role</code> can be used:
    </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">ceilometer-polling</code>
     </td></tr><tr><td><code class="literal">ceilometer-server</code>
     </td></tr><tr><td><code class="literal">cinder-controller</code>
     </td></tr><tr><td><code class="literal">cinder-volume</code>
     </td></tr><tr><td><code class="literal">crowbar</code>
     </td></tr><tr><td><code class="literal">database-server</code>
     </td></tr><tr><td><code class="literal">glance-server</code>
     </td></tr><tr><td><code class="literal">heat-server</code>
     </td></tr><tr><td><code class="literal">horizon-server</code>
     </td></tr><tr><td><code class="literal">keystone-server</code>
     </td></tr><tr><td><code class="literal">manila-server</code>
     </td></tr><tr><td><code class="literal">manila-share</code>
     </td></tr><tr><td><code class="literal">monasca-agent</code>
     </td></tr><tr><td><code class="literal">monasca-log-agent</code>
     </td></tr><tr><td><code class="literal">monasca-master</code>
     </td></tr><tr><td><code class="literal">monasca-server</code>
     </td></tr><tr><td><code class="literal">neutron-network</code>
     </td></tr><tr><td><code class="literal">neutron-server</code>
     </td></tr><tr><td><code class="literal">nova-controller</code>
     </td></tr><tr><td><code class="literal">nova-compute-*</code>
     </td></tr><tr><td><code class="literal">rabbitmq-server</code>
     </td></tr><tr><td><code class="literal">trove-server</code>
     </td></tr><tr><td><code class="literal">swift-dispersion</code>
     </td></tr><tr><td><code class="literal">swift-proxy</code>
     </td></tr><tr><td><code class="literal">swift-ring-compute</code>
     </td></tr><tr><td><code class="literal">swift-storage</code>
     </td></tr></table><p>
     The role <code class="literal">crowbar</code> refers to the Administration Server.
    </p></div></div></div><div id="id-1.3.4.6.6.10.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: The <code class="literal">crowbar</code> and Pattern Matching</h6><p>
     As explained in <a class="xref" href="#ex-conduits-machine" title="Network Modes for Certain Machines">Example 7.4, “Network Modes for Certain Machines”</a>, each node has an
     additional, unique role named <code class="literal">crowbar-<em class="replaceable ">FULLY
     QUALIFIED HOSTNAME</em></code>.
    </p><p>
     All three elements of the value of the <code class="literal">pattern</code> line
     are read as regular expressions. Therefore using the pattern
     <code class="literal"><em class="replaceable ">mode-name</em>/.*/crowbar</code> will
     match all nodes in your installation. <code class="literal">crowbar</code> is
     considered a substring and therefore will also match all strings
     <code class="literal">crowbar-<em class="replaceable ">FULLY QUALIFIED
     HOSTNAME</em></code>. As a consequence, all subsequent map
     definitions will be ignored. To make sure this does not happen, you must
     use the proper regular expression <code class="literal">^crowbar$</code>:
     <code class="literal"><em class="replaceable ">mode-name</em>/.*/^crowbar$</code>.
    </p></div><div class="complex-example"><div class="example" id="ex-conduits-machine"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 7.4: </span><span class="name">Network Modes for Certain Machines </span><a title="Permalink" class="permalink" href="#ex-conduits-machine">#</a></h6></div><div class="example-contents"><p>
     Apart from the roles listed under <a class="xref" href="#ex-conduits-role" title="Network Modes for Certain Roles">Example 7.3, “Network Modes for Certain Roles”</a>, each
     node in SUSE <span class="productname">OpenStack</span> Cloud has a unique role, which lets you create modes matching
     exactly one node. Each node can be addressed by its unique role name in
     the <code class="literal">pattern</code> entry of the
     <code class="literal">conduit_map</code>.
    </p><p>
     The role name depends on the fully qualified host name (FQHN) of the
     respective machine. The role is named after the scheme
     <code class="literal">crowbar-<em class="replaceable ">FULLY QUALIFIED
     HOSTNAME</em></code> where colons are replaced with dashes,
     and periods are replaced with underscores. The FQHN depends on whether the
     respective node was booted via PXE or not.
    </p><p>
     To determine the host name of a node, log in to the Crowbar Web interface and got to
     <span class="guimenu ">Nodes</span> › <span class="guimenu ">Dashboard</span>. Click the respective node name to get detailed data for the
     node. The FQHN is listed first under <span class="guimenu ">Full Name</span>.
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.6.6.10.6.5.1"><span class="term ">Role Names for Nodes Booted via PXE</span></dt><dd><p>
        The <em class="replaceable ">FULLY QUALIFIED HOSTNAME</em> for nodes
        booted via PXE is composed of the following: a prefix 'd', the MAC
        address of the network interface used to boot the node via PXE, and the
        domain name as configured on the Administration Server. A machine with the fully
        qualified host name
        <code class="literal">d1a-12-05-1e-35-49.cloud.example.com</code> would get the
        following role name:
       </p><div class="verbatim-wrap"><pre class="screen">crowbar-d1a-12-05-1e-35-49_cloud_example_com</pre></div></dd><dt id="id-1.3.4.6.6.10.6.5.2"><span class="term ">Role Names for the Administration Server and Nodes Added Manually</span></dt><dd><p>
        The fully qualified hostnames of the Administration Server and all nodes added
        manually (as described in
        <a class="xref" href="#sec-depl-inst-nodes-install-external" title="11.3. Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE OpenStack Cloud Nodes">Section 11.3, “Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes”</a>) are
        defined by the system administrator. They typically have the form
        hostname+domain, for example
        <em class="replaceable ">admin.cloud.example.com</em>, which would
        result in the following role name:
       </p><div class="verbatim-wrap"><pre class="screen">crowbar-admin_cloud_example_com</pre></div></dd></dl></div><p>
     Network mode definitions for certain machines must be listed first in the
     conduit map. This prevents other, general rules which would also map from
     being applied.
    </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "dual",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
          {
            "pattern" : "dual/.*/crowbar-d1a-12-05-1e-35-49_cloud_example_com",
            "conduit_list" : {
               ...
            }
          },
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div></div></div></div></div><div class="sect2 " id="sec-deploy-network-json-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Definitions</span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-networks">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-deploy-network-json-networks</li></ul></div></div></div></div><p>
    The network definitions contain IP address assignments, the bridge and VLAN
    setup, and settings for the router preference. Each network is also
    assigned to a logical interface defined in the network conduit section. In
    the following the network definition is explained using the example of the
    admin network definition:
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
             ...
         ],
         "networks" : {
           "admin" : {
             "conduit" : "intf0"<span id="network-conduit"></span><span class="callout">1</span>,
             "add_bridge" : false<span id="network-bridge"></span><span class="callout">2</span>,
             "use_vlan" : false<span id="network-vlan"></span><span class="callout">3</span>,
             "vlan" : 100<span id="network-vlanid"></span><span class="callout">4</span>,
             "router_pref" : 10<span id="network-routerpref"></span><span class="callout">5</span>,
             "subnet" : "192.168.124.0"<span id="network-addresses"></span><span class="callout">6</span>,
             "netmask" : "255.255.255.0",
             "router" : "192.168.124.1",
             "broadcast" : "192.168.124.255",
             "ranges" : {
               "admin" : {
                 "start" : "192.168.124.10",
                 "end" : "192.168.124.11"
               },
               "switch" : {
                 "start" : "192.168.124.241",
                 "end" : "192.168.124.250"
               },
               "dhcp" : {
                 "start" : "192.168.124.21",
                 "end" : "192.168.124.80"
               },
               "host" : {
                 "start" : "192.168.124.81",
                 "end" : "192.168.124.160"
               }
             }
           },
           "nova_floating": {
             "add_ovs_bridge": false<span id="network-ovs-bridge"></span><span class="callout">7</span>,
             "bridge_name": "br-public"<span id="network-ovs-bridge-name"></span><span class="callout">8</span>,
             ....
           }
         ...
         },
      }
   }
}</pre></div><div class="calloutlist "><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#network-conduit"><span class="callout">1</span></a> </p></td><td valign="top" align="left"><p>
      Logical interface assignment. The interface must be defined in the
      network conduit section and must be part of the active network mode.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#network-bridge"><span class="callout">2</span></a> </p></td><td valign="top" align="left"><p>
      Bridge setup. Do not touch. Should be <code class="literal">false</code> for all
      networks.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#network-vlan"><span class="callout">3</span></a> </p></td><td valign="top" align="left"><p>
      Create a VLAN for this network. Changing this setting is not recommended.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#network-vlanid"><span class="callout">4</span></a> </p></td><td valign="top" align="left"><p>
      ID of the VLAN. Change this to the VLAN ID you intend to use for the
      specific network, if required. This setting can also be changed using the
      YaST Crowbar interface. The VLAN ID for the
      <code class="literal">nova-floating</code> network must always match the ID for the
      <code class="literal">public network</code>.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#network-routerpref"><span class="callout">5</span></a> </p></td><td valign="top" align="left"><p>
      Router preference, used to set the default route. On nodes hosting
      multiple networks the router with the lowest
      <code class="literal">router_pref</code> becomes the default gateway. Changing this
      setting is not recommended.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#network-addresses"><span class="callout">6</span></a> </p></td><td valign="top" align="left"><p>
      Network address assignments. These values can also be changed by using
      the YaST Crowbar interface.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#network-ovs-bridge"><span class="callout">7</span></a> </p></td><td valign="top" align="left"><p>
      Openvswitch virtual switch setup. This attribute is maintained by Crowbar
      on a per-node level and should not be changed manually.
     </p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#network-ovs-bridge-name"><span class="callout">8</span></a> </p></td><td valign="top" align="left"><p>
      Name of the openvswitch virtual switch. This attribute is maintained by
      Crowbar on a per-node level and should not be changed manually.
     </p></td></tr></table></div><div id="id-1.3.4.6.6.11.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: VLAN Settings</h6><p>
     As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>, using a VLAN for the admin network is
     only supported on a native/untagged VLAN. If you need VLAN support for the
     admin network, it must be handled at switch level.
    </p><p>
     When changing the network configuration with YaST or by editing
     <code class="filename">/etc/crowbar/network.json</code>, you can define VLAN
     settings for each network. For the networks <code class="literal">nova-fixed</code>
     and <code class="literal">nova-floating</code>, however, special rules apply:
    </p><p>
     <span class="bold"><strong>nova-fixed</strong></span>: The <span class="guimenu ">USE
     VLAN</span> setting will be ignored. However, VLANs will automatically
     be used if deploying Neutron with VLAN support (using the plugins
     linuxbridge, openvswitch plus VLAN, or cisco plus VLAN). In this case, you
     need to specify a correct <span class="guimenu ">VLAN ID</span> for this network.
    </p><p>
     <span class="bold"><strong>nova-floating</strong></span>: When using a VLAN for
     <code class="literal">nova-floating</code> (which is the default), the <span class="guimenu ">USE
     VLAN</span> and <span class="guimenu ">VLAN ID</span> settings for
     <span class="guimenu ">nova-floating</span> and <span class="guimenu ">public</span> default to
     the same.
    </p><p>
     You have the option of separating public and floating networks with a
     custom configuration. Configure your own separate floating network (not as
     a subnet of the public network), and give the floating network its own
     router. For example, define <code class="literal">nova-floating</code> as part of an
     external network with a custom <code class="literal">bridge-name</code>. When you
     are using different networks and OpenVSwitch is configured, the
     pre-defined <code class="literal">bridge-name</code> won't work.
    </p></div></div><div class="sect2 " id="sec-depl-inst-admserv-post-network-external"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Providing Access to External Networks</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network-external">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-admserv-post-network-external</li></ul></div></div></div></div><p>
    By default, external networks cannot be reached from nodes in the SUSE <span class="productname">OpenStack</span> Cloud.
    To access external services such as a SUSE Manager server, an SMT server, or
    a SAN, you need to make the external network(s) known to SUSE <span class="productname">OpenStack</span> Cloud. Do so by
    adding a network definition for each external network to
    <code class="filename">/etc/crowbar/network.json</code>. Refer to
    <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for setup
    instructions.
   </p><div class="example" id="id-1.3.4.6.6.12.3"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 7.5: </span><span class="name">Example Network Definition for the External Network 192.168.150.0/16 </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.6.12.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">            "external" : {
               "add_bridge" : false,
               "vlan" : <em class="replaceable ">XXX</em>,
               "ranges" : {
                  "host" : {
                     "start" : "192.168.150.1",
                     "end" : "192.168.150.254"
                  }
               },
               "broadcast" : "192.168.150.255",
               "netmask" : "255.255.255.0",
               "conduit" : "intf1",
               "subnet" : "192.168.150.0",
               "use_vlan" : true
            }</pre></div></div></div><p>
    Replace the value <em class="replaceable ">XXX</em> for the VLAN by a value
    not used within the SUSE <span class="productname">OpenStack</span> Cloud network and not used by Neutron. By default,
    the following VLANs are already used:
   </p><div class="table" id="id-1.3.4.6.6.12.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 7.3: </span><span class="name">VLANs used by the SUSE <span class="productname">OpenStack</span> Cloud Default Network Setup </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.6.12.5">#</a></h6></div><div class="table-contents"><table class="table" summary="VLANs used by the SUSE OpenStack Cloud Default Network Setup" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
        <p>
         VLAN ID
        </p>
       </th><th>
        <p>
         Used by
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         100
        </p>
       </td><td>
        <p>
         BMC VLAN (bmc_vlan)
        </p>
       </td></tr><tr><td>
        <p>
         200
        </p>
       </td><td>
        <p>
         Storage Network
        </p>
       </td></tr><tr><td>
        <p>
         300
        </p>
       </td><td>
        <p>
         Public Network (nova-floating, public)
        </p>
       </td></tr><tr><td>
        <p>
         400
        </p>
       </td><td>
        <p>
         Software-defined network (os_sdn)
        </p>
       </td></tr><tr><td>
        <p>
         500
        </p>
       </td><td>
        <p>
         Private Network (nova-fixed)
        </p>
       </td></tr><tr><td>
        <p>
         501 - 2500
        </p>
       </td><td>
        <p>
         Neutron (value of nova-fixed plus 2000)
        </p>
       </td></tr></tbody></table></div></div></div><div class="sect2 " id="sec-depl-inst-admserv-post-network-split"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Split Public and Floating Networks on Different VLANs</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network-split">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-admserv-post-network-split</li></ul></div></div></div></div><p>
    For custom setups, the public and floating networks can be separated.
    Configure your own separate floating network (not as a subnet of the public
    network), and give the floating network its own router. For example, define
    <code class="literal">nova-floating</code> as part of an external network with a
    custom <code class="literal">bridge-name</code>. When you are using different
    networks and OpenVSwitch is configured, the pre-defined
    <code class="literal">bridge-name</code> won't work.
   </p></div><div class="sect2 " id="sec-depl-inst-admserv-post-network-mtu"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adjusting the Maximum Transmission Unit for the Admin and Storage Network</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network-mtu">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-admserv-post-network-mtu</li></ul></div></div></div></div><p>
    If you need to adjust the Maximum Transmission Unit (MTU) for the Admin
    and/or Storage Network, adjust
    <code class="filename">/etc/crowbar/network.json</code> as shown below. You can also
    enable jumbo frames this way by setting the MTU to 9000. The following
    example enables jumbo frames for both, the storage and the admin network by
    setting <code class="literal">"mtu": 9000</code>.
   </p><div class="verbatim-wrap"><pre class="screen">        "admin": {
          "add_bridge": false,
          "broadcast": "192.168.124.255",
          "conduit": "intf0",
          "mtu": 9000,
          "netmask": "255.255.255.0",
          "ranges": {
            "admin": {
              "end": "192.168.124.11",
              "start": "192.168.124.10"
            },
            "dhcp": {
              "end": "192.168.124.80",
              "start": "192.168.124.21"
            },
            "host": {
              "end": "192.168.124.160",
              "start": "192.168.124.81"
            },
            "switch": {
              "end": "192.168.124.250",
              "start": "192.168.124.241"
            }
          },
          "router": "192.168.124.1",
          "router_pref": 10,
          "subnet": "192.168.124.0",
          "use_vlan": false,
          "vlan": 100
        },
        "storage": {
          "add_bridge": false,
          "broadcast": "192.168.125.255",
          "conduit": "intf1",
          "mtu": 9000,
          "netmask": "255.255.255.0",
          "ranges": {
            "host": {
              "end": "192.168.125.239",
              "start": "192.168.125.10"
            }
          },
          "subnet": "192.168.125.0",
          "use_vlan": true,
          "vlan": 200
        },</pre></div><div id="id-1.3.4.6.6.14.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</h6><p>
     After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, you cannot change the network
     setup, and you cannot change the MTU size.
    </p></div></div><div class="sect2 " id="sec-network-json-resolve"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Matching Logical and Physical Interface Names with network-json-resolve</span> <a title="Permalink" class="permalink" href="#sec-network-json-resolve">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_conf_admin_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-network-json-resolve</li></ul></div></div></div></div><p>
    SUSE <span class="productname">OpenStack</span> Cloud includes a new script, <code class="filename">network-json-resolve</code>,
    which matches the physical and logical names of network interfaces, and
    prints them to stdout. Use this to verify that you are using the correct
    interface names in <code class="filename">network.json</code>. Note that it will
    only work if <span class="productname">OpenStack</span> nodes have been deployed. The following command
    prints a help menu:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve -h</code></pre></div><p>
    <code class="filename">network-json-resolve</code> reads your deployed
    <code class="filename">network.json</code> file. To use a different
    <code class="filename">network.json</code> file, specify its full path with the
    <code class="command">--network-json</code> option. The following example shows how
    to use a different <code class="filename">network.json</code> file, and prints the
    interface mappings of a single node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve --network-json /opt/configs/network.json aliases compute1</code>
 eth0: 0g1, 1g1
 eth1: 0g1, 1g1</pre></div><p>
    You may query the mappings of a specific network interface:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve aliases compute1 eth0</code>
 eth0: 0g1, 1g1</pre></div><p>
    Print the bus ID order on a node. This returns <code class="literal">no bus order
    defined for <em class="replaceable ">node</em></code> if you did not
    configure any bus ID mappings:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve bus_order compute1</code></pre></div><p>
    Print the defined conduit map for the node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve conduit_map compute1</code>
bastion: ?1g1
intf0: ?1g1
intf1: ?1g1
intf2: ?1g1</pre></div><p>
    Resolve conduits to the standard interface names:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve conduits compute1</code>
bastion:
intf0: eth0
intf1: eth0
intf2: eth0</pre></div><p>
    Resolve the configured networks on a node to the standard interface names:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve networks compute1</code>
bastion:
bmc_vlan:  eth0
nova_fixed: eth0
nova_floating: eth0
os_sdn: eth0
public: eth0
storage: eth0</pre></div><p>
    Resolve the specified network to the standard interface name(s):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve networks compute1 public</code>
   public: eth0</pre></div><p>
    Resolve a <code class="filename">network.json</code>-style interface to its standard
    interface name(s):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve resolve compute1 1g1</code>
 eth0</pre></div></div></div></div><div class="chapter " id="sec-depl-adm-start-crowbar"><div class="titlepage"><div><div><h2 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-start-crowbar">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar_start.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.4.7.1.3">#</a></h6></div><p>
    The last step in configuring the Administration Server is starting Crowbar.
   </p></div></div></div></div><div class="line"></div><p>
   Before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation to finish the configuration
   of the Administration Server, make sure to double-check the following items.
  </p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Final Check Points </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
     Make sure the network configuration is correct. Run <span class="guimenu ">YaST</span> › <span class="guimenu ">Crowbar</span>
     to review/change the configuration. See
     <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a> for further instructions.
    </p><div id="id-1.3.4.7.3.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: An HA Setup Requires Team Network Mode</h6><p>
      If you are planning to make SUSE <span class="productname">OpenStack</span> Cloud highly available, whether upon the
      initial setup or later, set up the
      network in the team mode. Such a setup requires at least two network
      cards for each node.
     </p></div></li><li class="listitem "><p>
     Make sure <code class="command">hostname</code> <code class="option">-f</code> returns a
     fully qualified host name. See
     <a class="xref" href="#sec-depl-adm-inst-network" title="Chapter 6. Service Configuration: Administration Server Network Configuration">Chapter 6, <em>Service Configuration:  Administration Server Network Configuration</em></a> for further instructions.
    </p></li><li class="listitem "><p>
     Make sure all update and product repositories are available. See
     <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a> for further instructions.
    </p></li><li class="listitem "><p>
     Make sure the operating system and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> are up-to-date and
     have the latest patches installed. Run <code class="command">zypper patch</code>
     to install them.
    </p></li><li class="listitem "><p>
     To use the Web interface for the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation you need network access to
     the Administration Server via a second network interface. As the network will be
     reconfigured during the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, make sure to either have a bastion
     network or an external gateway configured. (For details on bastion
     networks, see <a class="xref" href="#sec-depl-adm-inst-crowbar-mode-bastion" title="7.3.1. Setting Up a Bastion Network">Section 7.3.1, “Setting Up a Bastion Network”</a>.)
    </p></li></ul></div><p>
   Now everything is in place to finally set up Crowbar and install the
   Administration Server. Crowbar requires a MariaDB database—you can either create
   one on the Administration Server or use an existing MariaDB database on a remote
   server.
  </p><div class="procedure " id="id-1.3.4.7.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 8.1: </span><span class="name">Setting up Crowbar with a Local Database </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Start Crowbar:
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start crowbar-init</pre></div></li><li class="step "><p>
     Create a new database on the Administration Server. By default the credentials
     <code class="literal">crowbar</code>/<code class="literal">crowbar</code> are used:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database create</pre></div><p>
     To use a different user name and password, run the following command
     instead:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database create \
--db_username=<em class="replaceable ">USERNAME</em> --db_password=<em class="replaceable ">PASSWORD</em></pre></div><p>
     Run <code class="command">crowbarctl database help create</code> for help and more
     information.
    </p></li></ol></div></div><div class="procedure " id="id-1.3.4.7.6"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 8.2: </span><span class="name">Setting up Crowbar with a Remote MariaDB Database </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.6">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Start Crowbar:
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start crowbar-init</pre></div></li><li class="step "><p>
     Make sure a user account that can be used for the Crowbar database exists
     on the remote MariaDB database. If not, create such an account.
    </p></li><li class="step "><p>
     Test the database connection using the credentials from the previous
     step:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database test --db-username=<em class="replaceable ">USERNAME</em> \
--db-password=<em class="replaceable ">PASSWORD</em> --database=<em class="replaceable ">DBNAME</em> \
--host=<em class="replaceable ">IP_or_FQDN</em> --port=<em class="replaceable ">PORT</em></pre></div><p>
     You need to be able to successfully connect to the database before you
     can proceed. Run <code class="command">crowbarctl database help test</code> for
     help and more information.
    </p></li><li class="step "><p>
     To connect to the database, use the following command:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database connect --db-username=<em class="replaceable ">USERNAME</em> \
--db-password=<em class="replaceable ">PASSWORD</em> --database=<em class="replaceable ">DBNAME</em> \
--host=<em class="replaceable ">IP_or_FQDN</em> --port=<em class="replaceable ">PORT</em></pre></div><p>
     Run <code class="command">crowbarctl database help connect</code> for help and more
     information.
    </p></li></ol></div></div><p>
   After the database is successfully created and you can connect to it, access
   the Web interface from a Web browser, using the following address:
   </p><div class="verbatim-wrap"><pre class="screen">http://<em class="replaceable ">ADDRESS</em></pre></div><p>
   Replace <em class="replaceable ">ADDRESS</em> either with the IP address of the
   second network interface or its associated host name.  Logging in to the
   Web interface requires the credentials you configured with YaST Crowbar (see <a class="xref" href="#sec-depl-adm-inst-crowbar-user" title="7.1. User Settings">Section 7.1, “<span class="guimenu ">User Settings</span>”</a>). If you have not changed the
   defaults, user name and password are both <code class="literal">crowbar</code>. Refer
   to <a class="xref" href="#cha-depl-crowbar" title="Chapter 10. The Crowbar Web Interface">Chapter 10, <em>The Crowbar Web Interface</em></a> for details.
  </p><p>
    The Web interface shows the SUSE <span class="productname">OpenStack</span> Cloud installation wizard. Click <span class="guimenu ">Start
    Installation</span> to begin. The installation progress is shown in the
    Web interface:
   </p><div class="figure" id="id-1.3.4.7.11"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_web_installer_init.png" target="_blank"><img src="images/depl_web_installer_init.png" width="" alt="The SUSE OpenStack Cloud Crowbar installation Web interface" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 8.1: </span><span class="name">The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation Web interface </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.11">#</a></h6></div></div><p>
    If the installation has successfully finished, you will be
    redirected to the Crowbar Dashboard:
   </p><div class="figure" id="id-1.3.4.7.13"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_node_dashboard_initial.png" target="_blank"><img src="images/depl_node_dashboard_initial.png" width="" alt="Crowbar Web Interface: The Dashboard" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 8.2: </span><span class="name">Crowbar Web Interface: The Dashboard </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13">#</a></h6></div></div><p>
    From here you can start allocating nodes and then deploy the <span class="productname">OpenStack</span>
    services. Refer to <a class="xref" href="#part-depl-ostack" title="Part III. Setting Up OpenStack Nodes and Services">Part III, “Setting Up <span class="productname">OpenStack</span> Nodes and Services”</a> for more information.
   </p></div><div class="chapter " id="sec-depl-adm-crowbar-extra-features"><div class="titlepage"><div><div><h2 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing Crowbar</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-crowbar-extra-features">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar_extra-features.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-crowbar-extra-features</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.4.8.2"><span class="number">9.1 </span><span class="name">Skip Unready Nodes</span></a></span></dt><dt><span class="section"><a href="#id-1.3.4.8.3"><span class="number">9.2 </span><span class="name">Skip Unchanged Nodes</span></a></span></dt><dt><span class="section"><a href="#id-1.3.4.8.4"><span class="number">9.3 </span><span class="name">Controlling Chef Restarts Manually</span></a></span></dt><dt><span class="section"><a href="#id-1.3.4.8.5"><span class="number">9.4 </span><span class="name">Prevent Automatic Restart</span></a></span></dt></dl></div></div><div class="sect1" id="id-1.3.4.8.2"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Skip Unready Nodes</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.2">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar_extra-features.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In large deployments with many nodes, there are always some nodes that are
   in a fail or unknown state. New barclamps cannot be applied to them and
   values cannot be updated in some barclamps that are already deployed. This
   happens because Crowbar will refuse to apply a barclamp to a list of nodes if
   they are not all in <code class="literal">ready</code> state.
  </p><p>
   To avoid having to manually take out nodes that are not
   <code class="literal">ready</code>, there is a feature called <code class="literal">skip unready
   nodes</code>. Instead of refusing to apply the barclamp, it will skip the
   nodes that it finds in any other state than <code class="literal">ready</code>.
  </p><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><p>
   In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
   the option <code class="literal">skip_unready_nodes</code> to <code class="literal">true</code>.
  </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
skip_unready_nodes:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div><p>
   <span class="bold"><strong>Roles Affected</strong></span>
  </p><p>
   All Barclamp roles are affected. The default config file includes all the
   roles that have been tested and found to be working. Adding roles to the
   default list is not supported for the <code class="literal">skip_unready_nodes</code>
   feature. Removing default roles is supported.
  </p><p>
   The list of currently supported roles to skip:
  </p><div class="verbatim-wrap"><pre class="screen">- bmc-nat-client
- ceilometer-agent
- deployer-client
- dns-client
- ipmi
- logging-client
- nova-compute-ironic
- nova-compute-kvm
- nova-compute-qemu
- nova-compute-vmware
- nova-compute-xen
- ntp-client
- provisioner-base
- suse-manager-client
- swift-storage
- updater</pre></div><p>
   <span class="bold"><strong>Determining Which Nodes Were Skipped</strong></span>
  </p><p>
   Skipped nodes are logged to the Crowbar log
   (<code class="filename">/var/log/crowbar/production.log</code>) where you can search
   for the text <code class="literal">skipped until next chef run</code>. This will print
   the log lines where nodes were skipped, the name of the node, and the
   barclamp which was being applied.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>grep "skipped until next chef run" /var/log/crowbar/production.log</pre></div><div id="id-1.3.4.8.2.14" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    After enabling/disabling the <code class="literal">skip_unready_nodes</code> feature
    or adding/removing roles, the Crowbar framework service must be restarted
    (<code class="command">systemctl restart crowbar</code>) in order to use the updated
    settings.
   </p></div></div><div class="sect1" id="id-1.3.4.8.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Skip Unchanged Nodes</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.3">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar_extra-features.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When a barclamp is applied, all nodes will run <code class="literal">chef-client</code>. Sometimes it is
   not necessary to run chef for each node as attributes or roles may have not
   changed for some of the nodes.
  </p><p>
   The <code class="literal">skip unchanged nodes</code> feature allows nodes to be
   skipped if their attributes or roles have not changed. This can help speed
   up applying a barclamp, especially in large deployments and with roles that
   are usually applied to a lot of nodes, such as Nova.
  </p><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><p>
   In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
   the option <code class="literal">skip_unchanged_nodes</code> to
   <code class="literal">true</code>.
  </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
skip_unchanged_nodes:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div></div><div class="sect1" id="id-1.3.4.8.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Controlling Chef Restarts Manually</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.4">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar_extra-features.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When a service configuration has changed, Chef forces this service to
   restart. Sometimes it is useful to have manual control of these
   restarts. This feature supports avoiding automatic restart of services and
   including them in a pending restart list.  Disabling restarts is
   enabled/disabled by barclamp and cannot be done on a service level. In other
   words, enabling this feature on the Cinder barclamp will disable
   automatic restarts for all Cinder-* services.
  </p><p>
   Two steps are necessary to activate this feature:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Enable <code class="literal">disallow_restart</code> in the Crowbar configuration file
    </p></li><li class="step "><p>
     Set the <code class="literal">disable_restart</code> flag for a specific barclamp
     using <code class="literal">crowbar-client</code> or API
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
     the option <code class="literal">disallow_restart</code> to <code class="literal">true</code>.
    </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
disallow_restart:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div></li><li class="step "><p>
     The <code class="literal">disable_restart</code> flag can be set with the Crowbar
     client or with the API.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Set Flag with Crowbar Client
      </p><p>
       The crowbar client options for this feature are accessed with the
       <code class="command">crowbarctl services</code> command, and only work for
       OpenStack services. The options are:
      </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.8.4.6.2.2.1.3.1"><span class="term ">disable_restart</span></dt><dd><p>
          The parameters are the barclamp name and the flag value
          (<code class="literal">true</code> to disable automatic restart and
          <code class="literal">false</code> to enable automatic restart). The command is
          <code class="literal">crowbarctl services disable_restart
          <em class="replaceable ">BARCLAMP</em> &lt;true|false&gt;</code>. For
          example, to disable restart of the Keystone barclamp, enter the
          command <code class="command">crowbarctl services disable_restart keystone
          true</code>.
         </p></dd><dt id="id-1.3.4.8.4.6.2.2.1.3.2"><span class="term ">restart_flags</span></dt><dd><p>
          Used to check the <code class="literal">disable_restart</code> flag for each
          barclamp
         </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services restart_flags
{
  "nova": true,
  "keystone": true,
  "database": true
}</pre></div></dd><dt id="id-1.3.4.8.4.6.2.2.1.3.3"><span class="term ">list_restarts</span></dt><dd><p>
          Displays the list of pending restarts. The <code class="literal">pending
          restart</code> flag indicates that a service tried to
          restart due to the Chef run but it did not due to the automatic
          restart being disabled. It also indicates that the service might have
          a new configuration and it will not be applied until it is manually
          restarted.
         </p><p>
          In the following example, the <code class="literal">pacemaker_service</code>
          attribute indicates whether this service is managed by Pacemaker
          (usually in an HA environment) or it is a standalone service managed
          by <code class="literal">systemd</code> (usually in non-HA environments). More
          on Pacemaker at <a class="xref" href="#sec-depl-ostack-pacemaker" title="12.2. Deploying Pacemaker (Optional, HA Setup Only)">Section 12.2, “Deploying Pacemaker (Optional, HA Setup Only)”</a>. The
          service to restart will be <code class="literal">apache2</code> not managed by
          Pacemaker.
         </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services list_restarts
{
  "<em class="replaceable ">NODE_IP</em>": {
    "alias": "controller1",
    "keystone": {
      "apache2": {
        "pacemaker_service": false,
        "timestamp": "2017-11-22 11:17:49 UTC"
      }
    }
  }
}</pre></div></dd><dt id="id-1.3.4.8.4.6.2.2.1.3.4"><span class="term ">clear_restart</span></dt><dd><p>
          Removes the flag on a specific node for a specific service. It can
          be executed when the service has restarted manually. The command is
          <code class="literal">crowbarctl services clear_restart
          <em class="replaceable ">NODE</em>
          <em class="replaceable ">SERVICE</em></code>. For example, <code class="command">crowbarctl
          services clear_restart <em class="replaceable ">NODE_IP</em>
          apache2</code>
         </p></dd></dl></div></li><li class="listitem "><p>
       Using the API to List, Get Status, Set and Clear Flags
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         /restart_management/configuration
        </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.8.4.6.2.2.2.2.1.2.1"><span class="term ">GET</span></dt><dd><p>
           Lists the barclamps and the status of service reboots disallowed
          </p></dd><dt id="id-1.3.4.8.4.6.2.2.2.2.1.2.2"><span class="term ">POST (parameters: disallow_restarts, barclamp)</span></dt><dd><p>
            Sets the disallow_restart flag for a barclamp
           </p></dd></dl></div></li><li class="listitem "><p>
         /restart_management/restarts
        </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.8.4.6.2.2.2.2.2.2.1"><span class="term ">GET</span></dt><dd><p>
           Lists all of the services needing restarts
          </p></dd><dt id="id-1.3.4.8.4.6.2.2.2.2.2.2.2"><span class="term ">POST (parameters: node, service)</span></dt><dd><p>
            Clears the restart flag for the given service in the given node
           </p></dd></dl></div></li></ul></div></li></ul></div></li></ol></div></div></div><div class="sect1" id="id-1.3.4.8.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prevent Automatic Restart</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.5">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar_extra-features.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Sometimes a change in a proposal requires services to be restarted on all
   implicated nodes. This could be a problem in a production environment where
   interrupting a service completely is not an option, and where manual restart
   control is needed. With the service <code class="literal">disable_restart</code>
   feature in <code class="literal">crowbarctl</code>, you can set a flag for certain
   proposals to prevent the automatic restart.
  </p><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
     the option <code class="literal">disallow_restart</code> to <code class="literal">true</code>.
    </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
disallow_restart:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div></li><li class="step "><p>
     Restart Crowbar
    </p></li><li class="step "><p>
     Enable the <code class="literal">disable_restart</code> flag in the affected
     cookbook.
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services disable_restart <em class="replaceable ">COOKBOOK</em> true</pre></div><p>
     For example, to disable Keystone and RabbitMQ restarts:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services disable_restart keystone true
crowbarctl services disable_restart rabbitmq true</pre></div></li><li class="step "><p>
     To check the proposals that have <code class="literal">disable_restart</code>
     enabled:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services restart_flags
{
  "rabbitmq": true,
  "keystone": true,
}</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Check Pending Restarts</strong></span>
  </p><p>
   When a proposal is applied with the <code class="literal">disable_restart</code> flag
   enabled, the implicated nodes will not restart. They will be listed as
   pending restarts. To check this list, run the command <code class="command">crowbarctl
   services list_restarts</code>.
  </p><p>
   In the following example, <code class="literal">disable_restart</code> is enabled for
   RabbitMQ. After applying rabbitmq, <code class="literal">list_restarts</code> will
   show the affected nodes, the proposal, and the services to restart.
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services list_restarts
{
   "d52-54-77-77-01-01.vo5.cloud.suse.de": {
    "alias": "controller1",
    "rabbitmq": {
      "rabbitmq-server": {
        "pacemaker_service": false,
        "timestamp": "2018-03-07 15:30:30 UTC"
      }
    }
  }
}</pre></div><p>
   After a manual restart of the service, the flags should be cleaned. The
   following command will clean the flag for a specific node, for a specific
   proposal or all proposals, and for a specific service.
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services clear_restart <em class="replaceable ">NODE [COOKBOOK [SERVICE]]</em></pre></div><p>
   For example, to clean the RabbitMQ flag from the
   <code class="literal">controller1</code> node, run the command:
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl service clear_restart controller1 rabbitmq</pre></div><p>
   To clean the <code class="literal">controller1</code> node, run the command:
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl service clear_restart controller1</pre></div></div></div></div><div class="part" id="part-depl-ostack"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part III </span><span class="name">Setting Up <span class="productname">OpenStack</span> Nodes and Services </span><a title="Permalink" class="permalink" href="#part-depl-ostack">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-depl-crowbar"><span class="number">10 </span><span class="name">The Crowbar Web Interface</span></a></span></dt><dd class="toc-abstract"><p>The Crowbar Web interface runs on the Administration Server. It provides an
    overview of the most important deployment details in your cloud. This includes a
    view of the nodes and which roles are deployed on which nodes, and the
    barclamp proposals that can be edited and deployed. In addition, the
    Crowbar Web interface shows details about the networks and switches in your
    cloud. It also provides graphical access to tools for managing
    your repositories, backing up or restoring the Administration Server, exporting the
    Chef configuration, or generating a <code class="literal">supportconfig</code> TAR
    archive with the most important log files.</p></dd><dt><span class="chapter"><a href="#cha-depl-inst-nodes"><span class="number">11 </span><span class="name">Installing the <span class="productname">OpenStack</span> Nodes</span></a></span></dt><dd class="toc-abstract"><p>
  The <span class="productname">OpenStack</span> nodes represent the actual cloud infrastructure. Node
  installation and service deployment is done automatically from the
  Administration Server. Before deploying the <span class="productname">OpenStack</span> services, SUSE Linux Enterprise Server 12 SP3 will be installed on all Control Nodes and Storage Nodes.
 </p></dd><dt><span class="chapter"><a href="#cha-depl-ostack"><span class="number">12 </span><span class="name">Deploying the <span class="productname">OpenStack</span> Services</span></a></span></dt><dd class="toc-abstract"><p>After the nodes are installed and configured you can start deploying the OpenStack components to finalize the installation. The components need to be deployed in a given order, because they depend on one another. The Pacemaker component for an HA setup is the only exception from this rule—it can be …</p></dd><dt><span class="chapter"><a href="#sec-deploy-policy-json"><span class="number">13 </span><span class="name">Limiting Users' Access Rights</span></a></span></dt><dd class="toc-abstract"><p>To limit users' access rights (or to define more fine-grained access rights), you can use Role Based Access Control (RBAC, only available with Keystone v3). In the example below, we will create a new role (ProjectAdmin). It allows users with this role to add and remove other users to the Member role…</p></dd><dt><span class="chapter"><a href="#cha-depl-ostack-configs"><span class="number">14 </span><span class="name">Configuration Files for <span class="productname">OpenStack</span> Services</span></a></span></dt><dd class="toc-abstract"><p>
    Typically, each <span class="productname">OpenStack</span> component comes with a configuration file, for
    example: <code class="filename">/etc/nova/nova.conf</code>.
   </p><p>
    These configuration files can still be used. However, to configure an
    <span class="productname">OpenStack</span> component and its different components and roles, it is now
    preferred to add custom configuration file snippets to a
    <code class="filename"><em class="replaceable ">SERVICE</em>.conf.d/</code> directory
    instead.

   </p></dd><dt><span class="chapter"><a href="#install-heat-templates"><span class="number">15 </span><span class="name">Installing SUSE CaaS Platform Heat Templates</span></a></span></dt><dd class="toc-abstract"><p>
  This chapter describes how to install SUSE CaaS Platform Heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
 </p></dd></dl></div><div class="chapter " id="cha-depl-crowbar"><div class="titlepage"><div><div><h2 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Crowbar Web Interface</span> <a title="Permalink" class="permalink" href="#cha-depl-crowbar">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>cha-depl-crowbar</li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.5.2.2.2">#</a></h6></div><p>The Crowbar Web interface runs on the Administration Server. It provides an
    overview of the most important deployment details in your cloud. This includes a
    view of the nodes and which roles are deployed on which nodes, and the
    barclamp proposals that can be edited and deployed. In addition, the
    Crowbar Web interface shows details about the networks and switches in your
    cloud. It also provides graphical access to tools for managing
    your repositories, backing up or restoring the Administration Server, exporting the
    Chef configuration, or generating a <code class="literal">supportconfig</code> TAR
    archive with the most important log files.</p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-crow-login"><span class="number">10.1 </span><span class="name">Logging In</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-crow-overview"><span class="number">10.2 </span><span class="name">Overview: Main Elements</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-barclamps"><span class="number">10.3 </span><span class="name">Deploying Barclamp Proposals</span></a></span></dt></dl></div></div><div id="tip-crow-api" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Crowbar API Documentation</h6><p>
   You can access the Crowbar API documentation from the following static page:
   <code class="literal">http://<em class="replaceable ">CROWBAR_SERVER</em>/apidoc</code>.
  </p><p>
    The documentation contains information about the crowbar API endpoints and
    its parameters, including response examples, possible errors (and their
    HTTP response codes), parameter validations, and required headers.
   </p></div><div class="sect1 " id="sec-depl-crow-login"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging In</span> <a title="Permalink" class="permalink" href="#sec-depl-crow-login">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-crow-login</li></ul></div></div></div></div><p> The Crowbar Web interface uses the HTTP protocol and port
   <code class="literal">80</code>. </p><div class="procedure " id="pro-depl-crow-login"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.1: </span><span class="name">Logging In to the Crowbar Web Interface </span><a title="Permalink" class="permalink" href="#pro-depl-crow-login">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p> On any machine, start a Web browser and make sure that JavaScript and
     cookies are enabled.</p></li><li class="step "><p> As URL, enter the IP address of the Administration Server, for example:</p><div class="verbatim-wrap"><pre class="screen">http://192.168.124.10/</pre></div></li><li class="step "><p>Log in as user
     <code class="systemitem">crowbar</code>. If you have not changed
     the password, it is <code class="literal">crowbar</code> by default.
    </p></li></ol></div></div><div class="procedure " id="pro-depl-crow-password"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.2: </span><span class="name">Changing the Password for the Crowbar Web Interface </span><a title="Permalink" class="permalink" href="#pro-depl-crow-password">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>After logging in to the Crowbar Web interface, select <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span>.</p></li><li class="step "><p>Select the <code class="literal">Crowbar</code> barclamp entry and
    <span class="guimenu ">Edit</span> the proposal.</p></li><li class="step "><p>In the <span class="guimenu ">Attributes</span> section, click
      <span class="guimenu ">Raw</span> to edit the configuration file.</p></li><li class="step "><p>Search for the following entry:</p><div class="verbatim-wrap"><pre class="screen">"crowbar": {
     "password": "crowbar"</pre></div></li><li class="step "><p>Change the password.
      </p></li><li class="step "><p>Confirm your change by clicking <span class="guimenu ">Save</span> and
     <span class="guimenu ">Apply</span>.</p></li></ol></div></div></div><div class="sect1 " id="sec-depl-crow-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview: Main Elements</span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-crow-overview</li></ul></div></div></div></div><p>After logging in to Crowbar, you will see a navigation bar at the
   top-level row. Its menus and the respective views are described in the
   following sections.</p><div class="figure" id="id-1.3.5.2.5.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_node_dashboard_groups_installed.png" target="_blank"><img src="images/depl_node_dashboard_groups_installed.png" width="" alt="Crowbar UI—Dashboard (Main Screen)" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 10.1: </span><span class="name">Crowbar UI—Dashboard (Main Screen) </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.5.3">#</a></h6></div></div><div class="sect2 " id="sec-depl-crow-overview-nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-nodes">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-crow-overview-nodes</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.2.5.4.2.1"><span class="term "><span class="guimenu ">Dashboard</span></span></dt><dd><p>This is the default view after logging in to the Crowbar
       Web interface. The Dashboard shows the groups (which you can create to
       arrange nodes according to their purpose), which nodes belong to each
       group, and which state the nodes and groups are in. In addition, the
       total number of nodes is displayed in the top-level row.</p><p>The color of the dot in front of each node or group indicates the status. If the dot for
       a group shows more than one color, hover the mouse pointer over the dot to view the total
       number of nodes and the statuses they are in.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Gray means the node is being discovered by the Administration Server, or that there is no
         up-to-date information about a deployed node. If the status is shown for a node longer than
         expected, check if the chef-client is still running on the node.</p></li><li class="listitem "><p>
         Yellow means the node has been successfully
         <code class="literal">Discovered</code>. As long as the node has not been
         allocated the dot will flash. A solid (non-flashing) yellow dot
         indicates that the node has been allocated, but installation has not
         yet started.
        </p></li><li class="listitem "><p>
         Flashing from yellow to green means the node has been allocated and is
         currently being installed.
        </p></li><li class="listitem "><p>Solid green means the node is in status <code class="literal">Ready</code>.
        </p></li><li class="listitem "><p>Red means the node is in status <code class="literal">Problem</code>.</p></li></ul></div><p>During the initial state of the setup, the Dashboard only shows
       one group called <code class="literal">sw_unknown</code> into which the
       Administration Server is automatically sorted. Initially, all nodes (except
       the Administration Server) are listed with their MAC address as a name.
       However, we recommend creating an alias for each node. This makes
       it easier to identify the node in the admin network and on the
       Dashboard. For details on how to create groups, how to assign nodes
       to a group, and how to create node aliases, see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a>.</p></dd><dt id="id-1.3.5.2.5.4.2.2"><span class="term "><span class="guimenu ">Bulk Edit</span></span></dt><dd><p>This screen allows you to edit multiple nodes at once instead of
       editing them individually. It lists all nodes, including
        <span class="guimenu ">Name</span> (in form of the MAC address),
<span class="guimenu ">Hardware</span> configuration, <span class="guimenu ">Alias</span> (used within the admin network),
        <span class="guimenu ">Public Name</span> (name used outside of the SUSE <span class="productname">OpenStack</span> Cloud
       network), <span class="guimenu ">Group</span>, <span class="guimenu ">Intended
        Role</span>, <span class="guimenu ">Platform</span> (the operating
       system that is going to be installed on the node),
        <span class="guimenu ">License</span> (if available), and allocation
       status. You can toggle the list view between <span class="guimenu ">Show
        unallocated</span> or <span class="guimenu ">Show all</span> nodes.</p><p>For details on how to fill in the data for all nodes and how to
       start the installation process, see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a>.</p></dd><dt id="id-1.3.5.2.5.4.2.3"><span class="term "><span class="guimenu ">HA Clusters</span></span></dt><dd><p>This menu entry only appears if your cloud contains a High Availability setup. The overview
       shows all clusters in your setup, including the <span class="guimenu ">Nodes</span> that are members of
       the respective cluster and the <span class="guimenu ">Roles</span> assigned to the cluster. It also
       shows if a cluster contains <span class="guimenu ">Remote Nodes</span> and which roles are assigned to
       the remote nodes.</p></dd><dt id="id-1.3.5.2.5.4.2.4"><span class="term "><span class="guimenu ">Actives Roles</span></span></dt><dd><p>This overview shows which roles have been deployed on which
       node(s). The roles are grouped according to the service to which they
       belong. You cannot edit anything here. To change role deployment, you
       need to edit and redeploy the appropriate barclamps as described in
        <a class="xref" href="#cha-depl-ostack" title="Chapter 12. Deploying the OpenStack Services">Chapter 12, <em>Deploying the <span class="productname">OpenStack</span> Services</em></a>.</p></dd></dl></div></div><div class="sect2 " id="sec-depl-crow-overview-barclamps"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Barclamps</span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-barclamps">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-crow-overview-barclamps</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.2.5.5.2.1"><span class="term "><span class="guimenu ">All Barclamps</span></span></dt><dd><p>This screen shows a list of all available barclamp proposals, including
       their <span class="guimenu ">Status</span>, <span class="guimenu ">Name</span>, and a short
       <span class="guimenu ">Description</span>. From here, you can
       <span class="guimenu ">Edit</span> individual barclamp
       proposals as described in <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
      </p></dd><dt id="id-1.3.5.2.5.5.2.2"><span class="term "><span class="guimenu ">Crowbar</span></span></dt><dd><p>This screen only shows the barclamps that are included with the core
       Crowbar framework. They contain general recipes for setting up and
       configuring all nodes. From here, you can <span class="guimenu ">Edit</span>
       individual barclamp proposals.</p></dd><dt id="id-1.3.5.2.5.5.2.3"><span class="term "><span class="guimenu "><span class="productname">OpenStack</span></span></span></dt><dd><p>This screen only shows the barclamps that are dedicated to <span class="productname">OpenStack</span>
       service deployment and configuration.  From here, you can
       <span class="guimenu ">Edit</span> individual barclamp
       proposals. </p></dd><dt id="id-1.3.5.2.5.5.2.4"><span class="term "><span class="guimenu ">Deployment Queue</span></span></dt><dd><p>If barclamps are applied to one or more nodes that are not yet
       available for deployment (for example, because they are rebooting or have
       not been fully installed yet), the proposals will be put in a queue. This
       screen shows the proposals that are <span class="guimenu ">Currently deploying</span>
       or <span class="guimenu ">Waiting in queue</span>.</p></dd></dl></div></div><div class="sect2 " id="sec-depl-crow-overview-utilities"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Utilities</span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-utilities">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-crow-overview-utilities</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.2.5.6.2.1"><span class="term "><span class="guimenu ">Exported Items</span></span></dt><dd><p>The <span class="guimenu ">Exported Files</span> screen allows you to export the
       Chef configuration and the <code class="literal">supportconfig</code> TAR
       archive. The <code class="literal">supportconfig</code> archive contains system
        information such as the current kernel version being used, the hardware, RPM
        database, partitions, and the most important log files for analysis of any
        problems. To access the export options, click <span class="guimenu ">New
        Export</span>. After the export has been successfully finished, the
        <span class="guimenu ">Exported Files</span> screen will show any files that are
       available for download.</p></dd><dt id="id-1.3.5.2.5.6.2.2"><span class="term "><span class="guimenu ">Repositories</span></span></dt><dd><p>This screen shows an overview of the mandatory, recommended, and
       optional repositories for all architectures of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. On each
       reload of the screen the Crowbar Web interface checks the availability and
       status of the repositories. If a mandatory repository is not present, it
       is marked red in the screen. Any repositories marked green are usable and
       available to each node in the cloud. Usually, the available repositories
       are also shown as <span class="guimenu ">Active</span> in the rightmost column. This
       means that the managed nodes will automatically be configured to use this
       repository. If you disable the <span class="guimenu ">Active</span> check box for a
       repository, managed nodes will not use that repository.</p><p>You cannot edit any repositories in this screen. If you need additional, third-party
       repositories, or want to modify the repository metadata, edit
        <code class="filename">/etc/crowbar/repos.yml</code>. Find an example of a repository
       definition below:</p><div class="verbatim-wrap"><pre class="screen">suse-12.3:
  x86_64:
    Custom-Repo-12.3:
      url: 'http://example.com/12-SP3:/x86_64/custom-repo/'
      ask_on_error: true # sets the ask_on_error flag in
                         # the autoyast profile for that repo
      priority: 99 # sets the repo priority for zypper</pre></div><p>Alternatively, use the YaST Crowbar module to add or edit repositories as described in <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a>.</p></dd><dt id="id-1.3.5.2.5.6.2.3"><span class="term "><span class="guimenu ">Swift Dashboard</span></span></dt><dd><p>This screen allows you to run
       <code class="command">swift-dispersion-report</code> on the node or nodes
       to which it has been deployed. Use this tool to measure the
       overall health of the swift cluster. For details, see <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/object-storage-dispersion.html" target="_blank">http://docs.openstack.org/liberty/config-reference/content/object-storage-dispersion.html</a>.
       </p></dd><dt id="id-1.3.5.2.5.6.2.4"><span class="term "><span class="guimenu ">Backup &amp; Restore</span></span></dt><dd><p>This screen is for creating and downloading a backup of the Administration Server. You can also restore from a backup or upload a backup image from your
       local file system.
       
       </p></dd><dt id="id-1.3.5.2.5.6.2.5"><span class="term "><span class="guimenu ">Cisco UCS</span></span></dt><dd><p>
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> can communicate with a Cisco UCS Manager instance via
       its XML-based API server to perform the following functions:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Instantiate UCS service profiles for Compute Nodes and Storage Nodes
         from predefined UCS service profile templates.
        </p></li><li class="listitem "><p>
         Reboot, start, and stop nodes.
        </p></li></ul></div><p>
       The following prerequisites need to be fulfilled on the Cisco UCS side:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Templates for Compute Nodes and Storage Nodes need to be created. These
         service profile templates will be used for preparing systems as
         SUSE <span class="productname">OpenStack</span> Cloud  nodes. Minimum requirements are a processor supporting
         AMD-V or Intel-VT, 8 GB RAM, one network interface and at least 20 GB
         of storage (more for Storage Nodes). The templates must be named
         <code class="literal">suse-cloud-compute</code> and <code class="literal">suse-cloud-storage</code>.
        </p></li><li class="listitem "><p>
         A user account with administrative permissions needs to be created
         for communicating with SUSE <span class="productname">OpenStack</span> Cloud. The account needs to have access to
         the service profile templates listed above. It also need permission
         to create service profiles and associate them with physical hardware.
        </p></li></ul></div><p>
       To initially connect to the Cisco UCS Manager, provide the login
       credentials of the user account mentioned above. The <span class="guimenu ">API
       URL</span> has the form
       <code class="literal">http://<em class="replaceable ">UCSMANAGERHOST</em>/nuova</code>. Click
       <span class="guimenu ">Login</span> to connect. When connected, you will see a
       list of servers and associated actions. Applying an action with the
       <span class="guimenu ">Update</span> button can take up to several minutes.
      </p></dd></dl></div></div><div class="sect2 " id="sec-depl-crow-overview-help"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Help</span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-help">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-crow-overview-help</li></ul></div></div></div></div><p>From this screen you can access HTML and PDF versions of the
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> manuals that are installed on the Administration Server.</p></div></div><div class="sect1 " id="sec-depl-ostack-barclamps"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Barclamp Proposals</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barclamps</li></ul></div></div></div></div><p>
    Barclamps are a set of recipes, templates, and installation
    instructions. They are used to automatically install <span class="productname">OpenStack</span> components on the
    nodes. Each barclamp is configured via a so-called proposal. A proposal
    contains the configuration of the service(s) associated with the barclamp
    and a list of machines onto which to deploy the barclamp.
   </p><p>Most barclamps consist of two sections:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.2.6.4.1"><span class="term "><span class="guimenu ">Attributes</span></span></dt><dd><p>For changing the barclamp's configuration, either by editing the
      respective Web forms (<span class="guimenu ">Custom</span> view) or by switching to the
       <span class="guimenu ">Raw</span> view, which exposes all configuration options for the barclamp. In
      the <span class="guimenu ">Raw</span> view, you directly edit the configuration file. </p><div id="id-1.3.5.2.6.4.1.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Saving Your Changes</h6><p> Before you switch to <span class="guimenu ">Raw</span> view or back again to <span class="guimenu ">Custom</span> view,
       <span class="guimenu ">Save</span> your changes. Otherwise they will
       be lost.</p></div></dd><dt id="id-1.3.5.2.6.4.2"><span class="term "><span class="guimenu ">Deployment</span></span></dt><dd><p>Lets you choose onto which nodes to deploy the barclamp. On the left-hand side, you
      see a list of <span class="guimenu ">Available Nodes</span>. The right-hand side shows a list of roles
      that belong to the barclamp.</p><p>Assign the nodes to the roles that should be deployed on that node. Some barclamps
      contain roles that can also be deployed to a cluster. If you have deployed the Pacemaker
      barclamp, the <span class="guimenu ">Deployment</span> section additionally lists <span class="guimenu ">Available
       Clusters</span> and <span class="guimenu ">Available Clusters with Remote Nodes</span> in this case.
      The latter are clusters that contain both <span class="quote">“<span class="quote ">normal</span>”</span> nodes and Pacemaker remote
      nodes. See <a class="xref" href="#sec-depl-reg-ha-compute" title="2.6.3. High Availability of the Compute Node(s)">Section 2.6.3, “High Availability of the Compute Node(s)”</a> for the basic details.</p><div id="id-1.3.5.2.6.4.2.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Clusters with Remote Nodes</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Clusters (or clusters with remote nodes) cannot be assigned to roles that need to
         be deployed on individual nodes. If you try to do so, the Crowbar Web interface shows an error
         message.</p></li><li class="listitem "><p>If you assign a cluster with remote nodes to a role that can only be applied to
          <span class="quote">“<span class="quote ">normal</span>”</span> (Corosync) nodes, the role will only be applied to the Corosync
         nodes of that cluster. The role will not be applied to the remote nodes of the same cluster.</p></li></ul></div></div></dd></dl></div><div class="sect2 " id="sec-depl-ostack-barclamps-deploy"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating, Editing and Deploying Barclamp Proposals</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-deploy">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barclamps-deploy</li></ul></div></div></div></div><p>The following procedure shows how to generally edit, create and deploy barclamp
    proposals. For the description and deployment of the individual barclamps, see
    <a class="xref" href="#cha-depl-ostack" title="Chapter 12. Deploying the OpenStack Services">Chapter 12, <em>Deploying the <span class="productname">OpenStack</span> Services</em></a>.</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Log in to the Crowbar Web interface.
     </p></li><li class="step "><p>Click
      <span class="guimenu ">Barclamps</span> and select
      <span class="guimenu ">All Barclamps</span>. Alternatively, filter for categories by selecting
      either <span class="guimenu ">Crowbar</span> or <span class="guimenu "><span class="productname">OpenStack</span></span>.</p></li><li class="step "><p>To create a new proposal or edit an existing one, click <span class="guimenu ">Create</span> or
      <span class="guimenu ">Edit</span> next to the appropriate barclamp.</p></li><li class="step "><p>Change the configuration in the <span class="guimenu ">Attributes</span> section:</p><ol type="a" class="substeps "><li class="step "><p>Change the available options via the Web form.</p></li><li class="step "><p>To edit the configuration file directly, first save changes made in the Web form. Click <span class="guimenu ">Raw</span> to edit the configuration in the editor view.</p></li><li class="step "><p>After you have finished, <span class="guimenu ">Save</span> your changes. (They
       are not applied yet).</p></li></ol></li><li class="step "><p>Assign nodes to a role in the <span class="guimenu ">Deployment</span> section of the
     barclamp. By default, one or more nodes are automatically pre-selected for
     available roles.</p><ol type="a" class="substeps "><li class="step "><p>If this pre-selection does not meet your requirements, click the
       <span class="guimenu ">Remove</span> icon next to the role to remove the assignment.</p></li><li class="step "><p>To assign a node or cluster of your choice, select the item you want from the list of
       nodes or clusters on the left-hand side, then drag and drop the item onto the desired role name on the right.</p><div id="id-1.3.5.2.6.5.3.5.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Do <span class="emphasis"><em>not</em></span> drop a node or cluster onto the text box—this is
       used to filter the list of available nodes or clusters!</p></div></li><li class="step "><p>To save your changes without deploying them yet, click <span class="guimenu ">Save</span>.</p></li></ol></li><li class="step "><p>
      Deploy the proposal by clicking <span class="guimenu ">Apply</span>.
     </p><div id="id-1.3.5.2.6.5.3.6.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Wait Until a Proposal Has Been Deployed</h6><p>If you deploy a proposal onto a node where a previous one is still
      active, the new proposal will overwrite the old one.</p><p>
       Deploying a proposal might take some time (up to several minutes). Always
       wait until you see the message <span class="quote">“<span class="quote ">Successfully applied the proposal</span>”</span>
       before proceeding to the next proposal.
      </p></div></li></ol></div></div><p>A proposal that has not been deployed yet can be deleted in the <span class="guimenu ">Edit
     Proposal</span> view by clicking <span class="guimenu ">Delete</span>. To delete a proposal that has
    already been deployed, see <a class="xref" href="#sec-depl-ostack-barclamps-delete" title="10.3.3. Deleting a Proposal That Already Has Been Deployed">Section 10.3.3, “Deleting a Proposal That Already Has Been Deployed”</a>.</p></div><div class="sect2 " id="sec-depl-ostack-barclamps-failure"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Barclamp Deployment Failure</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-failure">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barclamps-failure</li></ul></div></div></div></div><div id="id-1.3.5.2.6.6.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Deployment Failure</h6><p> A deployment failure of a barclamp may leave your node in an inconsistent state. If
    deployment of a barclamp fails:</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       Fix the reason that has caused the failure.
      </p></li><li class="listitem "><p>Re-deploy the barclamp.</p></li></ol></div><p>For help, see the respective troubleshooting section at <a class="xref" href="#sec-depl-trouble-faq-ostack" title="OpenStack Node Deployment">Q &amp; A 2, “<span class="productname">OpenStack</span> Node Deployment”</a>. </p></div></div><div class="sect2 " id="sec-depl-ostack-barclamps-delete"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deleting a Proposal That Already Has Been Deployed</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-delete">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barclamps-delete</li></ul></div></div></div></div><p>To delete a proposal that has already been deployed, you first need to
     <span class="guimenu ">Deactivate</span> it.</p><div class="procedure " id="id-1.3.5.2.6.7.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.3: </span><span class="name">Deactivating and Deleting a Proposal </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.6.7.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Log in to the Crowbar Web interface. </p></li><li class="step "><p>Click <span class="guimenu ">Barclamps</span> › <span class="guimenu ">All Barclamps</span>.</p></li><li class="step "><p>Click <span class="guimenu ">Edit</span> to open the editing view. </p></li><li class="step "><p>Click <span class="guimenu ">Deactivate</span> and confirm your choice in the following
      pop-up.</p><p>Deactivating a proposal removes the chef role from the nodes, so the routine that
      installed and set up the services is not executed anymore.</p></li><li class="step "><p>Click <span class="guimenu ">Delete</span> to confirm your choice in the following
      pop-up.</p><p>This removes the barclamp configuration data from the server. </p></li></ol></div></div><p>However, deactivating and deleting a barclamp that already had been deployed does
    <span class="emphasis"><em>not</em></span> remove packages installed when the barclamp was deployed.
    Nor does it stop any services that were started during the barclamp deployment.
   On the affected node, proceed as follows to undo the deployment:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Stop the respective services:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop <em class="replaceable ">service</em></pre></div></li><li class="step "><p>Disable the respective services:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl disable <em class="replaceable ">service</em></pre></div></li></ol></div></div><p>Uninstalling the packages should not be necessary.</p></div><div class="sect2 " id="sec-depl-ostack-barclamps-queues"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Queuing/Dequeuing Proposals</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-queues">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_crowbar.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barclamps-queues</li></ul></div></div></div></div><p>
     When a proposal is applied to one or more nodes that are not yet
     available for deployment (for example, because they are rebooting or have
     not been yet fully installed), the proposal will be put in a queue. A
     message like
    </p><div class="verbatim-wrap"><pre class="screen">Successfully queued the proposal until the following become ready: d52-54-00-6c-25-44</pre></div><p>
     will be shown when having applied the proposal. A new button
     <span class="guimenu ">Dequeue</span> will also become available. Use it to cancel
     the deployment of the proposal by removing it from the queue.
    </p></div></div></div><div class="chapter " id="cha-depl-inst-nodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the <span class="productname">OpenStack</span> Nodes</span> <a title="Permalink" class="permalink" href="#cha-depl-inst-nodes">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>cha-depl-inst-nodes</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-inst-nodes-prep"><span class="number">11.1 </span><span class="name">Preparations</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-install"><span class="number">11.2 </span><span class="name">Node Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-install-external"><span class="number">11.3 </span><span class="name">Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-post"><span class="number">11.4 </span><span class="name">Post-Installation Configuration</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-inst-nodes-edit"><span class="number">11.5 </span><span class="name">Editing Allocated Nodes</span></a></span></dt></dl></div></div><p>
  The <span class="productname">OpenStack</span> nodes represent the actual cloud infrastructure. Node
  installation and service deployment is done automatically from the
  Administration Server. Before deploying the <span class="productname">OpenStack</span> services, SUSE Linux Enterprise Server 12 SP3 will be installed on all Control Nodes and Storage Nodes.
 </p><p>
  To prepare the installation, each node needs to be booted using PXE, which
  is provided by the <code class="systemitem">tftp</code> server
  from the Administration Server. Afterward you can allocate the nodes and trigger
  the operating system installation.
 </p><div class="sect1 " id="sec-depl-inst-nodes-prep"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparations</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-prep">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-prep</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.3.5.2.1"><span class="term ">Meaningful Node Names</span></dt><dd><p>
      Make a note of the MAC address and the purpose of each node (for
      example, controller, block storage, object storage, compute).
      This will make deploying the <span class="productname">OpenStack</span> components a lot easier and
      less error-prone. It also enables you to assign meaningful names
      (aliases) to the nodes, which are otherwise listed with the MAC
      address by default.
     </p></dd><dt id="id-1.3.5.3.5.2.2"><span class="term ">BIOS Boot Settings</span></dt><dd><p>
      Make sure booting using PXE (booting from the network) is enabled and
      configured as the <span class="emphasis"><em>primary</em></span> boot-option for each
      node. The nodes will boot twice from the network during the allocation
      and installation phase. Booting from the first hard disk needs to be
      configured as the second boot option.
     </p></dd><dt id="id-1.3.5.3.5.2.3"><span class="term ">Custom Node Configuration</span></dt><dd><p>
      All nodes are installed using AutoYaST with the same configuration
      located at
      <code class="filename">/opt/dell/chef/cookbooks/provisioner/templates/default/autoyast.xml.erb</code>.
      If this configuration does not match your needs (for example if you
      need special third party drivers) you need to make adjustments to this
      file. See the <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-autoyast/" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-autoyast/</a> for details.
      If you change the AutoYaST configuration file, you need to re-upload
      it to Chef using the following command:
     </p><div class="verbatim-wrap"><pre class="screen">knife cookbook upload -o /opt/dell/chef/cookbooks/ provisioner</pre></div></dd><dt id="var-depl-inst-nodes-prep-root-login"><span class="term ">Direct <code class="systemitem">root</code> Login</span></dt><dd><p>
      By default, the <code class="systemitem">root</code> account on the nodes has no password
      assigned, so a direct <code class="systemitem">root</code> login is not possible. Logging in
      on the nodes as <code class="systemitem">root</code> is only possible via SSH public keys
      (for example, from the Administration Server).
     </p><p>
      If you want to allow direct <code class="systemitem">root</code> login, you can set a
      password via the Crowbar Provisioner barclamp before deploying the
      nodes. That password will be used for the <code class="systemitem">root</code> account on all
      <span class="productname">OpenStack</span> nodes. Using this method after the nodes are deployed is
      not possible. In that case you would need to log in to each node via
      SSH from the Administration Server and change the password manually with
      <code class="command">passwd</code>.
     </p><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Setting a <code class="systemitem">root</code> Password for the <span class="productname">OpenStack</span> Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.5.2.4.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>
        Create an md5-hashed <code class="systemitem">root</code>-password, for example by using
        <code class="command">openssl passwd</code> <code class="option">-1</code>.
       </p></li><li class="listitem "><p>
        Open a browser and point it to the Crowbar Web interface on the
        Administration Server, for example <code class="literal">http://192.168.124.10</code>. Log
        in as user <code class="systemitem">crowbar</code>. The
        password is <code class="literal">crowbar</code> by default, if you have not
        changed it during the installation.
       </p></li><li class="listitem "><p>
        Open the barclamp menu by clicking <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span>. Click the <span class="guimenu ">Provisioner</span> barclamp
        entry and <span class="guimenu ">Edit</span> the <span class="guimenu ">Default</span>
        proposal.
       </p></li><li class="listitem "><p>
        Click <span class="guimenu ">Raw</span> in the <span class="guimenu ">Attributes</span>
        section to edit the configuration file.
       </p></li><li class="listitem "><p>
        Add the following line to the end of the file before the last
        closing curly bracket:
       </p><div class="verbatim-wrap"><pre class="screen">, "root_password_hash": "<em class="replaceable ">HASHED_PASSWORD</em>"</pre></div><p>
        replacing "<em class="replaceable ">HASHED_PASSWORD</em>" with the
        password you generated in the first step.
       </p></li><li class="listitem "><p>
        Click <span class="guimenu ">Apply</span>.
       </p></li></ol></div></dd></dl></div></div><div class="sect1 " id="sec-depl-inst-nodes-install"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Installation</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-install">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-install</li></ul></div></div></div></div><p>
   To install a node, you need to boot it first using PXE. It will be booted
   with an image that enables the Administration Server to discover the node and make
   it available for installation. When you have allocated the node, it will
   boot using PXE again and the automatic installation will start.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Boot all nodes that you want to deploy using PXE. The nodes will boot
     into the SLEShammer image, which performs the initial
     hardware discovery.
    </p><div id="id-1.3.5.3.6.3.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Limit the Number of Concurrent Boots using PXE</h6><p>
      Booting many nodes at the same time using PXE will cause heavy load on
      the TFTP server, because all nodes will request the boot image at the
      same time. We recommend booting the nodes at different intervals.
     </p></div></li><li class="step "><p>
     Open a browser and point it to the Crowbar Web interface on the Administration Server,
     for example <code class="literal">http://192.168.124.10/</code>. Log in as user
     <code class="systemitem">crowbar</code>. The password is
     <code class="literal">crowbar</code> by default, if you have not changed it.
    </p><p>
     Click <span class="guimenu ">Nodes</span> › <span class="guimenu ">Dashboard</span> to open the <span class="guimenu ">Node
     Dashboard</span>.
    </p></li><li class="step "><p>
     Each node that has successfully booted will be listed as being in state
     <code class="literal">Discovered</code>, indicated by a yellow bullet. The nodes
     will be listed with their MAC address as a name. Wait until all nodes
     are listed as <code class="literal">Discovered</code> before proceeding. If a node does not report as <code class="literal">Discovered</code>, it
     may need to be rebooted manually.
    </p><div class="figure" id="id-1.3.5.3.6.3.3.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_node_dashboard_initial_nodes.png" target="_blank"><img src="images/depl_node_dashboard_initial_nodes.png" width="" alt="Discovered Nodes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.1: </span><span class="name">Discovered Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.6.3.3.2">#</a></h6></div></div></li><li class="step "><p>
     Although this step is optional, we recommend properly grouping
     your nodes at this stage, since it lets you clearly arrange all nodes.
     Grouping the nodes by role would be one option, for example control,
     compute and object storage (Swift).
    </p><ol type="a" class="substeps "><li class="step "><p>
       Enter the name of a new group into the <span class="guimenu ">New Group</span>
       text box and click <span class="guimenu ">Add Group</span>.
      </p></li><li class="step "><p>
       Drag and drop a node onto the title of the newly created group.
       Repeat this step for each node you want to put into the group.
       
      </p><div class="figure" id="id-1.3.5.3.6.3.4.2.2.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_node_dashboard_groups_initial.png" target="_blank"><img src="images/depl_node_dashboard_groups_initial.png" width="" alt="Grouping Nodes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.2: </span><span class="name">Grouping Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.6.3.4.2.2.2">#</a></h6></div></div></li></ol></li><li class="step "><p>
     To allocate all nodes, click <span class="guimenu ">Nodes</span> › <span class="guimenu ">Bulk Edit</span>. To allocate a single node,
     click the name of a node, then click <span class="guimenu ">Edit</span>.
    </p><div class="figure" id="id-1.3.5.3.6.3.5.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_node_edit.png" target="_blank"><img src="images/depl_node_edit.png" width="" alt="Editing a Single Node" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.3: </span><span class="name">Editing a Single Node </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.6.3.5.2">#</a></h6></div></div><div id="id-1.3.5.3.6.3.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Limit the Number of Concurrent Node Deployments</h6><p>
      Deploying many nodes in bulk mode will cause heavy load
      on the Administration Server. The subsequent concurrent Chef client runs
      triggered by the nodes will require a lot of RAM on the Administration Server.
     </p><p>
      Therefore it is recommended to limit the number of concurrent
      <span class="quote">“<span class="quote ">Allocations</span>”</span> in bulk mode. The maximum number depends on
      the amount of RAM on the Administration Server—limiting concurrent
      deployments to five up to ten is recommended.
     </p></div></li><li class="step "><p> In single node editing mode, you can also specify the
      <span class="guimenu ">Filesystem Type</span> for the node. By default, it is set to
      <code class="literal">ext4</code> for all nodes. We recommended using the default.</p></li><li class="step "><p>
     Provide a meaningful <span class="guimenu ">Alias</span>, <span class="guimenu ">Public
     Name</span>, and a <span class="guimenu ">Description</span> for each node, and then
     check the <span class="guimenu ">Allocate</span> box. You can also specify the
     <span class="guimenu ">Intended Role</span> for the node. This optional setting is
     used to make reasonable proposals for the barclamps.
    </p><p>
     By default the <span class="guimenu ">Target Platform</span> is set to <span class="guimenu ">SLES 12
     SP3</span>.
    </p><div id="id-1.3.5.3.6.3.7.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Alias Names</h6><p>
      Providing an alias name will change the default node names (MAC
      address) to the name you provided, making it easier to identify the
      node. Furthermore, this alias will also be used as a DNS
      <code class="literal">CNAME</code> for the node in the admin network. As a
      result, you can access the node via this alias when, for example,
      logging in via SSH.
     </p></div><div id="id-1.3.5.3.6.3.7.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Public Names</h6><p>
      A node's <span class="guimenu ">Alias Name</span> is resolved by the DNS server
      installed on the Administration Server and therefore only available within the
      cloud network. The <span class="productname">OpenStack</span> Dashboard or some APIs
      (<code class="systemitem">keystone-server</code>,
      <code class="systemitem">glance-server</code>,
      <code class="systemitem">cinder-controller</code>,
      <code class="systemitem">neutron-server</code>,
      <code class="systemitem">nova-controller</code>, and
      <code class="systemitem">swift-proxy</code>) can be accessed
      from outside the SUSE <span class="productname">OpenStack</span> Cloud network. To be able to access them by
      name, these names need to be resolved by a name server placed outside
      of the SUSE <span class="productname">OpenStack</span> Cloud network. If you have created DNS entries for nodes,
      specify the name in the <span class="guimenu ">Public Name</span> field.
     </p><p>
      The <span class="guimenu ">Public Name</span> is never used within the SUSE <span class="productname">OpenStack</span> Cloud
      network. However, if you create an SSL certificate for a node that has
      a public name, this name must be added as an
      <code class="literal">AlternativeName</code> to the certificate. See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for more information.
     </p></div><div class="figure" id="id-1.3.5.3.6.3.7.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_node_bulk_edit_allocate.png" target="_blank"><img src="images/depl_node_bulk_edit_allocate.png" width="" alt="Bulk Editing Nodes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.4: </span><span class="name">Bulk Editing Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.6.3.7.5">#</a></h6></div></div></li><li class="step "><p> When you have filled in the data for all nodes, click
      <span class="guimenu ">Save</span>. The nodes will reboot and commence the
     AutoYaST-based SUSE Linux Enterprise Server installation (or installation of other target platforms,
     if selected) via a second boot using PXE. Click <span class="guimenu ">Nodes</span> › <span class="guimenu ">Dashboard</span> to return to the <span class="guimenu ">Node Dashboard</span>. </p></li><li class="step "><p>
     Nodes that are being installed are listed with the status
     <code class="literal">Installing</code> (yellow/green bullet). When the
     installation of a node has finished, it is listed as being
     <code class="literal">Ready</code>, indicated by a green bullet. Wait until all
     nodes are listed as <code class="literal">Ready</code> before proceeding.
    </p><div class="figure" id="id-1.3.5.3.6.3.9.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_node_dashboard_groups_installed.png" target="_blank"><img src="images/depl_node_dashboard_groups_installed.png" width="" alt="All Nodes Have Been Installed" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.5: </span><span class="name">All Nodes Have Been Installed </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.6.3.9.2">#</a></h6></div></div></li></ol></div></div></div><div class="sect1 " id="sec-depl-inst-nodes-install-external"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-install-external">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-install-external</li></ul></div></div></div></div><p>
   SUSE <span class="productname">OpenStack</span> Cloud allows adding existing machines installed with SUSE Linux Enterprise Server 12 SP3 to
   the pool of nodes. This enables you to use spare machines for
   SUSE <span class="productname">OpenStack</span> Cloud, and offers an alternative way of provisioning and installing
   nodes (via SUSE Manager for example). The
   machine must run SUSE Linux Enterprise Server 12 SP3.
  </p><p>
   The machine also needs to be on the same network as the
   Administration Server, because it needs to communicate with this server. Since the
   Administration Server provides a DHCP server, we recommend configuring this machine to get its network assignments from DHCP. If it has a static IP address, make
   sure it is not already used in the admin network. Check the list of used
   IP addresses with the YaST Crowbar module as described in
   <a class="xref" href="#sec-depl-adm-inst-crowbar-network" title="7.2. Networks">Section 7.2, “<span class="guimenu ">Networks</span>”</a>.
  </p><p>
   Proceed as follows to convert an existing SUSE Linux Enterprise Server 12 SP3 machine into a
   SUSE <span class="productname">OpenStack</span> Cloud node:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Download the <code class="filename">crowbar_register</code> script from the
     Administration Server at
     <code class="literal">http://<em class="replaceable ">192.168.124.10</em>:8091/suse-12.3/x86_64/crowbar_register</code>.
     Replace the IP address with the IP address of your Administration Server using
     <code class="command">curl</code> or <code class="command">wget</code>. Note that the
     download only works from within the admin network.
    </p></li><li class="step "><p>
     Make the <code class="filename">crowbar_register</code> script executable
     (<code class="command">chmod</code> <code class="option">a+x</code> crowbar_register).
    </p></li><li class="step "><p>
     Run the <code class="filename">crowbar_register</code> script. If you have
     multiple network interfaces, the script tries to automatically detect
     the one that is connected to the admin network. You may also explicitly
     specify which network interface to use by using the
     <code class="option">--interface</code> switch, for example
     <code class="command">crowbar_register</code> <code class="option">--interface eth1</code>.
    </p></li><li class="step "><p>
     After the script has successfully run, the machine has been added to
     the pool of nodes in the SUSE <span class="productname">OpenStack</span> Cloud and can be used as any other node
     from the pool.
    </p></li></ol></div></div></div><div class="sect1 " id="sec-depl-inst-nodes-post"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Configuration</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post</li></ul></div></div></div></div><p>
   The following lists some <span class="emphasis"><em>optional</em></span> configuration
   steps like configuring node updates, monitoring, access, and
   enabling SSL. You may entirely skip the following steps or perform any
   of them at a later stage.
  </p><div class="sect2 " id="sec-depl-inst-nodes-post-updater"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Node Updates with the Updater Barclamp</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-updater">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-updater</li></ul></div></div></div></div><p>
    To keep the operating system and the SUSE <span class="productname">OpenStack</span> Cloud software itself
    up-to-date on the nodes, you can deploy either the Updater barclamp or
    the SUSE Manager barclamp. The latter requires access to a
    SUSE Manager server. The Updater barclamp uses Zypper to install
    updates and patches from repositories made available on the
    Administration Server.
   </p><p>
    The easiest way to provide the required repositories on the Administration Server
    is to set up an SMT server as described in
    <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>. Alternatives to setting up an SMT
    server are described in <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
   </p><p>
    The Updater barclamp lets you deploy updates that are available on the
    update repositories at the moment of deployment. Each time you deploy
    updates with this barclamp you can choose a different set of nodes to
    which the updates are deployed. This lets you exactly control where and
    when updates are deployed.
   </p><p>
    To deploy the Updater barclamp, proceed as follows. For general
    instructions on how to edit barclamp proposals refer to
    <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step "><p>
      Open the barclamp menu by clicking <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span>.
      Click the <span class="guimenu ">Updater</span> barclamp entry and
      <span class="guimenu ">Create</span> to open the proposal.
     </p></li><li class="step "><p>
      Configure the barclamp by the following attributes. This
      configuration always applies to all nodes on which the barclamp is
      deployed. Individual configurations for certain nodes are only supported
      by creating a separate proposal.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.3.8.3.6.3.2.1"><span class="term "><span class="guimenu ">Use zypper</span>
       </span></dt><dd><p>
         Define which Zypper subcommand to use for updating.
         <span class="guimenu ">patch</span> will install all patches applying to the
         system from the configured update repositories that are available.
         <span class="guimenu ">update</span> will update packages from all configured
         repositories (not just the update repositories) that have a higher
         version number than the installed packages.
         <span class="guimenu ">dist-upgrade</span> replaces each package installed
         with the version from the repository and deletes packages not
         available in the repositories.
        </p><p>
         We recommend using <span class="guimenu ">patch</span>.
        </p></dd><dt id="id-1.3.5.3.8.3.6.3.2.2"><span class="term "><span class="guimenu ">Enable GPG Checks</span>
       </span></dt><dd><p>
         If set to true (recommended), checks if packages are correctly
         signed.
        </p></dd><dt id="id-1.3.5.3.8.3.6.3.2.3"><span class="term "><span class="guimenu ">Automatically Agree With Licenses</span>
       </span></dt><dd><p>
         If set to true (recommended), Zypper automatically accepts third
         party licenses.
        </p></dd><dt id="id-1.3.5.3.8.3.6.3.2.4"><span class="term "><span class="guimenu ">Include Patches that need Reboots (Kernel)</span>
       </span></dt><dd><p>
         Installs patches that require a reboot (for example Kernel or glibc
         updates). Only set this option to <code class="literal">true</code> when you
         can safely reboot the affected nodes. Refer to
         <a class="xref" href="#cha-depl-maintenance" title="Chapter 17. SUSE OpenStack Cloud Maintenance">Chapter 17, <em>SUSE <span class="productname">OpenStack</span> Cloud Maintenance</em></a> for more information.
         Installing a new Kernel and not rebooting may result in an unstable
         system.
        </p></dd><dt id="id-1.3.5.3.8.3.6.3.2.5"><span class="term "><span class="guimenu ">Reboot Nodes if Needed</span>
       </span></dt><dd><p>
         Automatically reboots the system in case a patch requiring a reboot
         has been installed. Only set this option to <code class="literal">true</code>
         when you can safely reboot the affected nodes. Refer to
         <a class="xref" href="#cha-depl-maintenance" title="Chapter 17. SUSE OpenStack Cloud Maintenance">Chapter 17, <em>SUSE <span class="productname">OpenStack</span> Cloud Maintenance</em></a> for more information.
        </p></dd></dl></div><div class="figure" id="id-1.3.5.3.8.3.6.3.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_updater_attributes.png" target="_blank"><img src="images/depl_barclamp_updater_attributes.png" width="" alt="SUSE Updater barclamp: Configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.6: </span><span class="name">SUSE Updater barclamp: Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.8.3.6.3.3">#</a></h6></div></div></li><li class="step "><p>
      Choose the nodes on which the Updater barclamp should be deployed in
      the <span class="guimenu ">Node Deployment</span> section by dragging them to the
      <span class="guimenu ">Updater</span> column.
     </p><div class="figure" id="id-1.3.5.3.8.3.6.4.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_updater_nodes.png" target="_blank"><img src="images/depl_barclamp_updater_nodes.png" width="" alt="SUSE Updater barclamp: Node Deployment" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.7: </span><span class="name">SUSE Updater barclamp: Node Deployment </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.8.3.6.4.2">#</a></h6></div></div></li></ol></div></div><p>
    <code class="command">zypper</code> keeps track of the packages and patches it
    installs in <code class="filename">/var/log/zypp/history</code>. Review that log
    file on a node to find out which updates have been installed. A second
    log file recording debug information on the <code class="command">zypper</code>
    runs can be found at <code class="filename">/var/log/zypper.log</code> on each
    node.
   </p><div id="id-1.3.5.3.8.3.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Updating Software Packages on Cluster Nodes</h6><p>
     Before starting an update for a cluster node, either stop the cluster
     stack on that node or put the cluster into maintenance mode. If the
     cluster resource manager on a node is active during the software update,
     this can lead to unpredictable results like fencing of active nodes. For
     detailed instructions refer to <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-migration-update" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-migration-update</a>.
    </p></div></div><div class="sect2 " id="sec-depl-inst-nodes-post-manager"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Node Updates with the <span class="guimenu ">SUSE Manager Client</span>
    Barclamp</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-manager">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-manager</li></ul></div></div></div></div><p>
    To keep the operating system and the SUSE <span class="productname">OpenStack</span> Cloud software itself
    up-to-date on the nodes, you can deploy either <span class="guimenu ">SUSE Manager
    Client</span> barclamp or the Updater barclamp. The latter uses
    Zypper to install updates and patches from repositories made available
    on the Administration Server.
   </p><p>
    To enable the SUSE Manager server to manage the SUSE <span class="productname">OpenStack</span> Cloud nodes, you must make the
    respective <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> channels, the SUSE Linux Enterprise Server 12 SP3 channels,
    and the channels for extensions used with your deployment (High Availability Extension,
    SUSE Enterprise Storage) available via an activation key.
   </p><p>
    The <span class="guimenu ">SUSE Manager Client</span> barclamp requires access to
    the SUSE Manager server from every node it is deployed to.
   </p><p>
    To deploy the <span class="guimenu ">SUSE Manager Client</span> barclamp, proceed
    as follows. For general instructions on how to edit barclamp proposals
    refer to <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the package
      <code class="literal">rhn-org-trusted-ssl-cert-<em class="replaceable ">VERSION</em>-<em class="replaceable ">RELEASE</em>.noarch.rpm</code>
      from
      https://<em class="replaceable ">susemanager.example.com</em>/pub/.
      <em class="replaceable ">VERSION</em> and
      <em class="replaceable ">RELEASE</em> may vary, ask the administrator of
      the SUSE Manager for the correct values.
      <em class="replaceable ">susemanager.example.com</em> needs to be
      replaced by the address of your SUSE Manager server. Copy the file you
      downloaded to
      <code class="filename">/opt/dell/chef/cookbooks/suse-manager-client/files/default/ssl-cert.rpm</code>
      on the Administration Server. The package contains the SUSE Manager's CA SSL
      Public Certificate. The certificate installation has not been
      automated on purpose, because downloading the certificate manually
      enables you to check it before copying it.
     </p></li><li class="step "><p>
      Re-install the barclamp by running the following command:
     </p><div class="verbatim-wrap"><pre class="screen">/opt/dell/bin/barclamp_install.rb --rpm core</pre></div></li><li class="step "><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step "><p>
      Open the barclamp menu by clicking <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span>.
      Click the <span class="guimenu ">SUSE Manager Client</span> barclamp entry and
      <span class="guimenu ">Create</span> to open the proposal.
     </p></li><li class="step "><p>
      Specify the URL of the script for activation of the clients in the <span class="guimenu ">URL of the bootstrap script</span> field.
     </p></li><li class="step "><p>
      Choose the nodes on which the SUSE Manager barclamp should be
      deployed in the <span class="guimenu ">Deployment</span> section by dragging
      them to the <span class="guimenu ">suse-manager-client</span> column. We
      recommend deploying it on all nodes in the SUSE <span class="productname">OpenStack</span> Cloud.
     </p><div class="figure" id="id-1.3.5.3.8.4.6.6.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_susemgr.png" target="_blank"><img src="images/depl_barclamp_susemgr.png" width="" alt="SUSE Manager barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.8: </span><span class="name">SUSE Manager barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.8.4.6.6.2">#</a></h6></div></div></li></ol></div></div><div id="id-1.3.5.3.8.4.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Updating Software Packages on Cluster Nodes</h6><p>
     Before starting an update for a cluster node, either stop the cluster
     stack on that node or put the cluster into maintenance mode. If the
     cluster resource manager on a node is active during the software update,
     this can lead to unpredictable results like fencing of active nodes. For
     detailed instructions refer to <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-migration-update" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-migration-update</a>.
    </p></div></div><div class="sect2 " id="sec-depl-inst-nodes-post-nfs"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Mounting NFS Shares on a Node</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-nfs">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-nfs</li></ul></div></div></div></div><p>
    The NFS barclamp allows you to mount NFS share from a remote host on
    nodes in the cloud. This feature can, for example, be used to provide an
    image repository for Glance. Note that all nodes which are to mount
    an NFS share must be able to reach the NFS server. This requires manually adjusting the network configuration.
   </p><p>
    To deploy the NFS barclamp, proceed as follows. For general
    instructions on how to edit barclamp proposals refer to
    <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step "><p>
      Open the barclamp menu by clicking <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span>.
      Click the <span class="guimenu ">NFS Client</span> barclamp entry and
      <span class="guimenu ">Create</span> to open the proposal.
     </p></li><li class="step "><p>
      Configure the barclamp by the following attributes. Each set of
      attributes is used to mount a single NFS share.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.3.8.5.4.3.2.1"><span class="term "><span class="guimenu ">Name</span>
       </span></dt><dd><p>
         Unique name for the current configuration. This name is used in the
         Web interface only to distinguish between different shares.
        </p></dd><dt id="id-1.3.5.3.8.5.4.3.2.2"><span class="term "><span class="guimenu ">NFS Server</span>
       </span></dt><dd><p>
         Fully qualified host name or IP address of the NFS server.
        </p></dd><dt id="id-1.3.5.3.8.5.4.3.2.3"><span class="term "><span class="guimenu ">Export</span>
       </span></dt><dd><p>
         Export name for the share on the NFS server.
        </p></dd><dt id="id-1.3.5.3.8.5.4.3.2.4"><span class="term "><span class="guimenu ">Path</span>
       </span></dt><dd><p>
         Mount point on the target machine.
        </p></dd><dt id="id-1.3.5.3.8.5.4.3.2.5"><span class="term "><span class="guimenu ">Mount Options</span>
       </span></dt><dd><p>
         Mount options that will be used on the node. See <code class="command">man 8
         mount </code> for general mount options and <code class="command">man 5
         nfs</code> for a list of NFS-specific options. Note that the
         general option <code class="option">nofail</code> (do not report errors if
         device does not exist) is automatically set.
        </p></dd></dl></div></li><li class="step "><p>
      After having filled in all attributes, click <span class="guimenu ">Add</span>. If
      you want to mount more than one share, fill in the data for another
      NFS mount. Otherwise click <span class="guimenu ">Save</span> to save the data,
      or <span class="guimenu ">Apply</span> to deploy the proposal. Note that you must
      always click <span class="guimenu ">Add</span> before saving or applying the
      barclamp, otherwise the data that was entered will be lost.
     </p><div class="figure" id="id-1.3.5.3.8.5.4.4.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nfs.png" target="_blank"><img src="images/depl_barclamp_nfs.png" width="" alt="NFS barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.9: </span><span class="name">NFS barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.8.5.4.4.2">#</a></h6></div></div></li><li class="step "><p>
      Go to the <span class="guimenu ">Node Deployment</span> section and drag and drop
      all nodes, on which the NFS shares defined above should be mounted, to
      the <span class="guimenu ">nfs-client</span> column. Click
      <span class="guimenu ">Apply</span> to deploy the proposal.
     </p><p>
      The NFS barclamp is the only barclamp that lets you create
      different proposals, enabling you to mount different NFS
      shares on different nodes. When you have created an NFS proposal, a
      special <span class="guimenu ">Edit</span> is shown in the barclamp overview of the
      Crowbar Web interface. Click it to either
      <span class="guimenu ">Edit</span> an existing proposal or
      <span class="guimenu ">Create</span> a new one. New proposals must have unique names.
     </p><div class="figure" id="id-1.3.5.3.8.5.4.5.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nfs_edit.png" target="_blank"><img src="images/depl_barclamp_nfs_edit.png" width="" alt="Editing an NFS barclamp Proposal" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.10: </span><span class="name">Editing an NFS barclamp Proposal </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.8.5.4.5.3">#</a></h6></div></div></li></ol></div></div></div><div class="sect2 " id="sec-depl-inst-nodes-post-ceph-ext"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using an Externally Managed Ceph Cluster</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ceph-ext">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-ceph-ext</li></ul></div></div></div></div><p>
    The following chapter provides instructions on using an external Ceph
    cluster in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p><div class="sect3 " id="sec-depl-inst-nodes-post-ceph-ext-requirements"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.4.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ceph-ext-requirements">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-ceph-ext-requirements</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.3.8.6.3.2.1"><span class="term ">Ceph Release</span></dt><dd><p>
        External Ceph cluster are supported with SUSE Enterprise Storage 5 or higher. The
        version of Ceph should be compatible with the version of the Ceph
        client supplied with SUSE Linux Enterprise Server 12 SP3.
       </p></dd><dt id="id-1.3.5.3.8.6.3.2.2"><span class="term ">Network Configuration</span></dt><dd><p>
        The external Ceph cluster needs to be connected to a separate
        VLAN, which is mapped to the SUSE <span class="productname">OpenStack</span> Cloud storage VLAN. See
        <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for more information.
       </p></dd></dl></div></div><div class="sect3 " id="sec-depl-inst-nodes-post-ceph-ext-install"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.4.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Ceph Available on the SUSE <span class="productname">OpenStack</span> Cloud Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ceph-ext-install">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-ceph-ext-install</li></ul></div></div></div></div><p>
     Ceph can be used from the KVM Compute Nodes, with
     Cinder, and with Glance. The following installation
     steps need to be executed on each node accessing Ceph:
    </p><div id="id-1.3.5.3.8.6.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Installation Workflow</h6><p>
      The following steps need to be executed before the barclamps get
      deployed.
     </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Log in as user <code class="systemitem">root</code> to a machine in the Ceph cluster
       and generate keyring files for Cinder users. Optionally, you can
       generate keyring files for the Glance users (only needed
       when using Glance with Ceph/Rados). The keyring file that will be
       generated for Cinder will also be used on the Compute Nodes.
       To do so, you need to specify pool names and user names for both services. The default
       names are:
      </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th>
           
          </th><th>
           <p>
            Glance
           </p>
          </th><th>
           <p>
            Cinder
           </p>
          </th></tr></thead><tbody><tr><td>
           <p>
            <span class="bold"><strong>User</strong></span>
           </p>
          </td><td>
           <p>
            glance
           </p>
          </td><td>
           <p>
            cinder
           </p>
          </td></tr><tr><td>
           <p>
            <span class="bold"><strong>Pool</strong></span>
           </p>
          </td><td>
           <p>
            images
           </p>
          </td><td>
           <p>
            volumes
           </p>
          </td></tr></tbody></table></div><p>
       Make a note of user and pool names in case you do not use the default
       values. You will need this information later, when deploying
       Glance and Cinder.
      </p></li><li class="step "><div id="id-1.3.5.3.8.6.4.4.2.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Automatic Changes to the Cluster</h6><p>
        If you decide to use the admin keyring file to connect the external
        Ceph cluster, be aware that after Crowbar discovers this admin keyring,
        it will create client keyring files, pools, and capabilities needed to run
        Glance, Cinder, or Nova integration.
       </p></div><p>
       If you have access to the admin keyring file and agree that automatic
       changes will be done to the cluster as described above, copy it together
       with the Ceph configuration file to the Administration Server. If you cannot
       access this file, create a keyring:
      </p><ol type="a" class="substeps "><li class="step "><p>
         When you can access the admin keyring file
         <code class="filename">ceph.client.admin.keyring</code>, copy it together with
         <code class="filename">ceph.conf</code> (both files are usually located in
         <code class="filename">/etc/ceph</code>) to a temporary location on the
         Administration Server, for example <code class="filename">/root/tmp/</code>.
        </p></li><li class="step "><p>
         If you cannot access the admin keyring file create a new keyring file
         with the following commands. Re-run the commands for Glance, too, if
         needed. First create a key:
        </p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.<em class="replaceable ">USERNAME</em> mon "allow r" \
osd 'allow class-read object_prefix rbd_children, allow rwx \
pool=<em class="replaceable ">POOLNAME</em>'</pre></div><p>
         Replace <em class="replaceable ">USERNAME</em> and
         <em class="replaceable ">POOLNAME</em> with the respective values.
        </p><p>
         Now use the key to generate the keyring file
         <code class="filename">/etc/ceph/ceph.client.<em class="replaceable ">USERNAME</em>.keyring</code>:
        </p><div class="verbatim-wrap"><pre class="screen">ceph-authtool \
/etc/ceph/ceph.client.<em class="replaceable ">USERNAME</em>.keyring \
--create-keyring --name=client.<em class="replaceable ">USERNAME</em>&gt; \
--add-key=<em class="replaceable ">KEY</em></pre></div><p>
         Replace <em class="replaceable ">USERNAME</em> with the respective
         value.
        </p><p>
         Copy the Ceph configuration file <code class="filename">ceph.conf</code>
         (usually located in <code class="filename">/etc/ceph</code>) and the keyring
         file(s) generated above to a temporary location on the Administration Server, for
         example <code class="filename">/root/tmp/</code>.
        </p></li></ol></li><li class="step "><p>
       Log in to the Crowbar Web interface and check whether the nodes
       which should have access to the Ceph cluster already have an IP
       address from the storage network. Do so by going to the
       <span class="guimenu ">Dashboard</span> and clicking the node name. An
       <span class="guimenu ">IP address</span> should be listed for
       <span class="guimenu ">storage</span>. Make a note of the <span class="guimenu ">Full
       name</span> of each node that has <span class="emphasis"><em>no</em></span> storage
       network IP address.
      </p></li><li class="step "><p>
       Log in to the Administration Server as user <code class="systemitem">root</code> and run the
       following command for all nodes you noted down in the previous step:
      </p><div class="verbatim-wrap"><pre class="screen">crowbar network allocate_ip "default" <em class="replaceable ">NODE</em> "storage" "host"
chef-client</pre></div><p>
       <em class="replaceable ">NODE</em> needs to be replaced by the node's
       name.
      </p></li><li class="step "><p>
       After executing the command in the previous step for all
       affected nodes, run the command <code class="command">chef-client</code> on the
       Administration Server.
      </p></li><li class="step "><p>
       Log in to each affected node as user <code class="systemitem">root</code>. See
       <a class="xref" href="#var-depl-trouble-faq-ostack-login" title="Q:"><em>
       How can I log in to a node as root?
      </em></a> for instructions.
       On each node, do the following:
      </p><ol type="a" class="substeps "><li class="step "><p>
         Manually install nova, cinder (if using cinder) and/or glance
         (if using glance) packages with the following commands:
        </p><div class="verbatim-wrap"><pre class="screen">zypper in openstack-glance
zypper in openstack-cinder
zypper in openstack-nova</pre></div></li><li class="step "><p>
         Copy the ceph.conf file from the Administration Server to
         <code class="filename">/etc/ceph</code>:
        </p><div class="verbatim-wrap"><pre class="screen">mkdir -p /etc/ceph
scp root@admin:/root/tmp/ceph.conf /etc/ceph
chmod 664 /etc/ceph/ceph.conf</pre></div></li><li class="step "><p>
         Copy the keyring file(s) to <code class="filename">/etc/ceph</code>. The
         exact process depends on whether you have copied the admin keyring
         file or whether you have created your own keyrings:
        </p><ol type="i" class="substeps "><li class="step "><p>
           If you have copied the admin keyring file, run the following
           command on the Control Node(s) on which Cinder and Glance
           will be deployed, and on all KVM Compute Nodes:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.admin.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.admin.keyring</pre></div></li><li class="step "><p>
           If you have created you own keyrings, run the following command on
           the Control Node on which Cinder will be deployed, and on all
           KVM Compute Nodes to copy the Cinder keyring:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.cinder.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
	    On Control Node on which Cinder will be deployed run the
	    following command to update file ownership:
          </p><div class="verbatim-wrap"><pre class="screen">chown root.cinder /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
	    On KVM Compute Nodes run the following command to update file ownership:
          </p><div class="verbatim-wrap"><pre class="screen">chown root.nova /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
           Now copy the Glance keyring to the Control Node on which Glance
           will be deployed:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.glance.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.glance.keyring
chown root.glance /etc/ceph/ceph.client.glance.keyring</pre></div></li></ol></li></ol></li></ol></div></div></div></div><div class="sect2 " id="sec-depl-inst-nodes-post-access"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing the Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-access">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-access</li></ul></div></div></div></div><p>
    The nodes can only be accessed via SSH from the Administration Server—it
    is not possible to connect to them from any other host in the network.
   </p><p>
    The <code class="systemitem">root</code> account <span class="emphasis"><em>on the nodes</em></span> has no
    password assigned, therefore logging in to a node as
    <code class="systemitem">root</code>@<em class="replaceable ">node</em> is only possible via SSH
    with key authentication. By default, you can only log in with the key of
    the <code class="systemitem">root</code> of the Administration Server
    (root@<em class="replaceable ">admin</em>) via SSH only.
   </p><p>
    If you have added users to the Administration Server and want to
    give them permission to log in to the nodes as well, you need to add
    these users' public SSH keys to <code class="systemitem">root</code>'s
    <code class="filename">authorized_keys</code> file on all nodes. Proceed as
    follows:
   </p><div class="procedure " id="id-1.3.5.3.8.7.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 11.1: </span><span class="name">Copying SSH Keys to All Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.8.7.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      If they do not already exist, generate an SSH key pair with
      <code class="command">ssh-keygen</code>. This key pair belongs to the user that you use to log in to the nodes. Alternatively, copy an existing public
      key with <code class="command">ssh-copy-id</code>. Refer to the respective man
      pages for more information.
     </p></li><li class="step "><p>
      Log in to the Crowbar Web interface on the Administration Server, for
      example <code class="literal">http://192.168.124.10/</code> (user name and default
      password: <code class="literal">crowbar</code>).
     </p></li><li class="step "><p>
      Open the barclamp menu by clicking <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span>. Click the <span class="guimenu ">Provisioner</span> barclamp
      entry and <span class="guimenu ">Edit</span> the <span class="guimenu ">Default</span>
      proposal.
     </p></li><li class="step "><p>
      Copy and paste the <span class="emphasis"><em>public</em></span> SSH key of the user
      into the <span class="guimenu ">Additional SSH Keys</span> text box. If adding
      keys for multiple users, note that each key needs to be placed on a
      new line.
     </p></li><li class="step "><p>
      Click <span class="guimenu ">Apply</span> to deploy the keys and save your
      changes to the proposal.
     </p></li></ol></div></div></div><div class="sect2 " id="sec-depl-inst-nodes-post-ssl"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling SSL</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ssl">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-post-ssl</li></ul></div></div></div></div><p>
    To enable SSL to encrypt communication within the cloud (see
    <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for details), all nodes running encrypted services need SSL certificates. An SSL certificate is, at a minimum, required on the Control Node.
   </p><p>
    Each certificate consists
    of a pair of files: the certificate file (for example,
    <code class="filename">signing_cert.pem</code>) and the key file (for example,
    <code class="filename">signing_key.pem</code>). If you use your own certificate
    authority (CA) for signing, you will also need a certificate file for
    the CA (for example, <code class="filename">ca.pem</code>). We recommend copying the files to the <code class="filename">/etc</code> directory using the
    directory structure outlined below. If you use a dedicated certificate
    for each service, create directories named after the services (for
    example, <code class="filename">/etc/keystone</code>). If you are using shared
    certificates, use a directory such as <code class="filename">/etc/cloud</code>.
   </p><div class="variablelist "><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="name">Recommended Locations for Shared Certificates </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.8.8.4">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.3.8.8.4.2"><span class="term ">SSL Certificate File</span></dt><dd><p>
       <code class="filename">/etc/cloud/ssl/certs/signing_cert.pem</code>
      </p></dd><dt id="id-1.3.5.3.8.8.4.3"><span class="term ">SSL Key File</span></dt><dd><p>
       <code class="filename">/etc/cloud/private/signing_key.pem</code>
      </p></dd><dt id="id-1.3.5.3.8.8.4.4"><span class="term ">CA Certificates File</span></dt><dd><p>
       <code class="filename">/etc/cloud/ssl/certs/ca.pem</code>
      </p></dd></dl></div></div></div><div class="sect1 " id="sec-depl-inst-nodes-edit"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Editing Allocated Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-edit">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_inst_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-inst-nodes-edit</li></ul></div></div></div></div><p>
   All nodes that have been allocated can be decommissioned or re-installed.
   Click a node's name in the <span class="guimenu ">Node Dashboard</span> to open a
   screen with the node details. The following options are available:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.3.9.3.1"><span class="term "><span class="guimenu ">Forget</span>
    </span></dt><dd><p>
      Deletes a node from the pool. If you want to re-use this node again,
      it needs to be reallocated and re-installed from scratch.
     </p></dd><dt id="id-1.3.5.3.9.3.2"><span class="term "><span class="guimenu ">Reinstall</span>
    </span></dt><dd><p>
      Triggers a reinstallation. The machine stays allocated. Any barclamps that were deployed on the machine will be re-applied after the installation.
     </p></dd><dt id="id-1.3.5.3.9.3.3"><span class="term "><span class="guimenu ">Deallocate</span>
    </span></dt><dd><p>
      Temporarily removes the node from the pool of nodes. After you
      reallocate the node it will take its former role. Useful for adding
      additional machines in times of high load or for decommissioning
      machines in times of low load.
     </p></dd><dt id="id-1.3.5.3.9.3.4"><span class="term ">
     <span class="guimenu ">Power Actions</span> › <span class="guimenu ">Reboot</span>
    </span></dt><dd><p>
      Reboots the node.
     </p></dd><dt id="id-1.3.5.3.9.3.5"><span class="term ">
     <span class="guimenu ">Power Actions</span> › <span class="guimenu ">Shutdown</span>
    </span></dt><dd><p>
      Shuts the node down.
     </p></dd><dt id="id-1.3.5.3.9.3.6"><span class="term ">
     <span class="guimenu ">Power Actions</span> › <span class="guimenu ">Power Cycle</span>
    </span></dt><dd><p>
      Forces a (non-clean) shuts down and a restart afterward. Only use if a
      reboot does not work.
     </p></dd><dt id="id-1.3.5.3.9.3.7"><span class="term ">
     <span class="guimenu ">Power Actions</span> › <span class="guimenu ">Power Off</span>
    </span></dt><dd><p>
      Forces a (non-clean) node shut down. Only use if a clean shut down does
      not work.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.3.9.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_nodeinfo.png" target="_blank"><img src="images/depl_nodeinfo.png" width="" alt="Node Information" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.11: </span><span class="name">Node Information </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.9.4">#</a></h6></div></div><div id="id-1.3.5.3.9.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Editing Nodes in a Production System</h6><p>
    When de-allocating nodes that provide essential services, the complete
    cloud will become unusable. If you have not disabled redundancy, you can disable single storage nodes or single
    compute nodes. However, disabling Control Node(s) will cause major problems. It
    will either <span class="quote">“<span class="quote ">kill</span>”</span> certain services (for example
    Swift) or, at worst the complete cloud (when deallocating the Control Node
    hosting Neutron). You should also not disable the nodes providing
    swift ring and proxy services.
   </p></div></div></div><div class="chapter " id="cha-depl-ostack"><div class="titlepage"><div><div><h2 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the <span class="productname">OpenStack</span> Services</span> <a title="Permalink" class="permalink" href="#cha-depl-ostack">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>cha-depl-ostack</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-ostack-designate"><span class="number">12.1 </span><span class="name">Deploying Designate</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-pacemaker"><span class="number">12.2 </span><span class="name">Deploying Pacemaker (Optional, HA Setup Only)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-db"><span class="number">12.3 </span><span class="name">Deploying the Database</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-rabbit"><span class="number">12.4 </span><span class="name">Deploying RabbitMQ</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-keystone"><span class="number">12.5 </span><span class="name">Deploying Keystone</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-monasca"><span class="number">12.6 </span><span class="name">Deploying Monasca (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-swift"><span class="number">12.7 </span><span class="name">Deploying Swift (optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-glance"><span class="number">12.8 </span><span class="name">Deploying Glance</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-cinder"><span class="number">12.9 </span><span class="name">Deploying Cinder</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-quantum"><span class="number">12.10 </span><span class="name">Deploying Neutron</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-nova"><span class="number">12.11 </span><span class="name">Deploying Nova</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-dash"><span class="number">12.12 </span><span class="name">Deploying Horizon (<span class="productname">OpenStack</span> Dashboard)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-heat"><span class="number">12.13 </span><span class="name">Deploying Heat (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-ceilometer"><span class="number">12.14 </span><span class="name">Deploying Ceilometer (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-manila"><span class="number">12.15 </span><span class="name">Deploying Manila</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-tempest"><span class="number">12.16 </span><span class="name">Deploying Tempest (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-magnum"><span class="number">12.17 </span><span class="name">Deploying Magnum (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-barbican"><span class="number">12.18 </span><span class="name">Deploying Barbican (Optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-sahara"><span class="number">12.19 </span><span class="name">Deploying Sahara</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-ironic"><span class="number">12.20 </span><span class="name">Deploying Ironic (optional)</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-final"><span class="number">12.21 </span><span class="name">How to Proceed</span></a></span></dt><dt><span class="sect1"><a href="#crow-ses-integration"><span class="number">12.22 </span><span class="name">SUSE Enterprise Storage integration</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-services"><span class="number">12.23 </span><span class="name">Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-crowbatch-description"><span class="number">12.24 </span><span class="name">Crowbar Batch Command</span></a></span></dt></dl></div></div><p>
  After the nodes are installed and configured you can start deploying the
  <span class="productname">OpenStack</span> components to finalize the installation. The components need to be
  deployed in a given order, because they depend on one another. The
  <span class="guimenu ">Pacemaker</span> component for an HA setup is the only exception
  from this rule—it can be set up at any time. However, when deploying
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> from scratch, we recommend deploying the
  <span class="guimenu ">Pacemaker</span> proposal(s) first. Deployment for all components
  is done from the Crowbar Web interface through recipes, so-called
  <span class="quote">“<span class="quote ">barclamps</span>”</span>. (See <a class="xref" href="#sec-depl-services" title="12.23. Roles and Services in SUSE OpenStack Cloud Crowbar">Section 12.23, “Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>”</a> for a table
  of all roles and services, and how to start and stop them.)
 </p><p>
  The components controlling the cloud, including storage management and
  control components, need to be installed on the Control Node(s) (refer to
  <a class="xref" href="#sec-depl-arch-components-control" title="1.2. The Control Node(s)">Section 1.2, “The Control Node(s)”</a> for more information).
  However, you may <span class="emphasis"><em>not</em></span> use your Control Node(s) as a
  compute node or storage host for Swift. Do not install he components
  <span class="guimenu ">swift-storage</span> and <span class="guimenu ">nova-compute-*</span> on the
  Control Node(s). These components must be installed on dedicated Storage Nodes
  and Compute Nodes.
 </p><p>
  When deploying an HA setup, the  Control Nodes are replaced by one or more
  controller clusters consisting of at least two nodes, and three are
  recommended. We recommend setting up three separate clusters for data,
  services, and networking. See <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a> for more
  information on requirements and recommendations for an HA setup.
 </p><p>
  The <span class="productname">OpenStack</span> components need to be deployed in the following order. For
  general instructions on how to edit and deploy barclamps, refer to
  <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>. Any optional components that you
  elect to use must be installed in their correct order.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-designate" title="12.1. Deploying Designate">Deploying Designate</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-pacemaker" title="12.2. Deploying Pacemaker (Optional, HA Setup Only)">Deploying Pacemaker (Optional, HA Setup Only)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-db" title="12.3. Deploying the Database">Deploying the Database</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-rabbit" title="12.4. Deploying RabbitMQ">Deploying RabbitMQ</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-keystone" title="12.5. Deploying Keystone">Deploying Keystone</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-monasca" title="12.6. Deploying Monasca (Optional)">Deploying Monasca (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-swift" title="12.7. Deploying Swift (optional)">Deploying Swift (optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-glance" title="12.8. Deploying Glance">Deploying Glance</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-cinder" title="12.9. Deploying Cinder">Deploying Cinder</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-quantum" title="12.10. Deploying Neutron">Deploying Neutron</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-nova" title="12.11. Deploying Nova">Deploying Nova</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-dash" title="12.12. Deploying Horizon (OpenStack Dashboard)">Deploying Horizon (<span class="productname">OpenStack</span> Dashboard)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-heat" title="12.13. Deploying Heat (Optional)">Deploying Heat (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-ceilometer" title="12.14. Deploying Ceilometer (Optional)">Deploying Ceilometer (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-manila" title="12.15. Deploying Manila">Deploying Manila</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-tempest" title="12.16. Deploying Tempest (Optional)">Deploying Tempest (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-depl-ostack-magnum" title="12.17. Deploying Magnum (Optional)">Deploying Magnum (Optional)</a>
   </p></li></ol></div><div class="sect1 " id="sec-depl-ostack-designate"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Designate</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-designate">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-designate</li></ul></div></div></div></div><p>
    Designate provides <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> DNS as a Service (DNSaaS). It is used to
    create and propagate zones and records over the network using pools of DNS
    servers. Deployment defaults are in place, so not much is required to
    configure Designate. Neutron needs additional settings for integration with
    Designate, which are also present in the <code class="literal">[designate]</code> section in Neutron configuration.
  </p><p>
    The Designate barclamp relies heavily on the DNS barclamp and expects
    it to be applied without any failures.
  </p><div id="id-1.3.5.4.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    In order to deploy Designate, at least one node is necessary in the DNS
    barclamp that is not the admin node. The admin node is not added to the
    public network. So another node is needed that can be attached to the public
    network and appear in the designate default pool.
   </p><p>
    We recommend that DNS services are running in a cluster in highly available
    deployments where Designate services are running in a cluster.
    For example, in a typical HA deployment where the controllers
    are deployed in a 3-node cluster, the DNS barclamp should be applied to all
    the controllers, in the same manner as Designate.
   </p></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.8.5.1"><span class="term ">designate-server role</span></dt><dd><p>
          Installs the Designate server packages and configures the mini-dns (mdns)
          service required by Designate.
        </p></dd><dt id="id-1.3.5.4.8.5.2"><span class="term ">designate-worker role</span></dt><dd><p>
        Configures a Designate worker on the selected nodes. Designate uses the
        workers to distribute its workload.
        </p></dd></dl></div><p>
    <code class="literal">Designate Sink</code> is an optional service and is not configured as part
    of this barclamp.
  </p><p>
    Designate uses pool(s) over which it can distribute zones and
    records. Pools can have varied configuration. Any misconfiguration can lead to
    information leakage.
  </p><p>
    The Designate barclamp creates default Bind9 pool out of the box, which can be
    modified later as needed. The default Bind9 pool configuration is created by Crowbar
    on a node with <code class="literal">designate-server</code> role in
    <code class="filename">/etc/designate/pools.crowbar.yaml</code>. You can copy
    this file and edit it according to your requirements. Then provide this
    configuration to Designate using the command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>designate-manage pool update --file /etc/designate/pools.crowbar.yaml</pre></div><p>
    The <code class="literal">dns_domain</code> specified in Neutron configuration in <code class="literal">[designate]</code> section
    is the default Zone where DNS records for Neutron resources are created via
    Neutron-Designate integration. If this is desired, you have to create this zone
    explicitly using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack zone create &lt; email &gt; &lt; dns_domain &gt;</pre></div><p>
   Editing the Designate proposal:
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/designate-edit-proposal.png" target="_blank"><img src="images/designate-edit-proposal.png" width="" alt="Edit Designate Proposal" /></a></div></div><div class="sect2 " id="sec-depl-ostack-designate-powerdns"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using PowerDNS Backend</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-designate-powerdns">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-designate-powerdns</li></ul></div></div></div></div><p>
    Designate uses Bind9 backend by default. It is also possible to
    use PowerDNS backend in addition to, or as an alternative, to Bind9 backend.
    To do so PowerDNS must be manually deployed as The Designate barclamp
    currently does not provide any facility to automatically install and
    configure PowerDNS. This section outlines the steps to deploy PowerDNS
    backend.
   </p><div id="id-1.3.5.4.8.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     If PowerDNS is already deployed, you may skip the
     <a class="xref" href="#sec-depl-ostack-install-powerdns" title="12.1.1.1. Install PowerDNS">Section 12.1.1.1, “Install PowerDNS”</a> section and jump to
     the <a class="xref" href="#sec-depl-ostack-designate-use-powerdns-backend" title="12.1.1.2. Configure Designate To Use PowerDNS Backend">Section 12.1.1.2, “Configure Designate To Use PowerDNS Backend”</a>
     section.
    </p></div><div class="sect3 " id="sec-depl-ostack-install-powerdns"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install PowerDNS</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-install-powerdns">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-install-powerdns</li></ul></div></div></div></div><p>
     Follow these steps to install and configure PowerDNS on a Crowbar node.
     Keep in mind that PowerDNS must be deployed with MySQL backend.
    </p><div id="id-1.3.5.4.8.14.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      We recommend that PowerDNS are running in a cluster in highly
      availability deployments where Designate services are running in a
      cluster. For example, in a typical HA deployment
      where the controllers are deployed in a 3-node cluster, PowerDNS should
      be running on all the controllers, in the same manner as Designate.
     </p></div><div class="procedure " id="pro-deploy-powerdns"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Install PowerDNS packages.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install pdns pdns-backend-mysql</pre></div></li><li class="step "><p>
       Edit <code class="literal">/etc/pdns/pdns.conf</code> and provide these options:
       (See
       <a class="link" href="https://doc.powerdns.com/authoritative/settings.html" target="_blank">https://doc.powerdns.com/authoritative/settings.html</a> for a complete reference).
      </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.8.14.4.4.2.2.1"><span class="term ">api</span></dt><dd><p>
          Set it to <code class="literal">yes</code> to enable Web service Rest API.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.2"><span class="term ">api-key</span></dt><dd><p>
          Static Rest API access key. Use a secure random string here.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.3"><span class="term ">launch</span></dt><dd><p>
          Must set to <code class="literal">gmysql</code> to use MySQL backend.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.4"><span class="term ">gmysql-host</span></dt><dd><p>
          Hostname (i.e. FQDN) or IP address of the MySQL server.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.5"><span class="term ">gmysql-user</span></dt><dd><p>
          MySQL user which have full access to the PowerDNS database.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.6"><span class="term ">gmysql-password</span></dt><dd><p>
          Password for the MySQL user.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.7"><span class="term ">gmysql-dbname</span></dt><dd><p>
          MySQL database name for PowerDNS.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.8"><span class="term ">local-port</span></dt><dd><p>
          Port number where PowerDNS is listening for upcoming requests.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.9"><span class="term ">setgid</span></dt><dd><p>
          The group where the PowerDNS process is running under.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.10"><span class="term ">setuid</span></dt><dd><p>
          The user where the PowerDNS process is running under.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.11"><span class="term ">webserver</span></dt><dd><p>
          Must set to <code class="literal">yes</code> to enable web service RestAPI.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.12"><span class="term ">webserver-address</span></dt><dd><p>
          Hostname (FQDN) or IP address of the PowerDNS web service.
         </p></dd><dt id="id-1.3.5.4.8.14.4.4.2.2.13"><span class="term ">webserver-allow-from</span></dt><dd><p>
          List of IP addresses (IPv4 or IPv6) of the nodes that are permitted
          to talk to the PowerDNS web service. These must include the IP
          address of the Designate worker nodes.
         </p></dd></dl></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen">api=yes
api-key=Sfw234sDFw90z
launch=gmysql
gmysql-host=mysql.acme.com
gmysql-user=powerdns
gmysql-password=SuperSecured123
gmysql-dbname=powerdns
local-port=54
setgid=pdns
setuid=pdns
webserver=yes
webserver-address=192.168.124.83
webserver-allow-from=0.0.0.0/0,::/0</pre></div></li><li class="step "><p>
       Login to MySQL from a Crowbar MySQL node and create the PowerDNS database
       and the user which has full access to the PowerDNS database. Remember,
       the database name, username, and password must match
       <code class="literal">gmysql-dbname</code>, <code class="literal">gmysql-user</code>,
       and <code class="literal">gmysql-password</code> that were specified above
       respectively.
      </p><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 20075
Server version: 10.2.29-MariaDB-log SUSE package

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE powerdns;
Query OK, 1 row affected (0.01 sec)

MariaDB [(none)]&gt; GRANT ALL ON powerdns.* TO 'powerdns'@'localhost' IDENTIFIED BY 'SuperSecured123';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL ON powerdns.* TO 'powerdns'@'192.168.124.83' IDENTIFIED BY 'SuperSecured123';
Query OK, 0 rows affected, 1 warning (0.02 sec)

MariaDB [(none)]&gt; FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.01 sec)

MariaDB [(none)]&gt; exit
Bye</pre></div></li><li class="step "><p>
       Create a MySQL schema file, named <code class="literal">powerdns-schema.sql</code>,
       with the following content:
      </p><div class="verbatim-wrap"><pre class="screen">/*
 SQL statements to create tables in designate_pdns DB.
 Note: This file is taken as is from:
 https://raw.githubusercontent.com/openstack/designate/master/devstack/designate_plugins/backend-pdns4-mysql-db.sql
*/
CREATE TABLE domains (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255) NOT NULL,
  master                VARCHAR(128) DEFAULT NULL,
  last_check            INT DEFAULT NULL,
  type                  VARCHAR(6) NOT NULL,
  notified_serial       INT DEFAULT NULL,
  account               VARCHAR(40) DEFAULT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE UNIQUE INDEX name_index ON domains(name);


CREATE TABLE records (
  id                    INT AUTO_INCREMENT,
  domain_id             INT DEFAULT NULL,
  name                  VARCHAR(255) DEFAULT NULL,
  type                  VARCHAR(10) DEFAULT NULL,
  -- Changed to "TEXT", as VARCHAR(65000) is too big for most MySQL installs
  content               TEXT DEFAULT NULL,
  ttl                   INT DEFAULT NULL,
  prio                  INT DEFAULT NULL,
  change_date           INT DEFAULT NULL,
  disabled              TINYINT(1) DEFAULT 0,
  ordername             VARCHAR(255) BINARY DEFAULT NULL,
  auth                  TINYINT(1) DEFAULT 1,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX nametype_index ON records(name,type);
CREATE INDEX domain_id ON records(domain_id);
CREATE INDEX recordorder ON records (domain_id, ordername);


CREATE TABLE supermasters (
  ip                    VARCHAR(64) NOT NULL,
  nameserver            VARCHAR(255) NOT NULL,
  account               VARCHAR(40) NOT NULL,
  PRIMARY KEY (ip, nameserver)
) Engine=InnoDB;


CREATE TABLE comments (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  name                  VARCHAR(255) NOT NULL,
  type                  VARCHAR(10) NOT NULL,
  modified_at           INT NOT NULL,
  account               VARCHAR(40) NOT NULL,
  -- Changed to "TEXT", as VARCHAR(65000) is too big for most MySQL installs
  comment               TEXT NOT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX comments_domain_id_idx ON comments (domain_id);
CREATE INDEX comments_name_type_idx ON comments (name, type);
CREATE INDEX comments_order_idx ON comments (domain_id, modified_at);


CREATE TABLE domainmetadata (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  kind                  VARCHAR(32),
  content               TEXT,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX domainmetadata_idx ON domainmetadata (domain_id, kind);


CREATE TABLE cryptokeys (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  flags                 INT NOT NULL,
  active                BOOL,
  content               TEXT,
  PRIMARY KEY(id)
) Engine=InnoDB;

CREATE INDEX domainidindex ON cryptokeys(domain_id);


CREATE TABLE tsigkeys (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255),
  algorithm             VARCHAR(50),
  secret                VARCHAR(255),
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE UNIQUE INDEX namealgoindex ON tsigkeys(name, algorithm);</pre></div></li><li class="step "><p>
       Create the PowerDNS schema for the database using <code class="literal">mysql</code>
       CLI. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql powerdns &lt; powerdns-schema.sql</pre></div></li><li class="step "><p>
       Enable <code class="literal">pdns</code> systemd service.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable pdns
<code class="prompt user">root # </code>systemctl start pdns</pre></div><p>
       If <code class="literal">pdns</code> is successfully running, you should see the
       following logs by running <code class="literal">journalctl -u pdns</code> command.
      </p><div class="verbatim-wrap"><pre class="screen">Feb 07 01:44:12 d52-54-77-77-01-01 systemd[1]: Started PowerDNS Authoritative Server.
Feb 07 01:44:12 d52-54-77-77-01-01 pdns_server[21285]: Done launching threads, ready to distribute questions</pre></div></li></ol></div></div></div><div class="sect3 " id="sec-depl-ostack-designate-use-powerdns-backend"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Designate To Use PowerDNS Backend</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-designate-use-powerdns-backend">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-designate-use-powerdns-backend</li></ul></div></div></div></div><p>
     Configure Designate to use PowerDNS backend by appending the PowerDNS
     servers to <code class="literal">/etc/designate/pools.crowbar.yaml</code> file
     on a Designate worker node.
    </p><div id="id-1.3.5.4.8.14.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      If we are replacing Bind9 backend with PowerDNS backend, make sure to
      remove the <code class="literal">bind9</code> entries from
      <code class="literal">/etc/designate/pools.crowbar.yaml</code>.
     </p><p>
      In HA deployment, there should be multiple PowerDNS entries.
     </p><p>
      Also, make sure the <code class="literal">api_token</code> matches the
      <code class="literal">api-key</code> that was specified in the
      <code class="literal">/etc/pdns/pdns.conf</code> file earlier.
     </p></div><p>
     Append the PowerDNS entries to the end of
     <code class="literal">/etc/designate/pools.crowbar.yaml</code>. For example:
    </p><div class="verbatim-wrap"><pre class="screen">---
- name: default-bind
  description: Default BIND9 Pool
  id: 794ccc2c-d751-44fe-b57f-8894c9f5c842
  attributes: {}
  ns_records:
  - hostname: public-d52-54-77-77-01-01.virtual.cloud.suse.de.
    priority: 1
  - hostname: public-d52-54-77-77-01-02.virtual.cloud.suse.de.
    priority: 1
  nameservers:
  - host: 192.168.124.83
    port: 53
  - host: 192.168.124.81
    port: 53
  also_notifies: []
  targets:
  - type: bind9
    description: BIND9 Server
    masters:
    - host: 192.168.124.83
      port: 5354
    - host: 192.168.124.82
      port: 5354
    - host: 192.168.124.81
      port: 5354
    options:
      host: 192.168.124.83
      port: 53
      rndc_host: 192.168.124.83
      rndc_port: 953
      rndc_key_file: "/etc/designate/rndc.key"
  - type: bind9
    description: BIND9 Server
    masters:
    - host: 192.168.124.83
      port: 5354
    - host: 192.168.124.82
      port: 5354
    - host: 192.168.124.81
      port: 5354
    options:
      host: 192.168.124.81
      port: 53
      rndc_host: 192.168.124.81
      rndc_port: 953
      rndc_key_file: "/etc/designate/rndc.key"
  - type: pdns4
    description: PowerDNS4 DNS Server
    masters:
      - host: 192.168.124.83
        port: 5354
      - host: 192.168.124.82
        port: 5354
      - host: 192.168.124.81
        port: 5354
    options:
      host: 192.168.124.83
      port: 54
      api_endpoint: http://192.168.124.83:8081
      api_token: Sfw234sDFw90z</pre></div><p>
     Update the pools using <code class="literal">designate-manage</code> CLI.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>designate-manage pool update --file /etc/designate/pools.crowbar.yaml</pre></div><p>
     Once Designate sync up with PowerDNS, you should see the domains in the
     PowerDNS database which reflects the zones in Designate.
    </p><div id="id-1.3.5.4.8.14.5.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      It make take a few minutes for Designate to sync with PowerDNS.
     </p></div><p>
     We can verify that the domains are successfully sync up with Designate
     by inpsecting the <code class="literal">domains</code> table in the database.
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql powerdns
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 21131
Server version: 10.2.29-MariaDB-log SUSE package

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [powerdns]&gt; select * from domains;
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
| id | name    | master                                                       | last_check | type  | notified_serial | account |
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
|  1 | foo.bar | 192.168.124.81:5354 192.168.124.82:5354 192.168.124.83:5354  |       NULL | SLAVE |            NULL |         |
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
1 row in set (0.00 sec)</pre></div></div></div></div><div class="sect1 " id="sec-depl-ostack-pacemaker"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Pacemaker (Optional, HA Setup Only)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-pacemaker">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-pacemaker</li></ul></div></div></div></div><p>
   To make the SUSE <span class="productname">OpenStack</span> Cloud controller functions and the Compute Nodes highly
   available, set up one or more clusters by deploying Pacemaker (see
   <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a separate proposal needs to
   be created for each cluster.
  </p><p>
   Deploying Pacemaker is optional. In case you do not want to deploy it, skip
   this section and start the node deployment by deploying the database as
   described in <a class="xref" href="#sec-depl-ostack-db" title="12.3. Deploying the Database">Section 12.3, “Deploying the Database”</a>.
  </p><div id="id-1.3.5.4.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Number of Cluster Nodes</h6><p>
    To set up a cluster, at least two nodes are required.  See <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a> for
    more information.
   </p></div><p>
   To create a proposal, go to <span class="guimenu ">Barclamps</span> › <span class="guimenu ">OpenStack</span> and click <span class="guimenu ">Edit</span>
   for the Pacemaker barclamp. A drop-down box where you can enter a name and a
   description for the proposal opens. Click <span class="guimenu ">Create</span> to open
   the configuration screen for the proposal.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_pacemaker_proposal.png" target="_blank"><img src="images/depl_barclamp_pacemaker_proposal.png" width="" alt="Create Pacemaker Proposal" /></a></div></div><div id="ann-depl-ostack-pacemaker-prop-name" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Proposal Name</h6><p>
    The name you enter for the proposal will be used to generate host names for
    the virtual IP addresses of HAProxy. By default, the names follow this
    scheme:
   </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">cluster-<em class="replaceable ">PROPOSAL_NAME</em>.<em class="replaceable ">FQDN</em></code>
    (for the internal name)</td></tr><tr><td><code class="literal">public-cluster-<em class="replaceable ">PROPOSAL_NAME</em>.<em class="replaceable ">FQDN</em></code>
    (for the public name)</td></tr></table><p>
    For example, when <em class="replaceable ">PROPOSAL_NAME</em> is set to
    <code class="literal">data</code>, this results in the following names:
   </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">cluster-data.example.com</code>
    </td></tr><tr><td><code class="literal">public-cluster-data.example.com</code>
    </td></tr></table><p>
    For requirements regarding SSL encryption and certificates, see
    <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a>.
   </p></div><p>
   The following options are configurable in the Pacemaker configuration
   screen:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.9.9.1"><span class="term ">Transport for Communication</span></dt><dd><p>
      Choose a technology used for cluster communication. You can choose
      between <span class="guimenu ">Multicast (UDP)</span>, sending a message to multiple
      destinations, or <span class="guimenu ">Unicast (UDPU)</span>, sending a message to
      a single destination. By default unicast is used.
     </p></dd><dt id="id-1.3.5.4.9.9.2"><span class="term "><span class="guimenu ">Policy when cluster does not have quorum</span>
    </span></dt><dd><p>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <span class="quote">“<span class="quote ">cluster partition</span>”</span> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes is
      defined to have <span class="quote">“<span class="quote ">quorum</span>”</span>.
     </p><p>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See
      <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-config-basics-global-quorum" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-config-basics-global-quorum</a>,
      for details.
     </p><p>
      The recommended setting is to choose <span class="guimenu ">Stop</span>. However,
      <span class="guimenu ">Ignore</span> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the other
      node fails. For clusters using shared resources, choosing
      <span class="guimenu ">freeze</span> may be used to ensure that these resources
      continue to be available.
     </p></dd><dt id="vle-pacemaker-barcl-stonith"><span class="term ">STONITH: Configuration mode for STONITH
    </span></dt><dd><p>
      <span class="quote">“<span class="quote ">Misbehaving</span>”</span> nodes in a cluster are shut down to prevent
      them from causing trouble. This mechanism is called STONITH
      (<span class="quote">“<span class="quote ">Shoot the other node in the head</span>”</span>). STONITH can be
      configured in a variety of ways, refer to
      <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-ha-fencing" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-ha-fencing</a>
      for details. The following configuration options exist:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.9.9.3.2.2.1"><span class="term "><span class="guimenu ">Configured manually</span>
       </span></dt><dd><p>
         STONITH will not be configured when deploying the barclamp. It needs
         to be configured manually as described in
         <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-ha-fencing" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-ha-fencing</a>.
         For experts only.
        </p></dd><dt id="id-1.3.5.4.9.9.3.2.2.2"><span class="term "><span class="guimenu ">Configured with IPMI data from the IPMI barclamp</span>
       </span></dt><dd><p>
         Using this option automatically sets up STONITH with data received
         from the IPMI barclamp. Being able to use this option requires that
         IPMI is configured for all cluster nodes. This should be done by
         default. To check or change the IPMI deployment, go to <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span> › <span class="guimenu ">IPMI</span> › <span class="guimenu ">Edit</span>. Also
         make sure the <span class="guimenu ">Enable BMC</span> option is set to
         <span class="guimenu ">true</span> on this barclamp.
        </p><div id="id-1.3.5.4.9.9.3.2.2.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: STONITH Devices Must Support IPMI</h6><p>
          To configure STONITH with the IPMI data, <span class="emphasis"><em>all</em></span>
          STONITH devices must support IPMI. Problems with this setup may
          occur with IPMI implementations that are not strictly standards
          compliant. In this case it is recommended to set up STONITH with
          STONITH block devices (SBD).
         </p></div></dd><dt id="id-1.3.5.4.9.9.3.2.2.3"><span class="term "><span class="guimenu ">Configured with STONITH Block Devices (SBD)</span>
       </span></dt><dd><p>
         This option requires manually setting up shared storage and a watchdog
         on the cluster nodes before applying the proposal. To do so, proceed
         as follows:
        </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
           Prepare the shared storage. The path to the shared storage device
           must be persistent and consistent across all nodes in the cluster.
           The SBD device must not use host-based RAID or cLVM2.
          </p></li><li class="listitem "><p>
           Install the package <code class="systemitem">sbd</code> on
           all cluster nodes.
          </p></li><li class="listitem "><p>
           Initialize the SBD device with by running the following command.
           Make sure to replace
           <code class="filename">/dev/<em class="replaceable ">SBD</em></code> with the
           path to the shared storage device.
          </p><div class="verbatim-wrap"><pre class="screen">sbd -d /dev/<em class="replaceable ">SBD</em> create</pre></div><p>
           Refer to
           <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#pro-ha-storage-protect-sbd-create" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#pro-ha-storage-protect-sbd-create</a>
           for details.
          </p></li></ol></div><p>
         In <span class="guimenu ">Kernel module for watchdog</span>, specify the
         respective kernel module to be used. Find the most commonly used
         watchdog drivers in the following table:
        </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><thead><tr><th>Hardware</th><th>Driver</th></tr></thead><tbody><tr><td>HP</td><td><code class="systemitem">hpwdt</code>
            </td></tr><tr><td>Dell, Fujitsu, Lenovo (Intel TCO)</td><td><code class="systemitem">iTCO_wdt</code>
            </td></tr><tr><td>Xen VM (DomU)</td><td><code class="systemitem">xen_xdt</code>
            </td></tr><tr><td>Generic</td><td><code class="systemitem">softdog</code>
            </td></tr></tbody></table></div><p>
         If your hardware is not listed above, either ask your hardware vendor
         for the right name or check the following directory for a list of
         choices:
         <code class="filename">/lib/modules/<em class="replaceable ">KERNEL_VERSION</em>/kernel/drivers/watchdog</code>.
        </p><p>
         Alternatively, list the drivers that have been installed with your
         kernel version:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">rpm</code> -ql kernel-<em class="replaceable ">VERSION</em> | <code class="command">grep</code> watchdog</pre></div><p>
         If the nodes need different watchdog modules, leave the text box
         empty.
        </p><p>
         After the shared storage has been set up, specify the path using the
         <span class="quote">“<span class="quote ">by-id</span>”</span> notation
         (<code class="filename">/dev/disk/by-id/<em class="replaceable ">DEVICE</em></code>).
         It is possible to specify multiple paths as a comma-separated list.
        </p><p>
         Deploying the barclamp will automatically complete the SBD setup on the
         cluster nodes by starting the SBD daemon and configuring the fencing
         resource.
        </p></dd><dt id="id-1.3.5.4.9.9.3.2.2.4"><span class="term "><span class="guimenu ">
         Configured with one shared resource for the whole cluster
        </span>
       </span></dt><dd><p>
         All nodes will use the identical configuration. Specify the
         <span class="guimenu ">Fencing Agent</span> to use and enter
         <span class="guimenu ">Parameters</span> for the agent.
        </p><p>
         To get a list of STONITH devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <code class="command">stonith -L</code>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable ">agent</em> -n</pre></div></dd><dt id="id-1.3.5.4.9.9.3.2.2.5"><span class="term "><span class="guimenu ">Configured with one resource per node</span>
       </span></dt><dd><p>
         All nodes in the cluster use the same <span class="guimenu ">Fencing
         Agent</span>, but can be configured with different parameters. This
         setup is, for example, required when nodes are in different chassis
         and therefore need different IPMI parameters.
        </p><p>
         To get a list of STONITH devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <code class="command">stonith -L</code>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable ">agent</em> -n</pre></div></dd><dt id="id-1.3.5.4.9.9.3.2.2.6"><span class="term "><span class="guimenu ">Configured for nodes running in libvirt</span>
       </span></dt><dd><p>
         Use this setting for completely virtualized test installations. This
         option is not supported.
        </p></dd></dl></div></dd><dt id="var-depl-ostack-pacemaker-corosync-fencing"><span class="term ">STONITH: Do not start corosync on boot after fencing</span></dt><dd><p>
      With STONITH, Pacemaker clusters with two nodes may sometimes hit an
      issue known as STONITH deathmatch where each node kills the other one,
      resulting in both nodes rebooting all the time. Another similar issue in
      Pacemaker clusters is the fencing loop, where a reboot caused by
      STONITH will not be enough to fix a node and it will be fenced again
      and again.
     </p><p>
      This setting can be used to limit these issues. When set to
      <span class="guimenu ">true</span>, a node that has not been properly shut down or
      rebooted will not start the services for Pacemaker on boot. Instead, the
      node will wait for action from the SUSE <span class="productname">OpenStack</span> Cloud operator. When set to
      <span class="guimenu ">false</span>, the services for Pacemaker will always be
      started on boot. The <span class="guimenu ">Automatic</span> value is used to have
      the most appropriate value automatically picked: it will be
      <span class="guimenu ">true</span> for two-node clusters (to avoid STONITH
      deathmatches), and <span class="guimenu ">false</span> otherwise.
     </p><p>
      When a node boots but not starts corosync because of this setting, then
      the node's status is in the <span class="guimenu ">Node Dashboard</span> is set to
      "<code class="literal">Problem</code>" (red dot).
      
     </p></dd><dt id="id-1.3.5.4.9.9.5"><span class="term ">Mail Notifications: Enable Mail Notifications</span></dt><dd><p>
      Get notified of cluster node failures via e-mail. If set to
      <span class="guimenu ">true</span>, you need to specify which <span class="guimenu ">SMTP
      Server</span> to use, a prefix for the mails' subject and sender and
      recipient addresses. Note that the SMTP server must be accessible by the
      cluster nodes.
     </p></dd><dt id="id-1.3.5.4.9.9.6"><span class="term "><span class="guimenu ">HAProxy: Public name for public virtual IP</span>
    </span></dt><dd><p>
      The public name is the host name that will be used instead of the
      generated public name (see
      <a class="xref" href="#ann-depl-ostack-pacemaker-prop-name" title="Important: Proposal Name">Important: Proposal Name</a>) for the public
      virtual IP address of HAProxy. (This is the case when registering
      public endpoints, for example). Any name specified here needs to be
      resolved by a name server placed outside of the SUSE <span class="productname">OpenStack</span> Cloud network.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.9.10"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_pacemaker.png" target="_blank"><img src="images/depl_barclamp_pacemaker.png" width="" alt="The Pacemaker Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.1: </span><span class="name">The Pacemaker Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.9.10">#</a></h6></div></div><p>
   The Pacemaker component consists of the following roles. Deploying the
   <span class="guimenu ">hawk-server</span> role is optional:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.9.12.1"><span class="term "><span class="guimenu ">pacemaker-cluster-member</span>
    </span></dt><dd><p>
      Deploy this role on all nodes that should become member of the cluster.
     </p></dd><dt id="id-1.3.5.4.9.12.2"><span class="term "><span class="guimenu ">hawk-server</span>
    </span></dt><dd><p>
      Deploying this role is optional. If deployed, sets up the Hawk Web
      interface which lets you monitor the status of the cluster. The Web
      interface can be accessed via
      <code class="literal">https://<em class="replaceable ">IP-ADDRESS</em>:7630</code>. The
      default hawk credentials are username <code class="literal">hacluster</code>, password
      <code class="literal">crowbar</code>.
      </p><p>
      The password is visible and editable in the <span class="guimenu ">Custom</span> view of the Pacemaker barclamp, and also in the <code class="literal">"corosync":</code> section of the
      <span class="guimenu ">Raw</span> view.
  </p><p>
      Note that the GUI on SUSE <span class="productname">OpenStack</span> Cloud can only be used to monitor the cluster
      status and not to change its configuration.
     </p><p>
      <span class="guimenu ">hawk-server</span> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </p></dd><dt id="id-1.3.5.4.9.12.3"><span class="term "><span class="guimenu ">pacemaker-remote</span>
    </span></dt><dd><p>
      Deploy this role on all nodes that should become members of the
      Compute Nodes cluster. They will run as Pacemaker remote nodes that are
      controlled by the cluster, but do not affect quorum. Instead of the
      complete cluster stack, only the <code class="literal">pacemaker-remote</code>
      component will be installed on this nodes.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.9.13"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_pacemaker_node_deployment.png" target="_blank"><img src="images/depl_barclamp_pacemaker_node_deployment.png" width="" alt="The Pacemaker Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.2: </span><span class="name">The Pacemaker Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.9.13">#</a></h6></div></div><p>
   After a cluster has been successfully deployed, it is listed under
   <span class="guimenu ">Available Clusters</span> in the <span class="guimenu ">Deployment</span>
   section and can be used for role deployment like a regular node.
  </p><div id="id-1.3.5.4.9.15" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Deploying Roles on Single Cluster Nodes</h6><p>
    When using clusters, roles from other barclamps must never be deployed to
    single nodes that are already part of a cluster. The only exceptions from
    this rule are the following roles:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      cinder-volume
     </p></li><li class="listitem "><p>
      swift-proxy + swift-dispersion
     </p></li><li class="listitem "><p>
      swift-ring-compute
     </p></li><li class="listitem "><p>
      swift-storage
     </p></li></ul></div></div><div id="id-1.3.5.4.9.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Service Management on the Cluster</h6><p>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <span class="emphasis"><em>never</em></span> manually start or stop
    an HA-managed service, nor configure it to start on boot. Services may only
    be started or stopped by using the cluster management tools Hawk or the crm
    shell. See
    <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-config-basics-resources" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-config-basics-resources</a>
    for more information.
   </p></div><div id="id-1.3.5.4.9.17" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Testing the Cluster Setup</h6><p>
    To check whether all cluster resources are running, either use the Hawk
    Web interface or run the command <code class="command">crm_mon</code>
    <code class="option">-1r</code>. If it is not the case, clean up the respective
    resource with <code class="command">crm</code> <code class="option">resource</code>
    <code class="option">cleanup</code> <em class="replaceable ">RESOURCE</em> , so it gets
    respawned.
   </p><p>
    Also make sure that STONITH correctly works before continuing with the
    SUSE <span class="productname">OpenStack</span> Cloud setup. This is especially important when having chosen a STONITH
    configuration requiring manual setup. To test if STONITH works, log in to
    a node on the cluster and run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">pkill -9 corosync</pre></div><p>
    In case STONITH is correctly configured, the node will reboot.
   </p><p>
    Before testing on a production cluster, plan a maintenance window in case
    issues should arise.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-db"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Database</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-db">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-db</li></ul></div></div></div></div><p>
   The very first service that needs to be deployed is the
   <span class="guimenu ">Database</span>. The database component is using MariaDB and
   is used by all other components. It must be installed on a Control Node. The
   Database can be made highly available by deploying it on a cluster.
  </p><p>
   The only attribute you may change is the maximum number of database
   connections (<span class="guimenu ">Global Connection Limit</span>). The default value
   should usually work—only change it for large deployments in case the
   log files show database connection failures.
  </p><div class="figure" id="id-1.3.5.4.10.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_database.png" target="_blank"><img src="images/depl_barclamp_database.png" width="" alt="The Database Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.3: </span><span class="name">The Database Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.10.5">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-db-mariadb"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying MariaDB</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-db-mariadb">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-db-mariadb</li></ul></div></div></div></div><p>Deploying the database requires the use of MariaDB</p><div id="id-1.3.5.4.10.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: MariaDB and HA</h6><p>
        MariaDB back end features full HA support based on the Galera
        clustering technology. The HA setup requires an odd number of
        nodes. The recommended number of nodes is 3.
      </p></div><div class="sect3 " id="sec-depl-ostack-db-mariadb-ssl"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SSL Configuration</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-db-mariadb-ssl">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-db-mariadb-ssl</li></ul></div></div></div></div><p>
        SSL can be enabled with either a stand-alone or cluster deployment.
        The replication traffic between database nodes is not encrypted,
        whilst traffic between the database server(s) and clients are, so
        a separate network for the database servers is recommended.
      </p><p>
        Certificates can be provided, or the barcamp can generate self-signed
        certificates. The certificate filenames are configurable in the
        barclamp, and the directories <code class="literal">/etc/mysql/ssl/certs</code>
        and <code class="literal">/etc/mysql/ssl/private</code> to use the defaults will
        need to be created before the barclamp is applied. The CA certificate
        and the certificate for MariaDB to use both go into
        <code class="literal">/etc/mysql/ssl/certs</code>. The appropriate private key
        for the certificate is placed into the
        <code class="literal">/etc/mysql/ssl/private</code> directory. As long as the
        files are readable when the barclamp is deployed, permissions can be
        tightened after a successful deployment once the appropriate UNIX
        groups exist.
      </p><p>
        The Common Name (CN) for the SSL certificate must be <code class="literal">fully
        qualified server name</code> for single host deployments, and
        cluster-<code class="literal">cluster name</code>.<code class="literal">full domain
        name</code> for cluster deployments.
      </p><div id="id-1.3.5.4.10.6.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Certificate validation errors</h6><p>
          If certificate validation errors are causing issues with deploying
          other barclamps (for example, when creating databases or users) you
          can check the configuration with
          <code class="command">mysql --ssl-verify-server-cert</code> which will perform
          the same verification that Crowbar does when connecting to the
          database server.
        </p></div><p>
        If certificates are supplied, the CA certificate and its full trust
        chain must be in the <code class="literal">ca.pem</code> file. The certificate
        must be trusted by the machine (or all cluster members in a cluster
        deployment), and it must be available on all client machines —
        IE, if the OpenStack services are deployed on separate machines or
        cluster members they will all require the CA certificate to be in
        <code class="literal">/etc/mysql/ssl/certs</code> as well as trusted by the
        machine.
      </p></div><div class="sect3 " id="id-1.3.5.4.10.6.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MariaDB Configuration Options</span> <a title="Permalink" class="permalink" href="#id-1.3.5.4.10.6.5">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="figure" id="id-1.3.5.4.10.6.5.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_database_mariadb.png" target="_blank"><img src="images/depl_barclamp_database_mariadb.png" width="" alt="MariaDB Configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.4: </span><span class="name">MariaDB Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.10.6.5.2">#</a></h6></div></div><p>
      The following configuration settings are available via the <span class="guimenu ">Database</span> barclamp
      graphical interface:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.10.6.5.4.1"><span class="term ">
          Datadir
        </span></dt><dd><p>
            Path to a directory for storing database data.
          </p></dd><dt id="id-1.3.5.4.10.6.5.4.2"><span class="term ">
          Maximum Number of Simultaneous Connections
        </span></dt><dd><p>
            The maximum number of simultaneous client connections.
          </p></dd><dt id="id-1.3.5.4.10.6.5.4.3"><span class="term ">
          Number of days after the binary logs can be automatically removed
        </span></dt><dd><p>
            A period after which the binary logs are removed.
          </p></dd><dt id="id-1.3.5.4.10.6.5.4.4"><span class="term ">
          Slow Query Logging
        </span></dt><dd><p>
            When enabled, all queries that take longer than usual to execute
            are logged to a separate log file (by default, it's
            <code class="filename">/var/log/mysql/mysql_slow.log</code>). This can be
            useful for debugging.
          </p></dd></dl></div><div id="id-1.3.5.4.10.6.5.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: MariaDB Deployment Restriction</h6><p>
        When MariaDB is used as the database back end, the <span class="guimenu ">monasca-server</span>
        role cannot be deployed to the node with the
        <span class="guimenu ">database-server</span> role. These two roles cannot
        coexist due to the fact that
        Monasca uses its own MariaDB instance.
      </p></div></div></div></div><div class="sect1 " id="sec-depl-ostack-rabbit"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying RabbitMQ</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbit">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbit</li></ul></div></div></div></div><p>
   The RabbitMQ messaging system enables services to communicate with the other
   nodes via Advanced Message Queue Protocol (AMQP). Deploying it is mandatory.
   RabbitMQ needs to be installed on a Control Node. RabbitMQ can be made highly
   available by deploying it on a cluster. We recommend not changing the
   default values of the proposal's attributes.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.11.3.1"><span class="term "><span class="guimenu ">Virtual Host</span>
    </span></dt><dd><p>
      Name of the default virtual host to be created and used by the RabbitMQ
      server (<code class="literal">default_vhost</code> configuration option in
      <code class="filename">rabbitmq.config</code>).
     </p></dd><dt id="id-1.3.5.4.11.3.2"><span class="term ">Port</span></dt><dd><p>
      Port the RabbitMQ server listens on (<code class="literal">tcp_listeners</code>
      configuration option in <code class="filename">rabbitmq.config</code>).
     </p></dd><dt id="id-1.3.5.4.11.3.3"><span class="term ">User</span></dt><dd><p>
      RabbitMQ default user (<code class="literal">default_user</code> configuration
      option in <code class="filename">rabbitmq.config</code>).
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.11.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_rabbitmq.png" target="_blank"><img src="images/depl_barclamp_rabbitmq.png" width="" alt="The RabbitMQ Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.5: </span><span class="name">The RabbitMQ Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.11.4">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-rabbit-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for RabbitMQ</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbit-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbit-ha</li></ul></div></div></div></div><p>
    To make RabbitMQ highly available, deploy it on a cluster instead of a
    single Control Node. This also requires shared storage for the cluster that
    hosts the RabbitMQ data. We recommend using a dedicated cluster to deploy
    RabbitMQ together with the database,
    since both components require shared storage.
   </p><p>
    Deploying RabbitMQ on a cluster makes an additional <span class="guimenu ">High
    Availability</span> section available in the
    <span class="guimenu ">Attributes</span> section of the proposal. Configure the
    <span class="guimenu ">Storage Mode</span> in this section.
   </p></div><div class="sect2 " id="sec-depl-ostack-rabbitmq-ssl"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SSL Configuration for RabbitMQ</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbitmq-ssl">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbitmq-ssl</li></ul></div></div></div></div><p>
    The RabbitMQ barclamp supports securing traffic via SSL. This is similar to
    the SSL support in other barclamps, but with these differences:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      RabbitMQ can listen on two ports at the same time, typically port 5672
      for unsecured and port 5671 for secured traffic.
     </p></li><li class="listitem "><p>
      The Ceilometer pipeline for <span class="productname">OpenStack</span> Swift cannot be passed
      SSL-related parameters. When SSL is enabled for RabbitMQ the Ceilometer
      pipeline in Swift is turned off, rather than sending it over an
      unsecured channel.
     </p></li></ul></div><p>
The following steps are the fastest way to set up and test a new SSL certificate authority (CA).
</p><div class="procedure " id="pro-rabbitmq-test"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
  In the RabbitMQ barclamp set <span class="guimenu ">Enable SSL</span> to <span class="guimenu ">true</span>, and <span class="guimenu ">Generate (self-signed) certificates (implies insecure)</span>
    to <code class="literal">true</code>, then apply the barclamp. The barclamp will create a new CA, enter the correct settings in <code class="filename">/etc/rabbitmq/rabbitmq.config</code>, and start RabbitMQ.
</p></li><li class="step "><p>
Test your new CA with OpenSSL, substituting the hostname of your control node:</p><div class="verbatim-wrap"><pre class="screen">openssl s_client -connect d52-54-00-59-e5-fd:5671
[...]
Verify return code: 18 (self signed certificate)</pre></div><p>
  This outputs a lot of information, including a copy of the server's public certificate, protocols, ciphers, and the chain of trust.
</p></li><li class="step "><p>
      The last step is to configure client services to use SSL to access the
      RabbitMQ service. (See
      <a class="link" href="https://docs.openstack.org/oslo.messaging/pike/#oslo-messaging-rabbit" target="_blank">https://docs.openstack.org/oslo.messaging/pike/#oslo-messaging-rabbit</a> for a complete reference).
     </p></li></ol></div></div><p>
    It is preferable to set up your own CA. The best practice is to use a commercial certificate authority. You may also deploy your own self-signed certificates, provided that your cloud is not publicly-accessible, and only for your internal use. Follow these steps to enable your own CA in RabbitMQ and deploy it to SUSE <span class="productname">OpenStack</span> Cloud:
   </p><div class="procedure " id="pro-rabbitmq-production"><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
      Configure the RabbitMQ barclamp to use the control node's
      certificate authority (CA), if it already has one, or create a CA specifically for RabbitMQ and configure the barclamp to use that. (See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a>, and the RabbitMQ manual has a detailed howto on creating your CA at <a class="link" href="http://www.rabbitmq.com/ssl.html" target="_blank">http://www.rabbitmq.com/ssl.html</a>, with customizations for .NET and Java clients.)
    </p><div class="figure" id="id-1.3.5.4.11.6.7.1.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/rabbitmq-ssl-1.png" target="_blank"><img src="images/rabbitmq-ssl-1.png" width="" alt="Example RabbitMQ SSL barclamp configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.6: </span><span class="name">SSL Settings for RabbitMQ Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.11.6.7.1.2">#</a></h6></div></div></li></ul></div></div><p>
    The configuration options in the RabbitMQ barclamp allow tailoring the barclamp to your SSL setup.
</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.11.6.9.1"><span class="term "><span class="guimenu ">Enable SSL</span>
    </span></dt><dd><p>
          Set this to <span class="guimenu ">True</span> to expose all of your configuration options.
      </p></dd><dt id="id-1.3.5.4.11.6.9.2"><span class="term "><span class="guimenu ">SSL Port</span>
    </span></dt><dd><p>
      RabbitMQ's SSL listening port. The default is 5671.
      </p></dd><dt id="id-1.3.5.4.11.6.9.3"><span class="term "><span class="guimenu ">Generate (self-signed) certificates (implies insecure)</span>
    </span></dt><dd><p>
    When this is set to <code class="literal">true</code>, self-signed certificates are automatically generated and copied to the correct locations on the control node, and all other barclamp options are set automatically. This is the fastest way to apply and test the barclamp. Do not use this on production systems. When this is set to <code class="literal">false</code> the remaining options are exposed.
      </p></dd><dt id="id-1.3.5.4.11.6.9.4"><span class="term "><span class="guimenu ">SSL Certificate File</span>
    </span></dt><dd><p>
     The location of your public root CA certificate.
      </p></dd><dt id="id-1.3.5.4.11.6.9.5"><span class="term "><span class="guimenu ">SSL (Private) Key File</span>
    </span></dt><dd><p>
     The location of your private server key.
      </p></dd><dt id="id-1.3.5.4.11.6.9.6"><span class="term "><span class="guimenu ">Require Client Certificate</span>
    </span></dt><dd><p>
          This goes with <span class="guimenu ">SSL CA Certificates File</span>. Set to <span class="guimenu ">true</span> to require clients to present SSL certificates to RabbitMQ.
      </p></dd><dt id="id-1.3.5.4.11.6.9.7"><span class="term "><span class="guimenu ">SSL CA Certificates File</span>
    </span></dt><dd><p>
     Trust client certificates presented by the clients that are signed by other CAs. You'll need to store copies of the CA certificates; see "Trust the Client's Root CA" at <a class="link" href="http://www.rabbitmq.com/ssl.html" target="_blank">http://www.rabbitmq.com/ssl.html</a>.
      </p></dd><dt id="id-1.3.5.4.11.6.9.8"><span class="term "><span class="guimenu ">SSL Certificate is insecure (for instance, self-signed)</span>
    </span></dt><dd><p>
      When this is set to <span class="guimenu ">false</span>, clients validate the RabbitMQ server certificate with the <span class="guimenu ">SSL client CA</span> file.
      </p></dd><dt id="id-1.3.5.4.11.6.9.9"><span class="term "><span class="guimenu ">SSL client CA file (used to validate rabbitmq server certificate)</span>
    </span></dt><dd><p>
        Tells clients of RabbitMQ where to find the CA bundle that validates the certificate presented by the RabbitMQ server, when <span class="guimenu ">SSL Certificate is insecure (for instance, self-signed)</span> is set to <span class="guimenu ">false</span>.
      </p></dd></dl></div></div><div class="sect2 " id="sec-depl-ostack-rabbitmq-send-notifications"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Clients to Send Notifications</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbitmq-send-notifications">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbitmq-send-notifications</li></ul></div></div></div></div><p>
    RabbitMQ has an option called <code class="literal">Configure clients to send
    notifications</code>. It defaults to <code class="literal">false</code>, which
    means no events will be sent. It is required to be set to
    <code class="literal">true</code> for Ceilometer, Monasca, and any other services
    consuming notifications. When it is set to <code class="literal">true</code>,
    OpenStack services are configured to submit lifecycle audit events to the
    <code class="literal">notification</code> RabbitMQ queue.
   </p><p>
    This option should only be enabled if an active consumer is configured,
    otherwise events will accumulate on the RabbitMQ server, clogging up CPU,
    memory, and disk storage.
   </p><p>
    Any accumulation can be cleared by running:
   </p><div class="verbatim-wrap"><pre class="screen">$ rabbitmqctl -p /openstack purge_queue notifications.info
$ rabbitmqctl -p /openstack purge_queue notifications.error</pre></div></div></div><div class="sect1 " id="sec-depl-ostack-keystone"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Keystone</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone</li></ul></div></div></div></div><p>
   Keystone is another core component that is used by all
   other <span class="productname">OpenStack</span> components. It provides authentication and authorization
   services. Keystone needs to be installed on a
   Control Node. Keystone can be made highly available by deploying it on a
   cluster. You can configure the following parameters of this barclamp:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.12.3.1"><span class="term "><span class="guimenu ">Algorithm for Token Generation</span>
    </span></dt><dd><p>
      Set the algorithm used by Keystone to generate the tokens. You can
      choose between <code class="literal">Fernet</code> (the default) or
      <code class="literal">UUID</code>. Note that for performance and security reasons
      it is strongly recommended to use <code class="literal">Fernet</code>.
     </p></dd><dt id="id-1.3.5.4.12.3.2"><span class="term "><span class="guimenu ">Region Name</span>
    </span></dt><dd><p>
      Allows customizing the region name that crowbar is going to manage.
     </p></dd><dt id="id-1.3.5.4.12.3.3"><span class="term "><span class="guimenu ">Default Credentials: Default Tenant</span>
    </span></dt><dd><p>
      Tenant for the users. Do not change the default value of
      <code class="literal">openstack</code>.
     </p></dd><dt id="id-1.3.5.4.12.3.4"><span class="term "><span class="guimenu ">
      Default Credentials: Administrator User Name/Password
     </span>
    </span></dt><dd><p>
      User name and password for the administrator.
     </p></dd><dt id="id-1.3.5.4.12.3.5"><span class="term "><span class="guimenu ">
      Default Credentials: Create Regular User
     </span>
    </span></dt><dd><p>
      Specify whether a regular user should be created automatically. Not
      recommended in most scenarios, especially in an LDAP environment.
     </p></dd><dt id="id-1.3.5.4.12.3.6"><span class="term "><span class="guimenu ">
      Default Credentials: Regular User Username/Password
     </span>
    </span></dt><dd><p>
      User name and password for the regular user. Both the regular user and
      the administrator accounts can be used to log in to the SUSE <span class="productname">OpenStack</span> Cloud Dashboard.
      However, only the administrator can manage Keystone users and access.
     </p><div class="figure" id="id-1.3.5.4.12.3.6.2.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_keystone.png" target="_blank"><img src="images/depl_barclamp_keystone.png" width="" alt="The Keystone Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.7: </span><span class="name">The Keystone Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.12.3.6.2.2">#</a></h6></div></div></dd><dt id="sec-depl-ostack-keystone-ssl"><span class="term ">SSL Support: Protocol
    </span></dt><dd><p>
      When you use the default value <span class="guimenu ">HTTP</span>, public
      communication will not be encrypted. Choose <span class="guimenu ">HTTPS</span> to
      use SSL for encryption. See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for
      background information and <a class="xref" href="#sec-depl-inst-nodes-post-ssl" title="11.4.6. Enabling SSL">Section 11.4.6, “Enabling SSL”</a>
      for installation instructions. The following additional configuration
      options will become available when choosing <span class="guimenu ">HTTPS</span>:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.12.3.7.2.2.1"><span class="term "><span class="guimenu ">Generate (self-signed) certificates</span>
       </span></dt><dd><p>
         When set to <code class="literal">true</code>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </p></dd><dt id="id-1.3.5.4.12.3.7.2.2.2"><span class="term "><span class="guimenu ">SSL Certificate File</span> / <span class="guimenu ">SSL (Private) Key
        File</span>
       </span></dt><dd><p>
         Location of the certificate key pair files.
        </p></dd><dt id="id-1.3.5.4.12.3.7.2.2.3"><span class="term "><span class="guimenu ">SSL Certificate is insecure</span>
       </span></dt><dd><p>
         Set this option to <code class="literal">true</code> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </p></dd><dt id="id-1.3.5.4.12.3.7.2.2.4"><span class="term "><span class="guimenu ">SSL CA Certificates File</span>
       </span></dt><dd><p>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the barclamp to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <code class="systemitem">apache2</code> service, and
         re-deploy the barclamp.
        </p><p>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </p><div class="figure" id="id-1.3.5.4.12.3.7.2.2.4.2.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_ssl.png" target="_blank"><img src="images/depl_barclamp_ssl.png" width="" alt="The SSL Dialog" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.8: </span><span class="name">The SSL Dialog </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.12.3.7.2.2.4.2.3">#</a></h6></div></div></dd></dl></div></dd></dl></div><div class="sect2 " id="sec-depl-ostack-keystone-ldap"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authenticating with LDAP</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone-ldap">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone-ldap</li></ul></div></div></div></div><p>
  Keystone has the ability to separate
  identity backends by domains. SUSE <span class="productname">OpenStack</span> Cloud 8 uses this method for
  authenticating users.
 </p><p>
  The Keystone barclamp sets up a MariaDB database by default. Configuring
  an LDAP back-end is done in the <span class="guimenu ">Raw</span> view.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
             Set <span class="guimenu ">"domain_specific_drivers": true,</span>
         </p></li><li class="step "><p>
             Then in the <span class="guimenu ">"domain_specific_config":</span> section
             configure a map with domain names as keys, and configuration as
             values. In the default proposal the domain name key is
             <span class="guimenu ">"ldap_users"</span>, and the keys are the two required
             sections for an LDAP-based identity driver configuration, the <span class="guimenu ">[identity]</span> section which sets the driver, and the <span class="guimenu ">[ldap]</span> section which sets the LDAP connection options. You may configure multiple domains, each with its own configuration.
             </p></li></ol></div></div><p>
     You may make this available to Horizon by setting <span class="guimenu ">multi_domain_support</span> to <span class="guimenu ">true</span> in the Horizon barclamp.
 </p><p>
Users in the LDAP-backed domain have to know the name of the domain in order to authenticate, and must use the  Keystone v3 API endpoint. (See the OpenStack manuals, <a class="link" href="https://docs.openstack.org/keystone/pike/admin/identity-domain-specific-config.html" target="_blank">Domain-specific Configuration</a> and <a class="link" href="https://docs.openstack.org/keystone/pike/admin/identity-integrate-with-ldap.html" target="_blank">Integrate Identity with LDAP</a>, for additional details.)
 </p></div><div class="sect2 " id="sec-depl-ostack-keystone-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Keystone</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone-ha</li></ul></div></div></div></div><p>
    Making Keystone highly available requires no special
    configuration—it is sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-monasca"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Monasca (Optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-monasca">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-monasca</li></ul></div></div></div></div><p>
   Monasca is an open-source monitoring-as-a-service solution that
   integrates with OpenStack. Monasca is designed for scalability, high
   performance, and fault tolerance.
  </p><p>
   Accessing the <span class="guimenu ">Raw</span> interface is not required for
   day-to-day operation. But as not all Monasca settings are exposed in the
   barclamp graphical interface (for example, various performance tuneables), it
   is recommended to configure Monasca in the <span class="guimenu ">Raw</span>
   mode. Below are the options that can be configured via the
   <span class="guimenu ">Raw</span> interface of the Monasca barclamp.
  </p><div class="figure" id="id-1.3.5.4.13.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_monasca_raw.png" target="_blank"><img src="images/depl_barclamp_monasca_raw.png" width="" alt="The Monasca barclamp Raw Mode" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.9: </span><span class="name">The Monasca barclamp Raw Mode </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.4">#</a></h6></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.5"><span class="name"><span class="guimenu ">agent: settings for openstack-monasca-agent</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.5">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.6.1"><span class="term ">keystone</span></dt><dd><p>
      Contains Keystone credentials that the agents use to send metrics. Do
      not change these options, as they are configured by Crowbar.
     </p></dd><dt id="id-1.3.5.4.13.6.2"><span class="term ">insecure</span></dt><dd><p>
      Specifies whether SSL certificates are verified when communicating with
      Keystone. If set to <code class="literal">false</code>, the
      <code class="literal">ca_file</code> option must be specified.
     </p></dd><dt id="id-1.3.5.4.13.6.3"><span class="term ">ca_file</span></dt><dd><p>
      Specifies the location of a CA certificate that is used for verifying
      Keystone's SSL certificate.
     </p></dd><dt id="id-1.3.5.4.13.6.4"><span class="term ">log_dir</span></dt><dd><p>
      Path for storing log files. The specified path must exist. Do not change
      the default <code class="filename">/var/log/monasca-agent</code> path.
     </p></dd><dt id="id-1.3.5.4.13.6.5"><span class="term ">log_level</span></dt><dd><p>
      Agent's log level. Limits log messages to the specified level and above.
      The following levels are available: Error, Warning, Info (default), and
      Debug.
     </p></dd><dt id="id-1.3.5.4.13.6.6"><span class="term ">check_frequency</span></dt><dd><p>
      Interval in seconds between running agents' checks.
     </p></dd><dt id="id-1.3.5.4.13.6.7"><span class="term ">num_collector_threads</span></dt><dd><p>
      Number of simultaneous collector threads to run. This refers to the
      maximum number of different collector plug-ins (for example,
      <code class="literal">http_check</code>) that are allowed to run simultaneously. The
      default value <code class="literal">1</code> means that plug-ins are run
      sequentially.
     </p></dd><dt id="id-1.3.5.4.13.6.8"><span class="term ">pool_full_max_retries</span></dt><dd><p>
      If a problem with the results from multiple plug-ins results blocks the
      entire thread pool (as specified by the
      <code class="systemitem">num_collector_threads</code> parameter), the collector
      exits, so it can be restarted by the
      <code class="systemitem">supervisord</code>. The parameter
      <code class="systemitem">pool_full_max_retries</code> specifies when this event
      occurs. The collector exits when the defined number of consecutive
      collection cycles have ended with the thread pool completely full.
     </p></dd><dt id="id-1.3.5.4.13.6.9"><span class="term ">plugin_collect_time_warn</span></dt><dd><p>
      Upper limit in seconds for any collection plug-in's run time. A warning
      is logged if a plug-in runs longer than the specified limit.
     </p></dd><dt id="id-1.3.5.4.13.6.10"><span class="term ">max_measurement_buffer_size</span></dt><dd><p>
      Maximum number of measurements to buffer locally if the Monasca API
      is unreachable. Measurements will be dropped in batches, if the API is
      still unreachable after the specified number of messages are buffered.
      The default <code class="literal">-1</code> value indicates unlimited buffering.
      Note that a large buffer increases the agent's memory usage.
     </p></dd><dt id="id-1.3.5.4.13.6.11"><span class="term ">backlog_send_rate</span></dt><dd><p>
      Maximum number of measurements to send when the local measurement buffer
      is flushed.
     </p></dd><dt id="id-1.3.5.4.13.6.12"><span class="term ">amplifier</span></dt><dd><p>
      Number of extra dimensions to add to metrics sent to the Monasca API.
      This option is intended for load testing purposes only. Do not enable the
      option in production! The default <code class="literal">0</code> value disables the
      addition of dimensions.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.7"><span class="name"><span class="guimenu ">log_agent: settings for openstack-monasca-log-agent</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.7">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.8.1"><span class="term ">max_data_size_kb</span></dt><dd><p>
      Maximum payload size in kilobytes for a request sent to the Monasca
      log API.
     </p></dd><dt id="id-1.3.5.4.13.8.2"><span class="term ">num_of_logs</span></dt><dd><p>
      Maximum number of log entries the log agent sends to the Monasca log
      API in a single request. Reducing the number increases performance.
     </p></dd><dt id="id-1.3.5.4.13.8.3"><span class="term ">elapsed_time_sec</span></dt><dd><p>
      Time interval in seconds between sending logs to the Monasca log API.
     </p></dd><dt id="id-1.3.5.4.13.8.4"><span class="term ">delay</span></dt><dd><p>
      Interval in seconds for checking whether
      <code class="literal">elapsed_time_sec</code> has been reached.
     </p></dd><dt id="id-1.3.5.4.13.8.5"><span class="term ">keystone</span></dt><dd><p>
      Keystone credentials the log agents use to send logs to the Monasca
      log API. Do not change this option manually, as it is configured by
      Crowbar.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.9"><span class="name"><span class="guimenu ">api: Settings for openstack-monasca-api</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.9">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.10.1"><span class="term ">bind_host</span></dt><dd><p>
      Interfaces <code class="literal">monasca-api</code> listens on. Do not change this
      option, as it is configured by Crowbar.
     </p></dd><dt id="id-1.3.5.4.13.10.2"><span class="term ">processes</span></dt><dd><p>
      Number of processes to spawn.
     </p></dd><dt id="id-1.3.5.4.13.10.3"><span class="term ">threads</span></dt><dd><p>
      Number of WSGI worker threads to spawn.
     </p></dd><dt id="id-1.3.5.4.13.10.4"><span class="term ">log_level</span></dt><dd><p>
      Log level for <code class="systemitem">openstack-monasca-api</code>. Limits log
      messages to the specified level and above. The following levels are
      available: Critical, Error, Warning, Info (default), Debug, and Trace.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.11"><span class="name"><span class="guimenu ">elasticsearch: server-side settings for elasticsearch</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.11">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.12.1"><span class="term ">repo_dir</span></dt><dd><p>
      List of directories for storing elasticsearch snapshots. Must be created
      manually and be writeable by the
      <code class="systemitem">elasticsearch</code> user.
      Must contain at least one entry in order for the snapshot functionality
      to work.
     </p><p>
      For instructions on creating an elasticsearch snapshot, see
      <a class="link" href="https://documentation.suse.com/soc/8/html/suse-openstack-cloud-socmmsoperator/idg-msoperator-shared-operationmaintenance-c-operate-xml-1.html" target="_blank">https://documentation.suse.com/soc/8/html/suse-openstack-cloud-socmmsoperator/idg-msoperator-shared-operationmaintenance-c-operate-xml-1.html</a>.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.13"><span class="name"><span class="guimenu ">elasticsearch_curator: settings for
    elastisearch-curator</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.13">#</a></h4></div><p>
   <code class="systemitem">elasticsearch-curator</code> removes old and large
   elasticsearch indices. The settings below determine its behavior.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.15.1"><span class="term ">delete_after_days</span></dt><dd><p>
      Time threshold for deleting indices. Indices older the specified number
      of days are deleted. This parameter is unset by default, so indices are
      kept indefinitely.
     </p></dd><dt id="id-1.3.5.4.13.15.2"><span class="term ">delete_after_size</span></dt><dd><p>
      Maximum size in megabytes of indices. Indices larger than the specified
      size are deleted. This parameter is unset by default, so indices are kept
      irrespective of their size.
     </p></dd><dt id="id-1.3.5.4.13.15.3"><span class="term ">delete_exclude_index</span></dt><dd><p>
      List of indices to exclude from
      <code class="systemitem">elasticsearch-curator</code> runs. By default, only the
      <code class="filename">.kibana</code> files are excluded.
     </p></dd><dt id="id-1.3.5.4.13.15.4"><span class="term ">cron_config</span></dt><dd><p>
      Specifies when to run <code class="systemitem">elasticsearch-curator</code>.
      Attributes of this parameter correspond to the fields in
      <code class="systemitem">crontab(5)</code>.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.16"><span class="name"><span class="guimenu ">kafka: tunables for
Kafka</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.16">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.17.1"><span class="term ">log_retention_hours</span></dt><dd><p>
      Number of hours for retaining log segments in Kafka's on-disk log.
      Messages older than the specified value are dropped.
     </p></dd><dt id="id-1.3.5.4.13.17.2"><span class="term ">log_retention_bytes</span></dt><dd><p>
      Maximum size for Kafka's on-disk log in bytes. If the log grows beyond
      this size, the oldest log segments are dropped.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.18"><span class="name"><span class="guimenu ">master: configuration for
monasca-installer on the Crowbar node</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.18">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.19.1"><span class="term ">influxdb_retention_policy</span></dt><dd><p>
      Number of days to keep metrics records in influxdb.
     </p><p>
      For an overview of all supported values, see
      <a class="link" href="https://docs.influxdata.com/influxdb/v1.1/query_language/database_management/#create-retention-policies-with-create-retention-policy" target="_blank">https://docs.influxdata.com/influxdb/v1.1/query_language/database_management/#create-retention-policies-with-create-retention-policy</a>.
     </p></dd><dt id="id-1.3.5.4.13.19.2"><span class="term ">notification_enable_email</span></dt><dd><p>
      Enable or disable email alarm notifications.
     </p></dd><dt id="id-1.3.5.4.13.19.3"><span class="term ">smtp_host</span></dt><dd><p>
      SMTP smarthost for sending alarm notifications.
     </p></dd><dt id="id-1.3.5.4.13.19.4"><span class="term ">smtp_port</span></dt><dd><p>
      Port for the SMTP smarthost.
     </p></dd><dt id="id-1.3.5.4.13.19.5"><span class="term ">smtp_user</span></dt><dd><p>
      User name for authenticating against the smarthost.
     </p></dd><dt id="id-1.3.5.4.13.19.6"><span class="term ">smtp_password</span></dt><dd><p>
      Password for authenticating against the smarthost.
     </p></dd><dt id="id-1.3.5.4.13.19.7"><span class="term ">smtp_from_address</span></dt><dd><p>
      Sender address for alarm notifications.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.20"><span class="name"><span class="guimenu ">monasca: settings for libvirt and Ceph
monitoring</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.20">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.21.1"><span class="term ">
     monitor_libvirt
    </span></dt><dd><p>
      The global switch for toggling libvirt monitoring. If set to
      <em class="parameter ">true</em>, libvirt metrics will be gathered on all
      libvirt based Compute Nodes. This setting is available in the Crowbar UI.
     </p></dd><dt id="id-1.3.5.4.13.21.2"><span class="term ">
     monitor_ceph
    </span></dt><dd><p>
      The global switch for toggling Ceph monitoring. If set to
      <em class="parameter ">true</em>, Ceph metrics will be gathered on all
      Ceph-based Compute Nodes. This setting is available in Crowbar UI. If the
      Ceph cluster has been set up independently, Crowbar ignores this setting.
     </p></dd><dt id="id-1.3.5.4.13.21.3"><span class="term ">
     cache_dir
    </span></dt><dd><p>
      The directory where monasca-agent will locally cache various metadata
      about locally running VMs on each Compute Node.
     </p></dd><dt id="id-1.3.5.4.13.21.4"><span class="term ">
     customer_metadata
    </span></dt><dd><p>
      Specifies the list of instance metadata keys to be included as dimensions
      with customer metrics. This is useful for providing more information
      about an instance.
     </p></dd><dt id="id-1.3.5.4.13.21.5"><span class="term ">
     disk_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<code class="option">check_frequency</code>), it
      will be ignored in favor of the global collection period.
     </p></dd><dt id="id-1.3.5.4.13.21.6"><span class="term ">
     max_ping_concurrency
    </span></dt><dd><p>
      Specifies the number of ping command processes to run concurrently when
      determining whether the VM is reachable. This should be set to a value
      that allows the plug-in to finish within the agent's collection period,
      even if there is a networking issue. For example, if the expected number
      of VMs per Compute Node is 40 and each VM has one IP address, then the
      plug-in will take at least 40 seconds to do the ping checks in the
      worst-case scenario where all pings fail (assuming the default timeout of
      1 second). Increasing <code class="option">max_ping_concurrency</code> allows the
      plug-in to finish faster.
     </p></dd><dt id="id-1.3.5.4.13.21.7"><span class="term ">
     metadata
    </span></dt><dd><p>
      Specifies the list of Nova side instance metadata keys to be included
      as dimensions with the cross-tenant metrics for the
      <span class="guimenu ">monasca</span> project. This is useful for providing more
      information about an instance.
     </p></dd><dt id="id-1.3.5.4.13.21.8"><span class="term ">
     nova_refresh
    </span></dt><dd><p>
      Specifies the number of seconds between calls to the Nova API to
      refresh the instance cache. This is helpful for updating VM hostname and
      pruning deleted instances from the cache. By default, it is set to 14,400
      seconds (four hours). Set to 0 to refresh every time the Collector runs,
      or to <em class="parameter ">None</em> to disable regular refreshes entirely.
      In this case, the instance cache will only be refreshed when a new
      instance is detected.
     </p></dd><dt id="id-1.3.5.4.13.21.9"><span class="term ">
     ping_check
    </span></dt><dd><p>
      Includes the entire ping command (without the IP address, which is
      automatically appended) to perform a ping check against instances. The
      <code class="literal">NAMESPACE</code> keyword is automatically replaced with the
      appropriate network namespace for the VM being monitored. Set to
      <em class="parameter ">False</em> to disable ping checks.
     </p></dd><dt id="id-1.3.5.4.13.21.10"><span class="term ">
     vnic_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<code class="option">check_frequency</code>), it
      will be ignored in favor of the global collection period.
     </p></dd><dt id="id-1.3.5.4.13.21.11"><span class="term ">
     vm_cpu_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM CPU metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.3.5.4.13.21.12"><span class="term ">
     vm_disks_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM disk metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.3.5.4.13.21.13"><span class="term ">
     vm_extended_disks_check_enable
    </span></dt><dd><p>
      Toggles the collection of extended disk metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.3.5.4.13.21.14"><span class="term ">
     vm_network_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM network metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.3.5.4.13.21.15"><span class="term ">
     vm_ping_check_enable
    </span></dt><dd><p>
      Toggles ping checks for checking whether a host is alive. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.3.5.4.13.21.16"><span class="term ">
     vm_probation
    </span></dt><dd><p>
      Specifies a period of time (in seconds) in which to suspend metrics from
      a newly-created VM. This is to prevent quickly-obsolete metrics in an
      environment with a high amount of instance churn (VMs created and
      destroyed in rapid succession). The default probation length is 300
      seconds (5 minutes). Set to 0 to disable VM probation. In this case,
      metrics are recorded immediately after a VM is created.
     </p></dd><dt id="id-1.3.5.4.13.21.17"><span class="term ">
     vnic_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting VM network
      metrics. Increase this value to reduce I/O load. If the value is lower
      than the global agent collection period
      (<em class="parameter ">check_frequency</em>), it will be ignored in favor of
      the global collection period.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.13.22"><span class="name"><span class="guimenu ">Deployment</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.22">#</a></h4></div><p>
   The Monasca component consists of following roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.13.24.1"><span class="term ">monasca-server</span></dt><dd><p>
      Monasca server-side components that are deployed by Chef.
      Currently, this only creates Keystone resources required by Monasca,
      such as users, roles, endpoints, etc. The rest is left to the
      Ansible-based <code class="systemitem">monasca-installer</code> run by the
      <code class="systemitem">monasca-master</code> role.
     </p></dd><dt id="id-1.3.5.4.13.24.2"><span class="term ">monasca-master</span></dt><dd><p>
      Runs the Ansible-based <code class="systemitem">monasca-installer</code> from
      the Crowbar node. The installer deploys the Monasca server-side
      components to the node that has the
      <code class="systemitem">monasca-server</code> role assigned to it. These
      components are <code class="systemitem">openstack-monasca-api</code>, and
      <code class="systemitem">openstack-monasca-log-api</code>, as well as all the
      back-end services they use.
     </p></dd><dt id="id-1.3.5.4.13.24.3"><span class="term ">monasca-agent</span></dt><dd><p>
      Deploys <code class="systemitem">openstack-monasca-agent</code> that is
      responsible for sending metrics to <code class="systemitem">monasca-api</code>
      on nodes it is assigned to.
     </p></dd><dt id="id-1.3.5.4.13.24.4"><span class="term ">monasca-log-agent</span></dt><dd><p>
      Deploys <code class="systemitem">openstack-monasca-log-agent</code> responsible
      for sending logs to <code class="systemitem">monasca-log-api</code> on nodes it
      is assigned to.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.13.25"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_monasca_node_deployment.png" target="_blank"><img src="images/depl_barclamp_monasca_node_deployment.png" width="" alt="The Monasca Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.10: </span><span class="name">The Monasca Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.13.25">#</a></h6></div></div></div><div class="sect1 " id="sec-depl-ostack-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Swift (optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-swift">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-swift</li></ul></div></div></div></div><p>
   Swift adds an object storage service to SUSE <span class="productname">OpenStack</span> Cloud for storing single
   files such as images or snapshots. It offers high data security by storing
   the data redundantly on a pool of Storage Nodes—therefore Swift
   needs to be installed on at least two dedicated nodes.
  </p><p>
   To properly configure Swift it is important to understand how it
   places the data. Data is always stored redundantly within the hierarchy. The
   Swift hierarchy in SUSE <span class="productname">OpenStack</span> Cloud is formed out of zones, nodes, hard disks,
   and logical partitions. Zones are physically separated clusters, for example
   different server rooms each with its own power supply and network segment. A
   failure of one zone must not affect another zone. The next level in the
   hierarchy are the individual Swift storage nodes (on which
   <span class="guimenu ">swift-storage</span> has been deployed), followed by the hard
   disks. Logical partitions come last.
  </p><p>
   Swift automatically places three copies of each object on the highest
   hierarchy level possible. If three zones are available, then each copy of
   the object will be placed in a different zone. In a one zone setup with more
   than two nodes, the object copies will each be stored on a different node.
   In a one zone setup with two nodes, the copies will be distributed on
   different hard disks. If no other hierarchy element fits, logical partitions
   are used.
  </p><p>
   The following attributes can be set to configure Swift:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.14.6.1"><span class="term "><span class="guimenu ">Allow Public Containers</span>
    </span></dt><dd><p>
      Set to <code class="literal">true</code> to enable public access to containers.
     </p></dd><dt id="id-1.3.5.4.14.6.2"><span class="term "><span class="guimenu ">Enable Object Versioning</span>
    </span></dt><dd><p>
      If set to true, a copy of the current version is archived each time an
      object is updated.
     </p></dd><dt id="id-1.3.5.4.14.6.3"><span class="term "><span class="guimenu ">Zones</span>
    </span></dt><dd><p>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <code class="literal">1</code>.
     </p></dd><dt id="id-1.3.5.4.14.6.4"><span class="term "><span class="guimenu ">Create 2^X Logical Partitions</span>
    </span></dt><dd><p>
      Partition power. The number entered here is used to compute the number of
      logical partitions to be created in the cluster. The number you enter is
      used as a power of 2 (2^X).
     </p><p>
      We recommend using a minimum of 100 partitions per disk. To measure the
      partition power for your setup, multiply the number of disks from all
      Swift nodes by 100, and then round up to the nearest power of two.
      Keep in mind that the first disk of each node is not used by
      Swift, but rather for the operating system.
     </p><p><span class="formalpara-title">Example: 10 Swift nodes with 5 hard disks each. </span>
       Four hard disks on each node are used for Swift, so there is a
       total of forty disks. 40 x 100 = 4000. The nearest power of two, 4096,
       equals 2^12. So the partition power that needs to be entered is
       <code class="literal">12</code>.
      </p><div id="id-1.3.5.4.14.6.4.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Value Cannot be Changed After the Proposal Has Been Deployed</h6><p>
       Changing the number of logical partition after Swift has been
       deployed is not supported. Therefore the value for the partition power
       should be calculated from the maximum number of partitions this cloud
       installation is likely going to need at any point in time.
      </p></div></dd><dt id="id-1.3.5.4.14.6.5"><span class="term "><span class="guimenu ">Minimum Hours before Partition is reassigned</span>
    </span></dt><dd><p>
      This option sets the number of hours before a logical partition is
      considered for relocation. <code class="literal">24</code> is the recommended
      value.
     </p></dd><dt id="id-1.3.5.4.14.6.6"><span class="term "><span class="guimenu ">Replicas</span>
    </span></dt><dd><p>
      The number of copies generated for each object. The number of replicas
      depends on the number of disks and zones.
     </p></dd><dt id="id-1.3.5.4.14.6.7"><span class="term "><span class="guimenu ">Replication interval (in seconds)</span>
    </span></dt><dd><p>
      Time (in seconds) after which to start a new replication process.
     </p></dd><dt id="id-1.3.5.4.14.6.8"><span class="term "><span class="guimenu ">Debug</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <code class="literal">true</code>.
     </p></dd><dt id="id-1.3.5.4.14.6.9"><span class="term "><span class="guimenu ">SSL Support: Protocol</span>
    </span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If you choose <span class="guimenu ">HTTPS</span>,
      you have two options. You can either <span class="guimenu ">Generate (self-signed)
      certificates</span> or provide the locations for the certificate key
      pair files. Using self-signed certificates is for testing purposes only
      and should never be used in production environments!
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.14.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_swift.png" target="_blank"><img src="images/depl_barclamp_swift.png" width="" alt="The Swift Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.11: </span><span class="name">The Swift Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.14.7">#</a></h6></div></div><p>
   Apart from the general configuration described above, the Swift
   barclamp lets you also activate and configure <span class="guimenu ">Additional
   Middlewares</span>. The features these middlewares provide can be used
   via the Swift command line client only. The Ratelimit and S3
   middleware provide for the most interesting features, and we recommend
   enabling other middleware only for specific use-cases.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.14.9.1"><span class="term "><span class="guimenu ">S3 Middleware</span>
    </span></dt><dd><p>
      Provides an S3 compatible API on top of Swift.
     </p></dd><dt id="id-1.3.5.4.14.9.2"><span class="term "><span class="guimenu ">StaticWeb</span>
    </span></dt><dd><p>
      Serve container data as a static Web site with an index file and optional
      file listings. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#staticweb" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#staticweb</a>
      for details.
     </p><p>
      This middleware requires setting <span class="guimenu ">Allow Public
      Containers</span> to <code class="literal">true</code>.
     </p></dd><dt id="id-1.3.5.4.14.9.3"><span class="term "><span class="guimenu ">TempURL</span>
    </span></dt><dd><p>
      Create URLs to provide time-limited access to objects. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#tempurl" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#tempurl</a>
      for details.
     </p></dd><dt id="id-1.3.5.4.14.9.4"><span class="term "><span class="guimenu ">FormPOST</span>
    </span></dt><dd><p>
      Upload files to a container via Web form. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#formpost" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#formpost</a>
      for details.
     </p></dd><dt id="id-1.3.5.4.14.9.5"><span class="term "><span class="guimenu ">Bulk</span>
    </span></dt><dd><p>
      Extract TAR archives into a Swift account, and delete multiple objects or
      containers with a single request. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk</a>
      for details.
     </p></dd><dt id="id-1.3.5.4.14.9.6"><span class="term "><span class="guimenu ">Cross-domain</span>
    </span></dt><dd><p>
      Interact with the Swift API via Flash, Java, and Silverlight from an
      external network. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain</a>
      for details.
     </p></dd><dt id="id-1.3.5.4.14.9.7"><span class="term "><span class="guimenu ">Domain Remap</span>
    </span></dt><dd><p>
      Translates container and account parts of a domain to path parameters
      that the Swift proxy server understands. Can be used to create
      short URLs that are easy to remember, for example by rewriting
      <code class="literal">home.tux.example.com/$ROOT/tux/home/myfile</code>
      to <code class="literal">home.tux.example.com/myfile</code>.
      See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap</a>
      for details.
     </p></dd><dt id="id-1.3.5.4.14.9.8"><span class="term "><span class="guimenu ">Ratelimit</span>
    </span></dt><dd><p>
      Throttle resources such as requests per minute to provide denial of
      service protection. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit</a>
      for details.
     </p></dd></dl></div><p>
   The Swift component consists of four different roles. Deploying
   <span class="guimenu ">swift-dispersion</span> is optional:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.14.11.1"><span class="term "><span class="guimenu ">swift-storage</span>
    </span></dt><dd><p>
      The virtual object storage service. Install this role on all dedicated
      Swift Storage Nodes (at least two), but not on any other node.
     </p><div id="id-1.3.5.4.14.11.1.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: swift-storage Needs Dedicated Machines</h6><p>
       Never install the swift-storage service on a node that runs other
       <span class="productname">OpenStack</span> components.
      </p></div></dd><dt id="id-1.3.5.4.14.11.2"><span class="term "><span class="guimenu ">swift-ring-compute</span>
    </span></dt><dd><p>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index that is used by
      various <span class="productname">OpenStack</span> components to look up the physical location of objects.
      <span class="guimenu ">swift-ring-compute</span> must only be installed on a single
      node, preferably a Control Node.
     </p></dd><dt id="id-1.3.5.4.14.11.3"><span class="term "><span class="guimenu ">swift-proxy</span>
    </span></dt><dd><p>
      The Swift proxy server takes care of routing requests to
      Swift. Installing a single instance of
      <span class="guimenu ">swift-proxy</span> on a Control Node is recommended. The
      <span class="guimenu ">swift-proxy</span> role can be made highly available by
      deploying it on a cluster.
     </p></dd><dt id="id-1.3.5.4.14.11.4"><span class="term "><span class="guimenu ">swift-dispersion</span>
    </span></dt><dd><p>
      Deploying <span class="guimenu ">swift-dispersion</span> is optional. The
      Swift dispersion tools can be used to test the health of the
      cluster. It creates a heap of dummy objects (using 1% of the total space
      available). The state of these objects can be queried using the
      swift-dispersion-report query. <span class="guimenu ">swift-dispersion</span> needs
      to be installed on a Control Node.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.14.12"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_swift_node_deployment.png" target="_blank"><img src="images/depl_barclamp_swift_node_deployment.png" width="" alt="The Swift Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.12: </span><span class="name">The Swift Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.14.12">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-swift-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Swift</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-swift-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-swift-ha</li></ul></div></div></div></div><p>
    Swift replicates by design, so there is no need for a special HA setup.
    Make sure to fulfill the requirements listed in
    <a class="xref" href="#sec-depl-reg-ha-storage-swift" title="2.6.4.1. Swift—Avoiding Points of Failure">Section 2.6.4.1, “Swift—Avoiding Points of Failure”</a>.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-glance"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Glance</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-glance">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-glance</li></ul></div></div></div></div><p>
   Glance provides discovery, registration, and delivery services for virtual
   disk images. An image is needed to start an instance—it is its
   pre-installed root-partition. All images you want to use in your cloud to
   boot instances from, are provided by Glance. Glance must be deployed onto
   a Control Node. Glance can be made highly available by deploying it on a
   cluster.
  </p><p>
   There are a lot of options to configure Glance. The most important ones are
   explained below—for a complete reference refer to
   <a class="link" href="http://github.com/crowbar/crowbar/wiki/Glance-barclamp" target="_blank">http://github.com/crowbar/crowbar/wiki/Glance-barclamp</a>.
  </p><div id="note-glance-api-versions" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Glance API Versions</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7, the Glance API v1 is no longer enabled by default.
    Instead, Glance API v2 is used by default.
   </p><p>
    If you need to re-enable API v1 for compatibility reasons:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Switch to the <span class="guimenu ">Raw</span> view of the Glance barclamp.
     </p></li><li class="listitem "><p>
      Search for the <code class="literal">enable_v1</code> entry and set it to
      <code class="literal">true</code>:
     </p><div class="verbatim-wrap"><pre class="screen">"enable_v1": true</pre></div><p>
      In new installations, this entry is set to <code class="literal">false</code> by
      default. When upgrading from an older version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> it is set
      to <code class="literal">true</code> by default.
     </p></li><li class="listitem "><p>
      Apply your changes.
     </p></li></ol></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.15.5.1"><span class="term "><span class="guimenu ">Image Storage: Default Storage Store</span>
    </span></dt><dd><p><span class="formalpara-title"><span class="guimenu ">File</span>. </span>
       Images are stored in an image file on the Control Node.
      </p><p><span class="formalpara-title"><span class="guimenu ">Cinder</span>. </span>
       Provides volume block storage to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Use it to
       store images.
      </p><p><span class="formalpara-title"><span class="guimenu ">Swift</span>. </span>
       Provides an object storage service to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">Rados</span>. </span>
       SUSE Enterprise Storage (based on Ceph) provides block storage service to
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">VMware</span>. </span>
       If you are using VMware as a hypervisor, it is recommended to use
       <span class="guimenu ">VMware</span> for storing images. This will make starting
       VMware instances much faster.
      </p><p><span class="formalpara-title"><span class="guimenu ">Expose Backend Store Location</span>. </span>
       If this is set to <span class="guimenu ">true</span>, the API will communicate the
       direct URl of the image's back-end location to HTTP clients. Set to
       <span class="guimenu ">false</span> by default.
      </p><p>
      Depending on the storage back-end, there are additional configuration
      options available:
     </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.5.4.15.5.1.2.8"><span class="name"><span class="guimenu ">File Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.5.1.2.8">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">File</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.15.5.1.2.10.1"><span class="term "><span class="guimenu ">Image Store Directory</span>
       </span></dt><dd><p>
         Specify the directory to host the image file. The directory specified
         here can also be an NFS share. See
         <a class="xref" href="#sec-depl-inst-nodes-post-nfs" title="11.4.3. Mounting NFS Shares on a Node">Section 11.4.3, “Mounting NFS Shares on a Node”</a> for more information.
        </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.5.4.15.5.1.2.11"><span class="name"><span class="guimenu ">Swift Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.5.1.2.11">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">Swift</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.15.5.1.2.13.1"><span class="term "><span class="guimenu ">Swift Container</span>
       </span></dt><dd><p>
         Set the name of the container to use for the images in Swift.
        </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.5.4.15.5.1.2.14"><span class="name"><span class="guimenu ">RADOS Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.5.1.2.14">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">Rados</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.15.5.1.2.16.1"><span class="term ">RADOS User for CephX Authentication</span></dt><dd><p>
         If you are using an external Ceph cluster, specify the user you have
         set up for Glance (see <a class="xref" href="#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for more information).
        </p></dd><dt id="id-1.3.5.4.15.5.1.2.16.2"><span class="term ">RADOS Pool for Glance images</span></dt><dd><p>
         If you are using a SUSE <span class="productname">OpenStack</span> Cloud internal Ceph setup, the pool you specify
         here is created if it does not exist. If you are using an external
         Ceph cluster, specify the pool you have set up for Glance (see
         <a class="xref" href="#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for more
         information).
        </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.5.4.15.5.1.2.17"><span class="name"><span class="guimenu ">VMware Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.5.1.2.17">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">VMware</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.15.5.1.2.19.1"><span class="term "><span class="guimenu ">vCenter Host/IP Address</span>
       </span></dt><dd><p>
         Name or IP address of the vCenter server.
        </p></dd><dt id="id-1.3.5.4.15.5.1.2.19.2"><span class="term "><span class="guimenu ">vCenter Username</span> / <span class="guimenu ">vCenter
        Password</span>
       </span></dt><dd><p>
         vCenter login credentials.
        </p></dd><dt id="id-1.3.5.4.15.5.1.2.19.3"><span class="term "><span class="guimenu ">Datastores for Storing Images</span>
       </span></dt><dd><p>
         A comma-separated list of datastores specified in the format:
         <em class="replaceable ">DATACENTER_NAME</em>:<em class="replaceable ">DATASTORE_NAME</em>
        </p></dd><dt id="id-1.3.5.4.15.5.1.2.19.4"><span class="term "><span class="guimenu ">
         Path on the datastore, where the glance images will be
         stored
        </span>
       </span></dt><dd><p>
         Specify an absolute path here.
        </p></dd></dl></div></dd><dt id="id-1.3.5.4.15.5.2"><span class="term "><span class="guimenu ">SSL Support: Protocol</span>
    </span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If you choose <span class="guimenu ">HTTPS</span>,
      refer to <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration
      details.
     </p></dd><dt id="id-1.3.5.4.15.5.3"><span class="term "><span class="guimenu ">Caching</span>
    </span></dt><dd><p>
      Enable and configure image caching in this section. By default, image
      caching is disabled. You can see this the Raw view of your Nova barclamp:
     </p><div class="verbatim-wrap"><pre class="screen">image_cache_manager_interval = -1</pre></div><p>
      This option sets the number of seconds to wait between runs of the image
      cache manager. Disabling it means that the cache manager will not
      automatically remove the unused images from the cache, so if you have
      many Glance images and are running out of storage you must manually
      remove the unused images from the cache. We recommend leaving this option
      disabled as it is known to cause issues, especially with shared storage.
      The cache manager may remove images still in use, e.g. when network
      outages cause synchronization problems with compute nodes.
     </p><p>
      If you wish to enable caching, re-enable it in a custom Nova
      configuration file, for example
      <code class="filename">/etc/nova/nova.conf.d/500-nova.conf</code>. This sets the
      interval to four minutes:
     </p><div class="verbatim-wrap"><pre class="screen">image_cache_manager_interval = 2400</pre></div><p>
      See <a class="xref" href="#cha-depl-ostack-configs" title="Chapter 14. Configuration Files for OpenStack Services">Chapter 14, <em>Configuration Files for <span class="productname">OpenStack</span> Services</em></a> for more information on
      custom configurations.
     </p><p>
      Learn more about Glance's caching feature at
      <a class="link" href="http://docs.openstack.org/developer/glance/cache.html" target="_blank">http://docs.openstack.org/developer/glance/cache.html</a>.
     </p></dd><dt id="id-1.3.5.4.15.5.4"><span class="term "><span class="guimenu ">Logging: Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu ">true</span>.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.15.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_glance.png" target="_blank"><img src="images/depl_barclamp_glance.png" width="" alt="The Glance Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.13: </span><span class="name">The Glance Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.6">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-glance-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Glance</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-glance-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-glance-ha</li></ul></div></div></div></div><p>
    Glance can be made highly available by deploying it on a cluster. We
    strongly recommended doing this for the image data as well. The recommended
    way is to use Swift or an external Ceph cluster for the image
    repository. If you are using a directory on the node instead (file storage
    back-end), you should set up shared storage on the cluster for it.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-cinder"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Cinder</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-cinder">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-cinder</li></ul></div></div></div></div><p>
   Cinder, the successor of Nova Volume, provides volume block storage.
   It adds persistent storage to an instance that will persist until deleted,
   contrary to ephemeral volumes that only persist while the instance is
   running.
  </p><p>
   Cinder can provide volume storage by using different back-ends such
   as local file, one or more local disks, Ceph (RADOS), VMware, or network
   storage solutions from EMC, EqualLogic, Fujitsu, NetApp or Pure Storage.
   Since <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 5, Cinder supports using several back-ends
   simultaneously. It is also possible to deploy the same network storage
   back-end multiple times and therefore use different installations at the
   same time.
  </p><p>
   The attributes that can be set to configure Cinder depend on the
   back-end. The only general option is <span class="guimenu ">SSL Support:
   Protocol</span> (see <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for
   configuration details).
  </p><div id="id-1.3.5.4.16.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Adding or Changing a Back-End</h6><p>
    When first opening the Cinder barclamp, the default
    proposal—<span class="guimenu ">Raw Devices</span>—is already available
    for configuration. To optionally add a back-end, go to the section
    <span class="guimenu ">Add New Cinder Back-End</span> and choose a <span class="guimenu ">Type Of
    Volume</span> from the drop-down box. Optionally, specify the
    <span class="guimenu ">Name for the Backend</span>. This is recommended when deploying
    the same volume type more than once. Existing back-end configurations
    (including the default one) can be deleted by clicking the trashcan icon if
    no longer needed. Note that you must configure at least one back-end.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.16.6"><span class="name"><span class="guimenu ">Raw devices</span> (local disks)
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.6">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.7.1"><span class="term "><span class="guimenu ">Disk Selection Method</span>
    </span></dt><dd><p>
      Choose whether to use the <span class="guimenu ">First Available</span> disk or
      <span class="guimenu ">All Available</span> disks. <span class="quote">“<span class="quote ">Available disks</span>”</span>
      are all disks currently not used by the system. Note that one disk
      (usually <code class="filename">/dev/sda</code>) of every block storage node is
      already used for the operating system and is not available for
      Cinder.
     </p></dd><dt id="id-1.3.5.4.16.7.2"><span class="term "><span class="guimenu ">Name of Volume</span>
    </span></dt><dd><p>
      Specify a name for the Cinder volume.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.16.8"><span class="name"><span class="guimenu ">EMC</span> (EMC² Storage)
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.8">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.9.1"><span class="term "><span class="guimenu ">IP address of the ECOM server</span> / <span class="guimenu ">Port of the ECOM server</span>
    </span></dt><dd><p>
      IP address and Port of the ECOM server.
     </p></dd><dt id="id-1.3.5.4.16.9.2"><span class="term "><span class="guimenu ">Username for accessing the ECOM server</span> / <span class="guimenu ">Password for accessing the ECOM server</span>
    </span></dt><dd><p>
      Login credentials for the ECOM server.
     </p></dd><dt id="id-1.3.5.4.16.9.3"><span class="term "><span class="guimenu ">VMAX port groups to expose volumes managed by this backend</span>
    </span></dt><dd><p>
      VMAX port groups that expose volumes managed by this back-end.
     </p></dd><dt id="id-1.3.5.4.16.9.4"><span class="term "><span class="guimenu ">Serial number of the VMAX Array</span>
    </span></dt><dd><p>
      Unique VMAX array serial number.
     </p></dd><dt id="id-1.3.5.4.16.9.5"><span class="term "><span class="guimenu ">Pool name within a given array</span>
    </span></dt><dd><p>
      Unique pool name within a given array.
     </p></dd><dt id="id-1.3.5.4.16.9.6"><span class="term "><span class="guimenu ">FAST Policy name to be used</span>
    </span></dt><dd><p>
      Name of the FAST Policy to be used. When specified, volumes managed by
      this back-end are managed as under FAST control.
     </p></dd></dl></div><p>
   For more information on the EMC driver refer to the <span class="productname">OpenStack</span> documentation
   at
   <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html" target="_blank">http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html</a>.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.16.11"><span class="name"><span class="guimenu ">EqualLogic</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.11">#</a></h4></div><p>
   EqualLogic drivers are included as a technology preview and are not
   supported.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.16.13"><span class="name"><span class="guimenu ">Fujitsu ETERNUS DX</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.13">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.14.1"><span class="term "><span class="guimenu ">Connection Protocol</span>
    </span></dt><dd><p>
      Select the protocol used to connect, either
      <span class="guimenu ">FibreChannel</span> or <span class="guimenu ">iSCSI</span>.
     </p></dd><dt id="id-1.3.5.4.16.14.2"><span class="term "><span class="guimenu ">IP for SMI-S</span> / <span class="guimenu ">Port for SMI-S</span>
    </span></dt><dd><p>
      IP address and port of the ETERNUS SMI-S Server.
     </p></dd><dt id="id-1.3.5.4.16.14.3"><span class="term "><span class="guimenu ">Username for SMI-S</span> / <span class="guimenu ">Password for SMI-S</span>
    </span></dt><dd><p>
      Login credentials for the ETERNUS SMI-S Server.
     </p></dd><dt id="id-1.3.5.4.16.14.4"><span class="term "><span class="guimenu ">Snapshot (Thick/RAID Group) Pool Name</span>
    </span></dt><dd><p>
      Storage pool (RAID group) in which the volumes are created. Make sure
      that the RAID group on the server has already been created. If a RAID
      group that does not exist is specified, the RAID group is built from
      unused disk drives. The RAID level is automatically determined by the
      ETERNUS DX Disk storage system.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.16.15"><span class="name"><span class="guimenu ">Hitachi HUSVM</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.15">#</a></h3></div><p>
   For information on configuring the Hitachi HUSVM back-end, refer to
   <a class="link" href="http://docs.openstack.org/ocata/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html" target="_blank">http://docs.openstack.org/ocata/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html</a>.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.16.17"><span class="name"><span class="guimenu ">NetApp</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.17">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.18.1"><span class="term "><span class="guimenu ">Storage Family Type</span> / <span class="guimenu ">Storage Protocol</span>
    </span></dt><dd><p>
      SUSE <span class="productname">OpenStack</span> Cloud can use <span class="quote">“<span class="quote ">Data ONTAP</span>”</span> in <span class="guimenu ">7-Mode</span>,
      or in <span class="guimenu ">Clustered Mode</span>. In <span class="guimenu ">7-Mode</span>
      vFiler will be configured, in <span class="guimenu ">Clustered Mode</span> vServer
      will be configured. The <span class="guimenu ">Storage Protocol</span> can be set to
      either <span class="guimenu ">iSCSI</span> or <span class="guimenu ">NFS</span>. Choose the
      driver and the protocol your NetApp is licensed for.
     </p></dd><dt id="id-1.3.5.4.16.18.2"><span class="term "><span class="guimenu ">Server host name</span>
    </span></dt><dd><p>
      The management IP address for the 7-Mode storage controller, or the
      cluster management IP address for the clustered Data ONTAP.
     </p></dd><dt id="id-1.3.5.4.16.18.3"><span class="term "><span class="guimenu ">Transport Type</span>
    </span></dt><dd><p>
      Transport protocol for communicating with the storage controller or
      clustered Data ONTAP. Supported protocols are HTTP and HTTPS. Choose the
      protocol your NetApp is licensed for.
     </p></dd><dt id="id-1.3.5.4.16.18.4"><span class="term "><span class="guimenu ">Server port</span>
    </span></dt><dd><p>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </p></dd><dt id="id-1.3.5.4.16.18.5"><span class="term "><span class="guimenu ">Username for accessing NetApp</span> / <span class="guimenu ">Password for Accessing NetApp</span>
    </span></dt><dd><p>
      Login credentials.
     </p></dd><dt id="id-1.3.5.4.16.18.6"><span class="term "><span class="guimenu ">
      The vFiler Unit Name for provisioning OpenStack volumes (netapp_vfiler)
     </span>
    </span></dt><dd><p>
      The vFiler unit to be used for provisioning of <span class="productname">OpenStack</span> volumes. This
      setting is only available in <span class="guimenu ">7-Mode</span>.
     </p></dd><dt id="id-1.3.5.4.16.18.7"><span class="term "><span class="guimenu ">Restrict provisioning on iSCSI to these volumes (netapp_volume_list)</span>
    </span></dt><dd><p>
      Provide a list of comma-separated volume names to be used for
      provisioning. This setting is only available when using iSCSI as storage
      protocol.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.16.19"><span class="name"><span class="guimenu ">NFS</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.19">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.20.1"><span class="term "><span class="guimenu ">List of NFS Exports</span>
    </span></dt><dd><p>
      A list of available file systems on an NFS server. Enter your NFS mountpoints
      in the <span class="guimenu ">List of NFS Exports</span> form in this format: <em class="replaceable ">host:mountpoint -o options</em>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">host1:/srv/nfs/share1 /mnt/nfs/share1 -o rsize=8192,wsize=8192,timeo=14,intr</pre></div></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.16.21"><span class="name"><span class="guimenu ">Pure Storage (FlashArray)</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.21">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.22.1"><span class="term "><span class="guimenu ">IP address of the management VIP</span>
    </span></dt><dd><p>
      IP address of the FlashArray management VIP
     </p></dd><dt id="id-1.3.5.4.16.22.2"><span class="term "><span class="guimenu ">API token for the FlashArray</span>
    </span></dt><dd><p>
      API token for access to the FlashArray
     </p></dd><dt id="id-1.3.5.4.16.22.3"><span class="term "><span class="guimenu ">iSCSI CHAP authentication enabled</span>
    </span></dt><dd><p>
      Enable or disable iSCSI CHAP authentication
     </p></dd></dl></div><p>
   For more information on the Pure Storage FlashArray driver refer to the <span class="productname">OpenStack</span> documentation
   at
   <a class="link" href="https://docs.openstack.org/ocata/config-reference/block-storage/drivers/pure-storage-driver.html" target="_blank">https://docs.openstack.org/ocata/config-reference/block-storage/drivers/pure-storage-driver.html</a>.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.16.24"><span class="name"><span class="guimenu ">RADOS</span> (Ceph)
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.24">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.25.1"><span class="term "><span class="guimenu ">Use Ceph Deployed by Crowbar</span>
    </span></dt><dd><p>
      Select <span class="guimenu ">false</span>, if you are using an external Ceph cluster (see
      <a class="xref" href="#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for setup
      instructions).
     </p></dd><dt id="id-1.3.5.4.16.25.2"><span class="term "><span class="guimenu ">RADOS pool for Cinder volumes</span>
    </span></dt><dd><p>
      Name of the pool used to store the Cinder volumes.
     </p></dd><dt id="id-1.3.5.4.16.25.3"><span class="term "><span class="guimenu ">
      RADOS user (Set Only if Using CephX authentication)
     </span>
    </span></dt><dd><p>
      Ceph user name.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.16.26"><span class="name"><span class="guimenu ">VMware Parameters</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.26">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.27.1"><span class="term "><span class="guimenu ">vCenter Host/IP Address</span>
    </span></dt><dd><p>
      Host name or IP address of the vCenter server.
     </p></dd><dt id="id-1.3.5.4.16.27.2"><span class="term "><span class="guimenu ">vCenter Username</span> / <span class="guimenu ">vCenter
     Password</span>
    </span></dt><dd><p>
      vCenter login credentials.
     </p></dd><dt id="id-1.3.5.4.16.27.3"><span class="term "><span class="guimenu ">vCenter Cluster Names for Volumes</span>
    </span></dt><dd><p>
      Provide a comma-separated list of cluster names.
     </p></dd><dt id="id-1.3.5.4.16.27.4"><span class="term "><span class="guimenu ">Folder for Volumes</span>
    </span></dt><dd><p>
      Path to the directory used to store the Cinder volumes.
     </p></dd><dt id="id-1.3.5.4.16.27.5"><span class="term "><span class="guimenu ">CA file for verifying the vCenter certificate</span>
    </span></dt><dd><p>
      Absolute path to the vCenter CA certificate.
     </p></dd><dt id="id-1.3.5.4.16.27.6"><span class="term "><span class="guimenu ">
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </span>
    </span></dt><dd><p>
      Default value: <code class="literal">false</code> (the CA truststore is used for
      verification). Set this option to <code class="literal">true</code> when using
      self-signed certificates to disable certificate checks. This setting is
      for testing purposes only and must not be used in production
      environments!
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.16.28"><span class="name"><span class="guimenu ">Local file</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.28">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.29.1"><span class="term "><span class="guimenu ">Volume File Name</span>
    </span></dt><dd><p>
      Absolute path to the file to be used for block storage.
     </p></dd><dt id="id-1.3.5.4.16.29.2"><span class="term "><span class="guimenu ">Maximum File Size (GB)</span>
    </span></dt><dd><p>
      Maximum size of the volume file. Make sure not to overcommit the size,
      since it will result in data loss.
     </p></dd><dt id="id-1.3.5.4.16.29.3"><span class="term "><span class="guimenu ">Name of Volume</span>
    </span></dt><dd><p>
      Specify a name for the Cinder volume.
     </p></dd></dl></div><div id="id-1.3.5.4.16.30" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Using <span class="guimenu ">Local File</span> for Block Storage</h6><p>
    Using a file for block storage is not recommended for production systems,
    because of performance and data security reasons.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.3.5.4.16.31"><span class="name"><span class="guimenu ">Other driver</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.31">#</a></h4></div><p>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, as it is not supported.
  </p><div class="figure" id="id-1.3.5.4.16.33"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_cinder.png" target="_blank"><img src="images/depl_barclamp_cinder.png" width="" alt="The Cinder Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.14: </span><span class="name">The Cinder Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.33">#</a></h6></div></div><p>
   The Cinder component consists of two different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.16.35.1"><span class="term "><span class="guimenu ">cinder-controller</span>
    </span></dt><dd><p>
      The Cinder controller provides the scheduler and the API.
      Installing <span class="guimenu ">cinder-controller</span> on a Control Node is
      recommended.
     </p></dd><dt id="id-1.3.5.4.16.35.2"><span class="term "><span class="guimenu ">cinder-volume</span>
    </span></dt><dd><p>
      The virtual block storage service. It can be installed on a Control Node.
      However, we recommend deploying it on one or more dedicated nodes
      supplied with sufficient networking capacity to handle the increase in
      network traffic.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.16.36"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_cinder_node_deployment.png" target="_blank"><img src="images/depl_barclamp_cinder_node_deployment.png" width="" alt="The Cinder Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.15: </span><span class="name">The Cinder Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.16.36">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-cinder-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Cinder</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-cinder-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-cinder-ha</li></ul></div></div></div></div><p>
    Both the <span class="guimenu ">cinder-controller</span> and the
    <span class="guimenu ">cinder-volume</span> role can be deployed on a cluster.
   </p><div id="id-1.3.5.4.16.37.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Moving <span class="guimenu ">cinder-volume</span> to a Cluster</h6><p>
     If you need to re-deploy <span class="guimenu ">cinder-volume</span> role from a
     single machine to a cluster environment, the following will happen:
     Volumes that are currently attached to instances will continue to work,
     but adding volumes to instances will not succeed.
    </p><p>
     To solve this issue, run the following script once on each node that
     belongs to the <span class="guimenu ">cinder-volume</span> cluster:
     <code class="filename">/usr/bin/cinder-migrate-volume-names-to-cluster</code>.
    </p><p>
     The script is automatically installed by Crowbar on every machine or
     cluster that has a <span class="guimenu ">cinder-volume</span> role applied to it.
    </p></div><p>
    In combination with Ceph or a network storage solution, deploying
    Cinder in a cluster minimizes the potential downtime. For <span class="guimenu ">cinder-volume</span> to
    be applicable to a cluster, the role needs all Cinder backends to be configured for non-local storage. If you are using local volumes or raw devices
    in any of your volume backends, you cannot apply <span class="guimenu ">cinder-volume</span> to a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-quantum"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Neutron</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-quantum">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-quantum</li></ul></div></div></div></div><p>
   Neutron provides network connectivity between interface devices managed by
   other <span class="productname">OpenStack</span> components (most likely Nova). The service works by
   enabling users to create their own networks and then attach interfaces to
   them.
  </p><p>
   Neutron must be deployed on a Control Node. You first need to choose a core
   plug-in—<span class="guimenu ">ml2</span> or <span class="guimenu ">vmware</span>. Depending
   on your choice, more configuration options will become available.
  </p><p>
   The <span class="guimenu ">vmware</span> option lets you use an existing VMware
   installation. Using this plug-in is not a prerequisite for the VMware
   vSphere hypervisor support. For all other scenarios, choose
   <span class="guimenu ">ml2</span>.
  </p><p>
   The only global option that can be configured is <span class="guimenu ">SSL
   Support</span>. Choose whether to encrypt public communication
   (<span class="guimenu ">HTTPS</span>) or not (<span class="guimenu ">HTTP</span>). If choosing
   <span class="guimenu ">HTTPS</span>, refer to
   <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.17.6"><span class="name"><span class="guimenu ">ml2</span> (Modular Layer 2)
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.17.6">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.17.7.1"><span class="term "><span class="guimenu ">Modular Layer 2 Mechanism Drivers</span>
    </span></dt><dd><p>
      Select which mechanism driver(s) shall be enabled for the ml2 plug-in. It
      is possible to select more than one driver by holding the <span class="keycap">Ctrl</span> key while clicking. Choices are:
     </p><p><span class="formalpara-title"><span class="guimenu ">openvswitch</span>. </span>
       Supports GRE, VLAN and VXLAN networks (to be configured via the
       <span class="guimenu ">Modular Layer 2 type drivers</span> setting).
      </p><p><span class="formalpara-title"><span class="guimenu ">linuxbridge</span>. </span>
       Supports VLANs only. Requires to specify the <span class="guimenu ">Maximum Number of
       VLANs</span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">cisco_nexus</span>. </span>
       Enables Neutron to dynamically adjust the VLAN settings of the ports of
       an existing Cisco Nexus switch when instances are launched. It also
       requires <span class="guimenu ">openvswitch</span> which will automatically be
       selected. With <span class="guimenu ">Modular Layer 2 type drivers</span>,
       <span class="guimenu ">vlan</span> must be added. This option also requires to
       specify the <span class="guimenu ">Cisco Switch Credentials</span>. See
       <a class="xref" href="#app-deploy-cisco" title="Appendix B. Using Cisco Nexus Switches with Neutron">Appendix B, <em>Using Cisco Nexus Switches with Neutron</em></a> for details.
      </p><p><span class="formalpara-title"><span class="guimenu ">vmware_dvs</span>. </span>
       vmware_dvs driver makes it possible to use Neutron for networking in a
       VMware-based environment. Choosing <span class="guimenu ">vmware_dvs</span>,
       automatically selects the required <span class="guimenu ">openswitch</span>, <span class="guimenu ">vxlan</span>, and
       <span class="guimenu ">vlan</span> drivers. In the <span class="guimenu ">Raw</span> view,
       it is also possible to configure two additional attributes:
       <span class="guimenu ">clean_on_start</span> (clean up the DVS portgroups on the
       target vCenter Servers when neutron-server is restarted) and
       <span class="guimenu ">precreate_networks</span> (create DVS portgroups
       corresponding to networks in advance, rather than when virtual machines are attached to these networks).
      </p></dd><dt id="id-1.3.5.4.17.7.2"><span class="term "><span class="guimenu ">Use Distributed Virtual Router Setup</span>
    </span></dt><dd><p>
      With the default setup, all intra-Compute Node traffic flows through the
      network Control Node. The same is true for all traffic from floating IPs.
      In large deployments the network Control Node can therefore quickly become
      a bottleneck. When this option is set to <span class="guimenu ">true</span>, network
      agents will be installed on all compute nodes. This will de-centralize
      the network traffic, since Compute Nodes will be able to directly
      <span class="quote">“<span class="quote ">talk</span>”</span> to each other. Distributed Virtual Routers (DVR)
      require the <span class="guimenu ">openvswitch</span> driver and will not work with
      the <span class="guimenu ">linuxbridge</span> driver. For details on DVR refer to
      <a class="link" href="https://wiki.openstack.org/wiki/Neutron/DVR" target="_blank">https://wiki.openstack.org/wiki/Neutron/DVR</a>.
     </p></dd><dt id="id-1.3.5.4.17.7.3"><span class="term "><span class="guimenu ">Modular Layer 2 Type Drivers</span>
    </span></dt><dd><p>
      This option is only available when having chosen the
      <span class="guimenu ">openvswitch</span> or the <span class="guimenu ">cisco_nexus</span>
      mechanism drivers. Options are <span class="guimenu ">vlan</span>,
      <span class="guimenu ">gre</span> and <span class="guimenu ">vxlan</span>. It is possible to
      select more than one driver by holding the <span class="keycap">Ctrl</span>
      key while clicking.
     </p><p>
      When multiple type drivers are enabled, you need to select the
      <span class="guimenu ">Default Type Driver for Provider Network</span>, that will be
      used for newly created provider networks. This also includes the
      <code class="literal">nova_fixed</code> network, that will be created when applying
      the Neutron proposal. When manually creating provider networks with the
      <code class="command">neutron</code> command, the default can be overwritten with
      the <code class="option">--provider:network_type
      <em class="replaceable ">type</em></code> switch. You will also need to
      set a <span class="guimenu ">Default Type Driver for Tenant Network</span>. It is
      not possible to change this default when manually creating tenant
      networks with the <code class="command">neutron</code> command. The non-default
      type driver will only be used as a fallback.
     </p><p>
      Depending on your choice of the type driver, more configuration options
      become available.
     </p><p><span class="formalpara-title"><span class="guimenu ">gre</span>. </span>
       Having chosen <span class="guimenu ">gre</span>, you also need to specify the start
       and end of the tunnel ID range.
      </p><p><span class="formalpara-title"><span class="guimenu ">vlan</span>. </span>
       The option <span class="guimenu ">vlan</span> requires you to specify the
       <span class="guimenu ">Maximum number of VLANs</span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">vxlan</span>. </span>
       Having chosen <span class="guimenu ">vxlan</span>, you also need to specify the
       start and end of the VNI range.
      </p></dd></dl></div><div id="id-1.3.5.4.17.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Drivers for the VMware Compute Node</h6><p>
    Neutron must not be deployed with the <code class="literal">openvswitch with
    gre</code> plug-in. See <a class="xref" href="#app-deploy-vmware" title="Appendix A. VMware vSphere Installation Instructions">Appendix A, <em>VMware vSphere Installation Instructions</em></a> for details.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.17.9"><span class="name"><span class="guimenu ">z/VM Configuration</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.17.9">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.17.10.1"><span class="term ">xCAT Host/IP Address</span></dt><dd><p>
      Host name or IP address of the xCAT Management Node.
      
     </p></dd><dt id="id-1.3.5.4.17.10.2"><span class="term ">xCAT Username/Password</span></dt><dd><p>
      xCAT login credentials.
     </p></dd><dt id="id-1.3.5.4.17.10.3"><span class="term ">rdev list for physnet1 vswitch uplink (if available)</span></dt><dd><p>
      List of rdev addresses that should be connected to this vswitch.
     </p></dd><dt id="id-1.3.5.4.17.10.4"><span class="term ">xCAT IP Address on Management Network</span></dt><dd><p>
      IP address of the xCAT management interface.
     </p></dd><dt id="id-1.3.5.4.17.10.5"><span class="term ">Net Mask of Management Network</span></dt><dd><p>
      Net mask of the xCAT management interface.
      
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.17.11"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_network.png" target="_blank"><img src="images/depl_barclamp_network.png" width="" alt="The Neutron Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.16: </span><span class="name">The Neutron Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.17.11">#</a></h6></div></div><p>
   The Neutron component consists of two different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.17.13.1"><span class="term "><span class="guimenu ">neutron-server</span>
    </span></dt><dd><p>
      <span class="guimenu ">neutron-server</span> provides the scheduler and the API. It
      needs to be installed on a Control Node.
     </p></dd><dt id="id-1.3.5.4.17.13.2"><span class="term "><span class="guimenu ">neutron-network</span>
    </span></dt><dd><p>
      This service runs the various agents that manage the network traffic of
      all the cloud instances. It acts as the DHCP and DNS server and as a
      gateway for all cloud instances. It is recommend to deploy this role on a
      dedicated node supplied with sufficient network capacity.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.17.14"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_neutron_node_deployment.png" target="_blank"><img src="images/depl_barclamp_neutron_node_deployment.png" width="" alt="The Neutron barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.17: </span><span class="name">The Neutron barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.17.14">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-network-infoblox"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Infoblox IPAM Plug-in</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-network-infoblox">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-network-infoblox</li></ul></div></div></div></div><p>
    In the Neutron barclamp, you can enable support for the infoblox IPAM
    plug-in and configure it. For configuration, the
    <code class="literal">infoblox</code> section contains the subsections
    <code class="literal">grids</code> and <code class="literal">grid_defaults</code>.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.17.15.3.1"><span class="term ">grids</span></dt><dd><p>
       This subsection must contain at least one entry. For each entry, the
       following parameters are required:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         admin_user_name
        </p></li><li class="listitem "><p>
         admin_password
        </p></li><li class="listitem "><p>
         grid_master_host
        </p></li><li class="listitem "><p>
         grid_master_name
        </p></li><li class="listitem "><p>
         data_center_name
        </p></li></ul></div><p>
       You can also add multiple entries to the <code class="literal">grids</code>
       section. However, the upstream infoblox agent only supports a single
       grid currently.
      </p></dd><dt id="id-1.3.5.4.17.15.3.2"><span class="term ">grid_defaults</span></dt><dd><p>
       This subsection contains the default settings that are used for each
       grid (unless you have configured specific settings within the
       <code class="literal">grids</code> section).
      </p></dd></dl></div><p>
    For detailed information on all infoblox-related configuration settings,
    see
    <a class="link" href="https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst" target="_blank">https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst</a>.
   </p><p>
    Currently, all configuration options for infoblox are only available in the
    <code class="literal">raw</code> mode of the Neutron barclamp. To enable support for
    the infoblox IPAM plug-in and configure it, proceed as follows:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      <span class="guimenu ">Edit</span> the Neutron barclamp proposal or create a new
      one.
     </p></li><li class="step "><p>
      Click <span class="guimenu ">Raw</span> and search for the following section:
     </p><div class="verbatim-wrap"><pre class="screen">"use_infoblox": false,</pre></div></li><li class="step "><p>
      To enable support for the infoblox IPAM plug-in, change this entry to:
     </p><div class="verbatim-wrap"><pre class="screen">"use_infoblox": true,</pre></div></li><li class="step "><p>
      In the <code class="literal">grids</code> section, configure at least one grid by
      replacing the example values for each parameter with real values.
     </p></li><li class="step "><p>
      If you need specific settings for a grid, add some of the parameters from
      the <code class="literal">grid_defaults</code> section to the respective grid entry
      and adjust their values.
     </p><p>
      Otherwise Crowbar applies the default setting to each grid when you save
      the barclamp proposal.
     </p></li><li class="step "><p>
      Save your changes and apply them.
     </p></li></ol></div></div></div><div class="sect2 " id="sec-depl-ostack-network-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Neutron</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-network-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-network-ha</li></ul></div></div></div></div><p>
    Neutron can be made highly available by deploying
    <span class="guimenu ">neutron-server</span> and <span class="guimenu ">neutron-network</span> on
    a cluster. While <span class="guimenu ">neutron-server</span> may be deployed on a
    cluster shared with other services, it is strongly recommended to use a
    dedicated cluster solely for the <span class="guimenu ">neutron-network</span> role.
   </p></div><div class="sect2 " id="sec-setup-multi-ext-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up Multiple External Networks</span> <a title="Permalink" class="permalink" href="#sec-setup-multi-ext-networks">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-setup-multi-ext-networks</li></ul></div></div></div></div><p>
   This section shows you how to create external networks on SUSE <span class="productname">OpenStack</span> Cloud.
  </p><div class="sect3 " id="sec-config-multi-ext-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.10.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Network Configurations</span> <a title="Permalink" class="permalink" href="#sec-config-multi-ext-networks">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-config-multi-ext-networks</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      If you have not yet deployed Crowbar, add the following configuration to
      <code class="filename">/etc/crowbar/network.json</code>
      to set up an external network, using the name of your new network, VLAN
      ID, and network addresses. If you have already deployed Crowbar, then add
      this configuration to the <span class="guimenu ">Raw</span> view of the Network Barclamp.
     </p><div class="verbatim-wrap"><pre class="screen">"<em class="replaceable ">public2</em>": {
          "conduit": "intf1",
          "vlan": <em class="replaceable ">600</em>,
          "use_vlan": true,
          "add_bridge": false,
          "subnet": "<em class="replaceable ">192.168.135.128</em>",
          "netmask": "<em class="replaceable ">255.255.255.128</em>",
          "broadcast": "<em class="replaceable ">192.168.135.255</em>",
          "ranges": {
            "host": { "start": "<em class="replaceable ">192.168.135.129</em>",
               "end": "<em class="replaceable ">192.168.135.254</em>" }
          }
    },</pre></div></li><li class="step "><p>
      Modify the <em class="parameter ">additional_external_networks</em> in the
      <span class="guimenu ">Raw</span> view of the Neutron Barclamp with the name of your
      new external network.
     </p></li><li class="step "><p>
       Apply both barclamps, and it may also be necessary to re-apply the Nova
       Barclamp.
   </p></li><li class="step "><p>
      Then follow the steps in the next section to create the new external network.
    </p></li></ol></div></div></div><div class="sect3 " id="sec-confignet"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.10.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create the New External Network</span> <a title="Permalink" class="permalink" href="#sec-confignet">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-confignet</li></ul></div></div></div></div><p>
    The following steps add the network settings, including IP address pools,
    gateway, routing, and virtual switches to your new network.
   </p><div class="procedure " id="pro-confignet"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Set up interface mapping using either Open vSwitch (OVS) or Linuxbridge.
      For Open vSwitch run the following command:
     </p><div class="verbatim-wrap"><pre class="screen">neutron net-create <em class="replaceable ">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable ">public2</em> --router:external=True</pre></div><p>
      For Linuxbridge run the following command:
     </p><div class="verbatim-wrap"><pre class="screen">neutron net-create --router:external True --provider:physical_network physnet1 \
 --provider:network_type vlan --provider:segmentation_id <em class="replaceable ">600</em></pre></div></li><li class="step "><p>
      If a different network is used then Crowbar will create a new interface
      mapping. Then you can use a flat network:
     </p><div class="verbatim-wrap"><pre class="screen">neutron net-create <em class="replaceable ">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable ">public2</em> --router:external=True</pre></div></li><li class="step "><p>
      Create a subnet:
     </p><div class="verbatim-wrap"><pre class="screen">neutron subnet-create --name <em class="replaceable ">public2</em> --allocation-pool \
 start=<em class="replaceable ">192.168.135.2</em>,end=<em class="replaceable ">192.168.135.127</em> --gateway <em class="replaceable ">192.168.135.1</em> <em class="replaceable ">public2</em> \
 <em class="replaceable ">192.168.135.0/24</em> --enable_dhcp False</pre></div></li><li class="step "><p>
      Create a router, <em class="replaceable ">router2</em>:
     </p><div class="verbatim-wrap"><pre class="screen">neutron router-create <em class="replaceable ">router2</em></pre></div></li><li class="step "><p>
      Connect <em class="replaceable ">router2</em> to the new external network:
     </p><div class="verbatim-wrap"><pre class="screen">neutron router-gateway-set <em class="replaceable ">router2</em>  <em class="replaceable ">public2</em></pre></div></li><li class="step "><p>
      Create a new private network and connect it to
      <em class="replaceable ">router2</em>
     </p><div class="verbatim-wrap"><pre class="screen">neutron net-create priv-net
neutron subnet-create priv-net --gateway <em class="replaceable ">10.10.10.1 10.10.10.0/24</em> \
 --name priv-net-sub
neutron router-interface-add <em class="replaceable ">router2</em> priv-net-sub</pre></div></li><li class="step "><p>
      Boot a VM on priv-net-sub and set a security group that allows SSH.
     </p></li><li class="step "><p>
      Assign a floating IP address to the VM, this time from network
      <em class="replaceable ">public2</em>.
     </p></li><li class="step "><p>
      From the node verify that SSH is working by opening an SSH session to the
      VM.
     </p></li></ol></div></div></div><div class="sect3 " id="sec-howbridges"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.10.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How the Network Bridges are Created</span> <a title="Permalink" class="permalink" href="#sec-howbridges">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-howbridges</li></ul></div></div></div></div><p>
    For OVS, a new bridge will be created by Crowbar, in this case
    <code class="literal">br-public2</code>. In the bridge mapping the new network will
    be assigned to the bridge. The interface specified in
    <code class="filename">/etc/crowbar/network.json</code> (in this case eth0.600) will
    be plugged into <code class="literal">br-public2</code>. The new public network can
    be created in Neutron using the new public network name as
    <em class="parameter ">provider:physical_network</em>.
   </p><p>
    For Linuxbridge, Crowbar will check the interface associated with
    <em class="replaceable ">public2</em>. If this is the same as physnet1 no
    interface mapping will be created. The new public network can be created in
    Neutron using physnet1 as physical network and specifying the correct VLAN
    ID:
   </p><div class="verbatim-wrap"><pre class="screen">neutron net-create <em class="replaceable ">public2</em> --router:external True \
 --provider:physical_network physnet1 --provider:network_type vlan \
 --provider:segmentation_id <em class="replaceable ">600</em></pre></div><p>
    A bridge named <code class="varname">brq-NET_ID</code> will be created and the
    interface specified in <code class="filename">/etc/crowbar/network.json</code> will
    be plugged into it. If a new interface is associated in
    <code class="filename">/etc/crowbar/network.json</code> with
    <em class="replaceable ">public2</em> then Crowbar will add a new interface
    mapping and the second public network can be created using
    <em class="replaceable ">public2</em> as the physical network:
   </p><div class="verbatim-wrap"><pre class="screen">neutron net-create <em class="replaceable ">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable ">public2</em> --router:external=True</pre></div></div></div></div><div class="sect1 " id="sec-depl-ostack-nova"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Nova</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-nova">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-nova</li></ul></div></div></div></div><p>
   Nova provides key services for managing the SUSE <span class="productname">OpenStack</span> Cloud, sets up the
   Compute Nodes. SUSE <span class="productname">OpenStack</span> Cloud currently supports KVM, Xen and VMware vSphere. The
   unsupported QEMU option is included to enable test setups with virtualized
   nodes. The following attributes can be configured for Nova:
   
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.18.3.1"><span class="term "><span class="guimenu ">
      Scheduler Options: Virtual RAM to Physical RAM allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote ">overcommit ratio</span>”</span> for RAM for instances on the
      Compute Nodes. A ratio of <code class="literal">1.0</code> means no overcommitment.
      Changing this value is not recommended.
     </p></dd><dt id="id-1.3.5.4.18.3.2"><span class="term "><span class="guimenu ">
      Scheduler Options: Virtual CPU to Physical CPU allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote ">overcommit ratio</span>”</span> for CPUs for instances on the
      Compute Nodes. A ratio of <code class="literal">1.0</code> means no overcommitment.
     </p></dd><dt id="id-1.3.5.4.18.3.3"><span class="term "><span class="guimenu ">
      Scheduler Options: Virtual Disk to Physical Disk allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote ">overcommit ratio</span>”</span> for virtual disks for instances
      on the Compute Nodes. A ratio of <code class="literal">1.0</code> means no
      overcommitment.
     </p></dd><dt id="id-1.3.5.4.18.3.4"><span class="term "><span class="guimenu ">
      Scheduler Options: Reserved Memory for Nova Compute hosts (MB)
     </span>
    </span></dt><dd><p>
      Amount of reserved host memory that is not used for allocating VMs by
      Nova Compute.
     </p></dd><dt id="id-1.3.5.4.18.3.5"><span class="term "><span class="guimenu ">Live Migration Support: Enable Libvirt Migration</span>
    </span></dt><dd><p>
      Allows to move KVM and Xen instances to a different Compute Node
      running the same hypervisor (cross hypervisor migrations are not
      supported). Useful when a Compute Node needs to be shut down or rebooted
      for maintenance or when the load of the Compute Node is very high.
      Instances can be moved while running (Live Migration).
     </p><div id="id-1.3.5.4.18.3.5.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Libvirt Migration and Security</h6><p>
       Enabling the libvirt migration option will open a TCP port on the
       Compute Nodes that allows access to all instances from all machines in
       the admin network. Ensure that only authorized machines have access to
       the admin network when enabling this option.
      </p></div><div id="id-1.3.5.4.18.3.5.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Specifying Network for Live Migration</h6><p>
        It is possible to change a network to live migrate images. This is done
        in the raw view of the Nova barclamp. In the
        <code class="literal">migration</code> section, change the
        <code class="varname">network</code> attribute to the appropriate value (for
        example, <code class="literal">storage</code> for Ceph).
      </p></div></dd><dt id="id-1.3.5.4.18.3.6"><span class="term ">Live Migration Support: Setup Shared Storage</span></dt><dd><p>
      Sets up a directory <code class="filename">/var/lib/nova/instances</code> on the
      Control Node on which <span class="guimenu ">nova-controller</span> is running. This
      directory is exported via NFS to all compute nodes and will host a copy
      of the root disk of <span class="emphasis"><em>all</em></span> Xen instances. This setup
      is required for live migration of Xen instances (but not for KVM)
      and is used to provide central handling of instance data. Enabling this
      option is only recommended if Xen live migration is
      required—otherwise it should be disabled.
     </p><div id="id-1.3.5.4.18.3.6.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Do Not Set Up Shared Storage When instances are Running</h6><p>
       Setting up shared storage in a SUSE <span class="productname">OpenStack</span> Cloud where instances are running will
       result in connection losses to all running instances. It is strongly
       recommended to set up shared storage when deploying SUSE <span class="productname">OpenStack</span> Cloud. If it needs
       to be done at a later stage, make sure to shut down all instances prior
       to the change.
      </p></div></dd><dt id="id-1.3.5.4.18.3.7"><span class="term "><span class="guimenu ">KVM Options: Enable Kernel Samepage Merging</span>
    </span></dt><dd><p>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the Compute Nodes when using
      the KVM hypervisor at the cost of slightly increasing CPU usage.
     </p></dd><dt id="id-1.3.5.4.18.3.8"><span class="term ">VMware vCenter Settings</span></dt><dd><p>
      Setting up VMware support is described in a separate section. See
      <a class="xref" href="#app-deploy-vmware" title="Appendix A. VMware vSphere Installation Instructions">Appendix A, <em>VMware vSphere Installation Instructions</em></a>.
     </p></dd><dt id="id-1.3.5.4.18.3.9"><span class="term ">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If choosing
      <span class="guimenu ">HTTPS</span>,refer to
      <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
     </p></dd><dt id="id-1.3.5.4.18.3.10"><span class="term ">VNC Settings: Keymap</span></dt><dd><p>
      Change the default VNC keymap for instances. By default,
      <code class="literal">en-us</code> is used. Enter the value in lowercase, either as
      a two character code (such as <code class="literal">de</code> or
      <code class="literal">jp</code>) or, as a five character code such as
      <code class="literal">de-ch</code> or <code class="literal">en-uk</code>, if applicable.
     </p></dd><dt id="id-1.3.5.4.18.3.11"><span class="term ">VNC Settings: NoVNC Protocol</span></dt><dd><p>
      After having started an instance you can display its VNC console in the
      <span class="productname">OpenStack</span> Dashboard (Horizon) via the browser using the noVNC
      implementation. By default this connection is not encrypted and can
      potentially be eavesdropped.
     </p><p>
      Enable encrypted communication for noVNC by choosing
      <span class="guimenu ">HTTPS</span> and providing the locations for the certificate
      key pair files.
     </p></dd><dt id="id-1.3.5.4.18.3.12"><span class="term "><span class="guimenu ">Logging: Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu ">true</span>.
     </p></dd></dl></div><div id="note-custom-vendor" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Custom Vendor Data for Instances</h6><p>
    You can pass custom vendor data to all VMs via Nova's metadata server.
    For example, information about a custom SMT server can be used by the
    SUSE guest images to automatically configure the repositories for the
    guest.
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To pass custom vendor data, switch to the <span class="guimenu ">Raw</span> view of
      the Nova barclamp.
     </p></li><li class="listitem "><p>
      Search for the following section:
     </p><div class="verbatim-wrap"><pre class="screen">"metadata": {
  "vendordata": {
    "json": "{}"
  }
}</pre></div></li><li class="listitem "><p>
      As value of the <code class="literal">json</code> entry, enter valid JSON data. For
      example:
     </p><div class="verbatim-wrap"><pre class="screen">"metadata": {
  "vendordata": {
    "json": "{\"<em class="replaceable ">CUSTOM_KEY</em>\": \"<em class="replaceable ">CUSTOM_VALUE</em>\"}"
  }
}</pre></div><p>
      The string needs to be escaped because the barclamp file is in JSON
      format, too.
     </p></li></ol></div><p>
    Use the following command to access the custom vendor data from inside a
    VM:
   </p><div class="verbatim-wrap"><pre class="screen">curl -s http://<em class="replaceable ">METADATA_SERVER</em>/openstack/latest/vendor_data.json</pre></div><p>
    The IP address of the metadata server is always the same from within a VM.
    For more details, see
    <a class="link" href="https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/" target="_blank">https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/</a>.
   </p></div><div class="figure" id="id-1.3.5.4.18.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nova.png" target="_blank"><img src="images/depl_barclamp_nova.png" width="" alt="The Nova Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.18: </span><span class="name">The Nova Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.18.5">#</a></h6></div></div><p>
   The Nova component consists of eight different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.18.7.1"><span class="term "><span class="guimenu ">nova-controller</span>
    </span></dt><dd><p>
      Distributing and scheduling the instances is managed by the
      <span class="guimenu ">nova-controller</span>. It also provides networking and
      messaging services. <span class="guimenu ">nova-controller</span> needs to be
      installed on a Control Node.
     </p></dd><dt id="id-1.3.5.4.18.7.2"><span class="term "><span class="guimenu ">nova-compute-kvm</span> /
    <span class="guimenu ">nova-compute-qemu</span> /
    <span class="guimenu ">nova-compute-vmware</span> /
    <span class="guimenu ">nova-compute-xen</span> /
    </span></dt><dd><p>
      Provides the hypervisors (KVM, QEMU, VMware vSphere, Xen, and z/VM)
      and tools needed to manage the instances. Only one hypervisor can be
      deployed on a single compute node. To use different hypervisors in your
      cloud, deploy different hypervisors to different Compute Nodes. A
      <code class="literal">nova-compute-*</code> role needs to be installed on every
      Compute Node. However, not all hypervisors need to be deployed.
     </p><p>
      Each image that will be made available in SUSE <span class="productname">OpenStack</span> Cloud to start an instance
      is bound to a hypervisor. Each hypervisor can be deployed on multiple
      Compute Nodes (except for the VMware vSphere role, see below). In a
      multi-hypervisor deployment you should make sure to deploy the
      <code class="literal">nova-compute-*</code> roles in a way, that enough compute
      power is available for each hypervisor.
     </p><div id="id-1.3.5.4.18.7.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Re-assigning Hypervisors</h6><p>
       Existing <code class="literal">nova-compute-*</code> nodes can be changed in a
       production SUSE <span class="productname">OpenStack</span> Cloud without service interruption. You need to
       <span class="quote">“<span class="quote ">evacuate</span>”</span>

       the node, re-assign a new <code class="literal">nova-compute</code> role via the
       Nova barclamp and <span class="guimenu ">Apply</span> the change.
       <span class="guimenu ">nova-compute-vmware</span> can only be deployed on a single
       node.
      </p></div><div id="id-1.3.5.4.18.7.2.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Deploying VMware vSphere (vmware)</h6><p>
       
       VMware vSphere is not supported <span class="quote">“<span class="quote ">natively</span>”</span> by
       SUSE <span class="productname">OpenStack</span> Cloud—it rather delegates requests to an existing vCenter. It
       requires preparations at the vCenter and post install adjustments of the
       Compute Node. See <a class="xref" href="#app-deploy-vmware" title="Appendix A. VMware vSphere Installation Instructions">Appendix A, <em>VMware vSphere Installation Instructions</em></a> for instructions.
       <span class="guimenu ">nova-compute-vmware</span> can only be deployed on a single
       Compute Node.
      </p></div></dd></dl></div><div class="figure" id="id-1.3.5.4.18.8"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nova_node_deployment.png" target="_blank"><img src="images/depl_barclamp_nova_node_deployment.png" width="" alt="The Nova Barclamp: Node Deployment Example with Two KVM Nodes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.19: </span><span class="name">The Nova Barclamp: Node Deployment Example with Two KVM Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.18.8">#</a></h6></div></div><p>
    When deploying a <span class="guimenu ">nova-compute-vmware</span> node with the
    <span class="guimenu ">vmware_dvs</span> ML2 driver enabled in the Neutron barclamp, the following
    new attributes are also available in the <span class="guimenu ">vcenter</span> section of the
    <span class="guimenu ">Raw</span> mode:<span class="guimenu ">dvs_name</span> (the name of the
    DVS switch configured on the target vCenter cluster) and
    <span class="guimenu ">dvs_security_groups</span> (enable or disable implementing
    security groups through DVS traffic rules).
  </p><p>
    It is important to specify the correct he <span class="guimenu ">dvs_name</span>
    value, as the barclamp expects the  DVS switch to be preconfigured on the
    target VMware vCenter cluster.
  </p><div id="id-1.3.5.4.18.11" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: vmware_dvs must be enabled</h6><p>
      Deploying <span class="guimenu ">nova-compute-vmware</span> nodes will not result in
      a functional cloud setup if the <span class="guimenu ">vmware_dvs</span> ML2 plug-in
      is not enabled in the Neutron barclamp.
     </p></div><div class="sect2 " id="sec-depl-ostack-nova-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Nova</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-nova-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-nova-ha</li></ul></div></div></div></div><p>
    Making <span class="guimenu ">nova-controller</span> highly available requires no
    special configuration—it is sufficient to deploy it on a cluster.
   </p><p>
    To enable High Availability for Compute Nodes, deploy the following roles to one or more
    clusters with remote nodes:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      nova-compute-kvm
     </p></li><li class="listitem "><p>
      nova-compute-qemu
     </p></li><li class="listitem "><p>
      nova-compute-xen
     </p></li><li class="listitem "><p>
      ec2-api
     </p></li></ul></div><p>
    The cluster to which you deploy the roles above can be completely
    independent of the one to which the role <code class="literal">nova-controller</code>
    is deployed.
   </p><p>
    However, the <code class="literal">nova-controller</code> and
    <code class="literal">ec2-api</code> roles must be deployed the same way (either
    <span class="emphasis"><em>both</em></span> to a cluster or <span class="emphasis"><em>both</em></span> to
    individual nodes. This is due to Crowbar design limitations.
   </p><div id="id-1.3.5.4.18.12.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Shared Storage</h6><p>
     It is recommended to use shared storage for the
     <code class="filename">/var/lib/nova/instances</code> directory, to ensure that
     ephemeral disks will be preserved during recovery of VMs from failed
     compute nodes. Without shared storage, any ephemeral disks will be lost,
     and recovery will rebuild the VM from its original image.
    </p><p>
     If an external NFS server is used, enable the following option in the
     Nova barclamp proposal: <span class="guimenu ">Shared Storage for Nova instances has
     been manually configured</span>.
    </p></div></div></div><div class="sect1 " id="sec-depl-ostack-dash"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Horizon (<span class="productname">OpenStack</span> Dashboard)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-dash">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-dash</li></ul></div></div></div></div><p>
   The last component that needs to be deployed is Horizon, the <span class="productname">OpenStack</span>
   Dashboard. It provides a Web interface for users to start and stop instances
   and for administrators to manage users, groups, roles, etc. Horizon should
   be installed on a Control Node. To make Horizon highly available, deploy it
   on a cluster.
  </p><p>
   The following attributes can be configured:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.19.4.1"><span class="term ">Session Timeout</span></dt><dd><p>
      Timeout (in minutes) after which a user is been logged out automatically.
      The default value is set to four hours (240 minutes).
     </p><div id="id-1.3.5.4.19.4.1.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Timeouts Larger than Four Hours</h6><p>
       Every Horizon session requires a valid Keystone token. These tokens
       also have a lifetime of four hours (14400 seconds). Setting the Horizon
       session timeout to a value larger than 240 will therefore have no
       effect, and you will receive a warning when applying the barclamp.
      </p><p>
       To successfully apply a timeout larger than four hours, you first need
       to adjust the Keystone token expiration accordingly. To do so, open the
       Keystone barclamp in <span class="guimenu ">Raw</span> mode and adjust the value of
       the key <code class="literal">token_expiration</code>. Note that the value has to
       be provided in <span class="emphasis"><em>seconds</em></span>. When the change is
       successfully applied, you can adjust the Horizon session timeout (in
       <span class="emphasis"><em>minutes</em></span>). Note that extending the Keystone token
       expiration may cause scalability issues in large and very busy SUSE <span class="productname">OpenStack</span> Cloud
       installations.
      </p></div></dd><dt id="id-1.3.5.4.19.4.2"><span class="term "><span class="guimenu ">
      User Password Validation: Regular expression used for password
      validation
     </span>
    </span></dt><dd><p>
      Specify a regular expression with which to check the password. The
      default expression (<code class="literal">.{8,}</code>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see
      <a class="link" href="http://docs.python.org/2.7/library/re.html#module-re" target="_blank">http://docs.python.org/2.7/library/re.html#module-re</a>
      for a reference).
     </p></dd><dt id="id-1.3.5.4.19.4.3"><span class="term "><span class="guimenu ">
      User Password Validation: Text to display if the password does not pass
      validation
     </span>
    </span></dt><dd><p>
      Error message that will be displayed in case the password validation
      fails.
     </p></dd><dt id="id-1.3.5.4.19.4.4"><span class="term ">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If choosing <span class="guimenu ">HTTPS</span>,
      you have two choices. You can either <span class="guimenu ">Generate (self-signed)
      certificates</span> or provide the locations for the certificate key
      pair files and,—optionally— the certificate chain file. Using
      self-signed certificates is for testing purposes only and should never be
      used in production environments!
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.19.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nova_dashboard.png" target="_blank"><img src="images/depl_barclamp_nova_dashboard.png" width="" alt="The Horizon Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.20: </span><span class="name">The Horizon Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.19.5">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-dash-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Horizon</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-dash-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-dash-ha</li></ul></div></div></div></div><p>
    Making Horizon highly available requires no special configuration—it
    is sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-heat"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Heat (Optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-heat">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-heat</li></ul></div></div></div></div><p>
   Heat is a template-based orchestration engine that enables you to, for
   example, start workloads requiring multiple servers or to automatically
   restart instances if needed. It also brings auto-scaling to SUSE <span class="productname">OpenStack</span> Cloud by
   automatically starting additional instances if certain criteria are met.
   For more information about Heat refer to the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/heat/" target="_blank">http://docs.openstack.org/developer/heat/</a>.
  </p><p>
   Heat should be deployed on a Control Node. To make Heat highly
   available, deploy it on a cluster.
  </p><p>
   The following attributes can be configured for Heat:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.20.5.1"><span class="term "><span class="guimenu ">Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu ">true</span>.
     </p></dd><dt id="id-1.3.5.4.20.5.2"><span class="term ">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If choosing
      <span class="guimenu ">HTTPS</span>, refer to
      <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.20.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_heat.png" target="_blank"><img src="images/depl_barclamp_heat.png" width="" alt="The Heat Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.21: </span><span class="name">The Heat Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.20.6">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-heat-delegated-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Identity Trusts Authorization (Optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-heat-delegated-roles">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-heat-delegated-roles</li></ul></div></div></div></div><p>
    Heat uses Keystone Trusts to delegate a subset of user roles to the
    Heat engine for deferred operations (see
    <a class="link" href="http://hardysteven.blogspot.de/2014/04/heat-auth-model-updates-part-1-trusts.html" target="_blank">Steve
    Hardy's blog</a> for details). It can either delegate all user roles or
    only those specified in the <code class="literal">trusts_delegated_roles</code>
    setting. Consequently, all roles listed in
    <code class="literal">trusts_delegated_roles</code> need to be assigned to a user,
    otherwise the user will not be able to use Heat.
   </p><p>
    The recommended setting for <code class="literal">trusts_delegated_roles</code> is
    <code class="literal">Member</code>, since this is the default role most users are
    likely to have. This is also the default setting when installing SUSE <span class="productname">OpenStack</span> Cloud
    from scratch.
   </p><p>
    On installations where this setting is introduced through an upgrade,
    <code class="literal">trusts_delegated_roles</code> will be set to
    <code class="literal">heat_stack_owner</code>. This is a conservative choice to
    prevent breakage in situations where unprivileged users may already have
    been assigned the <code class="literal">heat_stack_owner</code> role to enable them
    to use Heat but lack the <code class="literal">Member</code> role. As long as you can
    ensure that all users who have the <code class="literal">heat_stack_owner</code> role
    also have the <code class="literal">Member</code> role, it is both safe and
    recommended to change trusts_delegated_roles to <code class="literal">Member</code>.
    
   </p><p>
    To view or change the trusts_delegated_role setting you need to open the
    Heat barclamp and click <span class="guimenu ">Raw</span> in the
    <span class="guimenu ">Attributes</span> section. Search for the
    <code class="literal">trusts_delegated_roles</code> setting and modify the list of
    roles as desired.
   </p><div class="figure" id="id-1.3.5.4.20.7.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_heat_raw.png" target="_blank"><img src="images/depl_barclamp_heat_raw.png" width="" alt="the Heat barclamp: Raw Mode" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.22: </span><span class="name">the Heat barclamp: Raw Mode </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.20.7.6">#</a></h6></div></div><div id="id-1.3.5.4.20.7.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: Empty Value</h6><p>
     An empty value for <code class="literal">trusts_delegated_roles</code> will delegate
     <span class="emphasis"><em>all</em></span> of user roles to Heat. This may create a security
     risk for users who are assigned privileged roles, such as
     <code class="literal">admin</code>, because these privileged roles will also be
     delegated to the Heat engine when these users create Heat stacks.
    </p></div></div><div class="sect2 " id="sec-depl-ostack-heat-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Heat</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-heat-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-heat-ha</li></ul></div></div></div></div><p>
    Making Heat highly available requires no special configuration—it
    is sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-ceilometer"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Ceilometer (Optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ceilometer">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ceilometer</li></ul></div></div></div></div><p>
   
   Ceilometer collects CPU and networking data from SUSE <span class="productname">OpenStack</span> Cloud. This data can be
   used by a billing system to enable customer billing. Deploying Ceilometer is
   optional.
  </p><p>
   For more information about Ceilometer refer to the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/" target="_blank">http://docs.openstack.org/developer/ceilometer/</a>.
  </p><div id="id-1.3.5.4.21.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Ceilometer Restrictions</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> data measuring is only supported for
    KVM, Xen and Windows instances. Other hypervisors and SUSE <span class="productname">OpenStack</span> Cloud features
    such as object or block storage will not be measured.
   </p></div><p>
   The following attributes can be configured for Ceilometer:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.21.6.1"><span class="term ">
     Interval used for CPU/disk/network/other meter updates (in seconds)
    </span></dt><dd><p>
      Specify an interval in seconds after which Ceilometer performs an update
      of the specified meter.
     </p></dd><dt id="id-1.3.5.4.21.6.2"><span class="term ">Evaluation interval for threshold alarms (in seconds)</span></dt><dd><p>
      Set the interval after which to check whether to raise an alarm because a
      threshold has been exceeded. For performance reasons, do not set a value
      lower than the default (60s).
     </p></dd><dt id="id-1.3.5.4.21.6.3"><span class="term ">How long are metering/event samples kept in the database (in days)
    </span></dt><dd><p>
      Specify how long to keep the data. -1 means that samples are kept in the
      database forever.
     </p></dd><dt id="id-1.3.5.4.21.6.4"><span class="term ">Verbose Logging
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu ">true</span>.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.21.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_ceilometer.png" target="_blank"><img src="images/depl_barclamp_ceilometer.png" width="" alt="The Ceilometer Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.23: </span><span class="name">The Ceilometer Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.21.7">#</a></h6></div></div><div class="variablelist "><dl class="variablelist"><dt id="sec-depl-ostack-ceilometer-ssl"><span class="term ">SSL Support: Protocol
        </span></dt><dd><p>
      With the default value <span class="guimenu ">HTTP</span> enabled, public
      communication is not be encrypted. Choose <span class="guimenu ">HTTPS</span> to use
      SSL for encryption. See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for background
      information and <a class="xref" href="#sec-depl-inst-nodes-post-ssl" title="11.4.6. Enabling SSL">Section 11.4.6, “Enabling SSL”</a> for
      installation instructions. The following additional configuration options
      will become available when choosing <span class="guimenu ">HTTPS</span>:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.21.8.1.2.2.1"><span class="term "><span class="guimenu ">Generate (self-signed) certificates</span>
       </span></dt><dd><p>
         When set to <code class="literal">true</code>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </p></dd><dt id="id-1.3.5.4.21.8.1.2.2.2"><span class="term "><span class="guimenu ">SSL Certificate File</span> / <span class="guimenu ">SSL (Private) Key
        File</span>
       </span></dt><dd><p>
         Location of the certificate key pair files.
        </p></dd><dt id="id-1.3.5.4.21.8.1.2.2.3"><span class="term "><span class="guimenu ">SSL Certificate is insecure</span>
       </span></dt><dd><p>
         Set this option to <code class="literal">true</code> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </p></dd><dt id="id-1.3.5.4.21.8.1.2.2.4"><span class="term "><span class="guimenu ">SSL CA Certificates File</span>
       </span></dt><dd><p>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the barclamp to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <code class="systemitem">apache2</code> service, and
         re-deploy the barclamp.
        </p><p>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </p></dd></dl></div></dd></dl></div><p>
   The Ceilometer component consists of five different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.21.10.1"><span class="term "><span class="guimenu ">ceilometer-server</span>
    </span></dt><dd><p>
      The Ceilometer API server role. This role needs to be deployed on a
      Control Node. Ceilometer collects approximately 200 bytes of data per hour
      and instance. Unless you have a very huge number of instances, there is
      no need to install it on a dedicated node.
     </p></dd><dt id="id-1.3.5.4.21.10.2"><span class="term "><span class="guimenu ">ceilometer-polling</span>
    </span></dt><dd><p>
      The polling agent listens to the message bus to collect data. It needs to
      be deployed on a Control Node. It can be deployed on the same node as
      <span class="guimenu ">ceilometer-server</span>.
     </p></dd><dt id="id-1.3.5.4.21.10.3"><span class="term "><span class="guimenu ">ceilometer-agent</span>
    </span></dt><dd><p>
      The compute agents collect data from the compute nodes. They need to be
      deployed on all KVM and Xen compute nodes in your cloud (other
      hypervisors are currently not supported).
     </p></dd><dt id="id-1.3.5.4.21.10.4"><span class="term "><span class="guimenu ">ceilometer-swift-proxy-middleware</span>
    </span></dt><dd><p>
      An agent collecting data from the Swift nodes. This role needs to be
      deployed on the same node as swift-proxy.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.21.11"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_ceilometer_deployment.png" target="_blank"><img src="images/depl_barclamp_ceilometer_deployment.png" width="" alt="The Ceilometer Barclamp: Node Deployment" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.24: </span><span class="name">The Ceilometer Barclamp: Node Deployment </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.21.11">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-ceilometer-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Ceilometer</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ceilometer-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ceilometer-ha</li></ul></div></div></div></div><p>
    Making Ceilometer highly available requires no special
    configuration—it is sufficient to deploy the roles
    <span class="guimenu ">ceilometer-server</span> and
    <span class="guimenu ">ceilometer-polling</span> on a cluster. If you are using MySQL
    or PostgreSQL, you can use two nodes.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-manila"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Manila</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-manila">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-manila</li></ul></div></div></div></div><p>
   Manila provides coordinated access to shared or distributed file
   systems, similar to what Cinder does for block storage. These file
   systems can be shared between instances in SUSE <span class="productname">OpenStack</span> Cloud.
  </p><p>
   Manila uses different back-ends. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>
   currently supported back-ends include <span class="guimenu ">Hitachi HNAS</span>,
   <span class="guimenu ">NetApp Driver</span>, and <span class="guimenu ">CephFS</span>. Two more
   back-end options, <span class="guimenu ">Generic Driver</span> and <span class="guimenu ">Other
   Driver</span> are available for testing purposes and are not supported.
  </p><div id="note-limit-cephfs" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Limitations for CephFS Back-end</h6><p>
    Manila uses some CephFS features that are currently
    <span class="emphasis"><em>not</em></span> supported by the SUSE Linux Enterprise Server 12 SP3 CephFS kernel
    client:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      RADOS namespaces
     </p></li><li class="listitem "><p>
      MDS path restrictions
     </p></li><li class="listitem "><p>
      Quotas
     </p></li></ul></div><p>
    As a result, to access CephFS shares provisioned by Manila, you must
    use ceph-fuse. For details, see
    <a class="link" href="http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html" target="_blank">http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html</a>.
   </p></div><p>
   When first opening the Manila barclamp, the default proposal
   <span class="guimenu ">Generic Driver</span> is already available for configuration. To
   replace it, first delete it by clicking the trashcan icon and then choose a
   different back-end in the section <span class="guimenu ">Add new Manila Backend</span>.
   Select a <span class="guimenu ">Type of Share</span> and—optionally—provide
   a <span class="guimenu ">Name for Backend</span>. Activate the back-end with
   <span class="guimenu ">Add Backend</span>. Note that at least one back-end must be
   configured.
  </p><p>
   The attributes that can be set to configure Cinder depend on the back-end:
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.22.7"><span class="name"><span class="guimenu ">Back-end: Generic</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.22.7">#</a></h3></div><p>
   The generic driver is included as a technology preview and is not supported.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.22.9"><span class="name"><span class="guimenu ">Hitachi HNAS</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.22.9">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.22.10.1"><span class="term "><span class="guimenu ">Specify which EVS this backend is assigned to</span>
    </span></dt><dd><p>
      Provide the name of the Enterprise Virtual Server that the selected
      back-end is assigned to.
      
     </p></dd><dt id="id-1.3.5.4.22.10.2"><span class="term "><span class="guimenu ">Specify IP for mounting shares</span>
    </span></dt><dd><p>
      IP address for mounting shares.
      
     </p></dd><dt id="id-1.3.5.4.22.10.3"><span class="term "><span class="guimenu ">Specify file-system name for creating shares</span>
    </span></dt><dd><p>
      Provide a file-system name for creating shares.
      
     </p></dd><dt id="id-1.3.5.4.22.10.4"><span class="term "><span class="guimenu ">HNAS management interface IP</span>
    </span></dt><dd><p>
      IP address of the HNAS management interface for communication between
      Manila controller and HNAS.
     </p></dd><dt id="id-1.3.5.4.22.10.5"><span class="term "><span class="guimenu ">HNAS username Base64 String</span>
    </span></dt><dd><p>
      HNAS username Base64 String required to perform tasks like creating
      file-systems and network interfaces.
     </p></dd><dt id="id-1.3.5.4.22.10.6"><span class="term "><span class="guimenu ">HNAS user password</span>
    </span></dt><dd><p>
      HNAS user password. Required only if private key is not provided.
      
     </p></dd><dt id="id-1.3.5.4.22.10.7"><span class="term "><span class="guimenu ">RSA/DSA private key</span>
    </span></dt><dd><p>
      RSA/DSA private key necessary for connecting to HNAS. Required only if
      password is not provided.
      
     </p></dd><dt id="id-1.3.5.4.22.10.8"><span class="term "><span class="guimenu ">The time to wait for stalled HNAS jobs before aborting</span>
    </span></dt><dd><p>
      Time in seconds to wait before aborting stalled HNAS jobs.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.22.11"><span class="name"><span class="guimenu ">Back-end: Netapp</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.22.11">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.22.12.1"><span class="term "><span class="guimenu ">Name of the Virtual Storage Server (vserver)</span>
    </span></dt><dd><p>
      Host name of the Virtual Storage Server.
     </p></dd><dt id="id-1.3.5.4.22.12.2"><span class="term "><span class="guimenu ">Server Host Name</span>
    </span></dt><dd><p>
      The name or IP address for the storage controller or the cluster.
     </p></dd><dt id="id-1.3.5.4.22.12.3"><span class="term "><span class="guimenu ">Server Port</span>
    </span></dt><dd><p>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </p></dd><dt id="id-1.3.5.4.22.12.4"><span class="term "><span class="guimenu ">User name/Password for Accessing NetApp</span>
    </span></dt><dd><p>
      Login credentials.
     </p></dd><dt id="id-1.3.5.4.22.12.5"><span class="term "><span class="guimenu ">Transport Type</span>
    </span></dt><dd><p>
      Transport protocol for communicating with the storage controller or
      cluster. Supported protocols are HTTP and HTTPS. Choose the protocol your
      NetApp is licensed for.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.22.13"><span class="name"><span class="guimenu ">Back-end: CephFS</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.22.13">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.22.14.1"><span class="term ">Use Ceph deployed by Crowbar</span></dt><dd><p>
      Set to <code class="systemitem">true</code> to use Ceph deployed with Crowbar.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.22.15"><span class="name"><span class="guimenu ">Back-end: Manual</span>
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.22.15">#</a></h3></div><p>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </p><div class="figure" id="id-1.3.5.4.22.17"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_manila.png" target="_blank"><img src="images/depl_barclamp_manila.png" width="" alt="The Manila Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.25: </span><span class="name">The Manila Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.22.17">#</a></h6></div></div><p>
   The Manila component consists of two different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.22.19.1"><span class="term "><span class="guimenu ">manila-server</span>
    </span></dt><dd><p>
      The Manila server provides the scheduler and the API. Installing it
      on a Control Node is recommended.
     </p></dd><dt id="id-1.3.5.4.22.19.2"><span class="term "><span class="guimenu ">manila-share</span>
    </span></dt><dd><p>
      The shared storage service. It can be installed on a Control Node, but it
      is recommended to deploy it on one or more dedicated nodes supplied with
      sufficient disk space and networking capacity, since it will generate a
      lot of network traffic.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.22.20"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_manila_node_deployment.png" target="_blank"><img src="images/depl_barclamp_manila_node_deployment.png" width="" alt="The Manila Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.26: </span><span class="name">The Manila Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.22.20">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-manila-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Manila</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-manila-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-manila-ha</li></ul></div></div></div></div><p>
    While the <span class="guimenu ">manila-server</span> role can be deployed on a
    cluster, deploying <span class="guimenu ">manila-share</span> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <span class="guimenu ">manila-share</span> on several nodes—this ensures the
    service continues to be available even when a node fails.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-tempest"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Tempest (Optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-tempest">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-tempest</li></ul></div></div></div></div><p>
   Tempest is an integration test suite for SUSE <span class="productname">OpenStack</span> Cloud written in Python. It
   contains multiple integration tests for validating your SUSE <span class="productname">OpenStack</span> Cloud deployment.
   For more information about Tempest refer to the <span class="productname">OpenStack</span> documentation
   at <a class="link" href="http://docs.openstack.org/developer/tempest/" target="_blank">http://docs.openstack.org/developer/tempest/</a>.
  </p><div id="id-1.3.5.4.23.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Technology Preview</h6><p>
    Tempest is only included as a technology preview and not supported.
   </p><p>
    Tempest may be used for testing whether the intended setup will run
    without problems. It should not be used in a production environment.
   </p></div><p>
   Tempest should be deployed on a Control Node.
  </p><p>
   The following attributes can be configured for Tempest:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.23.6.1"><span class="term "><span class="guimenu ">Choose User name / Password</span>
    </span></dt><dd><p>
      Credentials for a regular user. If the user does not exist, it will be
      created.
     </p></dd><dt id="id-1.3.5.4.23.6.2"><span class="term "><span class="guimenu ">Choose Tenant</span>
    </span></dt><dd><p>
      Tenant to be used by Tempest. If it does not exist, it will be
      created. It is safe to stick with the default value.
     </p></dd><dt id="id-1.3.5.4.23.6.3"><span class="term "><span class="guimenu ">Choose Tempest Admin User name/Password</span>
    </span></dt><dd><p>
      Credentials for an admin user. If the user does not exist, it will be
      created.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.23.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_tempest.png" target="_blank"><img src="images/depl_barclamp_tempest.png" width="" alt="The Tempest Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.27: </span><span class="name">The Tempest Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.23.7">#</a></h6></div></div><div id="id-1.3.5.4.23.8" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Running Tests</h6><p>
    To run tests with Tempest, log in to the Control Node on which
    Tempest was deployed. Change into the directory
    <code class="filename">/var/lib/openstack-tempest-test</code>. To get an overview of
    available commands, run:
   </p><div class="verbatim-wrap"><pre class="screen">./tempest --help</pre></div><p>
    To serially invoke a subset of all tests (<span class="quote">“<span class="quote ">the gating
    smoketests</span>”</span>) to help validate the working functionality of your
    local cloud instance, run the following command. It will save the output to
    a log file
    <code class="filename">tempest_<em class="replaceable ">CURRENT_DATE</em>.log</code>.
   </p><div class="verbatim-wrap"><pre class="screen">./tempest run --smoke --serial 2&gt;&amp;1 \
| tee "tempest_$(date +%Y-%m-%d_%H%M%S).log"</pre></div></div><div class="sect2 " id="sec-depl-ostack-tempest-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Tempest</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-tempest-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-tempest-ha</li></ul></div></div></div></div><p>
    Tempest cannot be made highly available.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-magnum"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Magnum (Optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-magnum">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-magnum</li></ul></div></div></div></div><p>
   Magnum is an <span class="productname">OpenStack</span> project which offers container orchestration
   engines for deploying and managing containers as first class resources in
   <span class="productname">OpenStack</span>.
  </p><p>
   For more information about Magnum, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/magnum/" target="_blank">http://docs.openstack.org/developer/magnum/</a>.
  </p><p>
   For information on how to deploy a Kubernetes cluster (either from command
   line or from the Horizon Dashboard), see the <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">End User Guide</em></em>. It is available
   from <a class="link" href="https://documentation.suse.com/soc/8/" target="_blank">https://documentation.suse.com/soc/8/</a>.
  </p><p>
   The following <span class="guimenu ">Attributes</span> can be configured for
   Magnum:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.24.6.1"><span class="term "><span class="guimenu ">Trustee Domain</span>: <span class="guimenu ">Delegate trust to
    cluster users if required</span></span></dt><dd><p>
      Deploying Kubernetes clusters in a cloud without an Internet connection
      (see also <a class="link" href="https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-supplement/#sec-deploy-kubernetes-without" target="_blank">https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-supplement/#sec-deploy-kubernetes-without</a>)
      requires the <code class="literal">registry_enabled</code> option in its cluster
      template set to <code class="literal">true</code>. To make this offline scenario
      work, you also need to set the <span class="guimenu ">Delegate trust to cluster users if
      required</span> option to <code class="literal">true</code>. This restores the old,
      insecure behavior for clusters with the
      <code class="literal">registry-enabled</code> or <code class="literal">volume_driver=Rexray</code> options enabled.
     </p></dd><dt id="id-1.3.5.4.24.6.2"><span class="term "><span class="guimenu ">Trustee Domain</span>: <span class="guimenu ">Domain Name</span></span></dt><dd><p>
      Domain name to use for creating trustee for bays.
     </p></dd><dt id="id-1.3.5.4.24.6.3"><span class="term "><span class="guimenu ">Logging</span>: <span class="guimenu ">Verbose</span></span></dt><dd><p>
      Increases the amount of information that is written to the log files when
      set to <span class="guimenu ">true</span>.
     </p></dd><dt id="id-1.3.5.4.24.6.4"><span class="term "><span class="guimenu ">Logging</span>: <span class="guimenu ">Debug</span></span></dt><dd><p>
      Shows debugging output in the log files when set to <span class="guimenu ">true</span>.
     </p></dd><dt id="id-1.3.5.4.24.6.5"><span class="term "><span class="guimenu ">Certificate Manager</span>: <span class="guimenu ">Plug-in</span>
    </span></dt><dd><p>
      To store certificates, either use the <span class="guimenu ">Barbican</span>
      <span class="productname">OpenStack</span> service, a local directory (<span class="guimenu ">Local</span>), or the
      <span class="guimenu ">Magnum Database (x590keypair)</span>.
     </p><div id="id-1.3.5.4.24.6.5.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Barbican As Certificate Manager</h6><p>
       If you choose to use Barbican for managing certificates, make sure
       that the Barbican barclamp is enabled.
      </p></div></dd></dl></div><div class="figure" id="id-1.3.5.4.24.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_magnum_attributes.png" target="_blank"><img src="images/depl_barclamp_magnum_attributes.png" width="" alt="The Magnum Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.28: </span><span class="name">The Magnum Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.24.7">#</a></h6></div></div><p>
   The Magnum barclamp consists of the following roles:
   <span class="guimenu ">magnum-server</span>. It can either be deployed on a Control Node
   or on a cluster—see <a class="xref" href="#sec-depl-ostack-magnum-ha" title="12.17.1. HA Setup for Magnum">Section 12.17.1, “HA Setup for Magnum”</a>. When
   deploying the role onto a Control Node, additional RAM is required for the
   Magnum server. It is recommended to only deploy the role to a
   Control Node that has 16 GB RAM.
  </p><div class="sect2 " id="sec-depl-ostack-magnum-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Magnum</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-magnum-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-magnum-ha</li></ul></div></div></div></div><p>
    Making Magnum highly available requires no special configuration. It
    is sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-barbican"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Barbican (Optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barbican">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barbican</li></ul></div></div></div></div><p>
   Barbican is a component designed for storing secrets in a secure and
   standardized manner protected by Keystone authentication. Secrets include
   SSL certificates and passwords used by various <span class="productname">OpenStack</span> components.
  </p><p>
   Barbican settings can be configured in <code class="literal">Raw</code> mode
   only. To do this, open the Barbican barclamp <span class="guimenu ">Attribute
   </span>configuration in <span class="guimenu ">Raw</span> mode.
  </p><div class="figure" id="id-1.3.5.4.25.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_barbican_raw.png" target="_blank"><img src="images/depl_barclamp_barbican_raw.png" width="" alt="The Barbican Barclamp: Raw Mode" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.29: </span><span class="name">The Barbican Barclamp: Raw Mode </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.25.4">#</a></h6></div></div><p>
   When configuring Barbican, pay particular attention to the following
   settings:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="literal">bind_host</code> Bind host for the Barbican API service
    </p></li><li class="listitem "><p>
     <code class="literal">bind_port</code> Bind port for the Barbican API service
    </p></li><li class="listitem "><p>
     <code class="literal">processes</code> Number of API processes to run in Apache
    </p></li><li class="listitem "><p>
     <code class="literal">ssl</code> Enable or disable SSL
    </p></li><li class="listitem "><p>
     <code class="literal">threads</code> Number of API worker threads
    </p></li><li class="listitem "><p>
     <code class="literal">debug</code> Enable or disable debug logging
    </p></li><li class="listitem "><p>
     <code class="literal">enable_keystone_listener</code> Enable or disable the
     Keystone listener services
    </p></li><li class="listitem "><p>
     <code class="literal">kek</code> An encryption key (fixed-length 32-byte
     Base64-encoded value) for Barbican's
     <code class="systemitem">simple_crypto</code> plug-in. If left unspecified, the
     key will be generated automatically.
    </p><div id="id-1.3.5.4.25.6.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Existing Encryption Key</h6><p>
      If you plan to restore and use the existing Barbican database after
      a full reinstall (including a complete wipe of the Crowbar node), make
      sure to save the specified encryption key beforehand. You will need to
      provide it after the full reinstall in order to access the data in the
      restored Barbican database.
     </p></div></li></ul></div><div class="variablelist "><dl class="variablelist"><dt id="sec-depl-ostack-barbican-ssl"><span class="term ">SSL Support: Protocol
    </span></dt><dd><p>
      With the default value <span class="guimenu ">HTTP</span>, public communication will
      not be encrypted. Choose <span class="guimenu ">HTTPS</span> to use SSL for
      encryption. See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for background
      information and <a class="xref" href="#sec-depl-inst-nodes-post-ssl" title="11.4.6. Enabling SSL">Section 11.4.6, “Enabling SSL”</a> for
      installation instructions. The following additional configuration options
      will become available when choosing <span class="guimenu ">HTTPS</span>:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.25.7.1.2.2.1"><span class="term "><span class="guimenu ">Generate (self-signed) certificates</span>
       </span></dt><dd><p>
         When set to <code class="literal">true</code>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </p></dd><dt id="id-1.3.5.4.25.7.1.2.2.2"><span class="term "><span class="guimenu ">SSL Certificate File</span> / <span class="guimenu ">SSL (Private) Key
        File</span>
       </span></dt><dd><p>
         Location of the certificate key pair files.
        </p></dd><dt id="id-1.3.5.4.25.7.1.2.2.3"><span class="term "><span class="guimenu ">SSL Certificate is insecure</span>
       </span></dt><dd><p>
         Set this option to <code class="literal">true</code> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </p></dd><dt id="id-1.3.5.4.25.7.1.2.2.4"><span class="term "><span class="guimenu ">SSL CA Certificates File</span>
       </span></dt><dd><p>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the barclamp to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <code class="systemitem">apache2</code> service, and
         re-deploy the barclamp.
        </p><p>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </p><div class="figure" id="id-1.3.5.4.25.7.1.2.2.4.2.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barbican_ssl.png" target="_blank"><img src="images/depl_barbican_ssl.png" width="" alt="The SSL Dialog" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.30: </span><span class="name">The SSL Dialog </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.25.7.1.2.2.4.2.3">#</a></h6></div></div></dd></dl></div></dd></dl></div><div class="sect2 " id="sec-depl-ostack-barbican-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.18.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Barbican</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barbican-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barbican-ha</li></ul></div></div></div></div><p>
    To make Barbican highly available, assign the
    <span class="guimenu ">barbican-controller</span> role to the Controller Cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-sahara"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Sahara</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-sahara">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-sahara</li></ul></div></div></div></div><p>
   Sahara provides users with simple means to provision data processing
   frameworks (such as Hadoop, Spark, and Storm) on OpenStack. This is
   accomplished by specifying configuration parameters such as the framework
   version, cluster topology, node hardware details, etc.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.26.3.1"><span class="term ">Logging: Verbose</span></dt><dd><p>
      Set to <code class="systemitem">true</code> to increase the amount of
      information written to the log files.
     </p></dd></dl></div><div class="figure" id="id-1.3.5.4.26.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_sahara.png" target="_blank"><img src="images/depl_barclamp_sahara.png" width="" alt="The Sahara Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.31: </span><span class="name">The Sahara Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.26.4">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-sahara-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.19.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Sahara</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-sahara-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-sahara-ha</li></ul></div></div></div></div><p>
    Making Sahara highly available requires no special configuration. It is
    sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-ironic"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Ironic (optional)</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ironic">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ironic</li></ul></div></div></div></div><p>
  Ironic is the OpenStack bare metal service for provisioning physical
  machines. Refer to the OpenStack <a class="link" href="https://docs.openstack.org/ironic/latest/" target="_blank">developer and admin
  manual</a> for information on drivers, and administering Ironic.
 </p><p>
  Deploying the Ironic barclamp is done in five steps:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
Set options in the Custom view of the barclamp.
</p></li><li class="listitem "><p>
List the <code class="literal">enabled_drivers</code> in the Raw view.
</p></li><li class="listitem "><p>
Configure the Ironic network in <code class="filename">network.json</code>.
</p></li><li class="listitem "><p>
Apply the barclamp to a Control Node.
</p></li><li class="listitem "><p>
          Apply the <span class="guimenu ">nova-compute-ironic</span> role to the same node
          you applied the Ironic barclamp to, in place of the other
          <span class="guimenu ">nova-compute-*</span> roles.
</p></li></ul></div><div class="sect2 " id="sec-depl-ostack-ironic-custom-view"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.20.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Custom View Options</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ironic-custom-view">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ironic-custom-view</li></ul></div></div></div></div><p>
        Currently, there are two options in the Custom view of the barclamp.
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.27.5.3.1"><span class="term "><span class="guimenu ">Enable automated node cleaning</span>
    </span></dt><dd><p>
         Node cleaning prepares the node to accept a new workload. When you set this to <span class="guimenu ">true</span>,
         Ironic collects a list of cleaning steps from the Power, Deploy, Management, and RAID
         interfaces of the driver assigned to the node. Ironic automatically prioritizes and
         executes the cleaning steps, and changes the state of the node to "cleaning". When cleaning
         is complete the state becomes "available". After a new workload is assigned to the machine
         its state changes to "active".
     </p><p>
        <span class="guimenu ">false</span> disables automatic cleaning, and you must configure and apply
        node cleaning manually. This requires the admin to create and prioritize the cleaning steps,
        and to set up a cleaning network. Apply manual cleaning when you have long-running or
        destructive tasks that you wish to monitor and control more closely.
        (See <a class="link" href="https://docs.openstack.org/ironic/latest/admin/cleaning.html" target="_blank">Node Cleaning</a>.)
        </p></dd><dt id="id-1.3.5.4.27.5.3.2"><span class="term "><span class="guimenu ">SSL Support: Protocol</span>
    </span></dt><dd><p>
         SSL support is not yet enabled, so the only option is <span class="guimenu ">HTTP</span>.
        </p></dd></dl></div><div class="figure" id="id-1.3.5.4.27.5.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/ironic-1.png" target="_blank"><img src="images/ironic-1.png" width="" alt="The Ironic barclamp Custom view" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.32: </span><span class="name">The Ironic barclamp Custom view </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.27.5.4">#</a></h6></div></div></div><div class="sect2 " id="sec-depl-ostack-ironic-drivers"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.20.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Drivers</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ironic-drivers">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ironic-drivers</li></ul></div></div></div></div><p>
    You must enter the Raw view of barclamp and specify a list of drivers to load during service initialization.
    <code class="literal">pxe_ipmitool</code> is the recommended default Ironic driver. It uses the
    Intelligent Platform Management Interface (IPMI) to control the power state
    of your bare metal machines, creates the appropriate PXE configurations
    to start them, and then performs the steps to provision and configure the machines.</p><div class="verbatim-wrap"><pre class="screen">"enabled_drivers": ["pxe_ipmitool"],</pre></div><p>
     See <a class="link" href="https://docs.openstack.org/ironic/latest/admin/drivers.html" target="_blank">Ironic
     Drivers</a> for more information.
    </p></div><div class="sect2 " id="id-1.3.5.4.27.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.20.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Ironic Network Configuration</span> <a title="Permalink" class="permalink" href="#id-1.3.5.4.27.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
          This is a complete Ironic <code class="filename">network.json</code> example, using
          the default <code class="filename">network.json</code>, followed by a diff that shows
          the Ironic-specific configurations.</p><div class="example" id="ex-ironic-network-json"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 12.1: </span><span class="name">Example network.json </span><a title="Permalink" class="permalink" href="#ex-ironic-network-json">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
  "start_up_delay": 30,
  "enable_rx_offloading": true,
  "enable_tx_offloading": true,
  "mode": "single",
  "teaming": {
    "mode": 1
  },
  "interface_map": [
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R610"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01.1/0000:01:00.0",
        "0000:00/0000:00:01.1/0000.01:00.1",
        "0000:00/0000:00:01.0/0000:02:00.0",
        "0000:00/0000:00:01.0/0000:02:00.1"
      ],
      "pattern": "PowerEdge R620"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R710"
    },
    {
      "bus_order": [
        "0000:00/0000:00:04",
        "0000:00/0000:00:02"
      ],
      "pattern": "PowerEdge C6145"
    },
    {
      "bus_order": [
        "0000:00/0000:00:03.0/0000:01:00.0",
        "0000:00/0000:00:03.0/0000:01:00.1",
        "0000:00/0000:00:1c.4/0000:06:00.0",
        "0000:00/0000:00:1c.4/0000:06:00.1"
      ],
      "pattern": "PowerEdge R730xd"
    },
    {
      "bus_order": [
        "0000:00/0000:00:1c",
        "0000:00/0000:00:07",
        "0000:00/0000:00:09",
        "0000:00/0000:00:01"
      ],
      "pattern": "PowerEdge C2100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03",
        "0000:00/0000:00:07"
      ],
      "pattern": "C6100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:02"
      ],
      "pattern": "product"
    }
  ],
  "conduit_map": [
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        }
      },
      "pattern": "team/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "dual/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "single/.*/.*ironic.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "single/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1"
          ]
        }
      },
      "pattern": ".*/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "mode/1g_adpt_count/role"
    }
  ],
  "networks": {
    "ironic": {
      "conduit": "intf3",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-ironic",
      "subnet": "192.168.128.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.128.255",
      "router": "192.168.128.1",
      "router_pref": 50,
      "ranges": {
        "admin": {
          "start": "192.168.128.10",
          "end": "192.168.128.11"
        },
        "dhcp": {
          "start": "192.168.128.21",
          "end": "192.168.128.254"
        }
      },
      "mtu": 1500
    },
    "storage": {
      "conduit": "intf1",
      "vlan": 200,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.125.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.125.255",
      "ranges": {
        "host": {
          "start": "192.168.125.10",
          "end": "192.168.125.239"
        }
      }
    },
    "public": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.122.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.122.255",
      "router": "192.168.122.1",
      "router_pref": 5,
      "ranges": {
        "host": {
          "start": "192.168.122.2",
          "end": "192.168.122.127"
        }
      },
      "mtu": 1500
    },
    "nova_fixed": {
      "conduit": "intf1",
      "vlan": 500,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-fixed",
      "subnet": "192.168.123.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.123.255",
      "router": "192.168.123.1",
      "router_pref": 20,
      "ranges": {
        "dhcp": {
          "start": "192.168.123.1",
          "end": "192.168.123.254"
        }
      },
      "mtu": 1500
    },
    "nova_floating": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-public",
      "subnet": "192.168.122.128",
      "netmask": "255.255.255.128",
      "broadcast": "192.168.122.255",
      "ranges": {
        "host": {
          "start": "192.168.122.129",
          "end": "192.168.122.254"
        }
      },
      "mtu": 1500
    },
    "bmc": {
      "conduit": "bmc",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.162",
          "end": "192.168.124.240"
        }
      },
      "router": "192.168.124.1"
    },
    "bmc_vlan": {
      "conduit": "intf2",
      "vlan": 100,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.161",
          "end": "192.168.124.161"
        }
      }
    },
    "os_sdn": {
      "conduit": "intf1",
      "vlan": 400,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.130.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.130.255",
      "ranges": {
        "host": {
          "start": "192.168.130.10",
          "end": "192.168.130.254"
        }
      }
    },
    "admin": {
      "conduit": "intf0",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "router": "192.168.124.1",
      "router_pref": 10,
      "ranges": {
        "admin": {
          "start": "192.168.124.10",
          "end": "192.168.124.11"
        },
        "dhcp": {
          "start": "192.168.124.21",
          "end": "192.168.124.80"
        },
        "host": {
          "start": "192.168.124.81",
          "end": "192.168.124.160"
        },
        "switch": {
          "start": "192.168.124.241",
          "end": "192.168.124.250"
        }
      }
    }
  }
}</pre></div></div></div><div class="complex-example"><div class="example" id="ex-ironic-network-json-diff"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 12.2: </span><span class="name">Diff of Ironic Configuration </span><a title="Permalink" class="permalink" href="#ex-ironic-network-json-diff">#</a></h6></div><div class="example-contents"><p>
      This diff should help you separate the Ironic items from the default
      <code class="filename">network.json</code>.
</p><div class="verbatim-wrap"><pre class="screen">--- network.json        2017-06-07 09:22:38.614557114 +0200
+++ ironic_network.json 2017-06-05 12:01:15.927028019 +0200
@@ -91,6 +91,12 @@
             "1g1",
             "1g2"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1",
+            "1g2"
+          ]
         }
       },
       "pattern": "team/.*/.*"
@@ -111,6 +117,11 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
         }
       },
       "pattern": "dual/.*/.*"
@@ -131,6 +142,36 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
+        }
+      },
+      "pattern": "single/.*/.*ironic.*"
+    },
+    {
+      "conduit_list": {
+        "intf0": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf1": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf2": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "single/.*/.*"
@@ -151,6 +192,11 @@
           "if_list": [
             "1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1"
+          ]
         }
       },
       "pattern": ".*/.*/.*"
@@ -171,12 +217,41 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "mode/1g_adpt_count/role"
     }
   ],
   "networks": {
+    "ironic": {
+      "conduit": "intf3",
+      "vlan": 100,
+      "use_vlan": false,
+      "add_bridge": false,
+      "add_ovs_bridge": false,
+      "bridge_name": "br-ironic",
+      "subnet": "192.168.128.0",
+      "netmask": "255.255.255.0",
+      "broadcast": "192.168.128.255",
+      "router": "192.168.128.1",
+      "router_pref": 50,
+      "ranges": {
+        "admin": {
+          "start": "192.168.128.10",
+          "end": "192.168.128.11"
+        },
+        "dhcp": {
+          "start": "192.168.128.21",
+          "end": "192.168.128.254"
+        }
+      },
+      "mtu": 1500
+    },
     "storage": {
       "conduit": "intf1",
       "vlan": 200,</pre></div></div></div></div></div></div><div class="sect1 " id="sec-depl-ostack-final"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to Proceed</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-final">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-final</li></ul></div></div></div></div><p>
   With a successful deployment of the <span class="productname">OpenStack</span> Dashboard, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   installation is finished. To be able to test your setup by starting an
   instance one last step remains to be done—uploading an image to the
   Glance component. Refer to the <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">End User Guide</em></em>, chapter <em class="citetitle ">Manage
   images</em>
   for instructions.
  </p><p>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.—refer to the <em class="citetitle ">Administrator Guide</em> for details. The default
   credentials for the <span class="productname">OpenStack</span> Dashboard are user name <code class="literal">admin</code>
   and password <code class="literal">crowbar</code>.
  </p></div><div class="sect1 " id="crow-ses-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.22 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage integration</span> <a title="Permalink" class="permalink" href="#crow-ses-integration">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>crow-ses-integration</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports integration with SUSE Enterprise Storage (SES), enabling Ceph block
   storage as well as image storage services in SUSE <span class="productname">OpenStack</span> Cloud.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.29.3"><span class="name">Enabling SES Integration
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.29.3">#</a></h3></div><p>
   To enable SES integration on Crowbar, an SES configuration file must be
   uploaded to Crowbar. SES integration functionality is included in the
   <code class="literal">crowbar-core</code> package and can be used with the Crowbar UI
   or CLI (<code class="literal">crowbarctl</code>). The SES configuration file
   describes various aspects of the Ceph environment, and keyrings for each
   user and pool created in the Ceph environment for SUSE <span class="productname">OpenStack</span> Cloud Crowbar services.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.29.5"><span class="name">SES Configuration
  </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.29.5">#</a></h3></div><p>
   For SES deployments that are version 5.5 and higher, a Salt runner is used
   to create all the users and pools. It also generates a YAML
   configuration that is needed to integrate with SUSE <span class="productname">OpenStack</span> Cloud. The integration
   runner creates separate users for Cinder, Cinder backup
   (not used by Crowbar currently) and Glance. Both the Cinder and
   Nova services have the same user, because Cinder needs
   access to create objects that Nova uses.
  </p><p>
   Configure SES with the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Login as <code class="literal">root</code> and run the SES 5.5 Salt runner on the
     Salt admin host.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run --out=yaml openstack.integrate prefix=mycloud</pre></div><p>
     The prefix parameter allows pools to be created with the specified
     prefix. By using different prefix parameters, multiple cloud deployments
     can support different users and pools on the same SES deployment.
    </p></li><li class="step "><p>
     A YAML file is created with content similar to the following
     example:
    </p><div class="verbatim-wrap"><pre class="screen">ceph_conf:
    cluster_network: 10.84.56.0/21
    fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
    mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
    mon_initial_members: ses-osd1, ses-osd2, ses-osd3
    public_network: 10.84.56.0/21
cinder:
    key: ABCDEFGaxefEMxAAW4zp2My/5HjoST2Y87654321==
    rbd_store_pool: mycloud-cinder
    rbd_store_user: cinder
cinder-backup:
    key: AQBb8hdbrY2bNRAAqJC2ZzR5Q4yrionh7V5PkQ==
    rbd_store_pool: mycloud-backups
    rbd_store_user: cinder-backup
glance:
    key: AQD9eYRachg1NxAAiT6Hw/xYDA1vwSWLItLpgA==
    rbd_store_pool: mycloud-glance
    rbd_store_user: glance
nova:
    rbd_store_pool: mycloud-nova
radosgw_urls:
    - http://10.84.56.7:80/swift/v1
    - http://10.84.56.8:80/swift/v1</pre></div></li><li class="step "><p>
     Upload the generated YAML file to Crowbar using the UI or
     <code class="literal">crowbarctl</code> CLI.
    </p></li><li class="step "><p>
     If the Salt runner is not available, you must manually create pools and
     users to allow SUSE <span class="productname">OpenStack</span> Cloud services to use the SES/Ceph cluster. Pools and
     users must be created for Cinder, Nova, and
     Glance. Instructions for creating and managing pools, users and keyrings
     can be found in the <a class="link" href="https://documentation.suse.com/ses/5.5/" target="_blank">
     SUSE Enterprise Storage</a> Administration Guide in the Key Management
     section.
    </p><p>
     After the required pools and users are set up on the SUSE Enterprise Storage/Ceph
     cluster, create an SES configuration file in YAML format (using the
     example template above). Upload this file to Crowbar using the UI or
     <code class="literal">crowbarctl</code> CLI.
    </p></li><li class="step "><p>
     As indicated above, the SES configuration file can be uploaded to Crowbar
     using the UI or <code class="literal">crowbarctl</code> CLI.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       From the main Crowbar UI, the upload page is under
       <span class="guimenu ">Utilities</span> › <span class="guimenu ">SUSE Enterprise
       Storage</span>.
      </p><p>
       If a configuration is already stored in Crowbar, it will be visible in
       the upload page. A newly uploaded configuration will replace existing
       one. The new configuration will be applied to the cloud on the next
       <code class="literal">chef-client</code> run. There is no need to reapply
       proposals.
      </p><p>
       Configurations can also be deleted from Crowbar. After deleting a
       configuration, you must manually update and reapply all proposals that
       used SES integration.
      </p></li><li class="listitem "><p>
       With the <code class="literal">crowbarctl</code> CLI, the command <code class="command">crowbarctl ses
       upload <em class="replaceable ">FILE</em></code> accepts a path to the
       SES configuration file.
      </p></li></ul></div></li></ol></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.29.9"><span class="name">Cloud Service Configuration</span><a title="Permalink" class="permalink" href="#id-1.3.5.4.29.9">#</a></h3></div><p>
   SES integration with SUSE <span class="productname">OpenStack</span> Cloud services is implemented with relevant Barclamps
   and installed with the <code class="literal">crowbar-openstack</code> package.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.29.11.1"><span class="term ">Glance</span></dt><dd><p>
      Set <code class="literal">Use SES Configuration</code> to <code class="literal">true</code>
      under <code class="literal">RADOS Store Parameters</code>. The Glance barclamp
      pulls the uploaded SES configuration from Crowbar when applying the
      Glance proposal and on <code class="literal">chef-client</code> runs. If the SES
      configuration is uploaded before the Glance proposal is created,
      <code class="literal">Use SES Configuration</code> is enabled automatically
      upon proposal creation.
     </p></dd><dt id="id-1.3.5.4.29.11.2"><span class="term ">Cinder</span></dt><dd><p>
      Create a new RADOS backend and set <code class="literal">Use SES
      Configuration</code> to <code class="literal">true</code>. The Cinder
      barclamp pulls the uploaded SES configuration from Crowbar when applying the
      Cinder proposal and on <code class="literal">chef-client</code> runs. If
      the SES configuration was uploaded before the Cinder proposal
      was created, a <code class="literal">ses-ceph</code> RADOS backend is created
      automatically on proposal creation with <code class="literal">Use SES
      Configuration</code> already enabled.
     </p></dd><dt id="id-1.3.5.4.29.11.3"><span class="term ">Nova</span></dt><dd><p>
      To connect with volumes stores in SES, Nova uses the configuration
      from the Cinder barclamp.
      For ephemeral storage, Nova re-uses the <code class="literal">rbd_store_user</code>
      and <code class="literal">key</code> from Cinder but has a separate <code class="literal">rbd_store_pool</code>
      defined in the SES configuration. Ephemeral storage on SES can be
      enabled or disabled by setting <code class="option">Use Ceph RBD Ephemeral Backend</code>
      in Nova proposal. In new deployments it is enabled by default.
      In existing ones it is disabled for compatibility reasons.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.3.5.4.29.12"><span class="name"><span class="productname">RADOS Gateway</span> Integration</span><a title="Permalink" class="permalink" href="#id-1.3.5.4.29.12">#</a></h3></div><p>
     Besides block storage, the SES cluster can also be used as a Swift
     replacement for object storage. If <code class="literal">radosgw_urls</code> section is present
     in uploaded SES configuration, first of the URLs is registered
     in the Keystone catalog as the "Swift"/"object-store" service. Some
     configuration is needed on SES side to fully integrate with Keystone
     auth.
     If SES integration is enabled on a cloud with Swift deployed,
     SES object storage service will get higher priority by default. To
     override this and use Swift for object storage instead, remove
     <code class="literal">radosgw_urls</code> section from the SES configuration file and re-upload it
     to Crowbar. Re-apply Swift proposal or wait for next periodic
     chef-client run to make changes effective.
   </p></div><div class="sect1 " id="sec-depl-services"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.23 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span> <a title="Permalink" class="permalink" href="#sec-depl-services">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-services</li></ul></div></div></div></div><p>
   The following table lists all roles (as defined in the barclamps), and their
   associated services. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span> this list is work
   in progress. Services can be manually started and stopped with the commands
   <code class="command">systemctl start <em class="replaceable ">SERVICE</em></code> and
   <code class="command">systemctl stop <em class="replaceable ">SERVICE</em></code>.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
       <p>
        Role
       </p>
      </th><th>
       <p>
        Service
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        ceilometer-agent
       </p>
      </td><td><code class="systemitem">
       openstack-ceilometer-agent-compute
      </code>
      </td></tr><tr><td rowspan="6">
       <p>
        ceilometer-polling
       </p>
       <p>
        ceilometer-server
       </p>
       <p>
        ceilometer-swift-proxy-middleware
       </p>
      </td><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-agent-notification
        </code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-alarm-evaluator
        </code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-alarm-notifier
        </code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-api </code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-collector
        </code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-polling </code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        cinder-controller
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-cinder-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-cinder-scheduler</code>
       </p>
      </td></tr><tr><td>
       <p>
        cinder-volume
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-cinder-volume</code>
       </p>
      </td></tr><tr><td>
       <p>
        database-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">mariadb</code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        glance-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-glance-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-glance-registry</code>
       </p>
      </td></tr><tr><td rowspan="4">
       <p>
        heat-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-heat-api-cfn</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-heat-api-cloudwatch</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-heat-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-heat-engine</code>
       </p>
      </td></tr><tr><td>
       <p>
        horizon
       </p>
      </td><td>
       <p>
        <code class="systemitem">apache2</code>
       </p>
      </td></tr><tr><td>
       <p>
        keystone-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-keystone</code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        manila-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-manila-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-manila-scheduler</code>
       </p>
      </td></tr><tr><td>
       <p>
        manila-share
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-manila-share</code>
       </p>
      </td></tr><tr><td>
       <p>
        neutron-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-neutron</code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        nova-compute-*
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-nova-compute</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-neutron-openvswitch-agent
        </code> (when neutron is deployed with openvswitch)
       </p>
      </td></tr><tr><td rowspan="7">
       <p>
        nova-controller
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-nova-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-cert</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-conductor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-consoleauth</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-novncproxy</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-objectstore</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-scheduler</code>
       </p>
      </td></tr><tr><td>
       <p>
        rabbitmq-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">rabbitmq-server</code>
       </p>
      </td></tr><tr><td>
       <p>
        swift-dispersion
       </p>
      </td><td>
       <p>
        none
       </p>
      </td></tr><tr><td>
       <p>
        swift-proxy
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-swift-proxy</code>
       </p>
      </td></tr><tr><td>
       <p>
        swift-ring-compute
       </p>
      </td><td>
       <p>
        none
       </p>
      </td></tr><tr><td rowspan="14">
       <p>
        swift-storage
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-swift-account-auditor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-account-reaper</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-account-replicator</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-account</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-auditor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-replicator</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-sync</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-updater</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-auditor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-expirer</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-replicator</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-updater</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object</code>
       </p>
      </td></tr></tbody></table></div></div><div class="sect1 " id="sec-deploy-crowbatch-description"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.24 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Crowbar Batch Command</span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-description">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-description</li></ul></div></div></div></div><p>
   This is the documentation for the <code class="command">crowbar batch</code>
   subcommand.
  </p><p>
   <code class="command">crowbar batch</code> provides a quick way of creating, updating,
   and applying Crowbar proposals. It can be used to:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Accurately capture the configuration of an existing Crowbar environment.
    </p></li><li class="listitem "><p>
     Drive Crowbar to build a complete new environment from scratch.
    </p></li><li class="listitem "><p>
     Capture one SUSE <span class="productname">OpenStack</span> Cloud environment and then reproduce it on another set of
     hardware (provided hardware and network configuration match to an
     appropriate extent).
    </p></li><li class="listitem "><p>
     Automatically update existing proposals.
    </p></li></ul></div><p>
   As the name suggests, <code class="command">crowbar batch</code> is intended to be run
   in <span class="quote">“<span class="quote ">batch mode</span>”</span> that is mostly unattended. It has two modes of
   operation:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.31.6.1"><span class="term ">crowbar batch export
    </span></dt><dd><p>
      Exports a YAML file which describes existing proposals and how their
      parameters deviate from the default proposal values for that barclamp.
     </p></dd><dt id="id-1.3.5.4.31.6.2"><span class="term ">crowbar batch build
    </span></dt><dd><p>
      Imports a YAML file in the same format as above. Uses it to build new
      proposals if they do not yet exist. Updates the existing proposals so
      that their parameters match those given in the YAML file.
     </p></dd></dl></div><div class="sect2 " id="sec-deploy-crowbatch-yaml"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.24.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">YAML file format</span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-yaml">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-yaml</li></ul></div></div></div></div><p>
    Here is an example YAML file. At the top-level there is a proposals array,
    each entry of which is a hash representing a proposal:
   </p><div class="verbatim-wrap"><pre class="screen">proposals:
- barclamp: provisioner
  # Proposal name defaults to 'default'.
  attributes:
    shell_prompt: USER@ALIAS:CWD SUFFIX
- barclamp: database
  # Default attributes are good enough, so we just need to assign
  # nodes to roles:
  deployment:
    elements:
      database-server:
        - "@@controller1@@"
- barclamp: rabbitmq
  deployment:
    elements:
      rabbitmq-server:
        - "@@controller1@@"</pre></div><div id="id-1.3.5.4.31.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Reserved Indicators in YAML</h6><p>
     Note that the characters <code class="literal">@</code> and <code class="literal">`</code> are
     reserved indicators in YAML. They can appear anywhere in a string
     <span class="emphasis"><em>except at the beginning</em></span>. Therefore a string such as
     <code class="literal">@@controller1@@</code> needs to be quoted using double quotes.
    </p></div></div><div class="sect2 " id="sec-deploy-crowbatch-yaml-attributes"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.24.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Top-level proposal attributes</span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-yaml-attributes">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-yaml-attributes</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.31.8.2.1"><span class="term ">barclamp
     </span></dt><dd><p>
       Name of the barclamp for this proposal (required).
      </p></dd><dt id="id-1.3.5.4.31.8.2.2"><span class="term ">name
     </span></dt><dd><p>
       Name of this proposal (optional; default is <code class="literal">default</code>).
       In <code class="command">build</code> mode, if the proposal does not already
       exist, it will be created.
      </p></dd><dt id="id-1.3.5.4.31.8.2.3"><span class="term ">attributes
     </span></dt><dd><p>
       An optional nested hash containing any attributes for this proposal
       which deviate from the defaults for the barclamp.
      </p><p>
       In <code class="command">export</code> mode, any attributes set to the default
       values are excluded to keep the YAML as short and readable as possible.
      </p><p>
       In <code class="command">build</code> mode, these attributes are deep-merged with
       the current values for the proposal. If the proposal did not already
       exist, batch build will create it first. The attributes are merged with
       the default values for the barclamp's proposal.
      </p></dd><dt id="id-1.3.5.4.31.8.2.4"><span class="term ">wipe_attributes
     </span></dt><dd><p>
       An optional array of paths to nested attributes which should be removed
       from the proposal.
      </p><p>
       Each path is a period-delimited sequence of attributes; for example
       <code class="literal">pacemaker.stonith.sbd.nodes</code> would remove all SBD
       nodes from the proposal if it already exists. If a path segment contains
       a period, it should be escaped with a backslash, for example
       <code class="literal">segment-one.segment\.two.segment_three</code>.
      </p><p>
       This removal occurs before the deep merge described above.

       For example, think of a YAML file which includes a Pacemaker barclamp
       proposal where the <code class="literal">wipe_attributes</code> entry contains
       <code class="literal">pacemaker.stonith.sbd.nodes</code>. A batch build with this
       YAML file ensures that only SBD nodes listed in the <code class="literal">attributes
       sibling</code> hash are used at the end of the run. In contrast,
       without the <code class="literal">wipe_attributes</code> entry, the given SBD
       nodes would be appended to any SBD nodes already defined in the
       proposal.
      </p></dd><dt id="id-1.3.5.4.31.8.2.5"><span class="term ">deployment
     </span></dt><dd><p>
       A nested hash defining how and where this proposal should be deployed.
      </p><p>
       In <code class="command">build</code> mode, this hash is deep-merged in the same
       way as the attributes hash, except that the array of elements for each
       Chef role is reset to the empty list before the deep merge. This
       behavior may change in the future.
      </p></dd></dl></div></div><div class="sect2 " id="sec-deploy-crowbatch-yaml-subst"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.24.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Alias Substitutions</span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-yaml-subst">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-yaml-subst</li></ul></div></div></div></div><p>
    A string like <code class="literal">@@<em class="replaceable ">node</em>@@</code> (where
    <em class="replaceable ">node</em> is a node alias) will be substituted for
    the name of that node, no matter where the string appears in the YAML file.
    For example, if <code class="literal">controller1</code> is a Crowbar alias for node
    <code class="literal">d52-54-02-77-77-02.mycloud.com</code>, then
    <code class="literal">@@controller1@@</code> will be substituted for that host name.
    This allows YAML files to be reused across environments.
   </p></div><div class="sect2 " id="sec-deploy-crowbatch-options"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.24.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Options</span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-options">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-options</li></ul></div></div></div></div><p>
    In addition to the standard options available to every
    <code class="command">crowbar</code> subcommand (run <code class="command">crowbar batch
    --help</code> for a full list), there are some extra options
    specifically for <code class="command">crowbar batch</code>:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.4.31.10.3.1"><span class="term ">--include &lt;barclamp[.proposal]&gt;
    </span></dt><dd><p>
       Only include the barclamp / proposals given.
      </p><p>
       This option can be repeated multiple times. The inclusion value can
       either be the name of a barclamp (for example,
       <code class="literal">pacemaker</code>) or a specifically named proposal within
       the barclamp (for example, <code class="literal">pacemaker.network_cluster</code>).
      </p><p>
       If it is specified, then only the barclamp / proposals specified are
       included in the build or export operation, and all others are ignored.
      </p></dd><dt id="id-1.3.5.4.31.10.3.2"><span class="term ">--exclude &lt;barclamp[.proposal]&gt;
    </span></dt><dd><p>
       This option can be repeated multiple times. The exclusion value is the
       same format as for <code class="option">--include</code>. The barclamps / proposals
       specified are excluded from the build or export operation.
      </p></dd><dt id="id-1.3.5.4.31.10.3.3"><span class="term ">--timeout &lt;seconds&gt;
    </span></dt><dd><p>
       Change the timeout for Crowbar API calls.
      </p><p>
       As Chef's run lists grow, some of the later <span class="productname">OpenStack</span> barclamp proposals
       (for example Nova, Horizon, or Heat) can take over 5 or even 10
       minutes to apply. Therefore you may need to increase this timeout to 900
       seconds in some circumstances.
      </p></dd></dl></div></div></div></div><div class="chapter " id="sec-deploy-policy-json"><div class="titlepage"><div><div><h2 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limiting Users' Access Rights</span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_policy_json.xml</li><li><span class="ds-label">ID: </span>sec-deploy-policy-json</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-deploy-policy-json-edit"><span class="number">13.1 </span><span class="name">Editing <code class="filename">policy.json</code></span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-keystone-policy-json-edit"><span class="number">13.2 </span><span class="name">Editing <code class="filename">keystone_policy.json</code></span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-policy-json-keystone"><span class="number">13.3 </span><span class="name">Adjusting the <span class="guimenu ">Keystone</span> Barclamp
   Proposal</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-policy-json-horizon"><span class="number">13.4 </span><span class="name">Adjusting the <span class="guimenu ">Horizon</span> Barclamp
   Proposal</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-policy-json-admin"><span class="number">13.5 </span><span class="name">Pre-Installed Service Admin Role Components</span></a></span></dt></dl></div></div><p>To limit users' access rights (or to define more fine-grained access
  rights), you can use Role Based Access Control (RBAC, only available with
  Keystone v3). In the example below, we will create a
  new role (<code class="literal">ProjectAdmin</code>). It allows users with this role to
  add and remove other users to the <code class="literal">Member</code> role on the same
  project.
  </p><p>To create a new role that can be assigned to a user-project pair, the
  following basic steps are needed:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Create a custom <code class="filename">policy.json</code> file for the
    Keystone component. On the node where the <code class="filename">keystone-server</code> role is
    deployed, copy the file to
    <code class="filename">/etc/keystone/<em class="replaceable ">CUSTOM_</em>policy.json</code>.
    For details, see <a class="xref" href="#sec-deploy-policy-json-edit" title="13.1. Editing policy.json">Section 13.1, “Editing <code class="filename">policy.json</code>”</a>.
   </p></li><li class="step "><p>Create a custom <code class="filename">keystone_policy.json</code> file for the
    Horizon component. On the node where the
     <code class="literal">nova_dashboard-server</code> role is deployed, copy the custom
    <code class="filename">keystone_policy.json</code> file to
    <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/</code>
    (default directory for policy files in Horizon). For details, see
     <a class="xref" href="#sec-deploy-keystone-policy-json-edit" title="13.2. Editing keystone_policy.json">Section 13.2, “Editing <code class="filename">keystone_policy.json</code>”</a>.</p></li><li class="step "><p>Make the Keystone component aware of the
      <code class="filename"><em class="replaceable ">CUSTOM_</em>policy.json</code> file by
    editing and reapplying the <span class="guimenu ">Keystone</span> barclamp. For details,
    see <a class="xref" href="#sec-deploy-policy-json-keystone" title="13.3. Adjusting the Keystone Barclamp Proposal">Section 13.3, “Adjusting the <span class="guimenu ">Keystone</span> Barclamp
   Proposal”</a>.</p></li><li class="step "><p>Make the Horizon component aware of the
     <code class="filename">keystone_policy.json</code> file by editing and reapplying
    the <span class="guimenu ">Horizon</span> barclamp. For details, see
    <a class="xref" href="#sec-deploy-policy-json-horizon" title="13.4. Adjusting the Horizon Barclamp Proposal">Section 13.4, “Adjusting the <span class="guimenu ">Horizon</span> Barclamp
   Proposal”</a>.</p></li></ol></div></div><div class="sect1 " id="sec-deploy-policy-json-edit"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Editing <code class="filename">policy.json</code></span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-edit">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_policy_json.xml</li><li><span class="ds-label">ID: </span>sec-deploy-policy-json-edit</li></ul></div></div></div></div><p> The <code class="filename">policy.json</code> file is located in
    <code class="filename">/etc/keystone/</code> on the node where the
    <code class="filename">keystone-server</code> role is deployed. </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Copy <code class="filename">/etc/keystone/policy.json</code> and save it under
     a different name, for example
      <code class="filename"><em class="replaceable ">CUSTOM_</em>policy.json</code>. </p><div id="id-1.3.5.5.6.3.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Use Different File Name</h6><p>If you use the same name as the original file, your custom file will
      be overwritten by the next package update.</p></div></li><li class="step "><p>To add the new role, enter the following two lines at the beginning of
     the file:</p><div class="verbatim-wrap"><pre class="screen">{
  "subadmin": "role:ProjectAdmin",
  "projectadmin": "rule:subadmin and project_id:%(target.project.id)s",
  [...]</pre></div></li><li class="step "><p>Adjust the other rules in the file accordingly:</p><div class="verbatim-wrap"><pre class="screen">  "identity:get_domain": "rule:admin_required or rule:subadmin",
  [...]
  "identity:get_project": "rule:admin_required or rule:projectadmin",
  [...]
  "identity:list_user_projects": "rule:admin_or_owner or rule:projectadmin",
  [...]
  "identity:update_project": "rule:admin_required or rule:projectadmin",
  [...]
  "identity:get_user": "rule:admin_required or rule:projectadmin",
  "identity:list_users": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_groups": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_roles": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_grants": "rule:admin_required or (rule:subadmin and project_id:%(target.project.id)s)",
  "identity:create_grant": "rule:admin_required or (rule:subadmin and project_id:%(target.project.id)s and 'Member':%(target.role.name)s)",
  "identity:revoke_grant": "rule:admin_required or (rule:subadmin and project_id:%(target.project.id)s and 'Member':%(target.role.name)s)",
  [...]
  "identity:list_role_assignments": "rule:admin_required or rule:subadmin",</pre></div></li><li class="step "><p>
     Save the changes.
    </p></li><li class="step "><p>
     On the node where the <code class="filename">keystone-server</code> role is
     deployed, copy the file to
     <code class="filename">/etc/keystone/<em class="replaceable ">CUSTOM_</em>policy.json</code>.
     Usually, the <code class="filename">keystone-server</code> role is deployed to a Control Node
     (or to a cluster, if you use a High Availability setup).
    </p></li></ol></div></div></div><div class="sect1 " id="sec-deploy-keystone-policy-json-edit"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Editing <code class="filename">keystone_policy.json</code></span> <a title="Permalink" class="permalink" href="#sec-deploy-keystone-policy-json-edit">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_policy_json.xml</li><li><span class="ds-label">ID: </span>sec-deploy-keystone-policy-json-edit</li></ul></div></div></div></div><p>By default, the <code class="filename">keystone_policy.json</code> file is
   located in
    <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/</code>
   on the node where the <code class="literal">nova_dashboard-server</code> role is
   deployed. It is similar (but not identical) to
    <code class="filename">policy.json</code> and defines which actions the user with a
   certain role is allowed to execute in Horizon. If the user is not
   allowed to execute a certain action, the <span class="productname">OpenStack</span> Dashboard will show an
   error message.</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Copy
      <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/keystone_policy.json</code>
     and save it under a different name, for example
       <code class="filename"><em class="replaceable ">CUSTOM_</em>keystone_policy.json</code>. </p><div id="id-1.3.5.5.7.3.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Use Different File Name</h6><p>If you use the same name as the original file, your custom file will
      be overwritten by the next package update.</p></div></li><li class="step "><p>To add the new role, enter the following two lines at the beginning of
     the file:</p><div class="verbatim-wrap"><pre class="screen">{
  "subadmin": "role:ProjectAdmin",
  "projectadmin": "rule:subadmin and project_id:%(target.project.id)s",
  [...]</pre></div></li><li class="step "><p>Adjust the other rules in the file accordingly:</p><div class="verbatim-wrap"><pre class="screen">  "identity:get_project": "rule:admin_required or rule:projectadmin",
  [...]
  "identity:list_user_projects": "rule:admin_or_owner or rule:projectadmin",
  [...]
  "identity:get_user": "rule:admin_required or rule:projectadmin",
  "identity:list_users": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_roles": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_role_assignments": "rule:admin_required or rule:subadmin",</pre></div></li><li class="step "><p>Save the changes and copy the file to
       <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/<em class="replaceable ">CUSTOM_</em>keystone_policy.json</code>
     on the node where the <code class="literal">nova_dashboard-server</code> role is
     deployed.</p></li></ol></div></div></div><div class="sect1 " id="sec-deploy-policy-json-keystone"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adjusting the <span class="guimenu ">Keystone</span> Barclamp
   Proposal</span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-keystone">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_policy_json.xml</li><li><span class="ds-label">ID: </span>sec-deploy-policy-json-keystone</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Log in to the Crowbar Web interface.</p></li><li class="step "><p>Select <span class="guimenu ">Barclamps</span> › <span class="guimenu ">All barclamps</span>.</p></li><li class="step "><p>Go to the <span class="guimenu ">Keystone</span> barclamp and click
      <span class="guimenu ">Edit</span>.</p></li><li class="step "><p>In the <span class="guimenu ">Attributes</span> section, click
     <span class="guimenu ">Raw</span>. This shows the complete configuration file
     and allows you to edit it directly.</p></li><li class="step "><p>Adjust the <code class="literal">policy_file</code> parameter to point to the
       <code class="filename"><em class="replaceable ">CUSTOM_</em>policy.json</code> file.
     For example:</p><div class="verbatim-wrap"><pre class="screen">{
  [...]
  "policy_file": "mypolicy.json",</pre></div></li><li class="step "><p>
     <span class="guimenu ">Save</span> and <span class="guimenu ">Apply</span> the changes to the
     Keystone barclamp.</p></li></ol></div></div></div><div class="sect1 " id="sec-deploy-policy-json-horizon"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adjusting the <span class="guimenu ">Horizon</span> Barclamp
   Proposal</span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-horizon">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_policy_json.xml</li><li><span class="ds-label">ID: </span>sec-deploy-policy-json-horizon</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Log in to the Crowbar Web interface.</p></li><li class="step "><p>Select <span class="guimenu ">Barclamps</span> › <span class="guimenu ">All barclamps</span>.</p></li><li class="step "><p>Go to the <span class="guimenu ">Horizon</span> barclamp and click
      <span class="guimenu ">Edit</span>.</p></li><li class="step "><p>In the <span class="guimenu ">Attributes</span> section, click
      <span class="guimenu ">Raw</span>. This shows the complete configuration file
     and allows you to edit it directly.</p></li><li class="step "><p>If needed, adjust the <code class="literal">policy_file_path</code> parameter to
     point to the directory where you copied the newly added
       <code class="filename"><em class="replaceable ">CUSTOM_</em>keystone_policy.json</code>
     file. By default, its value is an empty string—this means that
     the default directory will be used.
    </p></li><li class="step "><p>Enter the new file's name as value of the <code class="literal">identity</code>
     parameter within the <code class="literal">policy_file</code> section (<a class="xref" href="#co-horizon-barcl-policy"><span class="callout">1</span></a>):</p><div class="verbatim-wrap"><pre class="screen">{
  "policy_file_path": "",
  "policy_file": {
    "identity": "mykeystone_policy.json", <span id="co-horizon-barcl-policy"></span><span class="callout">1</span>
    "compute": "nova_policy.json",
    "volume": "cinder_policy.json",
    "image": "glance_policy.json",
    "orchestration": "heat_policy.json",
    "network": "neutron_policy.json",
    "telemetry": "ceilometer_policy.json"</pre></div></li><li class="step "><p>
     <span class="guimenu ">Save</span> and <span class="guimenu ">Apply</span> the changes to the
     Horizon barclamp.</p></li></ol></div></div></div><div class="sect1 " id="sec-deploy-policy-json-admin"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Installed Service Admin Role Components</span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-admin">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_policy_json.xml</li><li><span class="ds-label">ID: </span>sec-deploy-policy-json-admin</li></ul></div></div></div></div><p>
   The following are the roles defined in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. These roles serve as a
   way to group common administrative needs at the <span class="productname">OpenStack</span> service
   level. Each role represents administrative privilege into each
   service. Multiple roles can be assigned to a user. You can assign a Service
   Admin Role to a user once you have determined that the user is authorized to
   perform administrative actions and access resources in that service.
  </p><p>
   The main components of Service Administrator Roles are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="literal">nova_admin</code> role in the identity service (Keystone) and
     support in <code class="filename">nova policy.json</code>. Assign this role to
     users whose job function it is to perform Nova compute-related
     administrative tasks.
    </p></li><li class="listitem "><p>
     <code class="literal">neutron_admin</code> role in the identity service and support
     in <code class="filename">neutron policy.json</code>. Assign this role to users
     whose job function it is to perform neutron networking-related
     administrative tasks.
    </p></li><li class="listitem "><p>
     <code class="literal">cinder_admin</code> role in the identity service and support
     in <code class="filename">cinder policy.json</code>. Assign this role to users
     whose job function it is to perform Cinder storage-related administrative
     tasks.
    </p></li><li class="listitem "><p>
     <code class="literal">glance_admin</code> role in the identity service and support
     in <code class="filename">glance policy.json</code>. Assign this role to users
     whose job function it is to perform Cinder storage-related administrative
     tasks.
    </p><div id="id-1.3.5.5.10.4.4.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning: 
      Changing <code class="filename">glance_policy.json</code> may Introduce a
      Security Issue
     </h6><p>
      The OpenStack Security Note OSSN-0075 <a class="link" href="https://wiki.openstack.org/wiki/OSSN/OSSN-0075" target="_blank">https://wiki.openstack.org/wiki/OSSN/OSSN-0075</a> describes a
      scenario where a malicious tenant is able to reuse deleted Glance image
      IDs to share malicious images with other tenants in a manner that is
      undetectable to the victim tenant.
     </p><p>
      The default policy <code class="filename">glance_policy.json</code> that is
      shipped with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> prevents this by ensuring only admins can
      deactivate/reactivate images:
     </p><div class="verbatim-wrap"><pre class="screen">"deactivate": "role:admin"
"reactivate": "role:admin"</pre></div><p>
      SUSE suggests these settings should <span class="emphasis"><em>not</em></span> be
      changed. If you do change them please refer to the OSSN-0075 <a class="link" href="https://wiki.openstack.org/wiki/OSSN/OSSN-0075" target="_blank">https://wiki.openstack.org/wiki/OSSN/OSSN-0075</a> for details
      on the exact scope of the security issue.
      </p></div></li></ul></div></div></div><div class="chapter " id="cha-depl-ostack-configs"><div class="titlepage"><div><div><h2 class="title"><span class="number">14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Files for <span class="productname">OpenStack</span> Services</span> <a title="Permalink" class="permalink" href="#cha-depl-ostack-configs">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ostack_configs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#id-1.3.5.6.1.2">#</a></h6></div><p>
    Typically, each <span class="productname">OpenStack</span> component comes with a configuration file, for
    example: <code class="filename">/etc/nova/nova.conf</code>.
   </p><p>
    These configuration files can still be used. However, to configure an
    <span class="productname">OpenStack</span> component and its different components and roles, it is now
    preferred to add custom configuration file snippets to a
    <code class="filename"><em class="replaceable ">SERVICE</em>.conf.d/</code> directory
    instead.

   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#id-1.3.5.6.2"><span class="number">14.1 </span><span class="name">Default Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#id-1.3.5.6.3"><span class="number">14.2 </span><span class="name">Custom Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-configs-custom-naming"><span class="number">14.3 </span><span class="name">Naming Conventions for Custom Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-configs-custom-order"><span class="number">14.4 </span><span class="name">Processing Order of Configuration Files</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-ostack-configs-custom-more"><span class="number">14.5 </span><span class="name">For More Information</span></a></span></dt></dl></div></div><div class="sect1 " id="id-1.3.5.6.2"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Default Configuration Files</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.2">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>sec.depl.ostack.configs.default</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   By default, a configuration snippet with a basic configuration for each
   <span class="productname">OpenStack</span> component is available in the following directory:
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">SERVICE</em>.conf.d/010-<em class="replaceable ">SERVICE</em>.conf</pre></div><p>
   For example: <code class="filename">/etc/nova/nova.conf.d/010-nova.conf</code>
  </p><p>
   Those files should not be modified.
  </p></div><div class="sect1 " id="id-1.3.5.6.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Custom Configuration Files</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.3">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>sec.depl.ostack.configs.custom</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To adjust or overwrite settings for the respective <span class="productname">OpenStack</span> component, add a
   custom configuration file to the same directory,
   <code class="filename">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">SERVICE</em>.conf.d/</code>.
  </p><p>
   The same applies if you want to configure individual components or roles of
   an <span class="productname">OpenStack</span> component, like <code class="literal">nova-api</code> or
   <code class="literal">nova-compute</code>, for example. But in this case, add your
   custom configuration file to the following directory:
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">ROLE</em>.conf.d/</pre></div><p>
   For example: <code class="filename">/etc/nova/nova-api.conf.d/</code>
  </p><p>
   All custom configuration file must follow the rules listed in
   <a class="xref" href="#sec-depl-ostack-configs-custom-naming" title="14.3. Naming Conventions for Custom Configuration Files">Section 14.3, “Naming Conventions for Custom Configuration Files”</a>.
  </p></div><div class="sect1 " id="sec-depl-ostack-configs-custom-naming"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Naming Conventions for Custom Configuration Files</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-configs-custom-naming">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ostack_configs.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-configs-custom-naming</li></ul></div></div></div></div><p>
   Use the following rules for any configuration files you add:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The file name must start with a 3-digit number and a dash. For example:
     <code class="filename">/etc/nova/nova.conf.d/500-nova.conf</code>
    </p></li><li class="listitem "><p>
     The file must have the following file name extension:
     <code class="literal">.conf</code>
    </p></li><li class="listitem "><p>
     For configuration management systems (for example: Crowbar, Salt), use
     numbers between <code class="literal">100</code> and <code class="literal">499</code>.
    </p></li><li class="listitem "><p>
     To override settings written by the configuration management system, use
     numbers starting from <code class="literal">500</code>. They have higher priority.
    </p></li></ul></div></div><div class="sect1 " id="sec-depl-ostack-configs-custom-order"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Processing Order of Configuration Files</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-configs-custom-order">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ostack_configs.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-configs-custom-order</li></ul></div></div></div></div><p>
   The configuration files are processed in the following order:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="filename">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">SERVICE</em>.conf</code>
    </p></li><li class="listitem "><p>
     <code class="filename">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">SERVICE</em>.conf.d/*.conf</code>
     (in dictionary order)
    </p></li><li class="listitem "><p>
     <code class="filename">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">ROLE</em>.conf.d/*.conf</code>
     (in dictionary order)
    </p></li></ul></div><p>
   If conflicting values are set for the same parameter, the last configured
   value overwrites all previous ones. In particular, values defined in
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">SERVICE</em>.conf.d/<em class="replaceable ">XXX</em>-<em class="replaceable ">SERVICE</em>.conf</pre></div><p>
   overwrite configuration values in 
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable ">SERVICE</em>/<em class="replaceable ">SERVICE</em>.conf</pre></div></div><div class="sect1 " id="sec-depl-ostack-configs-custom-more"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-configs-custom-more">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ostack_configs.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-configs-custom-more</li></ul></div></div></div></div><p>
   For details, also see 
   <code class="filename">/etc/<em class="replaceable ">SERVICE</em>/README.config</code>.
  </p></div></div><div class="chapter " id="install-heat-templates"><div class="titlepage"><div><div><h2 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SUSE CaaS Platform Heat Templates</span> <a title="Permalink" class="permalink" href="#install-heat-templates">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>crowbar_install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span>install-heat-templates</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-heat-templates-install"><span class="number">15.1 </span><span class="name">SUSE CaaS Platform Heat Installation Procedure</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.4"><span class="number">15.2 </span><span class="name">Installing SUSE CaaS Platform with Multiple Masters</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.5"><span class="number">15.3 </span><span class="name">Enabling the Cloud Provider Integration (CPI) Feature</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.6"><span class="number">15.4 </span><span class="name">More Information about SUSE CaaS Platform</span></a></span></dt></dl></div></div><p>
  This chapter describes how to install SUSE CaaS Platform Heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
 </p><div class="sect1" id="sec-heat-templates-install"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE CaaS Platform Heat Installation Procedure</span> <a title="Permalink" class="permalink" href="#sec-heat-templates-install">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>crowbar_install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span>sec-heat-templates-install</li></ul></div></div></div></div><div class="procedure " id="id-1.3.5.7.3.2"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.1: </span><span class="name">Preparation </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.3.2">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Download the latest SUSE CaaS Platform for <span class="productname">OpenStack</span> image (for example,
     <code class="filename">SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</code>)
     from <a class="link" href="https://download.suse.com" target="_blank">https://download.suse.com</a>.
    </p></li><li class="step "><p>
     Upload the image to Glance:
    </p><div class="verbatim-wrap"><pre class="screen">openstack image create --public --disk-format qcow2 --container-format \
bare --file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
CaaSP-3</pre></div></li><li class="step "><p>
     Install the <span class="package ">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p><p>
     Alternatively, you can get official Heat templates by cloning the
     appropriate Git repository:
    </p><div class="verbatim-wrap"><pre class="screen">git clone https://github.com/SUSE/caasp-openstack-heat-templates</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.5.7.3.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.2: </span><span class="name">Installing Templates via Horizon </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.3.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In Horizon, go to
     <span class="guimenu ">Project</span> › <span class="guimenu ">Stacks</span> › <span class="guimenu ">Launch
     Stack</span>.
    </p></li><li class="step "><p>
     Select <span class="guimenu ">File</span> from the <span class="guimenu ">Template Source</span>
     drop-down box and upload the <code class="filename">caasp-stack.yaml</code> file.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Launch Stack</span> dialog, provide the required
     information (stack name, password, flavor size, external network of your
     environment, etc.).
    </p></li><li class="step "><p>
     Click <span class="guimenu ">Launch</span> to launch the stack. This creates all
     required resources for running SUSE CaaS Platform in an <span class="productname">OpenStack</span> environment. The
     stack creates one Admin Node, one Master Node, and server worker nodes as
     specified.
    </p></li></ol></div></div><div class="procedure " id="id-1.3.5.7.3.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.3: </span><span class="name">Install Templates from the Command Line </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.3.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-environment.yaml</code> file.
    </p></li><li class="step "><p>
     Create a stack in Heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen">openstack stack create -t caasp-stack.yaml -e caasp-environment.yaml \
--parameter image=CaaSP-3 caasp-stack</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.5.7.3.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.4: </span><span class="name">Accessing Velum SUSE CaaS Platform dashboard </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.3.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the Admin Node.
     You can access it using the Admin Node's floating IP address.
    </p></li><li class="step "><p>
     Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to complete the
     SUSE CaaS Platform installation.
    </p></li></ol></div></div><p>
   When you have successfully accessed the admin node web interface via the
   floating IP, follow the instructions at <a class="link" href="https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/" target="_blank">https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/</a> to
   continue the setup of SUSE CaaS Platform.
  </p></div><div class="sect1" id="id-1.3.5.7.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SUSE CaaS Platform with Multiple Masters</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.4">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>crowbar_install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.5.7.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    A Heat stack with load balancing and multiple master nodes can only be
    created from the command line, because Horizon does not have support for nested
    Heat templates.
   </p></div><p>
   Install the <span class="package ">caasp-openstack-heat-templates</span> package on a
   machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> repositories:
  </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
   The installed templates are located in
   <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
  </p><p>
   A working load balancer is needed in your SUSE <span class="productname">OpenStack</span> Cloud deployment. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   uses HAProxy.
  </p><p>
   Verify that load balancing with HAProxy is working correctly
   in your <span class="productname">OpenStack</span> installation by creating a load balancer manually and
   checking that the <code class="literal">provisioning_status</code> changes to
   <code class="literal">Active</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer show
&lt;<em class="replaceable ">LOAD_BALANCER_ID</em>&gt;</pre></div><p>
   HAProxy is the default load balancer provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
  </p><p>
   The steps below can be used to set up a network, subnet, router, security
   and IPs for a test <code class="literal">lb_net1</code> network with
   <code class="literal">lb_subnet1</code> subnet.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack network create lb_net1
  <code class="prompt user">tux &gt; </code>openstack subnet create --name lb_subnet1 lb_net1 \
--subnet-range 172.29.0.0/24 --gateway 172.29.0.2
<code class="prompt user">tux &gt; </code>openstack router create lb_router1
<code class="prompt user">tux &gt; </code>openstack router add subnet lb_router1 lb_subnet1
<code class="prompt user">tux &gt; </code>openstack router set lb_router1 --external-gateway ext-net
<code class="prompt user">tux &gt; </code>openstack network list</pre></div><div class="procedure " id="id-1.3.5.7.4.12"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.5: </span><span class="name">Steps to Install SUSE CaaS Platform with Multiple Masters </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.4.12">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file.
    </p></li><li class="step "><p>
     Set <code class="literal">master_count</code> to the desired number in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file. The master
     count must be set to an odd number of nodes.
    </p><div class="verbatim-wrap"><pre class="screen">master_count: 3</pre></div></li><li class="step "><p>
     Create a stack in Heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack stack create -t caasp-multi-master-stack.yaml \
-e caasp-multi-master-environment.yaml --parameter image=CaaSP-3 caasp-multi-master-stack</pre></div></li><li class="step "><p>
     Find the floating IP address of the load balancer. This is necessary for
     accessing the Velum SUSE CaaS Platform dashboard.
    </p><ol type="a" class="substeps "><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer list --provider</pre></div></li><li class="step "><p>
       From the output, copy the <code class="literal">id</code> and enter it in the
       following command as shown in the following example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer show id</pre></div><div class="verbatim-wrap"><pre class="screen">+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| id                  | 0d973d80-1c79-40a4-881b-42d111ee9625           |
| listeners           | {"id": "c9a34b63-a1c8-4a57-be22-75264769132d"} |
|                     | {"id": "4fa2dae0-126b-4eb0-899f-b2b6f5aab461"} |
| name                | caasp-stack-master_lb-bhr66gtrx3ue             |
| operating_status    | ONLINE                                         |
| pools               | {"id": "8c011309-150c-4252-bb04-6550920e0059"} |
|                     | {"id": "c5f55af7-0a25-4dfa-a088-79e548041929"} |
| provider            | haproxy                                        |
| provisioning_status | ACTIVE                                         |
| tenant_id           | fd7ffc07400642b1b05dbef647deb4c1               |
| vip_address         | 172.28.0.6                                     |
| vip_port_id         | 53ad27ba-1ae0-4cd7-b798-c96b53373e8b           |
| vip_subnet_id       | 87d18a53-ad0c-4d71-b82a-050c229b710a           |
+---------------------+------------------------------------------------+</pre></div></li><li class="step "><p>
       Search the floating IP list for <code class="literal">vip_address</code>
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack floating ip list | grep 172.28.0.6
| d636f3...481b0c | fd7ff...deb4c1 | 172.28.0.6  | 10.84.65.37  | 53ad2...373e8b |</pre></div></li><li class="step "><p>
       The load balancer floating ip address is 10.84.65.37
      </p></li></ol></li></ol></div></div><p>
   <span class="bold"><strong>Accessing the Velum SUSE CaaS Platform Dashboard</strong></span>
  </p><p>
   After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the
   admin node. You can access it using the floating IP address of the admin
   node.
  </p><p>
   Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to
   complete the SUSE CaaS Platform installation.
  </p><p>
   SUSE CaaS Platform Admin Node Install: Screen 1
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_1.png" target="_blank"><img src="images/caasp_1.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 2
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_2.png" target="_blank"><img src="images/caasp_2.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 3
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_3.png" target="_blank"><img src="images/caasp_3.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 4
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_4.png" target="_blank"><img src="images/caasp_4.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 5
  </p><p>
   Set External Kubernetes API to
   <em class="replaceable ">LOADBALANCER_FLOATING_IP</em>, External Dashboard FQDN
   to <em class="replaceable ">ADMIN_NODE_FLOATING_IP</em>
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_5.png" target="_blank"><img src="images/caasp_5.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 6
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_6.png" target="_blank"><img src="images/caasp_6.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 7
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_7.png" target="_blank"><img src="images/caasp_7.png" width="" /></a></div></div></div><div class="sect1" id="id-1.3.5.7.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the Cloud Provider Integration (CPI) Feature</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.5">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>crowbar_install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    When deploying a CaaaSP cluster using SUSE CaaS Platform <span class="productname">OpenStack</span> Heat
    templates, the following CPI parameters can be set in
    <code class="filename">caasp-environment.yaml</code> or
    <code class="filename">caasp-multi-master-environment.yaml</code>.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.7.5.3.1"><span class="term ">cpi_auth_url</span></dt><dd><p>
       The URL of the Keystone API used to authenticate the user. This value
       can be found on <span class="productname">OpenStack</span> Dashboard under
       <span class="guimenu ">Access and Security</span> › <span class="guimenu ">API
       Access</span> › <span class="guimenu ">Credentials</span> (for
       example, https://api.keystone.example.net:5000/v3/)
      </p></dd><dt id="id-1.3.5.7.5.3.2"><span class="term ">cpi_domain_name</span></dt><dd><p>
       Name of the domain the user belongs to
      </p></dd><dt id="id-1.3.5.7.5.3.3"><span class="term ">cpi_tenant_name</span></dt><dd><p>
       Name of the project the user belongs to. This is the project where the
       resources will be created.
      </p></dd><dt id="id-1.3.5.7.5.3.4"><span class="term ">cpi_region</span></dt><dd><p>
       Name of the region to use when running a multi-region <span class="productname">OpenStack</span>
       cloud. The region is a general division of an <span class="productname">OpenStack</span> deployment.
      </p></dd><dt id="id-1.3.5.7.5.3.5"><span class="term ">cpi_username</span></dt><dd><p>
       Username of a valid user that has been set in Keystone. Default: admin
      </p></dd><dt id="id-1.3.5.7.5.3.6"><span class="term ">cpi_password</span></dt><dd><p>
       Password of a valid user that has been set in Keystone.
      </p></dd><dt id="id-1.3.5.7.5.3.7"><span class="term ">cpi_monitor_max_retries</span></dt><dd><p>
       Neutron load balancer monitoring max retries. Default: 3
      </p></dd><dt id="id-1.3.5.7.5.3.8"><span class="term ">cpi_bs_version</span></dt><dd><p>
       Cinder Block Storage API version. Possible values are v1, v2 , v3 or
       auto. Default: auto
      </p></dd><dt id="id-1.3.5.7.5.3.9"><span class="term ">cpi_ignore_volume_az</span></dt><dd><p>
       Ignore Cinder and Nova availability zones. Default: true
      </p></dd></dl></div><p>
    When the SUSE CaaS Platform cluster has been deployed by the Heat templates, the
    Velum dashboard on the admin node
    (https://<em class="replaceable ">ADMIN-NODE-IP</em>/) will have entries for
    <span class="guimenu ">Cloud Provider Integration</span> (CPI). The <span class="productname">OpenStack</span>
    settings form will show the values that were set in the
    <code class="filename">caasp-environment.yaml</code> or
    <code class="filename">caasp-multi-master-environment.yaml</code> files.
   </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/cpi_1.png" target="_blank"><img src="images/cpi_1.png" width="" /></a></div></div><p>
    After the SUSE CaaS Platform cluster comes up, install the latest SUSE CaaS Platform 3.0 Maintenance
    Update using the following steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Spin up a SUSE CaaS Platform cluster using Heat templates following the
      instructions in <a class="xref" href="#sec-heat-templates-install" title="15.1. SUSE CaaS Platform Heat Installation Procedure">Section 15.1, “SUSE CaaS Platform Heat Installation Procedure”</a>. Do not go
      through the bootstrapping steps, because the SUSE CaaS Platform Maintenance Update
      (MU) must be applied first. After the maintenance update process below
      succeeds, return to the SUSE CaaS Platform installation instructions and follow the
      admin node bootstrapping steps.
     </p><p>
      Apply the SUSE CaaS Platform 3.0 Maintenance Update with the following steps:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Log in to each node and add the update repository.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper ar http://nu.novell.com/SUSE/Updates/SUSE-CAASP/3.0/x86_64/update/ caasp_update</pre></div></li><li class="step "><p>
        With root privileges, run <code class="literal">transactional-update</code> on each node.
       </p></li><li class="step "><p>
        Reboot each node
       </p></li><li class="step "><p>
        Verify that the Velum image packages were updated
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>zypper se --detail velum-image
i | sles12-velum-image | package    | 3.1.7-3.27.3  | x86_64 | update_caasp</pre></div></li></ol></li><li class="step "><p>
      Upload a valid trust certificate that can validate a certificate
      presented by Keystone at the specified
      <code class="literal">keystone_auth_url</code> in the <code class="literal">system-wide
      certificate</code> section of Velum. If the SSL certificate provided
      by Keystone cannot be verified, bootstrapping fails with the error
      message <code class="literal">x509: certificate signed by unknown authority</code>.
     </p><div id="id-1.3.5.7.5.7.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
       If your <span class="productname">OpenStack</span> endpoints operate on the Internet, or if the SSL
       certificates in use have been signed by a public authority, no action is
       needed to enable secure communication with them.
      </p><p>
       If your <span class="productname">OpenStack</span> services operate in a private network using SSL
       certificates signed by an organizational certificate authority, provide
       that CA certificate as the system-wide certificate.
      </p><p>
       If your <span class="productname">OpenStack</span> service SSL infrastructure was self-signed during the
       installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> (as is done by default), its CA
       certificate (with the file extension <code class="literal">.pem</code>) can be
       retrieved from the admin node in the
       <code class="filename">/etc/ssl/certs/</code> directory. The filename should
       match the node name of your primary controller node. Download this file
       and provide it as the system-wide certificate.
      </p></div></li><li class="step "><p>
      After the nodes come up, continue with the installation instructions in
      <a class="xref" href="#sec-heat-templates-install" title="15.1. SUSE CaaS Platform Heat Installation Procedure">Section 15.1, “SUSE CaaS Platform Heat Installation Procedure”</a> following the admin node
      cluster bootstrapping steps.
     </p></li></ol></div></div></div><div class="sect1" id="id-1.3.5.7.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information about SUSE CaaS Platform</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.6">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>crowbar_install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
  More information about SUSE CaaS Platform is available at <a class="link" href="https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/" target="_blank">https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/</a>
 </p></div></div></div><div class="part" id="part-depl-nostack"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part IV </span><span class="name">Setting Up Non-<span class="productname">OpenStack</span> Services </span><a title="Permalink" class="permalink" href="#part-depl-nostack">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-depl-nostack"><span class="number">16 </span><span class="name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></a></span></dt><dd class="toc-abstract"><p>
    In addition to <span class="productname">OpenStack</span> barclamps, SUSE <span class="productname">OpenStack</span> Cloud includes several components that can be configured using the appropriate  Crowbar barclamps.
   </p></dd></dl></div><div class="chapter " id="cha-depl-nostack"><div class="titlepage"><div><div><h2 class="title"><span class="number">16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Non-<span class="productname">OpenStack</span> Components</span> <a title="Permalink" class="permalink" href="#cha-depl-nostack">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nostack_services.xml</li><li><span class="ds-label">ID: </span>cha-depl-nostack</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-nostack-crowbar-tuning"><span class="number">16.1 </span><span class="name">Tuning the Crowbar Service</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-nostack-ntp"><span class="number">16.2 </span><span class="name">Configuring the NTP Service</span></a></span></dt></dl></div></div><p>
    In addition to <span class="productname">OpenStack</span> barclamps, SUSE <span class="productname">OpenStack</span> Cloud includes several components that can be configured using the appropriate  Crowbar barclamps.
   </p><div class="sect1 " id="sec-depl-nostack-crowbar-tuning"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning the Crowbar Service</span> <a title="Permalink" class="permalink" href="#sec-depl-nostack-crowbar-tuning">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nostack_services.xml</li><li><span class="ds-label">ID: </span>sec-depl-nostack-crowbar-tuning</li></ul></div></div></div></div><p>
    Crowbar is a self-referential barclamp used for enabling other barclamps. By
    creating a Crowbar proposal, you can modify the default number of threads
    and workers. This way, you can scale the admin
    server according to the actual usage or the number of available cores of
    the admin node.
  </p><div class="figure" id="id-1.3.6.2.4.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/crowbar_tuning_raw.png" target="_blank"><img src="images/crowbar_tuning_raw.png" width="" alt="The Crowbar barclamp: Raw Mode" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 16.1: </span><span class="name">The Crowbar barclamp: Raw Mode </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.4.3">#</a></h6></div></div><p>
    To change the default settings, create a Crowbar proposal and switch to the
    <span class="guimenu ">Raw</span> view. Adjust then the
    <code class="systemitem">workers</code> and <code class="systemitem">threads</code>
    values. The number of threads should be set to the number of available
    cores. The default number of workers should be increased to 3 if the
    graphical interface becomes slow. Save and apply the changes using the
    appropriate buttons.
  </p></div><div class="sect1 " id="sec-depl-nostack-ntp"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the NTP Service</span> <a title="Permalink" class="permalink" href="#sec-depl-nostack-ntp">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nostack_services.xml</li><li><span class="ds-label">ID: </span>sec-depl-nostack-ntp</li></ul></div></div></div></div><p>
    The NTP service is responsible for keeping the clocks in your cloud servers
    in sync. Among other things, synchronized clocks ensure that the
    chef-client works properly. It also makes it easier to read logs from
    different nodes by correlating timestamps in them. The NTP component is deployed on the Administration Server automatically using the default settings. The NTP barclamp can be used to specify IP addresses of the external NTP servers and assign specific roles to the desired nodes. The following parameter can be configured using the NTP barclamp:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.2.5.3.1"><span class="term ">External servers</span></dt><dd><p>
	  A comma-separated list of IP addresses of external NTP servers.
	</p></dd></dl></div><p>
    The NTP service consists of two different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.2.5.5.1"><span class="term "><span class="guimenu ">ntp-server</span>
    </span></dt><dd><p>
      A node that acts as an NTP server for NTP clients in your cloud. There
      can be more than one node with the ntp-server role in your cloud. In
      this scenario, the NTP server nodes can communicate with each other and the specified external servers to
      keep their time in sync.
     </p></dd><dt id="id-1.3.6.2.5.5.2"><span class="term "><span class="guimenu ">ntp-client</span>
    </span></dt><dd><p>
      The <code class="systemitem">ntp-client</code> role can be assigned to any
      node. Nodes with the ntp-client role assigned to them keep their time in
      sync using NTP servers in your cloud.
     </p></dd></dl></div></div></div></div><div class="part" id="part-depl-maintenance"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part V </span><span class="name">Maintenance and Support </span><a title="Permalink" class="permalink" href="#part-depl-maintenance">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-depl-maintenance"><span class="number">17 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Maintenance</span></a></span></dt><dd class="toc-abstract"><p></p></dd><dt><span class="chapter"><a href="#self-assign-certs"><span class="number">18 </span><span class="name">Generate SUSE <span class="productname">OpenStack</span> Cloud Self Signed Certificate</span></a></span></dt><dd class="toc-abstract"><p>The purpose of this document is to help set up SSL Support for several services in SUSE OpenStack Cloud. The scope of this document covers all public endpoints in your OpenStack cluster. In most cases you want to have a Secure CA or External CA where your certificates are signed. You will sign with …</p></dd><dt><span class="chapter"><a href="#cha-deploy-logs"><span class="number">19 </span><span class="name">Log Files</span></a></span></dt><dd class="toc-abstract"><p>
  Find a list of log files below, sorted according to the nodes where they
  can be found.
 </p></dd><dt><span class="chapter"><a href="#cha-depl-trouble"><span class="number">20 </span><span class="name">Troubleshooting and Support</span></a></span></dt><dd class="toc-abstract"><p>
  Find solutions for the most common pitfalls and technical details on how
  to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> here.
 </p></dd></dl></div><div class="chapter " id="cha-depl-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE <span class="productname">OpenStack</span> Cloud Maintenance</span> <a title="Permalink" class="permalink" href="#cha-depl-maintenance">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>cha-depl-maintenance</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-depl-maintenance-updates"><span class="number">17.1 </span><span class="name">Keeping the Nodes Up-To-Date</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-service-order"><span class="number">17.2 </span><span class="name">Service Order on SUSE <span class="productname">OpenStack</span> Cloud Start-up or Shutdown</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-upgrade"><span class="number">17.3 </span><span class="name">Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-recover-compute-node-failure"><span class="number">17.4 </span><span class="name">Recovering from Compute Node Failure</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-bootstrap-compute-plane"><span class="number">17.5 </span><span class="name">Bootstrapping Compute Plane</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.2.9"><span class="number">17.6 </span><span class="name">Updating MariaDB with Galera</span></a></span></dt><dt><span class="section"><a href="#database-maintenance"><span class="number">17.7 </span><span class="name">Periodic OpenStack Maintenance Tasks</span></a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-fernet-tokens"><span class="number">17.8 </span><span class="name">Rotating Fernet Tokens</span></a></span></dt></dl></div></div><div class="sect1" id="sec-depl-maintenance-updates"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Keeping the Nodes Up-To-Date</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-updates">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-updates</li></ul></div></div></div></div><p>
   Keeping the nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> up-to-date requires an appropriate
   setup of the update and pool repositories and the deployment of
   either the <span class="guimenu ">Updater</span> barclamp or the SUSE Manager
   barclamp. For details, see
   <a class="xref" href="#sec-depl-adm-conf-repos-scc" title="5.2. Update and Pool Repositories">Section 5.2, “Update and Pool Repositories”</a>, <a class="xref" href="#sec-depl-inst-nodes-post-updater" title="11.4.1. Deploying Node Updates with the Updater Barclamp">Section 11.4.1, “Deploying Node Updates with the Updater Barclamp”</a>, and
   <a class="xref" href="#sec-depl-inst-nodes-post-manager" title="11.4.2. Configuring Node Updates with the SUSE Manager Client Barclamp">Section 11.4.2, “Configuring Node Updates with the <span class="guimenu ">SUSE Manager Client</span>
    Barclamp”</a>.
  </p><p>
   If one of those barclamps is deployed, patches are installed on the
   nodes. Patches that do not require a reboot will not cause a service
   interruption. If a patch (for example, a kernel update) requires a reboot
   after the installation, services running on the machine that is rebooted
   will not be available within SUSE <span class="productname">OpenStack</span> Cloud.  Therefore, we strongly recommend
   installing those patches during a maintenance window.
  </p><div id="id-1.3.7.2.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Maintenance Mode</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>, it is not possible to put your entire
    SUSE <span class="productname">OpenStack</span> Cloud into <span class="quote">“<span class="quote ">Maintenance Mode</span>”</span> (such as limiting all users to
    read-only operations on the control plane), as <span class="productname">OpenStack</span> does not support
    this. However when Pacemaker is deployed to manage HA clusters, it should
    be used to place services and cluster nodes into <span class="quote">“<span class="quote ">Maintenance
    Mode</span>”</span> before performing maintenance functions on them. For more
    information, see <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2</a>.
   </p></div><div class="variablelist "><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="name">Consequences when Rebooting Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.7.2.4.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.7.2.4.6.2"><span class="term ">Administration Server</span></dt><dd><p>
      While the Administration Server is offline, it is not possible to deploy new
      nodes. However, rebooting the Administration Server has no effect on starting
      instances or on instances already running.
     </p></dd><dt id="id-1.3.7.2.4.6.3"><span class="term ">Control Nodes</span></dt><dd><p>
      The consequences a reboot of a Control Node depend on the
      services running on that node:
     </p><p><span class="formalpara-title">Database, Keystone, RabbitMQ, Glance, Nova: </span>
       No new instances can be started.
      </p><p><span class="formalpara-title">Swift: </span>
       No object storage data is available. If Glance uses
       Swift, it will not be possible to start new instances.
      </p><p><span class="formalpara-title">Cinder, Ceph: </span>
       No block storage data is available.
      </p><p><span class="formalpara-title">Neutron: </span>
       No new instances can be started. On running instances the
       network will be unavailable.
      </p><p><span class="formalpara-title">Horizon. </span>
       Horizon will be unavailable. Starting and managing instances
       can be done with the command line tools.
      </p></dd><dt id="id-1.3.7.2.4.6.4"><span class="term ">Compute Nodes</span></dt><dd><p>
      
      Whenever a Compute Node is rebooted, all instances running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <span class="quote">“<span class="quote ">evacuate</span>”</span> the node by
      migrating instances to another node, before rebooting it.

     </p></dd></dl></div></div><div class="sect1" id="sec-depl-maintenance-service-order"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service Order on SUSE <span class="productname">OpenStack</span> Cloud Start-up or Shutdown</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-service-order">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-service-order</li></ul></div></div></div></div><p>
   In case you need to restart your complete SUSE <span class="productname">OpenStack</span> Cloud (after a complete shut
   down or a power outage), ensure that the external Ceph cluster is started,
   available and healthy. Start then the nodes and services in the
   following order:
  </p><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Service Order on Start-up </span><a title="Permalink" class="permalink" href="#id-1.3.7.2.5.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>
     Control Node/Cluster on which the Database is deployed
    </p></li><li class="listitem "><p>
     Control Node/Cluster on which RabbitMQ is deployed
    </p></li><li class="listitem "><p>
     Control Node/Cluster on which Keystone is deployed
    </p></li><li class="listitem "><p>
     For Swift:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Storage Node on which the <code class="literal">swift-storage</code> role is deployed
      </p></li><li class="listitem "><p>
       Storage Node on which the <code class="literal">swift-proxy</code> role is deployed
      </p></li></ol></div></li><li class="listitem "><p>
     Any remaining Control Node/Cluster. The following additional rules apply:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       The Control Node/Cluster on which the <code class="literal">neutron-server</code>
       role is deployed needs to be started before starting the node/cluster
       on which the <code class="literal">neutron-l3</code> role is deployed.
      </p></li><li class="listitem "><p>
       The Control Node/Cluster on which the <code class="literal">nova-controller</code>
       role is deployed needs to be started before starting the node/cluster
       on which Heat is deployed.
      </p></li></ul></div></li><li class="listitem "><p>
     Compute Nodes
    </p></li></ol></div><p>
   If multiple roles are deployed on a single Control Node, the services are
   automatically started in the correct order on that node. If you have more
   than one node with multiple roles, make sure they are
   started as closely as possible to the order listed above.
  </p><p>
   If you need to shut down SUSE <span class="productname">OpenStack</span> Cloud, the nodes and services need to be
   terminated in reverse order than on start-up:
  </p><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Service Order on Shut-down </span><a title="Permalink" class="permalink" href="#id-1.3.7.2.5.6">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>
     Compute Nodes
    </p></li><li class="listitem "><p>
     Control Node/Cluster on which Heat is deployed
    </p></li><li class="listitem "><p>
     Control Node/Cluster on which the <code class="literal">nova-controller</code>
     role is deployed
    </p></li><li class="listitem "><p>
     Control Node/Cluster on which the <code class="literal">neutron-l3</code>
     role is deployed
    </p></li><li class="listitem "><p>
     All Control Node(s)/Cluster(s) on which neither of the following services
     is deployed: Database, RabbitMQ, and Keystone.
    </p></li><li class="listitem "><p>
     For Swift:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Storage Node on which the <code class="literal">swift-proxy</code> role is
       deployed
      </p></li><li class="listitem "><p>
       Storage Node on which the <code class="literal">swift-storage</code> role is
       deployed
      </p></li></ol></div></li><li class="listitem "><p>
     Control Node/Cluster on which Keystone is deployed
    </p></li><li class="listitem "><p>
     Control Node/Cluster on which RabbitMQ is deployed
    </p></li><li class="listitem "><p>
     Control Node/Cluster on which the Database is deployed
    </p></li><li class="listitem "><p>
     If required, gracefully shut down an external Ceph cluster
    </p></li></ol></div></div><div class="sect1" id="sec-depl-maintenance-upgrade"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-upgrade">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-upgrade</li></ul></div></div></div></div><p>
   Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 can be done either via a
   Web interface or from the command line. A <span class="quote">“<span class="quote ">non-disruptive</span>”</span> update is
   supported when the requirements listed at <a class="xref" href="#list-depl-maintenance-upgrade-non-disruptive" title="Non-Disruptive Upgrade Requirements">Non-Disruptive Upgrade Requirements</a> are met. The
   non-disruptive upgrade provides a fully-functional SUSE <span class="productname">OpenStack</span> Cloud operation
   during most of the upgrade procedure.
  </p><p>
   If the requirements for a non-disruptive upgrade are not met, the
   upgrade procedure will be done in normal mode. When
   live-migration is set up, instances will be migrated to another node
   before the respective Compute Node is updated to ensure continuous
   operation.
 </p><div id="id-1.3.7.2.6.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: STONITH and Administration Server</h6><p>
      Make sure that the STONITH mechanism in your cloud does not rely on the
      state of the Administration Server (for example, no SBD devices are located there,
      and IPMI is not using the network connection relying on the
      Administration Server). Otherwise, this may affect the clusters when the Administration Server is
      rebooted during the upgrade procedure.
     </p></div><div class="sect2" id="sec-depl-maintenance-upgrade-require"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-upgrade-require">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-upgrade-require</li></ul></div></div></div></div><p>
    When starting the upgrade process, several checks are performed to
    determine whether the SUSE <span class="productname">OpenStack</span> Cloud is in an upgradeable state and whether a
    non-disruptive update would be supported:
   </p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">General Upgrade Requirements </span><a title="Permalink" class="permalink" href="#id-1.3.7.2.6.5.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
      All nodes need to have the latest <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 updates <span class="bold"><strong>and</strong></span> the latest SLES 12 SP2 updates installed. If
      this is not the case, refer to <a class="xref" href="#sec-depl-inst-nodes-post-updater" title="11.4.1. Deploying Node Updates with the Updater Barclamp">Section 11.4.1, “Deploying Node Updates with the Updater Barclamp”</a> for instructions on how to
      update.
     </p></li><li class="listitem "><p>
      All allocated nodes need to be turned on and have to be in state
      <span class="quote">“<span class="quote ">ready</span>”</span>.
     </p></li><li class="listitem "><p>
      All barclamp proposals need to have been successfully deployed. If a
      proposal is in state <span class="quote">“<span class="quote ">failed</span>”</span>, the upgrade procedure will
      refuse to start. Fix the issue or—if possible—remove the
      proposal.
     </p></li><li class="listitem "><p>
      If the Pacemaker barclamp is deployed, all clusters
      need to be in a healthy state.
     </p></li><li class="listitem "><p> The upgrade will not start when Ceph is deployed via Crowbar. Only
     external Ceph is supported. Documentation for SUSE Enterprise Storage is available at
     <a class="link" href="https://documentation.suse.com/ses/5.5" target="_blank">https://documentation.suse.com/ses/5.5</a>.
     </p></li><li class="listitem "><p>
      Upgrade is only possible if the <code class="literal">SQL
      Engine</code> in the <code class="literal">Database</code> barclamp is set to
      <span class="guimenu ">MariaDB</span>. For further info, see <a class="xref" href="#sec-depl-maintenance-postgre-mariadb-upgrade" title="17.3.2. Preparing PostgreSQL-Based SUSE OpenStack Cloud Crowbar 7 for Upgrade">Section 17.3.2, “Preparing PostgreSQL-Based <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 for Upgrade”</a>
     </p></li><li class="listitem "><p>
      The following repositories need to be available on a server that is
      accessible from the Administration Server. The HA repositories are only needed if you
      have an HA setup. It is recommended to use the same server that also
      hosts the respective repositories of the current version.
     </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal"><span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Pool</code></td></tr><tr><td><code class="literal"><span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-8</span></span>-Update</code></td></tr><tr><td><code class="literal">SLES12-SP3-Pool</code></td></tr><tr><td><code class="literal">SLES12-SP3-Update</code></td></tr><tr><td>
       <code class="literal">SLE12-HA12-SP3-Pool</code> (for HA setups only)
      </td></tr><tr><td>
       <code class="literal">SLE12-HA12-SP3-Update</code> (for HA setups only)
      </td></tr></table><div id="id-1.3.7.2.6.5.3.8.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      Do not add repositories to the SUSE <span class="productname">OpenStack</span> Cloud repository configuration. This
      needs to be done during the upgrade procedure.
      </p></div></li><li class="listitem "><p>
       A non-disruptive upgrade is not supported if Cinder has been
       deployed with the <code class="literal">raw devices</code> or <code class="literal">local
       file</code> back-end. In this case, you have to perform a regular
       upgrade, or change the Cinder back-end for a non-disruptive
       upgrade.
     </p></li><li class="listitem "><p>
       If SUSE Enterprise Storage is deployed using Crowbar, migrate it to an
       external cluster. You may want to upgrade SUSE Enterprise Storage, refer to
       <a class="link" href="https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#ceph-upgrade-4to5crowbar" target="_blank">https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#ceph-upgrade-4to5crowbar</a>.
     </p></li><li class="listitem "><p>
        Run the command <code class="command">nova-manage db archive_deleted_rows</code> to purge deleted instances from the database table. This can significanly reduce time required for the database migration procedure.
      </p></li><li class="listitem "><p>
        Run the commands <code class="command">cinder-manage db purge</code> and <code class="command">heat-manage purge_deleted</code> to purge database entries that are marked as deleted.
      </p></li></ul></div><div class="itemizedlist " id="list-depl-maintenance-upgrade-non-disruptive"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Non-Disruptive Upgrade Requirements </span><a title="Permalink" class="permalink" href="#list-depl-maintenance-upgrade-non-disruptive">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>
      All Control Nodes need to be set up highly available.
     </p></li><li class="listitem "><p>
      A non-disruptive upgrade is not supported if the Cinder
      has been deployed with the <code class="literal">raw devices</code> or
      <code class="literal">local file</code> back-end. In this case, you have to perform
      a regular upgrade, or change the Cinder back-end for a
      non-disruptive upgrade.
     </p></li><li class="listitem "><p>
      A non-disruptive upgrade is prevented if the
      <code class="literal">cinder-volume</code> service is placed on Compute Node. For a
      non-disruptive upgrade, <code class="literal">cinder-volume</code> should either be
      HA-enabled or placed on non-compute nodes.
     </p></li><li class="listitem "><p>
      A non-disruptive upgrade is prevented if <code class="literal">manila-share</code>
      service is placed on a Compute Node. For more information, see <a class="xref" href="#sec-depl-ostack-manila" title="12.15. Deploying Manila">Section 12.15, “Deploying Manila”</a>
     </p></li><li class="listitem "><p>
      Live-migration support needs to be configured and enabled for the
      Compute Nodes. The amount of free resources (CPU and RAM) on the
      Compute Nodes needs to be sufficient to evacuate the nodes one by one.
     </p></li><li class="listitem "><p>
       In case of a non-disruptive upgrade, Glance must be configured as a
       shared storage if the <span class="guimenu ">Default Storage
       Store</span> value in the Glance is set to <code class="literal">File</code>.
     </p></li><li class="listitem "><p>
       For a non-disruptive upgrade, only KVM-based Compute Nodes with
       the <code class="literal">nova-computer-kvm</code> role are allowed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7.
     </p></li><li class="listitem "><p>
       Non-disruptive upgrade is limited to the following cluster
       configurations:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           Single cluster that has all supported controller roles on it
         </p></li><li class="listitem "><p>
          Two clusters where one only has
          <code class="systemitem">neutron-network</code> and the other one has the
          rest of the controller roles.
         </p></li><li class="listitem "><p>
          Two clusters where one only has
          <code class="systemitem">neutron-server</code> plus
          <code class="systemitem">neutron-network</code> and the other one has the
          rest of the controller roles.
         </p></li><li class="listitem "><p>
           Two clusters, where one cluster runs the database and RabbitMQ
         </p></li><li class="listitem "><p>
           Three clusters, where one cluster runs database and RabbitMQ,
           another cluster runs APIs, and the third cluster has the
           <code class="systemitem">neutron-network</code> role.
         </p></li></ul></div><p>
       If your cluster configuration is not supported by the non-disruptive
       upgrade procedure, you can still perform a normal upgrade.
     </p></li></ul></div></div><div class="sect2" id="sec-depl-maintenance-postgre-mariadb-upgrade"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparing PostgreSQL-Based <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 for Upgrade</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-postgre-mariadb-upgrade">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-postgre-mariadb-upgrade</li></ul></div></div></div></div><p>
    Upgrading <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7 is only possible when it uses MariaDB deployed
    with the Database barclamp. This means that before you can proceed with
    upgrading <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7, you must migrate PostgreSQL to MariaDB first. The following description covers several possible scenarios.
   </p><div class="sect3" id="sec-postgre-mariadb-upgrade-scenario1"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Non-HA Setup or HA Setup with More Than 2 Nodes in the Cluster and
    PostgreSQL Database Backend</span> <a title="Permalink" class="permalink" href="#sec-postgre-mariadb-upgrade-scenario1">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-postgre-mariadb-upgrade-scenario1</li></ul></div></div></div></div><p>
     Install the latest
     maintenance updates on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7. In the
     Crowbar Web interface, switch to the <span class="guimenu ">Database</span> barclamp. You should
     see the new <code class="systemitem">mysql-server</code> role in the
     <span class="guimenu ">Deployment</span> section. Do not change the
     <code class="literal">sql_engine</code> at this point. Add your Database Node or cluster to the
     <code class="systemitem">mysql-server</code> role and apply the
     barclamp. MariaDB is now deployed and running, but it is still not used
     as a back end for <span class="productname">OpenStack</span> services.
    </p><p>
     Follow <a class="xref" href="#postgre-mariadb-data-migration" title="Data Migration">Procedure 17.1, “Data Migration”</a> to migrate the data from PostgreSQL to MariaDB.
    </p><div class="procedure " id="postgre-mariadb-data-migration"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 17.1: </span><span class="name">Data Migration </span><a title="Permalink" class="permalink" href="#postgre-mariadb-data-migration">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
           Run the <code class="systemitem">/opt/dell/bin/prepare-mariadb</code>
           script on the Admin Node to prepare the MariaDB instance by creating
           the required users, databases, and tables.
      </p></li><li class="step "><p>
       After the script is finished, you'll find a list of all databases and
       URLs that are ready for data migration in the
       <code class="filename">/etc/pg2mysql/databases.yaml</code> located on one of the Database Nodes. The script's output
      may look as follows:</p><div class="verbatim-wrap"><pre class="screen">Preparing node d52-54-77-77-01-01.vo6.cloud.suse.de
Adding recipe[database::pg2mariadb_preparation] to run_list
Running chef-client on d52-54-77-77-01-01.vo6.cloud.suse.de...
Log: /var/log/crowbar/db-prepare.chef-client.log on
d52-54-77-77-01-01.vo6.cloud.suse.de Run time: 444.725193199s
Removing recipe[database::pg2mariadb_preparation] from run_list
Prepare completed for d52-54-77-77-01-01.vo6.cloud.suse.de
Summary of used databases: /etc/pg2mysql/databases.yaml on
d52-54-77-77-01-01.vo6.cloud.suse.de</pre></div><p>The <code class="literal">Summary of used databases:</code> line shows the
      exact location of the <code class="filename">/etc/pg2mysql/databases.yaml</code> file.
      </p><p>
       The <code class="filename">/etc/pg2mysql/databases.yaml</code> file contains a
       list of databases along with their source and target connection strings:
      </p><div class="verbatim-wrap"><pre class="screen">keystone:
  source: postgresql://keystone:vZn3nfxXzv97@192.168.243.87/keystone
  target: mysql+pymysql://keystone:vZn3nfxXzv97@192.168.243.88/keystone?charset=utf8
glance:
  source: postgresql://glance:cOau7NhaA54N@192.168.243.87/glance
  target: mysql+pymysql://glance:cOau7NhaA54N@192.168.243.88/glance?charset=utf8
cinder:
  source: postgresql://cinder:idRll2gJPodv@192.168.243.87/cinder
  target: mysql+pymysql://cinder:idRll2gJPodv@192.168.243.88/cinder?charset=utf8</pre></div></li><li class="step "><p>
       Install the <span class="package ">python-psql2mysql</span> package on the Database Node
       (preferably the one with the <code class="filename">/etc/pg2mysql/databases.yaml</code> file).
      </p></li><li class="step "><p>
       To determine whether the PostgreSQL databases contain data that cannot
       be migrated to MariaDB, run <code class="command">psql2mysql</code> with the
      <code class="literal">precheck</code> option:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>psql2mysql \
--source postgresql://keystone:vZn3nfxXzv97@192.168.243.87/keystone \
--target mysql+pymysql://keystone:vZn3nfxXzv97@192.168.243.88/keystone?charset=utf8 \
precheck</pre></div><p>
       To run precheck operation on all databases in a single operation, use the
       <code class="literal">--batch</code> option and the
       <code class="filename">/etc/pg2mysql/databases.yaml</code> file as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>psql2mysql --batch /etc/pg2mysql/databases.yaml precheck</pre></div><p>
       If the precheck indicates that there is data that cannot be imported
       into MariaDB, modify the offending data manually to fix
       the problems. The example below shows what an output containing issues may
       look like:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>psql2mysql --source postgresql://cinder:idRll2gJPodv@192.168.243.86/cinder precheck
Table 'volumes' contains 4 Byte UTF8 characters which are incompatible with the 'utf8' encoding used by MariaDB
The following rows are affected:
+-----------------------------------------+-----------------+-------+
|               Primary Key               | Affected Column | Value |
+-----------------------------------------+-----------------+-------+
| id=5c6b0274-d18d-4153-9fda-ef3d74ab4500 |   display_name  |   💫   |
+-----------------------------------------+-----------------+-------+
Error during prechecks. 4 Byte UTF8 characters found in the source database.</pre></div></li><li class="step "><p>
       Stop chef-client services on the nodes, to prevent regular runs of
       chef-client from starting database-related <span class="productname">OpenStack</span> services again. To
       do this from the Admin Node, you can use the <code class="command">knife ssh roles:dns-client systemctl
       stop chef-client</code> command. Stop
       all <span class="productname">OpenStack</span> services that make use of the database to prevent them from
       writing new data during the migration.
      </p><div id="id-1.3.7.2.6.6.3.4.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Testing Migration Procedure</h6><p>
        If you want to perform a dry run of the migration procedure, you can run the
       <code class="command">psql2mysql migrate</code> without stopping the
       database-related <span class="productname">OpenStack</span> services. This way, if the test migration
       fails due to errors that weren't caught by the precheck procedure, you
       can fix them with <span class="productname">OpenStack</span> services still running, thus minimizing the
       required downtime. When you perform the actual migration, the data in
       the target databases will be replaced with the latest one in the source databases.
       </p><p>
        After the test migration and before the actual migration, it is recommended to run the
        <code class="command">psql2mysql purge-tables</code> command to purge tables in
        the target database. While this step is optional, it speeds up the
        migration process.
       </p></div><p>
       On an HA setup, shut down all services that make use of the
       database. To do this, use the <code class="command">crm</code> command for
       example:
      </p><div class="verbatim-wrap"><pre class="screen">crm resource stop apache2 keystone cinder-api glance-api \
      neutron-server swift-proxy nova-api magnum-api sahara-api heat-api ceilometer-collector</pre></div><div id="id-1.3.7.2.6.6.3.4.6.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        If the <code class="literal">Manage stateless active/active services with
        Pacemaker</code> option in the Pacemaker barclamp is set to
        <code class="literal">false</code>, the <span class="productname">OpenStack</span> services must be stopped on each
        cluster node using the <code class="command">systemctl</code> command.
       </p></div><p>
       <span class="emphasis"><em>From this point, <span class="productname">OpenStack</span> services
       become unavailable.</em></span>
      </p></li><li class="step "><p>
       You can now migrate databases using the psql2mysql tool. However, before
       performing the migration, make sure that target database nodes have
       enough free space to accommodate the migrated data. To upgrade a single
       database, use the following command format:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>psql2mysql \
--source postgresql://neutron:secret@192.168.1.1/neutron \
--target mysql+pymysql://neutron:evenmoresecret@192.168.1.2/neutron?charset=utf8 \
migrate</pre></div><p>
       To migrate all databases in one operation, use the
       <code class="literal">--batch</code> option and the
       <code class="filename">/etc/pg2mysql/databases.yaml</code> file as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>psql2mysql --batch /etc/pg2mysql/databases.yaml migrate</pre></div></li><li class="step "><p>
       In the Crowbar Web interface, switch to the <span class="guimenu ">Database</span>
       barclamp. Enable the raw view and set the value of
       <code class="literal">sql_engine</code> to <code class="literal">mysql</code>. Apply the
       barclamp. After this step, <span class="productname">OpenStack</span> services should be running again and
       reconfigured to use the MariaDB database back end.
      </p></li><li class="step "><p>
        To prevent the PostgreSQL-related chef code from running, unassign the values
        from <code class="literal">database-server</code> role in the <span class="guimenu ">Database</span>
       barclamp, and apply the barclamp.
      </p></li><li class="step "><p>
       Start chef-client services on the nodes again.
      </p></li><li class="step "><p>
       Stop PostgreSQL on the Database Nodes. Uninstall PostgreSQL packages.
      </p><ol type="a" class="substeps "><li class="step "><p>
         To stop the <code class="literal">postgresql</code> service, run the following
         command on one cluster node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>crm resource stop postgresql
<code class="prompt user">tux &gt; </code>crm resource stop fs-postgresql
<code class="prompt user">tux &gt; </code>crm resource stop drbd-postgresql</pre></div><p>
         Run the last command only if the previous setup used DRBD.
        </p></li><li class="step "><p>
         Remove the packages on all cluster nodes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper rm postgresql94 postgresql94-server</pre></div></li><li class="step "><p>
         If you choose not to upgrade to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 right away, delete unused pacemaker resource from one cluster node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>crm conf delete drbd-postgresql
<code class="prompt user">tux &gt; </code>crm conf delete fs-postgresql
<code class="prompt user">tux &gt; </code>crm conf delete postgresql</pre></div><div id="id-1.3.7.2.6.6.3.4.11.2.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
  Run the <code class="command">crm conf delete drbd-postgresql</code> command only
  if the cloud setup your are upgrading uses DRBD.
 </p></div></li></ol></li><li class="step "><p>
       If DRBD is not used as a backend for RabbitMQ, it is possible to remove it at this point, using the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper rm drbd drbd-utils</pre></div><p>
       You can then reclaim the disk space used by Crowbar for DRBD. To do this, edit the node data using <code class="systemitem">knife</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>knife node edit -a <em class="replaceable ">DRBD_NODE</em></pre></div><p>
        Search for <code class="literal">claimed_disks</code> and remove the entry with owner set to <code class="literal">LVM_DRBD</code>.
      </p><p>
       Otherwise, skip this step until after the full upgrade is done, since the RabbitMQ
       setup will be automatically switched from DRBD during the upgrade procedure.
      </p></li></ol></div></div></div><div class="sect3" id="sec-postgre-mariadb-upgrade-scenario2"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Cluster with 2 Control Nodes</span> <a title="Permalink" class="permalink" href="#sec-postgre-mariadb-upgrade-scenario2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-postgre-mariadb-upgrade-scenario2</li></ul></div></div></div></div><p>
     Before your proceed, extend the 2-node cluster with additional node that
     has no role assigned to
     it. Make sure that the new node has enough memory to serve as a Control Node.
     </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      In Crowbar Web interface, switch to the <span class="guimenu ">Pacemaker</span> barclamp,
     enable the raw view mode, find the <code class="literal">allow_larger_cluster</code>
     option, and set it value to <code class="literal">true</code>. Note that this
     is relevant only for DRBD clusters.
     </p></li><li class="step "><p>
      Add the <code class="literal">pacemaker-cluster-member</code> role to the new node
      and apply the barclamp.
     </p></li><li class="step "><p>
      Proceed with the migration procedure as described in
     <a class="xref" href="#sec-postgre-mariadb-upgrade-scenario1" title="17.3.2.1. Non-HA Setup or HA Setup with More Than 2 Nodes in the Cluster and PostgreSQL Database Backend">Section 17.3.2.1, “Non-HA Setup or HA Setup with More Than 2 Nodes in the Cluster and
    PostgreSQL Database Backend”</a>.
     </p></li></ol></div></div></div></div><div class="sect2" id="sec-depl-maintenance-upgrade-ui"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrading Using the Web Interface</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-upgrade-ui">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-upgrade-ui</li></ul></div></div></div></div><p>
    The Web interface features a wizard that guides you through the upgrade
    procedure.
   </p><div id="id-1.3.7.2.6.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Canceling Upgrade</h6><p>
     You can cancel the upgrade process by clicking <span class="guimenu ">Cancel
     Upgrade</span>. Note that the upgrade operation can be canceled only
     before the Administration Server upgrade is started. When the upgrade has been
     canceled, the nodes return to the ready state. However any user
     modifications must be undone manually. This includes reverting repository
     configuration.
    </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      To start the upgrade procedure, open the Crowbar Web interface on the Administration Server and choose <span class="guimenu ">Utilities</span> › <span class="guimenu ">Upgrade</span>. Alternatively, point the browser directly to the upgrade
      wizard on the Administration Server, for example
      <code class="literal">http://192.168.124.10/upgrade/</code>.
     </p></li><li class="step "><p>
      On the first screen of the Web interface you will run preliminary checks, get
      information about the upgrade mode and start the upgrade process.
     </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/depl_upgrade7-8_prepare.png" target="_blank"><img src="images/depl_upgrade7-8_prepare.png" width="" /></a></div></div></li><li class="step "><p>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met by clicking <span class="guimenu ">Check</span> in
      <code class="literal">Preliminary Checks</code>.
     </p><p>
      The Web interface displays the progress of the checks. Make sure all checks are
      passed (you should see a green marker next to each check). If errors
      occur, fix them and run the <span class="guimenu ">Check</span> again. Do not
      proceed until all checks are passed.
     </p></li><li class="step "><p>
      When all checks in the previous step have passed, <code class="literal">Upgrade
      Mode</code> shows the result of the upgrade analysis. It will indicate
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </p></li><li class="step "><p>
      To start the upgrade process, click <span class="guimenu ">Begin Upgrade</span>.
     </p></li><li class="step "><p>
      While the upgrade of the Administration Server is prepared, the upgrade wizard
      prompts you to <span class="guimenu ">Download the Backup of the
      Administration Server</span>. When the backup is done, move it to a safe place. If
      something goes wrong during the upgrade procedure of the Administration Server, you
      can restore the original state from this backup using the
      <code class="command">crowbarctl backup restore
      <em class="replaceable ">NAME</em></code> command.
     </p></li><li class="step "><p>
      Check that the repositories required for upgrading the Administration Server are
      available and updated. To do this, click the <span class="guimenu ">Check</span>
      button. If the checks fail, add the software repositories as described in
      <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a> of the Deployment Guide. Run the
      checks again, and click <span class="guimenu ">Next</span>.
     </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/depl_upgrade7-8_repocheck-admin.png" target="_blank"><img src="images/depl_upgrade7-8_repocheck-admin.png" width="" /></a></div></div></li><li class="step "><p>
        Click <span class="guimenu ">Upgrade Administration Server</span> to upgrade and
        reboot the admin node. Note that this operation may take a while. When
        the Administration Server has been updated, click <span class="guimenu ">Next</span>.
      </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/depl_upgrade7-8_admin.png" target="_blank"><img src="images/depl_upgrade7-8_admin.png" width="" /></a></div></div></li><li class="step "><p>
      Check that the repositories required for upgrading all nodes are
      available and updated.  To do this click the <span class="guimenu ">Check</span>
      button. If the check fails, add the software repositories as described in
      <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a> of the Deployment Guide. Run the
      checks again, and click <span class="guimenu ">Next</span>.
     </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/depl_upgrade7-8_repocheck-nodes.png" target="_blank"><img src="images/depl_upgrade7-8_repocheck-nodes.png" width="" /></a></div></div></li><li class="step "><p>
      Stop the <span class="productname">OpenStack</span> services. Before you proceed, be aware that no changes
      can be made to your cloud during and after stopping the services. The
      <span class="productname">OpenStack</span> API will not be available until the upgrade process is
      completed. When you are ready, click <span class="guimenu ">Stop
      Services</span>. Wait until the services are stopped and click
      <span class="guimenu ">Next</span>.
      </p></li><li class="step "><p>
        Before upgrading the nodes, the wizard prompts you to <span class="guimenu ">Back up
        OpenStack Database</span>. The MariaDB database backup will be
        stored on the Administration Server. It can be used to restore the database in case
        something goes wrong during the upgrade. To back up the database, click
        <span class="guimenu ">Create Backup</span>. When the backup operation is
        finished, click <span class="guimenu ">Next</span>.
      </p></li><li class="step "><p>
        Start the upgrade by clicking <span class="guimenu ">Upgrade Nodes</span>. The
        number of nodes determines how long the upgrade process will take. When
        the upgrade is completed, press <span class="guimenu ">Finish</span> to return to
        the Dashboard.
      </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/depl_upgrade7-8_finished.png" target="_blank"><img src="images/depl_upgrade7-8_finished.png" width="" /></a></div></div></li></ol></div></div><div id="id-1.3.7.2.6.7.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    With this first maintenance update, only systems already using MariaDB as
    their OpenStack database will be able to upgrade.  In a future maintenance
    update, there will be a way to migrate from PostgreSQL to MariaDB so
    PostgreSQL users will be able to upgrade.
   </p></div><div id="id-1.3.7.2.6.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Dealing with Errors</h6><p>
     If an error occurs during the upgrade process, the wizard displays a
     message with a description of the error and a possible solution. After
     fixing the error, re-run the step where the error occurred.
    </p></div></div><div class="sect2" id="sec-depl-maintenance-upgrade-cmdl"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrading from the Command Line</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-upgrade-cmdl">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-upgrade-cmdl</li></ul></div></div></div></div><p>
    The upgrade procedure on the command line is performed by using the program
    <code class="command">crowbarctl</code>. For general help, run <code class="command">crowbarctl
    help</code>. To get help on a certain subcommand, run
    <code class="command">crowbarctl <em class="replaceable ">COMMAND</em> help</code>.
   </p><p>
    To review the process of the upgrade procedure, you may call
    <code class="command">crowbarctl upgrade status</code> at any time. Steps may have
    three states: <code class="literal">pending</code>, <code class="literal">running</code>, and
    <code class="literal">passed</code>.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      To start the upgrade procedure from the command line, log in to the
      Administration Server as <code class="systemitem">root</code>.
     </p></li><li class="step "><p>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade prechecks</pre></div><p>
      The command's result is shown in a table. Make sure the column
      <span class="guimenu ">Errors</span> does not contain any entries. If there are
      errors, fix them and restart the <code class="command">precheck</code> command
      afterwards. Do not proceed before all checks are passed.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade prechecks
+-------------------------------+--------+----------+--------+------+
| Check ID                      | Passed | Required | Errors | Help |
+-------------------------------+--------+----------+--------+------+
| network_checks                | true   | true     |        |      |
| cloud_healthy                 | true   | true     |        |      |
| maintenance_updates_installed | true   | true     |        |      |
| compute_status                | true   | false    |        |      |
| ha_configured                 | true   | false    |        |      |
| clusters_healthy              | true   | true     |        |      |
+-------------------------------+--------+----------+--------+------+</pre></div><p>
      Depending on the outcome of the checks, it is automatically decided
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </p><div id="id-1.3.7.2.6.8.4.2.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg" /><h6>Tip: Forcing Normal Mode Upgrade</h6><p>
       The non-disruptive update will take longer than an upgrade in normal
       mode, because it performs certain tasks in parallel which are done
       sequentially during the non-disruptive upgrade. Live-migrating guests to
       other Compute Nodes during the non-disruptive
       upgrade takes additional time.
      </p><p>
       Therefore, if a non-disruptive upgrade is not a requirement for you, you
       may want to switch to the normal upgrade mode, even if your setup
       supports the non-disruptive method. To force the normal upgrade mode,
       run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade mode normal</pre></div><p>
       To query the current upgrade mode run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade mode</pre></div><p>
       To switch back to the non-disruptive mode run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade mode non_disruptive</pre></div><p>
       It is possible to call this command at any time during the upgrade
       process until the <code class="literal">services</code> step is started. After
       that point the upgrade mode can no longer be changed.
      </p></div></li><li class="step "><p>
      Prepare the nodes by transitioning them into the <span class="quote">“<span class="quote ">upgrade</span>”</span>
      state and stopping the chef daemon:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade prepare</pre></div><p>
      Depending of the size of your SUSE <span class="productname">OpenStack</span> Cloud deployment, this step may take
      some time. Use the command <code class="command">crowbarctl upgrade status</code>
      to monitor the status of the process named
      <code class="literal">steps.prepare.status</code>. It needs to be in state
      <code class="literal">passed</code> before you proceed:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade status
+--------------------------------+----------------+
| Status                         | Value          |
+--------------------------------+----------------+
| current_step                   | backup_crowbar |
| current_substep                |                |
| current_substep_status         |                |
| current_nodes                  |                |
| current_node_action            |                |
| remaining_nodes                |                |
| upgraded_nodes                 |                |
| crowbar_backup                 |                |
| openstack_backup               |                |
| suggested_upgrade_mode         | non_disruptive |
| selected_upgrade_mode          |                |
| compute_nodes_postponed        | false          |
| steps.prechecks.status         | passed         |
| steps.prepare.status           | passed         |
| steps.backup_crowbar.status    | pending        |
| steps.repocheck_crowbar.status | pending        |
| steps.admin.status             | pending        |
| steps.repocheck_nodes.status   | pending        |
| steps.services.status          | pending        |
| steps.backup_openstack.status  | pending        |
| steps.nodes.status             | pending        |
+--------------------------------+----------------+</pre></div></li><li class="step "><p>
      Create a backup of the existing Administration Server installation. In case something
      goes wrong during the upgrade procedure of the Administration Server you can restore
      the original state from this backup with the command <code class="command">crowbarctl
      backup restore <em class="replaceable ">NAME</em></code>
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade backup crowbar</pre></div><p>
      To list all existing backups including the one you have just created, run
      the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl backup list
+----------------------------+--------------------------+--------+---------+
| Name                       | Created                  | Size   | Version |
+----------------------------+--------------------------+--------+---------+
| crowbar_upgrade_1534864741 | 2018-08-21T15:19:03.138Z | 219 KB | 4.0     |
+----------------------------+--------------------------+--------+---------+</pre></div></li><li class="step "><p>
      This step prepares the upgrade of the Administration Server by checking the
      availability of the update and pool repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
      <span class="phrase"><span class="phrase">8</span></span> and SUSE Linux Enterprise Server 12 SP3. Run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade repocheck crowbar
+----------------------------------------+-------------------------------------+-----------+
| Repository                             | Status                              | Type      |
+----------------------------------------+-------------------------------------+-----------+
| SLE12-SP3-HA-Pool                      | missing (x86_64), inactive (x86_64) | ha        |
| SLE12-SP3-HA-Updates                   | available                           | ha        |
| SLES12-SP3-Pool                        | available                           | os        |
| SLES12-SP3-Updates                     | available                           | os        |
| SUSE-OpenStack-Cloud-Crowbar-8-Pool    | available                           | openstack |
| SUSE-OpenStack-Cloud-Crowbar-8-Updates | available                           | openstack |
+----------------------------------------+-------------------------------------+-----------+</pre></div><p>
      The output above indicates that the <code class="literal">SLE12-SP3-HA-Pool</code>
      repository is  missing, because it has
      not yet been added to the Crowbar configuration. To add it to the
      Administration Server proceed as follows.
     </p><p>
      Note that this step is for setting up the repositories for the Administration Server,
      not for the nodes in SUSE <span class="productname">OpenStack</span> Cloud (this will be done in a subsequent step).
     </p><ol type="a" class="substeps "><li class="step "><p>
        Start <code class="command">yast repositories</code> and proceed with
        <span class="guimenu ">Continue</span>. Replace the repositories
        <code class="literal">SLES12-SP2-Pool</code> and
        <code class="literal">SLES12-SP2-Updates</code> with the respective SP3
        repositories.
       </p><p>
        If you prefer to use zypper over YaST, you may alternatively make the
        change using <code class="command">zypper mr</code>.
       </p></li><li class="step "><p>
        Next, replace the <code class="literal">SUSE-OpenStack-Cloud-7</code> update and
        pool repositories with the respective <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>
        versions.
       </p></li><li class="step "><p>
        Check for other (custom) repositories. All SLES SP2 repositories need
        to be replaced with the respective SLES SP3 version. In case no SP3
        version exists, disable the repository—the respective packages
        from that repository will be deleted during the upgrade.
       </p></li></ol><p>
      Once the repository configuration on the Administration Server has been updated, run
      the command to check the repositories again. If the configuration is
      correct, the result should look like the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade repocheck crowbar
+---------------------+----------------------------------------+
| Status              | Value                                  |
+---------------------+----------------------------------------+
| os.available        | true                                   |
| os.repos            | SLES12-SP3-Pool                        |
|                     | SLES12-SP3-Updates                     |
| openstack.available | true                                   |
| openstack.repos     | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                     | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
+---------------------+----------------------------------------+</pre></div></li><li class="step "><p>
      Now that the repositories are available, the Administration Server itself will be
      upgraded. The update will run in the background using <code class="command">zypper
      dup</code>. Once all packages have been upgraded, the Administration Server will
      be rebooted and you will be logged out. To start the upgrade run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade admin</pre></div></li><li class="step "><p>
      After the Administration Server has been successfully updated, the Control Nodes and
      Compute Nodes will be upgraded. At first the availability of the
      repositories used to provide packages for the SUSE <span class="productname">OpenStack</span> Cloud nodes is tested.
     </p><div id="id-1.3.7.2.6.8.4.7.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Correct Metadata in the PTF Repository</h6><p>
         When adding new repositories to the nodes, make sure that the new PTF
         repository also contains correct metadata (even if it is empty). To do
         this, run the <code class="command">createrepo-cloud-ptf</code> command.
       </p></div><p>
      Note that the configuration for these repositories differs from the one
      for the Administration Server that was already done in a previous step. In this step
      the repository locations are made available to Crowbar rather than to
      libzypp on the Administration Server. To check the repository configuration run the
      following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade repocheck nodes
+---------------------------------+----------------------------------------+
| Status                          | Value                                  |
+---------------------------------+----------------------------------------+
| ha.available                    | false                                  |
| ha.repos                        | SLES12-SP3-HA-Pool                     |
|                                 | SLES12-SP3-HA-Updates                  |
| ha.errors.x86_64.missing        | SLES12-SP3-HA-Pool                     |
|                                 | SLES12-SP3-HA- Updates                 |
| os.available                    | false                                  |
| os.repos                        | SLES12-SP3-Pool                        |
|                                 | SLES12-SP3-Updates                     |
| os.errors.x86_64.missing        | SLES12-SP3-Pool                        |
|                                 | SLES12-SP3-Updates                     |
| openstack.available             | false                                  |
| openstack.repos                 | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                                 | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
| openstack.errors.x86_64.missing | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                                 | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
+---------------------------------+----------------------------------------+</pre></div><p>
      To update the locations for the listed repositories, start <code class="command">yast
      crowbar</code> and proceed as described in <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu ">Repositories</span>”</a>.
     </p><p>
      Once the repository configuration for Crowbar has been updated, run the
      command to check the repositories again to determine, whether the current
      configuration is correct.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade repocheck nodes
+---------------------+----------------------------------------+
| Status              | Value                                  |
+---------------------+----------------------------------------+
| ha.available        | true                                   |
| ha.repos            | SLE12-SP3-HA-Pool                      |
|                     | SLE12-SP3-HA-Updates                   |
| os.available        | true                                   |
| os.repos            | SLES12-SP3-Pool                        |
|                     | SLES12-SP3-Updates                     |
| openstack.available | true                                   |
| openstack.repos     | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                     | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
+---------------------+----------------------------------------+</pre></div><div id="id-1.3.7.2.6.8.4.7.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Shut Down Running instances in Normal Mode</h6><p>
       If the upgrade is done in normal mode (prechecks compute_status and
       ha_configured have not been passed), you need to shut down all running
       instances now.
      </p></div><div id="id-1.3.7.2.6.8.4.7.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Product Media Repository Copies</h6><p>
       To PXE boot new nodes, an additional SUSE Linux Enterprise Server 12 SP3 repository—a copy
       of the installation system— is required. Although not required
       during the upgrade procedure, it is recommended to set up this directory
       now. Refer to <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a> for
       details. If you had also copied the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 6 installation media
       (optional), you may also want to provide the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
       <span class="phrase"><span class="phrase">8</span></span> the same way.
      </p><p>
       Once the upgrade procedure has been successfully finished, you may
       delete the previous copies of the installation media in
       <code class="filename">/srv/tftpboot/suse-12.2/x86_64/install</code> and
       <code class="filename">/srv/tftpboot/suse-12.2/x86_64/repos/Cloud</code>.
      </p></div></li><li class="step "><p>
      To ensure the status of the nodes does not change during the upgrade
      process, the majority of the <span class="productname">OpenStack</span> services will be stopped on the
      nodes. As a result, the <span class="productname">OpenStack</span> API will no longer be
      accessible. The instances, however, will continue to run and will also
      be accessible. Run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade services</pre></div><p>
      This step takes a while to finish. Monitor the process by running
      <code class="command">crowbarctl upgrade status</code>. Do not proceed before
      <code class="literal">steps.services.status</code> is set to
      <code class="literal">passed</code>.
     </p></li><li class="step "><p>
      The last step before upgrading the nodes is to make a backup of the
      <span class="productname">OpenStack</span> PostgreSQL database. The database dump will be stored on the
      Administration Server and can be used to restore the database in case something goes
      wrong during the upgrade.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade backup openstack</pre></div></li><li class="step "><p>
      The final step of the upgrade procedure is upgrading the
      nodes.  To start the process, enter:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade nodes all</pre></div><p>
      The upgrade process runs in the background and can be queried with
      <code class="command">crowbarctl upgrade status</code>. Depending on the size of
      your SUSE <span class="productname">OpenStack</span> Cloud it may take several hours, especially when performing a
      non-disruptive update. In that case, the Compute Nodes are updated
      one-by-one after instances have been live-migrated to other nodes.
     </p><p>
      Instead of upgrading all nodes you may also upgrade
      the Control Nodes first and individual Compute Nodes afterwards. Refer to
      <code class="command">crowbarctl upgrade nodes --help</code> for details. If you
      choose this approach, you can use the <code class="command">crowbarctl upgrade
      status</code> command to monitor the upgrade process. The output of
      this command contains the following entries:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.7.2.6.8.4.10.5.1"><span class="term ">
            current_node_action
          </span></dt><dd><p>
            The current action applied to the node.
          </p></dd><dt id="id-1.3.7.2.6.8.4.10.5.2"><span class="term ">
            current_substep
          </span></dt><dd><p>
            Shows the current substep of the node upgrade step. For example,
            for the <code class="command">crowbarctl upgrade nodes controllers</code>,
            the <code class="literal">current_substep</code> entry displays the
            <code class="literal">controller_nodes</code> status when upgrading controllers.
          </p></dd></dl></div><p>
       After the controllers have been upgraded, the
       <code class="literal">steps.nodes.status</code> entry in the output displays the
       <code class="literal">running</code> status. Check then the status of the
       <code class="literal">current_substep_status</code> entry. If it displays
       <code class="literal">finished</code>, you can move to the next step of upgrading
       the Compute Nodes.
     </p><p>
      <span class="bold"><strong>Postponing the Upgrade</strong></span>
     </p><p>
      It is possible to stop the upgrade of compute nodes and postpone their
      upgrade with the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade nodes postpone</pre></div><p>
      After the upgrade of compute nodes is postponed, you can go to Crowbar
      Web interface, check the configuration. You can also apply some changes, provided
      they do not affect the Compute Nodes. During the postponed upgrade, all
      <span class="productname">OpenStack</span> services should be up and running. Compute Nodes are still
      running old version of services.
     </p><p>
      To resume the upgrade, issue the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade nodes resume</pre></div><p>
      And finish the upgrade with either <code class="command">crowbarctl upgrade nodes
      all</code> or upgrade nodes one node by one with <code class="command">crowbarctl
      upgrade nodes <em class="replaceable ">NODE_NAME</em></code>.
     </p><p>
       When upgrading individual Compute Nodes using the <code class="command">crowbarctl
       upgrade nodes <em class="replaceable ">NODE_NAME</em></code> command, the
       <code class="literal">current_substep_status</code> entry changes to
       <code class="literal">node_finished</code> when the upgrade of a single node is
       done. After all nodes have been upgraded, the
       <code class="literal">current_substep_status</code> entry displays <code class="literal">finished</code>.
     </p></li></ol></div></div><div id="id-1.3.7.2.6.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Dealing with Errors</h6><p>
     If an error occurs during the upgrade process, the output of the
     <code class="command">crowbarctl upgrade status</code> provides a detailed
     description of the failure. In most cases, both the output and the error
     message offer enough information for fixing the issue. When the problem has
     been solved, run the previously-issued upgrade command to resume the
     upgrade process.
    </p></div></div><div class="sect2" id="sec-depl-maintenance-parallel-upgrade-cmdl"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Simultaneous Upgrade of Multiple Nodes</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-parallel-upgrade-cmdl">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-parallel-upgrade-cmdl</li></ul></div></div></div></div><p>
    It is possible to select more Compute Nodes for selective upgrade instead of
    just one. Upgrading multiple nodes simultaneously significantly reduces the
    time required for the upgrade.
   </p><p>
    To upgrade multiple nodes simultaneously, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade nodes <em class="replaceable ">NODE_NAME_1</em>,<em class="replaceable ">NODE_NAME_2</em>,<em class="replaceable ">NODE_NAME_3</em></pre></div><p>
    Node names can be separated by comma, semicolon, or space. When using
    space as separator, put the part containing node names in quotes.
   </p><p>
    Use the following command to find the names of the nodes that haven't been upgraded:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>crowbarctl upgrade status nodes</pre></div><p>
    Since the simultaneous upgrade is intended to be non-disruptive, all
    Compute Nodes targeted for a simultaneous upgrade must be cleared of any
   running instances.</p><div id="id-1.3.7.2.6.9.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     You can check what instances are running on a specific
    node using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova list --all-tenants --host <em class="replaceable ">NODE_NAME</em></pre></div></div><p>
    This means that it is not possible to pick an arbitrary number of
    Compute Nodes for the simultaneous upgrade operation: you have to make sure
    that it is possible to live-migrate every instance away from the batch of
    nodes that are supposed to be upgraded in parallel. In case of high load
    on all Compute Nodes, it might not be possible to upgrade more than one node
    at a time. Therefore, it is recommended to perform the following steps for
    each node targeted for the simultaneous upgrade prior to running the
    <code class="command">crowbarctl upgrade nodes</code> command.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Disable the Compute Node so it's not used as a target during
      live-evacuation of any other node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack compute service set --disable <em class="replaceable ">"NODE_NAME"</em> nova-compute</pre></div></li><li class="step "><p>
      Evacuate all running instances from the node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova host-evacuate-live <em class="replaceable ">"NODE_NAME"</em></pre></div></li></ol></div></div><p>
    After completing these steps, you can perform a simultaneous upgrade of
    the selected nodes.
   </p></div><div class="sect2" id="sec-depl-maintenance-upgrade-troubleshooting"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Upgrade Issues</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-upgrade-troubleshooting">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-upgrade-troubleshooting</li></ul></div></div></div></div><div class="qandaset" id="id-1.3.7.2.6.10.2"><div class="free-id" id="id-1.3.7.2.6.10.2.1"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.1.1"><strong>Q: 1.</strong>
        Upgrade of the admin server has failed.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.1.2"><p>
        Check for empty, broken, and not signed repositories in the Administration Server
        upgrade log file <code class="filename">/var/log/crowbar/admin-server-upgrade.log</code>. Fix the
        repository setup. Upgrade then remaining packages manually to SUSE Linux Enterprise Server 12 SP3
        and SUSE <span class="productname">OpenStack</span> Cloud <span class="phrase"><span class="phrase">8</span></span> using the command <code class="command">zypper dup</code>. Reboot the Administration Server.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.2"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.2.1"><strong>Q: 2.</strong>
        An upgrade step repeatedly fails due to timeout.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.2.2"><p>
        Timeouts for most upgrade operations can be adjusted in the
        <code class="filename">/etc/crowbar/upgrade_timeouts.yml</code> file. If the
        file doesn't exist, use the following template, and modify it to your needs:
       </p><div class="verbatim-wrap"><pre class="screen">        :prepare_repositories: 120
        :pre_upgrade: 300
        :upgrade_os: 1500
        :post_upgrade: 600
        :shutdown_services: 600
        :shutdown_remaining_services: 600
        :evacuate_host: 300
        :chef_upgraded: 1200
        :router_migration: 600
        :lbaas_evacuation: 600
        :set_network_agents_state: 300
        :delete_pacemaker_resources: 600
        :delete_cinder_services: 300
        :delete_nova_services: 300
        :wait_until_compute_started: 60
        :reload_nova_services: 120
        :online_migrations: 1800</pre></div><p>
        The following entries may require higher values (all values are
        specified in seconds):
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           <code class="literal">upgrade_os</code> Time allowed for upgrading all packages of one node.
          </p></li><li class="listitem "><p>
           <code class="literal">chef_upgraded</code> Time allowed for initial
           <code class="literal">crowbar_join</code> and <code class="literal">chef-client</code>
           run on a node that has been upgraded and rebooted.
          </p></li><li class="listitem "><p>
           <code class="literal">evacuate_host</code> Time allowed for live migrate all VMs from a host.
          </p></li></ul></div></dd></dl><div class="free-id" id="live-migration-failed"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.3.1"><strong>Q: 3.</strong>
        Node upgrade has failed during live migration.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.3.2"><p>
        The problem may occur when it is not possible to live migrate certain
        VMs anywhere. It may be necessary to shut down or suspend other VMs to
        make room for migration. Note that the Bash shell script that starts
        the live migration for the Compute Node is executed from the
        Control Node. An error message generated by the <code class="command">crowbarctl
        upgrade status</code> command contains the exact names of both
        nodes. Check the <code class="filename">/var/log/crowbar/node-upgrade.log</code>
        file on the Control Node for the information that can help you with
        troubleshooting. You might also need to check OpenStack logs in
        <code class="filename">/var/log/nova</code> on the Compute Node as well as on the
        Control Nodes.
       </p><p>
        It is possible that live-migration of a certain VM takes too long. This
        can happen if instances are very large or network connection between
        compute hosts is slow or overloaded. If this case, try to raise the
        global timeout in
        <code class="filename">/etc/crowbar/upgrade_timeouts.yml</code>.
       </p><p>
        We recommend to perform the live migration manually first. After it is
        completed successfully, call the <code class="command">crowbarctl upgrade</code>
        command again.
       </p><p>
        The following commands can be helpful for analyzing issues with live migrations:
       </p><div class="verbatim-wrap"><pre class="screen">        nova server-migration-list
        nova server-migration-show
        nova instance-action-list
        nova instance-action</pre></div><p>
        Note that these commands require OpenStack administrator privileges.
       </p><p>
        The following log files may contain useful information:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="filename">/var/log/nova/nova-compute</code> on the Compute Nodes
          that the migration is performed from and to.
         </p></li><li class="listitem "><p>
          <code class="filename">/var/log/nova/*.log</code> (especially log files for the
          conductor, scheduler and placement services) on the Control Nodes.
         </p></li></ul></div><p>
        It can happen that active instances and instances with heavy
        loads cannot be live migrated in a reasonable time. In that case, you
        can abort a running live-migration operation using the <code class="command">nova
        live-migration-abort <em class="replaceable ">MIGRATION-ID</em></code>
        command. You can then perform the upgrade of the specific node at a
        later time.
       </p><p>
        Alternatively, it is possible to force the completion of
        the live migration by using the <code class="command">nova
        live-migration-force-complete
        <em class="replaceable ">MIGRATION-ID</em></code> command. However,
        this might pause the instances for a prolonged period of time and have
        a negative impact on the workload running inside the instance.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.4"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.4.1"><strong>Q: 4.</strong>
        Node has failed during OS upgrade.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.4.2"><p>
        Possible reasons include an incorrect repository setup or package
        conflicts. Check the <code class="filename">/var/log/crowbar/node-upgrade.log</code> log file on the
        affected node. Check the repositories on node using the <code class="command">zypper
        lr</code> command. Make sure the required repositories are
        available. To test the setup, install a package manually or run the
        <code class="command">zypper dup</code> command (this command is executed by the
        upgrade script). Fix the repository setup and run the failed upgrade
        step again. If custom package versions or version locks are in place,
        make sure that they don't interfere with the <code class="command">zypper dup</code> command.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.5"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.5.1"><strong>Q: 5.</strong>
        Node does not come up after reboot.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.5.2"><p>
        In some cases, a node can take too long to reboot causing a timeout. We
        recommend to check the node manually, make sure it is online, and repeat the step.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.6"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.6.1"><strong>Q: 6.</strong>
        N number of nodes were provided to compute upgrade using
        <code class="command">crowbarctl upgrade nodes node_1,node_2,...,node_N</code>,
        but less then N were actually upgraded.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.6.2"><p>
        If the live migration cannot be performed for certain nodes due to a timeout,
        Crowbar upgrades only the nodes that it was able to
        live-evacuate in the specified time. Because some nodes have been upgraded, it is possible that
        more resources will be available for live-migration when you try to run this
        step again. See also <a class="xref" href="#live-migration-failed" title="Q: 3."><em>
        Node upgrade has failed during live migration.
       </em></a>.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.7"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.7.1"><strong>Q: 7.</strong>
        Node has failed at the initial chef client run stage.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.7.2"><p>
        An unsupported entry in the configuration file may prevent a service
        from starting. This causes the node to fail at the initial
        chef client run stage. Checking the
        <code class="filename">/var/log/crowbar/crowbar_join/chef.*</code> log files on
        the node is a good starting point.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.8"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.8.1"><strong>Q: 8.</strong>
        I need to change OpenStack configuration during the upgrade but I cannot access Crowbar.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.8.2"><p>
        Crowbar Web interface is accessible only when an upgrade is completed or
        when it is postponed. Postponing the upgrade can be done only after
        upgrading all Control Nodes using the <code class="command">crowbarctl upgrade nodes
        postpone</code> command. You can then access Crowbar and
        save your modifications. Before you can continue with the upgrade of
        rest of the nodes, resume the upgrade using the <code class="command">crowbarctl
        upgrade nodes resume</code> command.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.9"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.9.1"><strong>Q: 9.</strong>
        Failure occurred when evacuating routers.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.9.2"><p>
        Check the <code class="filename">/var/log/crowbar/node-upgrade.log</code> file on
        the node that performs the router evacuation (it should be mentioned in
        the error message). The ID of the router that failed to migrate (or the
        affected network port) is logged to
        <code class="filename">/var/log/crowbar/node-upgrade.log</code>. Use the
        OpenStack CLI tools to check the state of the affected router and
        its ports. Fix manually, if necessary. This can be done by bringing the
        router or port up and down again. The following
        commands can be useful for solving the issue:
       </p><div class="verbatim-wrap"><pre class="screen">        openstack router show <em class="replaceable ">ID</em>
        openstack port list --router <em class="replaceable ">ROUTER-ID</em>
        openstack port show <em class="replaceable ">PORT-ID</em>
        openstack port set</pre></div><p>
         Resume the upgrade by running the failed upgrade step
        again to continue with the router migration.
       </p></dd></dl><div class="free-id" id="id-1.3.7.2.6.10.2.10"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.2.6.10.2.10.1"><strong>Q: 10.</strong>
        Some non-controller nodes were upgraded after performing <code class="command">crowbarctl upgrade nodes
        controllers</code>.
       </dt><dd class="answer" id="id-1.3.7.2.6.10.2.10.2"><p>
        In the current upgrade implementation, OpenStack nodes are divided
        into Compute Nodes and other nodes. The <code class="command">crowbarctl upgrade nodes
        controllers</code> command starts the upgrade of all the nodes that
        do not host compute services. This includes the controllers.
       </p></dd></dl></div></div></div><div class="sect1" id="sec-depl-maintenance-recover-compute-node-failure"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering from Compute Node Failure</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-recover-compute-node-failure">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-recover-compute-node-failure</li></ul></div></div></div></div><p>
   The following procedure assumes that there is at least one Compute Node
   already running. Otherwise, see
   <a class="xref" href="#sec-depl-maintenance-bootstrap-compute-plane" title="17.5. Bootstrapping Compute Plane">Section 17.5, “Bootstrapping Compute Plane”</a>.
  </p><div class="procedure " id="pro-recover-compute-node-failure"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 17.2: </span><span class="name">Procedure for Recovering from Compute Node Failure </span><a title="Permalink" class="permalink" href="#pro-recover-compute-node-failure">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="st-compnode-failed-reason"><p>
     If the Compute Node failed, it should have been fenced. Verify that this is
     the case. Otherwise, check <code class="filename">/var/log/pacemaker.log</code> on
     the Designated Coordinator to determine why the Compute Node was not fenced.
     The most likely reason is a problem with STONITH devices.
    </p></li><li class="step "><p>
     Determine the cause of the Compute Node's failure.
    </p></li><li class="step "><p>
     Rectify the root cause.
    </p></li><li class="step "><p>
     Boot the Compute Node again.
    </p></li><li class="step "><p>
     Check whether the <code class="systemitem">crowbar_join</code> script ran
     successfully on the Compute Node. If this is not the case, check the log
     files to find out the reason. Refer to
     <a class="xref" href="#sec-deploy-logs-crownodes" title="19.2. On All Other Crowbar Nodes">Section 19.2, “On All Other Crowbar Nodes”</a> to find the exact
     location of the log file.
    </p></li><li class="step "><p>
     If the <code class="systemitem">chef-client</code> agent triggered by
     <code class="systemitem">crowbar_join</code> succeeded, confirm that the
     <code class="systemitem">pacemaker_remote</code> service is up and running.
    </p></li><li class="step "><p>
     Check whether the remote node is registered and considered healthy by the
     core cluster. If this is not the case check
     <code class="filename">/var/log/pacemaker.log</code> on the Designated Coordinator
     to determine the cause. There should be a remote primitive running on the
     core cluster (active/passive). This primitive is responsible for
     establishing a TCP connection to the
     <code class="systemitem">pacemaker_remote</code> service on port 3121 of the
     Compute Node. Ensure that nothing is preventing this particular TCP
     connection from being established (for example, problems with NICs,
     switches, firewalls etc.). One way to do this is to run the following
     commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>lsof -i tcp:3121
<code class="prompt user">tux &gt; </code>tcpdump tcp port 3121</pre></div></li><li class="step "><p>
     If Pacemaker can communicate with the remote node, it should start the
     <code class="systemitem">nova-compute</code> service on it as part of the cloned
     group <code class="literal">cl-g-nova-compute</code> using the NovaCompute OCF
     resource agent. This cloned group will block startup of
     <code class="systemitem">nova-evacuate</code> until at least one clone is
     started.
    </p><p>
     A necessary, related but different procedure is described in
     <a class="xref" href="#sec-depl-maintenance-bootstrap-compute-plane" title="17.5. Bootstrapping Compute Plane">Section 17.5, “Bootstrapping Compute Plane”</a>.
    </p></li><li class="step "><p>
     It may happen that <code class="systemitem">NovaCompute</code> has been launched
     correctly on the Compute Node by <code class="systemitem">lrmd</code>, but the
     <code class="systemitem">openstack-nova-compute</code> service is still not
     running. This usually happens when <code class="systemitem">nova-evacuate</code>
     did not run correctly.
    </p><p>
     If <code class="systemitem">nova-evacuate</code> is not
     running on one of the core cluster nodes, make sure that the service is
     marked as started (<code class="literal">target-role="Started"</code>). If this is
     the case, then your cloud does not have any Compute Nodes already running as
     assumed by this procedure.
    </p><p>
     If <code class="systemitem">nova-evacuate</code> is started but it is
     failing, check the Pacemaker logs to determine the cause.
    </p><p>
     If <code class="systemitem">nova-evacuate</code> is started and
     functioning correctly, it should call Nova's
     <code class="literal">evacuate</code> API to release resources used by the
     Compute Node and resurrect elsewhere any VMs that died when it failed.
    </p></li><li class="step "><p>
     If <code class="systemitem">openstack-nova-compute</code> is running, but VMs are
     not booted on the node, check that the service is not disabled or
     forced down using the <code class="command">nova service-list</code> command. In
     case the service is disabled, run the <code class="command">nova service-enable
     <em class="replaceable ">SERVICE_ID</em></code> command. If the service is
     forced down, run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>fence_nova_param () {
    key="$1"
    cibadmin -Q -A "//primitive[@id="fence-nova"]//nvpair[@name='$key']" | \
    sed -n '/.*value="/{s///;s/".*//;p}'
}
<code class="prompt user">tux &gt; </code>fence_compute \
    --auth-url=`fence_nova_param auth-url` \
    --endpoint-type=`fence_nova_param endpoint-type` \
    --tenant-name=`fence_nova_param tenant-name` \
    --domain=`fence_nova_param domain` \
    --username=`fence_nova_param login` \
    --password=`fence_nova_param passwd` \
    -n <em class="replaceable ">COMPUTE_HOSTNAME</em> \
    --action=on</pre></div></li></ol></div></div><p>
   The above steps should be performed automatically after the node is
   booted. If that does not happen, try the following debugging techniques.
  </p><p>
   Check the <code class="literal">evacuate</code> attribute for the Compute Node in the
   Pacemaker cluster's <code class="systemitem">attrd</code> service using the
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>attrd_updater -p -n evacuate -N <em class="replaceable ">NODE</em></pre></div><p>
   Possible results are the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The attribute is not set. Refer to
     <a class="xref" href="#st-compnode-failed-reason" title="Step 1">Step 1</a> in
     <a class="xref" href="#pro-recover-compute-node-failure" title="Procedure for Recovering from Compute Node Failure">Procedure 17.2, “Procedure for Recovering from Compute Node Failure”</a>.
    </p></li><li class="listitem "><p>
     The attribute is set to <code class="literal">yes</code>. This means that the
     Compute Node was fenced, but <code class="systemitem">nova-evacuate</code> never
     initiated the recovery procedure by calling Nova's evacuate API.
    </p></li><li class="listitem "><p>
     The attribute contains a time stamp, in which case the recovery procedure
     was initiated at the time indicated by the time stamp, but has not
     completed yet.
    </p></li><li class="listitem "><p>
     If the attribute is set to <code class="literal">no</code>, the recovery procedure
     recovered successfully and the cloud is ready for the Compute Node to
     rejoin.
    </p></li></ul></div><p>
   If the attribute is stuck with the wrong value, it can be set to
   <code class="literal">no</code> using the command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>attrd_updater -n evacuate -U no -N <em class="replaceable ">NODE</em></pre></div><p>
   After standard fencing has been performed, fence agent
   <code class="systemitem">fence_compute</code> should activate the secondary
   fencing device (<code class="literal">fence-nova</code>). It does this by setting
   the attribute to <code class="literal">yes</code> to mark the node as needing
   recovery. The agent also calls Nova's
   <code class="systemitem">force_down</code> API to notify it that the host is down.
   You should be able to see this in
   <code class="filename">/var/log/nova/fence_compute.log</code> on the node in the core
   cluster that was running the <code class="systemitem">fence-nova</code> agent at
   the time of fencing. During the recovery, <code class="literal">fence_compute</code>
   tells Nova that the host is up and running again.
  </p></div><div class="sect1" id="sec-depl-maintenance-bootstrap-compute-plane"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bootstrapping Compute Plane</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-bootstrap-compute-plane">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-bootstrap-compute-plane</li></ul></div></div></div></div><p>
   If the whole compute plane is down, it is not always obvious how to boot it
   up, because it can be subject to deadlock if evacuate attributes are set on
   every Compute Node. In this case, manual intervention is
   required. Specifically, the operator must manually choose one or more
   Compute Nodes to bootstrap the compute plane, and then run the
   <code class="command">attrd_updater -n evacuate -U no -N <em class="replaceable ">NODE</em></code>
   command for each
   of those Compute Nodes to indicate that they do not require the resurrection
   process and can have their <code class="literal">nova-compute</code> start up straight
   away. Once these Compute Nodes are up, this breaks the deadlock allowing
   <code class="literal">nova-evacuate</code> to start. This way, any other nodes that
   require resurrection can be processed automatically. If no resurrection is
   desired anywhere in the cloud, then the attributes should be set to
   <code class="literal">no</code> for all nodes.
  </p><div id="id-1.3.7.2.8.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If Compute Nodes are started too long after the
    <code class="literal">remote-*</code> resources are started on the control plane,
    they are liable to fencing. This should be avoided.
   </p></div></div><div class="sect1" id="id-1.3.7.2.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating MariaDB with Galera</span> <a title="Permalink" class="permalink" href="#id-1.3.7.2.9">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Updating MariaDB with Galera must be done manually. Crowbar does not
   install updates automatically. Updates can be done with Pacemaker or with
   the CLI. In particular, manual updating applies to upgrades to
   MariaDB 10.2.17 or higher from MariaDB 10.2.16 or earlier. See <a class="link" href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/" target="_blank">MariaDB
   10.2.22 Release Notes - Notable Changes</a>.
  </p><div id="id-1.3.7.2.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    In order to run the following update steps, the database cluster needs to
    be up and healthy.
   </p></div><p>
   Using the Pacemaker GUI, update MariaDB with the following procedure:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Put the cluster into maintenance mode. Detailed information about the
     Pacemaker GUI and its operation is available in the <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2</a>.
    </p></li><li class="step "><p>
     Perform a rolling upgrade to MariaDB following the instructions at <a class="link" href="https://mariadb.com/kb/en/library/upgrading-between-minor-versions-with-galera-cluster/" target="_blank">Upgrading
     Between Minor Versions with Galera Cluster</a>.
    </p><p>
     The process involves the following steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Stop MariaDB
      </p></li><li class="step "><p>
       Uninstall the old versions of MariaDB and the Galera wsrep provider
      </p></li><li class="step "><p>
       Install the new versions MariaDB and the Galera wsrep provider
      </p></li><li class="step "><p>
       Change configuration options if necessary
      </p></li><li class="step "><p>
       Start MariaDB
      </p></li><li class="step "><p>
       Run <code class="command">mysql_upgrade</code> with the
       <code class="literal">--skip-write-binlog</code> option
      </p></li></ol></li><li class="step "><p>
       Each node must upgraded individually so that the cluster is always
       operational.
    </p></li><li class="step "><p>
     Using the Pacemaker GUI, take the cluster out of maintenance mode.
    </p></li></ol></div></div><p>
   When updating with the CLI, the database cluster must be up and
   healthy. Update MariaDB with the following procedure:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Mark Galera as unmanaged:
    </p><div class="verbatim-wrap"><pre class="screen">crm resource unmanage galera</pre></div><p>
     Or put the whole cluster into maintenance mode:
    </p><div class="verbatim-wrap"><pre class="screen">crm configure property maintenance-mode=true</pre></div></li><li class="step "><p>
     Pick a node other than the one currently targeted by the loadbalancer and
     stop MariaDB on that node:
    </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-demote -r galera -V</pre></div></li><li class="step "><p>
     Perform updates with the following steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Uninstall the old versions of MariaDB and the Galera wsrep provider.
      </p></li><li class="step "><p>
       Install the new versions of MariaDB and the Galera wsrep
       provider. Select the appropriate instructions at <a class="link" href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/" target="_blank">Installing
       MariaDB with zypper</a>.
      </p></li><li class="step "><p>
       Change configuration options if necessary.
      </p></li></ol></li><li class="step "><p>
       Start MariaDB on the node.
      </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-promote -r galera -V</pre></div></li><li class="step "><p>
       Run <code class="command">mysql_upgrade</code> with the
       <code class="literal">--skip-write-binlog</code> option.
      </p></li><li class="step "><p>
       On the other nodes, repeat the process detailed above: stop MariaDB,
       perform updates, start MariaDB, run <code class="command">mysql_upgrade</code>.
      </p></li><li class="step "><p>
       Mark Galera as managed:
      </p><div class="verbatim-wrap"><pre class="screen">crm resource manage galera</pre></div><p>
       Or take the cluster out of maintenance mode.
      </p></li></ol></div></div></div><div class="sect1" id="database-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Periodic OpenStack Maintenance Tasks</span> <a title="Permalink" class="permalink" href="#database-maintenance">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-database_maintenance.xml</li><li><span class="ds-label">ID: </span>database-maintenance</li></ul></div></div></div></div><p>
    Heat-manage helps manage Heat specific database operations. The associated
    database should be periodically purged to save space. The following should
    be setup as a cron job on the servers where the heat service is running at
    <code class="literal">/etc/cron.weekly/local-cleanup-heat</code>
    with the following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su heat -s /bin/bash -c "/usr/bin/heat-manage purge_deleted -g days 14" || :</pre></div><p>
     nova-manage db archive_deleted_rows command will move deleted rows
     from production tables to shadow tables. Including
     <code class="literal">--until-complete</code> will make the command run continuously
     until all deleted rows are archived. It is recommended to setup this task
     as <code class="literal">/etc/cron.weekly/local-cleanup-nova</code>
     on the servers where the nova service is running, with the
     following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su nova -s /bin/bash -c "/usr/bin/nova-manage db archive_deleted_rows --until-complete" || :</pre></div></div><div class="sect1" id="sec-depl-maintenance-fernet-tokens"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rotating Fernet Tokens</span> <a title="Permalink" class="permalink" href="#sec-depl-maintenance-fernet-tokens">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_maintenance.xml</li><li><span class="ds-label">ID: </span>sec-depl-maintenance-fernet-tokens</li></ul></div></div></div></div><p>
     Fernet tokens should be rotated frequently for security purposes.
     It is recommended to setup this task as a cron job in
     <code class="literal">/etc/cron.weekly/openstack-keystone-fernet</code>
     on the keystone server designated as a master node in a highly
     available setup with the following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su keystone -s /bin/bash -c "keystone-manage fernet_rotate"

  /usr/bin/keystone-fernet-keys-push.sh 192.168.81.168; /usr/bin/keystone-fernet-keys-push.sh 192.168.81.169;</pre></div><p>
     The IP addresses in the above example, i.e. 192.168.81.168 and
     192.168.81.169 are the IP addresses of the other two nodes of a
     three-node cluster. Be sure to use the correct IP addresses
     when configuring the cron job. Note that if the master node is offline
     and a new master is elected, the cron job will need to be removed from
     the previous master node and then re-created on the new master node.
     Do not run the fernet_rotate cron job on multiple nodes.
  </p><p>
     For a non-HA setup, the cron job should be configured at
    <code class="literal">/etc/cron.weekly/openstack-keystone-fernet</code>
     on the keystone server as follows:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su keystone -s /bin/bash -c "keystone-manage fernet_rotate"</pre></div></div></div><div class="chapter " id="self-assign-certs"><div class="titlepage"><div><div><h2 class="title"><span class="number">18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate SUSE <span class="productname">OpenStack</span> Cloud Self Signed Certificate</span> <a title="Permalink" class="permalink" href="#self-assign-certs">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ssl_self_cert.xml</li><li><span class="ds-label">ID: </span>self-assign-certs</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#create-root-pair"><span class="number">18.1 </span><span class="name">Create the CA Root Pair</span></a></span></dt><dt><span class="section"><a href="#sign-server-client-cert"><span class="number">18.2 </span><span class="name">Sign server and client certificates</span></a></span></dt><dt><span class="section"><a href="#deploy-cert"><span class="number">18.3 </span><span class="name">Deploying the certificate</span></a></span></dt><dt><span class="section"><a href="#lets-encrypt-cert"><span class="number">18.4 </span><span class="name">Generate Public Certificate using Let’s Encrypt</span></a></span></dt></dl></div></div><p>
   The purpose of this document is to help set up SSL Support for several services
   in SUSE OpenStack Cloud. The scope of this document covers all public
   endpoints in your OpenStack cluster. In most cases you want to have a
   Secure CA or External CA where your certificates are signed. You will
   sign with either a public CA or self signed CA, and include x509
   extensions for Subject Alt Names since there might be a highly available
   control plane with alternate names.
 </p><div class="sect1" id="create-root-pair"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create the CA Root Pair</span> <a title="Permalink" class="permalink" href="#create-root-pair">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ssl_self_cert.xml</li><li><span class="ds-label">ID: </span>create-root-pair</li></ul></div></div></div></div><p>This section demonstrates how to create the certificate on the
     crowbar or admin node of the SUSE OpenStack Cloud Cluster.</p><div id="id-1.3.7.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         To avoid external access to your CA Root Pair, put it on an air-gapped system
         that is permanently isolated from the internet and unplug any cables from the
         ethernet port.
       </p></div><div class="procedure " id="id-1.3.7.3.3.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.1: </span><span class="name">Prepare the directory structure </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.3.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
           Create a directory for your CA Root pair:
         </p><div class="verbatim-wrap"><pre class="screen">           # ssh root@crowbar
           # mkdir -p ~/ssl/root/ca</pre></div></li><li class="step "><p>
           Create a directory structure and add <code class="filename">index.txt</code>
           and serial files to act as flat database of all signed certificates:
         </p><div class="verbatim-wrap"><pre class="screen">           # cd ~/ssl/root/ca
           # mkdir certs crl newcerts private csr
           # chmod 700 private
           # touch index.txt
           # echo 1000 &gt; serial</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.7.3.3.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.2: </span><span class="name">Prepare the configuration file </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.3.5">#</a></h6></div><div class="procedure-contents"><p>
         This procedure takes you through the full set up. Note that
         when you setup the crowbar server, there is a structure already setup
         under <code class="filename">/etc/ssl</code>. This is where SUSE Linux typically
         contains the CA cert bundle created through YaST when the SMT server
         is set up. However, if you are using an external SMT server
         you will not have this.
       </p><ol class="procedure" type="1"><li class="step "><p>
           Copy <code class="filename">/etc/ssl/openssl.cnf</code> file to your setup.
           We can use this since it is completely annotated.
         </p><div class="verbatim-wrap"><pre class="screen">           # cp /etc/ssl/openssl.cnf ./</pre></div></li><li class="step "><p>
           Edit the file and change the location variable:
         </p><div class="verbatim-wrap"><pre class="screen">           dir = /root/ssl/root/ca
           # Where everything is kept</pre></div><div id="id-1.3.7.3.3.5.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
             Make sure <code class="literal">dir</code> is the directory where we created
             <code class="filename">/root/ssl/root/ca</code>.
           </p></div></li></ol></div></div><div class="procedure " id="id-1.3.7.3.3.6"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.3: </span><span class="name">Create the root key </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.3.6">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Create the root key encrypted with AES 256-bit encryption
          and a password, using 4096 bits for the creation.
        </p><div class="verbatim-wrap"><pre class="screen">          # cd ~/ssl/root/ca
          # openssl genrsa -aes256 -out private/cakey.pem 4096</pre></div></li><li class="step "><p>
          You will be asked to enter a password here and then verify it.
        </p><div class="verbatim-wrap"><pre class="screen">          # chmod 400 private/cakey.pem</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.7.3.3.7"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.4: </span><span class="name">Create the root certificates </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.3.7">#</a></h6></div><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
           Use the root key (<code class="filename">cakey.pem</code>) to create the
           root certificate (cacert.pem). Its important to give it a long
           expiration since all the certificates signed from it will
           become invalid when it expires.
         </p><div class="verbatim-wrap"><pre class="screen">           # cd ~/ssl/root/ca
           # openssl req -config openssl.cnf -key private/cakey.pem -new -x509 -days 10950 -sha256 -extensions v3_ca -out cacert.pem
           Enter pass phrase for cakey.pem: enteryourpassword
           You are about to be asked to enter information that will be incorporated
           into your certificate request.
           -----
           Country Name (2 letter code) [AU]:US
           State or Province Name []:Idaho
           Locality Name []:Meridian
           Organization Name []:SUSEDojo
           Organizational Unit Name []:dojo
           Common Name []:susedojo.com
           Email Address []:admin@susedojo.com

           # chmod 444 cacert.pem</pre></div></li></ul></div></div><div class="procedure " id="id-1.3.7.3.3.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.5: </span><span class="name">Verify the root certificates </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.3.8">#</a></h6></div><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
          Verify the certificate has the correct dates of validity and the
          algorithm used, Issuer, Subject, and x509v3 extensions. The issuer
          and subject are the same since it is self signed.
        </p><div class="verbatim-wrap"><pre class="screen">          # cd ~/ssl/root/ca
          # openssl x509 -noout -text -in cacert.pem</pre></div></li></ul></div></div></div><div class="sect1" id="sign-server-client-cert"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sign server and client certificates</span> <a title="Permalink" class="permalink" href="#sign-server-client-cert">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ssl_self_cert.xml</li><li><span class="ds-label">ID: </span>sign-server-client-cert</li></ul></div></div></div></div><p>
      This section is if you are the perspective certificate authority (CA).
    </p><div class="procedure " id="id-1.3.7.3.4.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.6: </span><span class="name">Prepare config file </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.4.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Modify the <code class="filename">penssl.cnf</code> config file and add a
          line to the <code class="literal">[ v3_req ]</code> section:
        </p><div class="verbatim-wrap"><pre class="screen">          # cd ~/ssl/root/ca
          # vi openssl.cnf
          find v3_req
          Add the following line:
          subjectAltName = DNS:public.your_server_name.your_domain.com, DNS: cluster-control.your_domain.com
          At the bottom of the file create section server_cert with the follwing:
          [ server_cert ]
          subjectAltName = subjectAltName = DNS:public.your_server_name.your_domain.com, DNS: cluster-control.your_domain.com</pre></div></li><li class="step "><p>
          The first DNS name would be used if you only have a single node
          controller as you need the public URL for that server in your cluster.
          For example, <code class="literal">public.db8-ae-ed-77-14-9e.susedojo.com</code>.

          If you have a haproxy setup for your cluster or pacemaker, you have a
          cluster URL. For example, you may have
          <code class="literal">public.cluster.your_domain.com</code> and you need to
          have <code class="literal">cluster.your_domain.com</code> and <code class="literal">public.cluster.your_domain.com</code>
          as Alternative DNS names. This public URL can be used for all
          endpoints unless you have multiple High Availability Clusters for
          your control plane.
        </p></li><li class="step "><p>
          Save and close the file after you have those entered correctly.
        </p></li></ol></div></div><div class="procedure " id="id-1.3.7.3.4.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.7: </span><span class="name">Create a key </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.4.4">#</a></h6></div><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
           Create a key minus the <code class="literal">-aes256</code> option so that
           you are not presented with a password each time you restart a
           service. (i.e. Apache service) also in 2048 bit so it is quicker to decrypt.
         </p><div class="verbatim-wrap"><pre class="screen">           # cd ~/ssl/root/ca
           # openssl genrsa -out private/susedojo-com.key.pem 2048
           # chmod 400 private/susedojo-com.key.pem</pre></div></li></ul></div></div><div class="procedure " id="id-1.3.7.3.4.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.8: </span><span class="name">Create a certificate </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.4.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
            Use the private key we just created to create a certificate
            signing request (CSR). The common name must be a fully qualified
            domain name (i.e. www.susedojo.com) The Organization Name must be
            the same as the Organization Name in the CA.
          </p><div class="verbatim-wrap"><pre class="screen">            # cd ~/ssl/root/ca
            # openssl req -config openssl.cnf -key private/susedojo-com.key.pem -new -sha256 -out csr/susedojo-com.csr.pem
            You are about to be asked to enter information that will be incorporated
            into your certificate request.
            -----
            Country Name (2 letter code) [XX]:US
            State or Province Name []:Idaho
            Locality Name []:Meridian
            Organization Name []:SUSEDojo
            Organizational Unit Name []:dojo
            Common Name []:susedojo.com
            Email Address []:admin@susedojo.com</pre></div><div id="id-1.3.7.3.4.5.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
              You may be prompted for a challenge password and company name.
              This can be left blank.
            </p></div></li><li class="step "><p>
            Create the certificate using the CA to sign the CSR, using the
            <code class="literal">server_cert</code> extension as this will be used
            on a server. We will give it one year of validity.
          </p><div class="verbatim-wrap"><pre class="screen">            # cd ~/ssl/root/ca
            # openssl ca -config openssl.cnf -extensions server_cert -days 365 -notext -md sha256 -in  csr/susedojo-com.csr.pem -out certs/susedojo-com.cert.pem
              Using configuration from openssl.cnf
              Enter pass phrase for /root/ssl/root/ca/private/cakey.pem:
              Check that the request matches the signature
              Signature ok
                      Serial Number: 4096 (0x1000)
                      Validity
                        Not Before: Aug  8 04:21:08 2018 GMT
            	          Not After: Aug  8 04:21:08 2019 GMT
                     Subject:
                          countryName               = US
                          stateOrProvinceName       = Idaho
                          organizationName          = SUSEDojo
                          organizationalUnitName    = dojo
                          commonName                = susedojo.com
                          emailAddress              = admin@susedojo.com
                     X509v3 extensions:
                         X509v3 Basic Constraints:
                            CA:FALSE
                        X509v3 Key Usage:
                              Digital Signature, Non Repudiation, Key Encipherment
                         X509v3 Subject Alternative Name:
                             DNS:public.db8-ae-ed-77-14-9e.susedojo.com
            Certificate is to be certified until Aug  8 04:21:08 2019 GMT (365 days)
            Sign the certificate? [y/n]:y

            1 out of 1 certificate requests certified, commit? [y/n]y
            Write out database with 1 new entries
            Data Base Updated

            # chmod 444 certs/susedojo-com.cert.pem</pre></div></li><li class="step "><p>
            The <code class="filename">index.txt</code> file should now contain a line
            referring to the new certificate that has been created.
            For example, the output should look like the following:
          </p><div class="verbatim-wrap"><pre class="screen">            V       190808042108Z           1000    unknown
            /C=US/ST=Idaho/O=SUSEDojo/OU=dojo/CN=susedojo.com/emailAddress=admin@susedojo.com</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.7.3.4.6"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.9: </span><span class="name">Verifying the certificate </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.4.6">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
             Enter the following in your terminal:
           </p><div class="verbatim-wrap"><pre class="screen">             # openssl x509 -noout -text -in certs/susedojo-com.cert.pem</pre></div></li><li class="step "><p>
             You will notice the Issuer is the CA and you can also see the
             Subject Alternative Name as well in the extensions section.
           </p><div class="verbatim-wrap"><pre class="screen">             # openssl verify -CAfile cacert.pem certs/susedojo-com.cert.pem
             certs/susedojo-com.cert.pem: OK</pre></div></li></ol></div></div></div><div class="sect1" id="deploy-cert"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the certificate</span> <a title="Permalink" class="permalink" href="#deploy-cert">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ssl_self_cert.xml</li><li><span class="ds-label">ID: </span>deploy-cert</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Now you are ready to copy the newly created certificate and key
         to the control node or controllers in the cluster.</p><div class="verbatim-wrap"><pre class="screen">         # scp newcerts/1000.pem control:/root/
         # scp private/susedojo-com.key control:/root/</pre></div></li><li class="step "><p>
         Copy them into the right location on the controller host:
       </p><div class="verbatim-wrap"><pre class="screen">         # cp susedojo-com.key.pem /etc/keystone/ssl/private
         # cp 1000.pem /etc/keystone/ssl/certs
         # cd /etc/keystone/ssl/certs
         # mv signing_cert.pem signing_cert.pem.todays_date
         # cp 1000.pem signing_cert.pem
         # cd /etc/keystone/ssl/private
         # old signing_key.pem
         # cp susedojo-com.key.pem signing_key.pem</pre></div></li><li class="step "><p>
         Rerun the Barclamp for keystone in order to apply this change to
         the cluster.
       </p></li></ol></div></div></div><div class="sect1" id="lets-encrypt-cert"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate Public Certificate using Let’s Encrypt</span> <a title="Permalink" class="permalink" href="#lets-encrypt-cert">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_ssl_self_cert.xml</li><li><span class="ds-label">ID: </span>lets-encrypt-cert</li></ul></div></div></div></div><p>
     <code class="literal">Let’s Encrypt</code> is a free, automated, and open Certificate Authority.
     Its Root is trusted by all major operating systems now. For SUSE Linux
     Enterprise Server 12 SP3 and higher, the ISRG Root X1 is available in
     <code class="filename">/etc/ssl/certs/ISRG_Root_X1.pem</code>. If not, apply the
     latest updates for your operating system.</p><p><code class="literal">Let’s Encrypt</code> has several clients to choose from depending on your needs.
     For this example, we will be using the <code class="literal">acme.sh</code> client,
     which is written in bash and gives us greater flexibility and ease in our
     solution.</p><p>The next steps walk you through the installation of <code class="literal">acme.sh</code> and the issue
     of a certificate with <code class="literal">Let’s Encrypt</code> followed by the automated load
     of the certificate in OpenStack for the various API endpoints available.</p><div class="procedure " id="id-1.3.7.3.6.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.10: </span><span class="name">Installation of acme.sh letsencrypt client </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Login to your crowbar/admin server change to the root directory.
       </p></li><li class="step "><p>
         Create a new directory for letsencrypt and clone the
         <code class="literal">acme.sh</code> repo:
       </p><div class="verbatim-wrap"><pre class="screen">         # mkdir letsencrypt
         # cd letsencrypt
         # git clone https://github.com/Neilpang/acme.sh.git
         # cd acme.sh</pre></div></li><li class="step "><p>
         The system is prepared for installing <code class="literal">acme.sh</code>.
       </p></li><li class="step "><p>
         Install <code class="literal">socat</code>:
       </p><div class="verbatim-wrap"><pre class="screen">         # export BRANCH=2 #this makes sure you are using the v2 api version of letsencrypt
         # zypper in -y socat</pre></div></li><li class="step "><p>
         Install <code class="literal">acme.sh</code>:
       </p><div class="verbatim-wrap"><pre class="screen">         # ./acme.sh --install</pre></div></li><li class="step "><p>
         After the install of <code class="filename">acme.sh</code> is finished, you
         should see a new directory <code class="filename">/root/.acme.sh/</code> where
         <code class="filename">acme.sh</code> lives and all of its environment,
         account info, and certificates are stored.
         We recommend using this as a backup location if you are using a
         backup tool.
       </p></li></ol></div></div><div class="procedure " id="id-1.3.7.3.6.6"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.11: </span><span class="name">Issue a wildcard SSL Certificate </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.6">#</a></h6></div><div class="procedure-contents"><p>
      OpenStack and wildcard SSL uses the DNS validation method by
      validating your domain using a TXT record that can either be added
      manually or using the many (over 3 dozen) available DNS API’s.
    </p><div id="id-1.3.7.3.6.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        It is important to a wildcard certificate as you have the ability
        to use the same one for all of your public API endpoints in the
        OpenStack Cloud environment. Additional Cloud Native services
        like Kubernetes can also take advantage of it.
      </p></div><ol class="procedure" type="1"><li class="step "><p>
        The manual DNS mode is a method that displays the DNS records that
        have to be created in your DNS servers. It is beneficial to automate
        the injection of DNS records as the maximum days a certificate is
        viable is 60 days.
        In order to issue your wildcard certificate, the command without
        optional settings is:
      </p><div class="verbatim-wrap"><pre class="screen">        # acme.sh --issue -d yourdomain.com -d *.yourdomain.com --dns</pre></div></li><li class="step "><p>
        To debug or test, add the following optional settings:
      </p><div class="verbatim-wrap"><pre class="screen">        # acme.sh --debug --test –issue -d yourdomain.com -d *.yourdomain.com --dns</pre></div></li><li class="step "><p>
        A message returns. For example:
      </p><div class="verbatim-wrap"><pre class="screen">        Add the following TXT record:
        Domain: '_acme-challenge.yourdomain.com'
        TXT value: 'KZvgq3MpOCjUNW7Uzz5nE5kkFdplNk66WGfxE9-H63k'
        Please be aware that you prepend <code class="literal">_acme-challenge.</code> before your domain
        so the resulting subdomain will be: <code class="literal">_acme-challenge.yourdomain.com</code></pre></div></li><li class="step "><p>
        Using this information, you are ready to insert this TXT record into
        your DNS.
      </p><div id="id-1.3.7.3.6.6.7.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
          When setting this up for SUSE OpenStack Cloud with Crowbar, you need
          to have your external DNS server appended to <code class="filename">/etc/resolv.conf</code>
          in order to resolve as crowbar has its own internal
          DNS. It is not enough to change it in the barclamp as you need the
          DNS server entry to be at the top of the list in <code class="filename">resolv.conf</code>.
          Crowbar returns to the default after a period of time.
          Keep in mind that if you want to automate this step every 90 days
          then you need to ensure the <code class="filename">resolv.conf</code> changes
          every time to bypass the local crowbar DNS Server.
        </p></div></li><li class="step "><p>
        In order to set up TXT record in bind DNS, edit the zone file so
        it looks like the following example:
      </p><div class="verbatim-wrap"><pre class="screen">        yourdomain.com.     IN NS           admin.yourdomain.com.
        _acme-challenge.yourdomain.com. IN TXT "xxxx...your TXT value string here"</pre></div></li><li class="step "><p>
        Restart your named services for <code class="literal">bind</code>.
      </p></li><li class="step "><p>
        Issue the acme-challenge verification of the previous step with the
        following command:
      </p><div class="verbatim-wrap"><pre class="screen">        # acme.sh --renew -d yourdomain.com</pre></div></li><li class="step "><p>
        If the DNS validation is okay, <code class="filename">acme.sh</code> issues a
        wildcard certificate and displays the certificate and private-key path.
        For example:
      </p><div class="verbatim-wrap"><pre class="screen">        Your cert is in:  /root/.acme.sh/susedojo.com/susedojo.com.cer  
        Your cert key is in:  /root/.acme.sh/susedojo.com/susedojo.com.key  
        v2 chain.
        The intermediate CA cert is in:  /root/.acme.sh/susedojo.com/ca.cer  
        And the full chain certs is in:  /root/.acme.sh/susedojo.com/fullchain.cer_on_issue_success</pre></div></li><li class="step "><p>
        Notice the location of your certificate and key. These are now
        ready to be used by OpenStack Cloud.
      </p></li><li class="step "><p>
        To automate the process of setting up the TXT record in your DNS
        servers and prepare it for automated validation, the file
        <code class="filename">account.conf</code> holds account information
        for the DNS Provider. After exporting the authentication variables,
        it stores them automatically after the command is executed for later use.
        To issue your wildcard certificate, the command without optional settings is:
      </p><div class="verbatim-wrap"><pre class="screen">        # export LUA_Key=”your_API_token_from_account”
        # export LUA_Email=”cameron@yourdomain.com”
        # acme.sh -d yourdomain.com -d *.yourdomain.com --dns dns_lua</pre></div></li><li class="step "><p>
       You can now view your DNS records and you will see a new TXT record
       available. When it is finished and the DNS validation is okay,
       <code class="filename">acme.sh</code> issue your wildcard certificate and displays
       your certificate and private-key path just as before.
      </p><div class="verbatim-wrap"><pre class="screen">        Your cert is in:  /root/.acme.sh/susedojo.com/susedojo.com.cer  
        Your cert key is in:  /root/.acme.sh/susedojo.com/susedojo.com.key  
        v2 chain.
        The intermediate CA cert is in:  /root/.acme.sh/susedojo.com/ca.cer  
        And the full chain certs is in:  /root/.acme.sh/susedojo.com/fullchain.cer_on_issue_success</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.7.3.6.7"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.12: </span><span class="name">Set Up Certificate Store on Control and Compute Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.7">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Create a shared location for all Certificates on the control nodes.
        Execute these commands on all control nodes and compute nodes:
      </p><div class="verbatim-wrap"><pre class="screen">        mkdir -p /etc/cloud/ssl/certs
        mkdir -p /etc/cloud/ssl/private</pre></div></li><li class="step "><p>
        Copy all certificates to their shared locations on the control nodes
        and compute nodes:
      </p><div class="verbatim-wrap"><pre class="screen">        # scp /root/.acme.sh/susedojo.com/susedojo.com.cer \ root@control:/etc/cloud/ssl/certs
        # scp /root/.acme.sh/susedojo.com/ca.cer root@control:/etc/cloud/ssl/certs
        # scp /root/.acme.sh/susedojo.com/fullchain.cer root@control:/etc/cloud/ssl/certs
        # scp /root/.acme.sh/susedojo.com/susedojo.com.key \ root@control:/etc/cloud/ssl/private</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.7.3.6.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 18.13: </span><span class="name">Set Up Issued Certificates in Crowbar Barclamps </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false.
      </p><div class="figure" id="id-1.3.7.3.6.8.2.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/database-barclamp.png" target="_blank"><img src="images/database-barclamp.png" width="" alt="Database Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 18.1: </span><span class="name">Database Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8.2.2">#</a></h6></div></div></li><li class="step "><p>
        Click <code class="literal">Apply</code>. Your changes will apply in a
        few minutes.
      </p></li><li class="step "><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false.
      </p><div class="figure" id="id-1.3.7.3.6.8.4.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/rabbitmq-barclamp.png" target="_blank"><img src="images/rabbitmq-barclamp.png" width="" alt="RabbitMQ Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 18.2: </span><span class="name">RabbitMQ Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8.4.2">#</a></h6></div></div></li><li class="step "><p>
        Click <code class="literal">Apply</code>. Your changes will apply in a
        few minutes.
      </p></li><li class="step "><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false.
      </p><div class="figure" id="id-1.3.7.3.6.8.6.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/keystone-barclamp.png" target="_blank"><img src="images/keystone-barclamp.png" width="" alt="Keystone Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 18.3: </span><span class="name">Keystone Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8.6.2">#</a></h6></div></div></li><li class="step "><p>
        Click <code class="literal">Apply</code>. Your changes will apply in a
        few minutes.
      </p></li><li class="step "><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is true.
      </p><div class="figure" id="id-1.3.7.3.6.8.8.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/glance-barclamp.png" target="_blank"><img src="images/glance-barclamp.png" width="" alt="Glance Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 18.4: </span><span class="name">Glance Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8.8.2">#</a></h6></div></div></li><li class="step "><p>
        Click <code class="literal">Apply</code>. Your changes will apply in a
        few minutes.
      </p></li><li class="step "><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is false.
      </p><div class="figure" id="id-1.3.7.3.6.8.10.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/cinder-barclamp.png" target="_blank"><img src="images/cinder-barclamp.png" width="" alt="Cinder Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 18.5: </span><span class="name">Cinder Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8.10.2">#</a></h6></div></div></li><li class="step "><p>
        Click <code class="literal">Apply</code>. Your changes will apply in a
        few minutes.
      </p></li><li class="step "><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is false.
      </p><div class="figure" id="id-1.3.7.3.6.8.12.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/neutron-barclamp.png" target="_blank"><img src="images/neutron-barclamp.png" width="" alt="Neutron Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 18.6: </span><span class="name">Neutron Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8.12.2">#</a></h6></div></div></li><li class="step "><p>
        Click <code class="literal">Apply</code>. Your changes will apply in a
        few minutes.
      </p></li><li class="step "><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is false.
      </p><div class="figure" id="id-1.3.7.3.6.8.14.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/nova-barclamp.png" target="_blank"><img src="images/nova-barclamp.png" width="" alt="Nova Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 18.7: </span><span class="name">Nova Barclamp </span><a title="Permalink" class="permalink" href="#id-1.3.7.3.6.8.14.2">#</a></h6></div></div></li><li class="step "><p>
        Click <code class="literal">Apply</code>. Your changes will apply in a
        few minutes.
      </p></li></ol></div></div><p>
    Each Crowbar barclamp that have SSL support are the same. You can change
    the same settings and apply your certificate to the remaining barclamps.</p><div id="id-1.3.7.3.6.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Once this is completed, we recommend automating this process as the
      <code class="literal">Let's Encrypt</code> certificates expire after 90 days.
    </p></div></div></div><div class="chapter " id="cha-deploy-logs"><div class="titlepage"><div><div><h2 class="title"><span class="number">19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Log Files</span> <a title="Permalink" class="permalink" href="#cha-deploy-logs">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_logfiles.xml</li><li><span class="ds-label">ID: </span>cha-deploy-logs</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-deploy-logs-adminserv"><span class="number">19.1 </span><span class="name">On the Administration Server</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-logs-crownodes"><span class="number">19.2 </span><span class="name">On All Other Crowbar Nodes</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-logs-contrnode"><span class="number">19.3 </span><span class="name">On the Control Node(s)</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-logs-compnode"><span class="number">19.4 </span><span class="name">On Compute Nodes</span></a></span></dt></dl></div></div><p>
  Find a list of log files below, sorted according to the nodes where they
  can be found.
 </p><div class="sect1 " id="sec-deploy-logs-adminserv"><div class="titlepage"><div><div><h2 class="title"><span class="number">19.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">On the Administration Server</span> <a title="Permalink" class="permalink" href="#sec-deploy-logs-adminserv">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_logfiles.xml</li><li><span class="ds-label">ID: </span>sec-deploy-logs-adminserv</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Crowbar Web Interface:
     <code class="filename">/var/log/crowbar/production.log</code>
    </p></li><li class="listitem "><p>
     Chef server: <code class="filename">/var/log/chef/server.log</code>
    </p></li><li class="listitem "><p>
     Chef expander: <code class="filename">/var/log/chef/expander.log</code>
    </p></li><li class="listitem "><p>
     Chef client (for the Administration Server only):
     <code class="filename">/var/log/chef/client.log </code>
    </p></li><li class="listitem "><p>
     Upgrade log files (only available if the Administration Server has been upgraded
     from a previous version using <code class="command">suse-cloud-upgrade</code>):
     <code class="filename">/var/log/crowbar/upgrade/*</code>
    </p></li><li class="listitem "><p>
     Apache SOLR (Chef's search server):
     <code class="filename">/var/log/chef/solr.log</code>
    </p></li><li class="listitem "><p>
     HTTP (AutoYaST) installation server for provisioner barclamp:
     <code class="filename">/var/log/apache2/provisioner-{access,error}_log</code>
    </p></li><li class="listitem "><p>
     Log file from mirroring SMT repositories (optional):
     <code class="filename">/var/log/smt/smt-mirror.log</code>
    </p></li><li class="listitem "><p>
     Default SUSE log files: <code class="filename">/var/log/messages</code>,
     <code class="filename">/var/log/zypper.log </code> etc.
    </p></li><li class="listitem "><p>
     Syslogs for all nodes: <code class="filename">/var/log/nodes/*.log</code> (these
     are collected via remote syslogging)
    </p></li><li class="listitem "><p>
     Other client node log files saved on the Administration Server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="filename">/var/log/crowbar/sledgehammer/d*.log</code>: Initial
       Chef client run on nodes booted using PXE prior to discovery by
       Crowbar.
      </p></li><li class="listitem "><p>
       <code class="filename">/var/log/crowbar/chef-client/d*.log</code>: Output from
       Chef client when proposals are applied to nodes. This is the
       first place to look if a barclamp proposal fails to apply.
      </p></li></ul></div></li></ul></div></div><div class="sect1 " id="sec-deploy-logs-crownodes"><div class="titlepage"><div><div><h2 class="title"><span class="number">19.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">On All Other Crowbar Nodes</span> <a title="Permalink" class="permalink" href="#sec-deploy-logs-crownodes">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_logfiles.xml</li><li><span class="ds-label">ID: </span>sec-deploy-logs-crownodes</li></ul></div></div></div></div><p>
   Logs for when the node registers with the Administration Server:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="filename">/var/log/crowbar/crowbar_join/errlog</code>
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/crowbar/crowbar_join/$TOPIC.{log,err}</code>:
     STDOUT/STDERR from running commands associated with $TOPIC when the
     node joins the Crowbar cluster. $TOPIC can be:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="filename">zypper</code>: package management activity
      </p></li><li class="listitem "><p>
       <code class="filename">ifup</code>: network configuration activity
      </p></li><li class="listitem "><p>
       <code class="filename">Chef</code>: Chef client activity
      </p></li><li class="listitem "><p>
       <code class="filename">time</code>: starting of ntp client
      </p></li></ul></div></li><li class="listitem "><p>
     Chef client log: <code class="filename">/var/log/chef/client.log</code>
    </p></li><li class="listitem "><p>
     Default SUSE log files: <code class="filename">/var/log/messages</code>,
     <code class="filename">/var/log/zypper.log </code> etc.
    </p></li></ul></div></div><div class="sect1 " id="sec-deploy-logs-contrnode"><div class="titlepage"><div><div><h2 class="title"><span class="number">19.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">On the Control Node(s)</span> <a title="Permalink" class="permalink" href="#sec-deploy-logs-contrnode">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_logfiles.xml</li><li><span class="ds-label">ID: </span>sec-deploy-logs-contrnode</li></ul></div></div></div></div><p>
   On setups with multiple Control Nodes log files for certain services
   (such as <code class="filename">keystone.log</code>) are only available on the
   nodes where the services are deployed.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="filename">/var/log/apache2/openstack-dashboard-*</code>: Logs for
     the <span class="productname">OpenStack</span> Dashboard
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/ceilometer/*</code>: Ceilometer log files.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/cinder/*</code>: Cinder log files.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/glance/*</code>: Glance; log files.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/heat/*</code>: Heat log files.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/keystone/*</code>: Keystone log files.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/neutron/*</code>: Neutron log files.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/nova/*</code>: various log files relating to
     Nova services.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/rabbitmq/*</code>: RabbitMQ log files.
    </p></li><li class="listitem "><p>
     <code class="filename">/var/log/swift/*</code>: Swift log files.
    </p></li></ul></div></div><div class="sect1 " id="sec-deploy-logs-compnode"><div class="titlepage"><div><div><h2 class="title"><span class="number">19.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">On Compute Nodes</span> <a title="Permalink" class="permalink" href="#sec-deploy-logs-compnode">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_logfiles.xml</li><li><span class="ds-label">ID: </span>sec-deploy-logs-compnode</li></ul></div></div></div></div><p>
   <code class="filename">/var/log/nova/nova-compute.log</code>
  </p></div></div><div class="chapter " id="cha-depl-trouble"><div class="titlepage"><div><div><h2 class="title"><span class="number">20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting and Support</span> <a title="Permalink" class="permalink" href="#cha-depl-trouble">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_troubleshooting.xml</li><li><span class="ds-label">ID: </span>cha-depl-trouble</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-trouble-faq"><span class="number">20.1 </span><span class="name">FAQ</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-trouble-support"><span class="number">20.2 </span><span class="name">Support</span></a></span></dt></dl></div></div><p>
  Find solutions for the most common pitfalls and technical details on how
  to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> here.
 </p><div class="sect1 " id="sec-depl-trouble-faq"><div class="titlepage"><div><div><h2 class="title"><span class="number">20.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">FAQ</span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-faq">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_troubleshooting.xml</li><li><span class="ds-label">ID: </span>sec-depl-trouble-faq</li></ul></div></div></div></div><p>
   If your problem is not mentioned here, checking the log files on either
   the Administration Server or the <span class="productname">OpenStack</span> nodes may help. A list of log files
   is available at <a class="xref" href="#cha-deploy-logs" title="Chapter 19. Log Files">Chapter 19, <em>Log Files</em></a>.
  </p><div class="qandaset" id="id-1.3.7.5.4.3"><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="sec-depl-trouble-faq-admin">1. Admin Node Deployment</h4></div><div class="qandadiv"><div class="free-id" id="id-1.3.7.5.4.3.1.2"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.1.2.1"><strong>Q:</strong>
       What to do if the initial SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation on the Administration Server fails?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.1.2.2"><p>
       Check the installation routine's log file at
       <code class="filename">/var/log/crowbar/install.log</code> for error messages.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.1.3"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.1.3.1"><strong>Q:</strong>

       What to do if the initial SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation on the Administration Server fails while
       deploying the IPMI/BMC network?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.1.3.2"><p>
       As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <span class="phrase"><span class="phrase">8</span></span>, it is assumed that each
       machine can be accessed directly via IPMI/BMC. However, this is not
       the case on certain blade hardware, where several nodes are accessed
       via a common adapter. Such a hardware setup causes an error on
       deploying the IPMI/BMC network. You need to disable the IPMI
       deployment running the following command:
      </p><div class="verbatim-wrap"><pre class="screen">/opt/dell/bin/json-edit -r -a "attributes.ipmi.bmc_enable" \
-v "false" /opt/dell/chef/data_bags/crowbar/bc-template-ipmi.json</pre></div><p>
       Re-run the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation after having disabled the IPMI deployment.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.1.4"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.1.4.1"><strong>Q:</strong>
       Why am I not able to reach the Administration Server from outside the admin
       network via the bastion network?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.1.4.2"><p>
       If <code class="command">route</code> <code class="option">-n</code> shows no gateway for
       the bastion network, check the value of the following entries in
       <code class="filename">/etc/crowbar/network.json</code>:
       <code class="literal">"router_pref":</code> and <code class="literal">"router_pref":</code>.
       Make sure the value for the bastion network's
       <code class="literal">"router_pref":</code> is set to a
       <span class="emphasis"><em>lower</em></span> value than <code class="literal">"router_pref":</code>
       for the admin network.
      </p><p>
       If the router preference is set correctly, <code class="command">route</code>
       <code class="option">-n</code> shows a gateway for the bastion network. In case
       the Administration Server is still not accessible via its admin network
       address (for example,
       <code class="systemitem">192.168.124.10</code>), you need
       to disable route verification (<code class="literal">rp_filter</code>). Do so
       by running the following command on the Administration Server:
      </p><div class="verbatim-wrap"><pre class="screen">echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</pre></div><p>
       If this setting solves the problem, make it permanent by editing
       <code class="filename">/etc/sysctl.conf</code> and setting the value for
       <code class="literal">net.ipv4.conf.all.rp_filter</code> to
       <code class="literal">0</code>.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.1.5"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.1.5.1"><strong>Q:</strong>
       Can I change the host name of the Administration Server?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.1.5.2"><p>
       No, after you have run the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation you cannot change the host name.
       Services like Crowbar, Chef, and the RabbitMQ will fail after changing
       the host name.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.1.6"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.1.6.1"><strong>Q:</strong>
       What to do when browsing the Chef Web UI gives a
       <code class="literal">Tampered with cookie</code> error?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.1.6.2"><p>
       You probably have an old cookie in your browser from a previous
       Chef installation on the same IP address. Remove the cookie named
       <code class="literal">_chef_server_session_id</code> and try again.
      </p></dd></dl><div class="free-id" id="q-depl-trouble-faq-admin-custom-repos"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.1.7.1"><strong>Q:</strong>
       How to make custom software repositories from an external server (for
       example a remote SMT or SUSE Manager server) available for the nodes?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.1.7.2"><p>
       Custom repositories need to be added using the YaST Crowbar
       module:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Start the YaST Crowbar module and switch to the
         <span class="guimenu ">Repositories</span> tab: <span class="guimenu ">YaST</span> › <span class="guimenu ">Miscellaneous</span> › <span class="guimenu ">Crowbar</span> › <span class="guimenu ">Repositories</span>.
        </p></li><li class="step "><p>
         Choose <span class="guimenu ">Add Repositories</span>
        </p></li><li class="step "><p>
         Enter the following data:
        </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.7.5.4.3.1.7.2.2.3.2.1"><span class="term "><span class="guimenu ">Name</span>
          </span></dt><dd><p>
            A unique name to identify the repository.
           </p></dd><dt id="id-1.3.7.5.4.3.1.7.2.2.3.2.2"><span class="term "><span class="guimenu ">URL</span>
          </span></dt><dd><p>
            Link or path to the repository.
           </p></dd><dt id="id-1.3.7.5.4.3.1.7.2.2.3.2.3"><span class="term "><span class="guimenu ">Ask On Error</span>
          </span></dt><dd><p>
            Access errors to a repository are silently ignored by default.
            To ensure that you get notified of these errors, set the
            <code class="literal">Ask On Error</code> flag.
           </p></dd><dt id="id-1.3.7.5.4.3.1.7.2.2.3.2.4"><span class="term "><span class="guimenu ">Target Platform/Architecture</span>
          </span></dt><dd><p>
            Currently only repositories for <span class="guimenu ">SLE 12 SP3</span> on
            the <span class="guimenu ">x86_64</span> architecture are supported. Make
            sure to select both options.
           </p></dd></dl></div></li><li class="step "><p>
         Save your settings selecting <span class="guimenu ">OK</span>.
        </p></li></ol></div></div></dd></dl></div><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="sec-depl-trouble-faq-ostack">2. <span class="productname">OpenStack</span> Node Deployment</h4></div><div class="qandadiv"><div class="free-id" id="var-depl-trouble-faq-ostack-login"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.2.1"><strong>Q:</strong>
       How can I log in to a node as <code class="systemitem">root</code>?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.2.2"><p>
       By default you cannot directly log in to a node as <code class="systemitem">root</code>,
       because the nodes were set up without a <code class="systemitem">root</code> password. You
       can only log in via SSH from the Administration Server. You should be able to
       log in to a node with <code class="command">ssh root@<em class="replaceable ">NAME</em></code>
       where <em class="replaceable ">NAME</em>
       is the name (alias) of the node.
      </p><p>
       If name resolution does not work, go to the Crowbar Web interface
       and open the <span class="guimenu ">Node Dashboard</span>. Click the name of the
       node and look for its <span class="guimenu ">admin (eth0)</span> <span class="guimenu ">IP
       Address</span>. Log in to that IP address via SSH as user
       <code class="systemitem">root</code>.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.3"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.3.1"><strong>Q:</strong>
       What to do if a node refuses to boot or boots into a previous
       installation?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.3.2"><p>
       Make sure to change the boot order in the BIOS of the node, so that
       the first boot option is to boot from the network/boot using PXE.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.4"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.4.1"><strong>Q:</strong>
       What to do if a node hangs during hardware discovery after the very
       first boot using PXE into the <span class="quote">“<span class="quote ">SLEShammer</span>”</span> image?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.4.2"><p>
       The <code class="systemitem">root</code> login is enabled at a very early state in discovery
       mode, so chances are high that you can log in for debugging purposes as
       described in <a class="xref" href="#var-depl-trouble-faq-ostack-login" title="Q:"><em>
       How can I log in to a node as root?
      </em></a>.  If
       logging in as <code class="systemitem">root</code> does not work, you need to set the <code class="systemitem">root</code>
       password manually. This can either be done by setting the password via
       the Kernel command line as explained in <a class="xref" href="#qa-depl-trouble-faq-ostack-rootpw" title="Q:"><em>
       How to provide Kernel Parameters for the SLEShammer Discovery Image?
      </em></a>, or by creating a hook as
       explained below:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Create a directory on the Administration Server named
         <code class="filename">/updates/discovering-pre</code>
        </p><div class="verbatim-wrap"><pre class="screen">mkdir /updates/discovering-pre</pre></div></li><li class="step "><p>
         Create a hook script <code class="filename">setpw.hook</code> in the
         directory created in the previous step:
        </p><div class="verbatim-wrap"><pre class="screen">cat &gt; /updates/discovering-pre/setpw.hook &lt;&lt;EOF
#!/bin/sh
echo "root:linux" | chpasswd
EOF</pre></div></li><li class="step "><p>
         Make the script executable:
        </p><div class="verbatim-wrap"><pre class="screen">chmod a+x  /updates/discovering-pre/setpw.hook</pre></div></li></ol></div></div><p>
       If you are still cannot log in, you very likely hit a bug in the
       discovery image. Report it at
       <a class="link" href="http://bugzilla.suse.com/" target="_blank">http://bugzilla.suse.com/</a>.
      </p></dd></dl><div class="free-id" id="qa-depl-trouble-faq-ostack-rootpw"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.5.1"><strong>Q:</strong>
       How to provide Kernel Parameters for the SLEShammer Discovery Image?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.5.2"><p>
       Kernel Parameters for the SLEShammer Discovery Image can be provided
       via the Provisioner barclamp. The following example shows how to set a
       <code class="systemitem">root</code> password:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Open a browser and point it to the Crowbar Web interface available on
         the Administration Server, for example
         <code class="literal">http://192.168.124.10/</code>. Log in as user <code class="systemitem">crowbar</code>. The password is
         <code class="literal">crowbar</code> by default, if you have not changed it.
        </p></li><li class="step "><p>
         Open <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span> and click
         <span class="guimenu ">Edit</span> in the <span class="guimenu ">Provisioner</span> row.
        </p></li><li class="step "><p>
         Click <span class="guimenu ">Raw</span> in the <span class="guimenu ">Attributes</span>
         section and add the Kernel parameter(s) to the <code class="literal">"discovery":
         { "append": "" }</code> line, for example;
        </p><div class="verbatim-wrap"><pre class="screen">  "discovery": {
    "append": "DISCOVERY_ROOT_PASSWORD=<em class="replaceable ">PASSWORD</em>"
  },</pre></div></li><li class="step "><p>
         <span class="guimenu ">Apply</span> the proposal without changing the
         assignments in the <span class="guimenu ">Deployment</span> section.
        </p></li></ol></div></div></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.6"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.6.1"><strong>Q:</strong>
       What to do when a deployed node fails to boot using PXE with the
       following error message: <span class="quote">“<span class="quote ">Could not find kernel image:
       ../suse-12.2/install/boot/x86_64/loader/linux</span>”</span>?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.6.2"><p>
       The installation repository on the
       Administration Server at <code class="filename">/srv/tftpboot/suse-12.3/install</code>
       has not been set up correctly to contain the SUSE Linux Enterprise Server 12 SP3
       installation media. Review the instructions at
       <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a>.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.7"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.7.1"><strong>Q:</strong>
       Why does my deployed node hang at <code class="literal">Unpacking
       initramfs</code> during boot when using PXE?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.7.2"><p>
       The node probably does not have enough RAM. You need at least 4 GB
       RAM for the deployment process to work.
      </p></dd></dl><div class="free-id" id="q-depl-trouble-faq-ostack-problem"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.8.1"><strong>Q:</strong>
       What to do if a node is reported to be in the state
       <code class="literal">Problem</code>? What to do if a node hangs at
       <span class="quote">“<span class="quote ">Executing AutoYast script:
           /usr/sbin/crowbar_join --setup</span>”</span> after the installation is finished?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.8.2"><p>
       Be patient—the AutoYaST script may take a while to finish. If
       it really hangs, log in to the node as <code class="systemitem">root</code> (see
       <a class="xref" href="#var-depl-trouble-faq-ostack-login" title="Q:"><em>
       How can I log in to a node as root?
      </em></a> for details).
       Check for error messages at the end of
       <code class="filename">/var/log/crowbar/crowbar_join/chef.log</code>. Fix the
       errors and restart the AutoYaST script by running the following command:
      </p><div class="verbatim-wrap"><pre class="screen">crowbar_join --start</pre></div><p>
       If successful, the node will be listed in state
       <code class="literal">Ready</code>, when the script has finished.
      </p><p>
       In cases where the initial --setup wasn't able to complete successfully,
       you can rerun that once after the previous issue is solved.
      </p><p>
       If that does not help or if the log does not provide useful
       information, proceed as follows:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Administration Server and run the following command:
        </p><div class="verbatim-wrap"><pre class="screen">crowbar crowbar transition $<em class="replaceable ">NODE</em></pre></div><p>
         <em class="replaceable ">NODE</em> needs to be replaced by the alias
         name you have given to the node when having installed it. Note that
         this name needs to be prefixed with <code class="literal">$</code>.
        </p></li><li class="step "><p>
         Log in to the node and run <code class="command">chef-client</code>.
        </p></li><li class="step "><p>
         Check the output of the command for failures and error messages and
         try to fix the cause of these messages.
        </p></li><li class="step "><p>
         Reboot the node.
        </p></li></ol></div></div><p>
       If the node is in a state where login in from the Administration Server is not
       possible, you need to create a <code class="systemitem">root</code> password for it as
       described in <a class="xref" href="#var-depl-inst-nodes-prep-root-login">Direct <code class="systemitem">root</code> Login</a>.
       Now re-install the node by going to the node on the Crowbar Web
       interface and clicking <span class="guimenu ">Reinstall</span>. After having
       been re-installed, the node will hang again, but now you can log in
       and check the log files to find the cause.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.9"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.9.1"><strong>Q:</strong>
       Where to find more information when applying a barclamp proposal
       fails?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.9.2"><p>
       Check the Chef client log files on the Administration Server located at
       <code class="filename">/var/log/crowbar/chef-client/d*.log</code>. Further
       information is available from the Chef client log files located
       on the node(s) affected by the proposal
       (<code class="filename">/var/log/chef/client.log</code>), and from the log
       files of the service that failed to be deployed. Additional
       information may be gained from the Crowbar Web UI log files on the
       Administration Server. For a list of log file locations refer to
       <a class="xref" href="#cha-deploy-logs" title="Chapter 19. Log Files">Chapter 19, <em>Log Files</em></a>.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.10"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.10.1"><strong>Q:</strong>
       How to Prevent the Administration Server from Installing the <span class="productname">OpenStack</span> Nodes
       (Disable PXE and DNS Services)?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.10.2"><p>
       By default, the <span class="productname">OpenStack</span> nodes are installed by booting a discovery
       image from the Administration Server using PXE. They are allocated and then boot
       via PXE into an automatic installation (see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a> for details). To
       install the <span class="productname">OpenStack</span> nodes manually or with a custom provisioning tool,
       you need to disable the PXE boot service and the DNS service on the
       Administration Server.
      </p><p>
       As a consequence you also need to provide an external DNS server. Such
       a server needs to comply with the following requirements:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          It needs to handle all domain-to-IP requests for SUSE <span class="productname">OpenStack</span> Cloud.
        </p></li><li class="listitem "><p>
         It needs to handle all IP-to-domain requests for SUSE <span class="productname">OpenStack</span> Cloud.
        </p></li><li class="listitem "><p>
         It needs to forward unknown requests to other DNS servers.
        </p></li></ul></div><p>
       To disable the PXE and DNS services when setting up the Administration Server,
       proceed as follows:
      </p><div class="procedure " id="id-1.3.7.5.4.3.2.10.2.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 20.1: </span><span class="name">Disabling PXE/DNS when Setting Up the Administration Server </span><a title="Permalink" class="permalink" href="#id-1.3.7.5.4.3.2.10.2.5">#</a></h6></div><div class="procedure-contents"><p>
        The following steps need to be performed <span class="emphasis"><em>before</em></span>
        starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
       </p><ol class="procedure" type="1"><li class="step "><p>
         Create the file <code class="filename">/etc/crowbar/dns.json</code> with the
         following content:
        </p><div class="verbatim-wrap"><pre class="screen">{
  "attributes": {
    "dns": {
      "nameservers": [ "<em class="replaceable ">DNS_SERVER</em>", "<em class="replaceable ">DNS_SERVER2</em>" ],
      "auto_assign_server": false
    }
  }
}</pre></div><p>
         Replace <em class="replaceable ">DNS_SERVER</em> and
         <em class="replaceable ">DNS_SERVER2</em> with the IP address(es) of the
         external DNS server(s). Specifying more than one server is optional.
        </p></li><li class="step "><p>
         Create the file <code class="filename">/etc/crowbar/provisioner.json</code>
         with the following content:
        </p><div class="verbatim-wrap"><pre class="screen">{
  "attributes": {
    "provisioner": {
      "enable_pxe": false
    }
  }
}</pre></div></li><li class="step "><p>
         If these files are present when the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation is started, the
         Administration Server will be set up using external DNS services and no PXE boot
         server.
        </p></li></ol></div></div><p>
       If you already have deployed SUSE <span class="productname">OpenStack</span> Cloud, proceed as follows to
       disable the DNS and PXE services on the Administration Server:
      </p><div class="procedure " id="id-1.3.7.5.4.3.2.10.2.7"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 20.2: </span><span class="name">Disabling PXE/DNS on an Administration Server Running Crowbar </span><a title="Permalink" class="permalink" href="#id-1.3.7.5.4.3.2.10.2.7">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Open a browser and point it to the Crowbar Web interface available on
         the Administration Server, for example
         <code class="literal">http://192.168.124.10/</code>. Log in as user <code class="systemitem">crowbar</code>. The password is
         <code class="literal">crowbar</code> by default, if you have not changed it.
        </p></li><li class="step "><p>
         Open <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span> and click
         <span class="guimenu ">Edit</span> in the <span class="guimenu ">Provisioner</span> row.
        </p></li><li class="step "><p>
         Click <span class="guimenu ">Raw</span> in the <span class="guimenu ">Attributes</span>
         section and change the value for <span class="guimenu ">enable_pxe</span> to
         <code class="literal">false</code>:
        </p><div class="verbatim-wrap"><pre class="screen">"enable_pxe": false,</pre></div></li><li class="step "><p>
         <span class="guimenu ">Apply</span> the proposal without changing the
         assignments in the <span class="guimenu ">Deployment</span> section.
        </p></li><li class="step "><p>
         Change to the <span class="guimenu ">DNS</span> barclamp via <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span>
         and click <span class="guimenu ">Edit</span> in the <span class="guimenu ">DNS</span> row.
        </p></li><li class="step "><p>
         Click <span class="guimenu ">Raw</span> in the <span class="guimenu ">Attributes</span>
         section. Change the value for <span class="guimenu ">auto_assign_server</span> to <code class="literal">false</code> and
         add the address(es) for the external name server(s):
        </p><div class="verbatim-wrap"><pre class="screen">"auto_assign_server": false,
"nameservers": [
  "<em class="replaceable ">DNS_SERVER</em>",
  "<em class="replaceable ">DNS_SERVER2</em>"
],</pre></div><p>
         Replace <em class="replaceable ">DNS_SERVER</em> and
         <em class="replaceable ">DNS_SERVER2</em> with the IP address(es) of the
         external DNS server(s). Specifying more than one server is optional.
        </p></li><li class="step "><p>
         <span class="guimenu ">Save</span> your changes, but do not apply them, yet!
        </p></li><li class="step "><p>
         In the <span class="guimenu ">Deployment</span> section of the barclamp remove
         all nodes from the <span class="guimenu ">dns-server</span> role, but do not
         change the assignments for the <span class="guimenu ">dns-client</span> role.
        </p></li><li class="step "><p>
         <span class="guimenu ">Apply</span> the barclamp.
        </p></li><li class="step "><p>
         When the DNS barclamp has been successfully applied, log in to the
         Administration Server and stop the DNS service:
        </p><div class="verbatim-wrap"><pre class="screen">systemctl stop named</pre></div></li></ol></div></div><p>
       Now that the PXE and DNS services are disabled you can install
       SUSE Linux Enterprise Server 12 SP3 on the <span class="productname">OpenStack</span> nodes. When a node is ready, add it to the
       pool of nodes as described in <a class="xref" href="#sec-depl-inst-nodes-install-external" title="11.3. Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE OpenStack Cloud Nodes">Section 11.3, “Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes”</a>.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.11"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.11.1"><strong>Q:</strong>
       I have installed a new hard disk on a node that was already deployed.
       Why is it ignored by Crowbar?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.11.2"><p>
       When adding a new hard disk to a node that has already been deployed,
       it can take up to 15 minutes before the new disk is detected.
      </p></dd></dl><div class="free-id" id="id-1.3.7.5.4.3.2.12"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.2.12.1"><strong>Q:</strong>
       How to install additional packages (for example a driver) when nodes
       are deployed?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.2.12.2"><p>
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> offers the possibility to install additional packages that
       are not part of the default scope of packages installed on the <span class="productname">OpenStack</span>
       nodes. This is for example required if your hardware is only supported
       by a third party driver. It is also useful if your setup requires to
       install additional tools that would otherwise need to be installed
       manually.
      </p><p>
       Prerequisite for using this feature is that the packages are available
       in a repository known on the Administration Server. Refer to <a class="xref" href="#q-depl-trouble-faq-admin-custom-repos" title="Q:"><em>How to make custom software repositories from an external server (for example a remote SMT or SUSE M..?</em></a> for details, if the
       packages you want to install are not part of the repositories already
       configured.
      </p><p>
       To add packages for installation on node deployment, proceed as
       follows:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Open a browser and point it to the Crowbar Web interface on the Administration Server, for
         example <code class="literal">http://192.168.124.10/</code>. Log in as user
         <code class="systemitem">crowbar</code>. The password is
         <code class="literal">crowbar</code> by default, if you have not changed it
         during the installation.
        </p></li><li class="step "><p>
         Go to <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span> and click the
         <span class="guimenu ">Edit</span> button for <span class="guimenu ">Provisioner</span>.
        </p></li><li class="step "><p>
         Next click <span class="guimenu ">Raw</span> in the
         <span class="guimenu ">Attributes</span> page to open an editable view of the
         provisioner configuration.
        </p></li><li class="step "><p>
         Add the following JSON code <span class="emphasis"><em>before</em></span> the last
         closing curly bracket (replace the <em class="replaceable ">PACKAGE</em>
         placeholders with real package names):
        </p><div class="verbatim-wrap"><pre class="screen">         "packages": {
    "suse-12.2": ["<em class="replaceable ">PACKAGE_1</em>", "<em class="replaceable ">PACKAGE_2</em>"],
  }</pre></div></li></ol></div></div><p>
       Note that these packages will get installed on all <span class="productname">OpenStack</span> nodes. If
       the change to the Provisioner barclamp is made after nodes have already
       been deployed, the packages will be installed on the affected nodes
       with the next run of Chef or <code class="command">crowbar-register</code>. Package
       names will be validated against the package naming guidelines to
       prevent script-injection.
      </p></dd></dl></div><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="sec-depl-trouble-faq-misc">3. Miscellaneous</h4></div><div class="qandadiv"><div class="free-id" id="sec-depl-trouble-faq-misc-nova"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.3.2.1"><strong>Q:</strong>
       How to change the <code class="literal">nova</code> default configuration?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.3.2.2"><p>
       To change the <code class="literal">nova</code> default configuration, its Chef
       cookbook file needs to be adjusted. This file
       is stored on the Administration Server at
       <code class="filename">/opt/dell/chef/cookbooks/nova/templates/default/nova.conf.erb</code>.
       To activate changes to these files, execute the following command:
      </p><div class="verbatim-wrap"><pre class="screen">barclamp_install.rb --rpm /opt/dell/barclamps/openstack/</pre></div></dd></dl><div class="free-id" id="sec-depl-trouble-faq-misc-keystone-pw"></div><dl class="qandaentry"><dt class="question" id="id-1.3.7.5.4.3.3.3.1"><strong>Q:</strong>
       How to change the Keystone credentials after the Keystone barclamp has
       been deployed?
      </dt><dd class="answer" id="id-1.3.7.5.4.3.3.3.2"><p>
       To change the credentials for the Keystone administrator (<code class="systemitem">admin</code>) or the regular user (<code class="systemitem">crowbar</code> by default), proceed as follows:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Control Node on which Keystone is deployed as user
         <code class="systemitem">root</code> via the Administration Server.
        </p></li><li class="step "><p>
         In a shell, source the <span class="productname">OpenStack</span> RC file for the project that you want
         to upload an image to. For details, refer to <a class="link" href="http://docs.openstack.org/user-guide/common/cli_set_environment_variables_using_openstack_rc.html" target="_blank">Set
         environment variables using the <span class="productname">OpenStack</span> RC file</a> in the
         <span class="productname">OpenStack</span> documentation.
        </p></li><li class="step "><p>
         Enter the following command to change the
         <em class="replaceable ">PASSWORD</em> for the administrator or the
         regular user (<em class="replaceable ">USER</em>):
        </p><div class="verbatim-wrap"><pre class="screen"> keystone-manage bootstrap --bootstrap-password <em class="replaceable ">PASSWORD</em> \
--bootstrap-username <em class="replaceable ">USER</em></pre></div><p>
         For a complete list of command line options, run
         <code class="command">keystone-manage bootstrap --help</code>. Make sure to
         start the command with a <span class="keycap">Space</span> to make sure the
         password does not appear in the command history.
        </p></li><li class="step "><p>
         Access the Keystone barclamp on the Crowbar Web interface by going to
         <span class="guimenu ">Barclamps</span> › <span class="guimenu ">OpenStack</span> and click
         <span class="guimenu ">Edit</span> for the Keystone barclamp.
        </p></li><li class="step "><p>
         Enter the new password for the same user you specified on the command
         line before.
        </p></li><li class="step "><p>
         Activate the change by clicking <span class="guimenu ">Apply</span>. When the
         proposal has been re-applied, the password has changed and can be
         used.
        </p></li></ol></div></div></dd></dl></div></div></div><div class="sect1 " id="sec-depl-trouble-support"><div class="titlepage"><div><div><h2 class="title"><span class="number">20.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support</span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-support">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_troubleshooting.xml</li><li><span class="ds-label">ID: </span>sec-depl-trouble-support</li></ul></div></div></div></div><p>
   
   Before contacting support to help you with a problem on SUSE <span class="productname">OpenStack</span> Cloud, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   ships with a tool called <code class="command">supportconfig</code>. It gathers
   system information such as the current kernel version being used, the
   hardware, RPM database, partitions, and other items.
   <code class="command">supportconfig</code> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </p><p>
   It is recommended to always run <code class="command">supportconfig</code> on the
   Administration Server and on the Control Node(s). If a Compute Node or a
   Storage Node is part of the problem, run
   <code class="command">supportconfig</code> on the affected node as well. For
   details on how to run <code class="command">supportconfig</code>, see
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support</a>.
  </p><div class="sect2 " id="sec-depl-trouble-support-ptf"><div class="titlepage"><div><div><h3 class="title"><span class="number">20.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
    Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support
   </span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-support-ptf">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_troubleshooting.xml</li><li><span class="ds-label">ID: </span>sec-depl-trouble-support-ptf</li></ul></div></div></div></div><p>
    Under certain circumstances, the SUSE support may provide temporary
    fixes, the so-called PTFs, to customers with an L3 support contract.
    These PTFs are provided as RPM packages. To make them available on all
    nodes in SUSE <span class="productname">OpenStack</span> Cloud, proceed as follows.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the packages from the location provided by the SUSE L3
      Support to a temporary location on the Administration Server.
     </p></li><li class="step "><p>
      Move the packages from the temporary download location to the
      following directories on the Administration Server:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.7.5.5.4.3.2.2.1"><span class="term ">
        <span class="quote">“<span class="quote ">noarch</span>”</span> packages (<code class="filename">*.noarch.rpm</code>):
       </span></dt><dd><table border="0" summary="Simple list" class="simplelist "><tr><td>
          <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/PTF/rpm/noarch/</code>
         </td></tr><tr><td>
          <code class="filename">/srv/tftpboot/suse-12.3/s390x/repos/PTF/rpm/noarch/</code>
         </td></tr></table></dd><dt id="id-1.3.7.5.5.4.3.2.2.2"><span class="term ">
        <span class="quote">“<span class="quote ">x86_64</span>”</span> packages (<code class="filename">*.x86_64.rpm</code>)
       </span></dt><dd><p>
         <code class="filename">/srv/tftpboot/suse-12.3/x86_64/repos/PTF/rpm/x86_64/</code>
        </p></dd><dt id="id-1.3.7.5.5.4.3.2.2.3"><span class="term ">
        <span class="quote">“<span class="quote ">s390x</span>”</span> packages (<code class="filename">*.s390x.rpm</code>)
       </span></dt><dd><p>
         <code class="filename">/srv/tftpboot/suse-12.3/s390x/repos/PTF/rpm/s390x/</code>
        </p></dd></dl></div></li><li class="step "><p>
      Create or update the repository metadata:
     </p><div class="verbatim-wrap"><pre class="screen">createrepo-cloud-ptf</pre></div></li><li class="step "><p>
      The repositories are now set up and are available for all nodes in
      SUSE <span class="productname">OpenStack</span> Cloud except for the Administration Server. In case the PTF also contains
      packages to be installed on the Administration Server, make the
      repository available on the Administration Server as well:
     </p><div class="verbatim-wrap"><pre class="screen">zypper ar -f /srv/tftpboot/suse-12.3/x86_64/repos/PTF PTF</pre></div></li><li class="step "><p>
      To deploy the updates, proceed as described in
      <a class="xref" href="#sec-depl-inst-nodes-post-updater" title="11.4.1. Deploying Node Updates with the Updater Barclamp">Section 11.4.1, “Deploying Node Updates with the Updater Barclamp”</a>. Alternatively, run
      <code class="command">zypper up</code> manually on each node.
     </p></li></ol></div></div></div></div></div></div><div class="part" id="part-depl-poc"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part VI </span><span class="name">Proof of Concepts Deployments </span><a title="Permalink" class="permalink" href="#part-depl-poc">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-deploy-poc"><span class="number">21 </span><span class="name">Building a SUSE <span class="productname">OpenStack</span> Cloud Test lab</span></a></span></dt><dd class="toc-abstract"><p>
    This document will help you to prepare SUSE and prospective customers for a
    Proof of Concept (PoC) deployment of SUSE <span class="productname">OpenStack</span> Cloud. This document provides specific
    details for a PoC deployment. It serves as an addition to the SUSE <span class="productname">OpenStack</span> Cloud
    <em class="citetitle ">Deploying With Crowbar</em>.</p></dd></dl></div><div class="chapter " id="cha-deploy-poc"><div class="titlepage"><div><div><h2 class="title"><span class="number">21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Building a SUSE <span class="productname">OpenStack</span> Cloud Test lab</span> <a title="Permalink" class="permalink" href="#cha-deploy-poc">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>cha-deploy-poc</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-poc-scope"><span class="number">21.1 </span><span class="name">Document Scope</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-features"><span class="number">21.2 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Key Features</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-components"><span class="number">21.3 </span><span class="name">Main Components</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-objectives"><span class="number">21.4 </span><span class="name">Objectives and Preparations</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-matrix"><span class="number">21.5 </span><span class="name">Hardware and Software Matrix</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-topology"><span class="number">21.6 </span><span class="name">Network Topology</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-architecture"><span class="number">21.7 </span><span class="name">Network Architecture</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-services"><span class="number">21.8 </span><span class="name">Services Architecture</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-poc-testcases"><span class="number">21.9 </span><span class="name">Proof of Concept Test Cases</span></a></span></dt></dl></div></div><div class="sect1 " id="sec-depl-poc-scope"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Document Scope</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-scope">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-scope</li></ul></div></div></div></div><p>
    This document will help you to prepare SUSE and prospective customers for a
    Proof of Concept (PoC) deployment of SUSE <span class="productname">OpenStack</span> Cloud. This document provides specific
    details for a PoC deployment. It serves as an addition to the SUSE <span class="productname">OpenStack</span> Cloud
    <em class="citetitle ">Deploying With Crowbar</em>.</p></div><div class="sect1 " id="sec-depl-poc-features"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE <span class="productname">OpenStack</span> Cloud Key Features</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-features">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-features</li></ul></div></div></div></div><p>
    The latest version 8 of SUSE <span class="productname">OpenStack</span> Cloud supports all OpenStack Pike release components for best-in-class capabilities to deploy an open source private cloud. Here is a brief overview of SUSE <span class="productname">OpenStack</span> Cloud features and functionality.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <span class="emphasis"><em>Installation Framework</em></span> Integration with the Crowbar project speeds up and simplifies installation and administration of your physical cloud infrastructure.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Mixed Hypervisor Support</em></span> Enhanced virtualization management through support for multi-hypervisor environments that use KVM, Xen, VMware vSphere, and IBM z/VM.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>High Availability</em></span> Automated deployment and configuration of control plane clusters. This ensures continuous access to business services and delivery of enterprise-grade Service Level Agreements.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>High availability for KVM and Xen Compute Nodes and Workloads</em></span> Enhanced support for critical workloads not designed for cloud architectures.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Docker Support</em></span> Gives the ability to build and run innovative containerized applications through Magnum integration.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Scalability</em></span> Cloud control system designed to grow with your demands.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Open APIs</em></span> Using the standard APIs, customers can enhance and integrate OpenStack with third-party software.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Block Storage Plug-Ins</em></span> A wide range of block storage plug-ins available from storage vendors like EMC, NetApp, and others.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Networking Plug-Ins</em></span> SUSE <span class="productname">OpenStack</span> Cloud natively supports open source SDNs via Open vSwitch, harnessing the power of DPDK in SUSE Linux Enterprise Server 12 SP3. For more flexibility, SUSE <span class="productname">OpenStack</span> Cloud provides support for third-party tools from Cisco, Midokura, Infoblox, Nuage Networks, PLUMgrid and even VLAN bridging solutions.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Award-Winning Support</em></span> SUSE <span class="productname">OpenStack</span> Cloud is backed by 24x7 worldwide-technical support.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Full Integration with SUSE Update Processes</em></span> Easily maintain and patch cloud deployments.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Non-Disruptive Upgrade Capabilities</em></span> Simplify migration to future SUSE <span class="productname">OpenStack</span> Cloud releases.
      </p></li></ul></div></div><div class="sect1 " id="sec-depl-poc-components"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Main Components</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-components">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-components</li></ul></div></div></div></div><p>
      The following is a brief overview of components for setting up and managing SUSE <span class="productname">OpenStack</span> Cloud.
    </p><p>
      <span class="emphasis"><em>Administration Server</em></span> provides all services needed to manage and deploy all other nodes in the cloud. Most of these services are provided by the Crowbar tool. Together with Chef, Crowbar automates all the required installation and configuration tasks. The services provided by the server include DHCP, DNS, NTP, PXE, TFTP.
    </p><p>
      The Administration Server also hosts the software repositories for SUSE Linux Enterprise Server and SUSE <span class="productname">OpenStack</span> Cloud. These repositories are required for node deployment. If no other sources for the software repositories are available, the Administration Server can also host the Subscription Management Tool (SMT), providing up-to-date repositories with updates and patches for all nodes.
    </p><p>
      <span class="emphasis"><em>Control Nodes</em></span> host all OpenStack services for orchestrating virtual machines deployed on the compute nodes. OpenStack in SUSE <span class="productname">OpenStack</span> Cloud uses a PostgreSQL database that is also hosted on the Control Nodes. When deployed, the following OpenStack components run on the Control Nodes:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>MariaDB</p></li><li class="listitem "><p>Image (Glance)</p></li><li class="listitem "><p>Identity (Keystone)</p></li><li class="listitem "><p>Networking (Neutron)</p></li><li class="listitem "><p>Block Storage (Cinder)</p></li><li class="listitem "><p>Shared Storage (Manila)</p></li><li class="listitem "><p>OpenStack Dashboard</p></li><li class="listitem "><p>Keystone</p></li><li class="listitem "><p>Pacemaker</p></li><li class="listitem "><p>Nova controller</p></li><li class="listitem "><p>Message broker</p></li><li class="listitem "><p>Swift proxy server</p></li><li class="listitem "><p>Hawk monitor</p></li><li class="listitem "><p>Heat an orchestration engine</p></li><li class="listitem "><p>Ceilometer server and agents</p></li><li class="listitem "><p>Trove a Database-as-a-Service</p></li></ul></div><p>
      A single Control Node running multiple services can become a performance bottleneck, especially in large SUSE <span class="productname">OpenStack</span> Cloud deployments. It is possible to distribute the services listed above on more than one Control Node. This includes scenarios where each service runs on its own node.
    </p><p>
      <span class="emphasis"><em>Compute Nodes</em></span> are a pool of machines for running instances. These machines require an adequate number of CPUs and enough RAM to start several instances. They also need to provide sufficient storage. A Control Node distributes instances within the pool of compute nodes and provides the necessary network resources. The OpenStack service Compute (Nova) runs on Compute Nodes and provides means for setting up, starting, and stopping virtual machines. SUSE <span class="productname">OpenStack</span> Cloud supports several hypervisors such as KVM, VMware vSphere, and Xen. Each image that can be started with an instance is bound to one hypervisor. Each Compute Node can only run one hypervisor at a time. For a PoC deployment, SUSE recommends to leverage KVM  as hypervisor of choice.
    </p><p>
        <span class="emphasis"><em>Optional Storage Nodes</em></span> Storage Node is a pool of machines that provide object or block storage. Object storage supports several back-ends.
      </p></div><div class="sect1 " id="sec-depl-poc-objectives"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Objectives and Preparations</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-objectives">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-objectives</li></ul></div></div></div></div><p>
    Although each customer has a specific set of requirements, it is important to have 3-5 clearly-defined objectives. This objectives should be provable, measurable and have a specific time scale in which proof is required. The objectives can be adjusted and amended, provided that both parties are agreed on the changes. For a full record of the performed and completed work, it is recommended to use this document for making amendments to the proof requirements.
  </p><p>
      Before deploying SUSE <span class="productname">OpenStack</span> Cloud, it is necessary to meet certain requirements and consider various aspects of the deployment. Some decisions need to be made <span class="emphasis"><em>before</em></span> deploying SUSE <span class="productname">OpenStack</span> Cloud, since they <span class="emphasis"><em>cannot</em></span> be changed afterward.
    </p><p>
      The following procedure covers preparatory steps for the deployment of SUSE <span class="productname">OpenStack</span> Cloud along with the software and hardware components required for a successful implementation.
    </p><div class="procedure " id="id-1.3.8.2.6.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 21.1: </span><span class="name">Prerequisites </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.6.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Make sure that the required hardware and virtual machines are provided and configured
        </p></li><li class="step "><p>
          Check that PXE boot from the first NIC in BIOS is enabled
        </p></li><li class="step "><p>
          Ensure that the hardware is certified for use with SUSE Linux Enterprise Server 12 SP3
        </p></li><li class="step "><p>
          Check that booting from ISO images works
        </p></li><li class="step "><p>
          Make sure that all NICs are visible
        </p></li><li class="step "><p>
          Install <code class="systemitem">sar/sysstat</code> for performance troubleshooting
        </p></li><li class="step "><p>
          Ensure that all needed subscription records are available. Depending on the size of the cloud to be implemented, this includes the following:
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              SUSE <span class="productname">OpenStack</span> Cloud subscriptions
            </p></li><li class="listitem "><p>
              SUSE Linux Enterprise Server subscriptions
            </p></li><li class="listitem "><p>
              SLES High Availability Extensions (HAE) subscriptions
            </p></li><li class="listitem "><p>
              Optional SUSE Enterprise Storage (SES) subscriptions
            </p></li></ul></div></li><li class="step "><p>
          Check whether all needed channels and updates are available either locally or remotely. The following options can be used to provide the repositories and channels:
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              SMT server on the administration server (optional step)
            </p></li><li class="listitem "><p>
              Existing SMT server
            </p></li><li class="listitem "><p>
              Existing SUSE Manager
            </p></li></ul></div></li><li class="step "><p>
          Make sure that networking planed and wired according to the specified layout or topology
        </p></li><li class="step "><p>
          If SUSE Enterprise Storage is a part of the PoC deployment, all nodes must be
installed, configured, and optimized before installing SUSE <span class="productname">OpenStack</span> Cloud. Storage services (Nova, Cinder, Glance, Cluster STONITH,) required by SUSE <span class="productname">OpenStack</span> Cloud 6 must be available and accessible.
        </p></li><li class="step "><p>
          Check whether <code class="systemitem">network.json</code> is configured
	  according to the specific requirements. This step must be discussed
	  and completed in advance. 
        </p></li></ol></div></div></div><div class="sect1 " id="sec-depl-poc-matrix"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware and Software Matrix</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-matrix">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-matrix</li></ul></div></div></div></div><p>The hardware and software matrix below has the following requirements:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        All machines must run SUSE Linux Enterprise Server 12 SP3
      </p></li><li class="listitem "><p>
    KVM or Xen Hypervisors must be running on bare metal
      </p></li><li class="listitem "><p>
        The Admin Node can be deployed on a KVM or VMware virtual machine
      </p></li></ul></div><p>
The sizing recommendation includes an Admin Node (bare metal or VM), Controller Nodes, Compute Nodes to host all your OpenStack services, and the optional SES Nodes. The matrix also provides information on the necessary network equipment and bandwidth requirements.
  </p><div id="id-1.3.8.2.7.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: About Recommendations</h6><p>
      These recommendations are based on real-world use cases and experience gathered by SUSE in the last three years. However, these recommendations are meant to serve as guidelines and not as requirements. The final sizing decision depends on the actual customer workloads and architecture, which must be discussed in depth. The type and number of hardware components such as hard disks, CPU, and RAM also serve as starting points for further discussion and evaluation depending on workloads.
     </p></div><div class="table" id="id-1.3.8.2.7.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 21.1: </span><span class="name">BoM/SUSE <span class="productname">OpenStack</span> Cloud Services </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.7.6">#</a></h6></div><div class="table-contents"><table class="table" summary="BoM/SUSE OpenStack Cloud Services" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /><col class="4" /></colgroup><thead><tr><th><p>Number of Units</p></th><th><p>Function</p></th><th><p>Configuration</p></th><th><p>OpenStack Component</p></th></tr></thead><tbody><tr><td><p>3</p></td><td><p>Compute nodes</p></td><td><p>2 hard disks</p>
              <p>2 Quad Core Intel or AMD processors</p>
              <p>256GB RAM</p>
              <p>2 or 4 10Gb Ethernet NICs</p></td><td><p>Nova-multi-compute</p>
              <p>ML2 Agent</p>
              <p>OVS Agent</p></td></tr><tr><td><p>1</p></td><td><p>Admin Node or VM</p></td><td><p>2 hard disks</p>
              <p>1 Quad Core Intel or AMD processor</p>
              <p>8GB RAM</p>
              <p>2 or 4 10Gb Ethernet NICs</p></td><td><p>Crowbar, tftpboot, PXE</p>
              </td></tr><tr><td><p>3</p></td><td><p>Control node</p></td><td><p>2 hard disks</p>
              <p>2 Quad Core Intel or AMD processors</p>
              <p>2x64GB RAM</p>
              <p>2 or 4 10Gb Ethernet NICs</p></td><td><p>Horizon</p>
              <p>Rabbit MQ</p>
              <p>Nova multi-controller</p>
              <p>Cinder</p>
              <p>Glance</p>
              <p>Heat</p>
              <p>Ceilometer</p>
              <p>Neutron-Server</p>
              <p>ML2 Agent</p>
              <p>Keystone</p>
              <p>MariaDB</p>
              <p>Neutron ML2 Plugin</p>
              <p>L2/L3 Agents</p>
              <p>DHCP Agent</p>
              </td></tr><tr><td></td><td><p>CloudFoundry</p></td><td><p>48 vCPUs</p>
              <p>256GB RAM</p>
              <p>Min 2TB Storage</p>
              </td><td><p>SUSE Cloud Application Platform</p>
              </td></tr><tr><td><p>4</p></td><td><p>Storage Server – SUSE Enterprise Storage</p></td><td><p>2 hard disks</p>
              <p>2  Quad Core Intel or AMD processors</p>
              <p>64GB RAM</p>
              <p>2 or 4  10Gb Ethernet NICs </p>
              </td><td><p>Admin – Server</p>
              <p>MON - Server</p>
              <p>OSD - Server</p>
              </td></tr><tr><td><p>1</p></td><td><p>Switch min. 10 GbE ports</p></td><td></td><td><p>All VLANs/Tagged or Untagged</p>
              </td></tr><tr><td colspan="4" align="center">OS: SUSE Linux Enterprise Server 12 SP3</td></tr><tr><td><p>DHCP, DNS</p></td><td><p>Isolated within administrator network</p></td><td></td><td> </td></tr><tr><td><p>HA</p></td><td><p>3 Control Nodes</p></td><td> </td><td> </td></tr></tbody></table></div></div></div><div class="sect1 " id="sec-depl-poc-topology"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Topology</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-topology">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-topology</li></ul></div></div></div></div><p>
    Configuring and managing your network are <span class="emphasis"><em>two of the most challenging tasks of deploying a SUSE <span class="productname">OpenStack</span> Cloud</em></span>. Therefore they need to be planned carefully. Just as OpenStack provides flexibility and agility for compute and storage, SDN in OpenStack gives cloud administrators more control over their networks. However, building and manually configuring the virtual network infrastructure for OpenStack is difficult and error-prone. SUSE <span class="productname">OpenStack</span> Cloud solve this by delivering a structured installation process for OpenStack which could be customized to adapt the given environment.
  </p><div class="sect2 " id="sec-depl-poc-networkjson"><div class="titlepage"><div><div><h3 class="title"><span class="number">21.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The network.json Network Control File</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-networkjson">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-networkjson</li></ul></div></div></div></div><p>
    The deployment of the network configuration is done while setting up an Administrator Node. As a requirement for the deployment, the entire network configuration needs to be specified in the <code class="systemitem">network.json</code> file.
  </p><p>
    The Crowbar network barclamp provides two functions for the system:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        Initialization of network interfaces on the Crowbar managed systems
      </p></li><li class="listitem "><p>
        Address pool management. While the addresses can be managed with the YaST Crowbar module, complex network setups require to manually edit the network barclamp template file <code class="systemitem">/etc/crowbar/network.json</code>. For more detailed explanation and description see <a class="link" href="https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-deployment/#sec-deploy-network-json-edit" target="_blank">https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-deployment/#sec-deploy-network-json-edit</a>.
      </p></li></ul></div><p>
    The network definitions contain IP address assignments, the bridge and VLAN setup, and settings for the router preference. Each network is also assigned to a logical interface. These VLAN IDs and networks can be modified according to the customer's environment.
  </p></div><div class="sect2 " id="sec-depl-poc-networkmodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">21.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Network Mode</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-networkmodes">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-networkmodes</li></ul></div></div></div></div><p>
    SUSE <span class="productname">OpenStack</span> Cloud supports three network modes: single, dual and team. As of SUSE <span class="productname">OpenStack</span> Cloud 6, the network mode is applied to all nodes and the Administration Server. That means that all machines need to meet the hardware requirements for the chosen mode. The following network modes are available:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <span class="emphasis"><em>Single Network Mode</em></span> In single mode one Ethernet card is used for all the traffic.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Dual Network Mode</em></span> Dual mode needs two Ethernet cards (on
        all nodes but Administration Server). This allows to completely separate traffic to and from the administrator network and to and from the public network.
      </p></li><li class="listitem "><p>
        <span class="emphasis"><em>Team Network Mode</em></span> The team mode is almost identical to single mode, except it combines several Ethernet cards to a so-called bond (network device bonding). Team mode requires two or more Ethernet cards.
      </p></li></ul></div><div id="id-1.3.8.2.8.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Team Network Mode for HA</h6><p>
      In an HA configuration, make sure that SUSE <span class="productname">OpenStack</span> Cloud is deployed with the team network mode.
     </p></div><div class="figure" id="id-1.3.8.2.8.4.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_poc_network_mode.png" target="_blank"><img src="images/depl_poc_network_mode.png" width="" alt="Network Modes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 21.1: </span><span class="name">Network Modes </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.8.4.5">#</a></h6></div></div></div><div class="sect2 " id="sec-depl-poc-defaultlayout"><div class="titlepage"><div><div><h3 class="title"><span class="number">21.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Default Layout</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-defaultlayout">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-defaultlayout</li></ul></div></div></div></div><p>
    The following networks are pre-defined for use with SUSE <span class="productname">OpenStack</span> Cloud.
  </p><div class="table" id="id-1.3.8.2.8.5.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 21.2: </span><span class="name">Administrator Network Layout </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.8.5.3">#</a></h6></div><div class="table-contents"><table class="table" summary="Administrator Network Layout" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th><p>Network Name</p></th><th><p>VLAN</p></th><th><p>IP Range</p></th></tr></thead><tbody><tr><td><p>Router</p></td><td><p>No - untagged</p></td><td><p>192.168.124.1</p></td></tr><tr><td><p>Admin</p></td><td><p>No - untagged</p></td><td><p>192.168.124.10 – 192.168.124.11</p></td></tr><tr><td><p>DHCP</p></td><td><p>No - untagged</p></td><td><p>192.168.124.21 – 192.168.124.80</p></td></tr><tr><td><p>Host</p></td><td><p>No - untagged</p></td><td><p>192.168.124.81 – 192.168.124.160</p></td></tr><tr><td><p>BMC VLAN Host</p></td><td><p>100</p></td><td><p>192.168.124.61</p></td></tr><tr><td><p>BMC Host</p></td><td><p>No - untagged</p></td><td><p>192.168.124.162 – 192.168.124.240</p></td></tr><tr><td><p>Switch</p></td><td><p>No - untagged</p></td><td><p>192.168.124.241 – 192.168.124.250</p></td></tr></tbody></table></div></div><div class="table" id="id-1.3.8.2.8.5.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 21.3: </span><span class="name">Private Network Layout </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.8.5.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Private Network Layout" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th><p>Network Name</p></th><th><p>VLAN</p></th><th><p>IP Range</p></th></tr></thead><tbody><tr><td><p>Router</p></td><td><p>500</p></td><td><p>192.168.123.1 – 192.168.123.49</p></td></tr><tr><td><p>DHCP</p></td><td><p>500</p></td><td><p>192.168.123.50 – 192.168.123.254</p></td></tr></tbody></table></div></div><div class="table" id="id-1.3.8.2.8.5.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 21.4: </span><span class="name">Public/Nova Floating Network Layout/Externally Provided </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.8.5.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Public/Nova Floating Network Layout/Externally Provided" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th><p>Network Name</p></th><th><p>VLAN</p></th><th><p>IP Range</p></th></tr></thead><tbody><tr><td><p>Public Host</p></td><td><p>300</p></td><td><p>192.168.126.2 – 192.168.126.49</p></td></tr><tr><td><p>Public DHCP</p></td><td><p>300</p></td><td><p>192.168.126.50 – 192.168.126.127</p></td></tr><tr><td><p>Floating Host</p></td><td><p>300</p></td><td><p>192.168.126.129 – 192.168.126.191</p></td></tr></tbody></table></div></div><div class="table" id="id-1.3.8.2.8.5.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 21.5: </span><span class="name">Storage Network Layout </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.8.5.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Storage Network Layout" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th><p>Network Name</p></th><th><p>VLAN</p></th><th><p>IP Range</p></th></tr></thead><tbody><tr><td><p>Host</p></td><td><p>200</p></td><td><p>192.168.125.2 – 192.168.125.254</p></td></tr></tbody></table></div></div><p>
 The default IP addresses can be changed using YaST Crowbar module or by editing the appropriate JSON file. It is also possible to customize the network setup for your environment. This can be done by editing the network barclamp template.
</p></div></div><div class="sect1 " id="sec-depl-poc-architecture"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Architecture</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-architecture">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-architecture</li></ul></div></div></div></div><p>
    SUSE <span class="productname">OpenStack</span> Cloud requires a complex network setup consisting of several networks configured during installation. These networks are reserved for cloud usage. Access to these networks from an existing network requires a router.
  </p><div id="id-1.3.8.2.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Network Configuration with Crowbar</h6><p>
      The network configuration on the nodes in the SUSE <span class="productname">OpenStack</span> Cloud network is controlled by Crowbar. Any network configuration changes done outside Crowbar will be automatically overwritten. After the cloud is deployed, network settings cannot be changed without reinstalling the cloud.
    </p></div><p>
    <span class="emphasis"><em>Controller Node</em></span> serves as the front-end for API calls to the compute, image, volume, network, and orchestration services. In addition to that, the node hosts multiple Neutron plug-ins and agents. The node also aggregates all route traffic within tenant and between tenant network and outside world.
  </p><p>
    <span class="emphasis"><em>Compute Node</em></span> creates on-demand virtual machines using chosen hypervisor for customer application.
  </p><p>
    <span class="emphasis"><em>Administrator Node</em></span> automates the installation processes via Crowbar using pre-defined cookbooks for configuring and deploying a Control Node and Compute and Network Nodes.
  </p><div id="id-1.3.8.2.9.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: DHCP/PXE Environment for Administrator Node</h6><p>
      The Administrator Node requires a dedicated local and isolated DHCP/PXE environment controlled by Crowbar.
    </p></div><p>
    <span class="emphasis"><em>Optional storage access</em></span>. Cinder is used for block storage access exposed through iSCSI or NFS (FC connection is not supported in the current release of OpenStack). This could also be a dedicated Storage Node.
  </p><div id="id-1.3.8.2.9.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: High-Performance Network for Ceph</h6><p>
      Implementation of Ceph as a cloud storage requires a high-performance
      network. The Storage Net in the following network topology would allow
      the respective OpenStack services to connect to the external Ceph
      storage cluster public network. It is recommended to run a Ceph storage
      cluster with two networks: a public network and a cluster network. To
      support two networks, each Ceph node must have more than one NIC.
    </p></div><p>
    <span class="emphasis"><em>Network mode</em></span>. What mode to choose for a PoC deployment depends on the High Availability (HA) requirements. The team network mode is required for an HA setup of SUSE <span class="productname">OpenStack</span> Cloud.
  </p><div class="sect2 " id="sec-depl-poc-vlans"><div class="titlepage"><div><div><h3 class="title"><span class="number">21.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Architecture: Pre-Defined VLANs</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-vlans">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-vlans</li></ul></div></div></div></div><p>
    VLAN support for the administrator network must be handled at the switch level. The following networks are predefined when setting up SUSE <span class="productname">OpenStack</span> Cloud. The listed default IP addresses can be changed using the YaST Crowbar module. It is also possible to customize the network setup.
  </p><div id="id-1.3.8.2.9.11.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Limitations of the Default Network Proposal</h6><p>
      The default network proposal described below allows maximum 80 Compute Nodes, 61 floating IP addresses, and 204 addresses in the nova_fixed network. To overcome these limitations, you need to reconfigure the network setup by using appropriate address ranges manually.
    </p></div><div class="figure" id="id-1.3.8.2.9.11.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_network_arch.png" target="_blank"><img src="images/depl_network_arch.png" width="" alt="Network Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 21.2: </span><span class="name">Network Architecture </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.9.11.4">#</a></h6></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.8.2.9.11.5.1"><span class="term ">
        Administrator Network (192.168.124/24)
      </span></dt><dd><p>
          A private network to access the Administration Server and all nodes for administration purposes. The default setup lets you also access the Baseboard Management Controller (BMC) data via Intelligent Platform Management Interface (IPMI) from this network. If required, BMC access can be swapped to a separate network.
        </p><p>
          You have the following options for controlling access to this network:
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              Do not allow access from the outside and keep the administrator network completely separated
            </p></li><li class="listitem "><p>
              Allow access to the administration server from a single network (for example, your company's administration network) via the “bastion network” option configured on an additional network card with a fixed IP address
            </p></li><li class="listitem "><p>
              Allow access from one or more networks via a gateway
            </p></li></ul></div></dd><dt id="id-1.3.8.2.9.11.5.2"><span class="term ">
        Storage Network (192.168.125/24)
      </span></dt><dd><p>
          Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used by Ceph and Swift only. It should not be accessed by users.
        </p></dd><dt id="id-1.3.8.2.9.11.5.3"><span class="term ">
        Private Network (nova-fixed, 192.168.123/24)
      </span></dt><dd><p>
          Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used for communication between instances and provides them with access to the outside world. SUSE <span class="productname">OpenStack</span> Cloud automatically provides the required gateway.
        </p></dd><dt id="id-1.3.8.2.9.11.5.4"><span class="term ">
        Public Network (nova-floating, public, 192.168.126/24)
      </span></dt><dd><p>
          The only public network provided by SUSE <span class="productname">OpenStack</span> Cloud. On this network, you can access the Nova Dashboard and all instances (provided they are equipped with a floating IP). This network can only be accessed via a gateway that needs to be provided
externally. All SUSE <span class="productname">OpenStack</span> Cloud users and administrators need to be able to access the public network.
        </p></dd><dt id="id-1.3.8.2.9.11.5.5"><span class="term ">
        Software Defined Network (os_sdn, 192.168.130/24)
      </span></dt><dd><p>
          Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used when Neutron is configured to use <code class="systemitem">openvswitch</code> with GRE tunneling for the virtual networks. It should not be accessed by users.
        </p><p>
          SUSE <span class="productname">OpenStack</span> Cloud supports different network modes: single, dual, and team. Starting with SUSE <span class="productname">OpenStack</span> Cloud 6, the networking mode is applied to all nodes and the Administration Server. This means that all machines need to meet the hardware requirements for the chosen mode. The network mode can be configured using the YaST Crowbar module (see <a class="link" href="https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-deployment/#sec-depl-adm-inst-crowbar" target="_blank">https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-deployment/#sec-depl-adm-inst-crowbar</a>). The network mode cannot be changed after the cloud is deployed.
        </p><p>
          More flexible network mode setups can be configured by editing the Crowbar
          network configuration files (see <a class="link" href="https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-deployment/#sec-deploy-network-json-edit" target="_blank">https://documentation.suse.com/soc/8/single-html/suse-openstack-cloud-deployment/#sec-deploy-network-json-edit</a> for more information). SUSE or a partner can assist you in creating a custom setup within the scope of a consulting services agreement.
          For more information on SUSE consulting, visit <a class="link" href="http://www.suse.com/consulting/" target="_blank">http://www.suse.com/consulting/</a>.
        </p></dd></dl></div><div id="id-1.3.8.2.9.11.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Team Network Mode Is Required for HA</h6><p>
      Team network mode is required for an HA setup of SUSE <span class="productname">OpenStack</span> Cloud. If you are
      planning to move your cloud to an HA setup later, deploy SUSE <span class="productname">OpenStack</span> Cloud with
      team network mode right from the beginning. Migration to an HA setup is not supported.
    </p></div></div></div><div class="sect1 " id="sec-depl-poc-services"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Services Architecture</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-services">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-services</li></ul></div></div></div></div><p>
    SUSE <span class="productname">OpenStack</span> Cloud is based on SUSE Linux Enterprise Server 12 SP3, OpenStack, Crowbar and Chef. SUSE Linux Enterprise Server is used as the underlying operating system for all infrastructure nodes. Crowbar and Chef are used to automatically deploy and manage the OpenStack nodes from a central Administration Server.
  </p><div class="figure" id="id-1.3.8.2.10.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_poc_services_architecture.png" target="_blank"><img src="images/depl_poc_services_architecture.png" width="" alt="Services Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 21.3: </span><span class="name">Services Architecture </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.10.3">#</a></h6></div></div></div><div class="sect1 " id="sec-depl-poc-testcases"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Proof of Concept Test Cases</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-testcases">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-testcases</li></ul></div></div></div></div><p>
    After you have successfully deployed OpenStack, you need to test the environment by using either the Dashboard or the command line interface. This document provides the most important procedures and steps to perform functional tests agreed upon. A detailed list of test case should be provided with this document.
  </p><div id="id-1.3.8.2.11.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: About Test Cases</h6><p>
      All test cases are work in progress and by no means complete. Test cases
      have to be formulated by the entire team according to the requirements and type of workloads.
    </p></div><div class="sect2 " id="sec-depl-poc-testbasic"><div class="titlepage"><div><div><h3 class="title"><span class="number">21.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Basic Test Cases</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-testbasic">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-testbasic</li></ul></div></div></div></div><p>
    Add your own test cases here.
  </p></div><div class="sect2 " id="sec-depl-poc-testadvanced"><div class="titlepage"><div><div><h3 class="title"><span class="number">21.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Advanced Test Cases</span> <a title="Permalink" class="permalink" href="#sec-depl-poc-testadvanced">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_poc.xml</li><li><span class="ds-label">ID: </span>sec-depl-poc-testadvanced</li></ul></div></div></div></div><p>
    Add your own test cases here.
  </p></div></div></div></div><div class="appendix " id="app-deploy-vmware"><div class="titlepage"><div><div><h1 class="title"><span class="number">A </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VMware vSphere Installation Instructions</span> <a title="Permalink" class="permalink" href="#app-deploy-vmware">#</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_vmware.xml</li><li><span class="ds-label">ID: </span>app-deploy-vmware</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#app-deploy-vmware-requirements"><span class="number">A.1 </span><span class="name">Requirements</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-vmware-vcenter"><span class="number">A.2 </span><span class="name">Preparing the VMware vCenter Server</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-vmware-compnode"><span class="number">A.3 </span><span class="name">Finishing the Nova Compute VMware Node Installation</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-vmware-ha"><span class="number">A.4 </span><span class="name">Making the Nova Compute VMware Node Highly Available</span></a></span></dt></dl></div></div><p>
  SUSE <span class="productname">OpenStack</span> Cloud supports the Nova Compute VMware vCenter driver. It
  enables access to advanced features such as vMotion, High Availability,
  and Dynamic Resource Scheduling (DRS). However, VMware vSphere is not
  supported <span class="quote">“<span class="quote ">natively</span>”</span> by SUSE <span class="productname">OpenStack</span> Cloud—it rather
  delegates requests to an existing vCenter. It requires preparations at the
  vCenter and post install adjustments of the Compute Node.
 </p><div class="sect1 " id="app-deploy-vmware-requirements"><div class="titlepage"><div><div><h2 class="title"><span class="number">A.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="#app-deploy-vmware-requirements">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_vmware.xml</li><li><span class="ds-label">ID: </span>app-deploy-vmware-requirements</li></ul></div></div></div></div><p>
   The following requirements must be met to successfully deploy a
   Nova Compute VMware node:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     VMware vSphere vCenter 6.0 or higher
    </p></li><li class="listitem "><p>
     VMware vSphere ESXi nodes 6.0 or higher
    </p></li><li class="listitem "><p>
     A separate Compute Node that acts as a proxy to vCenter is required.
     Minimum system requirements for this node are:
    </p><table border="0" summary="Simple list" class="simplelist "><tr><td>CPU: x86_64 with 2 cores (4 recommended)</td></tr><tr><td>RAM: 2 GB (8 GB recommended)</td></tr><tr><td>Disk space: 4 GB (30 GB recommended)</td></tr></table><p>
     See <a class="xref" href="#app-deploy-vmware-compnode" title="A.3. Finishing the Nova Compute VMware Node Installation">Section A.3, “Finishing the Nova Compute VMware Node Installation”</a> for setup
     instructions.
    </p></li><li class="listitem "><p>
     Neutron must not be deployed with the <code class="literal">openvswitch with
     gre</code> plug-in, a VLAN setup is required.
    </p></li></ul></div></div><div class="sect1 " id="app-deploy-vmware-vcenter"><div class="titlepage"><div><div><h2 class="title"><span class="number">A.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparing the VMware vCenter Server</span> <a title="Permalink" class="permalink" href="#app-deploy-vmware-vcenter">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_vmware.xml</li><li><span class="ds-label">ID: </span>app-deploy-vmware-vcenter</li></ul></div></div></div></div><p>
   SUSE <span class="productname">OpenStack</span> Cloud requires the VMware vCenter server to run version 5.1 or
   better. You need to create a single data center for SUSE <span class="productname">OpenStack</span> Cloud (multiple
   data centers are currently not supported):
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the vCenter Server using the vSphere Web Client
    </p></li><li class="step "><p>
     Choose <span class="guimenu ">Hosts and Clusters</span> and create a single
     <span class="guimenu ">Datacenter</span>
    </p></li><li class="step "><p>
     Set up a <span class="guimenu ">New Cluster</span> which has
     <span class="guimenu ">DRS</span> enabled.
    </p></li><li class="step "><p>
     Set <span class="guimenu ">Automation Level</span> to <code class="literal">Fully
     Automated</code> and <span class="guimenu ">Migration Threshold</span> to
     <code class="literal">Aggressive</code>.
    </p></li><li class="step "><p>
     Create shared storage. Only shared storage is supported and data stores
     must be shared among all hosts in a cluster. It is recommended to
     remove data stores not intended for <span class="productname">OpenStack</span> from clusters being
     configured for <span class="productname">OpenStack</span>. Multiple data stores can be used
     per cluster.
    </p></li><li class="step "><p>
     Create a port group with the same name as the
     <code class="envar">vmware.integration_bridge</code> value in
     <code class="filename">nova.conf</code> (default is br-int). All VM NICs are
     attached to this port group for management by the <span class="productname">OpenStack</span>
     networking plug-in. Assign the same VLAN ID as for the neutron network.
     On the default network setup this is the same VLAN ID as for the
     <code class="literal">nova_fixed</code> network. Use
     <span class="guimenu ">YaST</span> › <span class="guimenu ">Miscellaneous</span> › <span class="guimenu ">Crowbar</span> › <span class="guimenu ">Networks</span> to look up the VLAN ID.
    </p></li></ol></div></div></div><div class="sect1 " id="app-deploy-vmware-compnode"><div class="titlepage"><div><div><h2 class="title"><span class="number">A.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Finishing the Nova Compute VMware Node Installation</span> <a title="Permalink" class="permalink" href="#app-deploy-vmware-compnode">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_vmware.xml</li><li><span class="ds-label">ID: </span>app-deploy-vmware-compnode</li></ul></div></div></div></div><p>
   Deploy Nova as described in <a class="xref" href="#sec-depl-ostack-nova" title="12.11. Deploying Nova">Section 12.11, “Deploying Nova”</a>
   on a single Compute Node and fill in the <span class="guimenu ">VMware vCenter
   Settings</span> attributes:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.9.6.3.1"><span class="term "><span class="guimenu ">vCenter IP Address</span>
    </span></dt><dd><p>
      IP address of the vCenter server.
     </p></dd><dt id="id-1.3.9.6.3.2"><span class="term "><span class="guimenu ">vCenter Username</span> / <span class="guimenu ">vCenter
     Password</span>
    </span></dt><dd><p>
      vCenter login credentials.
     </p></dd><dt id="id-1.3.9.6.3.3"><span class="term "><span class="guimenu ">Cluster Names</span></span></dt><dd><p>
      A comma-separated list of cluster names you have added on the vCenter
      server.
     </p></dd><dt id="id-1.3.9.6.3.4"><span class="term "><span class="guimenu ">Regex to match the name of a datastore</span></span></dt><dd><p>Regular expression to match the name of a data store. If you have
      several data stores, this option allows you to specify the
      data stores to use with Nova Compute. For example, the value <code class="literal">nas.*</code> selects
      all data stores that have a name starting with <code class="literal">nas</code>. If
      this option is omitted, Nova Compute uses the first data store returned by the vSphere
      API. However, it is recommended not to use this option and to remove data
      stores that are not intended for <span class="productname">OpenStack</span> instead.</p></dd><dt id="id-1.3.9.6.3.5"><span class="term "><span class="guimenu ">VLAN Interface</span></span></dt><dd><p>
      The physical interface that is to be used for VLAN networking. The
      default value of <code class="literal">vmnic0</code> references the first
      available interface (<span class="quote">“<span class="quote ">eth0</span>”</span>). <code class="literal">vmnic1</code>
      would be the second interface (<span class="quote">“<span class="quote ">eth1</span>”</span>).
     </p></dd><dt id="id-1.3.9.6.3.6"><span class="term ">
     <span class="guimenu ">CA file for verifying the vCenter certificate</span>
    </span></dt><dd><p>
      Absolute path to the vCenter CA certificate.
     </p></dd><dt id="id-1.3.9.6.3.7"><span class="term ">
     <span class="guimenu ">
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </span>
    </span></dt><dd><p>
      Default value: <code class="literal">false</code> (the CA truststore is used for verification).
      Set this option to <code class="literal">true</code> when using self-signed certificates to disable
      certificate checks. This setting is for testing purposes only and must not be used in
      production environments!
     </p></dd></dl></div><div class="figure" id="id-1.3.9.6.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nova_vmware.png" target="_blank"><img src="images/depl_barclamp_nova_vmware.png" width="" alt="The Nova barclamp: VMware Configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure A.1: </span><span class="name">The Nova barclamp: VMware Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.9.6.4">#</a></h6></div></div></div><div class="sect1 " id="app-deploy-vmware-ha"><div class="titlepage"><div><div><h2 class="title"><span class="number">A.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making the Nova Compute VMware Node Highly Available</span> <a title="Permalink" class="permalink" href="#app-deploy-vmware-ha">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_vmware.xml</li><li><span class="ds-label">ID: </span>app-deploy-vmware-ha</li></ul></div></div></div></div><p>
   <span class="productname">OpenStack</span> does not support deploying multiple VMware Compute Nodes. As
   a workaround, set up an instance on the vSphere Cluster, register it
   with Crowbar and deploy the
   <code class="literal">nova-compute-vmware</code> role on this node:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create an instance on the vSphere Cluster and install SUSE Linux Enterprise Server 12 SP3.
    </p></li><li class="step "><p>
     Configure a network interface in a way that it can access the
     SUSE <span class="productname">OpenStack</span> Cloud admin network.
    </p></li><li class="step "><p>
     Enable the High-Availability flag in vCenter for this instance.
    </p></li><li class="step "><p>
     Follow the instructions at
     <a class="xref" href="#sec-depl-inst-nodes-install-external" title="11.3. Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE OpenStack Cloud Nodes">Section 11.3, “Converting Existing SUSE Linux Enterprise Server 12 SP3 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes”</a> to register the
     instance with the Administration Server and add it to the pool of nodes
     available for deployment.
    </p></li><li class="step "><p>
     Deploy the <code class="literal">nova-compute-vmware</code> role on the new
     node as described in <a class="xref" href="#sec-depl-ostack-nova" title="12.11. Deploying Nova">Section 12.11, “Deploying Nova”</a> and
     <a class="xref" href="#app-deploy-vmware-compnode" title="A.3. Finishing the Nova Compute VMware Node Installation">Section A.3, “Finishing the Nova Compute VMware Node Installation”</a>.
    </p></li></ol></div></div></div></div><div class="appendix " id="app-deploy-cisco"><div class="titlepage"><div><div><h1 class="title"><span class="number">B </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Cisco Nexus Switches with Neutron</span> <a title="Permalink" class="permalink" href="#app-deploy-cisco">#</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_cisco.xml</li><li><span class="ds-label">ID: </span>app-deploy-cisco</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#app-deploy-cisco-requirements"><span class="number">B.1 </span><span class="name">Requirements</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-cisco-deploy"><span class="number">B.2 </span><span class="name">Deploying Neutron with the Cisco Plugin</span></a></span></dt></dl></div></div><div class="sect1 " id="app-deploy-cisco-requirements"><div class="titlepage"><div><div><h2 class="title"><span class="number">B.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="#app-deploy-cisco-requirements">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_cisco.xml</li><li><span class="ds-label">ID: </span>app-deploy-cisco-requirements</li></ul></div></div></div></div><p>
   The following requirements must be met to use Cisco Nexus switches with
   Neutron:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Cisco Nexus series 3000, 5000 or 7000
    </p></li><li class="listitem "><p>
     All Compute Nodes must be equipped with at least two network cards.
    </p></li><li class="listitem "><p>
     The switch needs to have the XML management interface enabled. SSH
     access to the management interface must be enabled (refer to the
     switch's documentation for details).
    </p></li><li class="listitem "><p>
     Enable VLAN trunking for all Neutron managed VLANs on the switch
     port to which the controller node running Neutron is connected to.
    </p></li><li class="listitem "><p>
     Before deploying Neutron, check if VLAN configurations for Neutron managed
     VLANs already exist on the switch (for example, from a previous SUSE <span class="productname">OpenStack</span> Cloud
     deployment). If yes, delete them via the switch's management interface prior to
     deploying Neutron.
    </p></li><li class="listitem "><p>
     When using the Cisco plugin, Neutron reconfigures the VLAN trunk
     configuration on all ports used for the <code class="literal">nova-fixed</code>
     traffic (the traffic between the instances). This requires to
     configure separate network interfaces exclusively used by
     <code class="literal">nova-fixed</code>. This can be achieved by adjusting
     <code class="filename">/etc/crowbar/network.json</code> (refer to
     <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a>). The following example shows
     an appropriate configuration for dual mode, where
     <span class="guimenu ">nova-fixed</span> has been mapped to conduit
     <span class="guimenu ">intf1</span> and all other networks to other conduits.
     Configuration attributes not relevant in this context have been
     replaced with <code class="literal">...</code>.
    </p><div class="example" id="id-1.3.10.4.3.6.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example B.1: </span><span class="name">Exclusively Mapping <span class="guimenu ">nova-fixed</span> to conduit <span class="guimenu ">intf1</span> in dual mode </span><a title="Permalink" class="permalink" href="#id-1.3.10.4.3.6.2">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "conduit_map" : [
            <em class="replaceable ">...</em>
         ],
         "mode" : "single",
         "networks" : {
            "nova_fixed" : {
              <em class="replaceable ">...</em>,
              "conduit" : "intf1"
            },
            "nova_floating" : {
              <em class="replaceable ">...</em>,
              "conduit" : "intf0"
            },
            "public" : {
              <em class="replaceable ">...</em>,
              "conduit" : "intf0"
            },
            "storage" : {
              <em class="replaceable ">...</em>,
              "conduit" : "intf0"
            },
            "os_sdn" : {
              <em class="replaceable ">...</em>,
              "conduit" : "intf0"
            },
            "admin" : {
              <em class="replaceable ">...</em>,
              "conduit" : "intf0"
            },
            "bmc" : {
              <em class="replaceable ">...</em>,
              "conduit" : "bmc"
            },
            "bmc_vlan" : {
              <em class="replaceable ">...</em>,
              "conduit" : "intf2"
            },
         },
         <em class="replaceable ">...</em>,
      },
   }
}</pre></div></div></div></li><li class="listitem "><p>
     Make a note of all switch ports to which the interfaces using the
     <code class="literal">nova-fixed</code> network on the Compute Nodes are
     connected. This information will be needed when deploying Neutron.
    </p></li></ul></div></div><div class="sect1 " id="app-deploy-cisco-deploy"><div class="titlepage"><div><div><h2 class="title"><span class="number">B.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Neutron with the Cisco Plugin</span> <a title="Permalink" class="permalink" href="#app-deploy-cisco-deploy">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_cisco.xml</li><li><span class="ds-label">ID: </span>app-deploy-cisco-deploy</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a Neutron barclamp proposal in the Crowbar Web
     interface.
    </p></li><li class="step "><p>
     As the <span class="guimenu ">Plugin</span>, select <code class="literal">ml2</code>.
    </p></li><li class="step "><p>
     As <span class="guimenu ">Modular Layer 2 mechanism drive</span>, select
     <code class="literal">cisco_nexus</code>.
    </p></li><li class="step "><p>
     In <span class="guimenu ">Modular Layer2 type drivers</span>, select
     <code class="literal">vlan</code>.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Cisco Switch Credentials</span> table, enter the 
     <span class="guimenu ">IP Address</span>, the SSH
     <span class="guimenu ">Port</span> number and the login credentials for the
     switch's management interface. If you have multiple switches, open a
     new row in the table by clicking <span class="guimenu ">Add</span> and enter the
     data for another switch.
    </p><div class="figure" id="id-1.3.10.5.2.5.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_network_cisco.png" target="_blank"><img src="images/depl_barclamp_network_cisco.png" width="" alt="The Neutron barclamp: Cisco Plugin" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure B.1: </span><span class="name">The Neutron barclamp: Cisco Plugin </span><a title="Permalink" class="permalink" href="#id-1.3.10.5.2.5.2">#</a></h6></div></div></li><li class="step "><p>
     Choose whether to encrypt public communication
     (<span class="guimenu ">HTTPS</span>) or not (<span class="guimenu ">HTTP</span>). If
     choosing <span class="guimenu ">HTTPS</span>, refer to
     <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration
     details.
    </p></li><li class="step "><p>
     Choose a node for deployment and <span class="guimenu ">Apply</span> the proposal.
    </p></li><li class="step "><p>
     Deploy Nova (see <a class="xref" href="#sec-depl-ostack-nova" title="12.11. Deploying Nova">Section 12.11, “Deploying Nova”</a>),
     Horizon (see <a class="xref" href="#sec-depl-ostack-dash" title="12.12. Deploying Horizon (OpenStack Dashboard)">Section 12.12, “Deploying Horizon (<span class="productname">OpenStack</span> Dashboard)”</a> and all other
     remaining barclamps.
    </p></li><li class="step "><p>
     When all barclamps have been deployed, return to the Neutron
     barclamp by choosing <span class="guimenu ">Barclamps</span> › <span class="guimenu ">OpenStack</span> › <span class="guimenu ">Neutron</span> › <span class="guimenu ">Edit</span>. The proposal now contains an
     additional table named <span class="guimenu ">Assign Switch Ports</span>, listing
     all Compute Nodes.
    </p><p>
     For each Compute Node enter the switch it is connected to and the port
     number from the notes you took earlier. The values need to be entered
     like the following: <code class="literal">1/13</code> or
     <code class="literal">Eth1/20</code>.
    </p></li><li class="step "><p>
     When you have entered the data for all Compute Nodes, re-apply the
     proposal.
    </p><div id="id-1.3.10.5.2.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Deploying Additional Compute Nodes</h6><p>
      Whenever you deploy additional Compute Nodes to an active SUSE <span class="productname">OpenStack</span> Cloud
      deployment using the Cisco plugin with Neutron, update
      the Neutron barclamp proposal by entering their port data as
      described in the previous step.
     </p></div></li></ol></div></div><div id="id-1.3.10.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: Verifying the Setup</h6><p>
    To verify if Neutron was correctly deployed, do the following:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Launch an instance (refer to the <em class="citetitle ">End User Guide</em>, chapter
      <em class="citetitle ">Launch and manage instances</em>

      for instructions).
     </p></li><li class="listitem "><p>
      Find out which VLAN was assigned to the network by running the command
      <code class="command">neutron net-show fixed</code>. The result lists a
      <span class="guimenu ">segmentation_id</span> matching the VLAN.
     </p></li><li class="listitem "><p>
      Log in to the switch's management interface and list the VLAN
      configuration. If the setup was deployed correctly, the port of the
      Compute Node the instance is running on, is in trunk mode for the
      matching VLAN.
     </p></li></ol></div></div></div></div><div class="appendix " id="app-deploy-docupdates"><div class="titlepage"><div><div><h1 class="title"><span class="number">C </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Documentation Updates</span> <a title="Permalink" class="permalink" href="#app-deploy-docupdates">#</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_docupdates.xml</li><li><span class="ds-label">ID: </span>app-deploy-docupdates</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-deploy-docupdates-c8-gm"><span class="number">C.1 </span><span class="name">April 2018 (Initial Release <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8)</span></a></span></dt></dl></div></div><p>
  This chapter lists content changes for this document since the release of
  <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8.0.
 </p><p>
  This manual was updated on the following dates:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#sec-deploy-docupdates-c8-gm" title="C.1. April 2018 (Initial Release SUSE OpenStack Cloud Crowbar 8)">Section C.1, “April 2018 (Initial Release <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8)”</a>
   </p></li></ul></div><div class="sect1 " id="sec-deploy-docupdates-c8-gm"><div class="titlepage"><div><div><h2 class="title"><span class="number">C.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">April 2018 (Initial Release <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8)</span> <a title="Permalink" class="permalink" href="#sec-deploy-docupdates-c8-gm">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_docupdates.xml</li><li><span class="ds-label">ID: </span>sec-deploy-docupdates-c8-gm</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.11.6.2.1"><span class="term ">Bugfixes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        In <a class="xref" href="#sec-depl-ostack-glance" title="12.8. Deploying Glance">Section 12.8, “Deploying Glance”</a>, corrected the name of
        an example Nova configuration file for custom settings (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1077947" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1077947</a>).
       </p></li><li class="listitem "><p>
        In <a class="xref" href="#sec-depl-ostack-glance" title="12.8. Deploying Glance">Section 12.8, “Deploying Glance”</a>, updated the entries of a
        drop-down box (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073333" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073333</a>).
       </p></li><li class="listitem "><p>
        Numerous small fixes and corrections throughout the document (<a class="link" href="http://bugzilla.suse.com/show_bug.cgi?id=1073508" target="_blank">http://bugzilla.suse.com/show_bug.cgi?id=1073508</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073516" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073516</a>).
       </p></li></ul></div></dd></dl></div></div></div><div class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="number"> </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Glossary of Terminology and Product Names</span> <a title="Permalink" class="permalink" href="#gl-cloud">#</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>glossary.xml</li><li><span class="ds-label">ID: </span>book-deployment</li></ul></div></div></div></div><div class="line"></div><dl><dt id="gloss-act-act"><span><span class="glossterm">Active/Active</span> <a title="Permalink" class="permalink" href="#gloss-act-act">#</a></span></dt><dd class="glossdef"><p>
    A concept of how services are running on nodes in a High Availability cluster. In
    an active/active setup, both the main and redundant systems are managed
    concurrently. If a failure of services occurs, the redundant system is
    already online, and can take over until the main system is fixed and
    brought back online.
   </p></dd><dt id="gloss-act-pass"><span><span class="glossterm">Active/Passive</span> <a title="Permalink" class="permalink" href="#gloss-act-pass">#</a></span></dt><dd class="glossdef"><p>
    A concept of how services are running on nodes in a High Availability cluster. In
    an active/passive setup, one or more services are running on an active
    cluster node, whereas the passive node stands by. If the active node fails then the services are transferred to the passive node.
   </p></dd><dt id="id-1.3.12.5"><span><span class="glossterm">Administration Server</span> <a title="Permalink" class="permalink" href="#id-1.3.12.5">#</a></span></dt><dd class="glossdef"><p>
    Also called Crowbar Administration Node. Manages all other nodes. It
    assigns IP addresses to them, boots them using PXE, configures them, and
    provides them the necessary software for their roles. To provide these
    services, the Administration Server runs Crowbar, Chef, DHCP, TFTP, NTP, and other
    services.
   </p></dd><dt id="id-1.3.12.6"><span><span class="glossterm">AMI (Amazon Machine Image)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.6">#</a></span></dt><dd class="glossdef"><p>
    A virtual machine that can be created and customized by a user. AMIs can
    be identified by an ID prefixed with <code class="literal">ami-</code>.
   </p></dd><dt id="gloss-az"><span><span class="glossterm">Availability Zone</span> <a title="Permalink" class="permalink" href="#gloss-az">#</a></span></dt><dd class="glossdef"><p>
    An <span class="productname">OpenStack</span> method of partitioning clouds. It enables you to arrange
    <span class="productname">OpenStack</span> Compute hosts into logical groups. The groups typically have
    physical isolation and redundancy from other availability zones, for
    example, by using separate power supply or network equipment for each
    zone. When users provision resources, they can specify from which
    availability zone their instance should be created. This allows cloud
    consumers to ensure that their application resources are spread across
    disparate machines to achieve high availability if the hardware fails.
    Since the Grizzly release, availability zones are implemented via host
    aggregates.

   </p></dd><dt id="id-1.3.12.8"><span><span class="glossterm">AWS (Amazon Web Services)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.8">#</a></span></dt><dd class="glossdef"><p>
    A collection of remote computing services (including Amazon EC2, Amazon
    S3, and others) that together make up Amazon's cloud computing platform.
   </p></dd><dt id="id-1.3.12.9"><span><span class="glossterm">Barclamp</span> <a title="Permalink" class="permalink" href="#id-1.3.12.9">#</a></span></dt><dd class="glossdef"><p>
    A set of Chef cookbooks, templates, and other logic. Used to apply
    a particular Chef role to individual nodes or a set of nodes.

   </p></dd><dt id="id-1.3.12.10"><span><span class="glossterm">Ceilometer</span> <a title="Permalink" class="permalink" href="#id-1.3.12.10">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-ceilo" title="Telemetry">Telemetry</a>.
   </p></dd><dt id="id-1.3.12.11"><span><span class="glossterm">Cell</span> <a title="Permalink" class="permalink" href="#id-1.3.12.11">#</a></span></dt><dd class="glossdef"><p>
    Cells provide a new way to scale Compute deployments. This includes the
    ability to have compute clusters (cells) in different geographic
    locations all under the same Compute API. This allows for a single API
    server being used to control access to multiple cloud installations.
    Cells provide logical partitioning of Compute resources in a
    child/parent relationship.
   </p></dd><dt id="id-1.3.12.13"><span><span class="glossterm">Ceph</span> <a title="Permalink" class="permalink" href="#id-1.3.12.13">#</a></span></dt><dd class="glossdef"><p>
    A massively scalable, open source, distributed storage system. It
    consists of an object store, a block store, and a POSIX-compliant
    distributed file system.
   </p></dd><dt id="id-1.3.12.12"><span><span class="glossterm">Chef</span> <a title="Permalink" class="permalink" href="#id-1.3.12.12">#</a></span></dt><dd class="glossdef"><p>
    An automated configuration management platform for deployment of your
    entire cloud infrastructure. The Chef server manages many of the
    software packages and allows the easy changing of nodes.
   </p></dd><dt id="id-1.3.12.14"><span><span class="glossterm">Cinder</span> <a title="Permalink" class="permalink" href="#id-1.3.12.14">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-cinder" title="OpenStack Block Storage"><span class="productname">OpenStack</span> Block Storage</a>.
   </p></dd><dt id="id-1.3.12.20"><span><span class="glossterm">cloud-init</span> <a title="Permalink" class="permalink" href="#id-1.3.12.20">#</a></span></dt><dd class="glossdef"><p>
    A package commonly installed in virtual machine images. It uses the SSH
    public key to initialize an instance after boot.
   </p></dd><dt id="id-1.3.12.15"><span><span class="glossterm">Cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.12.15">#</a></span></dt><dd class="glossdef"><p>
    A set of connected computers that work together. In many respects (and
    from the outside) they can be viewed as a single system. Clusters can be
    further categorized depending on their purpose, for example: High Availability
    clusters, high-performance clusters, or load-balancing clusters.
   </p></dd><dt id="gloss-partition"><span><span class="glossterm">Cluster Partition</span> <a title="Permalink" class="permalink" href="#gloss-partition">#</a></span></dt><dd class="glossdef"><p>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs: The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    <a class="xref" href="#gloss-splitbrain" title="Split Brain">Split Brain</a> scenario develops.
   </p></dd><dt id="gloss-crm"><span><span class="glossterm">Cluster Resource Manager</span> <a title="Permalink" class="permalink" href="#gloss-crm">#</a></span></dt><dd class="glossdef"><p>
    The main management entity in a High Availability cluster responsible for
    coordinating all non-local interactions. The
    <a class="xref" href="#gloss-hasi" title="SUSE Linux Enterprise High Availability Extension">SUSE Linux Enterprise High Availability Extension</a> uses Pacemaker as CRM. Each node of the
    cluster has its own CRM instance. The instance running on the
    <a class="xref" href="#gloss-dc" title="Designated Coordinator (DC)">Designated Coordinator (DC)</a> is the one elected to relay decisions to the
    other non-local CRMs and to process their input.
   </p></dd><dt id="id-1.3.12.21"><span><span class="glossterm">Compute Node</span> <a title="Permalink" class="permalink" href="#id-1.3.12.21">#</a></span></dt><dd class="glossdef"><p>
    Node within a SUSE <span class="productname">OpenStack</span> Cloud. A physical server running a Hypervisor. A
    Compute Node is a host for guest virtual machines that are deployed in
    the cloud. It starts virtual machines on demand using
    <code class="literal">nova-compute</code>. To split virtual machine load across
    more than one server, a cloud should contain multiple Compute Nodes.
   </p></dd><dt id="id-1.3.12.18"><span><span class="glossterm">Container</span> <a title="Permalink" class="permalink" href="#id-1.3.12.18">#</a></span></dt><dd class="glossdef"><p>
    A container is a storage compartment for data. It can be thought of as a
    directory, only that it cannot be nested.
   </p></dd><dt id="id-1.3.12.22"><span><span class="glossterm">Control Node</span> <a title="Permalink" class="permalink" href="#id-1.3.12.22">#</a></span></dt><dd class="glossdef"><p>
    Node within a SUSE <span class="productname">OpenStack</span> Cloud. The Control Node is configured through the
    Administration Server and registers with the Administration Server for all required
    software. Hosts the <span class="productname">OpenStack</span> API endpoints and the <span class="productname">OpenStack</span>
    scheduler and runs the <code class="literal">nova</code> services—except
    for <code class="literal">nova-compute</code>, which is run on the Compute Nodes.
    The Control Node coordinates everything about cloud virtual machines:
    like a central communication center it receives all requests (for
    example, if a user wants to start or stop a virtual machine). It
    communicates with the Compute Nodes to coordinate fulfillment of the
    request. A cloud can contain multiple Control Nodes.
   </p></dd><dt id="id-1.3.12.23"><span><span class="glossterm">Cookbook</span> <a title="Permalink" class="permalink" href="#id-1.3.12.23">#</a></span></dt><dd class="glossdef"><p>
    A collection of Chef recipes which deploy a software stack or
    functionality. The unit of distribution for Chef.
   </p></dd><dt id="id-1.3.12.19"><span><span class="glossterm">Corosync</span> <a title="Permalink" class="permalink" href="#id-1.3.12.19">#</a></span></dt><dd class="glossdef"><p>
    The messaging/infrastructure layer used in a High Availability cluster that is set
    up with SUSE Linux Enterprise High Availability Extension. For example, the cluster communication
    channels are defined in
    <code class="filename">/etc/corosync/corosync.conf</code>.
   </p></dd><dt id="id-1.3.12.24"><span><span class="glossterm">Crowbar</span> <a title="Permalink" class="permalink" href="#id-1.3.12.24">#</a></span></dt><dd class="glossdef"><p>
    Bare-metal installer and an extension of Chef server. The primary
    function of Crowbar is to get new hardware into a state where it can
    be managed by Chef. That means: Setting up BIOS and RAID, network,
    installing a basic operating system, and setting up services like DNS,
    NTP, and DHCP. The Crowbar server manages all nodes, supplying
    configuration of hardware and software.
   </p></dd><dt id="gloss-dc"><span><span class="glossterm">Designated Coordinator (DC)</span> <a title="Permalink" class="permalink" href="#gloss-dc">#</a></span></dt><dd class="glossdef"><p>
    One <a class="xref" href="#gloss-crm" title="Cluster Resource Manager">Cluster Resource Manager</a> in a High Availability cluster is elected as the
    Designated Coordinator (DC). The DC is the only entity in the cluster
    that can decide that a cluster-wide change needs to be performed. For
    example, fencing a node or moving resources around. After a membership change,
    the DC is elected from all nodes in the cluster.
   </p></dd><dt id="id-1.3.12.26"><span><span class="glossterm">DRBD (Distributed Replicated Block Device)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.26">#</a></span></dt><dd class="glossdef"><p>
    DRBD is a block device designed for building high availability
    clusters. The whole block device is mirrored via a dedicated network and
    is seen as a network RAID-1.
   </p></dd><dt id="id-1.3.12.27"><span><span class="glossterm">EBS (Amazon Elastic Block Store)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.27">#</a></span></dt><dd class="glossdef"><p>
    Block-level storage volumes for use with Amazon EC2 instances. Similar
    to <span class="productname">OpenStack</span> Cinder.
   </p></dd><dt id="id-1.3.12.28"><span><span class="glossterm">EC2 (Amazon Elastic Compute Cloud)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.28">#</a></span></dt><dd class="glossdef"><p>
    A public cloud run by Amazon. It provides similar functionality to
    <span class="productname">OpenStack</span> Compute.
   </p></dd><dt id="id-1.3.12.29"><span><span class="glossterm">Ephemeral Disk</span> <a title="Permalink" class="permalink" href="#id-1.3.12.29">#</a></span></dt><dd class="glossdef"><p>
    Ephemeral disks offer machine local disk storage linked to the life
    cycle of a virtual machine instance. When a virtual machine is
    terminated, all data on the ephemeral disk is lost. Ephemeral disks are
    not included in any snapshots.
   </p></dd><dt id="gloss-failover"><span><span class="glossterm">Failover</span> <a title="Permalink" class="permalink" href="#gloss-failover">#</a></span></dt><dd class="glossdef"><p>
    Occurs when a resource fails on a cluster node (or the node itself
    fails) and the affected resources are started on another node.
   </p></dd><dt id="gloss-fencing"><span><span class="glossterm">Fencing</span> <a title="Permalink" class="permalink" href="#gloss-fencing">#</a></span></dt><dd class="glossdef"><p>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. Should a cluster node fail, it will
    be shut down or reset to prevent it from causing trouble. The resources
    running on the cluster node will be moved away to another node. This
    way, resources are locked out of a node whose status is uncertain.
   </p></dd><dt id="gloss-IP-fixed"><span><span class="glossterm">Fixed IP Address</span> <a title="Permalink" class="permalink" href="#gloss-IP-fixed">#</a></span></dt><dd class="glossdef"><p>
    When an instance is launched, it is automatically assigned a fixed
    (private) IP address, which stays the same until the instance is
    explicitly terminated. Private IP addresses are used for communication
    between instances.
   </p></dd><dt id="gloss-flavor"><span><span class="glossterm">Flavor</span> <a title="Permalink" class="permalink" href="#gloss-flavor">#</a></span></dt><dd class="glossdef"><p>
    The compute, memory, and storage capacity of <code class="literal">nova</code>
    computing instances (in terms of virtual CPUs, RAM, etc.). Flavors can
    be thought of as <span class="quote">“<span class="quote ">templates</span>”</span> for the amount of cloud
    resources that are assigned to an instance.
   </p></dd><dt id="gloss-IP-float"><span><span class="glossterm">Floating IP Address</span> <a title="Permalink" class="permalink" href="#gloss-IP-float">#</a></span></dt><dd class="glossdef"><p>
    An IP address that a Compute project can associate with a virtual
    machine. A pool of floating IP addresses is available in <span class="productname">OpenStack</span> Compute,
    as configured by the cloud operator. After a floating IP address has
    been assigned to an instance, the instance can be reached from outside
    the cloud by this public IP address. Floating IP addresses can be
    dynamically disassociated and associated with other instances.
   </p></dd><dt id="id-1.3.12.35"><span><span class="glossterm">Glance</span> <a title="Permalink" class="permalink" href="#id-1.3.12.35">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-glance" title="OpenStack Image"><span class="productname">OpenStack</span> Image</a>.
   </p></dd><dt id="id-1.3.12.36"><span><span class="glossterm">Guest Operating System</span> <a title="Permalink" class="permalink" href="#id-1.3.12.36">#</a></span></dt><dd class="glossdef"><p>
    An instance of an operating system installed on a virtual machine.
   </p></dd><dt id="id-1.3.12.37"><span><span class="glossterm">Heat</span> <a title="Permalink" class="permalink" href="#id-1.3.12.37">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-heat" title="Orchestration">Orchestration</a>.
   </p></dd><dt id="id-1.3.12.38"><span><span class="glossterm">High Availability Cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.12.38">#</a></span></dt><dd class="glossdef"><p>
    High Availability clusters seek to minimize two things: system downtime and data
    loss. System downtime occurs when a user-facing service is unavailable
    beyond a specified maximum amount of time. System downtime and data
    loss (data is accidentally destroyed) can occur not only in case of a single
    failure. There are also cases of cascading failures,
    where a single failure deteriorates into a series of consequential
    failures.
   </p></dd><dt id="id-1.3.12.39"><span><span class="glossterm">Horizon</span> <a title="Permalink" class="permalink" href="#id-1.3.12.39">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-horizon" title="OpenStack Dashboard"><span class="productname">OpenStack</span> Dashboard</a>.
   </p></dd><dt id="id-1.3.12.40"><span><span class="glossterm">Host</span> <a title="Permalink" class="permalink" href="#id-1.3.12.40">#</a></span></dt><dd class="glossdef"><p>
    A physical computer.
   </p></dd><dt id="gloss-host-aggr"><span><span class="glossterm">Host Aggregate</span> <a title="Permalink" class="permalink" href="#gloss-host-aggr">#</a></span></dt><dd class="glossdef"><p>
    An <span class="productname">OpenStack</span> method of grouping hosts via a common set of metadata. It
    enables you to tag groups of hosts with certain capabilities or
    characteristics. A characteristic could be related to physical location,
    allowing creation or further partitioning of availability zones. It could
    also be related to performance (for example, indicating the
    availability of SSD storage) or anything else that the cloud
    administrators deem appropriate. A host can be in more than one host
    aggregate.
   </p></dd><dt id="id-1.3.12.42"><span><span class="glossterm">Hybrid Cloud</span> <a title="Permalink" class="permalink" href="#id-1.3.12.42">#</a></span></dt><dd class="glossdef"><p>
    One of several deployment models for a cloud infrastructure. A
    composition of both public and private clouds that remain unique
    entities, but are bound together by standardized technology for enabling
    data and application portability. Integrating SUSE Studio and
    SUSE Manager with SUSE <span class="productname">OpenStack</span> Cloud delivers a platform and tools with which to
    enable enterprise hybrid clouds.
    
   </p></dd><dt id="id-1.3.12.43"><span><span class="glossterm">Hypervisor</span> <a title="Permalink" class="permalink" href="#id-1.3.12.43">#</a></span></dt><dd class="glossdef"><p>
    A piece of computer software, firmware or hardware that creates and runs
    virtual machines. It arbitrates and controls access of the virtual
    machines to the underlying hardware.
   </p></dd><dt id="id-1.3.12.44"><span><span class="glossterm">IaaS (Infrastructure-as-a-Service)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.44">#</a></span></dt><dd class="glossdef"><p>
    A service model of cloud computing where processing, storage, networks,
    and other fundamental computing resources are rented over the Internet.
    It allows the customer to deploy and run arbitrary software, including
    operating systems and applications. The customer has control over
    operating systems, storage, and deployed applications but does not
    control the underlying cloud infrastructure. Housing and maintaining it
    is in the responsibility of the service provider.
   </p></dd><dt id="id-1.3.12.45"><span><span class="glossterm">Image</span> <a title="Permalink" class="permalink" href="#id-1.3.12.45">#</a></span></dt><dd class="glossdef"><p>
    A file that contains a complete Linux virtual machine.
   </p><p>
    In the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> context, images are virtual disk images that
    represent the contents and structure of a storage medium or device
    (such as a hard disk), in a single file. Images are used as a template from which
    a virtual machine can be started. For starting a virtual machine,
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> always uses a copy of the image.
   </p><p>
    Images have both content and metadata; the latter are also called
    image properties.
   </p></dd><dt id="id-1.3.12.46"><span><span class="glossterm">Instance</span> <a title="Permalink" class="permalink" href="#id-1.3.12.46">#</a></span></dt><dd class="glossdef"><p>
    A virtual machine that runs inside the cloud.
   </p></dd><dt id="gloss-instsnap"><span><span class="glossterm">Instance Snapshot</span> <a title="Permalink" class="permalink" href="#gloss-instsnap">#</a></span></dt><dd class="glossdef"><p>
    A point-in-time copy of an instance. It preserves the disk state of a
    running instance and can be used to launch a new instance or to create a
    new image based upon the snapshot.
   </p></dd><dt id="id-1.3.12.48"><span><span class="glossterm">Keypair</span> <a title="Permalink" class="permalink" href="#id-1.3.12.48">#</a></span></dt><dd class="glossdef"><p>
    <span class="productname">OpenStack</span> Compute injects SSH keypair credentials that are injected
    into images when they are launched.
   </p></dd><dt id="id-1.3.12.49"><span><span class="glossterm">Keystone</span> <a title="Permalink" class="permalink" href="#id-1.3.12.49">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-keystone" title="OpenStack Identity"><span class="productname">OpenStack</span> Identity</a>.
   </p></dd><dt id="id-1.3.12.50"><span><span class="glossterm">libvirt</span> <a title="Permalink" class="permalink" href="#id-1.3.12.50">#</a></span></dt><dd class="glossdef"><p>
    Virtualization API library. Used by <span class="productname">OpenStack</span> to interact with many of
    its supported hypervisors.
   </p></dd><dt id="id-1.3.12.51"><span><span class="glossterm">Linux Bridge</span> <a title="Permalink" class="permalink" href="#id-1.3.12.51">#</a></span></dt><dd class="glossdef"><p>
    A software allowing multiple virtual machines to share a single physical
    NIC within <span class="productname">OpenStack</span> Compute. It behaves like a hub: You can connect
    multiple (physical or virtual) network interface devices to it. Any
    Ethernet frames that come in from one interface attached to the bridge
    is transmitted to all other devices.
   </p></dd><dt id="id-1.3.12.52"><span><span class="glossterm">Logical Volume (LV)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.52">#</a></span></dt><dd class="glossdef"><p>
    Acts as a virtual disk partition. After creating a
    <a class="xref" href="#gloss-vg" title="Volume Group (VG)">Volume Group (VG)</a>, logical volumes can be created in that
    volume group. Logical volumes can be used as raw block devices, swap
    devices, or for creating a (mountable) file system like disk partitions.
   </p></dd><dt id="id-1.3.12.53"><span><span class="glossterm">Migration</span> <a title="Permalink" class="permalink" href="#id-1.3.12.53">#</a></span></dt><dd class="glossdef"><p>
    The process of moving a virtual machine instance from one Compute Node
    to another. This process can only be executed by cloud administrators.
   </p></dd><dt id="id-1.3.12.54"><span><span class="glossterm">Multicast</span> <a title="Permalink" class="permalink" href="#id-1.3.12.54">#</a></span></dt><dd class="glossdef"><p>
    A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast.
   </p></dd><dt id="id-1.3.12.55"><span><span class="glossterm">Network</span> <a title="Permalink" class="permalink" href="#id-1.3.12.55">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> Networking API: An isolated L2 network segment
    (similar to a VLAN). It forms the basis for describing the L2 network
    topology in a given <span class="productname">OpenStack</span> Networking deployment.
   </p></dd><dt id="id-1.3.12.56"><span><span class="glossterm">Neutron</span> <a title="Permalink" class="permalink" href="#id-1.3.12.56">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-neutron" title="OpenStack Networking"><span class="productname">OpenStack</span> Networking</a>.
   </p></dd><dt id="id-1.3.12.57"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.3.12.57">#</a></span></dt><dd class="glossdef"><p>
    A (physical) server that is managed by Crowbar.
   </p></dd><dt id="id-1.3.12.58"><span><span class="glossterm">Nova</span> <a title="Permalink" class="permalink" href="#id-1.3.12.58">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-nova" title="OpenStack Compute"><span class="productname">OpenStack</span> Compute</a>.
   </p></dd><dt id="id-1.3.12.59"><span><span class="glossterm">Object</span> <a title="Permalink" class="permalink" href="#id-1.3.12.59">#</a></span></dt><dd class="glossdef"><p>
    Basic storage entity in <span class="productname">OpenStack</span> Object Storage, representing a file
    that your store there. When you upload data to <span class="productname">OpenStack</span>
    Object Storage, the data is neither compressed nor encrypted, it is
    stored as-is.
   </p></dd><dt id="id-1.3.12.60"><span><span class="glossterm">Open vBridge</span> <a title="Permalink" class="permalink" href="#id-1.3.12.60">#</a></span></dt><dd class="glossdef"><p>
    A virtual networking device. It behaves like a virtual switch: network
    interface devices connect to its ports. The ports can be configured
    similar to a physical switch's port, including VLAN configurations.
   </p></dd><dt id="id-1.3.12.61"><span><span class="glossterm"><span class="productname">OpenStack</span></span> <a title="Permalink" class="permalink" href="#id-1.3.12.61">#</a></span></dt><dd class="glossdef"><p>
    A collection of open source software to build and manage public and
    private clouds. Its components are designed to work together to provide
    Infrastructure as a Service and massively scalable cloud computing
    software.
   </p><p>
    At the same time, <span class="productname">OpenStack</span> is also a community and a project.
   </p></dd><dt id="gloss-cinder"><span><span class="glossterm"><span class="productname">OpenStack</span> Block Storage</span> <a title="Permalink" class="permalink" href="#gloss-cinder">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components and services (code name:
    <code class="literal">Cinder</code>). It provides persistent block
    level storage devices for use <span class="productname">OpenStack</span> compute instances. The block
    storage system manages the creation, attaching and detaching of the
    block devices to servers.
    
    Prior to the <span class="productname">OpenStack</span> Grizzly release, the service was part of
    <code class="literal">nova-volume</code> (block service).

   </p></dd><dt id="gloss-nova"><span><span class="glossterm"><span class="productname">OpenStack</span> Compute</span> <a title="Permalink" class="permalink" href="#gloss-nova">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components and services (code name:
    <code class="literal">Nova</code>). It is a cloud computing fabric
    controller and as such, the main part of an IaaS system. It provides
    virtual machines on demand.
   </p></dd><dt id="gloss-horizon"><span><span class="glossterm"><span class="productname">OpenStack</span> Dashboard</span> <a title="Permalink" class="permalink" href="#gloss-horizon">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">Horizon</code>). It provides a modular Web interface for
    <span class="productname">OpenStack</span> services and allows end users and administrators to interact
    with each <span class="productname">OpenStack</span> service through the service's API.
   </p></dd><dt id="gloss-keystone"><span><span class="glossterm"><span class="productname">OpenStack</span> Identity</span> <a title="Permalink" class="permalink" href="#gloss-keystone">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">Keystone</code>). It provides authentication and
    authorization for all <span class="productname">OpenStack</span> services.
   </p></dd><dt id="gloss-glance"><span><span class="glossterm"><span class="productname">OpenStack</span> Image</span> <a title="Permalink" class="permalink" href="#gloss-glance">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">Glance</code>). It provides discovery, registration, and
    delivery services for virtual disk images.
   </p></dd><dt id="gloss-neutron"><span><span class="glossterm"><span class="productname">OpenStack</span> Networking</span> <a title="Permalink" class="permalink" href="#gloss-neutron">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">Neutron</code>). It provides <span class="quote">“<span class="quote ">network connectivity
    as a service</span>”</span> between interface devices (for example, vNICs)
    managed by other <span class="productname">OpenStack</span> services (for example, Compute). Allows
    users to create their own networks and attach interfaces to them.
   </p></dd><dt id="gloss-swift"><span><span class="glossterm"><span class="productname">OpenStack</span> Object Storage</span> <a title="Permalink" class="permalink" href="#gloss-swift">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">Swift</code>). Allows to store and retrieve files
    while providing built-in redundancy and fail-over. Can be used for
    backing up and archiving data, streaming data to a user's Web browser,
    or developing new applications with data storage integration.
   </p></dd><dt id="id-1.3.12.69"><span><span class="glossterm"><span class="productname">OpenStack</span> Service</span> <a title="Permalink" class="permalink" href="#id-1.3.12.69">#</a></span></dt><dd class="glossdef"><p>
    A collection of Linux services (or daemons) that work together to
    provide core functionality within the <span class="productname">OpenStack</span> project. This can be storing
    objects, providing virtual servers, or authentication and authorization.
    All services have code names, which are also used in configuration files,
    and command line programs.
   </p></dd><dt id="gloss-heat"><span><span class="glossterm">Orchestration</span> <a title="Permalink" class="permalink" href="#gloss-heat">#</a></span></dt><dd class="glossdef"><p>
    
    A module (code name: <code class="literal">Heat</code>) to orchestrate
    multiple composite cloud applications using file-based or Web-based
    templates. It contains both a user interface and an API and describes
    your cloud deployment in a declarative language. The module is an
    integrated project of <span class="productname">OpenStack</span> as of the Havana release.
   </p></dd><dt id="id-1.3.12.71"><span><span class="glossterm">PaaS (Platform-as-a-Service)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.71">#</a></span></dt><dd class="glossdef"><p>
    A service model of cloud computing where a computing platform and
    cloud-based application development tools are rented over the Internet.
    The customer controls software deployment and configuration settings,
    but not the underlying cloud infrastructure including network, servers,
    operating systems, or storage.
   </p></dd><dt id="id-1.3.12.72"><span><span class="glossterm">Pacemaker</span> <a title="Permalink" class="permalink" href="#id-1.3.12.72">#</a></span></dt><dd class="glossdef"><p>
    An open source cluster resource manager used in SUSE Linux Enterprise High Availability Extension.
   </p></dd><dt id="id-1.3.12.73"><span><span class="glossterm">Port</span> <a title="Permalink" class="permalink" href="#id-1.3.12.73">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> Networking API: An attachment port to an L2
    <span class="productname">OpenStack</span> Networking network.

   </p></dd><dt id="id-1.3.12.76"><span><span class="glossterm">Private Cloud</span> <a title="Permalink" class="permalink" href="#id-1.3.12.76">#</a></span></dt><dd class="glossdef"><p>
    One of several deployment models for a cloud infrastructure. The
    infrastructure is operated exclusively for a single organization and may
    exist on or off premises. The cloud is owned and managed by the
    organization itself, by a third party or a combination of both.
   </p></dd><dt id="id-1.3.12.77"><span><span class="glossterm">Private IP Address</span> <a title="Permalink" class="permalink" href="#id-1.3.12.77">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-IP-fixed" title="Fixed IP Address">Fixed IP Address</a>.
   </p></dd><dt id="gloss-project"><span><span class="glossterm">Project</span> <a title="Permalink" class="permalink" href="#gloss-project">#</a></span></dt><dd class="glossdef"><p>
    A concept in <span class="productname">OpenStack</span> Identity. Used to identify a group, an
    organization, or a project (or more generically, an individual customer
    environment in the cloud). Also called <code class="literal">tenant</code>. The
    term <code class="literal">tenant</code> is primarily used in the <span class="productname">OpenStack</span>
    command line tools.

   </p></dd><dt id="gloss-proposal"><span><span class="glossterm">Proposal</span> <a title="Permalink" class="permalink" href="#gloss-proposal">#</a></span></dt><dd class="glossdef"><p>
    Special configuration for a barclamp. It includes barclamp-specific
    settings, and a list of nodes to which the proposal should be applied.
   </p></dd><dt id="id-1.3.12.78"><span><span class="glossterm">Public Cloud</span> <a title="Permalink" class="permalink" href="#id-1.3.12.78">#</a></span></dt><dd class="glossdef"><p>
    One of several deployment models for a cloud infrastructure. The cloud
    infrastructure is designed for use by the general public and exists on
    the premises of the cloud provider. Services like applications, storage,
    and other resources are made available to the general public for free or
    are offered on a pay-per-use model. The infrastructure is owned and
    managed by a business, academic or government organization, or some
    combination of these.
   </p></dd><dt id="id-1.3.12.79"><span><span class="glossterm">Public IP Address</span> <a title="Permalink" class="permalink" href="#id-1.3.12.79">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-IP-float" title="Floating IP Address">Floating IP Address</a>.
   </p></dd><dt id="id-1.3.12.80"><span><span class="glossterm">qcow (QEMU Copy on Write)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.80">#</a></span></dt><dd class="glossdef"><p>
    A disk image format supported by the QEMU virtual machine manager. A
    <code class="literal">qcow2</code> image helps to optimize disk space. It
    consumes disk space only when contents are written on it and grows as
    data is added.
   </p><p>
    <code class="literal">qcow2</code> is a more recent version of the
    <code class="literal">qcow</code> format where a read-only base image is used, and
    all writes are stored to the <code class="literal">qcow2</code> image.
   </p></dd><dt id="gloss-quorum"><span><span class="glossterm">Quorum</span> <a title="Permalink" class="permalink" href="#gloss-quorum">#</a></span></dt><dd class="glossdef"><p>
    In a cluster, a <a class="xref" href="#gloss-partition" title="Cluster Partition">Cluster Partition</a> is defined to have
    quorum (is <span class="quote">“<span class="quote ">quorate</span>”</span>) if it has the majority of nodes (or
    votes). Quorum distinguishes exactly one partition. It is part of the
    algorithm to prevent several disconnected partitions or nodes from
    proceeding and causing data and service corruption
    (<a class="xref" href="#gloss-splitbrain" title="Split Brain">Split Brain</a>). Quorum is a prerequisite for
    <a class="xref" href="#gloss-fencing" title="Fencing">Fencing</a>, which then ensures that quorum is
    indeed unique.
   </p></dd><dt id="id-1.3.12.82"><span><span class="glossterm">Quota</span> <a title="Permalink" class="permalink" href="#id-1.3.12.82">#</a></span></dt><dd class="glossdef"><p>
    Restriction of resources to prevent overconsumption within a cloud. In
    <span class="productname">OpenStack</span>, quotas are defined per project and contain multiple
    parameters, such as amount of RAM, number of instances, or number of
    floating IP addresses.
   </p></dd><dt id="id-1.3.12.83"><span><span class="glossterm">RC File  (openrc.sh)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.83">#</a></span></dt><dd class="glossdef"><p>
    Environment file needed for the <span class="productname">OpenStack</span> command line tools. The RC
    file is project-specific and contains the credentials used by
    <span class="productname">OpenStack</span> Compute, Image, and Identity services.
   </p></dd><dt id="id-1.3.12.84"><span><span class="glossterm">Recipe</span> <a title="Permalink" class="permalink" href="#id-1.3.12.84">#</a></span></dt><dd class="glossdef"><p>
    A group of Chef scripts and templates. Recipes are used by
    Chef to deploy a unit of functionality.
   </p></dd><dt id="id-1.3.12.85"><span><span class="glossterm">Region</span> <a title="Permalink" class="permalink" href="#id-1.3.12.85">#</a></span></dt><dd class="glossdef"><p>
    
    An <span class="productname">OpenStack</span> method of aggregating clouds. Regions are a robust way to
    share some infrastructure between <span class="productname">OpenStack</span> compute installations,
    while allowing for a high degree of failure tolerance. Regions have a
    separate API endpoint per installation.
   </p></dd><dt id="gloss-rsc"><span><span class="glossterm">Resource</span> <a title="Permalink" class="permalink" href="#gloss-rsc">#</a></span></dt><dd class="glossdef"><p>
    In a High Availability context: Any type of service or application that is known
    to the cluster resource manager. Examples include an IP address, a file
    system, or a database.
   </p></dd><dt id="gloss-ra"><span><span class="glossterm">Resource Agent (RA)</span> <a title="Permalink" class="permalink" href="#gloss-ra">#</a></span></dt><dd class="glossdef"><p>
    A script acting as a proxy to manage a resource in a High Availability cluster.
    For example, it can start, stop or monitor a resource.
   </p></dd><dt id="id-1.3.12.88"><span><span class="glossterm">Role</span> <a title="Permalink" class="permalink" href="#id-1.3.12.88">#</a></span></dt><dd class="glossdef"><p>
    In the Crowbar/Chef context: an instance of a
    <a class="xref" href="#gloss-proposal" title="Proposal">Proposal</a> that is active on a node.
   </p><p>
    In the <a class="xref" href="#gloss-keystone" title="OpenStack Identity"><span class="productname">OpenStack</span> Identity</a> context: concept of controlling
    the actions or set of operations that a user is allowed to perform. A
    role includes a set of rights and privileges. A user assuming that role
    inherits those rights and privileges.
   </p></dd><dt id="id-1.3.12.89"><span><span class="glossterm">S3 (Amazon Simple Storage Service)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.89">#</a></span></dt><dd class="glossdef"><p>
    An object storage by Amazon that can be used to store and retrieve data
    on the Web. Similar in function to <span class="productname">OpenStack</span> Object Storage. It can act
    as a back-end store for Glance images.
   </p></dd><dt id="id-1.3.12.90"><span><span class="glossterm">SaaS (Software-as-a-Service)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.90">#</a></span></dt><dd class="glossdef"><p>
    A service model of cloud computing where applications are hosted by a
    service provider and made available to customers remotely as a Web-based
    service.
   </p></dd><dt id="id-1.3.12.91"><span><span class="glossterm">SBD (STONITH Block Device)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.91">#</a></span></dt><dd class="glossdef"><p>
    In an environment where all nodes of a High Availability cluster have access to
    shared storage, a small partition is used for disk-based fencing.
   </p></dd><dt id="id-1.3.12.92"><span><span class="glossterm">Security Group</span> <a title="Permalink" class="permalink" href="#id-1.3.12.92">#</a></span></dt><dd class="glossdef"><p>
    Concept in <span class="productname">OpenStack</span> Networking. A security group is a container for
    security group rules. Security group rules allow to specify the type of
    traffic and direction (ingress/egress) that is allowed to pass through a
    port.
   </p></dd><dt id="gloss-spof"><span><span class="glossterm">Single Point of Failure (SPOF)</span> <a title="Permalink" class="permalink" href="#gloss-spof">#</a></span></dt><dd class="glossdef"><p>
    An individual piece of equipment or software which will cause system
    downtime or data loss if it fails. To eliminate single points of
    failure, High Availability systems seek to provide redundancy for crucial pieces
    of equipment or software.
   </p></dd><dt id="gloss-hammer"><span><span class="glossterm">SLEShammer</span> <a title="Permalink" class="permalink" href="#gloss-hammer">#</a></span></dt><dd class="glossdef"><p>
    When you first boot a node in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> via PXE, it is booted with the SLEShammer image. This performs the initial hardware discovery, and registers the node with Crowbar.
    After you allocate the node, it is rebooted with a regular SLES installation image.
   </p></dd><dt id="id-1.3.12.95"><span><span class="glossterm">Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.12.95">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-volsnap" title="Volume Snapshot">Volume Snapshot</a> or <a class="xref" href="#gloss-instsnap" title="Instance Snapshot">Instance Snapshot</a>.
   </p></dd><dt id="gloss-splitbrain"><span><span class="glossterm">Split Brain</span> <a title="Permalink" class="permalink" href="#gloss-splitbrain">#</a></span></dt><dd class="glossdef"><p>
    Also known as a <span class="quote">“<span class="quote ">partitioned cluster</span>”</span> scenario. Either
    through a software or hardware failure, the cluster nodes are divided
    into two or more groups that do not know of each other.
    <a class="xref" href="#gloss-stonith" title="STONITH">STONITH</a> prevents a split brain situation from
    badly affecting the entire cluster.
   </p></dd><dt id="gloss-stateful"><span><span class="glossterm">Stateful Service</span> <a title="Permalink" class="permalink" href="#gloss-stateful">#</a></span></dt><dd class="glossdef"><p>
    A service where subsequent requests to the service depend on the results
    of the first request.
   </p></dd><dt id="gloss-stateless"><span><span class="glossterm">Stateless Service</span> <a title="Permalink" class="permalink" href="#gloss-stateless">#</a></span></dt><dd class="glossdef"><p>
    A service that provides a response after your request, and then requires
    no further attention.

   </p></dd><dt id="gloss-stonith"><span><span class="glossterm">STONITH</span> <a title="Permalink" class="permalink" href="#gloss-stonith">#</a></span></dt><dd class="glossdef"><p>
    The acronym for <span class="quote">“<span class="quote ">Shoot the other node in the head</span>”</span>. It
    refers to the fencing mechanism that shuts down a misbehaving node to
    prevent it from causing trouble in a cluster.
   </p></dd><dt id="id-1.3.12.100"><span><span class="glossterm">Storage Node</span> <a title="Permalink" class="permalink" href="#id-1.3.12.100">#</a></span></dt><dd class="glossdef"><p>
    Node within a SUSE <span class="productname">OpenStack</span> Cloud. Acts as the controller for cloud-based
    storage. A cloud can contain multiple Storage Nodes.
   </p></dd><dt id="id-1.3.12.101"><span><span class="glossterm">Subnet</span> <a title="Permalink" class="permalink" href="#id-1.3.12.101">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> Networking API: A block of IP addresses and other
    network configuration (for example, a default gateway, DNS servers) that
    can be associated with an <span class="productname">OpenStack</span> Networking network. Each subnet
    represents an IPv4 or IPv6 address block. Multiple subnets can be
    associated with a network, if necessary.
   </p></dd><dt id="gloss-hasi"><span><span class="glossterm">SUSE Linux Enterprise High Availability Extension</span> <a title="Permalink" class="permalink" href="#gloss-hasi">#</a></span></dt><dd class="glossdef"><p>
    An integrated suite of open source clustering technologies that enables
    you to implement highly available physical and virtual Linux clusters.
   </p></dd><dt id="id-1.3.12.102"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud Administrator</span> <a title="Permalink" class="permalink" href="#id-1.3.12.102">#</a></span></dt><dd class="glossdef"><p>
    User role in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Manages projects, users, images, flavors,
    and quotas within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p></dd><dt id="id-1.3.12.103"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud Dashboard</span> <a title="Permalink" class="permalink" href="#id-1.3.12.103">#</a></span></dt><dd class="glossdef"><p>
    The <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Dashboard is a Web interface that enables cloud
    administrators and users to manage various <span class="productname">OpenStack</span> services. It is based
    on <span class="productname">OpenStack</span> Dashboard (also known under its codename
    <code class="literal">Horizon</code>).
   </p></dd><dt id="id-1.3.12.104"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud Operator</span> <a title="Permalink" class="permalink" href="#id-1.3.12.104">#</a></span></dt><dd class="glossdef"><p>
    User role in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Installs and deploys <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p></dd><dt id="id-1.3.12.105"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud User</span> <a title="Permalink" class="permalink" href="#id-1.3.12.105">#</a></span></dt><dd class="glossdef"><p>
    User role in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. End user who launches and manages
    instances, can create snapshots, and use volumes for persistent storage
    within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p></dd><dt id="id-1.3.12.107"><span><span class="glossterm">Swift</span> <a title="Permalink" class="permalink" href="#id-1.3.12.107">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-swift" title="OpenStack Object Storage"><span class="productname">OpenStack</span> Object Storage</a>.
   </p></dd><dt id="id-1.3.12.108"><span><span class="glossterm">TAP Device</span> <a title="Permalink" class="permalink" href="#id-1.3.12.108">#</a></span></dt><dd class="glossdef"><p>
    A virtual networking device. A TAP device, such as
    <code class="literal">vnet0</code> is how hypervisors such as KVM and
    Xen implement a virtual network interface card (vNIC). An Ethernet
    frame sent to a TAP device is received by the guest operating system.
    The tap option connects the network stack of the guest operating system
    to a TAP network device on the host.
   </p></dd><dt id="gloss-ceilo"><span><span class="glossterm">Telemetry</span> <a title="Permalink" class="permalink" href="#gloss-ceilo">#</a></span></dt><dd class="glossdef"><p>
    A module (code name: <code class="literal">Ceilometer</code>) for metering
    <span class="productname">OpenStack</span>-based clouds. The project aims to provide a unique point of
    contact across all <span class="productname">OpenStack</span> core components for acquiring metrics.
    The metrics can then be consumed by other components such as customer billing.
    The module is an integrated project of <span class="productname">OpenStack</span> as of the Havana
    release.
   </p></dd><dt id="id-1.3.12.110"><span><span class="glossterm">Tenant</span> <a title="Permalink" class="permalink" href="#id-1.3.12.110">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-project" title="Project">Project</a>.
   </p></dd><dt id="id-1.3.12.111"><span><span class="glossterm">Unicast</span> <a title="Permalink" class="permalink" href="#id-1.3.12.111">#</a></span></dt><dd class="glossdef"><p>
    A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync,
    unicast is implemented as UDP-unicast (UDPU).
   </p></dd><dt id="id-1.3.12.112"><span><span class="glossterm">User</span> <a title="Permalink" class="permalink" href="#id-1.3.12.112">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> context, a digital representation of a person,
    system, or service who uses <span class="productname">OpenStack</span> cloud services. Users can be
    directly assigned to a particular project and behave as if they are
    contained in that project.
   </p></dd><dt id="id-1.3.12.113"><span><span class="glossterm">Veth Pair</span> <a title="Permalink" class="permalink" href="#id-1.3.12.113">#</a></span></dt><dd class="glossdef"><p>
    A virtual networking device.
    
    The acronym veth stands for virtual Ethernet interface. A veth is a pair
    of virtual network interfaces correctly

    directly together. An Ethernet frame sent to one end of a veth pair is
    received by the other end of a veth pair. <span class="productname">OpenStack</span> Networking uses
    veth pairs as virtual patch cables to make connections between virtual
    bridges.
   </p></dd><dt id="id-1.3.12.114"><span><span class="glossterm">VLAN</span> <a title="Permalink" class="permalink" href="#id-1.3.12.114">#</a></span></dt><dd class="glossdef"><p>
    A physical method for network virtualization. VLANs allow to create
    virtual networks across a distributed network. Disparate hosts
    (on independent networks) appear as if they were part of the same
    broadcast domain.
   </p></dd><dt id="id-1.3.12.115"><span><span class="glossterm">VM (Virtual Machine)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.115">#</a></span></dt><dd class="glossdef"><p>
    An operating system instance that runs on top of a hypervisor. Multiple
    virtual machines can run on the same physical host at the same time.
   </p></dd><dt id="id-1.3.12.116"><span><span class="glossterm">vNIC </span> <a title="Permalink" class="permalink" href="#id-1.3.12.116">#</a></span></dt><dd class="glossdef"><p>
    Virtual network interface card.
   </p></dd><dt id="id-1.3.12.117"><span><span class="glossterm">Volume</span> <a title="Permalink" class="permalink" href="#id-1.3.12.117">#</a></span></dt><dd class="glossdef"><p>
    Detachable block storage device. Unlike a SAN, it can only be attached
    to one instance at a time.
   </p></dd><dt id="gloss-vg"><span><span class="glossterm">Volume Group (VG)</span> <a title="Permalink" class="permalink" href="#gloss-vg">#</a></span></dt><dd class="glossdef"><p>
    A virtual disk consisting of aggregated physical volumes. Volume groups
    can be logically partitioned into logical volumes.
   </p></dd><dt id="gloss-volsnap"><span><span class="glossterm">Volume Snapshot</span> <a title="Permalink" class="permalink" href="#gloss-volsnap">#</a></span></dt><dd class="glossdef"><p>
    A point-in-time copy of an <span class="productname">OpenStack</span> storage volume. Used to back up
    volumes.
   </p></dd><dt id="id-1.3.12.120"><span><span class="glossterm">vSwitch (Virtual Switch)</span> <a title="Permalink" class="permalink" href="#id-1.3.12.120">#</a></span></dt><dd class="glossdef"><p>
    
    A software that runs on a host or node and provides the features and
    functions of a hardware-based network switch.
   </p></dd><dt id="id-1.3.12.121"><span><span class="glossterm">Zone</span> <a title="Permalink" class="permalink" href="#id-1.3.12.121">#</a></span></dt><dd class="glossdef"><p>
    
    A logical grouping of Compute services and virtual machine hosts.
   </p></dd></dl></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2022 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>