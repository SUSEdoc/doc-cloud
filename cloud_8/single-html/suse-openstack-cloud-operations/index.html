<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Operations Guide | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.2.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.81.0 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="Operations Guide" /><meta name="description" content="This guide provides a list of useful procedures for managing your SUSE OpenStack Cloud 8 cloud. The audience is the admin-level operator of the cloud." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#book-operations" accesskey="c"><span class="single-contents-icon"></span>Operations Guide</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#book-operations" accesskey="c"><span class="single-contents-icon"></span>Show Contents: Operations Guide</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#gettingstarted-ops"><span class="number">1 </span><span class="name">Operations Overview</span></a></li><li class="inactive"><a href="#tutorials"><span class="number">2 </span><span class="name">Tutorials</span></a></li><li class="inactive"><a href="#third-party-integrations"><span class="number">3 </span><span class="name">Third-Party Integrations</span></a></li><li class="inactive"><a href="#ops-managing-identity"><span class="number">4 </span><span class="name">Managing Identity</span></a></li><li class="inactive"><a href="#ops-managing-compute"><span class="number">5 </span><span class="name">Managing Compute</span></a></li><li class="inactive"><a href="#ops-managing-esx"><span class="number">6 </span><span class="name">Managing ESX</span></a></li><li class="inactive"><a href="#ops-managing-blockstorage"><span class="number">7 </span><span class="name">Managing Block Storage</span></a></li><li class="inactive"><a href="#ops-managing-objectstorage"><span class="number">8 </span><span class="name">Managing Object Storage</span></a></li><li class="inactive"><a href="#ops-managing-networking"><span class="number">9 </span><span class="name">Managing Networking</span></a></li><li class="inactive"><a href="#ops-managing-dashboards"><span class="number">10 </span><span class="name">Managing the Dashboard</span></a></li><li class="inactive"><a href="#ops-managing-orchestration"><span class="number">11 </span><span class="name">Managing Orchestration</span></a></li><li class="inactive"><a href="#topic-ttn-5fg-4v"><span class="number">12 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li class="inactive"><a href="#system-maintenance"><span class="number">13 </span><span class="name">System Maintenance</span></a></li><li class="inactive"><a href="#bura-overview"><span class="number">14 </span><span class="name">Backup and Restore</span></a></li><li class="inactive"><a href="#idg-all-operations-troubleshooting-troubleshooting-issues-xml-1"><span class="number">15 </span><span class="name">Troubleshooting Issues</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div class="book" id="book-operations"><div class="titlepage"><div><h6 class="version-info"><span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></h6><div><h1 class="title">Operations Guide <a title="Permalink" class="permalink" href="#book-operations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/book.operations.xml" title="Edit the source file for this section">Edit source</a></h1></div><div class="abstract "><p>
    This guide provides a list of useful procedures for managing your
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> cloud. The audience is the admin-level operator of the
    cloud.
   </p></div><div class="date"><span class="imprint-label">Publication Date: </span>07/27/2021</div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#gettingstarted-ops"><span class="number">1 </span><span class="name">Operations Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.6.3.3"><span class="number">1.1 </span><span class="name">What is a cloud operator?</span></a></span></dt><dt><span class="section"><a href="#tools"><span class="number">1.2 </span><span class="name">Tools provided to operate your cloud</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.5"><span class="number">1.3 </span><span class="name">Daily tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.6"><span class="number">1.4 </span><span class="name">Weekly or monthly tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.7"><span class="number">1.5 </span><span class="name">Semi-annual tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.8"><span class="number">1.6 </span><span class="name">Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-operations-overview-xml-11"><span class="number">1.7 </span><span class="name">Common Questions</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#tutorials"><span class="number">2 </span><span class="name">Tutorials</span></a></span></dt><dd><dl><dt><span class="section"><a href="#Quickstart-Guide"><span class="number">2.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Quickstart Guide</span></a></span></dt><dt><span class="section"><a href="#log-management-integration"><span class="number">2.2 </span><span class="name">Log Management and Integration</span></a></span></dt><dt><span class="section"><a href="#Integrating-Kibana-with-Splunk"><span class="number">2.3 </span><span class="name">Integrating Your Logs with Splunk</span></a></span></dt><dt><span class="section"><a href="#LDAP-Integration"><span class="number">2.4 </span><span class="name">Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with an LDAP System</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#third-party-integrations"><span class="number">3 </span><span class="name">Third-Party Integrations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#splunk-integration"><span class="number">3.1 </span><span class="name">Splunk Integration</span></a></span></dt><dt><span class="section"><a href="#nagios-integration"><span class="number">3.2 </span><span class="name">Nagios Integration</span></a></span></dt><dt><span class="section"><a href="#topic-kyf-brv-vw"><span class="number">3.3 </span><span class="name">Operations Bridge Integration</span></a></span></dt><dt><span class="section"><a href="#monitoring-3rd-party-components-with-monasca"><span class="number">3.4 </span><span class="name">Monitoring Third-Party Components With Monasca</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-identity"><span class="number">4 </span><span class="name">Managing Identity</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-operation-identity-overview"><span class="number">4.1 </span><span class="name">The Identity Service</span></a></span></dt><dt><span class="section"><a href="#supported-upstream-keystone-features"><span class="number">4.2 </span><span class="name">Supported Upstream Keystone Features</span></a></span></dt><dt><span class="section"><a href="#sec-operation-identity"><span class="number">4.3 </span><span class="name">Understanding Domains, Projects, Users, Groups, and Roles</span></a></span></dt><dt><span class="section"><a href="#topic-ffs-dvz-nw"><span class="number">4.4 </span><span class="name">Identity Service Token Validation Example</span></a></span></dt><dt><span class="section"><a href="#topic-qmz-fg3-btx"><span class="number">4.5 </span><span class="name">Configuring the Identity Service</span></a></span></dt><dt><span class="section"><a href="#admin-password"><span class="number">4.6 </span><span class="name">Retrieving the Admin Password</span></a></span></dt><dt><span class="section"><a href="#servicePasswords"><span class="number">4.7 </span><span class="name">Changing Service Passwords</span></a></span></dt><dt><span class="section"><a href="#topic-m43-2j3-bt"><span class="number">4.8 </span><span class="name">Reconfiguring the Identity Service</span></a></span></dt><dt><span class="section"><a href="#ldap"><span class="number">4.9 </span><span class="name">Integrating LDAP with the Identity Service</span></a></span></dt><dt><span class="section"><a href="#k2kfed"><span class="number">4.10 </span><span class="name">Keystone-to-Keystone Federation</span></a></span></dt><dt><span class="section"><a href="#websso"><span class="number">4.11 </span><span class="name">Configuring Web Single Sign-On</span></a></span></dt><dt><span class="section"><a href="#topic-qtp-cn3-bt"><span class="number">4.12 </span><span class="name">Identity Service Notes and Limitations</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-compute"><span class="number">5 </span><span class="name">Managing Compute</span></a></span></dt><dd><dl><dt><span class="section"><a href="#aggregates"><span class="number">5.1 </span><span class="name">Managing Compute Hosts using Aggregates and Scheduler Filters</span></a></span></dt><dt><span class="section"><a href="#topic-vhs-12v-vw"><span class="number">5.2 </span><span class="name">Using Flavor Metadata to Specify CPU Model</span></a></span></dt><dt><span class="section"><a href="#topic-pqr-lyx-yw"><span class="number">5.3 </span><span class="name">Forcing CPU and RAM Overcommit Settings</span></a></span></dt><dt><span class="section"><a href="#enabling-the-nova-resize"><span class="number">5.4 </span><span class="name">Enabling the Nova Resize and Migrate Features</span></a></span></dt><dt><span class="section"><a href="#resize"><span class="number">5.5 </span><span class="name">Enabling ESX Compute Instance(s) Resize Feature</span></a></span></dt><dt><span class="section"><a href="#configure-glance"><span class="number">5.6 </span><span class="name">Configuring the Image Service</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-esx"><span class="number">6 </span><span class="name">Managing ESX</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic-odg-33x-rt"><span class="number">6.1 </span><span class="name">Networking for ESXi Hypervisor (OVSvApp)</span></a></span></dt><dt><span class="section"><a href="#verify-neutron"><span class="number">6.2 </span><span class="name">Validating the Neutron Installation</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-cluster"><span class="number">6.3 </span><span class="name">Removing a Cluster from the Compute Resource Pool</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-esxi-host"><span class="number">6.4 </span><span class="name">Removing an ESXi Host from a Cluster</span></a></span></dt><dt><span class="section"><a href="#sec-esx-debug"><span class="number">6.5 </span><span class="name">Configuring Debug Logging</span></a></span></dt><dt><span class="section"><a href="#topic-ijt-dyh-rt"><span class="number">6.6 </span><span class="name">Making Scale Configuration Changes</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-monitoring-vcenter-clusters-xml-1"><span class="number">6.7 </span><span class="name">Monitoring vCenter Clusters</span></a></span></dt><dt><span class="section"><a href="#ovsvapp-monitoring"><span class="number">6.8 </span><span class="name">Monitoring Integration with OVSvApp Appliance</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-blockstorage"><span class="number">7 </span><span class="name">Managing Block Storage</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic-e5g-z3h-gt"><span class="number">7.1 </span><span class="name">Managing Block Storage using Cinder</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-objectstorage"><span class="number">8 </span><span class="name">Managing Object Storage</span></a></span></dt><dd><dl><dt><span class="section"><a href="#swift-healthcheck"><span class="number">8.1 </span><span class="name">Running the Swift Dispersion Report</span></a></span></dt><dt><span class="section"><a href="#swift-recon"><span class="number">8.2 </span><span class="name">Gathering Swift Data</span></a></span></dt><dt><span class="section"><a href="#topic-pcv-fy4-nt"><span class="number">8.3 </span><span class="name">Gathering Swift Monitoring Metrics</span></a></span></dt><dt><span class="section"><a href="#topic-m13-dgp-nt"><span class="number">8.4 </span><span class="name">Using the Swift Command-line Client (CLI)</span></a></span></dt><dt><span class="section"><a href="#swift-ring-management"><span class="number">8.5 </span><span class="name">Managing Swift Rings</span></a></span></dt><dt><span class="section"><a href="#topic-el2-cqv-mv"><span class="number">8.6 </span><span class="name">Configuring your Swift System to Allow Container Sync</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-networking"><span class="number">9 </span><span class="name">Managing Networking</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic-gll-nsn-15"><span class="number">9.1 </span><span class="name">Configuring the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Firewall</span></a></span></dt><dt><span class="section"><a href="#DesignateOverview"><span class="number">9.2 </span><span class="name">DNS Service Overview</span></a></span></dt><dt><span class="section"><a href="#neutron-overview"><span class="number">9.3 </span><span class="name">Networking Service Overview</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-dashboards"><span class="number">10 </span><span class="name">Managing the Dashboard</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic1564-1"><span class="number">10.1 </span><span class="name">Configuring the Dashboard Service</span></a></span></dt><dt><span class="section"><a href="#horizonTimeout"><span class="number">10.2 </span><span class="name">Changing the Dashboard Timeout Value</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-orchestration"><span class="number">11 </span><span class="name">Managing Orchestration</span></a></span></dt><dd><dl><dt><span class="section"><a href="#configure-heat"><span class="number">11.1 </span><span class="name">Configuring the Orchestration Service</span></a></span></dt><dt><span class="section"><a href="#topic-sqg-cvb-dx"><span class="number">11.2 </span><span class="name">Autoscaling using the Orchestration Service</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#topic-ttn-5fg-4v"><span class="number">12 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></span></dt><dd><dl><dt><span class="section"><a href="#mon"><span class="number">12.1 </span><span class="name">Monitoring</span></a></span></dt><dt><span class="section"><a href="#centralized-logging"><span class="number">12.2 </span><span class="name">Centralized Logging Service</span></a></span></dt><dt><span class="section"><a href="#ceilo-metering-overview"><span class="number">12.3 </span><span class="name">Metering Service (Ceilometer) Overview</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#system-maintenance"><span class="number">13 </span><span class="name">System Maintenance</span></a></span></dt><dd><dl><dt><span class="section"><a href="#planned-maintenance"><span class="number">13.1 </span><span class="name">Planned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#unplanned-maintenance"><span class="number">13.2 </span><span class="name">Unplanned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#maintenance-update"><span class="number">13.3 </span><span class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span></a></span></dt><dt><span class="section"><a href="#deploy-ptf"><span class="number">13.4 </span><span class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span></a></span></dt><dt><span class="section"><a href="#database-maintenance"><span class="number">13.5 </span><span class="name">Periodic OpenStack Maintenance Tasks</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#bura-overview"><span class="number">14 </span><span class="name">Backup and Restore</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.6.16.10"><span class="number">14.1 </span><span class="name">Architecture</span></a></span></dt><dt><span class="section"><a href="#idg-all-bura-bura-overview-xml-9"><span class="number">14.2 </span><span class="name">Architecture of the Backup/Restore Service</span></a></span></dt><dt><span class="section"><a href="#topic-bxm-gxr-st"><span class="number">14.3 </span><span class="name">Default Automatic Backup Jobs</span></a></span></dt><dt><span class="section"><a href="#topic-jsc-qps-qt"><span class="number">14.4 </span><span class="name">Enabling Default Backups of the Control Plane to an SSH Target</span></a></span></dt><dt><span class="section"><a href="#topic-pth-st3-mw"><span class="number">14.5 </span><span class="name">Changing Default Jobs</span></a></span></dt><dt><span class="section"><a href="#freezerUI"><span class="number">14.6 </span><span class="name">Backup/Restore Via the Horizon UI</span></a></span></dt><dt><span class="section"><a href="#previous-backups"><span class="number">14.7 </span><span class="name">Restore from a Specific Backup</span></a></span></dt><dt><span class="section"><a href="#topic-mlh-wtn-rt"><span class="number">14.8 </span><span class="name">Backup/Restore Scheduler</span></a></span></dt><dt><span class="section"><a href="#topic-pgq-mnw-dt"><span class="number">14.9 </span><span class="name">Backup/Restore Agent</span></a></span></dt><dt><span class="section"><a href="#bu-limitations"><span class="number">14.10 </span><span class="name">Backup and Restore Limitations</span></a></span></dt><dt><span class="section"><a href="#topic-i4l-xhn-tt"><span class="number">14.11 </span><span class="name">Disabling Backup/Restore before Deployment</span></a></span></dt><dt><span class="section"><a href="#topic-dsm-fbs-st"><span class="number">14.12 </span><span class="name">Enabling, Disabling and Restoring Backup/Restore Services</span></a></span></dt><dt><span class="section"><a href="#backup-audit-logs"><span class="number">14.13 </span><span class="name">Backing up and Restoring Audit Logs</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#idg-all-operations-troubleshooting-troubleshooting-issues-xml-1"><span class="number">15 </span><span class="name">Troubleshooting Issues</span></a></span></dt><dd><dl><dt><span class="section"><a href="#general-troubleshooting"><span class="number">15.1 </span><span class="name">General Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-controlplane"><span class="number">15.2 </span><span class="name">Control Plane Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#ts-compute"><span class="number">15.3 </span><span class="name">Troubleshooting Compute Service</span></a></span></dt><dt><span class="section"><a href="#neutron-troubleshooting"><span class="number">15.4 </span><span class="name">Network Service Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-glance"><span class="number">15.5 </span><span class="name">Troubleshooting the Image (Glance) Service</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-storage"><span class="number">15.6 </span><span class="name">Storage Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#monitoring-logging-usage-reporting"><span class="number">15.7 </span><span class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#topic-ly3-yyr-st"><span class="number">15.8 </span><span class="name">Backup and Restore Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-orchestration"><span class="number">15.9 </span><span class="name">Orchestration Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-tools"><span class="number">15.10 </span><span class="name">Troubleshooting Tools</span></a></span></dt></dl></dd></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#fig-keystone-authentication-flow"><span class="number">4.1 </span><span class="name">Keystone Authentication Flow</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#intel-82599-table"><span class="number">9.1 </span><span class="name">Intel 82599 devices supported with SRIOV and PCIPT</span></a></span></dt><dt><span class="table"><a href="#table-ztc-yn5-3y"><span class="number">12.1 </span><span class="name">Aggregated Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.12.3"><span class="number">12.2 </span><span class="name">HTTP Check Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.12.5"><span class="number">12.3 </span><span class="name">HTTP Metric Components</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.14.4"><span class="number">12.4 </span><span class="name">Tunable Libvirt Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.14.6"><span class="number">12.5 </span><span class="name">Untunable Libvirt Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.19.4"><span class="number">12.6 </span><span class="name">Per-router metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.19.5"><span class="number">12.7 </span><span class="name">Per-DHCP port and rate metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.23.3"><span class="number">12.8 </span><span class="name">CPU Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.23.4"><span class="number">12.9 </span><span class="name">Disk Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.23.5"><span class="number">12.10 </span><span class="name">Load Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.23.6"><span class="number">12.11 </span><span class="name">Memory Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.6.14.3.6.23.7"><span class="number">12.12 </span><span class="name">Network Metrics</span></a></span></dt><dt><span class="table"><a href="#FreezerBackupJobScheduleTable"><span class="number">13.1 </span><span class="name">Default Interval for Freezer backup jobs</span></a></span></dt></dl></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><dl><dt><span class="example"><a href="#ex-k2kclient"><span class="number">4.1 </span><span class="name">k2kclient.py</span></a></span></dt></dl></div><div><div class="legalnotice" id="id-1.6.2.1"><p>
  Copyright © 2006–
2021

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under
  <span class="bold"><strong>Creative Commons Attribution 3.0 License
  </strong></span>:
  <a class="link" href="http://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">


  </a>
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be held
  liable for possible errors or the consequences thereof.
 </p></div></div><div class="chapter " id="gettingstarted-ops"><div class="titlepage"><div><div><h1 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Overview</span> <a title="Permalink" class="permalink" href="#gettingstarted-ops">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span>gettingstarted-ops</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.6.3.3"><span class="number">1.1 </span><span class="name">What is a cloud operator?</span></a></span></dt><dt><span class="section"><a href="#tools"><span class="number">1.2 </span><span class="name">Tools provided to operate your cloud</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.5"><span class="number">1.3 </span><span class="name">Daily tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.6"><span class="number">1.4 </span><span class="name">Weekly or monthly tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.7"><span class="number">1.5 </span><span class="name">Semi-annual tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.6.3.8"><span class="number">1.6 </span><span class="name">Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-operations-overview-xml-11"><span class="number">1.7 </span><span class="name">Common Questions</span></a></span></dt></dl></div></div><p>
  A high-level overview of the processes related to operating a
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> cloud.
 </p><div class="sect1" id="id-1.6.3.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is a cloud operator?</span> <a title="Permalink" class="permalink" href="#id-1.6.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When we talk about a cloud operator it is important to understand the scope
   of the tasks and responsibilities we are referring to. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> defines a
   cloud operator as the person or group of people who will be administering
   the cloud infrastructure, which includes:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Monitoring the cloud infrastructure, resolving issues as they arise.
    </p></li><li class="listitem "><p>
     Managing hardware resources, adding/removing hardware due to capacity
     needs.
    </p></li><li class="listitem "><p>
     Repairing, and recovering if needed, any hardware issues.
    </p></li><li class="listitem "><p>
     Performing domain administration tasks, which involves creating and
     managing projects, users, and groups as well as setting and managing
     resource quotas.
    </p></li></ul></div></div><div class="sect1" id="tools"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tools provided to operate your cloud</span> <a title="Permalink" class="permalink" href="#tools">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span>tools</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides the following tools which are available to operate your
   cloud:
  </p><p>
   <span class="bold"><strong>Operations Console</strong></span>
  </p><p>
   Often referred to as the Ops Console, you can use this console to view data
   about your cloud infrastructure in a web-based graphical user interface
   (GUI) to make sure your cloud is operating correctly. By logging on to the
   console, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> administrators can manage data in the following ways:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Triage alarm notifications in the central dashboard
    </p></li><li class="listitem "><p>
     Monitor the environment by giving priority to alarms that take precedence
    </p></li><li class="listitem "><p>
     Manage compute nodes and easily use a form to create a new host
    </p></li><li class="listitem "><p>
     Refine the monitoring environment by creating new alarms to specify a
     combination of metrics, services, and hosts that match the triggers unique
     to an environment
    </p></li><li class="listitem "><p>
     Plan for future storage by tracking capacity over time to predict with
     some degree of reliability the amount of additional storage needed
    </p></li></ul></div><p>
   For more details on how to connect to and use the Operations Console, see
   <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span>.
  </p><p>
   <span class="bold"><strong>Dashboard</strong></span>
  </p><p>
   Often referred to as Horizon or the Horizon dashboard, you can use this
   console to manage resources on a domain and project level in a web-based
   graphical user interface (GUI). The following are some of the typical
   operational tasks that you may perform using the dashboard:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Creating and managing projects, users, and groups within your domain.
    </p></li><li class="listitem "><p>
     Assigning roles to users and groups to manage access to resources.
    </p></li><li class="listitem "><p>
     Setting and updating resource quotas for the projects.
    </p></li></ul></div><p>
   For more details, see the following pages:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#sec-operation-identity" title="4.3. Understanding Domains, Projects, Users, Groups, and Roles">Section 4.3, “Understanding Domains, Projects, Users, Groups, and Roles”</a>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>
    </p></li></ul></div><p>
   <span class="bold"><strong>Command-line interface (CLI)</strong></span>
  </p><p>
   Each service within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a command-line client, such as the
   novaclient (sometimes referred to as the python-novaclient or nova CLI) for
   the Compute service, the keystoneclient for the Identity service, etc. There
   is also an effort in the OpenStack community to make a unified client,
   called the openstackclient, which will combine the available commands in the
   various service-specific clients into one tool. By default, we install each
   of the necessary clients onto the hosts in your environment for you to use.
  </p><p>
   You will find processes defined in our documentation that use these
   command-line tools. There is also a list of common cloud administration
   tasks which we have outlined which you can use the command-line tools to do.
   For more details, see <span class="intraxref">Book “User Guide”, Chapter 4 “Cloud Admin Actions with the Command Line”</span>.
  </p><p>
   There are references throughout the SUSE <span class="productname">OpenStack</span> Cloud documentation to the HPE Smart
   Storage Administrator (HPE SSA) CLI. HPE-specific binaries that are not
   based on open source are distributed directly from and supported by HPE. To
   download and install the SSACLI utility, please refer to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
  </p></div><div class="sect1" id="id-1.6.3.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Daily tasks</span> <a title="Permalink" class="permalink" href="#id-1.6.3.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Ensure your cloud is running correctly</strong></span>:
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is deployed as a set of highly available services to minimize the
     impact of failures. That said, hardware and software systems can fail.
     Detection of failures early in the process will enable you to address
     issues before they affect the broader system. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a
     monitoring solution, based on OpenStack’s Monasca, which provides
     monitoring and metrics for all OpenStack components and much of the
     underlying system, including service status, performance metrics, compute
     node, and virtual machine status. Failures are exposed via the Operations Console
     and/or alarm notifications. In the case where more detailed
     diagnostics are required, you can use a centralized logging system based
     on the Elasticsearch, Logstash, and Kibana (ELK) stack. This provides the
     ability to search service logs to get detailed information on behavior and
     errors.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Perform critical maintenance</strong></span>: To ensure
     your OpenStack installation is running correctly, provides the right
     access and functionality, and is secure, you should make ongoing
     adjustments to the environment. Examples of daily maintenance tasks
     include:
    </p><div class="itemizedlist " id="ul-nx2-z4x-rv"><ul class="itemizedlist"><li class="listitem "><p>
       Add/remove projects and users. The frequency of this task depends on
       your policy.
      </p></li><li class="listitem "><p>
       Apply security patches (if released).
      </p></li><li class="listitem "><p>
       Run daily backups.
      </p></li></ul></div></li></ul></div></div><div class="sect1" id="id-1.6.3.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Weekly or monthly tasks</span> <a title="Permalink" class="permalink" href="#id-1.6.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist " id="ul-nz4-npx-rv"><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Do regular capacity planning</strong></span>: Your
     initial deployment will likely reflect the known near to mid-term scale
     requirements, but at some point your needs will outgrow your initial
     deployment’s capacity. You can expand <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in a variety of ways,
     such as by adding compute and storage capacity.
    </p></li></ul></div><p>
   To manage your cloud’s capacity, begin by determining the load on the
   existing system. OpenStack is a set of relatively independent components and
   services, so there are multiple subsystems that can affect capacity. These
   include control plane nodes, compute nodes, object storage nodes, block
   storage nodes, and an image management system. At the most basic level, you
   should look at the CPU used, RAM used, I/O load, and the disk space used
   relative to the amounts available. For compute nodes, you can also evaluate
   the allocation of resource to hosted virtual machines. This information can
   be viewed in the Operations Console. You can pull historical information
   from the monitoring service (OpenStack’s Monasca) by using its client or
   API. Also, OpenStack provides you some ability to manage the hosted resource
   utilization by using quotas for projects. You can track this usage over time
   to get your growth trend so that you can project when you will need to add
   capacity.
  </p></div><div class="sect1" id="id-1.6.3.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Semi-annual tasks</span> <a title="Permalink" class="permalink" href="#id-1.6.3.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist " id="ul-y1v-5sx-rv"><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Perform upgrades</strong></span>: OpenStack releases new
     versions on a six-month cycle. In general, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will release new
     major versions annually with minor versions and maintenance updates more
     often. Each new release consists of both new functionality and services,
     as well as bug fixes for existing functionality.
    </p></li></ul></div><div id="id-1.6.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you are planning to upgrade, this is also an excellent time to evaluate
    your existing capabilities, especially in terms of capacity (see Capacity
    Planning above).
   </p></div></div><div class="sect1" id="id-1.6.3.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#id-1.6.3.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As part of managing your cloud, you should be ready to troubleshoot issues,
   as needed. The following are some common troubleshooting scenarios and
   solutions:
  </p><p>
   <span class="bold"><strong>How do I determine if my cloud is operating correctly
   now?</strong></span>: <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a monitoring solution based on
   OpenStack’s Monasca service. This service provides monitoring and metrics
   for all OpenStack components, as well as much of the underlying system. By
   default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with a set of alarms that provide coverage of the
   primary systems. In addition, you can define alarms based on threshold
   values for any metrics defined in the system. You can view alarm information
   in the Operations Console. You can also receive or deliver this information
   to others by configuring email or other mechanisms. Alarms provide
   information about whether a component failed and is affecting the system,
   and also what condition triggered the alarm.
  </p><p>
   <span class="bold"><strong>How do I troubleshoot and resolve performance issues
   for my cloud?</strong></span>: There are a variety of factors that can affect the
   performance of a cloud system, such as the following:
  </p><div class="itemizedlist " id="ul-vb3-ttx-rv"><ul class="itemizedlist"><li class="listitem "><p>
     Health of the control plane
    </p></li><li class="listitem "><p>
     Health of the hosting compute node and virtualization layer
    </p></li><li class="listitem "><p>
     Resource allocation on the compute node
    </p></li></ul></div><p>
   If your cloud users are experiencing performance issues on your cloud, use
   the following approach:
  </p><div class="orderedlist " id="ol-wb3-ttx-rv"><ol class="orderedlist" type="1"><li class="listitem "><p>
     View the compute summary page on the Operations Console to determine if
     any alarms have been triggered.
    </p></li><li class="listitem "><p>
     Determine the hosting node of the virtual machine that is having issues.
    </p></li><li class="listitem "><p>
     On the compute hosts page, view the status and resource utilization of the
     compute node to determine if it has errors or is over-allocated.
    </p></li><li class="listitem "><p>
     On the compute instances page you can view the status of the VM along with
     its metrics.
    </p></li></ol></div><p>
   <span class="bold"><strong>How do I troubleshoot and resolve availability issues
   for my cloud?</strong></span>: If your cloud users are experiencing availability
   issues, determine what your users are experiencing that indicates to them
   the cloud is down. For example, can they not access the Dashboard service
   (Horizon) console or APIs, indicating a problem with the control plane? Or
   are they having trouble accessing resources? Console/API issues would
   indicate a problem with the control planes. Use the Operations Console to
   view the status of services to see if there is an issue. However, if it is
   an issue of accessing a virtual machine, then also search the consolidated
   logs that are available in the ELK stack or errors related to the virtual
   machine and supporting networking.
  </p></div><div class="sect1" id="idg-all-operations-operations-overview-xml-11"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Common Questions</span> <a title="Permalink" class="permalink" href="#idg-all-operations-operations-overview-xml-11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-operations-overview-xml-11</li></ul></div></div></div></div><p>
   <span class="bold"><strong>To manage a cloud, how many administrators do I
   need?</strong></span>
  </p><p>
   A 24x7 cloud needs a 24x7 cloud operations team. If you already have a NOC,
   managing the cloud can be added to their workload.
  </p><p>
   A cloud with 20 nodes will need a part-time person. You can manage a cloud
   with 200 nodes with two people. As the amount of nodes increases and
   processes and automation are put in place, you will need to increase the
   number of administrators but the need is not linear. As an example, if you
   have 3000 nodes and 15 clouds you will probably need 6 administrators.
  </p><p>
   <span class="bold"><strong>What skills do my cloud administrators need?</strong></span>
  </p><p>
   Your administrators should be experienced Linux admins. They should have
   experience in application management, as well as experience with Ansible.
   It is a plus if they have experience with Bash shell scripting and Python
   programming skills.
  </p><p>
   In addition, you will need networking engineers. A 3000 node environment
   will need two networking engineers.
  </p><p>
   <span class="bold"><strong>What operations should I plan on performing daily,
   weekly, monthly, or semi-annually?</strong></span>
  </p><p>
   You should plan for operations by understanding what tasks you need to do
   daily, weekly, monthly, or semi-annually. The specific list of tasks that
   you need to perform depends on your cloud configuration, but should include
   the following high-level tasks specified in the <a class="xref" href="#tutorials" title="Chapter 2. Tutorials">Chapter 2, <em>Tutorials</em></a>
  </p></div></div><div class="chapter " id="tutorials"><div class="titlepage"><div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tutorials</span> <a title="Permalink" class="permalink" href="#tutorials">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials.xml</li><li><span class="ds-label">ID: </span>tutorials</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#Quickstart-Guide"><span class="number">2.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Quickstart Guide</span></a></span></dt><dt><span class="section"><a href="#log-management-integration"><span class="number">2.2 </span><span class="name">Log Management and Integration</span></a></span></dt><dt><span class="section"><a href="#Integrating-Kibana-with-Splunk"><span class="number">2.3 </span><span class="name">Integrating Your Logs with Splunk</span></a></span></dt><dt><span class="section"><a href="#LDAP-Integration"><span class="number">2.4 </span><span class="name">Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with an LDAP System</span></a></span></dt></dl></div></div><p>
  This section contains tutorials for common tasks for your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
  cloud.
 </p><div class="sect1" id="Quickstart-Guide"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Quickstart Guide</span> <a title="Permalink" class="permalink" href="#Quickstart-Guide">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span>Quickstart-Guide</li></ul></div></div></div></div><div class="sect2" id="id-1.6.4.3.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#id-1.6.4.3.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This document provides simplified instructions for installing and setting up
   a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Use this quickstart guide to build testing, demonstration, and
   lab-type environments., rather than production installations. When you
   complete this quickstart process, you will have a fully functioning <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   demo environment.
  </p><div id="id-1.6.4.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    These simplified instructions are intended for testing or
    demonstration. Instructions for production installations are in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”</span>.
   </p></div></div><div class="sect2" id="id-1.6.4.3.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview of components</span> <a title="Permalink" class="permalink" href="#id-1.6.4.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are short descriptions of the components that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> employs
   when installing and deploying your cloud.
  </p><p><span class="formalpara-title">Ansible. </span>
    Ansible is a powerful configuration management tool used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to
    manage nearly all aspects of your cloud infrastructure. Most commands in
    this quickstart guide execute Ansible scripts, known as playbooks. You will
    run playbooks that install packages, edit configuration files, manage
    network settings, and take care of the general administration tasks
    required to get your cloud up and running.
   </p><p>
   Get more information on Ansible at
   <a class="link" href="https://www.ansible.com/" target="_blank">https://www.ansible.com/</a>.
  </p><p><span class="formalpara-title">Cobbler. </span>
    Cobbler is another third-party tool used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to deploy
    operating systems across the physical servers that make up your cloud. Find
    more info at <a class="link" href="http://cobbler.github.io/" target="_blank">http://cobbler.github.io/</a>.
   </p><p><span class="formalpara-title">Git. </span>
    Git is the version control system used to manage the configuration files
    that define your cloud. Any changes made to your cloud configuration files
    must be committed to the locally hosted git repository to take effect. Read
    more information on Git at <a class="link" href="https://git-scm.com/" target="_blank">https://git-scm.com/</a>.
   </p></div><div class="sect2" id="id-1.6.4.3.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation</span> <a title="Permalink" class="permalink" href="#id-1.6.4.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Successfully deploying a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment is a large endeavor, but it is
   not complicated. For a successful deployment, you must put a number of
   components in place before rolling out your cloud. Most importantly, a basic
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> requires the proper network infrastrucure. Because <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   segregates the network traffic of many of its elements, if the necessary
   networks, routes, and firewall access rules are not in place, communication
   required for a successful deployment will not occur.
  </p></div><div class="sect2" id="section-v5g-dvv-xw"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started</span> <a title="Permalink" class="permalink" href="#section-v5g-dvv-xw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span>section-v5g-dvv-xw</li></ul></div></div></div></div><p>
   When your network infrastructure is in place, go ahead and set up the Cloud Lifecycle Manager.
   This is the server that will orchestrate the deployment of the rest of your
   cloud. It is also the server you will run most of your deployment and
   management commands on.
  </p><p>
   <span class="bold"><strong>Set up the Cloud Lifecycle Manager</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     <span class="bold"><strong>Download the installation media</strong></span>
    </p><p>
     Obtain a copy of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation media, and make sure that it is
     accessible by the server that you are installing it on. Your method of
     doing this may vary. For instance, some may choose to load the
     installation ISO on a USB drive and physically attach it to the server,
     while others may run the IPMI Remote Console and attach the ISO to a
     virtual disc drive.
    </p></li><li class="step "><p>
     <span class="bold"><strong>Install the operating system</strong></span>
    </p><ol type="a" class="substeps "><li class="step "><p>
       Boot your server, using the installation media as the boot source.
      </p></li><li class="step "><p>
       Choose "install" from the list of options and choose your preferred
       keyboard layout, location, language, and other settings.
      </p></li><li class="step "><p>
       Set the address, netmask, and gateway for the primary network interface.
      </p></li><li class="step "><p>
       Create a root user account.
      </p></li></ol><p>
     Proceed with the OS installation. After the installation is complete and
     the server has rebooted into the new OS, log in with the user account you
     created.
    </p></li><li class="step "><p>
     <span class="bold"><strong>Configure the new server</strong></span>
    </p><ol type="a" class="substeps "><li class="step "><p>
       SSH to your new server, and set a valid DNS nameserver in the
       <code class="filename">/etc/resolv.conf</code> file.
      </p></li><li class="step "><p>
       Set the environment variable <code class="literal">LC_ALL</code>:
      </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div></li></ol><p>
     You now have a server running SUSE Linux Enterprise Server (SLES).
     The next step is to configure this machine as a Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     <span class="bold"><strong>Configure the Cloud Lifecycle Manager</strong></span>
    </p><p>
     The installation media you used to install the OS on the server also has
     the files that will configure your cloud. You need to mount this
     installation media on your new server in order to use these files.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Using the URL that you obtained the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation media from,
       run <code class="literal">wget</code> to download the ISO file to your server:
      </p><div class="verbatim-wrap"><pre class="screen">wget <em class="replaceable ">INSTALLATION_ISO_URL</em></pre></div></li><li class="step "><p>
       Now mount the ISO in the <code class="filename">/media/cdrom/</code> directory
      </p><div class="verbatim-wrap"><pre class="screen">sudo mount <em class="replaceable ">INSTALLATION_ISO</em> /media/cdrom/</pre></div></li><li class="step "><p>
       Unpack the tar file found in the
       <code class="filename">/media/cdrom/ardana/</code> directory where you just
       mounted the ISO:
      </p><div class="verbatim-wrap"><pre class="screen">tar xvf /media/cdrom/ardana/ardana-x.x.x-x.tar</pre></div></li><li class="step "><p>
       Now you will install and configure all the components needed to turn this
       server into a Cloud Lifecycle Manager. Run the <code class="filename">ardana-init.bash</code>
       script from the uncompressed tar file:
      </p><div class="verbatim-wrap"><pre class="screen">~/ardana-x.x.x/ardana-init.bash</pre></div><p>
       The <code class="filename">ardana-init.bash</code> script prompts you to enter an
       optional SSH passphrase. This passphrase protects the RSA key used to
       SSH to the other cloud nodes. This is an optional passphrase, and you
       can skip it by pressing <span class="keycap">Enter</span> at the prompt.
      </p><p>
       The <code class="filename">ardana-init.bash</code> script automatically installs
       and configures everything needed to set up this server as the lifecycle
       manager for your cloud.
      </p><p>
       When the script has finished running, you can proceed to the next step,
       editing your input files.
      </p></li></ol></li><li class="step "><p>
     <span class="bold"><strong>Edit your input files</strong></span>
    </p><p>
     Your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input files are where you define your cloud
     infrastructure and how it runs. The input files define options such as
     which servers are included in your cloud, the type of disks the servers
     use, and their network configuration. The input files also define which
     services your cloud will provide and use, the network architecture, and
     the storage backends for your cloud.
    </p><p>
     There are several example configurations, which you can find on your Cloud Lifecycle Manager
     in the <code class="filename">~/openstack/examples/</code> directory.
    </p><ol type="a" class="substeps "><li class="step "><p>
       The simplest way to set up your cloud is to copy the contents of one of
       these example configurations to your
       <code class="filename">~/openstack/mycloud/definition/</code> directory. You can
       then edit the copied files and define your cloud.
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/<em class="replaceable ">CHOSEN_EXAMPLE</em>/* ~/openstack/my_cloud/definition/</pre></div></li><li class="step "><p>
       Edit the files in your
       <code class="filename">~/openstack/my_cloud/definition/</code> directory to
       define your cloud.
      </p></li></ol></li><li class="step "><p>
     <span class="bold"><strong>Commit your changes</strong></span>
    </p><p>
     When you finish editing the necessary input files, stage them, and
     then commit the changes to the local Git repository:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My commit message"</pre></div></li><li class="step "><p>
     <span class="bold"><strong>Image your servers</strong></span>
    </p><p>
     Now that you have finished editing your input files, you can deploy the
     configuration to the servers that will comprise your cloud.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Image the servers. You will install the SLES operating system across
       all the servers in your cloud, using Ansible playbooks to trigger the
       process.
      </p></li><li class="step "><p>
       The following playbook confirms that your servers are accessible over
       their IPMI ports, which is a prerequisite for the imaging process:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Now validate that your cloud configuration files have proper YAML syntax
       by running the <code class="filename">config-processor-run.yml</code> playbook:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
       If you receive an error when running the preceeding playbook, one or
       more of your configuration files has an issue. Refer to the output of
       the Ansible playbook, and look for clues in the Ansible log file, found
       at <code class="filename">~/.ansible/ansible.log</code>.
      </p></li><li class="step "><p>
       The next step is to prepare your imaging system, Cobbler, to deploy
       operating systems to all your cloud nodes:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
       Now you can image your cloud nodes. You will use an Ansible playbook to
       trigger Cobbler to deploy operating systems to all the nodes you
       specified in your input files:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost bm-reimage.yml</pre></div><p>
       The <code class="filename">bm-reimage.yml</code> playbook performs the following
       operations:
      </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
         Powers down the servers.
        </p></li><li class="listitem "><p>
         Sets the servers to boot from a network interface.
        </p></li><li class="listitem "><p>
         Powers on the servers and performs a PXE OS installation.
        </p></li><li class="listitem "><p>
         Waits for the servers to power themselves down as part of a successful
         OS installation. This can take some time.
        </p></li><li class="listitem "><p>
         Sets the servers to boot from their local hard disks and powers on the
         servers.
        </p></li><li class="listitem "><p>
         Waits for the SSH service to start on the servers and verifies that
         they have the expected host-key signature.
        </p></li></ol></div></li></ol></li><li class="step "><p>
     <span class="bold"><strong>Deploy your cloud</strong></span>
    </p><p>
     Now that your servers are running the SLES operating system, it is time
     to configure them for the roles they will play in your new cloud.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Prepare the Cloud Lifecycle Manager to deploy your cloud configuration to all the nodes:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
       NOTE: The preceding playbook creates a new directory,
       <code class="filename">~/scratch/ansible/next/ardana/ansible/</code>, from which
       you will run many of the following commands.
      </p></li><li class="step "><p><span class="step-optional">(Optional)</span> 
       If you are reusing servers or disks to run your cloud, you can wipe the
       disks of your newly imaged servers by running the
       <code class="filename">wipe_disks.yml</code> playbook:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
       The <code class="filename">wipe_disks.yml</code> playbook removes any existing
       data from the drives on your new servers. This can be helpful if you are
       reusing servers or disks. This action will not affect the OS partitions
       on the servers.
      </p><div id="id-1.6.4.3.5.4.8.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        The <code class="filename">wipe_disks.yml</code> playbook is only meant to be
        run on systems immediately after running
        <code class="filename">bm-reimage.yml</code>.  If used for any other case, it
        may not wipe all of the expected partitions. For example, if
        <code class="filename">site.yml</code> fails, you cannot start fresh by running
        <code class="filename">wipe_disks.yml</code>. You must
        <code class="literal">bm-reimage</code> the node first and then run
        <code class="literal">wipe_disks</code>.
       </p></div></li><li class="step "><p>
       Now it is time to deploy your cloud. Do this by running the
       <code class="filename">site.yml</code> playbook, which pushes the configuration
       you defined in the input files out to all the servers that will host
       your cloud.
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
       The <code class="filename">site.yml</code> playbook installs packages, starts
       services, configures network interface settings, sets iptables firewall
       rules, and more. Upon successful completion of this playbook, your
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will be in place and in a running state. This playbook can take
       up to six hours to complete.
      </p></li></ol></li><li class="step "><p>
     <span class="bold"><strong>SSH to your nodes</strong></span>
    </p><p>
     Now that you have successfully run <code class="filename">site.yml</code>, your cloud
     will be up and running. You can verify connectivity to your nodes by
     connecting to each one by using SSH. You can find the IP addresses of your
     nodes by viewing the <code class="filename">/etc/hosts</code> file.
    </p><p>
     For security reasons, you can only SSH to your nodes from the Cloud Lifecycle Manager. SSH
     connections from any machine other than the Cloud Lifecycle Manager will be refused by the
     nodes.
    </p><p>
     From the Cloud Lifecycle Manager, SSH to your nodes:
    </p><div class="verbatim-wrap"><pre class="screen">ssh &lt;management IP address of node&gt;</pre></div><p>
     Also note that SSH is limited to your cloud's management network. Each
     node has an address on the management network, and you can find this
     address by reading the <code class="filename">/etc/hosts</code> or
     <code class="filename">server_info.yml</code> file.
    </p></li></ol></div></div></div></div><div class="sect1" id="log-management-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Log Management and Integration</span> <a title="Permalink" class="permalink" href="#log-management-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>log-management-integration</li></ul></div></div></div></div><div class="sect2" id="id-1.6.4.4.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#id-1.6.4.4.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the ELK (Elasticsearch, Logstash, Kibana) stack for log
   management across the entire cloud infrastructure. This configuration
   facilitates simple administration as well as integration with third-party
   tools. This tutorial covers how to forward your logs to a third-party tool
   or service, and how to access and search the Elasticsearch log stores
   through API endpoints.
  </p></div><div class="sect2" id="section-pvm-zkx-1x"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The ELK stack</span> <a title="Permalink" class="permalink" href="#section-pvm-zkx-1x">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>section-pvm-zkx-1x</li></ul></div></div></div></div><p>
   The ELK logging stack consists of the Elasticsearch, Logstash, and
   Kibana elements:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Logstash. </span>
      Logstash reads the log data from the services running on your servers,
      and then aggregates and ships that data to a storage location. By
      default,
    
    
      Logstash sends the data to the Elasticsearch indexes, but it can also
      be configured to send data to other storage and indexing tools such as
      Splunk.
     </p></li><li class="listitem "><p><span class="formalpara-title">Elasticsearch. </span>
      Elasticsearch is the storage and indexing component of the ELK stack.
      It stores and indexes the data received from Logstash. Indexing makes
      your log data searchable by tools designed for querying and analyzing
      massive sets of data. You can query the Elasticsearch datasets from the
      built-in Kibana console, a third-party data analysis tool, or through
      the Elasticsearch API (covered later).
     </p></li><li class="listitem "><p><span class="formalpara-title">Kibana. </span>
      Kibana provides a simple and easy-to-use method for searching,
      analyzing, and visualizing the log data stored in the Elasticsearch
      indexes. You can customize the Kibana console to provide graphs,
      charts, and other visualizations of your log data.
     </p></li></ul></div></div><div class="sect2" id="section-d3k-gnx-1x"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Elasticsearch API</span> <a title="Permalink" class="permalink" href="#section-d3k-gnx-1x">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>section-d3k-gnx-1x</li></ul></div></div></div></div><p>
   You can query the Elasticsearch indexes through various language-specific
   APIs, as well as directly over the IP address and port that Elasticsearch
   exposes on your implementation. By default, Elasticsearch presents from
   localhost, port 9200. You can run queries directly from a terminal using
   <code class="literal">curl</code>. For example:
  </p><div class="verbatim-wrap"><pre class="screen">curl -XGET 'http://localhost:9200/_search?q=tag:yourSearchTag'</pre></div><p>
   The preceding command searches all indexes for all data with the
   "yourSearchTag" tag.
  </p><p>
   You can also use the Elasticsearch API from outside the logging node. This
   method connects over the Kibana VIP address, port 5601, using basic http
   authentication. For example, you can use the following command to perform
   the same search as the preceding search:
  </p><div class="verbatim-wrap"><pre class="screen">curl -u kibana:&lt;password&gt; kibana_vip:5601/_search?q=tag:yourSearchTag</pre></div><p>
   You can further refine your search to a specific index of data, in this case
   the "elasticsearch" index:
  </p><div class="verbatim-wrap"><pre class="screen">curl -XGET 'http://localhost:9200/elasticsearch/_search?q=tag:yourSearchTag'</pre></div><p>
   The search API is RESTful, so responses are provided in JSON format. Here's
   a sample (though empty) response:
  </p><div class="verbatim-wrap"><pre class="screen">{
    "took":13,
    "timed_out":false,
    "_shards":{
        "total":45,
        "successful":45,
        "failed":0
    },
    "hits":{
        "total":0,
        "max_score":null,
        "hits":[]
    }
}</pre></div></div><div class="sect2" id="id-1.6.4.4.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.6.4.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can find more detailed Elasticsearch API documentation at
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html</a>.
  </p><p>
   Review the Elasticsearch Python API documentation at the following sources:
   <a class="link" href="http://elasticsearch-py.readthedocs.io/en/master/api.html" target="_blank">http://elasticsearch-py.readthedocs.io/en/master/api.html</a>
  </p><p>
   Read the Elasticsearch Java API documentation at
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html</a>.
  </p></div><div class="sect2" id="section-knd-hcf-bx"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forwarding your logs</span> <a title="Permalink" class="permalink" href="#section-knd-hcf-bx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>section-knd-hcf-bx</li></ul></div></div></div></div><p>
   You can configure Logstash to ship your logs to an outside storage and
   indexing system, such as Splunk. Setting up this configuration is as simple
   as editing a few configuration files, and then running the Ansible playbooks
   that implement the changes. Here are the steps.
  </p><div class="orderedlist " id="ol-mtz-mnr-bx"><ol class="orderedlist" type="1"><li class="listitem "><p>
     Begin by logging in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Verify that the logging system is up and running:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts logging-status.yml</pre></div><p>
     When the preceding playbook completes without error, proceed to the next
     step.
    </p></li><li class="listitem "><p>
     Edit the Logstash configuration file, found at the following location:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/logging-server/templates/logstash.conf.j2</pre></div><p>
     Near the end of the Logstash configuration file, you will find a section for
     configuring Logstash output destinations. The following example
     demonstrates the changes necessary to forward your logs to an outside
     server (changes in bold). The configuration block sets up a TCP connection
     to the destination server's IP address over port 5514.
    </p><div class="verbatim-wrap"><pre class="screen"># Logstash outputs
    output {
      # Configure Elasticsearch output
      # http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
      elasticsearch {
        index =&gt; "%{[@metadata][es_index]}
        hosts =&gt; ["{{ elasticsearch_http_host }}:{{ elasticsearch_http_port }}"]
        flush_size =&gt; {{ logstash_flush_size }}
        idle_flush_time =&gt; 5
        workers =&gt; {{ logstash_threads }}
      }
      <span class="bold"><strong>  # Forward Logs to Splunk on TCP port 5514 which matches the one specified in Splunk Web UI.
      tcp {
        mode =&gt; "client"
        host =&gt; "&lt;Enter Destination listener IP address&gt;"
        port =&gt; 5514
      }</strong></span>
    }</pre></div><p>
     Note that Logstash can forward log data to multiple sources, so there is no
     need to remove or alter the Elasticsearch section in the preceding file.
     However, if you choose to stop forwarding your log data to Elasticsearch,
     you can do so by removing the related section in this file, and then
     continue with the following steps.
    </p></li><li class="listitem "><p>
     Commit your changes to the local git repository:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "Your commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor to check the status of all configuration
     files:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Run the ready-deployment playbook:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Implement the changes to the Logstash configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts logging-server-configure.yml</pre></div></li></ol></div><p>
   Please note that configuring the receiving service will vary from
   product to product. Consult the documentation for your particular product
   for instructions on how to set it up to receive log files from Logstash.
  </p></div></div><div class="sect1" id="Integrating-Kibana-with-Splunk"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating Your Logs with Splunk</span> <a title="Permalink" class="permalink" href="#Integrating-Kibana-with-Splunk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-integrating_logstash_splunk.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-integrating_logstash_splunk.xml</li><li><span class="ds-label">ID: </span>Integrating-Kibana-with-Splunk</li></ul></div></div></div></div><div class="sect2" id="idg-all-operations-tutorials-integrating-logstash-splunk-xml-2"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with Splunk</span> <a title="Permalink" class="permalink" href="#idg-all-operations-tutorials-integrating-logstash-splunk-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-integrating_logstash_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-integrating_logstash_splunk.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-tutorials-integrating-logstash-splunk-xml-2</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> logging solution provides a flexible and extensible
   framework to centralize the collection and processing of logs from all nodes
   in your cloud. The logs are shipped to a highly available and fault-tolerant
   cluster where they are transformed and stored for better searching and
   reporting. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> logging solution uses the ELK stack
   (Elasticsearch, Logstash and Kibana) as a production-grade implementation
   and can support other storage and indexing technologies.
  </p><p>
   You can configure Logstash, the service that aggregates and forwards the
   logs to a searchable index, to send the logs to a third-party target, such
   as Splunk.
  </p><p>
   For how to integrate the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> centralized
   logging solution with Splunk, including the steps to set up and forward
   logs, please refer to <a class="xref" href="#splunk-integration" title="3.1. Splunk Integration">Section 3.1, “Splunk Integration”</a>.
  </p></div></div><div class="sect1" id="LDAP-Integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with an LDAP System</span> <a title="Permalink" class="permalink" href="#LDAP-Integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-keystone_ldap_integration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-keystone_ldap_integration.xml</li><li><span class="ds-label">ID: </span>LDAP-Integration</li></ul></div></div></div></div><p>
  You can configure your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud to work with an outside user
  authentication source such as Active Directory or OpenLDAP. Keystone, the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> identity service, functions as the first stop for any user
  authorization/authentication requests. Keystone can also function as a proxy
  for user account authentication, passing along authentication and
  authorization requests to any LDAP-enabled system that has been configured as
  an outside source. This type of integration lets you use an existing
  user-management system such as Active Directory and its powerful group-based
  organization features as a source for permissions in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  Upon successful completion of this tutorial, your cloud will refer user
  authentication requests to an outside LDAP-enabled directory system, such as
  Microsoft Active Directory or OpenLDAP.
 </p><div class="sect2" id="id-1.6.4.6.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure your LDAP source</span> <a title="Permalink" class="permalink" href="#id-1.6.4.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-tutorials-keystone_ldap_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-keystone_ldap_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To configure your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud to use an outside user-management source,
   perform the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Make sure that the LDAP-enabled system you plan to integrate with is up
     and running and accessible over the necessary ports from your cloud
     management network.
    </p></li><li class="step "><p>
     Edit the
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/keystone/keystone.conf.j2</code>
     file and set the following options:
    </p><div class="verbatim-wrap"><pre class="screen">domain_specific_drivers_enabled = True
domain_configurations_from_database = False</pre></div></li><li class="step " id="st-keystone-ldap-create-yaml"><p>
     Create a YAML file in the
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/keystone/</code>
     directory that defines your LDAP connection. You can make a copy of the
     sample Keystone-LDAP configuration file, and then edit that file with the
     details of your LDAP connection.
    </p><p>
     The following example copies the
     <code class="filename">keystone_configure_ldap_sample.yml</code> file and names the
     new file <code class="filename">keystone_configure_ldap_my.yml</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp /var/lib/ardana/openstack/my_cloud/config/keystone/keystone_configure_ldap_sample.yml \
  /var/lib/ardana/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div></li><li class="step "><p>
     Edit the new file to define the connection to your LDAP source. This guide
     does not provide comprehensive information on all aspects of the
     <code class="literal">keystone_configure_ldap.yml</code> file. Find a complete list
     of Keystone/LDAP configuration file options at:
     <a class="link" href="https://github.com/openstack/keystone/blob/stable/pike/etc/keystone.conf.sample" target="_blank">https://github.com/openstack/keystone/blob/stable/pike/etc/keystone.conf.sample</a>
    </p><p>
     The following file illustrates an example Keystone configuration that is
     customized for an Active Directory connection.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_domainldap_conf:

    # CA certificates file content.
    # Certificates are stored in Base64 PEM format. This may be entire LDAP server
    # certificate (in case of self-signed certificates), certificate of authority
    # which issued LDAP server certificate, or a full certificate chain (Root CA
    # certificate, intermediate CA certificate(s), issuer certificate).
    #
    cert_settings:
      cacert: |
        -----BEGIN CERTIFICATE-----

        certificate appears here

        -----END CERTIFICATE-----

    # A domain will be created in MariaDB with this name, and associated with ldap back end.
    # Installer will also generate a config file named /etc/keystone/domains/keystone.&lt;domain_name&gt;.conf
    #
    domain_settings:
      name: ad
      description: Dedicated domain for ad users

    conf_settings:
      identity:
         driver: ldap


      # For a full list and description of ldap configuration options, please refer to
      # http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html.
      #
      # Please note:
      #  1. LDAP configuration is read-only. Configuration which performs write operations (i.e. creates users, groups, etc)
      #     is not supported at the moment.
      #  2. LDAP is only supported for identity operations (reading users and groups from LDAP). Assignment
      #     operations with LDAP (i.e. managing roles, projects) are not supported.
      #  3. LDAP is configured as non-default domain. Configuring LDAP as a default domain is not supported.
      #

      ldap:
        url: ldap://<em class="replaceable ">YOUR_COMPANY_AD_URL</em>
        suffix: <em class="replaceable ">YOUR_COMPANY_DC</em>
        query_scope: sub
        user_tree_dn: CN=Users,<em class="replaceable ">YOUR_COMPANY_DC</em>
        user : CN=admin,CN=Users,<em class="replaceable ">YOUR_COMPANY_DC</em>
        password: REDACTED
        user_objectclass: user
        user_id_attribute: cn
        user_name_attribute: cn
        group_tree_dn: CN=Users,<em class="replaceable ">YOUR_COMPANY_DC</em>
        group_objectclass: group
        group_id_attribute: cn
        group_name_attribute: cn
        use_pool: True
        user_enabled_attribute: userAccountControl
        user_enabled_mask: 2
        user_enabled_default: 512
        use_tls: True
        tls_req_cert: demand
        # if you are configuring multiple LDAP domains, and LDAP server certificates are issued
        # by different authorities, make sure that you place certs for all the LDAP backend domains in the
        # cacert parameter as seen in this sample yml file so that all the certs are combined in a single CA file
        # and every LDAP domain configuration points to the combined CA file.
        # Note:
        # 1. Please be advised that every time a new ldap domain is configured, the single CA file gets overwritten
        # and hence ensure that you place certs for all the LDAP backend domains in the cacert parameter.
        # 2. There is a known issue on one cert per CA file per domain when the system processes
        # concurrent requests to multiple LDAP domains. Using the single CA file with all certs combined
        # shall get the system working properly.

        tls_cacertfile: /etc/keystone/ssl/certs/all_ldapdomains_ca.pem</pre></div></li><li class="step "><p>
     Add your new file to the local Git repository and commit the changes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Adding LDAP server integration config"</pre></div></li><li class="step "><p>
     Run the configuration processor and deployment preparation playbooks to
     validate the YAML files and prepare the environment for configuration.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the Keystone reconfiguration playbook to implement your changes,
     passing the newly created YAML file as an argument to the
     <code class="literal">-e@<em class="replaceable ">FILE_PATH</em></code> parameter:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml \
  -e@/var/lib/ardana/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div><p>
     To integrate your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud with multiple domains, repeat these
     steps starting from <a class="xref" href="#st-keystone-ldap-create-yaml" title="Step 3">Step 3</a>
     for each domain.
    </p></li></ol></div></div></div></div></div><div class="chapter " id="third-party-integrations"><div class="titlepage"><div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Third-Party Integrations</span> <a title="Permalink" class="permalink" href="#third-party-integrations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-thirdparty_integrations.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-thirdparty_integrations.xml</li><li><span class="ds-label">ID: </span>third-party-integrations</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#splunk-integration"><span class="number">3.1 </span><span class="name">Splunk Integration</span></a></span></dt><dt><span class="section"><a href="#nagios-integration"><span class="number">3.2 </span><span class="name">Nagios Integration</span></a></span></dt><dt><span class="section"><a href="#topic-kyf-brv-vw"><span class="number">3.3 </span><span class="name">Operations Bridge Integration</span></a></span></dt><dt><span class="section"><a href="#monitoring-3rd-party-components-with-monasca"><span class="number">3.4 </span><span class="name">Monitoring Third-Party Components With Monasca</span></a></span></dt></dl></div></div><div class="sect1" id="splunk-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Splunk Integration</span> <a title="Permalink" class="permalink" href="#splunk-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span>splunk-integration</li></ul></div></div></div></div><p>
  This documentation demonstrates the possible integration between the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> centralized logging solution and Splunk including the steps
  to set up and forward logs.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> logging solution provides a flexible and extensible
  framework to centralize the collection and processing of logs from all of
  the nodes in a cloud. The logs are shipped to a highly available and fault
  tolerant cluster where they are transformed and stored for better searching
  and reporting. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> logging solution uses the ELK stack
  (Elasticsearch, Logstash and Kibana) as a production grade implementation
  and can support other storage and indexing technologies. The Logstash
  pipeline can be configured to forward the logs to an alternative target if
  you wish.
 </p><p>
  This documentation demonstrates the possible integration between the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> centralized logging solution and Splunk including the steps
  to set up and forward logs.
 </p><div class="sect2" id="idg-all-operations-integrating-splunk-xml-5"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is Splunk?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-integrating-splunk-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-integrating-splunk-xml-5</li></ul></div></div></div></div><p>
   Splunk is software for searching, monitoring, and analyzing
   <a class="link" href="https://en.wikipedia.org/wiki/Machine-generated_data" target="_blank">machine-generated
   big data</a>, via a web-style interface. Splunk captures, indexes and
   correlates real-time data in a searchable repository from which it can
   generate graphs, reports, alerts, dashboards and visualizations. It is
   commercial software (unlike Elasticsearch) and more details about Splunk
   can be found at <a class="link" href="https://www.splunk.com" target="_blank">https://www.splunk.com</a>.
  </p></div><div class="sect2" id="idg-all-operations-integrating-splunk-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Splunk to receive log messages from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span></span> <a title="Permalink" class="permalink" href="#idg-all-operations-integrating-splunk-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-integrating-splunk-xml-6</li></ul></div></div></div></div><p>
   This documentation assumes that you already have Splunk set up and running.
   For help with installing and setting up Splunk, refer to
   <a class="link" href="http://docs.splunk.com/Documentation/Splunk/latest/SearchTutorial/Systemrequirements" target="_blank">Splunk
   Tutorial</a>.
  </p><p>
   There are different ways in which a log message (or "event" in Splunk's
   terminology) can be sent to Splunk. These steps will set up a TCP port where
   Splunk will listen for messages.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the Splunk web UI, click on the Settings menu in the upper right-hand
     corner.
    </p></li><li class="listitem "><p>
     In the <span class="guimenu ">Data</span> section of the Settings menu, click <span class="guimenu ">Data Inputs</span>.
    </p></li><li class="listitem "><p>
     Choose the <span class="guimenu ">TCP</span> option.
    </p></li><li class="listitem "><p>
     Click the <span class="guimenu ">New</span> button to add an input.
    </p></li><li class="listitem "><p>
     In the <span class="guimenu ">Port</span> field, enter the port number you want to use.
    </p><div id="id-1.6.5.2.6.4.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      If you are on a less secure network and want to restrict connections to
      this port, use the <span class="guimenu ">Only accept connection from</span> field
      to restrict the traffic to a specific IP address.
     </p></div></li><li class="listitem "><p>
     Click the <span class="guimenu ">Next</span> button.
    </p></li><li class="listitem "><p>
     Specify the Source Type by clicking on the <span class="guimenu ">Select</span> button and choosing
     <code class="literal">linux_messages_syslog</code> from the list.
    </p></li><li class="listitem "><p>
     Click the <span class="guimenu ">Review</span> button.
    </p></li><li class="listitem "><p>
     Review the configuration and click the <span class="guimenu ">Submit</span> button.
    </p></li><li class="listitem "><p>
     A success message will be displayed.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.5.2.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forwarding log messages from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> Centralized Logging to Splunk</span> <a title="Permalink" class="permalink" href="#id-1.6.5.2.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When you have Splunk set up and configured to receive log messages, you can
   configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> to forward the logs to Splunk.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Check the status of the logging service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-status.yml</pre></div><p>
     If everything is up and running, continue to the next step.
    </p></li><li class="step "><p>
     Edit the logstash config file at the location below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/logging-server/templates/logstash.conf.j2</pre></div><p>
     At the bottom of the file will be a section for the Logstash outputs. Add
     details about your Splunk environment details.
    </p><p>
     Below is an example, showing the placement in bold:
    </p><div class="verbatim-wrap"><pre class="screen"># Logstash outputs
#------------------------------------------------------------------------------
output {
  # Configure Elasticsearch output
  # http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
  elasticsearch {
    index =&gt; %{[@metadata][es_index]}
    hosts =&gt; ["{{ elasticsearch_http_host }}:{{ elasticsearch_http_port }}"]
    flush_size =&gt; {{ logstash_flush_size }}
    idle_flush_time =&gt; 5
    workers =&gt; {{ logstash_threads }}
  }
 <span class="bold"><strong>  # Forward Logs to Splunk on the TCP port that matches the one specified in Splunk Web UI.
 tcp {
   mode =&gt; "client"
   host =&gt; "&lt;Enter Splunk listener IP address&gt;"
   port =&gt; <em class="replaceable ">TCP_PORT_NUMBER</em>
 }</strong></span>
}</pre></div><div id="id-1.6.5.2.7.3.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      If you are not planning on using the Splunk UI to parse your centralized
      logs, there is no need to forward your logs to Elasticsearch. In this
      situation, comment out the lines in the Logstash outputs pertaining to
      Elasticsearch.
      However, you can continue to forward your centralized logs to multiple
      locations.
     </p></div></li><li class="step "><p>
     Commit your changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Logstash configuration change for Splunk integration"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Complete this change with a reconfigure of the logging environment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-configure.yml</pre></div></li><li class="step "><p>
     In your Splunk UI, confirm that the logs have begun to forward.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.6.5.2.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Searching for log messages from the Spunk dashboard</span> <a title="Permalink" class="permalink" href="#id-1.6.5.2.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To both verify that your integration worked and to search your log messages
   that have been forwarded you can navigate back to your Splunk dashboard. In
   the search field, use this string:
  </p><div class="verbatim-wrap"><pre class="screen">source="tcp:<em class="replaceable ">TCP_PORT_NUMBER</em>"</pre></div><p>
   Find information on using the Splunk search tool at
   <a class="link" href="http://docs.splunk.com/Documentation/Splunk/6.4.3/SearchTutorial/WelcometotheSearchTutorial" target="_blank">http://docs.splunk.com/Documentation/Splunk/6.4.3/SearchTutorial/WelcometotheSearchTutorial</a>.
  </p></div></div><div class="sect1" id="nagios-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nagios Integration</span> <a title="Permalink" class="permalink" href="#nagios-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_nagios.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_nagios.xml</li><li><span class="ds-label">ID: </span>nagios-integration</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud operators that are using Nagios or Icinga-based monitoring
  systems may wish to integrate them with the built-in monitoring
  infrastructure of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Integrating with the existing
  monitoring processes and procedures will reduce support overhead and avoid
  duplication. This document describes the different approaches that can be
  taken to create a well-integrated monitoring dashboard using both
  technologies.
 </p><div id="id-1.6.5.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   This document refers to Nagios but the proposals will work equally well with
   Icinga, Icinga2, or other Nagios clone monitoring systems.
  </p></div><div class="sect2" id="idg-all-operations-integrating-nagios-xml-5"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> monitoring and reporting</span> <a title="Permalink" class="permalink" href="#idg-all-operations-integrating-nagios-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_nagios.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_nagios.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-integrating-nagios-xml-5</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with a monitoring engine (Monasca) and a separate management
   dashboard (Operations Console). Monasca is extremely scalable, designed to
   cope with the constant change in monitoring sources and services found in a
   cloud environment. Monitoring agents running on hosts (physical and virtual)
   submit data to the Monasca message bus via a RESTful API. Threshold and
   notification engines then trigger alarms when predefined thresholds are
   passed. Notification methods are flexible and extensible. Typical examples
   of notification methods would be emails generated or creating alarms in
   PagerDuty.
  </p><p>
   While extensible, Monasca is largely focused on monitoring cloud
   infrastructures rather than traditional environments such as server
   hardware, network links, switches, etc. For more details about the
   monitoring service, see <a class="xref" href="#mon" title="12.1. Monitoring">Section 12.1, “Monitoring”</a>.
  </p><p>
   The Operations Console (Ops Console) provides cloud administrators a clear
   web interfaces to view alarm status, management alarm workflow, and
   configure alarms and thresholds. For more details about the Ops Console, see
   <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span>.
  </p></div><div class="sect2" id="nagios"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nagios monitoring and reporting</span> <a title="Permalink" class="permalink" href="#nagios">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_nagios.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_nagios.xml</li><li><span class="ds-label">ID: </span>nagios</li></ul></div></div></div></div><p>
   Nagios is an industry leading open source monitoring service with extensive
   plugins and agents. Nagios checks are either run directly from the
   monitoring server or run on a remote host via an agent and with results
   submitted back to the monitoring server. While Nagios has proven extremely
   flexible and scalable, it requires significant explicit configuration. Using
   Nagios to monitor guest virtual machines becomes more challenging because
   virtual machines can be ephemeral which means new virtual machines are
   created and destroyed regularly. Configuration automation (Chef, Puppet,
   Ansible etc) can create a more dynamic Nagios setup but they still require
   the Nagios service to be restarted every time a new host is added.
  </p><p>
   A key benefit of Nagios style monitoring is that it allows for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to
   be monitored externally, from a user or service perspective. For example,
   checks can be created to monitor availability of all the API endpoints from
   external locations or even to create and destroy instances to ensure the
   entire system is working as expected.
  </p></div><div class="sect2" id="idg-all-operations-integrating-nagios-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Monasca</span> <a title="Permalink" class="permalink" href="#idg-all-operations-integrating-nagios-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_nagios.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_nagios.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-integrating-nagios-xml-7</li></ul></div></div></div></div><p>
   Many private cloud operators already have existing monitoring solutions such
   as Nagios and Icinga. We recommend that you extend your existing solutions
   into Monasca or forward Monasca alerts to your existing solution to maximize
   coverage and reduce risk.
  </p></div><div class="sect2" id="integration-approaches"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integration Approaches</span> <a title="Permalink" class="permalink" href="#integration-approaches">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_nagios.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_nagios.xml</li><li><span class="ds-label">ID: </span>integration-approaches</li></ul></div></div></div></div><p>
   Integration between Nagios and Monasca can occur at two levels, at the
   individual check level or at the management interfaces. Both options are
   discussed in the following sections.
  </p><p>
   <span class="bold"><strong>Running Nagios-style checks in the Monasca
   agents</strong></span>
  </p><p>
   The Monasca agent is installed on all <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> servers and includes the
   ability to execute Nagios-style plugins as well as its own plugin scripts.
   For this configuration check, plugins need to be installed on the required
   server then added to the Monasca configuration under
   <code class="literal">/etc/monasca/agent/conf.d</code>. Care should be taken as
   plugins that take a long time (greater than 10 seconds) to run can result in
   the Monasca agent failing to run its own checks in the allotted time and
   therefore stopping all client monitoring. Issues have been seen with
   hardware monitoring plugins that can take greater than 30 seconds and any
   plugins relying on name resolution when DNS services are not available.
   Details on the required Monasca configuration can be found at
   <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#nagios-wrapper" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#nagios-wrapper</a>.
  </p><p>
   Use Case:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Local host checking. As an operator I want to run a local monitoring check
     on my host to check physical hardware. Check status and alert management
     will be based around the Operations Console, not Nagios.
    </p></li></ul></div><p>
   Limitation
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     As mentioned earlier, care should be taken to ensure checks do not
     introduce load or delays in the Monasca agent check cycle. Additionally,
     depending on the operating system the node is running, plugins or
     dependencies may not be available.
    </p></li></ul></div><p>
   <span class="bold"><strong>Using Nagios as a central dashboard</strong></span>
  </p><p>
   It is possible to create a Nagios-style plugin that will query the Monasca
   API endpoint for an alarm status to create Nagios alerts and alarms based on
   Monasca alarms and filters. Monasca alarms appear in Nagios using two
   approaches, one listing checks by service and the other listing checks by
   physical host.
  </p><p>
   In the top section of the Nagios-style plugin, services can be created under
   a dummy host, <code class="literal">monasca_endpoint</code>. Each service retrieves
   all alarms based on defined dimensions. For example the
   <code class="literal">ardana-compute</code> check will return all alarms with the
   compute (Nova) dimension.
  </p><p>
   In the bottom section, the physical servers making up the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   cluster can be defined and checks can be run. For example, one could check
   the server hardware from the Nagios server using a third party plugin and the
   another could retrieve all monasca alarms related to that host.
  </p><p>
   To build this configuration, a custom Nagios plugin
   (Please see example plugin at:
   <a class="link" href="https://github.com/openstack/python-monascaclient/tree/stable/pike/examples" target="_blank">https://github.com/openstack/python-monascaclient/tree/stable/pike/examples</a>)
   was created with the following options:
  </p><div class="verbatim-wrap"><pre class="screen">check_monasca –c <em class="replaceable ">CREDENTIALS</em> -d <em class="replaceable ">DIMENSION</em> -v <em class="replaceable ">VALUE</em></pre></div><p>
   Examples:
  </p><p>
   To check alarms on <code class="literal">test-ccp-comp001-mgmt</code> you would use:
  </p><div class="verbatim-wrap"><pre class="screen">check_monasca –c service.osrc –d hostname –v test-ccp-comp001-mgmt</pre></div><p>
   To check all Network related alarms, you would use:
  </p><div class="verbatim-wrap"><pre class="screen">check_monasca –c service.osrc –d service –v networking</pre></div><p>
   Use Cases:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Multiple clouds, integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> monitoring with existing monitoring
     capabilities or viewing Monasca alerts in Nagios, fully integrating
     Monasca alarms with Nagios alarms and workflow.
    </p></li><li class="listitem "><p>
     In a predominantly Nagios or Icinga-based monitoring environment,
     Monasca alarm status can be integrated into existing processes and
     workflows. This approach works best for checks associated with physical
     servers running the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services.
    </p></li><li class="listitem "><p>
     With multiple <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> clusters, all of their alarms can be consolidated
     into a single view, the current version of Operations Console is for a single
     cluster only.
    </p></li></ul></div><p>
   Limitations
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Nagios has a more traditional configuration model that requires checks to
     belong to predefined services and hosts, this is not well suited in highly
     dynamic cloud environments where the lifespan of virtual instances can be
     very short. One possible solution is with Icinga2 which has an API
     available to dynamically add host and service definitions, the check
     plugin could be extended to create alarm definitions dynamically as they
     occur.
    </p><p>
     The key disadvantage is that multiple alarms can appear as a single
     service. For example, suppose there are 3 warnings against one service.
     If the operator acknowledges this alarm and subsequently a 4th warning
     alarm occurs, it would not generate an alert and could get missed.
    </p><p>
     Care has to be taken that alarms are not missed. If the defined checks
     are only looking for checks in an ALARM status they will not report
     undetermined checks that might indicate other issues.
    </p></li></ul></div><p>
   <span class="bold"><strong>Using Operations Console as central dashboard</strong></span>
  </p><p>
   Nagios has the ability to run custom scripts in response to events. It is
   therefore possible to write a plugin to update Monasca whenever a Nagios
   alert occurs. The Operations Console could then be used as a central reporting
   dashboard for both Monasca and Nagios alarms. The external Nagios alarms can
   have their own check dimension and could be displayed as a separate group in
   the Operations Console.
  </p><p>
   Use Cases
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Using Operations Console the central monitoring tool.
    </p></li></ul></div><p>
   Limitations
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The alarm could not be acknowledged from the Operations Console so Nagios could
     send repetitive notifications unless configured to take this into account.
    </p></li></ul></div><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-specific Nagios Plugins</strong></span>
  </p><p>
   Several OpenStack plugin packages exist (see
   <a class="link" href="https://launchpad.net/ubuntu/+source/nagios-plugins-openstack" target="_blank">https://launchpad.net/ubuntu/+source/nagios-plugins-openstack</a>)
   that are useful to run from external sources to ensure the overall system is
   working as expected. Monasca requires some OpenStack components to be working
   in order to work at all. For example, if Keystone were unavailable,
   Monasca could not authenticate client or console requests. An external
   service check could highlight this.
  </p></div><div class="sect2" id="issues"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Common integration issues</span> <a title="Permalink" class="permalink" href="#issues">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_nagios.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_nagios.xml</li><li><span class="ds-label">ID: </span>issues</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Alarm status differences</strong></span>
  </p><p>
   Monasca and Nagios treat alarms and status in different ways and for the two
   systems to talk there needs to be a mapping between them. The following
   table details the alarm parameters available for each:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>System</th><th>Status</th><th>Severity</th><th>Details</th></tr></thead><tbody><tr><td rowspan="4">Nagios</td><td>OK</td><td> </td><td>Plugin returned OK with given thresholds</td></tr><tr><td>WARNING</td><td> </td><td>Plugin returned WARNING based on thresholds</td></tr><tr><td>CRITICAL</td><td> </td><td>Plugin returned CRITICAL alarm</td></tr><tr><td>UNKNOWN</td><td> </td><td>Plugin failed</td></tr><tr><td rowspan="5">Monasca</td><td>OK</td><td> </td><td>No alarm triggered</td></tr><tr><td>ALARM</td><td>LOW</td><td>Alarm state, LOW impact</td></tr><tr><td>ALARM</td><td>MEDIUM</td><td>Alarm state, MEDIUM impact</td></tr><tr><td>ALARM</td><td>HIGH</td><td>Alarm state, HIGH impact</td></tr><tr><td>UNDETERMINED</td><td> </td><td>No metrics received</td></tr></tbody></table></div><p>
   In the plugin described here, the mapping was created with this flow:
  </p><div class="verbatim-wrap"><pre class="screen">Monasca OK -&gt; Nagios OK
Monasca ALARM ( LOW or MEDIUM ) -&gt; Nagios Warning
Monasca ALARM ( HIGH ) -&gt; Nagios Critical</pre></div><p>
   <span class="bold"><strong>Alarm workflow differences</strong></span>
  </p><p>
   In both, system alarms can be acknowledged in the dashboards to indicate
   they are being worked on (or ignored). Not all the scenarios above will
   provide the same level of workflow integration.
  </p></div></div><div class="sect1" id="topic-kyf-brv-vw"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Bridge Integration</span> <a title="Permalink" class="permalink" href="#topic-kyf-brv-vw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integrating_opsbridge.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_opsbridge.xml</li><li><span class="ds-label">ID: </span>topic-kyf-brv-vw</li></ul></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> monitoring solution (Monasca) can easily be
  integrated with your existing monitoring tools. Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
  Monasca with Operations Bridge using the Operations Bridge Connector
  simplifies monitoring and managing events and topology information.
 </p><p>
  The integration provides the following functionality:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Forwarding of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monasca alerts and topology to Operations
    Bridge for event correlation
   </p></li><li class="listitem "><p>
    Customization of forwarded events and topology
   </p></li></ul></div><p>
  For more information about this connector please see
  <a class="link" href="https://software.microfocus.com/en-us/products/operations-bridge-suite/overview" target="_blank">https://software.microfocus.com/en-us/products/operations-bridge-suite/overview</a>.
 </p></div><div class="sect1" id="monitoring-3rd-party-components-with-monasca"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Third-Party Components With Monasca</span> <a title="Permalink" class="permalink" href="#monitoring-3rd-party-components-with-monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring_third_party_components.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring_third_party_components.xml</li><li><span class="ds-label">ID: </span>monitoring-3rd-party-components-with-monasca</li></ul></div></div></div></div><div class="sect2" id="monasca-integration-overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Monitoring Integration Overview</span> <a title="Permalink" class="permalink" href="#monasca-integration-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monasca_integration_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monasca_integration_overview.xml</li><li><span class="ds-label">ID: </span>monasca-integration-overview</li></ul></div></div></div></div><p>
  Monasca, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> monitoring service, collects information about
  your cloud's systems, and allows you to create alarm definitions based on
  these measurements. Monasca-agent is the component that collects metrics such
  as metric storage and alarm thresholding and forwards them to the monasca-api
  for further processing.
 </p><p>
  With a small amount of configuration, you can use the detection and check
  plugins that are provided with your cloud to monitor integrated third-party
  components. In addition, you can write custom plugins and integrate them with
  the existing monitoring service.
 </p><p>
  Find instructions for customizing existing plugins to monitor third-party
  components in the <a class="xref" href="#configuring-check-plugins" title="3.4.4. Configuring Check Plugins">Section 3.4.4, “Configuring Check Plugins”</a>.
 </p><p>
  Find instructions for installing and configuring new custom plugins in the
 <a class="xref" href="#writing-custom-plugins" title="3.4.3. Writing Custom Plugins">Section 3.4.3, “Writing Custom Plugins”</a>.
 </p><p>
  You can also use existing alarm definitions, as well as create new alarm
  definitions that relate to a custom plugin or metric. Instructions for
  defining new alarm definitions are in the <a class="xref" href="#configuring-alarm-definitions" title="3.4.6. Configuring Alarm Definitions">Section 3.4.6, “Configuring Alarm Definitions”</a>.
 </p><p>
  You can use the Operations Console and Monasca CLI to list all of the alarms,
  alarm-definitions, and metrics that exist on your cloud.
 </p></div><div class="sect2" id="monasca-agent"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Agent</span> <a title="Permalink" class="permalink" href="#monasca-agent">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monasca_agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monasca_agent.xml</li><li><span class="ds-label">ID: </span>monasca-agent</li></ul></div></div></div></div><p>
  The Monasca agent (monasca-agent) collects information about your cloud using
  the installed plugins. The plugins are written in Python, and determine the
  monitoring metrics for your system, as well as the interval for collection.
  The default collection interval is 30 seconds, and we strongly recommend
  not changing this default value.
 </p><p>
  The following two types of custom plugins can be added to your cloud.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Detection Plugin</strong></span>. Determines whether the
    monasca-agent has the ability to monitor the specified component or service
    on a host. If successful, this type of plugin configures an associated
    <span class="bold"><strong>check plugin</strong></span> by creating a YAML
    configuration file.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Check Plugin</strong></span>. Specifies the metrics to be
    monitored, using the configuration file created by the detection plugin.
   </p></li></ul></div><p>
  Monasca-agent is installed on every server in your cloud, and provides
  plugins that monitor the following.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    System metrics relating to CPU, memory, disks, host availability, etc.
   </p></li><li class="listitem "><p>
    Process health metrics (process, http_check)
   </p></li><li class="listitem "><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>-specific component metrics, such as apache
    rabbitmq, kafka, cassandra, etc.
   </p></li></ul></div><p>
  Monasca is pre-configured with default check plugins and associated detection
  plugins. The default plugins can be reconfigured to monitor third-party
  components, and often only require small adjustments to adapt them to this
  purpose. Find a list of the default plugins here:
  <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#detection-plugins" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#detection-plugins</a>
 </p><p>
  Often, a single check plugin will be used to monitor multiple services. For
  example, many services use the <code class="literal">http_check.py</code> detection
  plugin to detect the up/down status of a service endpoint. Often the
  <code class="literal">process.py</code> check plugin, which provides process monitoring
  metrics, is used as a basis for a custom process detection plugin.
 </p><p>
  More information about the Monasca agent can be found in the following
  locations
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Monasca agent overview:
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md</a>
   </p></li><li class="listitem "><p>
    Information on existing plugins:
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md</a>
   </p></li><li class="listitem "><p>
    Information on plugin customizations:
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md</a>
   </p></li></ul></div></div><div class="sect2" id="writing-custom-plugins"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Writing Custom Plugins</span> <a title="Permalink" class="permalink" href="#writing-custom-plugins">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-writing_custom_plugins.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-writing_custom_plugins.xml</li><li><span class="ds-label">ID: </span>writing-custom-plugins</li></ul></div></div></div></div><p>
  When the pre-built Monasca plugins do not meet your monitoring needs, you can
  write custom plugins to monitor your cloud. After you have written a plugin,
  you must install and configure it.
 </p><p>
  When your needs dictate a very specific custom monitoring check, you must
  provide both a detection and check plugin.
 </p><p>
  The steps involved in configuring a custom plugin include running a detection
  plugin and passing any necesssary parameters to the detection plugin so the
  resulting check configuration file is created with all necessary data.
 </p><p>
  When using an existing check plugin to monitor a third-party component, a
  custom detection plugin is needed only if there is not an associated default
  detection plugin.
 </p><p>
  <span class="bold"><strong>Check plugin configuration files</strong></span>
 </p><p>
  Each plugin needs a corresponding YAML configuration file with the same stem
  name as the plugin check file. For example, the plugin file
  <code class="filename">http_check.py</code> (in
  <code class="filename">/usr/lib/python2.7/site-packages/monasca_agent/collector/checks_d/</code>)
  should have a corresponding configuration file,
  <code class="filename">http_check.yaml</code> (in
  <code class="filename">/etc/monasca/agent/conf.d/http_check.yaml</code>).  The stem
  name <code class="literal">http_check</code> must be the same for both files.
 </p><p>
  Permissions for the YAML configuration file must be <span class="bold"><strong>read+write</strong></span> for <code class="literal">mon-agent</code> user (the
  user that must also own the file), and <span class="bold"><strong>read</strong></span>
  for the <code class="literal">mon-agent</code> group. Permissions for the file must be
  restricted to the <span class="bold"><strong>mon-agent</strong></span> user and
  <span class="bold"><strong>monasca</strong></span> group. The following example shows
  correct permissions settings for the file
  <code class="filename">http_check.yaml</code>.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -alt /etc/monasca/agent/conf.d/http_check.yaml
-rw-r----- 1 monasca-agent monasca 10590 Jul 26 05:44 http_check.yaml</pre></div><p>
  A check plugin YAML configuration file has the following structure.
 </p><div class="verbatim-wrap"><pre class="screen">init_config:
    key1: value1
    key2: value2

instances:
    - name: john_smith
      username: john_smith
      password: 123456
    - name: jane_smith
      username: jane_smith
      password: 789012</pre></div><p>
  In the above file structure, the <code class="literal">init_config</code> section
  allows you to specify any number of global
  <span class="bold"><strong>key:value</strong></span> pairs. Each pair will be available
  on every run of the check that relates to the YAML configuration file.
 </p><p>
  The <code class="literal">instances</code> section allows you to list the instances
  that the related check will be run on. The check will be run once on each
  instance listed in the <code class="literal">instances</code> section. Ensure that each
  instance listed in the <code class="literal">instances</code> section has a unique
  name.
 </p><p>
  <span class="bold"><strong>Custom detection plugins</strong></span>
 </p><p>
  Detection plugins should be written to perform checks that ensure that a
  component can be monitored on a host. Any arguments needed by the associated
  check plugin are passed into the detection plugin at setup (configuration)
  time. The detection plugin will write to the associated check configuration
  file.
 </p><p>
  When a detection plugin is successfully run in the configuration step, it
  will write to the check configuration YAML file. The configuration file for
  the check is written to the following directory.
 </p><div class="verbatim-wrap"><pre class="screen">/etc/monasca/agent/conf.d/</pre></div><p>
  <span class="bold"><strong>Writing process detection plugin using the
  ServicePlugin class</strong></span>
 </p><p>
  The monasca-agent provides a <code class="literal">ServicePlugin</code>
  class that makes process detection monitoring easy.
 </p><p>
  <span class="bold"><strong>Process check</strong></span>
 </p><p>
  The process check plugin generates metrics based on the process status for
  specified process names. It generates
  <code class="literal">process.pid_count</code> metrics for the specified
  dimensions, and a set of detailed process metrics for the specified
  dimensions by default.
 </p><p>
  The ServicePlugin class allows you to specify a list of process name(s) to
  detect, and uses <span class="bold"><strong>psutil</strong></span> to see if the
  process exists on the host. It then appends the <code class="filename">process.yml</code> configuration
  file with the process name(s), if they do not already exist.
 </p><p>
  The following is an example of a <code class="literal">process.py</code>
  check <code class="literal">ServicePlugin</code>.
 </p><div class="verbatim-wrap"><pre class="screen">import monasca_setup.detection

class MonascaTransformDetect(monasca_setup.detection.ServicePlugin):
    """Detect Monasca Transform daemons and setup configuration to monitor them."""
    def __init__(self, template_dir, overwrite=False, args=None):
        log.info("      Watching the monasca transform processes.")
        service_params = {
            'args': {},
            'template_dir': template_dir,
            'overwrite': overwrite,
            'service_name': 'monasca-transform',
            'process_names': ['monasca-transform','pyspark',
                              'transform/lib/driver']
        }
        super(MonascaTransformDetect, self).__init__(service_params)</pre></div><p>
  <span class="bold"><strong>Writing a Custom Detection Plugin using Plugin or
  ArgsPlugin classes</strong></span>
 </p><p>
  A custom detection plugin class should derive from either the Plugin or
  ArgsPlugin classes provided in the
  <code class="filename">/usr/lib/python2.7/site-packages/monasca_setup/detection</code> directory.
 </p><p>
  If the plugin parses command line arguments, the <code class="literal">ArgsPlugin</code> class is useful.
  The ArgsPlugin class derives from the Plugin class. The ArgsPlugin class has
  a method to check for required arguments, and a method to return the instance
  that will be used for writing to the configuration file with the dimensions
  from the command line parsed and included.
 </p><p>
  If the ArgsPlugin methods do not seem to apply, then derive directly from the
  Plugin class.
 </p><p>
  When deriving from these classes, the following methods should be
  implemented.
 </p><div class="itemizedlist " id="ul-dfd-kvs-px"><ul class="itemizedlist"><li class="listitem "><p>
    _<span class="emphasis"><em>detect - set self.available=True when conditions are met that
    the thing to monitor exists on a host.</em></span>
   </p></li><li class="listitem "><p>
    <span class="emphasis"><em>build</em></span>_config - writes the instance information to the
    configuration and return the configuration.
   </p></li><li class="listitem "><p>
    dependencies_installed (default implementation is in ArgsPlugin, but not
    Plugin) - return true when python dependent libraries are installed.
   </p></li></ul></div><p>
  The following is an example custom detection plugin.
 </p><div class="verbatim-wrap"><pre class="screen">import ast
import logging

import monasca_setup.agent_config
import monasca_setup.detection

log = logging.getLogger(__name__)


class HttpCheck(monasca_setup.detection.ArgsPlugin):
    """Setup an http_check according to the passed in args.
       Despite being a detection plugin this plugin does no detection and will be a noop without   arguments.
       Expects space separated arguments, the required argument is url. Optional parameters include:
       disable_ssl_validation and match_pattern.
    """

    def _detect(self):
        """Run detection, set self.available True if the service is detected.
        """
        self.available = self._check_required_args(['url'])

    def build_config(self):
        """Build the config as a Plugins object and return.
        """
        config = monasca_setup.agent_config.Plugins()
        # No support for setting headers at this time
        instance = self._build_instance(['url', 'timeout', 'username', 'password',
                                         'match_pattern', 'disable_ssl_validation',
                                         'name', 'use_keystone', 'collect_response_time'])

        # Normalize any boolean parameters
        for param in ['use_keystone', 'collect_response_time']:
            if param in self.args:
                instance[param] = ast.literal_eval(self.args[param].capitalize())
        # Set some defaults
        if 'collect_response_time' not in instance:
            instance['collect_response_time'] = True
        if 'name' not in instance:
            instance['name'] = self.args['url']

        config['http_check'] = {'init_config': None, 'instances': [instance]}

        return config</pre></div><p>
  <span class="bold"><strong>Installing a detection plugin in the <span class="productname">OpenStack</span> version delivered with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></strong></span>
 </p><p>
  Install a plugin by copying it to the plugin directory
  (<code class="filename">/usr/lib/python2.7/site-packages/monasca_agent/collector/checks_d/</code>).
 </p><p>
  The plugin should have file permissions of
  <span class="bold"><strong>read+write</strong></span> for the root user (the user that
  should also own the file) and <span class="bold"><strong>read</strong></span> for the
  root group and all other users.
 </p><p>
  The following is an example of correct file permissions for the
  <span class="bold"><strong>http_check.py</strong></span> file.
 </p><div class="verbatim-wrap"><pre class="screen">-rw-r--r-- 1 root root 1769 Sep 19 20:14 http_check.py</pre></div><p>
  Detection plugins should be placed in the following directory.
 </p><div class="verbatim-wrap"><pre class="screen">/usr/lib/monasca/agent/custom_detect.d/</pre></div><p>
  The detection plugin directory name should be accessed using the
  <code class="literal">monasca_agent_detection_plugin_dir</code> Ansible variable. This
  variable is defined in the
  <code class="literal">roles/monasca-agent/vars/main.yml</code> file.
 </p><div class="verbatim-wrap"><pre class="screen">monasca_agent_detection_plugin_dir: /usr/lib/monasca/agent/custom_detect.d/</pre></div><p>
  Example: Add Ansible <code class="literal">monasca_configure</code> task to install the
  plugin. (The <code class="literal">monasca_configure</code> task can be added to any
  service playbook.) In this example, it is added to
  <code class="filename">~/openstack/ardana/ansible/roles/_CEI-CMN/tasks/monasca_configure.yml</code>.
 </p><div class="verbatim-wrap"><pre class="screen">---
- name: _CEI-CMN | monasca_configure |
    Copy Ceilometer Custom plugin
  become: yes
  copy:
    src: ardanaceilometer_mon_plugin.py
    dest: "{{ monasca_agent_detection_plugin_dir }}"
    owner: root
    group: root
    mode: 0440</pre></div><p>
  <span class="bold"><strong>Custom check plugins</strong></span>
 </p><p>
  Custom check plugins generate metrics. Scalability should be taken into
  consideration on systems that will have hundreds of servers, as a large
  number of metrics can affect performance by impacting disk performance, RAM
  and CPU usage.
 </p><p>
  You may want to tune your configuration parameters so that less-important
  metrics are not monitored as frequently. When check plugins are configured
  (when they have an associated YAML configuration file) the agent will attempt
  to run them.
 </p><p>
  Checks should be able to run within the 30-second metric collection window.
  If your check runs a command, you should provide a timeout to prevent the
  check from running longer than the default 30-second window. You can use the
  <code class="literal">monasca_agent.common.util.timeout_command</code> to set a timeout
  for in your custom check plugin python code.
 </p><p>
  Find a description of how to write custom check plugins at
  <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md#creating-a-custom-check-plugin" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md#creating-a-custom-check-plugin</a>
 </p><p>
  Custom checks derive from the AgentCheck class located in the
  <code class="literal">monasca_agent/collector/checks/check.py</code> file. A check
  method is required.
 </p><p>
  Metrics should contain dimensions that make each item that you are monitoring
  unique (such as service, component, hostname). The hostname dimension is
  defined by default within the AgentCheck class, so every metric has this
  dimension.
 </p><p>
  A custom check will do the following.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Read the configuration instance passed into the check method.
   </p></li><li class="listitem "><p>
    Set dimensions that will be included in the metric.
   </p></li><li class="listitem "><p>
    Create the metric with gauge, rate, or counter types.
   </p></li></ul></div><p>
  Metric Types:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    gauge: Instantaneous reading of a particular value (for example,
    mem.free_mb).
   </p></li><li class="listitem "><p>
    rate: Measurement over a time period. The following equation can be used to
    define rate.
   </p><div class="verbatim-wrap"><pre class="screen">rate=delta_v/float(delta_t)</pre></div></li><li class="listitem "><p>
    counter: The number of events, increment and decrement methods, for
    example, zookeeper.timeouts
   </p></li></ul></div><p>
  The following is an example component check named SimpleCassandraExample.
 </p><div class="verbatim-wrap"><pre class="screen">import monasca_agent.collector.checks as checks
from monasca_agent.common.util import timeout_command

CASSANDRA_VERSION_QUERY = "SELECT version();"


class SimpleCassandraExample(checks.AgentCheck):

    def __init__(self, name, init_config, agent_config):
        super(SimpleCassandraExample, self).__init__(name, init_config, agent_config)

    @staticmethod
    def _get_config(instance):
        user = instance.get('user')
        password = instance.get('password')
        service = instance.get('service')
        timeout = int(instance.get('timeout'))

        return user, password, service, timeout

    def check(self, instance):
        user, password, service, node_name, timeout = self._get_config(instance)

        dimensions = self._set_dimensions({'component': 'cassandra', 'service': service}, instance)

        results, connection_status = self._query_database(user, password, timeout, CASSANDRA_VERSION_QUERY)

        if connection_status != 0:
            self.gauge('cassandra.connection_status', 1, dimensions=dimensions)
        else:
            # successful connection status
            self.gauge('cassandra.connection_status', 0, dimensions=dimensions)

    def _query_database(self, user, password, timeout, query):
        stdout, stderr, return_code = timeout_command(["/opt/cassandra/bin/vsql", "-U", user, "-w", password, "-A", "-R",
                                                       "|", "-t", "-F", ",", "-x"], timeout, command_input=query)
        if return_code == 0:
            # remove trailing newline
            stdout = stdout.rstrip()
            return stdout, 0
        else:
            self.log.error("Error querying cassandra with return code of {0} and error {1}".format(return_code, stderr))
            return stderr, 1</pre></div><p>
  <span class="bold"><strong>Installing check plugin</strong></span>
 </p><p>
  The check plugin needs to have the same file permissions as the detection
  plugin. File permissions must be <span class="bold"><strong>read+write</strong></span>
  for the root user (the user that should own the file), and
  <span class="bold"><strong>read</strong></span> for the root group and all other users.
 </p><p>
  Check plugins should be placed in the following directory.
 </p><div class="verbatim-wrap"><pre class="screen">/usr/lib/monasca/agent/custom_checks.d/</pre></div><p>
  The check plugin directory should be accessed using the
  <code class="literal">monasca_agent_check_plugin_dir</code> Ansible variable. This
  variable is defined in the
  <code class="literal">roles/monasca-agent/vars/main.yml</code> file.
 </p><div class="verbatim-wrap"><pre class="screen">monasca_agent_check_plugin_dir: /usr/lib/monasca/agent/custom_checks.d/</pre></div></div><div class="sect2" id="configuring-check-plugins"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Check Plugins</span> <a title="Permalink" class="permalink" href="#configuring-check-plugins">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring_check_plugins.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring_check_plugins.xml</li><li><span class="ds-label">ID: </span>configuring-check-plugins</li></ul></div></div></div></div><p>
  <span class="bold"><strong>Manually configure a plugin when unit-testing using the
  monasca-setup script installed with the monasca-agent</strong></span>
 </p><p>
  Find a good explanation of configuring plugins here:
  <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md#configuring" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md#configuring</a>
 </p><p>
  SSH to a node that has both the monasca-agent installed as well as the
  component you wish to monitor.
 </p><p>
  The following is an example command that configures a plugin that has no
  parameters (uses the detection plugin class name).
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>/usr/bin/monasca-setup -d ARDANACeilometer</pre></div><p>
  The following is an example command that configures the apache plugin and
  includes related parameters.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>/usr/bin/monasca-setup -d apache -a 'url=http://192.168.245.3:9095/server-status?auto'</pre></div><p>
  If there is a change in the configuration it will restart the monasca-agent
  on the host so the configuration is loaded.
 </p><p>
  After the plugin is configured, you can verify that the configuration file
  has your changes (see the next <span class="bold"><strong>Verify that your check plugin
  is configured</strong></span> section).
 </p><p>
  Use the monasca CLI to see if your metric exists (see the
  <span class="bold"><strong>Verify that metrics exist</strong></span> section).
 </p><p>
  <span class="bold"><strong>Using Ansible modules to configure plugins in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span></strong></span>
 </p><p>
  The <code class="literal">monasca_agent_plugin</code> module is installed as part of
  the monasca-agent role.
 </p><p>
  The following Ansible example configures the process.py plugin for the
  Ceilometer detection plugin. The following example only passes in the name of
  the detection class.
 </p><div class="verbatim-wrap"><pre class="screen">- name: _CEI-CMN | monasca_configure |
    Run Monasca agent Cloud Lifecycle Manager specific ceilometer detection plugin
  become: yes
  monasca_agent_plugin:
    name: "ARDANACeilometer"</pre></div><p>
  If a password or other sensitive data are passed to the detection plugin, the
  <code class="literal">no_log</code> option should be set to
  <span class="bold"><strong>True</strong></span>. If the <code class="literal">no_log</code>
  option is not set to <span class="bold"><strong>True</strong></span>, the data passed
  to the plugin will be logged to syslog.
 </p><p>
  The following Ansible example configures the Cassandra plugin and passes in
  related arguments.
 </p><div class="verbatim-wrap"><pre class="screen"> - name: Run Monasca Agent detection plugin for Cassandra
   monasca_agent_plugin:
     name: "Cassandra"
     args="directory_names={{ FND_CDB.vars.cassandra_data_dir }},{{ FND_CDB.vars.cassandra_commit_log_dir }} process_username={{ FND_CDB.vars.cassandra_user }}"
   when: database_type == 'cassandra'</pre></div><p>
  The following Ansible example configures the Keystone endpoint using the
  http_check.py detection plugin. The class name <code class="literal">httpcheck</code>
  of the http_check.py detection plugin is the name.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>- name:  keystone-monitor | local_monitor |
    Setup active check on keystone internal endpoint locally
  become: yes
  monasca_agent_plugin:
    name: "httpcheck"
    args: "use_keystone=False \
           url=http://{{ keystone_internal_listen_ip }}:{{
               keystone_internal_port }}/v3 \
           dimensions=service:identity-service,\
                       component:keystone-api,\
                       api_endpoint:internal,\
                       monitored_host_type:instance"
  tags:
    - keystone
    - keystone_monitor</pre></div><p>
  <span class="bold"><strong>Verify that your check plugin is configured</strong></span>
 </p><p>
  All check configuration files are located in the following directory. You can
  see the plugins that are running by looking at the plugin configuration
  directory.
 </p><div class="verbatim-wrap"><pre class="screen">/etc/monasca/agent/conf.d/</pre></div><p>
  When the monasca-agent starts up, all of the check plugins that have a
  matching configuration file in the
  <code class="literal">/etc/monasca/agent/conf.d/</code> directory will be loaded.
 </p><p>
  If there are errors running the check plugin they will be written to the
  following error log file.
 </p><div class="verbatim-wrap"><pre class="screen">/var/log/monasca/agent/collector.log</pre></div><p>
  You can change the monasca-agent log level by modifying the
  <code class="literal">log_level</code> option in the
  <code class="literal">/etc/monasca/agent/agent.yaml</code> configuration file, and then
  restarting the monasca-agent, using the following command.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>service openstack-monasca-agent restart</pre></div><p>
  You can debug a check plugin by running <code class="literal">monasca-collector</code>
  with the check option. The following is an example of the
  <code class="literal">monasca-collector</code> command.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo /usr/bin/monasca-collector check <em class="replaceable ">CHECK_NAME</em></pre></div><p>
  <span class="bold"><strong>Verify that metrics exist</strong></span>
 </p><p>
  Begin by logging in to your deployer or controller node.
 </p><p>
  Run the following set of commands, including the <code class="literal">monasca
  metric-list</code> command. If the metric exists, it will be displayed in
  the output.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>monasca metric-list --name <em class="replaceable ">METRIC_NAME</em></pre></div></div><div class="sect2" id="metric-performance-considerations"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metric Performance Considerations</span> <a title="Permalink" class="permalink" href="#metric-performance-considerations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-metric_performance_considerations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-metric_performance_considerations.xml</li><li><span class="ds-label">ID: </span>metric-performance-considerations</li></ul></div></div></div></div><p>
  Collecting metrics on your virtual machines can greatly affect performance.
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports 200 compute nodes, with up to 40 VMs each. If your
  environment is managing maximum number of VMs, adding a single metric for all
  VMs is the equivalent of adding 8000 metrics.
 </p><p>
  Because of the potential impact that new metrics have on system performance,
  consider adding only new metrics that are useful for alarm-definition,
  capacity planning, or debugging process failure.
 </p></div><div class="sect2" id="configuring-alarm-definitions"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Alarm Definitions</span> <a title="Permalink" class="permalink" href="#configuring-alarm-definitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring_alarm_definitions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring_alarm_definitions.xml</li><li><span class="ds-label">ID: </span>configuring-alarm-definitions</li></ul></div></div></div></div><p>
  The monasca-api-spec, found here
  <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md</a>
  provides an explanation of Alarm Definitions and Alarms. You can find more
  information on alarm definition expressions at the following page:
  <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definition-expressions" target="_blank">https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definition-expressions</a>.
 </p><p>
  When an alarm definition is defined, the monasca-threshold engine will
  generate an alarm for each unique instance of the match_by metric dimensions
  found in the metric. This allows a single alarm definition that can
  dynamically handle the addition of new hosts.
 </p><p>
  There are default alarm definitions configured for all "process check"
  (process.py check) and "HTTP Status" (http_check.py check) metrics in the
  monasca-default-alarms role. The monasca-default-alarms role is installed as
  part of the Monasca deployment phase of your cloud's deployment. You do not
  need to create alarm definitions for these existing checks.
 </p><p>
  Third parties should create an alarm definition when they wish to alarm on a
  custom plugin metric. The alarm definition should only be defined once.
  Setting a notification method for the alarm definition is recommended but not
  required.
 </p><p>
  The following Ansible modules used for alarm definitions are installed as
  part of the monasca-alarm-definition role. This process takes place during
  the Monasca set up phase of your cloud's deployment.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    monasca_alarm_definition
   </p></li><li class="listitem "><p>
    monasca_notification_method
   </p></li></ul></div><p>
  The following examples, found in the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms</code>
  directory, illustrate how Monasca sets up the default alarm definitions.
 </p><p>
  <span class="bold"><strong>Monasca Notification Methods</strong></span>
 </p><p>
  The monasca-api-spec, found in the following link, provides details about
  creating a notification
  <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#create-notification-method" target="_blank">https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#create-notification-method</a>
 </p><p>
  The following are supported notification types.
 </p><div class="itemizedlist " id="ul-t4b-pt4-qx"><ul class="itemizedlist"><li class="listitem "><p>
    EMAIL
   </p></li><li class="listitem "><p>
    WEBHOOK
   </p></li><li class="listitem "><p>
    PAGERDUTY
   </p></li></ul></div><p>
  The <code class="literal">keystone_admin_tenant</code> project is used so that the
  alarms will show up on the Operations Console UI.
 </p><p>
  The following file snippet shows variables from the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/defaults/main.yml</code>
  file.
 </p><div class="verbatim-wrap"><pre class="screen">---
notification_address: root@localhost
notification_name: 'Default Email'
notification_type: EMAIL

monasca_keystone_url: "{{ KEY_API.advertises.vips.private[0].url }}/v3"
monasca_api_url: "{{ MON_AGN.consumes_MON_API.vips.private[0].url }}/v2.0"
monasca_keystone_user: "{{ MON_API.consumes_KEY_API.vars.keystone_monasca_user }}"
monasca_keystone_password: "{{ MON_API.consumes_KEY_API.vars.keystone_monasca_password | quote }}"
monasca_keystone_project: "{{ KEY_API.vars.keystone_admin_tenant }}"

monasca_client_retries: 3
monasca_client_retry_delay: 2</pre></div><p>
  You can specify a single default notification method in the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/tasks/main.yml</code>
  file. You can also add or modify the notification type and related details
  using the Operations Console UI or Monasca CLI.
 </p><p>
  The following is a code snippet from the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/tasks/main.yml</code>
  file.
 </p><div class="verbatim-wrap"><pre class="screen">---
- name: monasca-default-alarms | main | Setup default notification method
  monasca_notification_method:
    name: "{{ notification_name }}"
    type: "{{ notification_type }}"
    address: "{{ notification_address }}"
    keystone_url: "{{ monasca_keystone_url }}"
    keystone_user: "{{ monasca_keystone_user }}"
    keystone_password: "{{ monasca_keystone_password }}"
    keystone_project: "{{ monasca_keystone_project }}"
    monasca_api_url: "{{ monasca_api_url }}"
  no_log: True
  tags:
    - system_alarms
    - monasca_alarms
    - openstack_alarms
  register: default_notification_result
  until: not default_notification_result | failed
  retries: "{{ monasca_client_retries }}"
  delay: "{{ monasca_client_retry_delay }}"</pre></div><p>
  <span class="bold"><strong>Monasca Alarm Definition</strong></span>
 </p><p>
  In the alarm definition "expression" field, you can specify the metric name
  and threshold. The "match_by" field is used to create a new alarm for every
  unique combination of the match_by metric dimensions.
 </p><p>
  Find more details on alarm definitions at the Monasca API documentation:
  (<a class="link" href="https://github.com/stackforge/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definitions-and-alarms" target="_blank">https://github.com/stackforge/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definitions-and-alarms</a>).
 </p><p>
  The following is a code snippet from the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/tasks/main.yml</code>
  file.
 </p><div class="verbatim-wrap"><pre class="screen">- name: monasca-default-alarms | main | Create Alarm Definitions
  monasca_alarm_definition:
    name: "{{ item.name }}"
    description: "{{ item.description | default('') }}"
    expression: "{{ item.expression }}"
    keystone_token: "{{ default_notification_result.keystone_token }}"
    match_by: "{{ item.match_by | default(['hostname']) }}"
    monasca_api_url: "{{ default_notification_result.monasca_api_url }}"
    severity: "{{ item.severity | default('LOW') }}"
    alarm_actions:
      - "{{ default_notification_result.notification_method_id }}"
    ok_actions:
      - "{{ default_notification_result.notification_method_id }}"
    undetermined_actions:
      - "{{ default_notification_result.notification_method_id }}"
  register: monasca_system_alarms_result
  until: not monasca_system_alarms_result | failed
  retries: "{{ monasca_client_retries }}"
  delay: "{{ monasca_client_retry_delay }}"
  with_flattened:
    - monasca_alarm_definitions_system
    - monasca_alarm_definitions_monasca
    - monasca_alarm_definitions_openstack
    - monasca_alarm_definitions_misc_services
  when: monasca_create_definitions</pre></div><p>
  In the following example
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/vars/main.yml</code>
  Ansible variables file, the alarm definition named
  <span class="bold"><strong>Process Check</strong></span> sets the
  <span class="bold"><strong>match_by</strong></span> variable with the following
  parameters.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    process_name
   </p></li><li class="listitem "><p>
    hostname
   </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">monasca_alarm_definitions_system:
  - name: "Host Status"
    description: "Alarms when the specified host is down or not reachable"
    severity: "HIGH"
    expression: "host_alive_status &gt; 0"
    match_by:
      - "target_host"
      - "hostname"
  - name: "HTTP Status"
    description: &gt;
      "Alarms when the specified HTTP endpoint is down or not reachable"
    severity: "HIGH"
    expression: "http_status &gt; 0"
    match_by:
      - "service"
      - "component"
      - "hostname"
      - "url"
  - name: "CPU Usage"
    description: "Alarms when CPU usage is high"
    expression: "avg(cpu.idle_perc) &lt; 10 times 3"
  - name: "High CPU IOWait"
    description: "Alarms when CPU IOWait is high, possible slow disk issue"
    expression: "avg(cpu.wait_perc) &gt; 40 times 3"
    match_by:
      - "hostname"
  - name: "Disk Inode Usage"
    description: "Alarms when disk inode usage is high"
    expression: "disk.inode_used_perc &gt; 90"
    match_by:
      - "hostname"
      - "device"
    severity: "HIGH"
  - name: "Disk Usage"
    description: "Alarms when disk usage is high"
    expression: "disk.space_used_perc &gt; 90"
    match_by:
      - "hostname"
      - "device"
    severity: "HIGH"
  - name: "Memory Usage"
    description: "Alarms when memory usage is high"
    severity: "HIGH"
    expression: "avg(mem.usable_perc) &lt; 10 times 3"
  - name: "Network Errors"
    description: &gt;
      "Alarms when either incoming or outgoing network errors are high"
    severity: "MEDIUM"
    expression: "net.in_errors_sec &gt; 5 or net.out_errors_sec &gt; 5"
  - name: "Process Check"
    description: "Alarms when the specified process is not running"
    severity: "HIGH"
    expression: "process.pid_count &lt; 1"
    match_by:
      - "process_name"
      - "hostname"
  - name: "Crash Dump Count"
    description: "Alarms when a crash directory is found"
    severity: "MEDIUM"
    expression: "crash.dump_count &gt; 0"
    match_by:
      - "hostname"</pre></div><p>
  The preceding configuration would result in the creation of an alarm for each
  unique metric that matched the following criteria.
 </p><div class="verbatim-wrap"><pre class="screen">process.pid_count + process_name + hostname</pre></div><p>
  <span class="bold"><strong>Check that the alarms exist</strong></span>
 </p><p>
  Begin by using the following commands, including <code class="literal">monasca
  alarm-definition-list</code>, to check that the alarm definition exists.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>monasca alarm-definition-list --name <em class="replaceable ">ALARM_DEFINITION_NAME</em></pre></div><p>
  Then use either of the following commands to check that the alarm has been
  generated. A status of "OK" indicates a healthy alarm.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-name <em class="replaceable ">metric name</em></pre></div><p>
  Or
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --alarm-definition-id <em class="replaceable ">ID_FROM_ALARM-DEFINITION-LIST</em></pre></div><div id="id-1.6.5.5.7.36" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   To see CLI options use the <code class="literal">monasca help</code> command.
  </p></div><p>
  <span class="bold"><strong>Alarm state upgrade considerations</strong></span>
 </p><p>
  If the name of a monitoring metric changes or is no longer being sent,
  existing alarms will show the alarm state as UNDETERMINED. You can update an
  alarm definition as long as you do not change the <span class="bold"><strong>metric
  name</strong></span> or <span class="bold"><strong>dimension name</strong></span> values in
  the <span class="bold"><strong>expression</strong></span> or
  <span class="bold"><strong>match_by</strong></span> fields. If you find that you need
  to alter either of these values, you must delete the old alarm definitions
  and create new definitions with the updated values.
 </p><p>
  If a metric is never sent, but had a related alarm definition, then no alarms
  would exist. If you find that no metrics are never sent, then you should
  remove the related alarm definition.
 </p><p>
  When removing an alarm definition, the Ansible module
  <span class="bold"><strong>monasca_alarm_definition</strong></span> supports the state
  "absent".
 </p><p>
  The following file snippet shows an example of how to remove an alarm
  definition by setting the state to <span class="bold"><strong>absent</strong></span>.
 </p><div class="verbatim-wrap"><pre class="screen">- name: monasca-pre-upgrade | Remove alarm definitions
   monasca_alarm_definition:
     name: "{{ item.name }}"
     state: "absent"
     keystone_url: "{{ monasca_keystone_url }}"
     keystone_user: "{{ monasca_keystone_user }}"
     keystone_password: "{{ monasca_keystone_password }}"
     keystone_project: "{{ monasca_keystone_project }}"
     monasca_api_url: "{{ monasca_api_url }}"
   with_items:
     - { name: "Kafka Consumer Lag" }</pre></div><p>
  An alarm exists in the OK state when the monasca threshold engine has seen at
  least one metric associated with the alarm definition and has not exceeded
  the alarm definition threshold.
 </p></div><div class="sect2" id="integration-of-plugins-with-monasca-agent"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Openstack Integration of Custom Plugins into Monasca-Agent (if applicable)</span> <a title="Permalink" class="permalink" href="#integration-of-plugins-with-monasca-agent">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-integration_of_plugins_with_monasca-agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integration_of_plugins_with_monasca-agent.xml</li><li><span class="ds-label">ID: </span>integration-of-plugins-with-monasca-agent</li></ul></div></div></div></div><p>
  Monasca-agent is an OpenStack open-source project. Monasca can also monitor
  non-openstack services. Third parties should install custom plugins into
  their <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> system using the steps outlined in the
  <a class="xref" href="#writing-custom-plugins" title="3.4.3. Writing Custom Plugins">Section 3.4.3, “Writing Custom Plugins”</a>. If the OpenStack community
  determines that the custom plugins are of general benefit, the plugin may be
  added to the openstack/monasca-agent so that they are installed with the
  monasca-agent. During the review process for openstack/monasca-agent there
  are no guarantees that code will be approved or merged by a deadline.
  Open-source contributors are expected to help with codereviews in order to get
  their code accepted. Once changes are approved and integrated into the
  openstack/monasca-agent and that version of the monasca-agent is integrated
  with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the third party can remove the custom plugin
  installation steps since they would be installed in the default monasca-agent
  venv.
 </p><p>
  Find the open source repository for the monaca-agent here:
  <a class="link" href="https://github.com/openstack/monasca-agent" target="_blank">https://github.com/openstack/monasca-agent</a>
 </p></div></div></div><div class="chapter " id="ops-managing-identity"><div class="titlepage"><div><div><h1 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Identity</span> <a title="Permalink" class="permalink" href="#ops-managing-identity">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_identity.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_identity.xml</li><li><span class="ds-label">ID: </span>ops-managing-identity</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-operation-identity-overview"><span class="number">4.1 </span><span class="name">The Identity Service</span></a></span></dt><dt><span class="section"><a href="#supported-upstream-keystone-features"><span class="number">4.2 </span><span class="name">Supported Upstream Keystone Features</span></a></span></dt><dt><span class="section"><a href="#sec-operation-identity"><span class="number">4.3 </span><span class="name">Understanding Domains, Projects, Users, Groups, and Roles</span></a></span></dt><dt><span class="section"><a href="#topic-ffs-dvz-nw"><span class="number">4.4 </span><span class="name">Identity Service Token Validation Example</span></a></span></dt><dt><span class="section"><a href="#topic-qmz-fg3-btx"><span class="number">4.5 </span><span class="name">Configuring the Identity Service</span></a></span></dt><dt><span class="section"><a href="#admin-password"><span class="number">4.6 </span><span class="name">Retrieving the Admin Password</span></a></span></dt><dt><span class="section"><a href="#servicePasswords"><span class="number">4.7 </span><span class="name">Changing Service Passwords</span></a></span></dt><dt><span class="section"><a href="#topic-m43-2j3-bt"><span class="number">4.8 </span><span class="name">Reconfiguring the Identity Service</span></a></span></dt><dt><span class="section"><a href="#ldap"><span class="number">4.9 </span><span class="name">Integrating LDAP with the Identity Service</span></a></span></dt><dt><span class="section"><a href="#k2kfed"><span class="number">4.10 </span><span class="name">Keystone-to-Keystone Federation</span></a></span></dt><dt><span class="section"><a href="#websso"><span class="number">4.11 </span><span class="name">Configuring Web Single Sign-On</span></a></span></dt><dt><span class="section"><a href="#topic-qtp-cn3-bt"><span class="number">4.12 </span><span class="name">Identity Service Notes and Limitations</span></a></span></dt></dl></div></div><p>
  The Identity service provides the structure for user authentication to your
  cloud.
 </p><div class="sect1" id="sec-operation-identity-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Identity Service</span> <a title="Permalink" class="permalink" href="#sec-operation-identity-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>sec-operation-identity-overview</li></ul></div></div></div></div><p>
  This topic explains the purpose and mechanisms of the identity service.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Identity service, based on the OpenStack Keystone API, is
  responsible for providing UserID authentication and access authorization to
  enable organizations to achieve their access security and compliance
  objectives and successfully deploy OpenStack. In short, the Identity Service
  is the gateway to the rest of the OpenStack services.
 </p><div class="sect2" id="idg-all-operations-configuring-identity-identity-overview-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Which version of the Keystone Identity service should you use?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-overview-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-overview-xml-7</li></ul></div></div></div></div><p>
   Use Identity API version 3.0. Identity API v2.0 is deprecated. Many
   features such as LDAP integration and fine-grained access control will not
   work with v2.0. Below are a few more questions you may have regarding
   versions.
  </p><p>
   <span class="bold"><strong>Why does the Keystone identity catalog still show
   version 2.0?</strong></span>
  </p><p>
   Tempest tests still use the v2.0 API. They are in the process of migrating
   to v3.0. We will remove the v2.0 version once tempest has migrated the
   tests. The Identity catalog has v2.0 version just to support tempest
   migration.
  </p><p>
   <span class="bold"><strong>Will the Keystone identity v3.0 API work if the
   identity catalog has only the v2.0 endpoint?</strong></span>
  </p><p>
   Identity v3.0 does not rely on the content of the catalog. It will continue
   to work regardless of the version of the API in the catalog.
  </p><p>
   <span class="bold"><strong>Which CLI client should you use?</strong></span>
  </p><p>
   You should use the OpenStack CLI, not the Keystone CLI as it is deprecated.
   The Keystone CLI does not support v3.0 API, only the OpenStack CLI supports
   the v3.0 API.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-overview-xml-8"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authentication</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-overview-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-overview-xml-8</li></ul></div></div></div></div><p>
   The authentication function provides the initial login function to
   OpenStack. Keystone supports multiple sources of authentication, including a
   native or built-in authentication system. The Keystone native system can be
   used for all user management functions for proof of concept deployments or
   small deployments not requiring integration with a corporate authentication
   system, but it lacks some of the advanced functions usually found in user
   management systems such as forcing password changes. The focus of the
   Keystone native authentication system is to be the source of authentication
   for OpenStack-specific users required for the operation of the various
   OpenStack services. These users are stored by Keystone in a default domain;
   the addition of these IDs to an external authentication system is not
   required.
  </p><p>
   Keystone is more commonly integrated with external authentication systems
   such as OpenLDAP or Microsoft Active Directory. These systems are usually
   centrally deployed by organizations to serve as the single source of user
   management and authentication for all in-house deployed applications and
   systems requiring user authentication. In addition to LDAP and Microsoft
   Active Directory, support for integration with Security Assertion Markup
   Language (SAML)-based identity providers from companies such as Ping, CA,
   IBM, Oracle, and others is also nearly "production-ready".
  </p><p>
   Keystone also provides architectural support via the underlying Apache
   deployment for other types of authentication systems such as Multi-Factor
   Authentication. These types of systems typically require driver support and
   integration from the respective provider vendors.
  </p><div id="id-1.6.6.3.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    While support for Identity Providers and Multi-factor authentication is
    available in Keystone, it has not yet been certified by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    engineering team and is an experimental feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
   </p></div><p>
   LDAP-compatible directories such as OpenLDAP and Microsoft Active Directory
   are recommended alternatives to using the Keystone local authentication.
   Both methods are widely used by organizations and are integrated with a
   variety of other enterprise applications. These directories act as the
   single source of user information within an organization. Keystone can be
   configured to authenticate against an LDAP-compatible directory on a
   per-domain basis.
  </p><p>
   Domains, as explained in <a class="xref" href="#sec-operation-identity" title="4.3. Understanding Domains, Projects, Users, Groups, and Roles">Section 4.3, “Understanding Domains, Projects, Users, Groups, and Roles”</a>,
   can be configured so that based on the user ID, a incoming user is
   automatically mapped to a specific domain. This domain can then be
   configured to authenticate against a specific LDAP directory. The user
   credentials provided by the user to Keystone are passed along to the
   designated LDAP source for authentication. This communication can be
   optionally configured to be secure via SSL encryption. No special LDAP
   administrative access is required, and only read-only access is needed for
   this configuration. Keystone will not add any LDAP information. All user
   additions, deletions, and modifications are performed by the application's
   front end in the LDAP directories. After a user has been successfully
   authenticated, he is then assigned to the groups, roles, and projects
   defined by the Keystone domain or project administrators. This information
   is stored within the Keystone service database.
  </p><p>
   Another form of external authentication provided by the Keystone service is
   via integration with SAML-based Identity Providers (IdP) such as Ping
   Identity, IBM Tivoli, and Microsoft Active Directory Federation Server. A
   SAML-based identity provider provides authentication that is often called
   "single sign-on". The IdP server is configured to authenticate against
   identity sources such as Active Directory and provides a single
   authentication API against multiple types of downstream identity sources.
   This means that an organization could have multiple identity storage sources
   but a single authentication source. In addition, if a user has logged into
   one such source during a defined session time frame, they do not need to
   re-authenticate within the defined session. Instead, the IdP will
   automatically validate the user to requesting applications and services.
  </p><p>
   A SAML-based IdP authentication source is configured with Keystone on a
   per-domain basis similar to the manner in which native LDAP directories are
   configured. Extra mapping rules are required in the configuration that
   define which Keystone group an incoming UID is automatically assigned to.
   This means that groups need to be defined in Keystone first, but it also
   removes the requirement that a domain or project admin assign user roles and
   project membership on a per-user basis. Instead, groups are used to define
   project membership and roles and incoming users are automatically mapped to
   Keystone groups based on their upstream group membership. This provides a
   very consistent role-based access control (RBAC) model based on the upstream
   identity source. The configuration of this option is fairly straightforward.
   IdP vendors such as Ping and IBM are contributing to the maintenance of this
   function and have also produced their own integration documentation.
   Microsoft Active Directory Federation Services (ADFS) is used for functional
   testing and future documentation.
  </p><p>
   The third Keystone-supported authentication source is known as Multi-Factor
   Authentication (MFA). MFA typically requires an external source of
   authentication beyond a login name and password, and can include options
   such as SMS text, a temporal token generator, a fingerprint scanner, etc.
   Each of these types of MFA are usually specific to a particular MFA vendor.
   The Keystone architecture supports an MFA-based authentication system, but
   this has not yet been certified or documented for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-overview-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authorization</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-overview-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-overview-xml-9</li></ul></div></div></div></div><p>
   The second major function provided by the Keystone service is access
   authorization that determines what resources and actions are available based
   on the UserID, the role of the user, and the projects that a user is
   provided access to. All of this information is created, managed, and stored
   by Keystone. These functions are applied via the Horizon web interface, the
   OpenStack command-line interface, or the direct Keystone API.
  </p><p>
   Keystone provides support for organizing users via three entities including:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.6.3.6.4.1"><span class="term ">Domains</span></dt><dd><p>
      Domains provide the highest level of organization. Domains are intended
      to be used as high-level containers for multiple projects. A domain can
      represent different tenants, companies or organizations for an OpenStack
      cloud deployed for public cloud deployments or represent major business
      units, functions, or any other type of top-level organization unit in an
      OpenStack private cloud deployment. Each domain has at least one Domain
      Admin assigned to it. This Domain Admin can then create multiple projects
      within the domain and assign the project admin role to specific project
      owners. Each domain created in an OpenStack deployment is unique and the
      projects assigned to a domain cannot exist in another domain.
     </p></dd><dt id="id-1.6.6.3.6.4.2"><span class="term ">Projects</span></dt><dd><p>
      Projects are entities within a domain that represent groups of users,
      each user role within that project, and how many underlying
      infrastructure resources can be consumed by members of the project.
     </p></dd><dt id="id-1.6.6.3.6.4.3"><span class="term ">Groups</span></dt><dd><p>
      Groups are an optional function and provide the means of assigning
      project roles to multiple users at once.
     </p></dd></dl></div><p>
   Keystone also provides the means to create and assign roles to groups of
   users or individual users. The role names are created and user assignments
   are made within Keystone. The actual function of a role is defined currently
   per each OpenStack service via scripts. When a user requests access to an
   OpenStack service, his access token contains information about his assigned
   project membership and role for that project. This role is then matched to
   the service-specific script and the user is allowed to perform functions
   within that service defined by the role mapping.
  </p></div></div><div class="sect1" id="supported-upstream-keystone-features"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Upstream Keystone Features</span> <a title="Permalink" class="permalink" href="#supported-upstream-keystone-features">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>supported-upstream-keystone-features</li></ul></div></div></div></div><div class="sect2" id="section-azv-113-jx"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack upstream features that are enabled by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span></span> <a title="Permalink" class="permalink" href="#section-azv-113-jx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>section-azv-113-jx</li></ul></div></div></div></div><p>
   The following supported Keystone features are enabled by default in the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> release.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /></colgroup><thead><tr><th>Name</th><th>User/Admin</th><th>Note: API support only. No CLI/UI support</th></tr></thead><tbody><tr><td>Implied Roles</td><td>Admin</td><td>https://blueprints.launchpad.net/keystone/+spec/implied-roles</td></tr><tr><td>Domain-Specific Roles</td><td>Admin</td><td>https://blueprints.launchpad.net/keystone/+spec/domain-specific-roles</td></tr></tbody></table></div><p>
   <span class="bold"><strong>Implied rules</strong></span>
  </p><p>
   To allow for the practice of hierarchical permissions in user roles, this
   feature enables roles to be linked in such a way that they function as a
   hierarchy with role inheritance.
  </p><p>
   When a user is assigned a superior role, the user will also be assigned all
   roles implied by any subordinate roles. The hierarchy of the assigned roles
   will be expanded when issuing the user a token.
  </p><p>
   <span class="bold"><strong>Domain-specific roles</strong></span>
  </p><p>
   This feature extends the principle of <span class="bold"><strong>implied
   roles</strong></span> to include a set of roles that are specific to a domain. At
   the time a token is issued, the domain-specific roles are not included in
   the token, however, the roles that they map to are.
  </p></div><div class="sect2" id="section-rpw-21h-jx"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack upstream features that are disabled by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span></span> <a title="Permalink" class="permalink" href="#section-rpw-21h-jx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>section-rpw-21h-jx</li></ul></div></div></div></div><p>
   The following is a list of features which are fully supported in the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> release, but are disabled by default. Customers can run a
   playbook to enable the features.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /></colgroup><thead><tr><th>Name</th><th>User/Admin</th><th>Reason Disabled</th></tr></thead><tbody><tr><td>Support multiple LDAP backends via per-domain configuration</td><td>Admin</td><td>Needs explicit configuration.</td></tr><tr><td>WebSSO</td><td>User and Admin</td><td>Needs explicit configuration.</td></tr><tr><td>Keystone-to-Keystone (K2K) federation</td><td>User and Admin</td><td>Needs explicit configuration.</td></tr><tr><td>Fernet token provider</td><td>User and Admin</td><td>Needs explicit configuration.</td></tr><tr><td>Domain-specific config in SQL</td><td>Admin</td><td>Domain specific configuration options can be stored in SQL instead of
                        configuration files, using the new REST APIs.</td></tr></tbody></table></div><p>
   <span class="bold"><strong>Multiple LDAP backends for each domain</strong></span>
  </p><p>
   This feature allows identity backends to be configured on a domain-by-domain
   basis. Domains will be capable of having their own exclusive LDAP service
   (or multiple services). A single LDAP service can also serve multiple
   domains, with each domain in a separate subtree.
  </p><p>
   To implement this feature, individual domains will require domain-specific
   configuration files. Domains that do not implement this feature will
   continue to share a common backend driver.
  </p><p>
   <span class="bold"><strong>WebSSO</strong></span>
  </p><p>
   This feature enables the Keystone service to provide federated identity
   services through a token-based single sign-on page. This feature is disabled
   by default, as it requires explicit configuration.
  </p><p>
   <span class="bold"><strong>Keystone-to-Keystone (K2K) federation</strong></span>
  </p><p>
   This feature enables separate Keystone instances to federate identities
   among the instances, offering inter-cloud authorization. This feature is
   disabled by default, as it requires explicit configuration.
  </p><p>
   <span class="bold"><strong>Fernet token provider</strong></span>
  </p><p>
   Provides tokens in the fernet format. This is an experimental feature and is
   disabled by default.
  </p><p>
   <span class="bold"><strong>Domain-specific config in SQL</strong></span>
  </p><p>
   Using the new REST APIs, domain-specific configuration options can be stored
   in a SQL database instead of in configuration files.
  </p></div><div class="sect2" id="section-fm3-mch-jx"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stack upstream features that have been specifically disabled in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span></span> <a title="Permalink" class="permalink" href="#section-fm3-mch-jx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>section-fm3-mch-jx</li></ul></div></div></div></div><p>
   The following is a list of extensions which are disabled by default in
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, according to Keystone policy.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><thead><tr><th>Target Release</th><th>Name</th><th>User/Admin</th><th>Reason Disabled</th></tr></thead><tbody><tr><td>TBD</td><td>Endpoint Filtering</td><td>Admin</td><td>
       <p>
        This extension was implemented to facilitate service activation.
        However, due to lack of enforcement at the service side, this feature
        is only half effective right now.
       </p>
      </td></tr><tr><td>TBD</td><td>Endpoint Policy</td><td>Admin</td><td>
       <p>
        This extension was intended to facilitate policy (policy.json)
        management and enforcement. This feature is useless right now due to
        lack of the needed middleware to utilize the policy files stored in
        Keystone.
       </p>
      </td></tr><tr><td>TBD</td><td>OATH 1.0a</td><td>User and Admin</td><td>
       <p>
        Complexity in workflow. Lack of adoption. Its alternative, Keystone
        Trust, is enabled by default. HEAT is using Keystone Trust.
       </p>
      </td></tr><tr><td>TBD</td><td>Revocation Events</td><td>Admin</td><td>
       <p>
        For PKI token only and PKI token is disabled by default due to
        usability concerns.
       </p>
      </td></tr><tr><td>TBD</td><td>OS CERT</td><td>Admin</td><td>
       <p>
        For PKI token only and PKI token is disabled by default due to
        usability concerns.
       </p>
      </td></tr><tr><td>TBD</td><td>PKI Token</td><td>Admin</td><td>
       <p>
        PKI token is disabled by default due to usability concerns.
       </p>
      </td></tr><tr><td>TBD</td><td>Driver level caching</td><td>Admin</td><td>
       <p>
        Driver level caching is disabled by default due to complexity in setup.
       </p>
      </td></tr><tr><td>TBD</td><td>Tokenless Authz</td><td>Admin</td><td>
       <p>
        Tokenless authorization with X.509 SSL client certificate.
       </p>
      </td></tr><tr><td>TBD</td><td>TOTP Authentication</td><td>User</td><td>
       <p>
        Not fully baked. Has not been battle-tested.
       </p>
      </td></tr><tr><td>TBD</td><td>is_admin_project</td><td>Admin</td><td>
       <p>
        No integration with the services.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect1" id="sec-operation-identity"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding Domains, Projects, Users, Groups, and Roles</span> <a title="Permalink" class="permalink" href="#sec-operation-identity">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>sec-operation-identity</li></ul></div></div></div></div><p>
  The identity service uses these concepts for authentication within your cloud
  and these are descriptions of each of them.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> identity service uses OpenStack Keystone and the concepts
  of domains, projects, users, groups, and roles to manage authentication. This
  page describes how these work together.
 </p><div class="sect2" id="domains-projects-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domains, Projects, Users, Groups, and Roles</span> <a title="Permalink" class="permalink" href="#domains-projects-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>domains-projects-roles</li></ul></div></div></div></div><p>
   Most large business organizations use an identity system such as Microsoft
   Active Directory to store and manage their internal user information. A
   variety of applications such as HR systems are, in turn, used to manage the
   data inside of Active Directory. These same organizations often deploy a
   separate user management system for external users such as contractors,
   partners, and customers. Multiple authentication systems are then deployed
   to support multiple types of users.
  </p><p>
   An LDAP-compatible directory such as Active Directory provides a top-level
   organization or domain component. In this example, the organization is
   called Acme. The domain component (DC) is defined as acme.com. Underneath
   the top level domain component are entities referred to as organizational
   units (OU). Organizational units are typically designed to reflect the
   entity structure of the organization. For example, this particular schema
   has 3 different organizational units for the Marketing, IT, and Contractors
   units or departments of the Acme organization. Users (and other types of
   entities like printers) are then defined appropriately underneath each
   organizational entity. The Keystone domain entity can be used to match the
   LDAP OU entity; each LDAP OU can have a corresponding Keystone domain
   created. In this example, both the Marketing and IT domains represent
   internal employees of Acme and use the same authentication source. The
   Contractors domain contains all external people associated with Acme.
   UserIDs associated with the Contractor domain are maintained in a separate
   user directory and thus have a different authentication source assigned to
   the corresponding Keystone-defined Contractors domain.
  </p><p>
   A public cloud deployment usually supports multiple, separate organizations.
   Keystone domains can be created to provide a domain per organization with
   each domain configured to the underlying organization's authentication
   source. For example, the ABC company would have a Keystone domain created
   called "abc". All users authenticating to the "abc" domain would be
   authenticated against the authentication system provided by the ABC
   organization; in this case ldap://ad.abc.com
  </p></div><div class="sect2" id="domains"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domains</span> <a title="Permalink" class="permalink" href="#domains">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>domains</li></ul></div></div></div></div><p>
   A domain is a top-level container targeted at defining major organizational
   entities.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Domains can be used in a multi-tenant OpenStack deployment to segregate
     projects and users from different companies in a public cloud deployment
     or different organizational units in a private cloud setting.
    </p></li><li class="listitem "><p>
     Domains provide the means to identify multiple authentication sources.
    </p></li><li class="listitem "><p>
     Each domain is unique within an OpenStack implementation.
    </p></li><li class="listitem "><p>
     Multiple projects can be assigned to a domain but each project can only
     belong to a single domain.
    </p></li><li class="listitem "><p>
     Each domain and project have an assigned admin.
    </p></li><li class="listitem "><p>
     Domains are created by the "admin" service account and domain admins are
     assigned by the "admin" user.
    </p></li><li class="listitem "><p>
     The "admin" UserID (UID) is created during the Keystone installation, has
     the "admin" role assigned to it, and is defined as the "Cloud Admin". This
     UID is created using the "magic" or "secret" admin token found in the
     default 'keystone.conf' file installed during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> keystone
     installation after the Keystone service has been installed. This secret
     token should be removed after installation and the "admin" password
     changed.
    </p></li><li class="listitem "><p>
     The "default" domain is created automatically during the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Keystone
     installation.
    </p></li><li class="listitem "><p>
     The "default" domain contains all OpenStack service accounts that are
     installed during the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> keystone installation process.
    </p></li><li class="listitem "><p>
     No users but the OpenStack service accounts should be assigned to the
     "default" domain.
    </p></li><li class="listitem "><p>
     Domain admins can be any UserID inside or outside of the domain.
    </p></li></ul></div></div><div class="sect2" id="domain-admin"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domain Administrator</span> <a title="Permalink" class="permalink" href="#domain-admin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>domain-admin</li></ul></div></div></div></div><p>
   A UUID is a domain administrator for a given domain if that UID has a
   domain-scoped token scoped for the given domain. This means that the UID has
   the "admin" role assigned to it for the selected domain.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Cloud Admin UID assigns the domain administrator role for a domain to
     a selected UID.
    </p></li><li class="listitem "><p>
     A domain administrator can create and delete local users who have
     authenticated against Keystone. These users will be assigned to the domain
     belonging to the domain administrator who creates the UserID.
    </p></li><li class="listitem "><p>
     A domain administrator can only create users and projects within her
     assigned domains.
    </p></li><li class="listitem "><p>
     A domain administrator can assign the "admin" role of their domains to
     another UID or revoke it; each UID with the "admin" role for a specified
     domain will be a co-administrator for that domain.
    </p></li><li class="listitem "><p>
     A UID can be assigned to be the domain admin of multiple domains.
    </p></li><li class="listitem "><p>
     A domain administrator can assign non-admin roles to any users and groups
     within their assigned domain, including projects owned by their assigned
     domain.
    </p></li><li class="listitem "><p>
     A domain admin UID can belong to projects within their administered
     domains.
    </p></li><li class="listitem "><p>
     Each domain can have a different authentication source.
    </p></li><li class="listitem "><p>
     The domain field is used during the initial login to define the source of
     authentication.
    </p></li><li class="listitem "><p>
     The "List Users" function can only be executed by a UID with the domain
     admin role.
    </p></li><li class="listitem "><p>
     A domain administrator can assign a UID from outside of their domain the
     "domain admin" role but it is assumed that the domain admin would know the
     specific UID and would not need to list users from an external domain.
    </p></li><li class="listitem "><p>
     A domain administrator can assign a UID from outside of their domain the
     "project admin" role for a specific project within their domain but it is
     assumed that the domain admin would know the specific UID and would not
     need to list users from an external domain.
    </p></li></ul></div></div><div class="sect2" id="projects"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Projects</span> <a title="Permalink" class="permalink" href="#projects">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>projects</li></ul></div></div></div></div><p>
   The domain administrator creates projects within his assigned domain and
   assigns the project admin role to each project to a selected UID. A UID is a
   project administrator for a given project if that UID has a project-scoped
   token scoped for the given project. There can be multiple projects per
   domain. The project admin sets the project quota settings, adds/deletes
   users and groups to and from the project, and defines the user/group roles
   for the assigned project. Users can be belong to multiple projects and have
   different roles on each project. Users are assigned to a specific domain and
   a default project. Roles are assigned per project.
  </p></div><div class="sect2" id="users-groups"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Users and Groups</span> <a title="Permalink" class="permalink" href="#users-groups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>users-groups</li></ul></div></div></div></div><p>
   Each user belongs to one domain only. Domain assignments are defined either
   by the domain configuration files or by a domain administrator when creating
   a new, local (user authenticated against Keystone) user. There is no current
   method for "moving" a user from one domain to another. A user can belong to
   multiple projects within a domain with a different role assignment per
   project. A group is a collection of users. Users can be assigned to groups
   either by the project admin or automatically via mappings if an external
   authentication source is defined for the assigned domain. Groups can be
   assigned to multiple projects within a domain and have different roles
   assigned to the group per project. A group can be assigned the "admin" role
   for a domain or project. All members of the group will be an "admin" for the
   selected domain or project.
  </p></div><div class="sect2" id="roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Roles</span> <a title="Permalink" class="permalink" href="#roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>roles</li></ul></div></div></div></div><p>
   Service roles represent the functionality used to implement the OpenStack
   role based access control (RBAC), model used to manage access to each
   OpenStack service. Roles are named and assigned per user or group for each
   project by the identity service. Role definition and policy
   enforcement are defined outside of the identity service independently by
   each OpenStack service. The token generated by the identity service for each
   user authentication contains the role assigned to that user for a particular
   project. When a user attempts to access a specific OpenStack service, the
   role is parsed by the service, compared to the service-specific policy file,
   and then granted the resource access defined for that role by the service
   policy file.
  </p><p>
   Each service has its own service policy file with the
   /etc/[SERVICE_CODENAME]/policy.json file name format where
   [SERVICE_CODENAME] represents a specific OpenStack service name. For
   example, the OpenStack Nova service would have a policy file called
   /etc/nova/policy.json. With Service policy files can be modified and
   deployed to control nodes from the Cloud Lifecycle Manager. Administrators are
   advised to validate policy changes before checking in the changes to the
   site branch of the local git repository before rolling the changes into
   production. Do not make changes to policy files without having a way to
   validate them.
  </p><p>
   The policy files are located at the following site branch locations on the
   Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/GLA-API/templates/policy.json.j2
~/openstack/ardana/ansible/roles/ironic-common/files/policy.json
~/openstack/ardana/ansible/roles/KEYMGR-API/templates/policy.json
~/openstack/ardana/ansible/roles/heat-common/files/policy.json
~/openstack/ardana/ansible/roles/CND-API/templates/policy.json
~/openstack/ardana/ansible/roles/nova-common/files/policy.json
~/openstack/ardana/ansible/roles/CEI-API/templates/policy.json.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/policy.json.j2</pre></div><p>
   For test and validation, policy files can be modified in a non-production
   environment from the <code class="filename">~/scratch/</code> directory. For a specific
   policy file, run a search for policy.json. To deploy policy changes for a
   service, run the service specific reconfiguration playbook (for example,
   nova-reconfigure.yml). For a complete list of reconfiguration playbooks,
   change directories to <code class="filename">~/scratch/ansible/next/ardana/ansible</code>
   and run this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls | grep reconfigure</pre></div><p>
   A read-only role named <code class="literal">project_observer</code> is explicitly
   created in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. Any user who is granted this role can use
   <code class="literal">list_project</code>.
  </p></div></div><div class="sect1" id="topic-ffs-dvz-nw"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity Service Token Validation Example</span> <a title="Permalink" class="permalink" href="#topic-ffs-dvz-nw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_token_validation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_token_validation.xml</li><li><span class="ds-label">ID: </span>topic-ffs-dvz-nw</li></ul></div></div></div></div><p>
  The following diagram illustrates the flow of typical Identity Service
  (Keystone) requests/responses between <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services and the Identity
  service. It shows how Keystone issues and validates tokens to ensure the
  identity of the caller of each service.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-keystone-KeystoneTokenValidationExample.png" target="_blank"><img src="images/media-keystone-KeystoneTokenValidationExample.png" width="" /></a></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     
     Horizon sends an HTTP authentication request to Keystone for user
     credentials.
    </p></li><li class="listitem "><p>
     
     Keystone validates the credentials and replies with token.
    </p></li><li class="listitem "><p>
     
     Horizon sends a POST request, with token to Nova to start
     provisioning a virtual machine.
    </p></li><li class="listitem "><p>
     
     Nova sends token to Keystone for validation.
    </p></li><li class="listitem "><p>
     
     Keystone validates the token.
    </p></li><li class="listitem "><p>
     
     Nova forwards a request for an image with the attached token.
    </p></li><li class="listitem "><p>
     
     Glance sends token to Keystone for validation.
    </p></li><li class="listitem "><p>
     
     Keystone validates the token.
    </p></li><li class="listitem "><p>
     
     Glance provides image-related information to Nova.
    </p></li><li class="listitem "><p>
     
     Nova sends request for networks to Neutron with token.
    </p></li><li class="listitem "><p>
     
     Neutron sends token to Keystone for validation.
    </p></li><li class="listitem "><p>
     
     Keystone validates the token.
    </p></li><li class="listitem "><p>
     
     Neutron provides network-related information to Nova.
    </p></li><li class="listitem "><p>
     
     Nova reports the status of the virtual machine provisioning request.
    </p></li></ol></div></div><div class="sect1" id="topic-qmz-fg3-btx"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Identity Service</span> <a title="Permalink" class="permalink" href="#topic-qmz-fg3-btx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>topic-qmz-fg3-btx</li></ul></div></div></div></div><div class="sect2" id="id-1.6.6.7.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is the Identity service?</span> <a title="Permalink" class="permalink" href="#id-1.6.6.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Identity service, based on the OpenStack Keystone API, provides
   UserID authentication and access authorization to help organizations achieve
   their access security and compliance objectives and successfully deploy
   OpenStack. In short, the Identity service is the gateway to the rest of the
   OpenStack services.
  </p><p>
   The identity service is installed automatically by the Cloud Lifecycle Manager
   (just after MySQL and RabbitMQ). When your cloud is up and running, you can
   customize Keystone in a number of ways, including integrating with LDAP
   servers. This topic describes the default configuration. See
   <a class="xref" href="#topic-m43-2j3-bt" title="4.8. Reconfiguring the Identity Service">Section 4.8, “Reconfiguring the Identity Service”</a> for changes you can
   implement. Also see <a class="xref" href="#ldap" title="4.9. Integrating LDAP with the Identity Service">Section 4.9, “Integrating LDAP with the Identity Service”</a> for information
   on integrating with an LDAP provider.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Which version of the Keystone Identity service should you use?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-6</li></ul></div></div></div></div><p>
   Note that you should use identity API version 3.0. Identity API v2.0 was has
   been deprecated. Many features such as LDAP integration and fine-grained
   access control will not work with v2.0. The following are a few questions you
   may have regarding versions.
  </p><p>
   <span class="bold"><strong>Why does the Keystone identity catalog still show
   version 2.0?</strong></span>
  </p><p>
   Tempest tests still use the v2.0 API. They are in the process of migrating
   to v3.0. We will remove the v2.0 version once tempest has migrated the
   tests. The Identity catalog has version 2.0 just to support tempest
   migration.
  </p><p>
   <span class="bold"><strong>Will the Keystone identity v3.0 API work if the
   identity catalog has only the v2.0 endpoint?</strong></span>
  </p><p>
   Identity v3.0 does not rely on the content of the catalog. It will continue
   to work regardless of the version of the API in the catalog.
  </p><p>
   <span class="bold"><strong>Which CLI client should you use?</strong></span>
  </p><p>
   You should use the OpenStack CLI, not the Keystone CLI, because it is
   deprecated. The Keystone CLI does not support the v3.0 API; only the
   OpenStack CLI supports the v3.0 API.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authentication</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-7</li></ul></div></div></div></div><p>
   The authentication function provides the initial login function to
   OpenStack. Keystone supports multiple sources of authentication, including a
   native or built-in authentication system. You can use the Keystone native
   system for all user management functions for proof-of-concept deployments or
   small deployments not requiring integration with a corporate authentication
   system, but it lacks some of the advanced functions usually found in user
   management systems such as forcing password changes. The focus of the
   Keystone native authentication system is to be the source of authentication
   for OpenStack-specific users required to operate various OpenStack services.
   These users are stored by Keystone in a default domain; the addition of
   these IDs to an external authentication system is not required.
  </p><p>
   Keystone is more commonly integrated with external authentication systems
   such as OpenLDAP or Microsoft Active Directory. These systems are usually
   centrally deployed by organizations to serve as the single source of user
   management and authentication for all in-house deployed applications and
   systems requiring user authentication. In addition to LDAP and Microsoft
   Active Directory, support for integration with Security Assertion Markup
   Language (SAML)-based identity providers from companies such as Ping, CA,
   IBM, Oracle, and others is also nearly "production-ready."
  </p><p>
   Keystone also provides architectural support through the underlying Apache
   deployment for other types of authentication systems, such as multi-factor
   authentication. These types of systems typically require driver support and
   integration from the respective providers.
  </p><div id="id-1.6.6.7.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    While support for Identity providers and multi-factor authentication is
    available in Keystone, it has not yet been certified by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    engineering team and is an experimental feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
   </p></div><p>
   LDAP-compatible directories such as OpenLDAP and Microsoft Active Directory
   are recommended alternatives to using Keystone local authentication. Both
   methods are widely used by organizations and are integrated with a variety
   of other enterprise applications. These directories act as the single source
   of user information within an organization. You can configure Keystone to
   authenticate against an LDAP-compatible directory on a per-domain basis.
  </p><p>
   Domains, as explained in <a class="xref" href="#sec-operation-identity" title="4.3. Understanding Domains, Projects, Users, Groups, and Roles">Section 4.3, “Understanding Domains, Projects, Users, Groups, and Roles”</a>,
   can be configured so that, based on the user ID, an incoming user is
   automatically mapped to a specific domain. You can then configure this
   domain to authenticate against a specific LDAP directory. User credentials
   provided by the user to Keystone are passed along to the designated LDAP
   source for authentication. You can optionally configure this communication
   to be secure through SSL encryption. No special LDAP administrative access
   is required, and only read-only access is needed for this configuration.
   Keystone will not add any LDAP information. All user additions, deletions,
   and modifications are performed by the application's front end in the LDAP
   directories. After a user has been successfully authenticated, that user
   is then assigned to the groups, roles, and projects defined by the
   Keystone domain or project administrators. This information is stored in
   the Keystone service database.
  </p><p>
   Another form of external authentication provided by the Keystone service is
   through integration with SAML-based identity providers (IdP) such as Ping
   Identity, IBM Tivoli, and Microsoft Active Directory Federation Server. A
   SAML-based identity provider provides authentication that is often called
   "single sign-on." The IdP server is configured to authenticate against
   identity sources such as Active Directory and provides a single
   authentication API against multiple types of downstream identity sources.
   This means that an organization could have multiple identity storage sources
   but a single authentication source. In addition, if a user has logged into
   one such source during a defined session time frame, that user does not need
   to reauthenticate within the defined session. Instead, the IdP automatically
   validates the user to requesting applications and services.
  </p><p>
   A SAML-based IdP authentication source is configured with Keystone on a
   per-domain basis similar to the manner in which native LDAP directories are
   configured. Extra mapping rules are required in the configuration that
   define which Keystone group an incoming UID is automatically assigned to.
   
   
   This means that groups need to be defined in Keystone first, but it also
   removes the requirement that a domain or project administrator assign user
   roles and project membership on a per-user basis. Instead, groups are used
   to define project membership and roles and incoming users are automatically
   mapped to Keystone groups based on their upstream group membership. This
   strategy provides a consistent role-based access control (RBAC) model based
   on the upstream identity source. The configuration of this option is fairly
   straightforward. IdP vendors such as Ping and IBM are contributing to the
   maintenance of this function and have also produced their own integration
   documentation. HPE is using the Microsoft Active Directory Federation
   Services (AD FS) for functional testing and future documentation.
  </p><p>
   The third Keystone-supported authentication source is known as multi-factor
   authentication (MFA). MFA typically requires an external source of
   authentication beyond a login name and password, and can include options
   such as SMS text, a temporal token generator, or a fingerprint scanner. Each
   of these types of MFAs are usually specific to a particular MFA vendor. The
   Keystone architecture supports an MFA-based authentication system, but this
   has not yet been certified or documented for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-8"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authorization</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-8</li></ul></div></div></div></div><p>
   Another major function provided by the Keystone service is access
   authorization that determines which resources and actions are available
   based on the UserID, the role of the user, and the projects that a user is
   provided access to. All of this information is created, managed, and stored
   by Keystone. These functions are applied through the Horizon web interface,
   the OpenStack command-line interface, or the direct Keystone API.
  </p><p>
   Keystone provides support for organizing users by using three entities:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.6.7.5.4.1"><span class="term ">Domains</span></dt><dd><p>
      Domains provide the highest level of organization. Domains are intended
      to be used as high-level containers for multiple projects. A domain can
      represent different tenants, companies, or organizations for an OpenStack
      cloud deployed for public cloud deployments or it can represent major
      business units, functions, or any other type of top-level organization
      unit in an OpenStack private cloud deployment. Each domain has at least
      one Domain Admin assigned to it. This Domain Admin can then create
      multiple projects within the domain and assign the project administrator
      role to specific project owners. Each domain created in an OpenStack
      deployment is unique and the projects assigned to a domain cannot exist
      in another domain.
     </p></dd><dt id="id-1.6.6.7.5.4.2"><span class="term ">Projects</span></dt><dd><p>
      Projects are entities within a domain that represent groups of users,
      each user role within that project, and how many underlying
      infrastructure resources can be consumed by members of the project.
     </p></dd><dt id="id-1.6.6.7.5.4.3"><span class="term ">Groups</span></dt><dd><p>
      Groups are an optional function and provide the means of assigning
      project roles to multiple users at once.
     </p></dd></dl></div><p>
   Keystone also makes it possible to create and assign roles to groups of
   users or individual users. Role names are created and user assignments are
   made within Keystone. The actual function of a role is defined currently for
   each OpenStack service via scripts. When users request access to an
   OpenStack service, their access tokens contain information about their
   assigned project membership and role for that project. This role is then
   matched to the service-specific script and users are allowed to perform
   functions within that service defined by the role mapping.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Default settings</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-9</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Identity service configuration settings</strong></span>
  </p><p>
   The identity service configuration options are described in the OpenStack
   documentation on the
   <a class="link" href="https://docs.openstack.org/keystone/pike/configuration/index.html" target="_blank">Keystone Configuration Options page</a>
   on the OpenStack site.
  </p><p>
   <span class="bold"><strong>Default domain and service accounts</strong></span>
  </p><p>
   The "default" domain is automatically created during the installation to
   contain the various required OpenStack service accounts, including the
   following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     neutron
    </p></li><li class="listitem "><p>
     glance
    </p></li><li class="listitem "><p>
     swift-monitor
    </p></li><li class="listitem "><p>
     ceilometer
    </p></li><li class="listitem "><p>
     swift
    </p></li><li class="listitem "><p>
     monasca-agent
    </p></li><li class="listitem "><p>
     glance-swift
    </p></li><li class="listitem "><p>
     swift-demo
    </p></li><li class="listitem "><p>
     nova
    </p></li><li class="listitem "><p>
     monasca
    </p></li><li class="listitem "><p>
     logging
    </p></li><li class="listitem "><p>
     demo
    </p></li><li class="listitem "><p>
     heat
    </p></li><li class="listitem "><p>
     cinder
    </p></li><li class="listitem "><p>
     admin
    </p></li></ul></div><p>
   These are required accounts and are used by the underlying OpenStack
   services. These accounts should not be removed or reassigned to a different
   domain. These "default" domain should be used only for these service
   accounts.
  </p><p>
   For details on how to create additional users, see
   <span class="intraxref">Book “User Guide”, Chapter 4 “Cloud Admin Actions with the Command Line”</span>.
  </p></div><div class="sect2" id="id-1.6.6.7.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preinstalled roles</span> <a title="Permalink" class="permalink" href="#id-1.6.6.7.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are the preinstalled roles. You can create additional roles by
   UIDs with the "admin" role. Roles are defined on a per-service basis (more
   information is available at
   <a class="link" href="http://docs.openstack.org/user-guide-admin/manage_projects_users_and_roles.html" target="_blank">Manage
   projects, users, and roles</a> on the OpenStack website).
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Role</th><th>Description</th></tr></thead><tbody><tr><td>admin</td><td>
       <p>
        The "superuser" role. Provides full access to all <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services
        across all domains and projects. This role should be given only to a
        cloud administrator.
       </p>
      </td></tr><tr><td>_member_</td><td>
       <p>
        A general role that enables a user to access resources within an
        assigned project including creating, modifying, and deleting compute,
        storage, and network resources.
       </p>
      </td></tr></tbody></table></div><p>
   You can find additional information on these roles in each service policy
   stored in the <code class="literal">/etc/PROJECT/policy.json</code> files where
   PROJECT is a placeholder for an OpenStack service. For example, the Compute
   (Nova) service roles are stored in the
   <code class="literal">/etc/nova/policy.json</code> file. Each service policy file
   defines the specific API functions available to a role label.
  </p></div></div><div class="sect1" id="admin-password"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the Admin Password</span> <a title="Permalink" class="permalink" href="#admin-password">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-retrieve_adminpassword.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-retrieve_adminpassword.xml</li><li><span class="ds-label">ID: </span>admin-password</li></ul></div></div></div></div><p>
  The admin password will be used to access the dashboard and Operations Console as
  well as allow you to authenticate to use the command-line tools and API.
 </p><p>
  In a default <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> installation there is a randomly generated
  password for the Admin user created. These steps will show you how to
  retrieve this password.
 </p><div class="sect2" id="retrieve"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the Admin Password</span> <a title="Permalink" class="permalink" href="#retrieve">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-retrieve_adminpassword.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-retrieve_adminpassword.xml</li><li><span class="ds-label">ID: </span>retrieve</li></ul></div></div></div></div><p>
   You can retrieve the randomly generated Admin password by using this command
   on the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/service.osrc</pre></div><p>
   In this example output, the value for <code class="literal">OS_PASSWORD</code> is the
   Admin password:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/service.osrc
unset OS_DOMAIN_NAME
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_VERSION=3
export OS_PROJECT_NAME=admin
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USERNAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PASSWORD=SlWSfwxuJY0
export OS_AUTH_URL=https://10.13.111.145:5000/v3
export OS_ENDPOINT_TYPE=internalURL
# OpenstackClient uses OS_INTERFACE instead of OS_ENDPOINT
export OS_INTERFACE=internal
export OS_CACERT=/etc/ssl/certs/ca-certificates.crt
export OS_COMPUTE_API_VERSION=2</pre></div></div></div><div class="sect1" id="servicePasswords"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing Service Passwords</span> <a title="Permalink" class="permalink" href="#servicePasswords">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>servicePasswords</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a process for changing the default service
  passwords, including your admin user password, which you may want to do for
  security or other purposes.
 </p><p>
  You can easily change the inter-service passwords used for authenticating
  communications between services in your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment,
  promoting better compliance with your organization’s security policies.
  The inter-service passwords that can be changed include (but are not limited
  to) Keystone, MariaDB, RabbitMQ, Cloud Lifecycle Manager cluster, Monasca and Barbican.
 </p><p>
  The general process for changing the passwords is to:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Indicate to the configuration processor which password(s) you want to
    change, and optionally include the value of that password
   </p></li><li class="listitem "><p>
    Run the configuration processor to generate the new passwords (you do not
    need to run <code class="command">git add</code> before this)
   </p></li><li class="listitem "><p>
    Run ready-deployment
   </p></li><li class="listitem "><p>
    Check your password name(s) against the tables included below to see which
    high-level credentials-change playbook(s) you need to run
   </p></li><li class="listitem "><p>
    Run the appropriate high-level credentials-change playbook(s)
   </p></li></ul></div><div class="sect2" id="password-strength"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Password Strength</span> <a title="Permalink" class="permalink" href="#password-strength">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>password-strength</li></ul></div></div></div></div><p>
   Encryption passwords supplied to the configuration processor for use with
   Ansible Vault and for encrypting the configuration processor’s persistent
   state must have a minimum length of 12 characters and a maximum of 128
   characters. Passwords must contain characters from each of the following
   three categories:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Uppercase characters (A-Z)
    </p></li><li class="listitem "><p>
     Lowercase characters (a-z)
    </p></li><li class="listitem "><p>
     Base 10 digits (0-9)
    </p></li></ul></div><p>
   Service Passwords that are automatically generated by the configuration
   processor are chosen from the 62 characters made up of the 26 uppercase,
   the 26 lowercase, and the 10 numeric characters, with no preference given
   to any character or set of characters, with the minimum and maximum lengths
   being determined by the specific requirements of individual services.
  </p><div id="id-1.6.6.9.6.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    It is possible to use special characters in passwords. However, the
    <code class="literal">$</code> character must be escaped by entering it twice. For
    example: the password <code class="literal">foo$bar</code> must be specified as <code class="literal">foo$$bar</code>.
   </p></div></div><div class="sect2" id="id-1.6.6.9.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Telling the configuration processor which password(s) you want to change</span> <a title="Permalink" class="permalink" href="#id-1.6.6.9.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the configuration processor will produce metadata about
   each of the passwords (and other variables) that it generates in the file
   <code class="literal">~/openstack/my_cloud/info/private_data_metadata_ccp.yml</code>. A
   snippet of this file follows. Expand the header to see the file:
  </p></div><div class="sect2" id="idg-all-operations-change-service-passwords-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">private_data_metadata_ccp.yml</span> <a title="Permalink" class="permalink" href="#idg-all-operations-change-service-passwords-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-change-service-passwords-xml-7</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">metadata_proxy_shared_secret:
  metadata:
  - clusters:
    - cluster1
    component: nova-metadata
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
mysql_admin_password:
  metadata:
  - clusters:
    - cluster1
    component: ceilometer
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: heat
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: keystone
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    - compute
    component: nova
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: cinder
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: glance
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    - compute
    component: neutron
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: horizon
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
mysql_barbican_password:
  metadata:
  - clusters:
    - cluster1
    component: barbican
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  version: '2.0'</pre></div><p>
   For each variable, there is a metadata entry for each pair of services that
   use the variable including a list of the clusters on which the service
   component that consumes the variable (defined as "component:" in
   <code class="literal">private_data_metadata_ccp.yml</code> above) runs.
  </p><p>
   Note above that the variable <code class="literal">mysql_admin_password</code> is used by a number of
   service components, and the service that is consumed in each case is <code class="literal">mysql</code>,
   which in this context refers to the MariaDB instance that is part of the
   product.
  </p></div><div class="sect2" id="steps-to-change-password"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps to change a password</span> <a title="Permalink" class="permalink" href="#steps-to-change-password">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>steps-to-change-password</li></ul></div></div></div></div><p>
   First, make sure that you have a copy of
   <code class="filename">private_data_metadata_ccp.yml</code>. If you
   do not, generate one to run the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   Make a copy of the <code class="literal">private_data_metadata_ccp.yml</code> file and
   place it into the <code class="literal">~/openstack/change_credentials</code> directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp ~/openstack/my_cloud/info/private_data_metadata_control-plane-1.yml \
 ~/openstack/change_credentials/</pre></div><p>
   Edit the copied file in <code class="literal">~/openstack/change_credentials</code>
   leaving only those passwords you intend to change. All entries in this
   template file should be deleted <span class="emphasis"><em>except for those
   passwords</em></span>.
  </p><div id="id-1.6.6.9.9.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If you leave other passwords in that file that you do
    <span class="emphasis"><em>not</em></span> want to change, they will be regenerated and no
    longer match those in use which could disrupt operations.
   </p></div><div id="id-1.6.6.9.9.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    It is required that you change passwords in batches of each category
    listed below.
   </p></div><p>
   For example, the snippet below would result in the configuration processor
   generating new random values for keystone_backup_password,
   keystone_ceilometer_password, and keystone_cinder_password:
  </p><div class="verbatim-wrap"><pre class="screen">keystone_backup_password:
  metadata:
  - clusters:
    - cluster0
    - cluster1
    - compute
    component: freezer-agent
    consumes: keystone-api
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
keystone_ceilometer_password:
  metadata:
  - clusters:
    - cluster1
    component: ceilometer-common
    consumes: keystone-api
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
keystone_cinder_password:
  metadata:
  - clusters:
    - cluster1
    component: cinder-api
    consumes: keystone-api
    consuming-cp: ccp
    cp: ccp
  version: '2.0'</pre></div></div><div class="sect2" id="specifying-password-value"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying password value</span> <a title="Permalink" class="permalink" href="#specifying-password-value">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>specifying-password-value</li></ul></div></div></div></div><p>
   Optionally, you can specify a value for the password by including a "value:"
   key and value at the same level as metadata:
  </p><div class="verbatim-wrap"><pre class="screen">keystone_backup_password:
    value: 'new_password'
    metadata:
    - clusters:
        - cluster0
        - cluster1
        - compute
        component: freezer-agent
        consumes: keystone-api
        consuming-cp: ccp
        cp: ccp
      version: '2.0'</pre></div><p>
   Note that you can have multiple files in openstack/change_credentials. The
   configuration processor will only read files that end in .yml or .yaml.
  </p><div id="id-1.6.6.9.10.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you have specified a password value in your credential change file, you
    may want to encrypt it using ansible-vault. If you decide to encrypt with
    ansible-vault, make sure that you use the encryption key you have already
    used when running the configuration processor.
   </p></div><p>
   To encrypt a file using ansible-vault, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/change_credentials
<code class="prompt user">ardana &gt; </code>ansible-vault encrypt <em class="replaceable ">credential change file ending in .yml or .yaml</em></pre></div><p>
   Be sure to provide the encryption key when prompted. Note that if you have
   specified the wrong ansible-vault password, the configuration-processor will
   error out with a message like the following:
  </p><div class="verbatim-wrap"><pre class="screen">################################################## Reading Persistent State ##################################################

################################################################################
# The configuration processor failed.
# PersistentStateCreds: User-supplied creds file test1.yml was not parsed properly
################################################################################</pre></div></div><div class="sect2" id="id-1.6.6.9.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the configuration processor to change passwords</span> <a title="Permalink" class="permalink" href="#id-1.6.6.9.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The directory openstack/change_credentials is not managed by git, so to rerun
   the configuration processor to generate new passwords and prepare for the
   next deployment just enter the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><div id="id-1.6.6.9.11.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    The files that you placed in
    <code class="filename">~/openstack/change_credentials</code> should be removed
    once you have run the configuration processor because the old password
    values and new password values will be stored in the configuration
    processor's persistent state.
   </p></div><p>
   Note that if you see output like the following after running the
   configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">################################################################################
# The configuration processor completed with warnings.
# PersistentStateCreds: User-supplied password name 'blah' is not valid
################################################################################</pre></div><p>
   this tells you that the password name you have supplied, 'blah,' does not
   exist. A failure to correctly parse the credentials change file will result
   in the configuration processor erroring out with a message like the
   following:
  </p><div class="verbatim-wrap"><pre class="screen">################################################## Reading Persistent State ##################################################

################################################################################
# The configuration processor failed.
# PersistentStateCreds: User-supplied creds file test1.yml was not parsed properly
################################################################################</pre></div><p>
   Once you have run the configuration processor to change passwords, an
   information file
   <code class="literal">~/openstack/my_cloud/info/password_change.yml</code> similar to the
   <code class="literal">private_data_metadata_ccp.yml</code> is written to tell you which
   passwords have been changed, including metadata but not including the
   values.
  </p></div><div class="sect2" id="id-1.6.6.9.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Password change playbooks and tables</span> <a title="Permalink" class="permalink" href="#id-1.6.6.9.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once you have completed the steps above to change password(s) value(s) and
   then prepare for the deployment that will actually switch over to the new
   passwords, you will need to run some high-level playbooks. The passwords
   that can be changed are grouped into six categories. The tables below list
   the password names that belong in each category. The categories are:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.6.9.12.3.1"><span class="term ">Keystone</span></dt><dd><p>
      Playbook: ardana-keystone-credentials-change.yml
     </p></dd><dt id="id-1.6.6.9.12.3.2"><span class="term ">RabbitMQ</span></dt><dd><p>
      Playbook: ardana-rabbitmq-credentials-change.yml
     </p></dd><dt id="id-1.6.6.9.12.3.3"><span class="term ">MariaDB</span></dt><dd><p>
      Playbook: ardana-reconfigure.yml
     </p></dd><dt id="id-1.6.6.9.12.3.4"><span class="term ">Cluster:</span></dt><dd><p>
      Playbook: ardana-cluster-credentials-change.yml
     </p></dd><dt id="id-1.6.6.9.12.3.5"><span class="term ">Monasca:</span></dt><dd><p>
      Playbook: monasca-reconfigure-credentials-change.yml
     </p></dd><dt id="id-1.6.6.9.12.3.6"><span class="term ">Other:</span></dt><dd><p>
      Playbook: ardana-other-credentials-change.yml
     </p></dd></dl></div><p>
   It is recommended that you change passwords in batches; in other words, run
   through a complete password change process for each batch of passwords,
   preferably in the above order. Once you have followed the process indicated
   above to change password(s), check the names against the tables below to see
   which password change playbook(s) you should run.
  </p><p>
   <span class="bold"><strong>Changing identity service credentials</strong></span>
  </p><p>
   The following table lists identity service credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>Keystone credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
barbican_admin_password
barbican_service_password
keystone_admin_pwd
keystone_admin_token
keystone_backup_password
keystone_ceilometer_password
keystone_cinder_password
keystone_cinderinternal_password
keystone_demo_pwd
keystone_designate_password
keystone_freezer_password
keystone_glance_password
keystone_glance_swift_password
keystone_heat_password
keystone_magnum_password
keystone_monasca_agent_password
keystone_monasca_password
keystone_neutron_password
keystone_nova_password
keystone_octavia_password
keystone_swift_dispersion_password
keystone_swift_monitor_password
keystone_swift_password
logging_keystone_password
nova_monasca_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change Keystone credentials is
   <code class="literal">ardana-keystone-credentials-change.yml</code>. Execute the
   following commands to make the changes:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-keystone-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing RabbitMQ credentials</strong></span>
  </p><p>
   The following table lists the RabbitMQ credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>RabbitMQ credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
ops_mon_rmq_password
rmq_barbican_password
rmq_ceilometer_password
rmq_cinder_password
rmq_designate_password
rmq_keystone_password
rmq_magnum_password
rmq_monasca_monitor_password
rmq_nova_password
rmq_octavia_password
rmq_service_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change RabbitMQ credentials is
   <code class="literal">ardana-rabbitmq-credentials-change.yml</code>. Execute the
   following commands to make the changes:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-rabbitmq-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing MariaDB credentials</strong></span>
  </p><p>
   The following table lists the MariaDB credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>MariaDB credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
mysql_admin_password
mysql_barbican_password
mysql_clustercheck_pwd
mysql_designate_password
mysql_magnum_password
mysql_monasca_api_password
mysql_monasca_notifier_password
mysql_monasca_thresh_password
mysql_octavia_password
mysql_powerdns_password
mysql_root_pwd
mysql_service_pwd
mysql_sst_password
ops_mon_mdb_password
mysql_monasca_transform_password
mysql_nova_api_password
password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change MariaDB credentials is
   <code class="literal">ardana-reconfigure.yml</code>. To make the changes, execute the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div><p>
   <span class="bold"><strong>Changing cluster credentials</strong></span>
  </p><p>
   The following table lists the cluster credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>cluster credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
haproxy_stats_password
keepalive_vrrp_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change cluster credentials is
   <code class="literal">ardana-cluster-credentials-change.yml</code>. To make changes,
   execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cluster-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing Monasca credentials</strong></span>
  </p><p>
   The following table lists the Monasca credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>Monasca credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
mysql_monasca_api_password
mysql_monasca_persister_password
monitor_user_password
cassandra_monasca_api_password
cassandra_monasca_persister_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change Monasca credentials is
   <code class="literal">monasca-reconfigure-credentials-change.yml</code>. To make the
   changes, execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing other credentials</strong></span>
  </p><p>
   The following table lists the other credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>Other credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
logging_beaver_password
logging_api_password
logging_monitor_password
logging_kibana_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change these credentials is
   <code class="literal">ardana-other-credentials-change.yml</code>. To make the changes,
   execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-other-credentials-change.yml</pre></div></div><div class="sect2" id="changing-keystone-credentials-for-rgw"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing RADOS Gateway Credential</span> <a title="Permalink" class="permalink" href="#changing-keystone-credentials-for-rgw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>changing-keystone-credentials-for-rgw</li></ul></div></div></div></div><p>
   To change the keystone credentials of RADOS Gateway, follow the preceding
   steps documented in <a class="xref" href="#servicePasswords" title="4.7. Changing Service Passwords">Section 4.7, “Changing Service Passwords”</a> by modifying the
   <code class="literal">keystone_rgw_password</code> section in
   <code class="literal">private_data_metadata_ccp.yml</code> file in
   <a class="xref" href="#steps-to-change-password" title="4.7.4. Steps to change a password">Section 4.7.4, “Steps to change a password”</a> or
   <a class="xref" href="#specifying-password-value" title="4.7.5. Specifying password value">Section 4.7.5, “Specifying password value”</a>.
  </p></div><div class="sect2" id="id-1.6.6.9.14"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.7.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Immutable variables</span> <a title="Permalink" class="permalink" href="#id-1.6.6.9.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The values of certain variables are immutable, which means that once they have
   been generated by the configuration processor they cannot be changed. These
   variables are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     barbican_master_kek_db_plugin
    </p></li><li class="listitem "><p>
     swift_hash_path_suffix
    </p></li><li class="listitem "><p>
     swift_hash_path_prefix
    </p></li><li class="listitem "><p>
     mysql_cluster_name
    </p></li><li class="listitem "><p>
     heartbeat_key
    </p></li><li class="listitem "><p>
     erlang_cookie
    </p></li></ul></div><p>
   The configuration processor will not re-generate the values of the above
   passwords, nor will it allow you to specify a value for them. In addition to
   the above variables, the following are immutable in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     All ssh keys generated by the configuration processor
    </p></li><li class="listitem "><p>
     All UUIDs generated by the configuration processor
    </p></li><li class="listitem "><p>
     metadata_proxy_shared_secret
    </p></li><li class="listitem "><p>
     horizon_secret_key
    </p></li><li class="listitem "><p>
     ceilometer_metering_secret
    </p></li></ul></div></div></div><div class="sect1" id="topic-m43-2j3-bt"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring the Identity Service</span> <a title="Permalink" class="permalink" href="#topic-m43-2j3-bt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>topic-m43-2j3-bt</li></ul></div></div></div></div><div class="sect2" id="id-1.6.6.10.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the Keystone Identity Service</span> <a title="Permalink" class="permalink" href="#id-1.6.6.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This topic explains configuration options for the Identity service.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> lets you perform updates on the following parts of the Identity
   service configuration:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Any content in the main keystone configuration file:
     <code class="filename">/etc/keystone/keystone.conf</code>. This lets you
     manipulate Keystone configuration parameters. Next, continue with
     <a class="xref" href="#idg-all-operations-configuring-identity-identity-reconfigure-xml-6" title="4.8.2. Updating the Main Identity Service Configuration File">Section 4.8.2, “Updating the Main Identity Service Configuration File”</a>.
    </p></li><li class="listitem "><p>
     Updating certain configuration options and enabling features, such as:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Verbosity of logs being written to Keystone log files.
      </p></li><li class="listitem "><p>
       Process counts for the Apache2 WSGI module, separately for admin and
       public Keystone interfaces.
      </p></li><li class="listitem "><p>
       Enabling/disabling auditing.
      </p></li><li class="listitem "><p>
       Enabling/disabling Fernet tokens.
      </p></li></ul></div><p>
     For more information, see <a class="xref" href="#enable-features" title="4.8.3. Enabling Identity Service Features">Section 4.8.3, “Enabling Identity Service Features”</a>.
    </p></li><li class="listitem "><p>
     Creating and updating domain-specific configuration files:
     <span class="bold"><strong>/etc/keystone/domains/keystone.&lt;domain_name&gt;.conf</strong></span>.
     This lets you integrate Keystone with one or more external authentication
     sources, such as LDAP server. See the topic on
     <a class="xref" href="#ldap" title="4.9. Integrating LDAP with the Identity Service">Section 4.9, “Integrating LDAP with the Identity Service”</a>.
    </p></li></ul></div></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-reconfigure-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the Main Identity Service Configuration File</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-reconfigure-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-reconfigure-xml-6</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The main Keystone Identity service configuration file
     (<span class="bold"><strong>/etc/keystone/keystone.conf</strong></span>), located on
     each control plane server, is generated from the following template file
     located on a Cloud Lifecycle Manager:
     <code class="literal">~/openstack/my_cloud/config/keystone/keystone.conf.j2</code>
    </p><p>
     Modify this template file as appropriate. See
     <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html" target="_blank">Keystone
     Liberty documentation</a> for full descriptions of all settings. This
     is a Jinja2 template, which expects certain template variables to be set.
     Do not change values inside double curly braces: <code class="literal">{{ }}</code>.
    </p><div id="id-1.6.6.10.3.2.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> has the following token expiration setting, which
       differs from the upstream value <code class="literal">3600</code>:
      </p><div class="verbatim-wrap"><pre class="screen">[token]
expiration = 14400</pre></div></div></li><li class="listitem "><p>
     After you modify the template, commit the change to the local git
     repository, and rerun the configuration processor / deployment area
     preparation playbooks (as suggested in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone.conf.j2
<code class="prompt user">ardana &gt; </code>git commit -m "Adjusting some parameters in keystone.conf"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in the deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="enable-features"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Identity Service Features</span> <a title="Permalink" class="permalink" href="#enable-features">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>enable-features</li></ul></div></div></div></div><p>
   To enable or disable Keystone features, do the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Adjust respective parameters in
     <span class="bold"><strong>~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</strong></span>
    </p></li><li class="listitem "><p>
     Commit the change into local git repository, and rerun the configuration
     processor/deployment area preparation playbooks (as suggested in
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone_deploy_config.yml
<code class="prompt user">ardana &gt; </code>git commit -m "Adjusting some WSGI or logging parameters for keystone"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in the deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="fernet-tokens"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Fernet Tokens</span> <a title="Permalink" class="permalink" href="#fernet-tokens">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>fernet-tokens</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports Fernet tokens by default. The benefit of using Fernet
   tokens is that tokens are not persisted in a database, which is helpful if
   you want to deploy the Keystone Identity service as one master and multiple
   slaves; only roles, projects, and other details will need to be replicated
   from master to slaves, not the token table.
  </p><div id="id-1.6.6.10.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Tempest does not work with Fernet tokens in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. If Fernet
    tokens are enabled, do not run token tests in Tempest.
   </p></div><div id="id-1.6.6.10.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    During reconfiguration when switching to a Fernet token provider or during
    Fernet key rotation, you may see a warning in
    <code class="literal">keystone.log</code> stating <code class="literal">[fernet_tokens]
    key_repository is world readable: /etc/keystone/fernet-keys/</code>.
    This is expected. You can safely ignore this message. For other Keystone
    operations, you will not see this warning. Directory permissions are
    actually set to 600 (read/write by owner only), not world readable.
   </p></div><p>
   Fernet token-signing key rotation is being handled by a cron job, which is
   configured on one of the controllers. The controller with the Fernet
   token-signing key rotation cron job is also known as the Fernet Master node.
   By default, the Fernet token-signing key is being rotated once every 24
   hours. The Fernet token-signing keys are distributed from the Fernet Master
   node to the rest of the controllers at each rotation. Therefore, the Fernet
    token-signing keys are consistent for all the controlers at all time.
  </p><p>
   When enabling Fernet token provider the first time, specific steps are
   needed to set up the necessary mechanisms for Fernet token-signing key
   distributions.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Set <code class="literal">keystone_configure_fernet</code> to
     <code class="literal">True</code> in
     <code class="filename">~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</code>.
    </p></li><li class="step "><p>
     Run the following commands to commit your change in Git and enable Fernet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone_deploy_config.yml
<code class="prompt user">ardana &gt; </code>git commit -m "enable Fernet token provider"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-deploy.yml</pre></div></li></ol></div></div><p>
   When the Fernet token provider is enabled, a Fernet Master alarm definition
   is also created on Monasca to monitor the Fernet Master node. If the Fernet
   Master node is offline or unreachable, a <code class="literal">CRITICAL</code> alarm
   will be raised for the Cloud Admin to take corrective actions. If the Fernet
   Master node is offline for a prolonged period of time, Fernet token-signing
   key rotation will not be performed. This may introduce security risks to the
   cloud. The Cloud Admin must take immediate actions to resurrect the Fernet
   Master node.
  </p></div></div><div class="sect1" id="ldap"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating LDAP with the Identity Service</span> <a title="Permalink" class="permalink" href="#ldap">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span>ldap</li></ul></div></div></div></div><div class="sect2" id="id-1.6.6.11.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with an external LDAP server</span> <a title="Permalink" class="permalink" href="#id-1.6.6.11.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The Keystone identity service provides two primary functions: user
   authentication and access authorization. The user authentication function
   validates a user's identity. Keystone has a very basic user management
   system that can be used to create and manage user login and password
   credentials but this system is intended only for proof of concept
   deployments due to the very limited password control functions. The internal
   identity service user management system is also commonly used to store and
   authenticate OpenStack-specific service account information.
  </p><p>
   The recommended source of authentication is external user management systems
   such as LDAP directory services. The identity service can be configured to
   connect to and use external systems as the source of user authentication.
   The identity service domain construct is used to define different
   authentication sources based on domain membership. For example, cloud
   deployment could consist of as few as two domains:
  </p><div class="itemizedlist " id="ul-msq-q3h-4v"><ul class="itemizedlist"><li class="listitem "><p>
     The default domain that is pre-configured for the service account users
     that are authenticated directly against the identity service internal user
     management system
    </p></li><li class="listitem "><p>
     A customer-defined domain that contains all user projects and membership
     definitions. This domain can then be configured to use an external LDAP
     directory such as Microsoft Active Directory as the authentication source.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can support multiple domains for deployments that support multiple
   tenants. Multiple domains can be created with each domain configured to
   either the same or different external authentication sources. This
   deployment model is known as a "per-domain" model.
  </p><p>
   There are currently two ways to configure "per-domain" authentication
   sources:
  </p><div class="itemizedlist " id="ul-nsq-q3h-4v"><ul class="itemizedlist"><li class="listitem "><p>
     File store – each domain configuration is created and stored in separate
     text files. This is the older and current default method for defining
     domain configurations.
    </p></li><li class="listitem "><p>
     Database store – each domain configuration can be created using either
     the identity service manager utility (recommenced) or a
     <a class="link" href="http://developer.openstack.org/api-ref-identity-v3.html#domains-config-v3" target="_blank">Domain
     Admin API</a> (from OpenStack.org), and the results are stored in the
     identity service MariaDB database. This database store is a new method
     introduced in the OpenStack Kilo release and now available in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li></ul></div><p>
   Instructions for initially creating per-domain configuration files and then
   migrating to the Database store method via the identity service manager
   utility are provided as follows.
  </p></div><div class="sect2" id="filestore"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up domain-specific driver configuration - file store</span> <a title="Permalink" class="permalink" href="#filestore">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span>filestore</li></ul></div></div></div></div><p>
   To update configuration to a specific LDAP domain:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that the following configuration options are in the main
     configuration file template:
     ~/openstack/my_cloud/config/keystone/keystone.conf.j2
    </p><div class="verbatim-wrap"><pre class="screen">[identity]
domain_specific_drivers_enabled = True
domain_configurations_from_database = False</pre></div></li><li class="listitem "><p>
     Create a YAML file that contains the definition of the LDAP server
     connection. The sample file below is already provided as part of the
     Cloud Lifecycle Manager in the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>. It is available on
     the Cloud Lifecycle Manager in the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_sample.yml</pre></div><p>
     Save a copy of this file with a new name, for example:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div><div id="id-1.6.6.11.3.3.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Please refer to the LDAP section of the
      <a class="link" href="https://github.com/openstack/keystone/blob/stable/pike/etc/keystone.conf.sample" target="_blank">Keystone</a>
      configuration example for OpenStack for the full option list and
      description.
     </p></div><p>
     Below are samples of YAML configurations for identity service LDAP
     certificate settings, optimized for Microsoft Active Directory server.
    </p><p>
     Sample YAML configuration keystone_configure_ldap_my.yml
    </p><div class="verbatim-wrap"><pre class="screen">---
keystone_domainldap_conf:

    # CA certificates file content.
    # Certificates are stored in Base64 PEM format. This may be entire LDAP server
    # certificate (in case of self-signed certificates), certificate of authority
    # which issued LDAP server certificate, or a full certificate chain (Root CA
    # certificate, intermediate CA certificate(s), issuer certificate).
    #
    cert_settings:
      cacert: |
        -----BEGIN CERTIFICATE-----

        certificate appears here

        -----END CERTIFICATE-----

    # A domain will be created in MariaDB with this name, and associated with ldap back end.
    # Installer will also generate a config file named /etc/keystone/domains/keystone.&lt;domain_name&gt;.conf
    #
    domain_settings:
      name: ad
      description: Dedicated domain for ad users

    conf_settings:
      identity:
         driver: ldap


      # For a full list and description of ldap configuration options, please refer to
      # https://github.com/openstack/keystone/blob/master/etc/keystone.conf.sample or
      # http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html.
      #
      # Please note:
      #  1. LDAP configuration is read-only. Configuration which performs write operations (i.e. creates users, groups, etc)
      #     is not supported at the moment.
      #  2. LDAP is only supported for identity operations (reading users and groups from LDAP). Assignment
      #     operations with LDAP (i.e. managing roles, projects) are not supported.
      #  3. LDAP is configured as non-default domain. Configuring LDAP as a default domain is not supported.
      #
      ldap:
        url: ldap://ad.hpe.net
        suffix: DC=hpe,DC=net
        query_scope: sub
        user_tree_dn: CN=Users,DC=hpe,DC=net
        user : CN=admin,CN=Users,DC=hpe,DC=net
        password: REDACTED
        user_objectclass: user
        user_id_attribute: cn
        user_name_attribute: cn
        group_tree_dn: CN=Users,DC=hpe,DC=net
        group_objectclass: group
        group_id_attribute: cn
        group_name_attribute: cn
        use_pool: True
        user_enabled_attribute: userAccountControl
        user_enabled_mask: 2
        user_enabled_default: 512
        use_tls: True
        tls_req_cert: demand
        # if you are configuring multiple LDAP domains, and LDAP server certificates are issued
        # by different authorities, make sure that you place certs for all the LDAP backend domains in the
        # cacert parameter as seen in this sample yml file so that all the certs are combined in a single CA file
        # and every LDAP domain configuration points to the combined CA file.
        # Note:
        # 1. Please be advised that every time a new ldap domain is configured, the single CA file gets overwritten
        # and hence ensure that you place certs for all the LDAP backend domains in the cacert parameter.
        # 2. There is a known issue on one cert per CA file per domain when the system processes
        # concurrent requests to multiple LDAP domains. Using the single CA file with all certs combined
        # shall get the system working properly*.

        tls_cacertfile: /etc/keystone/ssl/certs/all_ldapdomains_ca.pem

        # The issue is in the underlying SSL library. Upstream is not investing in python-ldap package anymore.
        # It is also not python3 compliant.</pre></div><div class="verbatim-wrap"><pre class="screen">keystone_domain_MSAD_conf:

    # CA certificates file content.
    # Certificates are stored in Base64 PEM format. This may be entire LDAP server
    # certificate (in case of self-signed certificates), certificate of authority
    # which issued LDAP server certificate, or a full certificate chain (Root CA
    # certificate, intermediate CA certificate(s), issuer certificate).
    #
    cert_settings:
      cacert: |
        -----BEGIN CERTIFICATE-----

        certificate appears here

        -----END CERTIFICATE-----

    # A domain will be created in MariaDB with this name, and associated with ldap back end.
    # Installer will also generate a config file named /etc/keystone/domains/keystone.&lt;domain_name&gt;.conf
    #
        domain_settings:
          name: msad
          description: Dedicated domain for msad users

        conf_settings:
          identity:
            driver: ldap

    # For a full list and description of ldap configuration options, please refer to
    # https://github.com/openstack/keystone/blob/master/etc/keystone.conf.sample or
    # http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html.
    #
    # Please note:
    #  1. LDAP configuration is read-only. Configuration which performs write operations (i.e. creates users, groups, etc)
    #     is not supported at the moment.
    #  2. LDAP is only supported for identity operations (reading users and groups from LDAP). Assignment
    #     operations with LDAP (i.e. managing roles, projects) are not supported.
    #  3. LDAP is configured as non-default domain. Configuring LDAP as a default domain is not supported.
    #
    ldap:
      # If the url parameter is set to ldap then typically use_tls should be set to True. If
      # url is set to ldaps, then use_tls should be set to False
      url: ldaps://10.16.22.5
      use_tls: False
      query_scope: sub
      user_tree_dn: DC=l3,DC=local
      # this is the user and password for the account that has access to the AD server
      user: administrator@l3.local
      password: OpenStack123
      user_objectclass: user
      # For a default Active Directory schema this is where to find the user name, openldap uses a different value
      user_id_attribute: userPrincipalName
      user_name_attribute: sAMAccountName
      group_tree_dn: DC=l3,DC=local
      group_objectclass: group
      group_id_attribute: cn
      group_name_attribute: cn
      # An upstream defect requires use_pool to be set false
      use_pool: False
      user_enabled_attribute: userAccountControl
      user_enabled_mask: 2
      user_enabled_default: 512
      tls_req_cert: allow
      # Referals may contain urls that can't be resolved and will cause timeouts, ignore them
      chase_referrals: False
      # if you are configuring multiple LDAP domains, and LDAP server certificates are issued
      # by different authorities, make sure that you place certs for all the LDAP backend domains in the
      # cacert parameter as seen in this sample yml file so that all the certs are combined in a single CA file
      # and every LDAP domain configuration points to the combined CA file.
      # Note:
      # 1. Please be advised that every time a new ldap domain is configured, the single CA file gets overwritten
      # and hence ensure that you place certs for all the LDAP backend domains in the cacert parameter.
      # 2. There is a known issue on one cert per CA file per domain when the system processes
      # concurrent requests to multiple LDAP domains. Using the single CA file with all certs combined
      # shall get the system working properly.

      tls_cacertfile: /etc/keystone/ssl/certs/all_ldapdomains_ca.pem</pre></div></li><li class="listitem "><p>
     As suggested in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, commit the new file to the
     local git repository, and rerun the configuration processor and ready
     deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone_configure_ldap_my.yml
<code class="prompt user">ardana &gt; </code>git commit -m "Adding LDAP server integration config"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in a deployment area, passing the YAML
     file created in the previous step as a command-line option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@~/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div></li><li class="listitem "><p>
     Follow these same steps for each LDAP domain with which you are
     integrating the identity service, creating a YAML file for each and
     running the reconfigure playbook once for each additional domain.
    </p></li><li class="listitem "><p>
     Ensure that a new domain was created for LDAP (Microsoft AD in this
     example) and set environment variables for admin level access
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source keystone.osrc</pre></div><p>
     Get a list of domains
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack domain list</pre></div><p>
     As output here:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------+---------+----------------------------------------------------------------------+
| ID                               | Name    | Enabled | Description                                                          |
+----------------------------------+---------+---------+----------------------------------------------------------------------+
| 6740dbf7465a4108a36d6476fc967dbd | heat    | True    | Owns users and projects created by heat                              |
| default                          | Default | True    | Owns users and tenants (i.e. projects) available on Identity API v2. |
| b2aac984a52e49259a2bbf74b7c4108b | ad      | True    | Dedicated domain for users managed by Microsoft AD server            |
+----------------------------------+---------+---------+----------------------------------------------------------------------+</pre></div><div id="id-1.6.6.11.3.3.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      LDAP domain is read-only. This means that you cannot create new user or
      group records in it.
     </p></div></li><li class="listitem "><p>
     Once the LDAP user is granted the appropriate role, he can authenticate
     within the specified domain. Set environment variables for admin-level
     access
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source keystone.osrc</pre></div><p>
     Get user record within the ad (Active Directory) domain
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user show testuser1 --domain ad</pre></div><p>
     Note the output:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| id        | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
| name      | testuser1                                                        |
+-----------+------------------------------------------------------------------+</pre></div><p>
     Now, get list of LDAP groups:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack group list --domain ad</pre></div><p>
     Here you see testgroup1 and testgroup2:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
|  ID                                                              | Name       |
+------------------------------------------------------------------+------------+
|  03976b0ea6f54a8e4c0032e8f756ad581f26915c7e77500c8d4aaf0e83afcdc6| testgroup1 |
7ba52ee1c5829d9837d740c08dffa07ad118ea1db2d70e0dc7fa7853e0b79fcf   | testgroup2 |
+------------------------------------------------------------------+------------+</pre></div><p>
     Create a new role. Note that the role is not bound to the domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role create testrole1</pre></div><p>
     Testrole1 has been created:
    </p><div class="verbatim-wrap"><pre class="screen">+-------+----------------------------------+
| Field | Value                            |
+-------+----------------------------------+
| id    | 02251585319d459ab847409dea527dee |
| name  | testrole1                        |
+-------+----------------------------------+</pre></div><p>
     Grant the user a role within the domain by executing the code below. Note
     that due to a current OpenStack CLI limitation, you must use the user ID
     rather than the user name when working with a non-default domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add testrole1 --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --domain ad</pre></div><p>
     Verify that the role was successfully granted, as shown here:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role assignment list --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --domain ad
+----------------------------------+------------------------------------------------------------------+-------+---------+----------------------------------+
| Role                             | User                                                             | Group | Project | Domain                           |
+----------------------------------+------------------------------------------------------------------+-------+---------+----------------------------------+
| 02251585319d459ab847409dea527dee | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |       |         | 143af847018c4dc7bd35390402395886 |
+----------------------------------+------------------------------------------------------------------+-------+---------+----------------------------------+</pre></div><p>
     Authenticate (get a domain-scoped token) as a new user with a new role.
     The --os-* command-line parameters specified below override the respective
     OS_* environment variables set by the keystone.osrc script to provide
     admin access. To ensure that the command below is executed in a clean
     environment, you may want log out from the node and log in again.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack --os-identity-api-version 3 \
            --os-username testuser1 \
            --os-password testuser1_password \
            --os-auth-url http://10.0.0.6:35357/v3 \
            --os-domain-name ad \
            --os-user-domain-name ad \
            token issue</pre></div><p>
     Here is the result:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| expires   | 2015-09-09T21:36:15.306561Z                                      |
| id        | 6f8f9f1a932a4d01b7ad9ab061eb0917                                 |
| user_id   | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
+-----------+------------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     Users can also have a project within the domain and get a project-scoped
     token. To accomplish this, set environment variables for admin level
     access:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source keystone.osrc</pre></div><p>
     Then create a new project within the domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack project create testproject1 --domain ad</pre></div><p>
     The result shows that they have been created:
    </p><div class="verbatim-wrap"><pre class="screen">+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description |                                  |
| domain_id   | 143af847018c4dc7bd35390402395886 |
| enabled     | True                             |
| id          | d065394842d34abd87167ab12759f107 |
| name        | testproject1                     |
+-------------+----------------------------------+</pre></div><p>
     Grant the user a role with a project, re-using the role created in the
     previous example. Note that due to a current OpenStack CLI limitation, you
     must use user ID rather than user name when working with a non-default
     domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add testrole1 --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --project testproject1</pre></div><p>
     Verify that the role was successfully granted by generating a list:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role assignment list --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --project testproject1</pre></div><p>
     The output shows the result:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+------------------------------------------------------------------+-------+----------------------------------+--------+
| Role                             | User                                                             | Group | Project                          | Domain |
+----------------------------------+------------------------------------------------------------------+-------+----------------------------------+--------+
| 02251585319d459ab847409dea527dee | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |       | d065394842d34abd87167ab12759f107 |        |
+----------------------------------+------------------------------------------------------------------+-------+----------------------------------+--------+</pre></div><p>
     Authenticate (get a project-scoped token) as the new user with a new role.
     The --os-* command line parameters specified below override their
     respective OS_* environment variables set by keystone.osrc to provide
     admin access. To ensure that the command below is executed in a clean
     environment, you may want log out from the node and log in again. Note
     that both the --os-project-domain-name and --os-project-user-name
     parameters are needed to verify that both user and project are not in the
     default domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack --os-identity-api-version 3 \
            --os-username testuser1 \
            --os-password testuser1_password \
            --os-auth-url http://10.0.0.6:35357/v3 \
            --os-project-name testproject1 \
            --os-project-domain-name ad \
            --os-user-domain-name ad \
            token issue</pre></div><p>
     Below is the result:
    </p><div class="verbatim-wrap"><pre class="screen">+------------+------------------------------------------------------------------+
| Field      | Value                                                            |
+------------+------------------------------------------------------------------+
| expires    | 2015-09-09T21:50:49.945893Z                                      |
| id         | 328e18486f69441fb13f4842423f52d1                                 |
| project_id | d065394842d34abd87167ab12759f107                                 |
| user_id    | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
+------------+------------------------------------------------------------------+</pre></div></li></ol></div></div><div class="sect2" id="id-1.6.6.11.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up or switch to domain-specific driver configuration using a database store</span> <a title="Permalink" class="permalink" href="#id-1.6.6.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To make the switch, execute the steps below. Remember, you must have already
   set up the configuration for a file store as explained in
   <a class="xref" href="#filestore" title="4.9.2. Set up domain-specific driver configuration - file store">Section 4.9.2, “Set up domain-specific driver configuration - file store”</a>, and it must be working properly.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that the following configuration options are set in the main
     configuration file,
     ~/openstack/my_cloud/config/keystone/keystone.conf.j2:
    </p><div class="verbatim-wrap"><pre class="screen">[identity]
domain_specific_drivers_enabled = True
domain_configurations_from_database = True

[domain_config]
driver = sql</pre></div></li><li class="listitem "><p>
     Once the template is modified, commit the change to the local git
     repository, and rerun the configuration processor / deployment area
     preparation playbooks (as suggested at Using Git for Configuration
     Management):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A</pre></div><p>
     Verify that the files have been added using git status:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div><p>
     Then commit the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Use Domain-Specific Driver Configuration - Database Store: more description here..."</pre></div><p>
     Next, run the configuration processor and ready deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in a deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Upload the domain-specific config files to the database if they have not
     been loaded. If they have already been loaded and you want to switch back
     to database store mode, then skip this upload step and move on to step 5.
    </p><div class="orderedlist " id="ol-tff-px4-mv"><ol class="orderedlist" type="a"><li class="listitem "><p>
       Go to one of the controller nodes where Keystone is deployed.
      </p></li><li class="listitem "><p>
       Verify that domain-specific driver configuration files are located under
       the directory (default /etc/keystone/domains) with the format:
       keystone.&lt;domain name&gt;.conf Use the Keystone manager utility to
       load domain-specific config files to the database. There are two options
       for uploading the files:
      </p><div class="orderedlist " id="ol-uff-px4-mv"><ol class="orderedlist" type="i"><li class="listitem "><p>
         Option 1: Upload all configuration files to the SQL database:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>keystone-manage domain_config_upload --all</pre></div></li><li class="listitem "><p>
         Option 2: Upload individual domain-specific configuration files by
         specifying the domain name one by one:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>keystone-manage domain_config_upload --domain-name <em class="replaceable ">domain name</em></pre></div><p>
         Here is an example:
        </p><div class="verbatim-wrap"><pre class="screen">keystone-manage domain_config_upload --domain-name ad</pre></div><p>
         Note that the Keystone manager utility does not upload the
         domain-specific driver configuration file the second time for the same
         domain. For the management of the domain-specific driver configuration
         in the database store, you may refer to
         <a class="link" href="http://developer.openstack.org/api-ref-identity-v3.html#domains-config-v3" target="_blank">OpenStack
         Identity API - Domain Configuration</a>.
        </p></li></ol></div></li></ol></div></li><li class="listitem "><p>
     Verify that the switched domain driver configuration for LDAP (Microsoft
     AD in this example) in the database store works properly. Then set the
     environment variables for admin level access:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc</pre></div><p>
     Get a list of domain users:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user list --domain ad</pre></div><p>
     Note the three users returned:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| e7dbec51ecaf07906bd743debcb49157a0e8af557b860a7c1dadd454bdab03fe | testuser1  |
| 8a09630fde3180c685e0cd663427e8638151b534a8a7ccebfcf244751d6f09bd | testuser2  |
| ea463d778dadcefdcfd5b532ee122a70dce7e790786678961420ae007560f35e | testuser3  |
+------------------------------------------------------------------+------------+</pre></div><p>
     Get user records within the ad domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user show testuser1 --domain ad</pre></div><p>
     Here testuser1 is returned:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| id        | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
| name      | testuser1                                                        |
+-----------+------------------------------------------------------------------+</pre></div><p>
     Get a list of LDAP groups:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack group list --domain ad</pre></div><p>
     Note that testgroup1 and testgroup2 are returned:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| 03976b0ea6f54a8e4c0032e8f756ad581f26915c7e77500c8d4aaf0e83afcdc6 | testgroup1 |
| 7ba52ee1c5829d9837d740c08dffa07ad118ea1db2d70e0dc7fa7853e0b79fcf | testgroup2 |
+------------------------------------------------------------------+------------+</pre></div><div id="id-1.6.6.11.4.3.5.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      LDAP domain is read-only. This means that you cannot create new user or
      group records in it.
     </p></div></li></ol></div></div><div class="sect2" id="id-1.6.6.11.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domain-specific driver configuration. Switching from a database to a file store</span> <a title="Permalink" class="permalink" href="#id-1.6.6.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Following is the procedure to switch a domain-specific driver configuration
   from a database store to a file store. It is assumed that:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The domain-specific driver configuration with a database store has been
     set up and is working properly.
    </p></li><li class="listitem "><p>
     Domain-specific driver configuration files with the format:
     keystone.&lt;domain name&gt;.conf have already been located and verified
     in the specific directory (by default, /etc/keystone/domains/) on all of
     the controller nodes.
    </p></li></ul></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that the following configuration options are set in the main
     configuration file template in
     ~/openstack/my_cloud/config/keystone/keystone.conf.j2:
    </p><div class="verbatim-wrap"><pre class="screen">[identity]
 domain_specific_drivers_enabled = True
 domain_configurations_from_database = False

[domain_config]
# driver = sql</pre></div></li><li class="listitem "><p>
     Once the template is modified, commit the change to the local git
     repository, and rerun the configuration processor / deployment area
     preparation playbooks (as suggested at Using Git for Configuration
     Management):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A</pre></div><p>
     Verify that the files have been added using git status, then commit the
     changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status
<code class="prompt user">ardana &gt; </code>git commit -m "Domain-Specific Driver Configuration - Switch From Database Store to File Store: more description here..."</pre></div><p>
     Then run the configuration processor and ready deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run reconfiguration playbook in a deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Verify that the switched domain driver configuration for LDAP (Microsoft
     AD in this example) using file store works properly: Set environment
     variables for admin level access
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc</pre></div><p>
     Get list of domain users:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user list --domain ad</pre></div><p>
     Here you see the three users:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| e7dbec51ecaf07906bd743debcb49157a0e8af557b860a7c1dadd454bdab03fe | testuser1  |
| 8a09630fde3180c685e0cd663427e8638151b534a8a7ccebfcf244751d6f09bd | testuser2  |
| ea463d778dadcefdcfd5b532ee122a70dce7e790786678961420ae007560f35e | testuser3  |
+------------------------------------------------------------------+------------+</pre></div><p>
     Get user records within the ad domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user show testuser1 --domain ad</pre></div><p>
     Here is the result:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| id        | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
| name      | testuser1                                                        |
+-----------+------------------------------------------------------------------+</pre></div><p>
     Get a list of LDAP groups:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack group list --domain ad</pre></div><p>
     Here are the groups returned:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| 03976b0ea6f54a8e4c0032e8f756ad581f26915c7e77500c8d4aaf0e83afcdc6 | testgroup1 |
| 7ba52ee1c5829d9837d740c08dffa07ad118ea1db2d70e0dc7fa7853e0b79fcf | testgroup2 |
+------------------------------------------------------------------+------------+</pre></div><p>
     Note: Note: LDAP domain is read-only. This means that you can not create
     new user or group record in it.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.6.11.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update LDAP CA certificates</span> <a title="Permalink" class="permalink" href="#id-1.6.6.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There is a chance that LDAP CA certificates may expire or for some reason
   not work anymore. Below are steps to update the LDAP CA certificates on the
   identity service side. Follow the steps below to make the updates.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Locate the file keystone_configure_ldap_certs_sample.yml
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_certs_sample.yml</pre></div></li><li class="listitem "><p>
     Save a copy of this file with a new name, for example:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_certs_all.yml</pre></div></li><li class="listitem "><p>
     Edit the file and specify the correct single file path name for the ldap
     certificates. This file path name has to be consistent with the one
     defined in tls_cacertfile of the domain-specific configuration. Edit the
     file and populate or update it with LDAP CA certificates for all LDAP
     domains.
    </p></li><li class="listitem "><p>
     As suggested in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, add the new file to the local
     git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A</pre></div><p>
     Verify that the files have been added using git status and commit the
     file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status
<code class="prompt user">ardana &gt; </code>git commit -m "Update LDAP CA certificates: more description here..."</pre></div><p>
     Then run the configuration processor and ready deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in the deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@~/openstack/my_cloud/config/keystone/keystone_configure_ldap_certs_all.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.6.6.11.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.9.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.6.6.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> domain-specific configuration:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No Global User Listing: Once domain-specific driver configuration is
     enabled, listing all users and listing all groups are not supported
     operations. Those calls require a specific domain filter and a
     domain-scoped token for the target domain.
    </p></li><li class="listitem "><p>
     You cannot have both a file store and a database store for domain-specific
     driver configuration in a single identity service instance. Once a
     database store is enabled within the identity service instance, any file
     store will be ignored, and vice versa.
    </p></li><li class="listitem "><p>
     The identity service allows a list limit configuration to globally set the
     maximum number of entities that will be returned in an identity collection
     per request but it does not support per-domain list limit setting at this
     time.
    </p></li><li class="listitem "><p>
     Each time a new domain is configured with LDAP integration the single CA
     file gets overwritten. Ensure that you place certs for all the LDAP
     back-end domains in the cacert parameter. Detailed CA file inclusion
     instructions are provided in the comments of the sample YAML configuration
     file <code class="filename">keystone_configure_ldap_my.yml</code>
     (<a class="xref" href="#filestore" title="4.9.2. Set up domain-specific driver configuration - file store">Section 4.9.2, “Set up domain-specific driver configuration - file store”</a>).
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP).
    </p></li><li class="listitem "><p>
     Keystone assignment operations from LDAP records such as managing or
     assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 'default' domain is pre-configured to store service account
     users and is authenticated locally against the identity service. Domains
     configured for external LDAP integration are non-default domains.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li><li class="listitem "><p>
     Each LDAP connection with the identity service is for read-only
     operations. Configurations that require identity service write operations
     (to create users, groups, etc.) are not currently supported.
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP). Keystone assignment operations from LDAP records such as
     managing or assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> API-based domain-specific configuration management
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No GUI dashboard for domain-specific driver configuration management
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for type of option.
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for option values
     supported.
    </p></li><li class="listitem "><p>
     API-based Domain config method does not provide retrieval of default
     values of domain-specific configuration options.
    </p></li><li class="listitem "><p>
     Status: Domain-specific driver configuration database store is a non-core
     feature for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
    </p></li></ul></div><div id="id-1.6.6.11.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    When integrating with an external identity provider, cloud security is
    dependent upon the security of that identify provider. You should examine
    the security of the identity provider, and in particular the SAML 2.0 token
    generation process and decide what security properties you need to ensure
    adequate security of your cloud deployment. More information about SAML can
    be found at
    <a class="link" href="https://www.owasp.org/index.php/SAML_Security_Cheat_Sheet" target="_blank">https://www.owasp.org/index.php/SAML_Security_Cheat_Sheet</a>.
   </p></div></div></div><div class="sect1" id="k2kfed"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Keystone-to-Keystone Federation</span> <a title="Permalink" class="permalink" href="#k2kfed">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span>k2kfed</li></ul></div></div></div></div><p>
  This topic explains how you can use one instance of Keystone as an identity
  provider and one as a service provider.
 </p><div class="sect2" id="id-1.6.6.12.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What Is Keystone-to-Keystone Federation?</span> <a title="Permalink" class="permalink" href="#id-1.6.6.12.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Identity federation lets you configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> using existing identity
   management systems such as an LDAP directory as the source of user access
   authentication. The Keystone-to-Keystone federation (K2K) function extends
   this concept for accessing resources in multiple, separate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> clouds.
   You can configure each cloud to trust the authentication credentials of
   other clouds to provide the ability for users to authenticate with their
   home cloud and to access authorized resources in another cloud without
   having to reauthenticate with the remote cloud. This function is sometimes
   referred to as "single sign-on" or SSO.
  </p><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that provides the initial user authentication is called
   the identity provider (IdP). The identity provider cloud can support
   domain-based authentication against external authentication sources
   including LDAP-based directories such as Microsoft Active Directory. The
   identity provider creates the user attributes, known as assertions, which
   are used to automatically authenticate users with other <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> clouds.
  </p><p>
   An <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that provides resources is called a service provider (SP).
   A service provider cloud accepts user authentication assertions from the
   identity provider and provides access to project resources based on the
   mapping file settings developed for each service provider cloud. The
   following are characteristics of a service provider:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Each service provider cloud has a unique set of projects, groups, and
     group role assignments that are created and managed locally.
    </p></li><li class="listitem "><p>
     The mapping file consists a set of rules that define user group
     membership.
    </p></li><li class="listitem "><p>
     The mapping file enables the ability to auto-assign incoming users to a
     specific group. Project membership and access are defined by group
     membership.
    </p></li><li class="listitem "><p>
     Project quotas are defined locally by each service provider cloud.
    </p></li></ul></div><p>
   Keystone-to-Keystone federation is supported and enabled in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
   using configuration parameters in specific Ansible files. Instructions are
   provided to define and enable the required configurations.
  </p><p>
   Support for Keystone-to-Keystone federation happens on the API level, and
   you must implement it using your own client code by calling the supported
   APIs. Python-keystoneclient has supported APIs to access the K2K APIs.
  </p><div class="complex-example"><div class="example" id="ex-k2kclient"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 4.1: </span><span class="name">k2kclient.py </span><a title="Permalink" class="permalink" href="#ex-k2kclient">#</a></h6></div><div class="example-contents"><p>
    The following k2kclient.py file is an example, and the request diagram
    <a class="xref" href="#fig-keystone-authentication-flow" title="Keystone Authentication Flow">Figure 4.1, “Keystone Authentication Flow”</a> explains the flow of
    client requests.
   </p><div class="verbatim-wrap highlight python"><pre class="screen">import json
import os
import requests

import xml.dom.minidom

from keystoneclient.auth.identity import v3
from keystoneclient import session

class K2KClient(object):

    def __init__(self):
        # IdP auth URL
        self.auth_url = "http://192.168.245.9:35357/v3/"
        self.project_name = "admin"
        self.project_domain_name = "Default"
        self.username = "admin"
        self.password = "vvaQIZ1S"
        self.user_domain_name = "Default"
        self.session = requests.Session()
        self.verify = False
        # identity provider Id
        self.idp_id = "z420_idp"
        # service provider Id
        self.sp_id = "z620_sp"
        #self.sp_ecp_url = "https://16.103.149.44:8443/Shibboleth.sso/SAML2/ECP"
        #self.sp_auth_url = "https://16.103.149.44:8443/v3"

    def v3_authenticate(self):
        auth = v3.Password(auth_url=self.auth_url,
                           username=self.username,
                           password=self.password,
                           user_domain_name=self.user_domain_name,
                           project_name=self.project_name,
                           project_domain_name=self.project_domain_name)

        self.auth_session = session.Session(session=requests.session(),
                                       auth=auth, verify=self.verify)
        auth_ref = self.auth_session.auth.get_auth_ref(self.auth_session)
        self.token = self.auth_session.auth.get_token(self.auth_session)

    def _generate_token_json(self):
        return {
            "auth": {
                "identity": {
                    "methods": [
                        "token"
                    ],
                    "token": {
                        "id": self.token
                    }
                },
                "scope": {
                    "service_provider": {
                        "id": self.sp_id
                    }
                }
            }
        }

    def get_saml2_ecp_assertion(self):
        token = json.dumps(self._generate_token_json())
        url = self.auth_url + 'auth/OS-FEDERATION/saml2/ecp'
        r = self.session.post(url=url,
                              data=token,
                              verify=self.verify)
        if not r.ok:
            raise Exception("Something went wrong, %s" % r.__dict__)
        self.ecp_assertion = r.text

    def _get_sp_url(self):
        url = self.auth_url + 'OS-FEDERATION/service_providers/' + self.sp_id
        r = self.auth_session.get(
           url=url,
           verify=self.verify)
        if not r.ok:
            raise Exception("Something went wrong, %s" % r.__dict__)

        sp = json.loads(r.text)[u'service_provider']
        self.sp_ecp_url = sp[u'sp_url']
        self.sp_auth_url = sp[u'auth_url']

    def _handle_http_302_ecp_redirect(self, response, method, **kwargs):
        location = self.sp_auth_url + '/OS-FEDERATION/identity_providers/' + self.idp_id + '/protocols/saml2/auth'
        return self.auth_session.request(location, method, authenticated=False, **kwargs)

    def exchange_assertion(self):
        """Send assertion to a Keystone SP and get token."""
        self._get_sp_url()
        print("SP ECP Url:%s" % self.sp_ecp_url)
        print("SP Auth Url:%s" % self.sp_auth_url)
        #self.sp_ecp_url = 'https://16.103.149.44:8443/Shibboleth.sso/SAML2/ECP'
        r = self.auth_session.post(
            self.sp_ecp_url,
            headers={'Content-Type': 'application/vnd.paos+xml'},
            data=self.ecp_assertion,
            authenticated=False, redirect=False)
        r = self._handle_http_302_ecp_redirect(r, 'GET',
            headers={'Content-Type': 'application/vnd.paos+xml'})
        self.fed_token_id = r.headers['X-Subject-Token']
        self.fed_token = r.text

if __name__ == "__main__":
    client = K2KClient()
    client.v3_authenticate()
    client.get_saml2_ecp_assertion()
    client.exchange_assertion()
    print('Unscoped token_id: %s' % client.fed_token_id)
    print('Unscoped token body:
%s' % client.fed_token)</pre></div></div></div></div></div><div class="sect2" id="id-1.6.6.12.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up a Keystone Provider</span> <a title="Permalink" class="permalink" href="#id-1.6.6.12.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To set up Keystone as a service provider, follow these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create a config file called <code class="literal">k2k.yml</code> with the following
     parameters and place it in any directory on your Cloud Lifecycle Manager, such
     as /tmp.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_trusted_idp: k2k
keystone_sp_conf:
  shib_sso_idp_entity_id: &lt;protocol&gt;://&lt;idp_host&gt;:&lt;port&gt;/v3/OS-FEDERATION/saml2/idp
  shib_sso_application_entity_id: http://service_provider_uri_entityId
  target_domain:
    name: domain1
    description: my domain
  target_project:
    name: project1
    description: my project
  target_group:
    name: group1
    description: my group
  role:
    name: service
  idp_metadata_file: /tmp/idp_metadata.xml
  identity_provider:
    id: my_idp_id
    description: This is the identity service provider.
  mapping:
    id: mapping1
    <span class="bold"><strong>rules_file: /tmp/k2k_sp_mapping.json</strong></span>
  protocol:
    id: saml2
  attribute_map:
    -
      name: name1
      id: id1</pre></div><p>
     The following are descriptions of each of the attributes.
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> Attribute</th><th>Definition</th></tr></thead><tbody><tr><td>keystone_trusted_idp</td><td>
         <p>
          A flag to indicate if this configuration is used for
          Keystone-to-Keystone or WebSSO. The value can be either k2k or adfs.
         </p>
        </td></tr><tr><td><span class="bold"><strong>keystone_sp_conf</strong></span>
        </td><td> </td></tr><tr><td>shib_sso_idp_entity_id</td><td>
         <p>
          The identity provider URI used as an entity Id to identity the IdP.
          You shoud use the following value:
          &lt;protocol&gt;://&lt;idp_host&gt;:&lt;port&gt;/v3/OS-FEDERATION/saml2/idp.
         </p>
        </td></tr><tr><td>shib_sso_application_entity_id</td><td>
         <p>
          The service provider URI used as an entity Id. It can be any URI here
          for Keystone-to-Keystone.
         </p>
        </td></tr><tr><td>target_domain</td><td>
         <p>
          A domain where the group will be created.
         </p>
        </td></tr><tr><td>name</td><td>
         <p>
          Any domain name. If it does not exist, it will be created or updated.
         </p>
        </td></tr><tr><td>description</td><td>
         <p>
          Any description.
         </p>
        </td></tr><tr><td>target_project</td><td>
         <p>
          A project scope of the group.
         </p>
        </td></tr><tr><td>name</td><td>
         <p>
          Any project name. If it does not exist, it will be created or
          updated.
         </p>
        </td></tr><tr><td>description</td><td>Any description. </td></tr><tr><td>target_group</td><td>
         <p>
          A group will be created from target_domain.
         </p>
        </td></tr><tr><td>name</td><td>
         <p>
          Any group name. If it does not exist, it will be created or updated.
         </p>
        </td></tr><tr><td>description</td><td>Any description. </td></tr><tr><td>role</td><td>
         <p>
          A role will be assigned on target_project. This role impacts the IdP
          user scoped token permission on the service provider side.
         </p>
        </td></tr><tr><td>name</td><td>Must be an existing role. </td></tr><tr><td>idp_metadata_file</td><td>
         <p>
          A reference to the IdP metadata file that validates the SAML2
          assertion.
         </p>
        </td></tr><tr><td>identity_provider</td><td>A supported IdP.</td></tr><tr><td>id</td><td>
         <p>
          Any Id. If it does not exist, it will be created or updated. This Id
          needs to be shared with the client so that the right mapping will be
          selected.
         </p>
        </td></tr><tr><td>description</td><td>Any description.</td></tr><tr><td>mapping</td><td>
         <p>
          A mapping in JSON format that maps a federated user to a
          corresponding group.
         </p>
        </td></tr><tr><td>id</td><td>
         <p>
          Any Id. If it does not exist, it will be created or updated.
         </p>
        </td></tr><tr><td>rules_file</td><td>
         <p>
          A reference to the file that has the mapping in JSON.
         </p>
        </td></tr><tr><td>protocol</td><td>
         <p>
          The supported federation protocol.
         </p>
        </td></tr><tr><td>id</td><td>
         <p>
          Security Assertion Markup Language 2.0 (SAML2) is the only supported
          protocol for K2K.
         </p>
        </td></tr><tr><td>attribute_map</td><td>
         <p>
          A shibboleth mapping that defines additional attributes to map the
          attributes from the SAML2 assertion to the K2K mapping that the
          service provider understands. K2K does not require any additional
          attribute mapping.
         </p>
        </td></tr><tr><td>name</td><td>An attribute name from the SAML2 assertion.</td></tr><tr><td>id</td><td>An Id that the preceding name will be mapped to.</td></tr></tbody></table></div></li><li class="listitem "><p>
     Create a metadata file that is referenced from <code class="literal">k2k.yml</code>,
     such as <code class="literal">/tmp/idp_metadata.xml</code>. The content of the
     metadata file comes from the identity provider and can be found in
     <code class="literal"> /etc/keystone/idp_metadata.xml</code>.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create a mapping file that is referenced in k2k.yml, shown previously.
       An example is <code class="literal">/tmp/k2k_sp_mapping.json</code>. You can see
       the reference in bold in the preceding k2k.yml example. The following is
       an example of the mapping file.
      </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://idp_host:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div><p>
       You can find more information on how the K2K mapping works at
       <a class="link" href="http://docs.openstack.org" target="_blank">http://docs.openstack.org</a>.
      </p></li></ol></div></li><li class="listitem "><p>
     Go to <code class="literal">~/stack/scratch/ansible/next/ardana/ansible</code> and
     run the following playbook to enable the service provider:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/tmp/k2k.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong><span class="bold"><strong>Setting Up an Identity
   Provider</strong></span></strong></span>
  </p><p>
   To set up Keystone as an identity provider, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create a config file <code class="literal">k2k.yml</code> with the following
     parameters and place it in any directory on your Cloud Lifecycle Manager, such
     as <code class="literal">/tmp</code>. Note that the certificate and key here are
     excerpted for space.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_k2k_idp_conf:
    service_provider:
          -
            id: my_sp_id
            description: This is service provider.
            sp_url: https://sp_host:5000
            auth_url: https://sp_host:5000/v3
    signer_cert: -----BEGIN CERTIFICATE-----
MIIDmDCCAoACCQDS+ZDoUfr
    cIzANBgkqhkiG9w0BAQsFADCBjDELMAkGA1UEBhMC\ nVVMxEzARBgNVB
    AgMCkNhbGlmb3JuaWExEjAQBgNVBAcMCVN1bm55dmFsZTEMMAoG\
   
            ...
    nOpKEvhlMsl5I/tle
-----END CERTIFICATE-----
    signer_key: -----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEA1gRiHiwSO6L5PrtroHi/f17DQBOpJ1KMnS9FOHS
            
            ...</pre></div><p>
     The following are descriptions of each of the attributes under
     keystone_k2k_idp_conf
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.6.12.4.6.1.4.1"><span class="term ">service_provider</span></dt><dd><p>
        One or more service providers can be defined. If it does not exist, it
        will be created or updated.
       </p></dd><dt id="id-1.6.6.12.4.6.1.4.2"><span class="term ">id</span></dt><dd><p>
        Any Id. If it does not exist, it will be created or updated. This Id
        needs to be shared with the client so that it knows where the service
        provider is.
       </p></dd><dt id="id-1.6.6.12.4.6.1.4.3"><span class="term ">description</span></dt><dd><p>
        Any description.
       </p></dd><dt id="id-1.6.6.12.4.6.1.4.4"><span class="term ">sp_url</span></dt><dd><p>
        Service provider base URL.
       </p></dd><dt id="id-1.6.6.12.4.6.1.4.5"><span class="term ">auth_url</span></dt><dd><p>
        Service provider auth URL.
       </p></dd><dt id="id-1.6.6.12.4.6.1.4.6"><span class="term ">signer_cert</span></dt><dd><p>
        Content of self-signed certificate that is embedded in the metadata
        file. We recommend setting the validity for a longer period of time,
        such as 3650 days (10 years).
       </p></dd><dt id="id-1.6.6.12.4.6.1.4.7"><span class="term ">signer_key</span></dt><dd><p>
        A private key that has a key size of 2048 bits.
       </p></dd></dl></div></li><li class="listitem "><p>
     Create a private key and a self-signed certificate. The command-line tool,
     openssl, is required to generate the keys and certificates. If the system
     does not have it, you must install it.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create a private key of size 2048.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl genrsa -out myidp.key 2048</pre></div></li><li class="listitem "><p>
       Generate a certificate request named myidp.csr. When prompted, choose
       CommonName for the server's hostname.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl req -new -key myidp.key -out myidp.csr</pre></div></li><li class="listitem "><p>
       Generate a self-signed certificate named myidp.cer.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl x509 -req -days 3650 -in myidp.csr -signkey myidp.key -out myidp.cer</pre></div></li></ol></div></li><li class="listitem "><p>
     Go to <code class="literal">~/scratch/ansible/next/ardana/ansible</code> and
     run the following playbook to enable the service provider in Keystone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/tmp/k2k.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.6.6.12.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test It Out</span> <a title="Permalink" class="permalink" href="#id-1.6.6.12.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can use the script listed earlier, <code class="literal">k2kclient.py</code>
   (<a class="xref" href="#ex-k2kclient" title="k2kclient.py">Example 4.1, “k2kclient.py”</a>), as an example for the end-to-end flows. To run
   <code class="literal">k2kclient.py</code>, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     A few parameters must be changed in the beginning of
     <code class="literal">k2kclient.py</code>. For example, enter your specific URL,
     project name, and user name, as follows:
    </p><div class="verbatim-wrap"><pre class="screen"># IdP auth URL
self.auth_url = "http://idp_host:5000/v3/"
self.project_name = "my_project_name"
self.project_domain_name = "my_project_domain_name"
self.username = "test"
self.password = "mypass"
self.user_domain_name = "my_domain"
# identity provider Id that is defined in the SP config
self.idp_id = "my_idp_id"
# service provider Id that is defined in the IdP config
self.sp_id = "my_sp_id"</pre></div></li><li class="listitem "><p>
     Install python-keystoneclient along with its dependencies.
    </p></li><li class="listitem "><p>
     Run the <code class="literal">k2kclient.py</code> script. An unscoped token will be
     returned from the service provider.
    </p></li></ol></div><p>
   At this point, the domain or project scope of the unscoped taken can be
   discovered by sending the following URLs:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl -k -X GET -H "X-Auth-Token: <em class="replaceable ">unscoped token</em>" \
 https://&lt;sp_public_endpoint&gt;:5000/v3/OS-FEDERATION/domains
<code class="prompt user">ardana &gt; </code>curl -k -X GET -H "X-Auth-Token: <em class="replaceable ">unscoped token</em>" \
 https://&lt;sp_public_endpoint:5000/v3/OS-FEDERATION/projects</pre></div></div><div class="sect2" id="id-1.6.6.12.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Inside Keystone-to-Keystone Federation</span> <a title="Permalink" class="permalink" href="#id-1.6.6.12.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   K2K federation places a lot of responsibility with the user. The complexity
   is apparent from the following diagram.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Users must first authenticate to their home or local cloud, or local
     identity provider Keystone instance to obtain a scoped token.
    </p></li><li class="listitem "><p>
     Users must discover which service providers (or remote clouds) are
     available to them by querying their local cloud.
    </p></li><li class="listitem "><p>
     For a given remote cloud, users must discover which resources are
     available to them by querying the remote cloud for the projects they can
     scope to.
    </p></li><li class="listitem "><p>
     To talk to the remote cloud, users must first exchange, with the local
     cloud, their locally scoped token for a SAML2 assertion to present to the
     remote cloud.
    </p></li><li class="listitem "><p>
     Users then present the SAML2 assertion to the remote cloud. The remote
     cloud applies its mapping for the incoming SAML2 assertion to map each
     user to a local ephemeral persona (such as groups) and issues an unscoped
     token.
    </p></li><li class="listitem "><p>
     Users query the remote cloud for the list of projects they have access to.
    </p></li><li class="listitem "><p>
     Users then rescope their token to a given project.
    </p></li><li class="listitem "><p>
     Users now have access to the resources owned by the project.
    </p></li></ol></div><p>
   The following diagram illustrates the flow of authentication requests.
  </p><div class="figure" id="fig-keystone-authentication-flow"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-keystone-Keystone-2-Keystone-Sequence-Generic.png" target="_blank"><img src="images/media-keystone-Keystone-2-Keystone-Sequence-Generic.png" width="" alt="Keystone Authentication Flow" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.1: </span><span class="name">Keystone Authentication Flow </span><a title="Permalink" class="permalink" href="#fig-keystone-authentication-flow">#</a></h6></div></div></div><div class="sect2" id="id-1.6.6.12.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Additional Testing Scenarios</span> <a title="Permalink" class="permalink" href="#id-1.6.6.12.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following tests assume one identity provider and one service provider.
  </p><p>
   <span class="bold"><strong>Test Case 1: Any federated user in the identity
   provider maps to a single designated group in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com
username=user1</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1
group_domain_name=domain1
'group1' scopes to 'project1'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_1.json</pre></div><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 2: A federated user in a specific domain in
   the identity provider maps to two different groups in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com
username=user1
user_domain_name=Default</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1
group_domain_name=domain1
'group1' scopes to 'project1' group=group2
group_domain_name=domain2
'group2' scopes to 'project2'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_2.json</pre></div><p>
     testcase1_2.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group2",
           "domain":{
             "name": "domain2"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_user_domain",
      "any_one_of": [
          "Default"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to both project1 and
     project2.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 3: A federated user with a specific project
   in the identity provider maps to a specific group in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com
username=user4
user_project_name=test1</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group4
group_domain_name=domain4
'group4' scopes to 'project4'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_3.json</pre></div><p>
     testcase1_3.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group4",
           "domain":{
             "name": "domain4"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_project",
      "any_one_of": [
          "test1"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   },
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group5",
           "domain":{
             "name": "domain5"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_roles",
      "not_any_of": [
          "_member_"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project4.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 4: A federated user with a specific role in
   the identity provider maps to a specific group in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user5, role_name=_member_</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group5, group_domain_name=domain5, 'group5' scopes to 'project5'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_3.json</pre></div><p>
     testcase1_3.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group4",
           "domain":{
             "name": "domain4"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_project",
      "any_one_of": [
          "test1"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   },
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group5",
           "domain":{
             "name": "domain5"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_roles",
      "not_any_of": [
          "_member_"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project5.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 5: Retain the previous scope for a federated
   user</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user1, user_domain_name=Default</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1, group_domain_name=domain1, 'group1' scopes to 'project1'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_1.json</pre></div><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1. Later, we
     would like to scope federated users who have the default domain in the
     identity provider to project2 in addition to project1.
    </p></li><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user1, user_domain_name=Default</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1
group_domain_name=domain1
'group1' scopes to 'project1' group=group2
group_domain_name=domain2
'group2' scopes to 'project2'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_2.json</pre></div><p>
     testcase1_2.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group2",
           "domain":{
             "name": "domain2"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_user_domain",
      "any_one_of": [
          "Default"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1 and project2.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 6: Scope a federated user to a domain
   </strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user1</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1, group_domain_name=domain1, 'group1' scopes to 'project1'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_1.json</pre></div><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       The federated user will scope to project1.
      </p></li><li class="listitem "><p>
       User uses CLI/Curl to assign any existing role to group1 on domain1.
      </p></li><li class="listitem "><p>
       User uses CLI/Curl to remove project1 scope from group1.
      </p></li></ul></div></li><li class="listitem "><p>
     Final result: The federated user will scope to domain1.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 7: Test five remote attributes for mapping
   </strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Test all five different remote attributes, as follows, with similar test
     cases as noted previously.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       openstack_user
      </p></li><li class="listitem "><p>
       openstack_user_domain
      </p></li><li class="listitem "><p>
       openstack_roles
      </p></li><li class="listitem "><p>
       openstack_project
      </p></li><li class="listitem "><p>
       openstack_project_domain
      </p></li></ul></div><p>
     The attribute openstack_user does not make much sense for testing because
     it is mapped only to a specific username. The preceding test cases have
     already covered the attributes openstack_user_domain, openstack_roles, and
     openstack_project.
    </p></li></ol></div><p>
    Note that similar tests have also been run for two identity providers with
    one service provider, and for one identity provider with two service
    providers.
  </p></div><div class="sect2" id="id-1.6.6.12.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.10.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Known Issues and Limitations</span> <a title="Permalink" class="permalink" href="#id-1.6.6.12.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Keep the following points in mind:
  </p><div class="itemizedlist " id="idg-all-operations-configuring-identity-keystone-federation-xml-14"><ul class="itemizedlist"><li class="listitem "><p>
     When a user is disabled in the identity provider, the issued federated
     token from the service provider still remains valid until the token is
     expired based on the Keystone expiration setting.
    </p></li><li class="listitem "><p>
     An already issued federated token will retain its scope until its
     expiration. Any changes in the mapping on the service provider will not
     impact the scope of an already issued federated token. For example, if an
     already issued federated token was mapped to group1 that has scope on
     project1, and mapping is changed to group2 that has scope on project2, the
     prevously issued federated token still has scope on project1.
    </p></li><li class="listitem "><p>
     Access to service provider resources is provided only through the
     python-keystone CLI client or the Keystone API. No Horizon web interface
     support is currently available.
    </p></li><li class="listitem "><p>
     Domains, projects, groups, roles, and quotas are created per the service
     provider cloud. Support for federated projects, groups, roles, and quotas
     is currently not available.
    </p></li><li class="listitem "><p>
     Keystone-to-Keystone federation and WebSSO cannot be configured by putting
     both sets of configuration attributes in the same config file; they will
     overwrite each other. Consequently, they need to be configured
     individually.
    </p></li><li class="listitem "><p>
     Scoping the federated user to a domain is not supported by default in the
     playbook. Please follow the steps at <a class="xref" href="#scopeToDomain" title="4.10.7. Scope Federated User to Domain">Section 4.10.7, “Scope Federated User to Domain”</a>.
    </p></li></ul></div></div><div class="sect2" id="scopeToDomain"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.10.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scope Federated User to Domain</span> <a title="Permalink" class="permalink" href="#scopeToDomain">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span>scopeToDomain</li></ul></div></div></div></div><p>
Use the following steps to scope a federated user to a domain:
</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the IdP side, set <code class="literal">hostname=myidp.com</code> and
     <code class="literal">username=user1</code>.
    </p></li><li class="listitem "><p>
     On the service provider side, set: <code class="literal">group=group1</code>,
     <code class="literal">group_domain_name=domain1</code>, group1 scopes to project1.
    </p></li><li class="listitem "><p>
     Mapping used: testcase1_1.json.
    </p><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1. Use CLI/Curl
     to assign any existing role to group1 on domain1. Use CLI/Curl to remove
     project1 scope from group1.
    </p></li><li class="listitem "><p>
     Result: The federated user will scope to domain1.
    </p></li></ol></div></div></div><div class="sect1" id="websso"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Web Single Sign-On</span> <a title="Permalink" class="permalink" href="#websso">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span>websso</li></ul></div></div></div></div><p>
  This topic explains how to implement web single sign-on.
 </p><div class="sect2" id="id-1.6.6.13.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is WebSSO?</span> <a title="Permalink" class="permalink" href="#id-1.6.6.13.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   WebSSO, or web single sign-on, is a method for web browsers to receive
   current authentication information from an identity provider system without
   requiring a user to log in again to the application displayed by the
   browser. Users initially access the identity provider web page and supply
   their credentials. If the user successfully authenticates with the identity
   provider, the authentication credentials are then stored in the user’s web
   browser and automatically provided to all web-based applications, such as
   the Horizon dashboard in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. If users have not yet
   authenticated with an identity provider or their credentials have timed out,
   they are automatically redirected to the identity provider to renew their
   credentials.
  </p></div><div class="sect2" id="id-1.6.6.13.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.6.6.13.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist " id="idg-all-operations-configuring-identity-websso-xml-6"><ul class="itemizedlist"><li class="listitem "><p>
     The WebSSO function supports only Horizon web authentication. It is not
     supported for direct API or CLI access.
    </p></li><li class="listitem "><p>
     WebSSO works only with Fernet token provider. See <a class="xref" href="#fernet-tokens" title="4.8.4. Fernet Tokens">Section 4.8.4, “Fernet Tokens”</a>.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> WebSSO function was tested with Microsoft Active Directory
     Federation Services (AD FS). The instructions provided are pertinent to
     AD FS and are intended to provide a sample configuration for deploying
     WebSSO with an external identity provider. If you have a different
     identity provider such as Ping Identity or IBM Tivoli, consult with those
     vendors for specific instructions for those products.
    </p></li><li class="listitem "><p>
     Only WebSSO federation using the SAML method is supported in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> . OpenID-based federation is not currently supported.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>WebSSO has a change password option in User
     Settings, but note that this function is not accessible for users
     authenticating with external systems such as LDAP or SAML Identity
     Providers.</strong></span>
    </p></li></ul></div></div><div class="sect2" id="id-1.6.6.13.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling WebSSO</span> <a title="Permalink" class="permalink" href="#id-1.6.6.13.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> provides WebSSO support for the Horizon web interface.
   This support requires several configuration steps including editing the
   Horizon configuration file as well as ensuring that the correct Keystone
   authentication configuration is enabled to receive the authentication
   assertions provided by the identity provider.
  </p><p>
   The following is the workflow that depicts how Horizon and Keystone supports
   WebSSO if no current authentication assertion is available.
  </p><div class="orderedlist " id="ul-fcj-z4g-4v"><ol class="orderedlist" type="1"><li class="listitem "><p>
     Horizon redirects the web browser to the Keystone endpoint.
    </p></li><li class="listitem "><p>
     Keystone automatically redirects the web browser to the correct identity
     provider authentication web page based on the Keystone configuration file.
    </p></li><li class="listitem "><p>
     The user authenticates with the identity provider.
    </p></li><li class="listitem "><p>
     The identity provider automatically redirects the web browser back to the
     Keystone endpoint.
    </p></li><li class="listitem "><p>
     Keystone generates the required Javascript code to POST a token back to
     Horizon.
    </p></li><li class="listitem "><p>
     Keystone automatically redirects the web browser back to Horizon and the
     user can then access projects and resources assigned to the user.
    </p></li></ol></div><p>
   The following diagram provides more details on the WebSSO authentication
   workflow.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/Keystone-ADFS-WebSSO-Authentication-Sequence.png" target="_blank"><img src="images/Keystone-ADFS-WebSSO-Authentication-Sequence.png" width="" /></a></div></div><p>
   Note that the Horizon dashboard service never talks directly to the Keystone
   identity service until the end of the sequence, after the federated unscoped
   token negotiation has completed. The browser interacts with the Horizon
   dashboard service, the Keystone identity service, and AD FS on their
   respective public endpoints.
  </p><p>
   The following sequence of events is depicted in the diagram.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The user's browser reaches the Horizon dashboard service's login page. The
     user selects AD FS login from the drop-down menu.
    </p></li><li class="listitem "><p>
     The Horizon dashboard service issues an HTTP Redirect (301) to redirect
     the browser to the Keystone identity service's (public) SAML2 Web SSO
     endpoint (/auth/OS-FEDERATION/websso/saml2). The endpoint is protected by
     Apache mod_shib (shibboleth).
    </p></li><li class="listitem "><p>
     The browser talks to the Keystone identity service. Because the user's
     browser does not have an active session with AD FS, the Keystone identity
     service issues an HTTP Redirect (301) to the browser, along with the
     required SAML2 request, to the AD FS endpoint.
    </p></li><li class="listitem "><p>
     The browser talks to AD FS. AD FS returns a login form. The browser presents
     it to the user.
    </p></li><li class="listitem "><p>
     The user enters credentials (such as username and password) and submits
     the form to AD FS.
    </p></li><li class="listitem "><p>
     Upon successful validation of the user's credentials, AD FS issues an HTTP
     Redirect (301) to the browser, along with the SAML2 assertion, to the
     Keystone identity service's (public) SAML2 endpoint
     (/auth/OS-FEDERATION/websso/saml2).
    </p></li><li class="listitem "><p>
     The browser talks to the Keystone identity service. the Keystone identity
     service validates the SAML2 assertion and issues a federated unscoped
     token. the Keystone identity service returns JavaScript code to be
     executed by the browser, along with the federated unscoped token in the
     headers.
    </p></li><li class="listitem "><p>
     Upon execution of the JavaScript code, the browser is redirected to the
     Horizon dashboard service with the federated unscoped token in the header.
    </p></li><li class="listitem "><p>
     The browser talks to the Horizon dashboard service with the federated
     unscoped token.
    </p></li><li class="listitem "><p>
     With the unscoped token, the Horizon dashboard service talks to the
     Keystone identity service's (internal) endpoint to get a list of projects
     the user has access to.
    </p></li><li class="listitem "><p>
     The Horizon dashboard service rescopes the token to the first project in
     the list. At this point, the user is successfully logged in.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.6.13.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#id-1.6.6.13.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3" id="id-1.6.6.13.6.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">4.11.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating AD FS metadata</span> <a title="Permalink" class="permalink" href="#id-1.6.6.13.6.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For information about creating Active Directory Federation Services
   metadata, see the section <em class="citetitle ">To create edited AD FS 2.0 metadata
   with an added scope element</em> of
   <a class="link" href="https://technet.microsoft.com/en-us/library/gg317734" target="_blank">https://technet.microsoft.com/en-us/library/gg317734</a>.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the AD FS computer, use a browser such as Internet Explorer to view
     <code class="literal">https://&lt;adfs_server_hostname&gt;/FederationMetadata/2007-06/FederationMetadata.xml</code>.
    </p></li><li class="listitem "><p>
     On the File menu, click Save as, and then navigate to the Windows desktop
     and save the file with the name adfs_metadata.xml. Make sure to change the
     Save as type drop-down box to All Files (*.*).
    </p></li><li class="listitem "><p>
     Use Windows Explorer to navigate to the Windows desktop, right-click
     adfs_metadata.xml, and then click Edit.
    </p></li><li class="listitem "><p>
     In Notepad, insert the following XML in the first element. Before editing,
     the EntityDescriptor appears as follows:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID=http://WIN-CAICP35LF2I.vlan44.domain/adfs/services/trust xmlns="urn:oasis:names:tc:SAML:2.0:metadata" &gt;</pre></div><p>
     After editing, it should look like this:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID="http://WIN-CAICP35LF2I.vlan44.domain/adfs/services/trust" xmlns="urn:oasis:names:tc:SAML:2.0:metadata" xmlns:shibmd="urn:mace:shibboleth:metadata:1.0"&gt;</pre></div></li><li class="listitem "><p>
     In Notepad, on the Edit menu, click Find. In Find what, type IDPSSO, and
     then click Find Next.
    </p></li><li class="listitem "><p>
     Insert the following XML in this section: Before editing, the
     IDPSSODescriptor appears as follows:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;IDPSSODescriptor protocolSupportEnumeration="urn:oasis:names:tc:SAML:2.0:protocol"&gt;&lt;KeyDescriptor use="encryption"&gt;</pre></div><p>
     After editing, it should look like this:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;IDPSSODescriptor protocolSupportEnumeration="urn:oasis:names:tc:SAML:2.0:protocol"&gt;&lt;Extensions&gt;&lt;shibmd:Scope regexp="false"&gt;vlan44.domain&lt;/shibmd:Scope&gt;&lt;/Extensions&gt;&lt;KeyDescriptor use="encryption"&gt;</pre></div></li><li class="listitem "><p>
     Delete the metadata document signature section of the file (the bold text
     shown in the following code). Because you have edited the document, the
     signature will now be invalid. Before editing the signature appears as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID="http://FSWEB.contoso.com/adfs/services/trust" xmlns="urn:oasis:names:tc:SAML:2.0:metadata" xmlns:shibmd="urn:mace:shibboleth:metadata:1.0"&gt;
<span class="bold"><strong>&lt;ds:Signature xmlns:ds="http://www.w3.org/2000/09/xmldsig#"&gt;
    SIGNATURE DATA
&lt;/ds:Signature&gt;</strong></span>
&lt;RoleDescriptor xsi:type=…&gt;</pre></div><p>
     After editing it should look like this:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID="http://FSWEB.contoso.com/adfs/services/trust" xmlns="urn:oasis:names:tc:SAML:2.0:metadata" xmlns:shibmd="urn:mace:shibboleth:metadata:1.0"&gt;
&lt;RoleDescriptor xsi:type=…&gt;</pre></div></li><li class="listitem "><p>
     Save and close adfs_metadata.xml.
    </p></li><li class="listitem "><p>
     Copy adfs_metadata.xml to the Cloud Lifecycle Manager node in your preferred
     location. Here it is /tmp.
    </p></li></ol></div></div><div class="sect3" id="id-1.6.6.13.6.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">4.11.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up WebSSO</span> <a title="Permalink" class="permalink" href="#id-1.6.6.13.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Start by creating a config file <code class="filename">adfs_config.yml</code> with the
   following parameters and place it in any directory on your Cloud Lifecycle Manager,
   such as <code class="filename">/tmp</code>.
  </p><div class="verbatim-wrap"><pre class="screen">keystone_trusted_idp: adfs
keystone_sp_conf:
    idp_metadata_file: /tmp/adfs_metadata.xml
    shib_sso_application_entity_id: http://sp_uri_entityId
    shib_sso_idp_entity_id: http://default_idp_uri_entityId
    target_domain:
        name: domain1
        description: my domain
    target_project:
        name: project1
        description: my project
    target_group:
        name: group1
        description: my group
    role:
        name: service
    identity_provider:
        id: adfs_idp1
        description: This is the AD FS identity provider.
    mapping:
        id: mapping1
        rules_file: adfs_mapping.json
    protocol:
        id: saml2
    attribute_map:
        -
          name: http://schemas.xmlsoap.org/claims/Group
          id: ADFS_GROUP
        -
          name: urn:oid:1.3.6.1.4.1.5923.1.1.1.6
          id: ADFS_LOGIN</pre></div><p>
   A sample config file like this exists in
   roles/KEY-API/files/samples/websso/keystone_configure_adfs_sample.yml. Here
   are some detailed descriptions for each of the config options:
  </p><div class="verbatim-wrap"><pre class="screen">keystone_trusted_idp: A flag to indicate if this configuration is used for WebSSO or K2K. The value can be either 'adfs' or 'k2k'.
keystone_sp_conf:
    shib_sso_idp_entity_id: The AD FS URI used as an entity Id to identity the IdP.
    shib_sso_application_entity_id: The Service Provider URI used as a entity Id. It can be any URI here for Websso as long as it is unique to the SP.
    target_domain: A domain where the group will be created from.
        name: Any domain name. If it does not exist, it will be created or be updated.
        description: Any description.
    target_project: A project scope that the group has.
        name: Any project name. If it does not exist, it will be created or be updated.
        description: Any description.
    target_group: A group will be created from 'target_domain'.
        name: Any group name. If it does not exist, it will be created or be updated.
        description: Any description.
    role: A role will be assigned on 'target_project'. This role impacts the idp user scoped token permission at sp side.
        name: It has to be an existing role.
    idp_metadata_file: A reference to the AD FS metadata file that validates the SAML2 assertion.
    identity_provider: An AD FS IdP
        id: Any Id. If it does not exist, it will be created or be updated. This Id needs to be shared with the client so that the right mapping will be selected.
        description: Any description.
    mapping: A mapping in json format that maps a federated user to a corresponding group.
        id: Any Id. If it does not exist, it will be created or be updated.
        rules_file: A reference to the file that has the mapping in json.
    protocol: The supported federation protocol.
        id: 'saml2' is the only supported protocol for Websso.
    attribute_map: A shibboleth mapping defined additional attributes to map the attributes from the SAML2 assertion to the Websso mapping that SP understands.
        -
          name: An attribute name from the SAML2 assertion.
          id: An Id that the above name will be mapped to.</pre></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In the preceding config file, /tmp/adfs_config.yml, make sure the
     idp_metadata_file references the previously generated AD FS metadata file.
     In this case:
    </p><div class="verbatim-wrap"><pre class="screen">idp_metadata_file: /tmp/adfs_metadata.xml</pre></div></li><li class="listitem "><p>
     Create a mapping file that is referenced from the preceding config file,
     such as /tmp/adfs_sp_mapping.json. rules_file: /tmp/adfs_sp_mapping.json.
     The following is an example of the mapping file, existing in
     roles/KEY-API/files/samples/websso/adfs_sp_mapping.json:
    </p><div class="verbatim-wrap"><pre class="screen">[
             {
               "local": [{
                     "user": {
                         "name": "{0}"
                     }
                 }],
                 "remote": [{
                     "type": "ADFS_LOGIN"
                 }]
              },
              {
                "local": [{
                    "group": {
                        "id": "GROUP_ID"
                    }
                }],
                "remote": [{
                    "type": "ADFS_GROUP",
                "any_one_of": [
                    "Domain Users"
                    ]
                }]
              }
 ]</pre></div><p>
     You can find more details about how the WebSSO mapping works at
     <a class="link" href="http://docs.openstack.org" target="_blank">http://docs.openstack.org</a>.
     Also see <a class="xref" href="#maprules" title="4.11.4.3. Mapping rules">Section 4.11.4.3, “Mapping rules”</a> for more information.
    </p></li><li class="listitem "><p>
     Go to ~/scratch/ansible/next/ardana/ansible and run the following
     playbook to enable WebSSO in the Keystone identity service:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/tmp/adfs_config.yml</pre></div></li><li class="listitem "><p>
     Enable WebSSO in the Horizon dashboard service by setting
     horizon_websso_enabled flag to True in roles/HZN-WEB/defaults/main.yml and
     then run the horizon-reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div><div class="sect3" id="maprules"><div class="titlepage"><div><div><h4 class="title"><span class="number">4.11.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Mapping rules</span> <a title="Permalink" class="permalink" href="#maprules">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span>maprules</li></ul></div></div></div></div><p>
   One IdP-SP has only one mapping. The last mapping that the customer
   configures will be the one used and will overwrite the old mapping setting.
   Therefore, if the example mapping adfs_sp_mapping.json is used, the
   following behavior is expected because it maps the federated user only to
   the one group configured in
   <code class="literal">keystone_configure_adfs_sample.yml</code>.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Configure domain1/project1/group1, mapping1; websso login horizon, see
     project1;
    </p></li><li class="listitem "><p>
     Then reconfigure: domain1/project2/group1. mapping1, websso login horizon,
     see project1 and project2;
    </p></li><li class="listitem "><p>
     Reconfigure: domain3/project3/group3; mapping1, websso login horizon, only
     see project3; because now the IDP mapping maps the federated user to
     group3, which only has priviliges on project3.
    </p></li></ul></div><p>
   If you need a more complex mapping, you can use a custom mapping file, which
   needs to be specified in keystone_configure_adfs_sample.yml -&gt;
   rules_file.
  </p><p>
   You can use different attributes of the AD FS user in order to map to
   different or multiple groups.
  </p><p>
   An example of a more complex mapping file is
   adfs_sp_mapping_multiple_groups.json, as follows.
  </p><p>
   adfs_sp_mapping_multiple_groups.json
  </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "ADFS_LOGIN"
    },
    {
      "type": "ADFS_GROUP",
      "any_one_of":[
         "Domain Users"
      ]
     }
    ]
   },
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group2",
           "domain":{
             "name": "domain2"
           }
        }
      }
    ],
    "remote":[{
      "type": "ADFS_LOGIN"
    },
    {
      "type": "ADFS_SCOPED_AFFILIATION",
      "any_one_of": [
          "member@contoso.com"
      ]
    },
    ]
   }
]</pre></div><p>
   The adfs_sp_mapping_multiple_groups.json must be run together with
   keystone_configure_mutiple_groups_sample.yml, which adds a new attribute for
   the shibboleth mapping. That file is as follows:
  </p><p>
   keystone_configure_mutiple_groups_sample.yml
  </p><div class="verbatim-wrap"><pre class="screen">#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---

keystone_trusted_idp: adfs
keystone_sp_conf:
    identity_provider:
        id: adfs_idp1
        description: This is the AD FS identity provider.
    idp_metadata_file: /opt/stack/adfs_metadata.xml

    shib_sso_application_entity_id: http://blabla
    shib_sso_idp_entity_id: http://WIN-CAICP35LF2I.vlan44.domain/adfs/services/trust

    target_domain:
        name: domain2
        description: my domain

    target_project:
        name: project6
        description: my project

    target_group:
        name: group2
        description: my group

    role:
        name: admin

    mapping:
        id: mapping1
        rules_file: /opt/stack/adfs_sp_mapping_multiple_groups.json

    protocol:
        id: saml2

    attribute_map:
        -
          name: http://schemas.xmlsoap.org/claims/Group
          id: ADFS_GROUP
        -
          name: urn:oid:1.3.6.1.4.1.5923.1.1.1.6
          id: ADFS_LOGIN
        -
          name: urn:oid:1.3.6.1.4.1.5923.1.1.1.9
          id: ADFS_SCOPED_AFFILIATION</pre></div></div></div><div class="sect2" id="id-1.6.6.13.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up the AD FS server as the identity provider</span> <a title="Permalink" class="permalink" href="#id-1.6.6.13.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For AD FS to be able to communicate with the Keystone identity service, you
   need to add the Keystone identity service as a trusted relying party for
   AD FS and also specify the user attributes that you want to send to the
   Keystone identity service when users authenticate via WebSSO.
  </p><p>
   For more information, see the
   <a class="link" href="https://technet.microsoft.com/en-us/library/gg317734" target="_blank">Microsoft
   AD FS wiki</a>, section "Step 2: Configure AD FS 2.0 as the identity
   provider and shibboleth as the Relying Party".
  </p><p>
   Log in to the AD FS server.
  </p><p>
   <span class="bold"><strong>Add a relying party using metadata</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     From Server Manager Dashboard, click Tools on the upper right, then ADFS
     Management.
    </p></li><li class="listitem "><p>
     Right-click ADFS, and then select Add Relying Party Trust.
    </p></li><li class="listitem "><p>
     Click Start, leave the already selected option <code class="literal">Import data about
     the relying party published online or on a local network</code>.
    </p></li><li class="listitem "><p>
     In the Federation metadata address field, type
     <code class="literal">&lt;keystone_publicEndpoint&gt;/Shibboleth.sso/Metadata</code>
     (your Keystone identity service Metadata endpoint), and then click Next.
     You can also import metadata from a file. Create a file with the content
     of the result of the following curl command
    </p><div class="verbatim-wrap"><pre class="screen">curl &lt;keystone_publicEndpoint&gt;/Shibboleth.sso/Metadata</pre></div><p>
     and then choose this file for importing the metadata for the relying
     party.
    </p></li><li class="listitem "><p>
     In the Specify Display Name page, choose a proper name to identify this
     trust relationship, and then click Next.
    </p></li><li class="listitem "><p>
     On the Choose Issuance Authorization Rules page, leave the default Permit
     all users to access the relying party selected, and then click Next.
    </p></li><li class="listitem "><p>
     Click Next, and then click Close.
    </p></li></ol></div><p>
   <span class="bold"><strong>Edit claim rules for relying party trust</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The Edit Claim Rules dialog box should already be open. If not, In the
     ADFS center pane, under Relying Party Trusts, right-click your newly
     created trust, and then click Edit Claim Rules.
    </p></li><li class="listitem "><p>
     On the Issuance Transform Rules tab, click Add Rule.
    </p></li><li class="listitem "><p>
     On the Select Rule Template page, select Send LDAP Attributes as Claims,
     and then click Next.
    </p></li><li class="listitem "><p>
     On the Configure Rule page, in the Claim rule name box, type Get Data.
    </p></li><li class="listitem "><p>
     In the Attribute Store list, select Active Directory.
    </p></li><li class="listitem "><p>
     In the Mapping of LDAP attributes section, create the following mappings.
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> LDAP Attribute</th><th>Outgoing Claim Type</th></tr></thead><tbody><tr><td>Token-Groups – Unqualified Names</td><td>Group</td></tr><tr><td>User-Principal-Name</td><td>UPN</td></tr></tbody></table></div></li><li class="listitem "><p>
     Click Finish.
    </p></li><li class="listitem "><p>
     On the Issuance Transform Rules tab, click Add Rule.
    </p></li><li class="listitem "><p>
     On the Select Rule Template page, select Send Claims Using a Custom Rule,
     and then click Next.
    </p></li><li class="listitem "><p>
     In the Configure Rule page, in the Claim rule name box, type Transform UPN
     to epPN.
    </p></li><li class="listitem "><p>
     In the Custom Rule window, type or copy and paste the following:
    </p><div class="verbatim-wrap"><pre class="screen">c:[Type == "http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn"]
=&gt; issue(Type = "urn:oid:1.3.6.1.4.1.5923.1.1.1.6", Value = c.Value, Properties["http://schemas.xmlsoap.org/ws/2005/05/identity/claimproperties/attributename"] = "urn:oasis:names:tc:SAML:2.0:attrname-format:uri");</pre></div></li><li class="listitem "><p>
     Click Finish.
    </p></li><li class="listitem "><p>
     On the Issuance Transform Rules tab, click Add Rule.
    </p></li><li class="listitem "><p>
     On the Select Rule Template page, select Send Claims Using a Custom Rule,
     and then click Next.
    </p></li><li class="listitem "><p>
     On the Configure Rule page, in the Claim rule name box, type Transform
     Group to epSA.
    </p></li><li class="listitem "><p>
     In the Custom Rule window, type or copy and paste the following:
    </p><div class="verbatim-wrap"><pre class="screen">c:[Type == "http://schemas.xmlsoap.org/claims/Group", Value == "Domain Users"]
=&gt; issue(Type = "urn:oid:1.3.6.1.4.1.5923.1.1.1.9", Value = "member@contoso.com", Properties["http://schemas.xmlsoap.org/ws/2005/05/identity/claimproperties/attributename"] = "urn:oasis:names:tc:SAML:2.0:attrname-format:uri");</pre></div></li><li class="listitem "><p>
     Click Finish, and then click OK.
    </p></li></ol></div><p>
   This list of Claim Rules is just an example and can be modified or enhanced
   based on the customer's necessities and AD FS setup specifics.
  </p><p>
   <span class="bold"><strong>Create a sample user on the AD FS server</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     From the Server Manager Dashboard, click Tools on the upper right, then
     Active Directory Users and Computer.
    </p></li><li class="listitem "><p>
     Right click User, then New, and then User.
    </p></li><li class="listitem "><p>
     Follow the on-screen instructions.
    </p></li></ol></div><p>
   You can test the Horizon dashboard service "Login with ADFS" by opening a
   browser at the Horizon dashboard service URL and choose
   <code class="literal">Authenticate using: ADFS Credentials</code>. You should be
   redirected to the ADFS login page and be able to log into the Horizon
   dashboard service with your ADFS credentials.
  </p></div></div><div class="sect1" id="topic-qtp-cn3-bt"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity Service Notes and Limitations</span> <a title="Permalink" class="permalink" href="#topic-qtp-cn3-bt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>topic-qtp-cn3-bt</li></ul></div></div></div></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-limitations-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-limitations-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-limitations-xml-6</li></ul></div></div></div></div><p>
   This topic describes limitations of and important notes pertaining to the
   identity service. <span class="bold"><strong>Domains</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Domains can be created and managed by the Horizon web interface, Keystone
     API and OpenStackClient CLI.
    </p></li><li class="listitem "><p>
     The configuration of external authentication systems requires the creation
     and usage of Domains.
    </p></li><li class="listitem "><p>
     All configurations are managed by creating and editing specific
     configuration files.
    </p></li><li class="listitem "><p>
     End users can authenticate to a particular project and domain via the
     Horizon web interface, Keystone API and OpenStackClient CLI.
    </p></li><li class="listitem "><p>
     A new Horizon login page that requires a Domain entry is now installed by
     default.
    </p></li></ul></div><p>
   <span class="bold"><strong>Keystone-to-Keystone Federation</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Keystone-to-Keystone (K2K) Federation provides the ability to authenticate
     once with one cloud and then use these credentials to access resources on
     other federated clouds.
    </p></li><li class="listitem "><p>
     All configurations are managed by creating and editing specific
     configuration files.
    </p></li></ul></div><p>
   <span class="bold"><strong>Multi-Factor Authentication (MFA)</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Keystone architecture provides support for MFA deployments.
    </p></li><li class="listitem "><p>
     MFA provides the ability to deploy non-password based authentication; for
     example: token providing hardware and text messages.
    </p></li></ul></div><p>
   <span class="bold"><strong>Hierarchical Multitenancy</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Provides the ability to create sub-projects within a Domain-Project
     hierarchy.
    </p></li></ul></div></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-limitations-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-limitations-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-limitations-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Authentication with external authentication systems
   (LDAP, Active Directory (AD) or Identity Providers)</strong></span>
  </p><div class="itemizedlist " id="ul-u52-jpd-bt"><ul class="itemizedlist"><li class="listitem "><p>
     No Horizon web portal support currently exists for the creation and
     management of external authentication system configurations.
    </p></li></ul></div><p>
   <span class="bold"><strong>Integration with LDAP services</strong></span>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> domain-specific configuration:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No Global User Listing: Once domain-specific driver configuration is
     enabled, listing all users and listing all groups are not supported
     operations. Those calls require a specific domain filter and a
     domain-scoped token for the target domain.
    </p></li><li class="listitem "><p>
     You cannot have both a file store and a database store for domain-specific
     driver configuration in a single identity service instance. Once a
     database store is enabled within the identity service instance, any file
     store will be ignored, and vice versa.
    </p></li><li class="listitem "><p>
     The identity service allows a list limit configuration to globally set the
     maximum number of entities that will be returned in an identity collection
     per request but it does not support per-domain list limit setting at this
     time.
    </p></li><li class="listitem "><p>
     Each time a new domain is configured with LDAP integration the single CA
     file gets overwritten. Ensure that you place certs for all the LDAP
     back-end domains in the cacert parameter. Detailed CA file inclusion
     instructions are provided in the comments of the sample YAML configuration
     file <code class="filename">keystone_configure_ldap_my.yml</code>
     (see <a class="xref" href="#filestore" title="4.9.2. Set up domain-specific driver configuration - file store">Section 4.9.2, “Set up domain-specific driver configuration - file store”</a>).
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP).
    </p></li><li class="listitem "><p>
     Keystone assignment operations from LDAP records such as managing or
     assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 'default' domain is pre-configured to store service account
     users and is authenticated locally against the identity service. Domains
     configured for external LDAP integration are non-default domains.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li><li class="listitem "><p>
     Each LDAP connection with the identity service is for read-only
     operations. Configurations that require identity service write operations
     (to create users, groups, etc.) are not currently supported.
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP). Keystone assignment operations from LDAP records such as
     managing or assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> API-based domain-specific configuration management
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No GUI dashboard for domain-specific driver configuration management
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for type of option.
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for option values
     supported.
    </p></li><li class="listitem "><p>
     API-based Domain config method does not provide retrieval of default
     values of domain-specific configuration options.
    </p></li><li class="listitem "><p>
     Status: Domain-specific driver configuration database store is a non-core
     feature for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
    </p></li></ul></div></div><div class="sect2" id="id-1.6.6.14.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Keystone-to-Keystone federation</span> <a title="Permalink" class="permalink" href="#id-1.6.6.14.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     When a user is disabled in the identity provider, the issued federated
     token from the service provider still remains valid until the token is
     expired based on the Keystone expiration setting.
    </p></li><li class="listitem "><p>
     An already issued federated token will retain its scope until its
     expiration. Any changes in the mapping on the service provider will not
     impact the scope of an already issued federated token. For example, if an
     already issued federated token was mapped to group1 that has scope on
     project1, and mapping is changed to group2 that has scope on project2, the
     prevously issued federated token still has scope on project1.
    </p></li><li class="listitem "><p>
     Access to service provider resources is provided only through the
     python-keystone CLI client or the Keystone API. No Horizon web interface
     support is currently available.
    </p></li><li class="listitem "><p>
     Domains, projects, groups, roles, and quotas are created per the service
     provider cloud. Support for federated projects, groups, roles, and quotas
     is currently not available.
    </p></li><li class="listitem "><p>
     Keystone-to-Keystone federation and WebSSO cannot be configured by putting
     both sets of configuration attributes in the same config file; they will
     overwrite each other. Consequently, they need to be configured
     individually.
    </p></li><li class="listitem "><p>
     Scoping the federated user to a domain is not supported by default in the
     playbook. To enable it, see the steps in <a class="xref" href="#scopeToDomain" title="4.10.7. Scope Federated User to Domain">Section 4.10.7, “Scope Federated User to Domain”</a>.
    </p></li><li class="listitem "><p>
     No Horizon web portal support currently exists for the creation and
     management of federation configurations.
    </p></li><li class="listitem "><p>
     All end user authentication is available only via the Keystone API and
     OpenStackClient CLI.
    </p></li><li class="listitem "><p>
     Additional information can be found at
     <a class="link" href="http://docs.openstack.org" target="_blank">http://docs.openstack.org</a>.
    </p></li></ul></div><p>
   <span class="bold"><strong>WebSSO</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The WebSSO function supports only Horizon web authentication. It is not
     supported for direct API or CLI access.
    </p></li><li class="listitem "><p>
     WebSSO works only with Fernet token provider. See <a class="xref" href="#fernet-tokens" title="4.8.4. Fernet Tokens">Section 4.8.4, “Fernet Tokens”</a>.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> WebSSO function was tested with Microsoft Active Directory
     Federation Services (ADFS). The instructions provided are pertinent to
     ADFS and are intended to provide a sample configuration for deploying
     WebSSO with an external identity provider. If you have a different
     identity provider such as Ping Identity or IBM Tivoli, consult with those
     vendors for specific instructions for those products.
    </p></li><li class="listitem "><p>
     Only WebSSO federation using the SAML method is supported in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> . OpenID-based federation is not currently supported.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>WebSSO has a change password option in User
     Settings, but note that this function is not accessible for users
     authenticating with external systems such as LDAP or SAML Identity
     Providers.</strong></span>
    </p></li></ul></div><p>
   <span class="bold"><strong>Multi-factor authentication (MFA)</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> MFA support is a custom configuration requiring <span class="phrase"><span class="phrase">Sales Engineering</span></span> support.
    </p></li><li class="listitem "><p>
     MFA drivers are not included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and need to be provided by a
     specific MFA vendor.
    </p></li><li class="listitem "><p>
     Additional information can be found at
     <a class="link" href="http://docs.openstack.org/security-guide/content/identity-authentication-methods.html#identity-authentication-methods-external-authentication-methods" target="_blank">http://docs.openstack.org/security-guide/content/identity-authentication-methods.html#identity-authentication-methods-external-authentication-methods</a>.
    </p></li></ul></div><p>
   <span class="bold"><strong>Hierarchical multitenancy</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     This function requires additional support from various OpenStack services
     to be functional. It is a non-core function in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and is not ready
     for either proof of concept or production deployments.
    </p></li><li class="listitem "><p>
     Additional information can be found at
     <a class="link" href="http://specs.openstack.org/openstack/keystone-specs/specs/juno/hierarchical_multitenancy.html" target="_blank">http://specs.openstack.org/openstack/keystone-specs/specs/juno/hierarchical_multitenancy.html</a>.
    </p></li></ul></div><p>
   <span class="bold"><strong>Missing quota information for compute
   resources</strong></span>
  </p><div id="id-1.6.6.14.4.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    An error message that will appear in the default Horizon page if you are
    running a Swift-only deployment (no Compute service). In this
    configuration, you will not see any quota information for Compute
    resources and will see the following error message:
   </p></div><p>
   <span class="emphasis"><em>The Compute service is not installed or is not configured
   properly. No information is available for Compute resources.</em></span> This
   error message is expected as no Compute service is configured for this
   deployment. Please ignore the error message.
  </p><p>
   The following is the benchmark of the performance that is based on 150
   concurrent requests and run for 10 minute periods of stable load time.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="newCol3" /></colgroup><thead><tr><th>Operation </th><th>In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> (secs/request)</th><th>In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> 3.0 (secs/request)</th></tr></thead><tbody><tr><td>Token Creation </td><td>0.86</td><td>0.42</td></tr><tr><td>Token Validation</td><td>0.47</td><td>0.41</td></tr></tbody></table></div><p>
   Considering that token creation operations do not happen as frequently as
   token validation operations, you are likely to experience less of a
   performance problem regardless of the extended time for token creation.
  </p></div><div class="sect2" id="sec-keystone-cron"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.12.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System cron jobs need setup</span> <a title="Permalink" class="permalink" href="#sec-keystone-cron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>sec-keystone-cron</li></ul></div></div></div></div><p>
   Keystone relies on two cron jobs to periodically clean up expired tokens and
   for token revocation. The following is how the cron jobs appear on the
   system:
  </p><div class="verbatim-wrap"><pre class="screen">1 1 * * * /opt/stack/service/keystone/venv/bin/keystone-manage token_flush
1 1,5,10,15,20 * * * /opt/stack/service/keystone/venv/bin/revocation_cleanup.sh</pre></div><p>
   By default, the two cron jobs are enabled on controller node 1 only, not on
   the other two nodes. When controller node 1 is down or has failed for any
   reason, these two cron jobs must be manually set up on one of the other two
   nodes.
  </p></div></div></div><div class="chapter " id="ops-managing-compute"><div class="titlepage"><div><div><h1 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Compute</span> <a title="Permalink" class="permalink" href="#ops-managing-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_compute.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_compute.xml</li><li><span class="ds-label">ID: </span>ops-managing-compute</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#aggregates"><span class="number">5.1 </span><span class="name">Managing Compute Hosts using Aggregates and Scheduler Filters</span></a></span></dt><dt><span class="section"><a href="#topic-vhs-12v-vw"><span class="number">5.2 </span><span class="name">Using Flavor Metadata to Specify CPU Model</span></a></span></dt><dt><span class="section"><a href="#topic-pqr-lyx-yw"><span class="number">5.3 </span><span class="name">Forcing CPU and RAM Overcommit Settings</span></a></span></dt><dt><span class="section"><a href="#enabling-the-nova-resize"><span class="number">5.4 </span><span class="name">Enabling the Nova Resize and Migrate Features</span></a></span></dt><dt><span class="section"><a href="#resize"><span class="number">5.5 </span><span class="name">Enabling ESX Compute Instance(s) Resize Feature</span></a></span></dt><dt><span class="section"><a href="#configure-glance"><span class="number">5.6 </span><span class="name">Configuring the Image Service</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Compute service.
 </p><div class="sect1" id="aggregates"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Compute Hosts using Aggregates and Scheduler Filters</span> <a title="Permalink" class="permalink" href="#aggregates">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-creating_aggregates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-creating_aggregates.xml</li><li><span class="ds-label">ID: </span>aggregates</li></ul></div></div></div></div><p>
  OpenStack Nova has the concepts of availability zones and host aggregates
  that enable you to segregate your compute hosts. Availability zones are used
  to specify logical separation within your cloud based on the physical
  isolation or redundancy you have set up. Host aggregates are used to group
  compute hosts together based upon common features, such as operation system.
  For more information, read this topic.
 </p><p>
  OpenStack Nova has the concepts of availability zones and host aggregates
  that enable you to segregate your Compute hosts. Availability zones are used
  to specify logical separation within your cloud based on the physical
  isolation or redundancy you have set up. Host aggregates are used to group
  compute hosts together based upon common features, such as operation system.
  For more information, see
  <a class="link" href="http://docs.openstack.org/openstack-ops/content/scaling.html" target="_blank">Scaling
  and Segregating your Cloud</a>.
 </p><p>
  The Nova scheduler also has a filter scheduler, which supports both filtering
  and weighting to make decisions on where new compute instances should be
  created. For more information, see
  <a class="link" href="http://docs.openstack.org/developer/nova/filter_scheduler.html" target="_blank">Filter
  Scheduler</a> and
  <a class="link" href="http://docs.openstack.org/mitaka/config-reference/compute/scheduler.html" target="_blank">Scheduling</a>.
 </p><p>
  This document is going to show you how to set up both a Nova host aggregate
  and configure the filter scheduler to further segregate your compute hosts.
 </p><div class="sect2" id="create-agg"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Nova Aggregate</span> <a title="Permalink" class="permalink" href="#create-agg">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-creating_aggregates.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-creating_aggregates.xml</li><li><span class="ds-label">ID: </span>create-agg</li></ul></div></div></div></div><p>
   These steps will show you how to create a Nova aggregate and how to add a
   compute host to it. You can run these steps on any machine that contains the
   NovaClient that also has network access to your cloud environment. These
   requirements are met by the Cloud Lifecycle Manager.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the administrative creds:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
     List your current Nova aggregates:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova aggregate-list</pre></div></li><li class="listitem "><p>
     Create a new Nova aggregate with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova aggregate-create <em class="replaceable ">AGGREGATE-NAME</em></pre></div><p>
     If you wish to have the aggregate appear as an availability zone, then
     specify an availability zone with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova aggregate-create <em class="replaceable ">AGGREGATE-NAME</em> <em class="replaceable ">AVAILABILITY-ZONE-NAME</em></pre></div><p>
     So, for example, if you wish to create a new aggregate for your SUSE Linux Enterprise
     compute hosts and you wanted that to show up as the
     <code class="literal">SLE</code> availability zone, you could use this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova aggregate-create SLE SLE</pre></div><p>
     This would produce an output similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">+----+------+-------------------+-------+------------------+
| Id | Name | Availability Zone | Hosts | Metadata                 
+----+------+-------------------+-------+--------------------------+
| 12 | SLE  | SLE               |       | 'availability_zone=SLE'
+----+------+-------------------+-------+--------------------------+</pre></div></li><li class="listitem "><p>
     Next, you need to add compute hosts to this aggregate so you can start by
     listing your current hosts. You will want to limit the output of this
     command to only the hosts running the <code class="literal">compute</code> service,
     like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova host-list | grep compute</pre></div></li><li class="listitem "><p>
     You can then add host(s) to your aggregate with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova aggregate-add-host <em class="replaceable ">AGGREGATE-NAME</em> <em class="replaceable ">HOST</em></pre></div></li><li class="listitem "><p>
     Then you can confirm that this has been completed by listing the details
     of your aggregate:
    </p><div class="verbatim-wrap"><pre class="screen">nova aggregate-details <em class="replaceable ">AGGREGATE-NAME</em></pre></div><p>
     You can also list out your availability zones using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova availability-zone-list</pre></div></li></ol></div></div><div class="sect2" id="filters"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Nova Scheduler Filters</span> <a title="Permalink" class="permalink" href="#filters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-creating_aggregates.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-creating_aggregates.xml</li><li><span class="ds-label">ID: </span>filters</li></ul></div></div></div></div><p>
   The Nova scheduler has two filters that can help with differentiating
   between different compute hosts that we'll describe here.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Filter</th><th>Description</th></tr></thead><tbody><tr><td>AggregateImagePropertiesIsolation</td><td>
       <p>
        Isolates compute hosts based on image properties and aggregate
        metadata. You can use commas to specify multiple values for the same
        property. The filter will then ensure at least one value matches.
       </p>
      </td></tr><tr><td>AggregateInstanceExtraSpecsFilter</td><td>
       <p>
        Checks that the aggregate metadata satisfies any extra specifications
        associated with the instance type. This uses
        <code class="literal">aggregate_instance_extra_specs</code>
       </p>
      </td></tr></tbody></table></div><div id="id-1.6.7.3.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    For details about other available filters, see
    <a class="link" href="http://docs.openstack.org/developer/nova/filter_scheduler.html" target="_blank">Filter
    Scheduler</a>.
   </p></div><p>
   <span class="bold"><strong>Using the AggregateImagePropertiesIsolation
   Filter</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~/openstack/my_cloud/config/nova/nova.conf.j2</code>
     file and add <code class="literal">AggregateImagePropertiesIsolation</code> to the
     scheduler_filters section. Example below, in bold:
    </p><div class="verbatim-wrap"><pre class="screen"># Scheduler
...
scheduler_available_filters = nova.scheduler.filters.all_filters
scheduler_default_filters = AvailabilityZoneFilter,RetryFilter,ComputeFilter,
 DiskFilter,RamFilter,ImagePropertiesFilter,ServerGroupAffinityFilter,
 ServerGroupAntiAffinityFilter,ComputeCapabilitiesFilter,NUMATopologyFilter,
 <span class="bold"><strong>AggregateImagePropertiesIsolation</strong></span>
...</pre></div><p>
     Optionally, you can also add these lines:
    </p><div class="verbatim-wrap"><pre class="screen">aggregate_image_properties_isolation_namespace = &lt;a prefix string&gt;</pre></div><div class="verbatim-wrap"><pre class="screen">aggregate_image_properties_isolation_separator = &lt;a separator character&gt;</pre></div><p>
     (defaults to <code class="literal">.</code>)
    </p><p>
     If these are added, the filter will only match image properties starting
     with the name space and separator - for example, setting to
     <code class="literal">my_name_space</code> and <code class="literal">:</code> would mean the
     image property <code class="literal">my_name_space:image_type=SLE</code> matches
     metadata <code class="literal">image_type=SLE</code>, but
     <code class="literal">an_other=SLE</code> would not be inspected for a match at
     all.
    </p><p>
     If these are not added all image properties will be matched against any
     similarly named aggregate metadata.
    </p></li><li class="listitem "><p>
     Add image properties to images that should be scheduled using the above
     filter
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "editing nova schedule filters"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Run the ready deployment playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>Using the AggregateInstanceExtraSpecsFilter
   Filter</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~/openstack/my_cloud/config/nova/nova.conf.j2</code>
     file and add <code class="literal">AggregateInstanceExtraSpecsFilter</code> to the
     scheduler_filters section. Example below, in bold:
    </p><div class="verbatim-wrap"><pre class="screen"># Scheduler
...
scheduler_available_filters = nova.scheduler.filters.all_filters
 scheduler_default_filters = AvailabilityZoneFilter,RetryFilter,ComputeFilter,
 DiskFilter,RamFilter,ImagePropertiesFilter,ServerGroupAffinityFilter,
 ServerGroupAntiAffinityFilter,ComputeCapabilitiesFilter,NUMATopologyFilter,
 <span class="bold"><strong>AggregateInstanceExtraSpecsFilter</strong></span>
...</pre></div></li><li class="listitem "><p>
     There is no additional configuration needed because the following is true:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       The filter assumes <code class="literal">:</code> is a separator
      </p></li><li class="listitem "><p>
       The filter will match all simple keys in extra_specs plus all keys with
       a separator if the prefix is
       <code class="literal">aggregate_instance_extra_specs</code> - for example,
       <code class="literal">image_type=SLE</code> and
       <code class="literal">aggregate_instance_extra_specs:image_type=SLE</code> will
       both be matched against aggregate metadata
       <code class="literal">image_type=SLE</code>
      </p></li></ol></div></li><li class="listitem "><p>
     Add <code class="literal">extra_specs</code> to flavors that should be scheduled
     according to the above.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Editing nova scheduler filters"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Run the ready deployment playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="topic-vhs-12v-vw"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Flavor Metadata to Specify CPU Model</span> <a title="Permalink" class="permalink" href="#topic-vhs-12v-vw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-using_flavor_metadata.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-using_flavor_metadata.xml</li><li><span class="ds-label">ID: </span>topic-vhs-12v-vw</li></ul></div></div></div></div><p>
  <code class="literal">Libvirt</code> is a collection of software used in <span class="productname">OpenStack</span> to
  manage virtualization. It has the ability to emulate a host CPU model in a
  guest VM. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Nova, the ComputeCapabilitiesFilter limits this
  ability by checking the exact CPU model of the compute host against the
  requested compute instance model. It will only pick compute hosts that have
  the <code class="literal">cpu_model</code> requested by the instance model, and if the
  selected compute host does not have that <code class="literal">cpu_model</code>, the
  ComputeCapabilitiesFilter moves on to find another compute host that matches,
  if possible. Selecting an unavailable vCPU model may cause Nova to fail
  with <code class="literal">no valid host found</code>.
 </p><p>
  To assist, there is a Nova scheduler filter that captures
  <code class="literal">cpu_models</code> as a subset of a particular CPU family. The
  filter determines if the host CPU model is capable of emulating the guest
  CPU model by maintaining the mapping of the vCPU models and comparing it with
  the host CPU model.
 </p><p>
  There is a limitation when a particular <code class="literal">cpu_model</code> is
  specified with <code class="literal">hw:cpu_model</code> via a compute flavor: the
  <code class="literal">cpu_mode</code> will be set to <code class="literal">custom</code>. This
  mode ensures that a persistent guest virtual machine will see the same
  hardware no matter what host physical machine the guest virtual machine is
  booted on. This allows easier live migration of virtual machines. Because of
  this limitation, only some of the features of a CPU are exposed to the guest.
  Requesting particular CPU features is not supported.
 </p><div class="sect2" id="id-1.6.7.4.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Editing the flavor metadata in the Horizon dashboard</span> <a title="Permalink" class="permalink" href="#id-1.6.7.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-using_flavor_metadata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-using_flavor_metadata.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   These steps can be used to edit a flavor's metadata in the Horizon
   dashboard to add the <code class="literal">extra_specs</code> for a
   <code class="literal">cpu_model</code>:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Access the Horizon dashboard and log in with admin credentials.
    </p></li><li class="listitem "><p>
     Access the Flavors menu by (A) clicking on the menu button, (B) navigating
     to the Admin section, and then (C) clicking on Flavors:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_1.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_1.png" width="" /></a></div></div></li><li class="listitem "><p>
     In the list of flavors, choose the flavor you wish to edit and click on
     the entry under the Metadata column:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_2.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_2.png" width="" /></a></div></div><div id="id-1.6.7.4.5.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can also create a new flavor and then choose that one to edit.
     </p></div></li><li class="listitem "><p>
     In the Custom field, enter <code class="literal">hw:cpu_model</code> and then click
     on the <code class="literal">+</code> (plus) sign to continue:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_3.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_3.png" width="" /></a></div></div></li><li class="listitem "><p>
     Then you will want to enter the CPU model into the field that you wish to
     use and then click Save:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_4.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_4.png" width="" /></a></div></div></li></ol></div></div></div><div class="sect1" id="topic-pqr-lyx-yw"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forcing CPU and RAM Overcommit Settings</span> <a title="Permalink" class="permalink" href="#topic-pqr-lyx-yw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-forcing_overcommit.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-forcing_overcommit.xml</li><li><span class="ds-label">ID: </span>topic-pqr-lyx-yw</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports overcommitting of CPU and RAM resources on compute nodes.
  Overcommitting is a technique of allocating more virtualized CPUs and/or
  memory than there are physical resources.
 </p><p>
  The default settings for this are:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>cpu_allocation_ratio</td><td>16</td><td>
      <p>
       Virtual CPU to physical CPU allocation ratio which affects all CPU
       filters. This configuration specifies a global ratio for CoreFilter.
       For AggregateCoreFilter, it will fall back to this configuration value
       if no per-aggregate setting found.
      </p>
      <div id="id-1.6.7.5.4.1.5.1.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        This can be set per-compute, or if set to <code class="literal">0.0</code>, the
        value set on the scheduler node(s) will be used and defaulted to
        <code class="literal">16.0</code>.
       </p></div>
     </td></tr><tr><td>ram_allocation_ratio</td><td>1.0</td><td>
      <p>
       Virtual RAM to physical RAM allocation ratio which affects all RAM
       filters. This configuration specifies a global ratio for RamFilter. For
       AggregateRamFilter, it will fall back to this configuration value if no
       per-aggregate setting found.
      </p>
      <div id="id-1.6.7.5.4.1.5.2.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        This can be set per-compute, or if set to <code class="literal">0.0</code>, the
        value set on the scheduler node(s) will be used and defaulted to
        <code class="literal">1.5</code>.
       </p></div>
     </td></tr><tr><td>disk_allocation_ratio</td><td>1.0</td><td>
      <p>
       This is the virtual disk to physical disk allocation ratio used by the
       disk_filter.py script to determine if a host has sufficient disk space
       to fit a requested instance. A ratio greater than 1.0 will result in
       over-subscription of the available physical disk, which can be useful
       for more efficiently packing instances created with images that do not
       use the entire virtual disk,such as sparse or compressed images. It can
       be set to a value between 0.0 and 1.0 in order to preserve a percentage
       of the disk for uses other than instances.
      </p>
      <div id="id-1.6.7.5.4.1.5.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        This can be set per-compute, or if set to <code class="literal">0.0</code>, the
        value set on the scheduler node(s) will be used and defaulted to
        <code class="literal">1.0</code>.
       </p></div>
     </td></tr></tbody></table></div><div class="sect2" id="change"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the overcommit ratios for your entire environment</span> <a title="Permalink" class="permalink" href="#change">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-forcing_overcommit.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-forcing_overcommit.xml</li><li><span class="ds-label">ID: </span>change</li></ul></div></div></div></div><p>
   If you wish to change the CPU and/or RAM overcommit ratio settings for your
   entire environment then you can do so via your Cloud Lifecycle Manager with these
   steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the Nova configuration settings located in this file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/nova.conf.j2</pre></div></li><li class="listitem "><p>
     Add or edit the following lines to specify the ratios you wish to use:
    </p><div class="verbatim-wrap"><pre class="screen">cpu_allocation_ratio = 16
ram_allocation_ratio = 1.0</pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "setting Nova overcommit settings"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="enabling-the-nova-resize"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the Nova Resize and Migrate Features</span> <a title="Permalink" class="permalink" href="#enabling-the-nova-resize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-enabling_resize.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize.xml</li><li><span class="ds-label">ID: </span>enabling-the-nova-resize</li></ul></div></div></div></div><p>
  The Nova resize and migrate features are disabled by default. If you wish
  to utilize these options, these steps will show you how to enable it in
  your cloud.
 </p><p>
  The two features below are disabled by default:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Resize</strong></span> - this feature allows you to
    change the size of a Compute instance by changing its flavor. See the
    <a class="link" href="http://docs.openstack.org/user-guide/cli_change_the_size_of_your_server.html" target="_blank">OpenStack
    User Guide</a> for more details on its use.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Migrate</strong></span> - read about the differences
    between "live" migration (enabled by default) and regular migration
    (disabled by default) in <a class="xref" href="#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a>.
   </p></li></ul></div><p>
  These two features are disabled by default because they require passwordless
  SSH access between Compute hosts with the user having access to the file
  systems to perform the copy.
 </p><div class="sect2" id="enable"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Nova Resize and Migrate</span> <a title="Permalink" class="permalink" href="#enable">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-enabling_resize.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize.xml</li><li><span class="ds-label">ID: </span>enable</li></ul></div></div></div></div><p>
   If you wish to enable these features, use these steps on your lifecycle
   manager. This will deploy a set of public and private SSH keys to the
   Compute hosts, allowing the <code class="literal">nova</code> user SSH access between
   each of your Compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Run the Nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml --extra-vars nova_migrate_enabled=true</pre></div></li><li class="listitem "><p>
     To ensure that the resize and migration options show up in the Horizon
     dashboard, run the Horizon reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="disable"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabling Nova Resize and Migrate</span> <a title="Permalink" class="permalink" href="#disable">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-enabling_resize.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize.xml</li><li><span class="ds-label">ID: </span>disable</li></ul></div></div></div></div><p>
   This feature is disabled by default. However, if you have previously enabled
   it and wish to re-disable it, you can use these steps on your lifecycle
   manager. This will remove the set of public and private SSH keys that were
   previously added to the Compute hosts, removing the <code class="literal">nova</code>
   users SSH access between each of your Compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Run the Nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml --extra-vars nova_migrate_enabled=false</pre></div></li><li class="listitem "><p>
     To ensure that the resize and migrate options are removed from the Horizon
     dashboard, run the Horizon reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="resize"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling ESX Compute Instance(s) Resize Feature</span> <a title="Permalink" class="permalink" href="#resize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-enabling_resize_esx_compute.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize_esx_compute.xml</li><li><span class="ds-label">ID: </span>resize</li></ul></div></div></div></div><p>
  The resize of ESX compute instance is disabled by default. If you want to
  utilize this option, these steps will show you how to configure and enable
  it in your cloud.
 </p><p>
  The following feature is disabled by default:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Resize</strong></span> - this feature allows you to
    change the size of a Compute instance by changing its flavor. See the
    <a class="link" href="http://docs.openstack.org/user-guide/cli_change_the_size_of_your_server.html" target="_blank">OpenStack
    User Guide</a> for more details on its use.
   </p></li></ul></div><div class="sect2" id="id-1.6.7.7.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="#id-1.6.7.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute-enabling_resize_esx_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize_esx_compute.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you want to configure and re-size ESX compute instance(s), perform the
   following steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~ /openstack/my_cloud/config/nova/nova.conf.j2</code> to
     add the following parameter under <span class="bold"><strong>Policy</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen"># Policy
allow_resize_to_same_host=True</pre></div></li><li class="listitem "><p>
     Commit your configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "&lt;commit message&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     By default the nova resize feature is disabled. To enable nova resize,
     refer to <a class="xref" href="#enabling-the-nova-resize" title="5.4. Enabling the Nova Resize and Migrate Features">Section 5.4, “Enabling the Nova Resize and Migrate Features”</a>.
    </p><p>
     By default an ESX console log is not set up. For more details about its
     setup, refer to <a class="link" href="https://docs.openstack.org/nova/pike/admin/configuration/hypervisor-vmware.html" target="_blank">VMware 
      vSphere</a>.
   </p></li></ol></div></div></div><div class="sect1" id="configure-glance"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Image Service</span> <a title="Permalink" class="permalink" href="#configure-glance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-image-configure_glance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-image-configure_glance.xml</li><li><span class="ds-label">ID: </span>configure-glance</li></ul></div></div></div></div><p>
  The image service, based on <span class="productname">OpenStack</span> Glance, works out of the box and does
  not need any special configuration. However, we show you how to enable
  Glance image caching as well as how to configure your environment to allow
  the Glance copy-from feature if you choose to do so. A few features
  detailed below will require some additional configuration if you choose to
  use them.
 </p><div id="image-warning" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   Glance images are assigned IDs upon creation, either automatically or
   specified by the user. The ID of an image should be unique, so if a user
   assigns an ID which already exists, a conflict (409) will occur.
  </p><p>
   This only becomes a problem if users can publicize or share images with
   others. If users can share images AND cannot publicize images then your
   system is not vulnerable. If the system has also been purged (via
   <code class="literal">glance-manage db purge</code>) then it is possible for deleted
   image IDs to be reused.
  </p><p>
   If deleted image IDs can be reused then recycling of public and shared
   images becomes a possibility. This means that a new (or modified) image can
   replace an old image, which could be malicious.
  </p><p>
   If this is a problem for you, please contact <span class="phrase"><span class="phrase">Sales Engineering</span></span>.
  </p></div><div class="sect2" id="idg-all-operations-image-configure-glance-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to enable Glance image caching</span> <a title="Permalink" class="permalink" href="#idg-all-operations-image-configure-glance-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-image-configure_glance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-image-configure_glance.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-image-configure-glance-xml-6</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, by default, the Glance image caching option is not
   enabled. You have the option to have image caching enabled and these steps
   will show you how to do that.
  </p><p>
   The main benefits to using image caching is that it will allow the Glance
   service to return the images faster and it will cause less load on other
   services to supply the image.
  </p><p>
   In order to use the image caching option you will need to supply a logical
   volume for the service to use for the caching.
  </p><p>
   If you wish to use the Glance image caching option, you will see the
   section below in your
   <code class="literal">~/openstack/my_cloud/definition/data/disks_controller.yml</code>
   file. You will specify the mount point for the logical volume you wish to
   use for this.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit your
     <code class="literal">~/openstack/my_cloud/definition/data/disks_controller.yml</code>
     file and specify the volume and mount point for your <code class="literal">glance-cache</code>. Here is
     an example:
    </p><div class="verbatim-wrap"><pre class="screen"># Glance cache: if a logical volume with consumer usage glance-cache
# is defined Glance caching will be enabled. The logical volume can be
# part of an existing volume group or a dedicated volume group.
 - name: glance-vg
   physical-volumes:
     - /dev/sdx
   logical-volumes:
     - name: glance-cache
       size: 95%
       mount: /var/lib/glance/cache
       fstype: ext4
       mkfs-opts: -O large_file
       consumer:
         name: glance-api
         usage: glance-cache</pre></div><p>
     If you are enabling image caching during your initial installation, prior
     to running <code class="literal">site.yml</code> the first time, then continue with
     the installation steps. However, if you are making this change
     post-installation then you will need to commit your changes with the steps
     below.
    </p></li><li class="step "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the Glance reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div><p>
   An existing volume image cache is not properly deleted when Cinder
   detects the source image has changed. After updating any source image,
   delete the cache volume so that the cache is refreshed.
  </p><p>
   The volume image cache must be deleted before trying to use the associated
   source image in any other volume operations. This includes creating bootable
   volumes or booting an instance with <code class="literal">create volume</code> enabled
   and the updated image as the source image.
  </p></div><div class="sect2" id="copyfrom"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allowing the Glance copy-from option in your environment</span> <a title="Permalink" class="permalink" href="#copyfrom">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-image-configure_glance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-image-configure_glance.xml</li><li><span class="ds-label">ID: </span>copyfrom</li></ul></div></div></div></div><p>
   When creating images, one of the options you have is to copy the image from
   a remote location to your local Glance store. You do this by specifying the
   <code class="literal">--copy-from</code> option when creating the image. To use this
   feature though you need to ensure the following conditions are met:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The server hosting the Glance service must have network access to the
     remote location that is hosting the image.
    </p></li><li class="listitem "><p>
     There cannot be a proxy between Glance and the remote location.
    </p></li><li class="listitem "><p>
     The Glance v1 API must be enabled, as v2 does not currently support the
     <code class="literal">copy-from</code> function.
    </p></li><li class="listitem "><p>
     The http Glance store must be enabled in the environment, following the
     steps below.
    </p></li></ul></div><p>
   <span class="bold"><strong>Enabling the HTTP Glance Store</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/config/glance/glance-api.conf.j2</code>
     file and add <code class="literal">http</code> to the list of Glance stores in the
     <code class="literal">[glance_store]</code> section as seen below in bold:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}<span class="bold"><strong>, http</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Glance reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Run the Horizon reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="ops-managing-esx"><div class="titlepage"><div><div><h1 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing ESX</span> <a title="Permalink" class="permalink" href="#ops-managing-esx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_esx.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_esx.xml</li><li><span class="ds-label">ID: </span>ops-managing-esx</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic-odg-33x-rt"><span class="number">6.1 </span><span class="name">Networking for ESXi Hypervisor (OVSvApp)</span></a></span></dt><dt><span class="section"><a href="#verify-neutron"><span class="number">6.2 </span><span class="name">Validating the Neutron Installation</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-cluster"><span class="number">6.3 </span><span class="name">Removing a Cluster from the Compute Resource Pool</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-esxi-host"><span class="number">6.4 </span><span class="name">Removing an ESXi Host from a Cluster</span></a></span></dt><dt><span class="section"><a href="#sec-esx-debug"><span class="number">6.5 </span><span class="name">Configuring Debug Logging</span></a></span></dt><dt><span class="section"><a href="#topic-ijt-dyh-rt"><span class="number">6.6 </span><span class="name">Making Scale Configuration Changes</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-monitoring-vcenter-clusters-xml-1"><span class="number">6.7 </span><span class="name">Monitoring vCenter Clusters</span></a></span></dt><dt><span class="section"><a href="#ovsvapp-monitoring"><span class="number">6.8 </span><span class="name">Monitoring Integration with OVSvApp Appliance</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the ESX service.
 </p><div class="sect1" id="topic-odg-33x-rt"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking for ESXi Hypervisor (OVSvApp)</span> <a title="Permalink" class="permalink" href="#topic-odg-33x-rt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-network_esx_ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-network_esx_ovsvapp.xml</li><li><span class="ds-label">ID: </span>topic-odg-33x-rt</li></ul></div></div></div></div><p>
  To provide the network as a service for tenant VM's hosted on ESXi
  Hypervisor, a service VM named OVSvApp VM is deployed on each ESXi Hypervisor
  within a cluster managed by OpenStack Nova, as shown in the following figure.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-esx_ovsvapp.png" target="_blank"><img src="images/media-esx-esx_ovsvapp.png" width="" /></a></div></div><p>
  The OVSvApp VM runs SLES as a guest operating system, and has Open vSwitch
  2.1.0 or above installed. It also runs an agent called OVSvApp agent, which
  is responsible for dynamically creating the port groups for the tenant VMs
  and manages OVS bridges, which contain the flows related to security groups
  and L2 networking.
 </p><p>
  To facilitate fault tolerance and mitigation of data path loss for tenant
  VMs, run the <span class="bold"><strong>neutron-ovsvapp-agent-monitor</strong></span>
  process as part of the <span class="bold"><strong>neutron-ovsvapp-agent
  service</strong></span>, responsible for monitoring the Open vSwitch module within
  the OVSvApp VM. It also uses a <code class="literal">nginx</code> server to provide the
  health status of the Open vSwitch module to the Neutron server for mitigation
  actions. There is a mechanism to keep the
  <span class="bold"><strong>neutron-ovsvapp-agent service</strong></span> alive through
  a <code class="literal">systemd</code> script.
 </p><p>
  When a OVSvApp Service VM crashes, an agent monitoring mechanism starts a
  cluster mitigation process. You can mitigate data path traffic loss for VMs
  on the failed ESX host in that cluster by putting the failed ESX host in the
  maintenance mode. This, in turn, triggers the vCenter DRS migrates tenant VMs
  to other ESX hosts within the same cluster. This ensures data path continuity
  of tenant VMs traffic.
 </p><p>
  <span class="bold"><strong>To View Cluster Mitigation</strong></span>
 </p><p>
  An administrator can view the cluster mitigation status using the following
  commands.
 </p><div class="orderedlist " id="ol-cmz-fjx-rt"><ol class="orderedlist" type="1"><li class="listitem "><p>
    <code class="literal">neutron ovsvapp-mitigated-cluster-list</code>
   </p><p>
    Lists all the clusters where at least one round of host mitigation has
    happened.
   </p><p>
    Example:
   </p><div class="verbatim-wrap"><pre class="screen">neutron ovsvapp-mitigated-cluster-list
+----------------+--------------+-----------------------+---------------------------+
| vcenter_id     | cluster_id   | being_mitigated       | threshold_reached         |
+----------------+--------------+-----------------------+---------------------------+
| vcenter1       | cluster1     | True                  | False                     |
| vcenter2       | cluster2     | False                 | True                      |
+---------------+------------+-----------------+------------------------------------+</pre></div></li><li class="listitem "><p>
    <code class="literal">neutron ovsvapp-mitigated-cluster-show --vcenter-id
    &lt;VCENTER_ID&gt; --cluster-id &lt;CLUSTER_ID&gt;</code>
   </p><p>
    Shows the status of a particular cluster.
   </p><p>
    Example :
   </p><div class="verbatim-wrap"><pre class="screen">neutron ovsvapp-mitigated-cluster-show --vcenter-id vcenter1 --cluster-id cluster1
+---------------------------+-------------+
| Field                     | Value       |
+---------------------------+-------------+
| being_mitigated           | True        |
| cluster_id                | cluster1    |
| threshold_reached         | False       |
| vcenter_id                | vcenter1    |
+---------------------------+-------------+</pre></div><p>
    There can be instances where a triggered mitigation may not succeed and the
    neutron server is not informed of such failure (for example, if the
    selected agent which had to mitigate the host, goes down before finishing
    the task). In this case, the cluster will be locked. To unlock the cluster
    for further mitigations, use the update command.
   </p></li><li class="listitem "><p>
    <code class="literal">neutron ovsvapp-mitigated-cluster-update --vcenter-id
    &lt;VCENTER_ID&gt; --cluster-id &lt;CLUSTER_ID&gt;</code>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Update the status of a mitigated cluster:
     </p><p>
      Modify the values of <span class="bold"><strong>being-mitigated</strong></span>
      from <span class="bold"><strong>True</strong></span> to
      <span class="bold"><strong>False</strong></span> to unlock the cluster.
     </p><p>
      Example:
     </p><div class="verbatim-wrap"><pre class="screen">neutron ovsvapp-mitigated-cluster-update --vcenter-id vcenter1 --cluster-id cluster1 --being-mitigated False</pre></div></li><li class="listitem "><p>
      Update the threshold value:
     </p><p>
      Update the threshold-reached value to
      <span class="bold"><strong>True</strong></span>, if no further migration is
      required in the selected cluster.
     </p><p>
      Example :
     </p><div class="verbatim-wrap"><pre class="screen">neutron ovsvapp-mitigated-cluster-update --vcenter-id vcenter1 --cluster-id cluster1 --being-mitigated False --threshold-reached True</pre></div></li></ul></div><p>
    <span class="bold"><strong>Rest API</strong></span>
   </p><div class="itemizedlist " id="ul-rkp-4kx-rt"><ul class="itemizedlist"><li class="listitem "><div class="verbatim-wrap"><pre class="screen">curl -i -X GET http://&lt;ip&gt;:9696/v2.0/ovsvapp_mitigated_clusters \
  -H "User-Agent: python-neutronclient" -H "Accept: application/json" -H \
  "X-Auth-Token: &lt;token_id&gt;"</pre></div></li></ul></div></li></ol></div><div class="sect2" id="id-1.6.8.3.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#id-1.6.8.3.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-network_esx_ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-network_esx_ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For more information on the Networking for ESXi Hypervisor (OVSvApp), see
   the following references:
  </p><div class="itemizedlist " id="ul-b2c-n2c-vt"><ul class="itemizedlist"><li class="listitem "><p>
     VBrownBag session in Vancouver OpenStack Liberty Summit:
    </p><p>
     <a class="link" href="https://www.youtube.com/watch?v=icYA_ixhwsM&amp;feature=youtu.be" target="_blank">https://www.youtube.com/watch?v=icYA_ixhwsM&amp;feature=youtu.be</a>
    </p></li><li class="listitem "><p>
     Wiki Link:
    </p><p>
     <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">https://wiki.openstack.org/wiki/Neutron/Networking-vSphere</a>
    </p></li><li class="listitem "><p>
     Codebase:
    </p><p>
     <a class="link" href="https://github.com/openstack/networking-vsphere/" target="_blank">https://github.com/openstack/networking-vsphere/</a>
    </p></li><li class="listitem "><p>
     Whitepaper:
    </p><p>
     <a class="link" href="https://github.com/hp-networking/ovsvapp/blob/master/OVSvApp_Solution.pdf" target="_blank">https://github.com/hp-networking/ovsvapp/blob/master/OVSvApp_Solution.pdf</a>
    </p></li></ul></div></div></div><div class="sect1" id="verify-neutron"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Validating the Neutron Installation</span> <a title="Permalink" class="permalink" href="#verify-neutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-enable_new_cluster_compute_resource.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-enable_new_cluster_compute_resource.xml</li><li><span class="ds-label">ID: </span>verify-neutron</li></ul></div></div></div></div><p>
   You can validate that the ESX compute cluster is added to the cloud
   successfully using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"># neutron agent-list

+------------------+----------------------+-----------------------+-------------------+-------+----------------+---------------------------+
| id               | agent_type           | host                  | availability_zone | alive | admin_state_up | binary                    |
+------------------+----------------------+-----------------------+-------------------+-------+----------------+---------------------------+
| 05ca6ef...999c09 | L3 agent             | doc-cp1-comp0001-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
| 3b9179a...28e2ef | Metadata agent       | doc-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| 3d756d7...a719a2 | Loadbalancerv2 agent | doc-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-lbaasv2-agent     |
| 4e8f84f...c9c58f | Metadata agent       | doc-cp1-comp0002-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| 55a5791...c17451 | L3 agent             | doc-cp1-c1-m1-mgmt    | nova              | :-)   | True           | neutron-vpn-agent         |
| 5e3db8f...87f9be | Open vSwitch agent   | doc-cp1-c1-m1-mgmt    |                   | :-)   | True           | neutron-openvswitch-agent |
| 6968d9a...b7b4e9 | L3 agent             | doc-cp1-c1-m2-mgmt    | nova              | :-)   | True           | neutron-vpn-agent         |
| 7b02b20...53a187 | Metadata agent       | doc-cp1-c1-m2-mgmt    |                   | :-)   | True           | neutron-metadata-agent    |
| 8ece188...5c3703 | Open vSwitch agent   | doc-cp1-comp0002-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
| 8fcb3c7...65119a | Metadata agent       | doc-cp1-c1-m1-mgmt    |                   | :-)   | True           | neutron-metadata-agent    |
| 9f48967...36effe | OVSvApp agent        | doc-cp1-comp0002-mgmt |                   | :-)   | True           | ovsvapp-agent             |
| a2a0b78...026da9 | Open vSwitch agent   | doc-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
| a2fbd4a...28a1ac | DHCP agent           | doc-cp1-c1-m2-mgmt    | nova              | :-)   | True           | neutron-dhcp-agent        |
| b2428d5...ee60b2 | DHCP agent           | doc-cp1-c1-m1-mgmt    | nova              | :-)   | True           | neutron-dhcp-agent        |
| c0983a6...411524 | Open vSwitch agent   | doc-cp1-c1-m2-mgmt    |                   | :-)   | True           | neutron-openvswitch-agent |
| c32778b...a0fc75 | L3 agent             | doc-cp1-comp0002-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
+------------------+----------------------+-----------------------+-------------------+-------+----------------+---------------------------+</pre></div></div><div class="sect1" id="sec-esx-remove-cluster"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Cluster from the Compute Resource Pool</span> <a title="Permalink" class="permalink" href="#sec-esx-remove-cluster">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span>sec-esx-remove-cluster</li></ul></div></div></div></div><div class="sect2" id="idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span>idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6</li></ul></div></div></div></div><p>
   Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs
   of that ESX cluster before deleting the VMs. These IP address and Hostname
   will be used to cleanup Monasca alarm definitions.
  </p><p>
   Perform the following steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to vSphere client.
    </p></li><li class="listitem "><p>
     Select the ovsvapp node running on each ESXi host and click
     <span class="bold"><strong>Summary</strong></span> tab as shown in the following
     example.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-esx_hostname.png" target="_blank"><img src="images/media-esx-esx_hostname.png" width="" /></a></div></div><p>
     Similarly you can retrieve the compute-proxy node information.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-esx_cluster2.png" target="_blank"><img src="images/media-esx-esx_cluster2.png" width="" /></a></div></div></li></ol></div></div><div class="sect2" id="id-1.6.8.5.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing an existing cluster from the compute resource pool</span> <a title="Permalink" class="permalink" href="#id-1.6.8.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to remove an existing cluster from the compute
   resource pool.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Run the following command to check for the instances launched in that
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"># nova list --host &lt;hostname&gt;
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 80e54965-758b-425e-901b-9ea756576331 | VM1  | ACTIVE | -          | Running     | private=10.0.0.2 |
+--------------------------------------+------+--------+------------+-------------+------------------+</pre></div><p>
     where:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>hostname</strong></span>: Specifies hostname of the
       compute proxy present in that cluster.
      </p></li></ul></div></li><li class="listitem "><p>
     Delete all instances spawned in that cluster:
    </p><div class="verbatim-wrap"><pre class="screen"># nova delete &lt;server&gt; [&lt;server ...&gt;]</pre></div><p>
     where:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>server</strong></span>: Specifies the name or ID of
       server (s)
      </p></li></ul></div><p>
     OR
    </p><p>
     Migrate all instances spawned in that cluster.
    </p><div class="verbatim-wrap"><pre class="screen"># nova migrate &lt;server&gt;</pre></div></li><li class="listitem "><p>
     Run the following playbooks for stop the Compute (Nova) and Networking
     (Neutron) services:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts nova-stop --limit &lt;hostname&gt;;
ansible-playbook -i hosts/verb_hosts neutron-stop --limit &lt;hostname&gt;;</pre></div><p>
     where:
    </p><div class="itemizedlist " id="ul-hll-qrh-rt"><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>hostname</strong></span>: Specifies hostname of the
       compute proxy present in that cluster.
      </p></li></ul></div></li></ol></div></div><div class="sect2" id="id-1.6.8.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cleanup Monasca Agent for OVSvAPP Service</span> <a title="Permalink" class="permalink" href="#id-1.6.8.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to cleanup Monasca agents for ovsvapp-agent
   service.
  </p><div class="orderedlist " id="idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-14"><ol class="orderedlist" type="1"><li class="listitem "><p>
     If Monasca-API is installed on different node, copy the
     <code class="literal">service.orsc</code> from Cloud Lifecycle Manager to Monasca API
     server.
    </p><div class="verbatim-wrap"><pre class="screen">scp service.orsc $USER@ardana-cp1-mtrmon-m1-mgmt:</pre></div></li><li class="listitem "><p>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-mtrmon-m1-mgmt</pre></div></li><li class="listitem "><p>
     Edit <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to
     remove the reference to the OVSvAPP you removed. This requires
     <code class="command">sudo</code> access.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</pre></div><p>
     A sample of <code class="literal">host_alive.yaml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: esx-cp1-esx-ovsvapp0001-mgmt
  name: esx-cp1-esx-ovsvapp0001-mgmt ping
  target_hostname: esx-cp1-esx-ovsvapp0001-mgmt</pre></div><p>
     where <em class="replaceable ">HOST_NAME</em> and
     <em class="replaceable ">TARGET_HOSTNAME</em> is mentioned at the DNS name
     field at the vSphere client. (Refer to
     <a class="xref" href="#idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6" title="6.3.1. Prerequisites">Section 6.3.1, “Prerequisites”</a>).
    </p></li><li class="listitem "><p>
     After removing the reference on each of the Monasca API servers, restart
     the monasca-agent on each of those servers by executing the following
     command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div></li><li class="listitem "><p>
     With the OVSvAPP references removed and the monasca-agent restarted, you
     can delete the corresponding alarm to complete the cleanup process. We
     recommend using the Monasca CLI which is installed on each of your Monasca
     API servers by default. Execute the following command from the Monasca API
     server (for example: <code class="literal">ardana-cp1-mtrmon-mX-mgmt</code>).
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</pre></div><p>
     For example: You can execute the following command to get the alarm ID, if
     the OVSvAPP appears as a preceding example.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</pre></div></li><li class="listitem "><p>
     Delete the Monasca alaram.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm</pre></div><p>
     After deleting the alarms and updating the monasca-agent configuration,
     those alarms will be removed from the Operations Console UI. You can login to
     Operations Console and view the status.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.8.5.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing the Compute Proxy from Monitoring</span> <a title="Permalink" class="permalink" href="#id-1.6.8.5.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once you have removed the Compute proxy, the alarms against them will still
   trigger. Therefore to resolve this, you must perform the following steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-mtrmon-m1-mgmt</pre></div></li><li class="listitem "><p>
     Edit <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to
     remove the reference to the Compute proxy you removed. This requires
     <code class="command">sudo</code> access.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</pre></div><p>
     A sample of <code class="literal">host_alive.yaml</code> file.
    </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-comp0001-mgmt
  name: MCP-VCP-cpesx-esx-comp0001-mgmt ping</pre></div></li><li class="listitem "><p>
     Once you have removed the references on each of your Monasca API servers,
     execute the following command to restart the monasca-agent on each of
     those servers.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div></li><li class="listitem "><p>
     With the Compute proxy references removed and the monasca-agent restarted,
     delete the corresponding alarm to complete this process. complete the
     cleanup process. We recommend using the Monasca CLI which is installed on
     each of your Monasca API servers by default.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname= &lt;compute node deleted&gt;</pre></div><p>
     For example: You can execute the following command to get the alarm ID, if
     the Compute proxy appears as a preceding example.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</pre></div></li><li class="listitem "><p>
     Delete the Monasca alarm
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div></li></ol></div></div><div class="sect2" id="sec-esx-clean-monasca"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cleaning the Monasca Alarms Related to ESX Proxy and vCenter Cluster</span> <a title="Permalink" class="permalink" href="#sec-esx-clean-monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span>sec-esx-clean-monasca</li></ul></div></div></div></div><p>
   Perform the following procedure:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="st-esx-clean-monasca-alarm"><p>
     Using the ESX proxy hostname, execute the following command to list all
     alarms.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=<em class="replaceable ">COMPUTE_NODE_DELETED</em></pre></div><p>
     where <em class="replaceable ">COMPUTE_NODE_DELETED</em> - hostname is
     taken from the vSphere client (refer to
     <a class="xref" href="#idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6" title="6.3.1. Prerequisites">Section 6.3.1, “Prerequisites”</a>).
    </p><div id="id-1.6.8.5.6.3.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Ensure to make a note of all the alarm IDs that is displayed after
      executing the preceding command.
     </p></div><p>
     For example, the compute proxy hostname is
     <code class="literal">MCP-VCP-cpesx-esx-comp0001-mgmt</code>.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=MCP-VCP-cpesx-esx-comp0001-mgmt
ardana@R28N6340-701-cp1-c1-m1-mgmt:~$ monasca alarm-list --metric-dimensions hostname=R28N6340-701-cp1-esx-comp0001-mgmt
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name  | metric_name            | metric_dimensions                                | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| 02342bcb-da81-40db-a262-09539523c482 | 3e302297-0a36-4f0e-a1bd-03402b937a4e | HTTP Status            | http_status            | service: compute                                 | HIGH     | OK    | None            | None | 2016-11-11T06:58:11.717Z | 2016-11-11T06:58:11.717Z | 2016-11-10T08:55:45.136Z |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | url: https://10.244.209.9:8774                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | component: nova-api                              |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
| 04cb36ce-0c7c-4b4c-9ebc-c4011e2f6c0a | 15c593de-fa54-4803-bd71-afab95b980a4 | Disk Usage             | disk.space_used_perc   | mount_point: /proc/sys/fs/binfmt_misc            | HIGH     | OK    | None            | None | 2016-11-10T08:52:52.886Z | 2016-11-10T08:52:52.886Z | 2016-11-10T08:51:29.197Z |
|                                      |                                      |                        |                        | service: system                                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | device: systemd-1                                |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</pre></div></li><li class="step "><p>
     Delete the alarm using the alarm IDs.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div><p>
     This step has to be performed for all alarm IDs listed from the preceding
     step (<a class="xref" href="#st-esx-clean-monasca-alarm" title="Step 1">Step 1</a>).
    </p><p>
     For Example:
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete 1cc219b1-ce4d-476b-80c2-0cafa53e1a12</pre></div></li></ol></div></div></div></div><div class="sect1" id="sec-esx-remove-esxi-host"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing an ESXi Host from a Cluster</span> <a title="Permalink" class="permalink" href="#sec-esx-remove-esxi-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span>sec-esx-remove-esxi-host</li></ul></div></div></div></div><p>
  This topic describes how to remove an existing ESXi host from a cluster and
  clean up of services for OVSvAPP VM.
 </p><div id="id-1.6.8.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   Before performing this procedure, wait until VCenter migrates all the tenant
   VMs to other active hosts in that same cluster.
  </p></div><div class="sect2" id="sec-esxi-host-pre"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisite</span> <a title="Permalink" class="permalink" href="#sec-esxi-host-pre">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span>sec-esxi-host-pre</li></ul></div></div></div></div><p>
   Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs of
   that ESX cluster before deleting the VMs. These IP address and Hostname will
   be used to clean up Monasca alarm definitions.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to vSphere client.
    </p></li><li class="listitem "><p>
     Select the ovsvapp node running on the ESXi host and click
     <span class="bold"><strong>Summary</strong></span> tab.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.8.6.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="#id-1.6.8.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Right-click and put the host in the maintenance mode. This will
     automatically migrate all the tenant VMs except OVSvApp.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_maintenance.png" target="_blank"><img src="images/media-esx-eon_maintenance.png" width="" /></a></div></div></li><li class="listitem "><p>
     Cancel the maintenance mode task.
    </p></li><li class="listitem "><p>
     Right-click the <span class="bold"><strong>ovsvapp VM (IP Address)</strong></span>
     node, select <span class="bold"><strong>Power</strong></span>, and then click
     <span class="bold"><strong>Power Off</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_poweroff_ovsvapp.png" target="_blank"><img src="images/media-esx-eon_poweroff_ovsvapp.png" width="" /></a></div></div></li><li class="listitem "><p>
     Right-click the node and then click <span class="bold"><strong>Delete from
     Disk</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_delete_ovsvapp.png" target="_blank"><img src="images/media-esx-eon_delete_ovsvapp.png" width="" /></a></div></div></li><li class="listitem "><p>
     Right-click the <span class="bold"><strong>Host</strong></span>, and then click
     <span class="bold"><strong>Enter Maintenance Mode</strong></span>.
    </p></li><li class="listitem "><p>
     Disconnect the VM. Right-click the VM, and then click
     <span class="bold"><strong>Disconnect</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_disconnect_maintenance.png" target="_blank"><img src="images/media-esx-eon_disconnect_maintenance.png" width="" /></a></div></div></li></ol></div><p>
   The ESXi node is removed from the vCenter.
  </p></div><div class="sect2" id="id-1.6.8.6.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean up Neutron Agent for OVSvAPP Service</span> <a title="Permalink" class="permalink" href="#id-1.6.8.6.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After removing ESXi node from a vCenter, perform the following procedure to
   clean up neutron agents for ovsvapp-agent service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the credentials.
    </p><div class="verbatim-wrap"><pre class="screen">source service.osrc</pre></div></li><li class="listitem "><p>
     Execute the following command.
    </p><div class="verbatim-wrap"><pre class="screen">neutron agent-list | grep &lt;OVSvapp hostname&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">neutron agent-list | grep MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
| 92ca8ada-d89b-43f9-b941-3e0cd2b51e49 | OVSvApp Agent      | MCP-VCP-cpesx-esx-ovsvapp0001-mgmt |                   | :-)   | True           | ovsvapp-agent             |</pre></div></li><li class="listitem "><p>
     Delete the OVSvAPP agent.
    </p><div class="verbatim-wrap"><pre class="screen">neutron agent-delete &lt;Agent -ID&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">neutron agent-delete 92ca8ada-d89b-43f9-b941-3e0cd2b51e49</pre></div></li></ol></div><p>
   If you have more than one host, perform the preceding procedure for all the
   hosts.
  </p></div><div class="sect2" id="id-1.6.8.6.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean up Monasca Agent for OVSvAPP Service</span> <a title="Permalink" class="permalink" href="#id-1.6.8.6.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to clean up Monasca agents for ovsvapp-agent
   service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     If Monasca-API is installed on different node, copy the
     <code class="literal">service.orsc</code> from Cloud Lifecycle Manager to Monasca API
     server.
    </p><div class="verbatim-wrap"><pre class="screen">scp service.orsc $USER@ardana-cp1-mtrmon-m1-mgmt:</pre></div></li><li class="listitem "><p>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-mtrmon-m1-mgmt</pre></div></li><li class="listitem "><p>
     Edit <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to
     remove the reference to the OVSvAPP you removed. This requires
     <code class="command">sudo</code> access.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</pre></div><p>
     A sample of <code class="literal">host_alive.yaml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
  name: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt ping
  target_hostname: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</pre></div><p>
     where <code class="literal">host_name</code> and
     <code class="literal">target_hostname</code> are mentioned at the DNS name field at
     the vSphere client.
     (Refer to <a class="xref" href="#sec-esxi-host-pre" title="6.4.1. Prerequisite">Section 6.4.1, “Prerequisite”</a>).
    </p></li><li class="listitem "><p>
     After removing the reference on each of the Monasca API servers, restart
     the monasca-agent on each of those servers by executing the following
     command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div></li><li class="listitem "><p>
     With the OVSvAPP references removed and the monasca-agent restarted, you
     can delete the corresponding alarm to complete the cleanup process. We
     recommend using the Monasca CLI which is installed on each of your Monasca
     API servers by default. Execute the following command from the Monasca API
     server (for example:
     <code class="literal">ardana-cp1-mtrmon-mX-mgmt</code>).
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</pre></div><p>
     For example: You can execute the following command to get the alarm ID, if
     the OVSvAPP appears as a preceding example.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</pre></div></li><li class="listitem "><p>
     Delete the Monasca alaram.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm</pre></div><p>
     After deleting the alarms and updating the monasca-agent configuration,
     those alarms will be removed from the Operations Console UI. You can login to
     Operations Console and view the status.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.8.6.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean up the entries of OVSvAPP VM from /etc/host</span> <a title="Permalink" class="permalink" href="#id-1.6.8.6.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to clean up the entries of OVSvAPP VM from
   <code class="literal">/etc/host</code>.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit <code class="literal">/etc/host</code>.
    </p><div class="verbatim-wrap"><pre class="screen">vi /etc/host</pre></div><p>
     For example: <code class="literal">MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</code> VM is
     present in the <code class="literal">/etc/host</code>.
    </p><div class="verbatim-wrap"><pre class="screen">192.168.86.17    MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</pre></div></li><li class="listitem "><p>
     Delete the OVSvAPP entries from <code class="literal">/etc/host</code>.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.8.6.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the OVSVAPP VM from the servers.yml and pass_through.yml files and run the Configuration Processor</span> <a title="Permalink" class="permalink" href="#id-1.6.8.6.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Complete these steps from the Cloud Lifecycle Manager to remove the OVSvAPP VM:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="listitem "><p>
     Edit <code class="literal">servers.yml</code> file to remove references to the
     OVSvAPP VM(s) you want to remove:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">- ip-addr:192.168.86.17
  server-group: AZ1    role:
  OVSVAPP-ROLE    id:
  6afaa903398c8fc6425e4d066edf4da1a0f04388</pre></div></li><li class="listitem "><p>
     Edit <code class="literal">~/openstack/my_cloud/definition/data/pass_through.yml</code>
     file to remove the OVSvAPP VM references using the server-id above section
     to find the references.
    </p><div class="verbatim-wrap"><pre class="screen">- data:
  vmware:
  vcenter_cluster: Clust1
  cluster_dvs_mapping: 'DC1/host/Clust1:TRUNK-DVS-Clust1'
  esx_hostname: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
  vcenter_id: 0997E2ED9-5E4F-49EA-97E6-E2706345BAB2
id: 6afaa903398c8fc6425e4d066edf4da1a0f04388</pre></div></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "Remove ESXi host &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor. You may want to use the
     <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> switches to free up the resources
     when running the configuration processor. See
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span> for more details.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.6.8.6.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove Distributed Resource Scheduler (DRS) Rules</span> <a title="Permalink" class="permalink" href="#id-1.6.8.6.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to remove DRS rules, which is added by
   OVSvAPP installer to ensure that OVSvAPP does not get migrated to other
   hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to vCenter.
    </p></li><li class="listitem "><p>
     Right click on cluster and select <span class="bold"><strong>Edit
     settings</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-drs-rule1.png" target="_blank"><img src="images/media-esx-drs-rule1.png" width="" /></a></div></div><p>
     A cluster settings page appears.
    </p></li><li class="listitem "><p>
     Click <span class="bold"><strong>DRS Groups Manager</strong></span> on the left hand
     side of the pop-up box. Select the group which is created for deleted
     OVSvAPP and click Remove.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-drs-group2.png" target="_blank"><img src="images/media-esx-drs-group2.png" width="" /></a></div></div></li><li class="listitem "><p>
     Click <span class="bold"><strong>Rules</strong></span> on the left hand side of the
     pop-up box and select the checkbox for deleted OVSvAPP and click
     <span class="bold"><strong>Remove</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-rules3.png" target="_blank"><img src="images/media-esx-rules3.png" width="" /></a></div></div></li><li class="listitem "><p>
     Click <span class="bold"><strong>OK</strong></span>.
    </p></li></ol></div></div></div><div class="sect1" id="sec-esx-debug"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Debug Logging</span> <a title="Permalink" class="permalink" href="#sec-esx-debug">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-eon_logging.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-eon_logging.xml</li><li><span class="ds-label">ID: </span>sec-esx-debug</li></ul></div></div></div></div><div class="sect2" id="id-1.6.8.7.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Modify the OVSVAPP VM Log Level</span> <a title="Permalink" class="permalink" href="#id-1.6.8.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-eon_logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-eon_logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To change the OVSVAPP log level to DEBUG, do the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the file below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/neutron-common/templates/ovsvapp-agent-logging.conf.j2</pre></div></li><li class="listitem "><p>
     Set the logging level value of the <code class="literal">logger_root</code> section
     to <code class="literal">DEBUG</code>, like this:
    </p><div class="verbatim-wrap"><pre class="screen">[logger_root]
qualname: root
handlers: watchedfile, logstash
level: DEBUG</pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Deploy your changes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.6.8.7.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Enable OVSVAPP Service for Centralized Logging</span> <a title="Permalink" class="permalink" href="#id-1.6.8.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-eon_logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-eon_logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To enable OVSVAPP Service for centralized logging:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the file below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/vars/neutron-ovsvapp-clr.yml</pre></div></li><li class="listitem "><p>
     Set the value of <code class="literal">centralized_logging</code> to
     <span class="bold"><strong>true</strong></span> as shown in the following sample:
    </p><div class="verbatim-wrap"><pre class="screen">logr_services:
  neutron-ovsvapp:
    logging_options:
    - centralized_logging:
        enabled: <span class="bold"><strong>true</strong></span>
        format: json
        ...</pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Deploy your changes, specifying the hostname for your OVSAPP host:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml --limit &lt;hostname&gt;</pre></div><p>
     The hostname of the node can be found in the list generated from the
     output of the following command:
    </p><div class="verbatim-wrap"><pre class="screen">grep hostname ~/openstack/my_cloud/info/server_info.yml</pre></div></li></ol></div></div></div><div class="sect1" id="topic-ijt-dyh-rt"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Scale Configuration Changes</span> <a title="Permalink" class="permalink" href="#topic-ijt-dyh-rt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-scale_config_changes.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-scale_config_changes.xml</li><li><span class="ds-label">ID: </span>topic-ijt-dyh-rt</li></ul></div></div></div></div><p>
  This procedure describes how to make the recommended configuration changes to
  achieve 8,000 virtual machine instances.
 </p><div id="id-1.6.8.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   In a scale environment for ESX computes, the configuration of vCenter Proxy
   VM has to be increased to 8 vCPUs and 16 GB RAM. By default it is 4 vCPUs
   and 4 GB RAM.
  </p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Change the directory. The <code class="literal">nova.conf.j2</code> file is present
    in following directories:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/roles/nova-common/templates</pre></div></li><li class="listitem "><p>
    Edit the DEFAULT section in the <code class="literal">nova.conf.j2</code> file as
    below:
   </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
rpc_responce_timeout = 180
server_down_time = 300
report_interval = 30</pre></div></li><li class="listitem "><p>
    Commit your configuration:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "&lt;commit message&gt;"</pre></div></li><li class="listitem "><p>
    Prepare your environment for deployment:
   </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml;
cd ~/scratch/ansible/next/ardana/ansible;</pre></div></li><li class="listitem "><p>
    Execute the <code class="literal">nova-reconfigure</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div><div class="sect1" id="idg-all-operations-monitoring-vcenter-clusters-xml-1"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring vCenter Clusters</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-vcenter-clusters-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring_vcenter_clusters.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring_vcenter_clusters.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-vcenter-clusters-xml-1</li></ul></div></div></div></div><p>
  Remote monitoring of activated ESX cluster is enabled through vCenter Plugin
  of Monasca. The Monasca-agent running in each ESX Compute proxy node is
  configured with the vcenter plugin, to monitor the cluster.
 </p><p>
  Alarm definitions are created with the default threshold values and whenever
  the threshold limit breaches respective alarms (OK/ALARM/UNDETERMINED) are
  generated.
 </p><p>
  The configuration file details is given below:
 </p><div class="verbatim-wrap"><pre class="screen">init_config: {}
instances:
  - vcenter_ip: &lt;vcenter-ip&gt;
      username: &lt;vcenter-username&gt;
      password: &lt;center-password&gt;
      clusters: &lt;[cluster list]&gt;</pre></div><p>
  Metrics List of metrics posted to monasca by vCenter Plugin are listed below:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    vcenter.cpu.total_mhz
   </p></li><li class="listitem "><p>
    vcenter.cpu.used_mhz
   </p></li><li class="listitem "><p>
    vcenter.cpu.used_perc
   </p></li><li class="listitem "><p>
    vcenter.cpu.total_logical_cores
   </p></li><li class="listitem "><p>
    vcenter.mem.total_mb
   </p></li><li class="listitem "><p>
    vcenter.mem.used_mb
   </p></li><li class="listitem "><p>
    vcenter.mem.used_perc
   </p></li><li class="listitem "><p>
    vcenter.disk.total_space_mb
   </p></li><li class="listitem "><p>
    vcenter.disk.total_used_space_mb
   </p></li><li class="listitem "><p>
    vcenter.disk.total_used_space_perc
   </p></li></ul></div><p>
  monasca measurement-list --dimensions
  esx_cluster_id=domain-c7.D99502A9-63A8-41A2-B3C3-D8E31B591224
  vcenter.disk.total_used_space_mb 2016-08-30T11:20:08
 </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------------------+----------------------------------------------------------------------------------------------+-----------------------------------+------------------+-----------------+
| name                                         | dimensions                                                                                   | timestamp                         | value            | value_meta      |
+----------------------------------------------+----------------------------------------------------------------------------------------------+-----------------------------------+------------------+-----------------+
| vcenter.disk.total_used_space_mb             | vcenter_ip: 10.1.200.91                                                                      | 2016-08-30T11:20:20.703Z          | 100371.000       |                 |
|                                              | esx_cluster_id: domain-c7.D99502A9-63A8-41A2-B3C3-D8E31B591224                               | 2016-08-30T11:20:50.727Z          | 100371.000       |                 |
|                                              | hostname: MCP-VCP-cpesx-esx-comp0001-mgmt                                                    | 2016-08-30T11:21:20.707Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:21:50.700Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:22:20.700Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:22:50.700Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:23:20.620Z          | 100371.000       |                 |
+----------------------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------+------------------+-----------------+</pre></div><p>
  <span class="bold"><strong>Dimensions</strong></span>
 </p><p>
  Each metric will have the dimension as below
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.8.9.12.1"><span class="term ">vcenter_ip</span></dt><dd><p>
     FQDN/IP Address of the registered vCenter
    </p></dd><dt id="id-1.6.8.9.12.2"><span class="term ">server esx_cluster_id</span></dt><dd><p>
     clusterName.vCenter-id, as seen in the nova hypervisor-list
    </p></dd><dt id="id-1.6.8.9.12.3"><span class="term ">hostname</span></dt><dd><p>
     ESX compute proxy name
    </p></dd></dl></div><p>
  <span class="bold"><strong>Alarms</strong></span>
 </p><p>
  Alarms are created for monitoring cpu, memory and disk usages for each
  activated clusters. The alarm definitions details are
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Name</th><th>Expression</th><th>Severity</th><th>Match_by</th></tr></thead><tbody><tr><td>ESX cluster CPU Usage</td><td>avg(vcenter.cpu.used_perc) &gt; 90 times 3</td><td>High</td><td>esx_cluster_id</td></tr><tr><td>ESX cluster Memory Usage</td><td>avg(vcenter.mem.used_perc) &gt; 90 times 3</td><td>High</td><td>esx_cluster_id</td></tr><tr><td>ESX cluster Disk Usage</td><td>vcenter.disk.total_used_space_perc &gt; 90</td><td>High</td><td>esx_cluster_id</td></tr></tbody></table></div></div><div class="sect1" id="ovsvapp-monitoring"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Integration with OVSvApp Appliance</span> <a title="Permalink" class="permalink" href="#ovsvapp-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-esx_monitoring.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-esx_monitoring.xml</li><li><span class="ds-label">ID: </span>ovsvapp-monitoring</li></ul></div></div></div></div><div class="sect2" id="processes"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Processes Monitored with Monasca Agent</span> <a title="Permalink" class="permalink" href="#processes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-esx_monitoring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-esx_monitoring.xml</li><li><span class="ds-label">ID: </span>processes</li></ul></div></div></div></div><p>
   Using the Monasca agent, the following services are monitored on the OVSvApp
   appliance:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Neutron_ovsvapp_agent service</strong></span> - This is
     the Neutron agent which runs in the appliance which will help enable
     networking for the tenant virtual machines.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Openvswitch</strong></span> - This service is used by the
     neutron_ovsvapp_agent service for enabling the datapath and security for
     the tenant virtual machines.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Ovsdb-server</strong></span> - This service is used by
     the neutron_ovsvapp_agent service.
    </p></li></ul></div><p>
   If any of the above three processes fail to run on the OVSvApp appliance it
   will lead to network disruption for the tenant virtual machines. This is why
   they are monitored.
  </p><p>
   The monasca-agent periodically reports the status of these processes and
   metrics data ('load' - cpu.load_avg_1min, 'process' - process.pid_count,
   'memory' - mem.usable_perc, 'disk' - disk.space_used_perc, 'cpu' -
   cpu.idle_perc for examples) to the Monasca server.
  </p></div><div class="sect2" id="how"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How It Works</span> <a title="Permalink" class="permalink" href="#how">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/esx-esx_monitoring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-esx_monitoring.xml</li><li><span class="ds-label">ID: </span>how</li></ul></div></div></div></div><p>
   Once the vApp is configured and up, the monasca-agent will attempt to
   register with the Monasca server. After successful registration, the
   monitoring begins on the processes listed above and you will be able to see
   status updates on the server side.
  </p><p>
   The monasca-agent monitors the processes at the system level so, in the case
   of failures of any of the configured processes, updates should be seen
   immediately from Monasca.
  </p><p>
   To check the events from the server side, log into the Operations Console. For more
   details on how to use the Operations Console, see
   <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span>.
  </p></div></div></div><div class="chapter " id="ops-managing-blockstorage"><div class="titlepage"><div><div><h1 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Block Storage</span> <a title="Permalink" class="permalink" href="#ops-managing-blockstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_blockstorage.xml</li><li><span class="ds-label">ID: </span>ops-managing-blockstorage</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic-e5g-z3h-gt"><span class="number">7.1 </span><span class="name">Managing Block Storage using Cinder</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Block Storage service.
 </p><div class="sect1" id="topic-e5g-z3h-gt"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Block Storage using Cinder</span> <a title="Permalink" class="permalink" href="#topic-e5g-z3h-gt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage_overview.xml</li><li><span class="ds-label">ID: </span>topic-e5g-z3h-gt</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Block Storage volume operations use the OpenStack Cinder service to
  manage storage volumes, which includes creating volumes, attaching/detaching
  volumes to Nova instances, creating volume snapshots, and configuring
  volumes.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports the following storage back ends for block storage
  volumes and backup datastore configuration:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Volumes
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      SUSE Enterprise Storage; for more information, see
      <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 22 “Integrations”, Section 22.3 “SUSE Enterprise Storage Integration”</span>.
     </p></li><li class="listitem "><p>
      3PAR FC or iSCSI; for more information, see
      <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 22 “Integrations”, Section 22.1 “Configuring for 3PAR Block Storage Backend”</span>.
     </p></li></ul></div></li><li class="listitem "><p>
    Backup
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Swift
     </p></li></ul></div></li></ul></div><div class="sect2" id="multiple"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up Multiple Block Storage Back-ends</span> <a title="Permalink" class="permalink" href="#multiple">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage_overview.xml</li><li><span class="ds-label">ID: </span>multiple</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports setting up multiple block storage backends and multiple
   volume types.
  </p><p>
   Whether you have a single or multiple block storage back-ends defined in
   your <code class="filename">cinder.conf.j2</code> file, you can create one or more
   volume types using the specific attributes associated with the back-end. You
   can find details on how to do that for each of the supported back-end types
   here:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 22 “Integrations”, Section 22.3 “SUSE Enterprise Storage Integration”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 22 “Integrations”, Section 22.1 “Configuring for 3PAR Block Storage Backend”</span>
    </p></li></ul></div></div><div class="sect2" id="creating-voltype"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Volume Type for your Volumes</span> <a title="Permalink" class="permalink" href="#creating-voltype">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>creating-voltype</li></ul></div></div></div></div><p>
  Creating volume types allows you to create standard specifications for your
  volumes.
 </p><p>
  Volume types are used to specify a standard Block Storage back-end and
  collection of extra specifications for your volumes. This allows an
  administrator to give its users a variety of options while simplifying the
  process of creating volumes.
 </p><p>
  The tasks involved in this process are:
 </p><div class="sect3" id="create-volumetype"><div class="titlepage"><div><div><h4 class="title"><span class="number">7.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Volume Type for your Volumes</span> <a title="Permalink" class="permalink" href="#create-volumetype">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>create-volumetype</li></ul></div></div></div></div><p>
   The default volume type will be thin provisioned and will have no fault
   tolerance (RAID 0). You should configure Cinder to fully provision volumes,
   and you may want to configure fault tolerance. Follow the instructions below
   to create a new volume type that is fully provisioned and fault tolerant:
  </p><p>
   Perform the following steps to create a volume type using the Horizon GUI:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Horizon dashboard. See <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span> for
     details on how to do this.
    </p></li><li class="step "><p>
     Ensure that you are scoped to your <code class="literal">admin</code> Project. Then
     under the <span class="guimenu ">Admin</span> menu in the navigation pane, click on
     <span class="guimenu ">Volumes</span> under the <span class="guimenu ">System</span> subheading.
    </p></li><li class="step "><p>
     Select the <span class="guimenu ">Volume Types</span> tab and then click the
     <span class="guimenu ">Create Volume Type</span> button to display a dialog box.
    </p></li><li class="step "><p>
     Enter a unique name for the volume type and then click the
     <span class="guimenu ">Create Volume Type</span> button to complete the
     action.
    </p></li></ol></div></div><p>
   The newly created volume type will be displayed in the <code class="literal">Volume
   Types</code> list confirming its creation.
  </p><div id="id-1.6.9.3.6.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If you do not specify a default type then your volumes will default
    unpredictably. We recommend that you create a volume type that meets the
    needs of your environment and specify it here.
   </p></div></div><div class="sect3" id="associate-volumetype"><div class="titlepage"><div><div><h4 class="title"><span class="number">7.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Associate the Volume Type to the Back-end</span> <a title="Permalink" class="permalink" href="#associate-volumetype">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>associate-volumetype</li></ul></div></div></div></div><p>
   After the volume type(s) have been created, you can assign extra
   specification attributes to the volume types. Each Block Storage back-end
   option has unique attributes that can be used.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#extraspecs-3par" title="7.1.2.3. Extra Specification Options for 3PAR">Section 7.1.2.3, “Extra Specification Options for 3PAR”</a>
    </p></li></ul></div><p>
   To map a volume type to a back-end, do the following:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Horizon dashboard. See <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span> for
     details on how to do this.
    </p></li><li class="step "><p>
     Ensure that you are scoped to your <span class="guimenu ">admin</span> Project (for
     more information, see <a class="xref" href="#scopeToDomain" title="4.10.7. Scope Federated User to Domain">Section 4.10.7, “Scope Federated User to Domain”</a>. Then under the
     <span class="guimenu ">Admin</span> menu in the navigation pane, click on
     <span class="guimenu ">Volumes</span> under the <span class="guimenu ">System</span> subheading.
    </p></li><li class="step "><p>
     Click the <span class="guimenu ">Volume Type</span> tab to list the volume types.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Actions</span> column of the Volume Type you created
     earlier, click the drop-down option and select <span class="guimenu ">View Extra
     Specs</span> which will bring up the <span class="guimenu ">Volume Type Extra
     Specs</span> options.
    </p></li><li class="step "><p>
     Click the <span class="guimenu ">Create</span> button on the <code class="literal">Volume Type
     Extra Specs</code> screen.
    </p></li><li class="step "><p>
     In the <code class="literal">Key</code> field, enter one of the key values in the
     table in the next section. In the <code class="literal">Value</code> box, enter its
     corresponding value. Once you have completed that, click the
     <span class="guimenu ">Create</span> button to create the extra volume type specs.
    </p></li></ol></div></div><p>
   Once the volume type is mapped to a back-end, you can create volumes with
   this volume type.
  </p></div><div class="sect3" id="extraspecs-3par"><div class="titlepage"><div><div><h4 class="title"><span class="number">7.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Extra Specification Options for 3PAR</span> <a title="Permalink" class="permalink" href="#extraspecs-3par">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>extraspecs-3par</li></ul></div></div></div></div><p>
   3PAR supports volumes creation with additional attributes. These attributes
   can be specified using the extra specs options for your volume type. The
   administrator is expected to define appropriate extra spec for 3PAR volume
   type as per the guidelines provided at
   <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/hp-3par-supported-ops.html" target="_blank">http://docs.openstack.org/liberty/config-reference/content/hp-3par-supported-ops.html</a>.
  </p><p>
   The following Cinder Volume Type extra-specs options enable control over the
   3PAR storage provisioning type:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Key</th><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>volume_backend_name</td><td><span class="emphasis"><em>volume backend name</em></span>
      </td><td>
       <p>
        The name of the back-end to which you want to associate the volume type,
        which you also specified earlier in the
        <code class="literal">cinder.conf.j2</code> file.
       </p>
      </td></tr><tr><td>hp3par:provisioning (optional)</td><td>thin, full, or dedup</td><td> </td></tr></tbody></table></div><p>
   See
   <a class="link" href="https://h20195.www2.hpe.com/v2/GetPDF.aspx/4AA5-1930ENW.pdf" target="_blank">OpenStack
   HPE 3PAR StoreServ Block Storage Driver Configuration Best Practices</a>
   for more details.
  </p></div></div><div class="sect2" id="sec-operation-manage-block-storage"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Cinder Volume and Backup Services</span> <a title="Permalink" class="permalink" href="#sec-operation-manage-block-storage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage-managing_cinder_volumebackup_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-managing_cinder_volumebackup_services.xml</li><li><span class="ds-label">ID: </span>sec-operation-manage-block-storage</li></ul></div></div></div></div><div id="id-1.6.9.3.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Use Only When Needed</h6><p>
   If the host running the <code class="literal">cinder-volume</code> service fails for
   any reason, it should be restarted as quickly as possible. Often, the host
   running Cinder services also runs high availability (HA) services
   such as MariaDB and RabbitMQ. These HA services are at risk while one of the
   nodes in the cluster is down. If it will take a significant amount of time
   to recover the failed node, then you may migrate the
   <code class="literal">cinder-volume</code> service and its backup service to one of
   the other controller nodes. When the node has been recovered, you should
   migrate the <code class="literal">cinder-volume</code> service and its backup service
   to the original (default) node.
  </p><p>
   The <code class="literal">cinder-volume</code> service and its backup service migrate
   as a pair. If you migrate the <code class="literal">cinder-volume</code> service, its
   backup service will also be migrated.
  </p></div><div class="sect3" id="idg-all-operations-blockstorage-managing-cinder-volumebackup-services-xml-5"><div class="titlepage"><div><div><h4 class="title"><span class="number">7.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating the cinder-volume service</span> <a title="Permalink" class="permalink" href="#idg-all-operations-blockstorage-managing-cinder-volumebackup-services-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-blockstorage-managing_cinder_volumebackup_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-managing_cinder_volumebackup_services.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-blockstorage-managing-cinder-volumebackup-services-xml-5</li></ul></div></div></div></div><p>
   The following steps will migrate the cinder-volume service and its backup service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager node.
    </p></li><li class="listitem "><p>
     Determine the host index numbers for each of your control plane nodes.
     This host index number will be used in a later step. They can be obtained
     by running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-show-volume-hosts.yml</pre></div><p>
     Here is an example snippet showing the output of a single three node
     control plane, with the host index numbers in bold:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [_CND-CMN | show_volume_hosts | Show Cinder Volume hosts index and hostname] ***
ok: [ardana-cp1-c1-m1] =&gt; (item=(0, 'ardana-cp1-c1-m1')) =&gt; {
    "item": [
        <span class="bold"><strong>0</strong></span>,
        "ardana-cp1-c1-m1"
    ],
    "msg": "Index 0 Hostname ardana-cp1-c1-m1"
}
ok: [ardana-cp1-c1-m1] =&gt; (item=(1, 'ardana-cp1-c1-m2')) =&gt; {
    "item": [
        <span class="bold"><strong>1</strong></span>,
        "ardana-cp1-c1-m2"
    ],
    "msg": "Index 1 Hostname ardana-cp1-c1-m2"
}
ok: [ardana-cp1-c1-m1] =&gt; (item=(2, 'ardana-cp1-c1-m3')) =&gt; {
    "item": [
        <span class="bold"><strong>2</strong></span>,
        "ardana-cp1-c1-m3"
    ],
    "msg": "Index 2 Hostname ardana-cp1-c1-m3"
}</pre></div></li><li class="listitem "><p>
     Locate the control plane fact file for the control plane you need to
     migrate the service from. It will be located in the following directory:
    </p><div class="verbatim-wrap"><pre class="screen">/etc/ansible/facts.d/</pre></div><p>
     These fact files use the following naming convention:
    </p><div class="verbatim-wrap"><pre class="screen">cinder_volume_run_location_<span class="emphasis"><em>&lt;control_plane_name&gt;</em></span>.fact</pre></div></li><li class="listitem "><p>
     Edit the fact file to include the host index number of the control plane
     node you wish to migrate the <code class="literal">cinder-volume</code> services to.
     For example, if they currently reside on your first controller node, host
     index 0, and you wish to migrate them to your second controller, you would
     change the value in the fact file to <code class="literal">1</code>.
    </p></li><li class="listitem "><p>
     If you are using data encryption on your Cloud Lifecycle Manager, ensure you
     have included the encryption key in your environment variables. For more
     information see <span class="intraxref">Book “Security Guide”, Chapter 9 “Encryption of Passwords and Sensitive Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="listitem "><p>
     After you have edited the control plane fact file, run the Cinder volume
     migration playbook for the control plane nodes involved in the migration.
     At minimum this includes the one to start cinder-volume manager on and the
     one on which to stop it:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml --limit=&lt;limit_pattern1,limit_pattern2&gt;</pre></div><div id="id-1.6.9.3.7.3.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      <span class="emphasis"><em>&lt;limit_pattern&gt;</em></span> is the pattern used to limit
      the hosts that are selected to those within a specific control plane. For
      example, with the nodes in the snippet shown above,
      <code class="literal">--limit=&gt;ardana-cp1-c1-m1,ardana-cp1-c1-m2&lt;</code>
     </p></div></li><li class="listitem "><p>
     Even though the playbook summary reports no errors, you may disregard
     informational messages such as:
    </p><div class="verbatim-wrap"><pre class="screen">msg: Marking ardana_notify_cinder_restart_required to be cleared from the fact cache</pre></div></li><li class="listitem "><p>
     Ensure that once your maintenance or other tasks are completed that you
     migrate the <code class="literal">cinder-volume</code> services back to their
     original node using these same steps.
    </p></li></ol></div></div></div></div></div><div class="chapter " id="ops-managing-objectstorage"><div class="titlepage"><div><div><h1 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Object Storage</span> <a title="Permalink" class="permalink" href="#ops-managing-objectstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_objectstorage.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_objectstorage.xml</li><li><span class="ds-label">ID: </span>ops-managing-objectstorage</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#swift-healthcheck"><span class="number">8.1 </span><span class="name">Running the Swift Dispersion Report</span></a></span></dt><dt><span class="section"><a href="#swift-recon"><span class="number">8.2 </span><span class="name">Gathering Swift Data</span></a></span></dt><dt><span class="section"><a href="#topic-pcv-fy4-nt"><span class="number">8.3 </span><span class="name">Gathering Swift Monitoring Metrics</span></a></span></dt><dt><span class="section"><a href="#topic-m13-dgp-nt"><span class="number">8.4 </span><span class="name">Using the Swift Command-line Client (CLI)</span></a></span></dt><dt><span class="section"><a href="#swift-ring-management"><span class="number">8.5 </span><span class="name">Managing Swift Rings</span></a></span></dt><dt><span class="section"><a href="#topic-el2-cqv-mv"><span class="number">8.6 </span><span class="name">Configuring your Swift System to Allow Container Sync</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Object Storage service.
 </p><p>
  Managing your object storage environment includes tasks related to ensuring
  your Swift rings stay balanced and we discuss that and other topics in more
  detail in this section.
 </p><p>
  You can verify the Swift object storage operational status using commands and
  utilities. This section covers the following topics:
 </p><div class="sect1" id="swift-healthcheck"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Swift Dispersion Report</span> <a title="Permalink" class="permalink" href="#swift-healthcheck">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_dispersion_report.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_dispersion_report.xml</li><li><span class="ds-label">ID: </span>swift-healthcheck</li></ul></div></div></div></div><p>
  Swift contains a tool called <code class="literal">swift-dispersion-report</code> that
  can be used to determine whether your containers and objects have three
  replicas like they are supposed to. This tool works by populating a
  percentage of partitions in the system with containers and objects (using
  <code class="literal">swift-dispersion-populate</code>) and then running the report to
  see if all the replicas of these containers and objects are in the correct
  place. For a more detailed explanation of this tool in Openstack Swift,
  please see
  <a class="link" href="http://docs.openstack.org/developer/swift/admin_guide.html#cluster-health" target="_blank">OpenStack
  Swift - Administrator's Guide</a>.
 </p><div class="sect2" id="id-1.6.10.5.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Swift dispersion populate</span> <a title="Permalink" class="permalink" href="#id-1.6.10.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_dispersion_report.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_dispersion_report.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once a Swift system has been fully deployed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you can
   setup the swift-dispersion-report using the default parameters found in
   <code class="literal">~/openstack/ardana/ansible/roles/swift-dispersion/templates/dispersion.conf.j2</code>.
   This populates 1% of the partitions on the system and if you are happy with
   this figure, please proceed to step 2 below. Otherwise, follow step 1 to
   edit the configuration file.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     If you wish to change the dispersion coverage percentage then edit the
     value of <code class="literal">dispersion_coverage</code> in the
     <code class="literal">~/openstack/ardana/ansible/roles/swift-dispersion/templates/dispersion.conf.j2</code>
     file to the value you wish to use. In the example below we have altered
     the file to create 5% dispersion:
    </p><div class="verbatim-wrap"><pre class="screen">...
[dispersion]
auth_url = {{ keystone_identity_uri }}/v3
auth_user = {{ swift_dispersion_tenant }}:{{ swift_dispersion_user }}
auth_key = {{ swift_dispersion_password  }}
endpoint_type = {{ endpoint_type }}
auth_version = {{ disp_auth_version }}
# Set this to the percentage coverage. We recommend a value
# of 1%. You can increase this to get more coverage. However, if you
# decrease the value, the dispersion containers and objects are
# not deleted.
<span class="bold"><strong>dispersion_coverage = 5.0</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Reconfigure the Swift servers:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Run this playbook to populate your Swift system for the health check:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-dispersion-populate.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.6.10.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Swift dispersion report</span> <a title="Permalink" class="permalink" href="#id-1.6.10.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_dispersion_report.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_dispersion_report.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Check the status of the Swift system by running the Swift dispersion report
   with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-dispersion-report.yml</pre></div><p>
   The output of the report will look similar to this:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [swift-dispersion | report | Display dispersion report results] *********
ok: [padawan-ccp-c1-m1-mgmt] =&gt; {
    "var": {
        "dispersion_report_result.stdout_lines": [
            "Using storage policy: General ",
            "",
            "[KQueried 40 containers for dispersion reporting, 0s, 0 retries",
            "100.00% of container copies found (120 of 120)",
            "Sample represents 0.98% of the container partition space",
            "",
            "[KQueried 40 objects for dispersion reporting, 0s, 0 retries",
            "There were 40 partitions missing 0 copies.",
            "100.00% of object copies found (120 of 120)",
            "Sample represents 0.98% of the object partition space"
        ]
    }
}
...</pre></div><p>
   In addition to being able to run the report above, there will be a cron-job
   running every 2 hours on the first proxy node of your system that will run
   <code class="literal">dispersion-report</code> and save the results to the following
   file:
  </p><div class="verbatim-wrap"><pre class="screen">/var/cache/swift/dispersion-report</pre></div><p>
   When interpreting the results you get from this report, we recommend using
   <a class="link" href="http://docs.openstack.org/developer/swift/admin_guide.html#cluster-health" target="_blank">Swift
   Administrator's Guide - Cluster Health</a>
  </p></div></div><div class="sect1" id="swift-recon"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Gathering Swift Data</span> <a title="Permalink" class="permalink" href="#swift-recon">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-validating_swift_recon.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-validating_swift_recon.xml</li><li><span class="ds-label">ID: </span>swift-recon</li></ul></div></div></div></div><p>
  The <code class="literal">swift-recon</code> command retrieves data from Swift servers
  and displays the results. To use this command, log on as a root user to any
  node which is running the swift-proxy service.
 </p><div class="sect2" id="idg-all-operations-objectstorage-validating-swift-recon-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-validating-swift-recon-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-validating_swift_recon.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-validating_swift_recon.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-validating-swift-recon-xml-7</li></ul></div></div></div></div><p>
   For help with the <code class="literal">swift-recon</code> command you can use this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --help</pre></div><div id="id-1.6.10.6.3.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    The <code class="literal">--driveaudit</code> option is not supported.
   </p></div><div id="id-1.6.10.6.3.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not support ec_type <code class="literal">isa_l_rs_vand</code> and
    <code class="literal">ec_num_parity_fragments</code> greater than or equal to
    <span class="bold"><strong>5</strong></span> in the storage-policy configuration.
    This particular policy is known to harm data durability.
   </p></div></div><div class="sect2" id="idg-all-operations-objectstorage-validating-swift-recon-xml-8"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the swift-recon Command</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-validating-swift-recon-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-validating_swift_recon.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-validating_swift_recon.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-validating-swift-recon-xml-8</li></ul></div></div></div></div><p>
   The following command retrieves and displays disk usage information:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --diskusage</pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --diskusage
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-09-14 16:01:40] Checking disk usage now
Distribution Graph:
 10%    3 *********************************************************************
 11%    1 ***********************
 12%    2 **********************************************
Disk usage: space used: 13745373184 of 119927734272
Disk usage: space free: 106182361088 of 119927734272
Disk usage: lowest: 10.39%, highest: 12.96%, avg: 11.4613798613%
===============================================================================</pre></div><p>
   In the above example, the results for several nodes are combined together.
   You can also view the results from individual nodes by adding the
   <span class="bold"><strong>-v</strong></span> option as shown in the following
   example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --diskusage -v
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-09-14 16:12:30] Checking disk usage now
-&gt; http://192.168.245.3:6000/recon/diskusage: [{'device': 'disk1', 'avail': 17398411264, 'mounted': True, 'used': 2589544448, 'size': 19987955712}, {'device': 'disk0', 'avail': 17904222208, 'mounted': True, 'used': 2083733504, 'size': 19987955712}]
-&gt; http://192.168.245.2:6000/recon/diskusage: [{'device': 'disk1', 'avail': 17769721856, 'mounted': True, 'used': 2218233856, 'size': 19987955712}, {'device': 'disk0', 'avail': 17793581056, 'mounted': True, 'used': 2194374656, 'size': 19987955712}]
-&gt; http://192.168.245.4:6000/recon/diskusage: [{'device': 'disk1', 'avail': 17912147968, 'mounted': True, 'used': 2075807744, 'size': 19987955712}, {'device': 'disk0', 'avail': 17404235776, 'mounted': True, 'used': 2583719936, 'size': 19987955712}]
Distribution Graph:
 10%    3 *********************************************************************
 11%    1 ***********************
 12%    2 **********************************************
Disk usage: space used: 13745414144 of 119927734272
Disk usage: space free: 106182320128 of 119927734272
Disk usage: lowest: 10.39%, highest: 12.96%, avg: 11.4614140152%
===============================================================================</pre></div><p>
   By default, <code class="literal">swift-recon</code> uses the object-0 ring for
   information about nodes and drives. For some commands, it is appropriate to
   specify <span class="bold"><strong>account</strong></span>,
   <span class="bold"><strong>container</strong></span>, or
   <span class="bold"><strong>object</strong></span> to indicate the type of ring. For
   example, to check the checksum of the account ring, use the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --md5 account
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-09-14 16:17:28] Checking ring md5sums
3/3 hosts matched, 0 error[s] while checking hosts.
===============================================================================
[2015-09-14 16:17:28] Checking swift.conf md5sum
3/3 hosts matched, 0 error[s] while checking hosts.
===============================================================================</pre></div></div></div><div class="sect1" id="topic-pcv-fy4-nt"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Gathering Swift Monitoring Metrics</span> <a title="Permalink" class="permalink" href="#topic-pcv-fy4-nt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-lm_scan_metering.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-lm_scan_metering.xml</li><li><span class="ds-label">ID: </span>topic-pcv-fy4-nt</li></ul></div></div></div></div><p>
  The <code class="literal">swiftlm-scan</code> command is the mechanism used to gather
  metrics for the Monasca system. These metrics are used to derive alarms. For
  a list of alarms that can be generated from this data, see
  <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a>.
 </p><p>
  To view the metrics, use the <code class="literal">swiftlm-scan</code> command
  directly. Log on to the Swift node as the root user. The following example
  shows the command and a snippet of the output:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swiftlm-scan --pretty
. . .
  {
    "dimensions": {
      "device": "sdc",
      "hostname": "padawan-ccp-c1-m2-mgmt",
      "service": "object-storage"
    },
    "metric": "swiftlm.swift.drive_audit",
    "timestamp": 1442248083,
    "value": 0,
    "value_meta": {
      "msg": "No errors found on device: sdc"
    }
  },
. . .</pre></div><div id="id-1.6.10.7.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   To make the JSON file easier to read, use the <code class="literal">--pretty</code>
   option.
  </p></div><p>
  The fields are as follows:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><code class="literal">metric</code>
     </td><td>
      <p>
       Specifies the name of the metric.
      </p>
     </td></tr><tr><td><code class="literal">dimensions</code>
     </td><td>
      <p>
       Provides information about the source or location of the metric. The
       dimensions differ depending on the metric in question. The following
       dimensions are used by <code class="literal">swiftlm-scan</code>:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>service</strong></span>: This is always
         <span class="bold"><strong>object-storage</strong></span>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>component</strong></span>: This identifies the
         component. For example,
         <span class="bold"><strong>swift-object-server</strong></span> indicates that
         the metric is about the swift-object-server process.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>hostname</strong></span>: This is the name of the
         node the metric relates to. This is not necessarily the name of the
         current node.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>url</strong></span>: If the metric is associated with
         a URL, this is the URL.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>port</strong></span>: If the metric relates to
         connectivity to a node, this is the port used.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>device</strong></span>: This is the block device a
         metric relates to.
        </p></li></ul></div>
     </td></tr><tr><td><code class="literal">value</code></td><td>
      <p>
       The value of the metric. For many metrics, this is simply the value of
       the metric. However, if the value indicates a status. If
       <code class="literal">value_meta</code> contains a
       <span class="bold"><strong>msg</strong></span> field, the value is a status. The
       following status values are used:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         0 - no error
        </p></li><li class="listitem "><p>
         1 - warning
        </p></li><li class="listitem "><p>
         2 - failure
        </p></li></ul></div>
     </td></tr><tr><td><code class="literal">value_meta</code>
     </td><td>
      <p>
       Additional information. The <span class="bold"><strong>msg</strong></span> field
       is the most useful of this information.
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="id-1.6.10.7.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional Parameters</span> <a title="Permalink" class="permalink" href="#id-1.6.10.7.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-lm_scan_metering.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-lm_scan_metering.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can focus on specific sets of metrics by using one of the following
   optional parameters:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><code class="literal">--replication</code></td><td>
       <p>
        Checks replication and health status.
       </p>
      </td></tr><tr><td><code class="literal">--file-ownership</code></td><td>
       <p>
        Checks that Swift owns its relevant files and directories.
       </p>
      </td></tr><tr><td><code class="literal">--drive-audit</code></td><td>
       <p>
        Checks for logged events about corrupted sectors (unrecoverable read
        errors) on drives.
       </p>
      </td></tr><tr><td><code class="literal">--connectivity</code></td><td>
       <p>
        Checks connectivity to various servers used by the Swift system,
        including:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Checks this node can connect to all memcachd servers
         </p></li><li class="listitem "><p>
          Checks that this node can connect to the Keystone service (only
          applicable if this is a proxy server node)
         </p></li></ul></div>
      </td></tr><tr><td><code class="literal">--swift-services</code></td><td>
       <p>
        Check that the relevant Swift processes are running.
       </p>
      </td></tr><tr><td><code class="literal">--network-interface</code></td><td>
       <p>
        Checks NIC speed and reports statistics for each interface.
       </p>
      </td></tr><tr><td><code class="literal">--check-mounts</code></td><td>
       <p>
        Checks that the node has correctly mounted drives used by Swift.
       </p>
      </td></tr><tr><td><code class="literal">--hpssacli</code></td><td>
       <p>
        If this server uses a Smart Array Controller, this checks the operation
        of the controller and disk drives.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect1" id="topic-m13-dgp-nt"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Swift Command-line Client (CLI)</span> <a title="Permalink" class="permalink" href="#topic-m13-dgp-nt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_cli.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_cli.xml</li><li><span class="ds-label">ID: </span>topic-m13-dgp-nt</li></ul></div></div></div></div><p>
  The <code class="literal">swift</code> utility (or Swift CLI) is installed on the
  Cloud Lifecycle Manager node and also on all other nodes running the Swift proxy
  service. To use this utility on the Cloud Lifecycle Manager, you can use the
  <code class="literal">~/service.osrc</code> file as a basis and then edit it with the
  credentials of another user if you need to.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp ~/service.osrc ~/swiftuser.osrc</pre></div><p>
  Then you can use your preferred editor to edit swiftuser.osrc so you can
  authenticate using the <code class="literal">OS_USERNAME</code>,
  <code class="literal">OS_PASSWORD</code>, and <code class="literal">OS_PROJECT_NAME</code> you
  wish to use. For example, if you would like to use the <code class="literal">demo</code>
  user that is created automatically for you, then it might look like this:
 </p><div class="verbatim-wrap"><pre class="screen">unset OS_DOMAIN_NAME
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_VERSION=3
export OS_PROJECT_NAME=demo
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USERNAME=demo
export OS_USER_DOMAIN_NAME=Default
export OS_PASSWORD=&lt;password&gt;
export OS_AUTH_URL=&lt;auth_URL&gt;
export OS_ENDPOINT_TYPE=internalURL
# OpenstackClient uses OS_INTERFACE instead of OS_ENDPOINT
export OS_INTERFACE=internal
export OS_CACERT=/etc/ssl/certs/ca-certificates.crt
export OS_COMPUTE_API_VERSION=2</pre></div><p>
  You must use the appropriate password for the demo user and select the
  correct endpoint for the <span class="bold"><strong>OS_AUTH_URL</strong></span> value,
  which should be in the <code class="literal">~/service.osrc</code> file you copied.
 </p><p>
  You can then examine the following account data using this command:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift stat</pre></div><p>
  Example showing an environment with no containers or objects:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift stat
        Account: AUTH_205804d000a242d385b8124188284998
     Containers: 0
        Objects: 0
          Bytes: 0
X-Put-Timestamp: 1442249536.31989
     Connection: keep-alive
    X-Timestamp: 1442249536.31989
     X-Trans-Id: tx5493faa15be44efeac2e6-0055f6fb3f
   Content-Type: text/plain; charset=utf-8</pre></div><p>
  Use the following command and create a container:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift post <em class="replaceable ">CONTAINER_NAME</em></pre></div><p>
  Example, creating a container named <code class="literal">documents</code>:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift post documents</pre></div><p>
  The newly created container appears. But there are no objects:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift stat documents
         Account: AUTH_205804d000a242d385b8124188284998
       Container: documents
         Objects: 0
           Bytes: 0
        Read ACL:
       Write ACL:
         Sync To:
        Sync Key:
   Accept-Ranges: bytes
X-Storage-Policy: General
      Connection: keep-alive
     X-Timestamp: 1442249637.69486
      X-Trans-Id: tx1f59d5f7750f4ae8a3929-0055f6fbcc
    Content-Type: text/plain; charset=utf-8</pre></div><p>
  Upload a document:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift upload <em class="replaceable ">CONTAINER_NAME</em> <em class="replaceable ">FILENAME</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift upload documents mydocument
mydocument</pre></div><p>
  List objects in the container:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift list <em class="replaceable ">CONTAINER_NAME</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift list documents
mydocument</pre></div><div id="id-1.6.10.8.25" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   This is a brief introduction to the <code class="command">swift</code> CLI. Use the
   <code class="command">swift --help</code> command for more information. You can also
   use the OpenStack CLI, see <code class="command">openstack -h</code> for more
   information.
  </p></div></div><div class="sect1" id="swift-ring-management"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Swift Rings</span> <a title="Permalink" class="permalink" href="#swift-ring-management">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-ring_management.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-ring_management.xml</li><li><span class="ds-label">ID: </span>swift-ring-management</li></ul></div></div></div></div><p>
  Swift rings are a machine-readable description of which disk drives are used
  by the Object Storage service (for example, a drive is used to store account
  or object data). Rings also specify the policy for data storage (for example,
  defining the number of replicas). The rings are automatically built during
  the initial deployment of your cloud, with the configuration provided during
  setup of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Input Model. For more information, see
  <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”</span>.
 </p><p>
  After successful deployment of your cloud, you may want to change or modify
  the configuration for Swift. For example, you may want to add or remove Swift
  nodes, add additional storage policies, or upgrade the size of the disk
  drives. For instructions, see <a class="xref" href="#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a> and
  <a class="xref" href="#add-storage-policy" title="8.5.6. Adding a New Swift Storage Policy">Section 8.5.6, “Adding a New Swift Storage Policy”</a>.
 </p><div id="id-1.6.10.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The process of modifying or adding a configuration is similar to other
   configuration or topology changes in the cloud. Generally, you make the
   changes to the input model files at
   <code class="literal">~/openstack/my_cloud/definition/</code> on the Cloud Lifecycle Manager and then
   run Ansible playbooks to reconfigure the system.
  </p></div><p>
  Changes to the rings require several phases to complete, therefore, you may
  need to run the playbooks several times over several days.
 </p><p>
  The following topics cover ring management.
 </p><div class="sect2" id="swift-ring-rebalance"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebalancing Swift Rings</span> <a title="Permalink" class="permalink" href="#swift-ring-rebalance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-rebalanced_explained.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-rebalanced_explained.xml</li><li><span class="ds-label">ID: </span>swift-ring-rebalance</li></ul></div></div></div></div><p>
  The Swift ring building process tries to distribute data evenly among the
  available disk drives. The data is stored in partitions. (For more
  information, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.10 “Understanding Swift Ring Specifications”</span>.) If you, for example,
  double the number of disk drives in a ring, you need to move 50% of the
  partitions to the new drives so that all drives contain the same number of
  partitions (and hence same amount of data). However, it is not possible to
  move the partitions in a single step. It can take minutes to hours to move
  partitions from the original drives to their new drives (this process is
  called the replication process).
 </p><p>
  If you move all partitions at once, there would be a period where Swift would
  expect to find partitions on the new drives, but the data has not yet
  replicated there so that Swift could not return the data to the user.
  Therefore, Swift will not be able to find all of the data in the middle of
  replication because some data has finished replication while other bits of
  data are still in the old locations and have not yet been moved. So it is
  considered best practice to move only one replica at a time. If the replica
  count is 3, you could first move 16.6% of the partitions and then wait until
  all data has replicated. Then move another 16.6% of partitions. Wait again
  and then finally move the remaining 16.6% of partitions. For any given
  object, only one of the replicas is moved at a time.
 </p><div class="sect3" id="id-1.6.10.9.7.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reasons to Move Partitions Gradually</span> <a title="Permalink" class="permalink" href="#id-1.6.10.9.7.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-rebalanced_explained.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-rebalanced_explained.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Due to the following factors, you must move the partitions gradually:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Not all devices are of the same size. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> automatically
     assigns different weights to drives so that smaller drives store fewer
     partitions than larger drives.
    </p></li><li class="listitem "><p>
     The process attempts to keep replicas of the same partition in different
     servers.
    </p></li><li class="listitem "><p>
     Making a large change in one step (for example, doubling the number of
     drives in the ring), would result in a lot of network traffic due to the
     replication process and the system performance suffers. There are two ways
     to mitigate this:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Add servers in smaller groups
      </p></li><li class="listitem "><p>
       Set the weight-step attribute in the ring specification. For more
       information, see <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
      </p></li></ul></div></li></ul></div></div></div><div class="sect2" id="swift-weight-att"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Weight-Step Attributes to Prepare for Ring Changes</span> <a title="Permalink" class="permalink" href="#swift-weight-att">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_weight_attribute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_weight_attribute.xml</li><li><span class="ds-label">ID: </span>swift-weight-att</li></ul></div></div></div></div><p>
  Swift rings are built during a deployment and this process sets the weights
  of disk drives such that smaller disk drives have a smaller weight than
  larger disk drives. When making changes in the ring, you should limit the
  amount of change that occurs. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> does this by limiting the
  weights of the new drives to a smaller value and then building new rings.
  Once the replication process has finished, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> will increase the
  weight and rebuild rings to trigger another round of replication. (For more
  information, see <a class="xref" href="#swift-ring-rebalance" title="8.5.1. Rebalancing Swift Rings">Section 8.5.1, “Rebalancing Swift Rings”</a>.)
 </p><p>
  In addition, you should become familiar with how the replication process
  behaves on your system during normal operation. Before making ring changes,
  use the <code class="literal">swift-recon</code> command to determine the typical
  oldest replication times for your system. For instructions, see
  <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the weight-step attribute is set in the ring specification of
  the input model. The weight-step value specifies a maximum value for the
  change of the weight of a drive in any single rebalance. For example, if you
  add a drive of 4TB, you would normally assign a weight of 4096. However, if
  the weight-step attribute is set to 1024 instead then when you add that drive
  the weight is initially set to 1024. The next time you rebalance the ring,
  the weight is set to 2048. The subsequent rebalance would then set the weight
  to the final value of 4096.
 </p><p>
  The value of the weight-step attribute is dependent on the size of the
  drives, number of the servers being added, and how experienced you are with
  the replication process. A common starting value is to use 20% of the size of
  an individual drive. For example, when adding X number of 4TB drives a value
  of 820 would be appropriate. As you gain more experience with your system,
  you may increase or reduce this value.
 </p><div class="sect3" id="setting-weight-step-attribute"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting the weight-step attribute</span> <a title="Permalink" class="permalink" href="#setting-weight-step-attribute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_weight_attribute.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_weight_attribute.xml</li><li><span class="ds-label">ID: </span>setting-weight-step-attribute</li></ul></div></div></div></div><p>
   Perform the following steps to set the weight-step attribute:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/definition/data/swift/rings.yml</code> file
     containing the ring-specifications for the account, container, and object
     rings.
    </p><p>
     Add the weight-step attribute to the ring in this format:
    </p><div class="verbatim-wrap"><pre class="screen">- name: account
  weight-step: <em class="replaceable ">WEIGHT_STEP_VALUE</em>
  display-name: Account Ring
  min-part-hours: 16
  ...</pre></div><p>
     For example, to set weight-step to 820, add the attribute like this:
    </p><div class="verbatim-wrap"><pre class="screen">- name: account
  weight-step: <span class="bold"><strong>820</strong></span>
  display-name: Account Ring
  min-part-hours: 16
  ...</pre></div></li><li class="listitem "><p>
     Repeat step 2 for the other rings, if necessary (container, object-0,
     etc).
    </p></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To complete the configuration, use the ansible playbooks documented in
     <a class="xref" href="#swift-ansible-playbooks" title="8.5.3. Managing Rings Using Swift Playbooks">Section 8.5.3, “Managing Rings Using Swift Playbooks”</a>.
    </p></li></ol></div></div></div><div class="sect2" id="swift-ansible-playbooks"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Rings Using Swift Playbooks</span> <a title="Permalink" class="permalink" href="#swift-ansible-playbooks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_ring_mgmt.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_ring_mgmt.xml</li><li><span class="ds-label">ID: </span>swift-ansible-playbooks</li></ul></div></div></div></div><p>
  The following table describes how playbooks relate to ring management.
 </p><p>
  All of these playbooks will be run from the Cloud Lifecycle Manager from the
  <code class="literal">~/scratch/ansible/next/ardana/ansible</code> directory.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Playbook</th><th>Description</th><th>Notes</th></tr></thead><tbody><tr><td><code class="literal">swift-update-from-model-rebalance-rings.yml</code>
     </td><td>
      <p>
       There are two steps in this playbook:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Make delta
        </p><p>
         It processes the input model and compares it against the existing
         rings. After comparison, it produces a list of differences between
         the input model and the existing rings. This is called the ring
         delta. The ring delta covers drives being added, drives being
         removed, weight changes, and replica count changes.
        </p></li></ul></div>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Rebalance
        </p><p>
         The ring delta is then converted into a series of commands (such as
         <code class="literal">add</code>) to the swift-ring-builder program. Finally,
         the <code class="literal">rebalance</code> command is issued to the
         swift-ring-builder program.
        </p></li></ul></div>
     </td><td>
      <p>
       This playbook performs its actions on the first node running the
       swift-proxy service. (For more information, see
       <a class="xref" href="#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a>.) However, it also scans all Swift
       nodes to find the size of disk drives.
      </p>
      <p>
       If there are no changes in the ring delta, the
       <code class="literal">rebalance</code> command is still executed to rebalance the
       rings. If <code class="literal">min-part-hours</code> has not yet elapsed or if
       no partitions need to be moved, new rings are not written.
      </p>
     </td></tr><tr><td><code class="literal">swift-compare-model-rings.yml</code>
     </td><td>
      <p>
       There are two steps in this playbook:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Make delta
        </p><p>
         This is the same as described for
         <code class="literal">swift-update-from-model-rebalance-rings.yml</code>.
        </p></li></ul></div>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Report
        </p><p>
         This prints a summary of the proposed changes that will be made to
         the rings (that is, what would happen if you rebalanced).
        </p></li></ul></div>
      <p>
       The playbook reports any issues or problems it finds with the input
       model.
      </p>
      <p>
       This playbook can be useful to confirm that there are no errors in the
       input model. It also allows you to check that when you change the input
       model, that the proposed ring changes are as expected. For example, if
       you have added a server to the input model, but this playbook reports
       that no drives are being added, you should determine the cause.
      </p>
     </td><td>
      <p>
       There is troubleshooting information related to the information that
       you receive in this report that you can view on this page:
       <a class="xref" href="#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
      </p>
     </td></tr><tr><td><code class="literal">swift-deploy.yml</code>
     </td><td>
      <p>
       <code class="literal">swift-deploy.yml</code> is responsible for installing
       software and configuring Swift on nodes. As part of installing and
       configuring, it runs the
       <code class="literal">swift-update-from-model-rebalance-rings.yml</code> and
       <code class="literal">swift-reconfigure.yml</code> playbooks.
      </p>
     </td><td>
      <p>
       This playbook is included in the <code class="literal">ardana-deploy.yml</code> and
       <code class="literal">site.yml</code> playbooks, so if you run either of those
       playbooks, the <code class="literal">swift-deploy.yml</code> playbook is also
       run.
      </p>
     </td></tr><tr><td><code class="literal">swift-reconfigure.yml</code>
     </td><td>
      <p>
       <code class="literal">swift-reconfigure.yml</code> takes rings that the
       <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
       playbook has changed and copies those rings to all Swift nodes.
      </p>
     </td><td>
      <p>
       Every time that you directly use the
       <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
       playbook, you must copy these rings to the system using the
       <code class="literal">swift-reconfigure.yml</code> playbook. If you forget and
       run <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
       twice, the process may move two replicates of some partitions at the
       same time.
      </p>
     </td></tr></tbody></table></div><div class="sect3" id="variables"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional Ansible variables related to ring management</span> <a title="Permalink" class="permalink" href="#variables">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_ring_mgmt.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_ring_mgmt.xml</li><li><span class="ds-label">ID: </span>variables</li></ul></div></div></div></div><p>
   The following optional variables may be specified when running the playbooks
   outlined above. They are specified using the <code class="literal">--extra-vars</code>
   option.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Variable</th><th>Description and Use</th></tr></thead><tbody><tr><td><code class="literal">limit_ring</code>
      </td><td>
       <p>
        Limit changes to the named ring. Other rings will not be examined or
        updated. This option may be used with any of the Swift playbooks. For
        example, to only update the <code class="literal">object-1</code> ring, use the
        following command:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml --extra-vars "limit-ring=object-1"</pre></div>
      </td></tr><tr><td>drive_detail</td><td>
       <p>
        Used only with the swift-compare-model-rings.yml playbook. The playbook
        will include details of changes to every drive where the model and
        existing rings differ. If you omit the drive_detail variable, only
        summary information is provided. The following shows how to use the
        drive_detail variable:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div>
      </td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.10.9.9.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interpreting the report from the swift-compare-model-rings.yml playbook</span> <a title="Permalink" class="permalink" href="#id-1.6.10.9.9.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_ring_mgmt.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_ring_mgmt.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <code class="literal">swift-compare-model-rings.yml</code> playbook compares the
   existing Swift rings with the input model and prints a report telling you
   how the rings and the model differ. Specifically, it will tell you what
   actions will take place when you next run the
   <code class="literal">swift-update-from-model-rebalance-rings.yml</code> playbook (or
   a playbook such as <code class="literal">ardana-deploy.yml</code> that runs
   <code class="literal">swift-update-from-model-rebalance-rings.yml</code>).
  </p><p>
   The <code class="literal">swift-compare-model-rings.yml</code> playbook will make no
   changes, but is just an advisory report.
  </p><p>
   Here is an example output from the playbook. The report is between
   "report.stdout_lines" and "PLAY RECAP":
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | validate-input-model | Print report] *********
ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
    "var": {
        "report.stdout_lines": [
            "Rings:",
            "  ACCOUNT:",
            "    ring exists (minimum time to next rebalance: 8:07:33)",
            "    will remove 1 devices (18.00GB)",
            "    ring will be rebalanced",
            "  CONTAINER:",
            "    ring exists (minimum time to next rebalance: 8:07:35)",
            "    no device changes",
            "    ring will be rebalanced",
            "  OBJECT-0:",
            "    ring exists (minimum time to next rebalance: 8:07:34)",
            "    no device changes",
            "    ring will be rebalanced"
        ]
    }
}</pre></div><p>
   The following describes the report in more detail:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Message</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        ring exists
       </p>
      </td><td>
       <p>
        The ring already exists on the system.
       </p>
      </td></tr><tr><td>
       <p>
        ring will be created
       </p>
      </td><td>
       <p>
        The ring does not yet exist on the system.
       </p>
      </td></tr><tr><td>
       <p>
        no device changes
       </p>
      </td><td>
       <p>
        The devices in the ring exactly match the input model. There are no
        servers being added or removed and the weights are appropriate for the
        size of the drives.
       </p>
      </td></tr><tr><td>
       <p>
        minimum time to next rebalance
       </p>
      </td><td>
       <p>
        If this time is <code class="literal">0:00:00</code>, if you run one of the Swift
        playbooks that update rings, the ring will be rebalanced.
       </p>
       <p>
        If the time is non-zero, it means that not enough time has elapsed
        since the ring was last rebalanced. Even if you run a Swift playbook
        that attempts to change the ring, the ring will not actually
        rebalance. This time is determined by the
        <code class="literal">min-part-hours</code> attribute.
       </p>
      </td></tr><tr><td>
       <p>
        set-weight ardana-ccp-c1-m1-mgmt:disk0:/dev/sdc 8.00 &gt; 12.00 &gt; 18.63
       </p>
      </td><td>
       <p>
        The weight of disk0 (mounted on /dev/sdc) on server
        <code class="literal">ardana-ccp-c1-m1-mgmt</code> is currently set to 8.0 but
        should be 18.83 given the size of the drive. However, in this example,
        we cannot go directly from 8.0 to 18.63 because of the weight-step
        attribute. Hence, the proposed weight change is from 8.0 to 12.0.
       </p>
       <p>
        This information is only shown when you the
        <code class="literal">drive_detail=yes</code> argument when running the playbook.
       </p>
      </td></tr><tr><td>
       <p>
        will change weight on 12 devices (6.00TB)
       </p>
      </td><td>
       <p>
        The weight of 12 devices will be increased. This might happen for
        example, if a server had been added in a prior ring update. However,
        with use of the <code class="literal">weight-step</code> attribute, the system
        gradually increases the weight of these new devices. In this example,
        the change in weight represents 6TB of total available storage. For
        example, if your system currently has 100TB of available storage, when
        the weight of these devices is changed, there will be 106TB of
        available storage. If your system is 50% utilized, this means that when
        the ring is rebalanced, up to 3TB of data may be moved by the
        replication process. This is an estimate - in practice, because only
        one copy of a given replica is moved in any given rebalance, it may not
        be possible to move this amount of data in a single ring rebalance.
       </p>
      </td></tr><tr><td>
       <p>
        add: ardana-ccp-c1-m1-mgmt:disk0:/dev/sdc
       </p>
      </td><td>
       <p>
        The disk0 device will be added to the ardana-ccp-c1-m1-mgmt server. This
        happens when a server is added to the input model or if a disk model is
        changed to add additional devices.
       </p>
       <p>
        This information is only shown when you the
        <code class="literal">drive_detail=yes</code> argument when running the
        playbook.
       </p>
      </td></tr><tr><td>
       <p>
        remove: ardana-ccp-c1-m1-mgmt:disk0:/dev/sdc
       </p>
      </td><td>
       <p>
        The device is no longer in the input model and will be removed from the
        ring. This happens if a server is removed from the model, a disk drive
        is removed from a disk model or the server is marked for removal using
        the pass-through feature.
       </p>
       <p>
        This information is only shown when you the
        <code class="literal">drive_detail=yes</code> argument when running the
        playbook.
       </p>
      </td></tr><tr><td>
       <p>
        will add 12 devices (6TB)
       </p>
      </td><td>
       <p>
        There are 12 devices in the input model that have not yet been added to
        the ring. Usually this is because one or more servers have been added.
        In this example, this could be one server with 12 drives or two
        servers, each with 6 drives. The size in the report is the change in
        total available capacity. When the weight-step attribute is used, this
        may be a fraction of the total size of the disk drives. In this
        example, 6TB of capacity is being added. For example, if your system
        currently has 100TB of available storage, when these devices are added,
        there will be 106TB of available storage. If your system is 50%
        utilized, this means that when the ring is rebalanced, up to 3TB of
        data may be moved by the replication process. This is an estimate - in
        practice, because only one copy of a given replica is moved in any
        given rebalance, it may not be possible to move this amount of data in
        a single ring rebalance.
       </p>
      </td></tr><tr><td>
       <p>
        will remove 12 devices (6TB)
       </p>
      </td><td>
       <p>
        There are 12 devices in rings that no longer appear in the input model.
        Usually this is because one or more servers have been removed. In this
        example, this could be one server with 12 drives or two servers, each
        with 6 drives. The size in the report is the change in total removed
        capacity. In this example, 6TB of capacity is being removed. For
        example, if your system currently has 100TB of available storage, when
        these devices are removed, there will be 94TB of available storage. If
        your system is 50% utilized, this means that when the ring is
        rebalanced, approximately 3TB of data must be moved by the replication
        process.
       </p>
      </td></tr><tr><td>
       <p>
        min-part-hours will be changed
       </p>
      </td><td>
       <p>
        The <code class="literal">min-part-hours</code> attribute has been changed in the
        ring specification in the input model.
       </p>
      </td></tr><tr><td>
       <p>
        replica-count will be changed
       </p>
      </td><td>
       <p>
        The <code class="literal">replica-count</code> attribute has been changed in the
        ring specification in the input model.
       </p>
      </td></tr><tr><td>
       <p>
        ring will be rebalanced
       </p>
      </td><td>
       <p>
        This is always reported. Every time the
        <code class="literal">swift-update-from-model-rebalance-rings.yml</code> playbook
        is run, it will execute the swift-ring-builder rebalance command. This
        happens even if there were no input model changes. If the ring is
        already well balanced, the swift-ring-builder will not rewrite the
        ring.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect2" id="topic-ohx-j1t-4t"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Determining When to Rebalance and Deploy a New Ring</span> <a title="Permalink" class="permalink" href="#topic-ohx-j1t-4t">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-safe_rebalance_deploy_ring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-safe_rebalance_deploy_ring.xml</li><li><span class="ds-label">ID: </span>topic-ohx-j1t-4t</li></ul></div></div></div></div><p>
  Before deploying a new ring, you must be sure the change that has been
  applied to the last ring is complete (that is, all the partitions are in
  their correct location). There are three aspects to this:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Is the replication system busy?
   </p><p>
    You might want to postpone a ring change until after replication has
    finished. If the replication system is busy repairing a failed drive, a
    ring change will place additional load on the system. To check that
    replication has finished, use the <code class="literal">swift-recon</code> command
    with the <span class="bold"><strong>--replication</strong></span> argument. (For more
    information, see <a class="xref" href="#swift-recon" title="8.2. Gathering Swift Data">Section 8.2, “Gathering Swift Data”</a>.) The
    oldest completion time can indicate that the replication process is very
    busy. If it is more than 15 or 20 minutes then the object replication
    process are probably still very busy. The following example indicates
    that the oldest completion is 120 seconds, so that the replication
    process is probably not busy:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>swift-recon --replication
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-10-02 15:31:45] Checking on replication
[replication_time] low: 0, high: 0, avg: 0.0, total: 0, Failed: 0.0%, no_result: 0, reported: 3
Oldest completion was 2015-10-02 15:31:32 (120 seconds ago) by 192.168.245.4:6000.
Most recent completion was 2015-10-02 15:31:43 (10 seconds ago) by 192.168.245.3:6000.
===============================================================================</pre></div></li></ul></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Are there drive or server failures?
   </p><p>
    A drive failure does not preclude deploying a new ring. In principle, there
    should be two copies elsewhere. However, another drive failure in the
    middle of replication might make data temporary unavailable. If possible,
    postpone ring changes until all servers and drives are operating normally.
   </p></li><li class="listitem "><p>
    Has <code class="literal">min-part-hours</code> elapsed?
   </p><p>
    The <code class="literal">swift-ring-builder</code> will refuse to build a new ring
    until the <code class="literal">min-part-hours</code> has elapsed since the last time
    it built rings. You must postpone changes until this time has elapsed.
   </p><p>
    You can determine how long you must wait by running the
    <code class="literal">swift-compare-model-rings.yml</code> playbook, which will tell
    you how long you until the <code class="literal">min-part-hours</code> has elapsed.
    For more details, see <a class="xref" href="#swift-ansible-playbooks" title="8.5.3. Managing Rings Using Swift Playbooks">Section 8.5.3, “Managing Rings Using Swift Playbooks”</a>.
   </p><p>
    You can change the value of <code class="literal">min-part-hours</code>. (For
    instructions, see <a class="xref" href="#min-part-hours" title="8.5.7. Changing min-part-hours in Swift">Section 8.5.7, “Changing min-part-hours in Swift”</a>).
   </p></li><li class="listitem "><p>
    Is the Swift dispersion report clean?
   </p><p>
    Run the <code class="literal">swift-dispersion-report.yml</code> playbook (as
    described in <a class="xref" href="#swift-healthcheck" title="8.1. Running the Swift Dispersion Report">Section 8.1, “Running the Swift Dispersion Report”</a>) and
    examine the results. If the replication process has not yet replicated
    partitions that were moved to new drives in the last ring rebalance, the
    dispersion report will indicate that some containers or objects are
    missing a copy.
   </p><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen">There were 462 partitions missing one copy.</pre></div><p>
    Assuming all servers and disk drives are operational, the reason for the
    missing partitions is that the replication process has not yet managed to
    copy a replica into the partitions.
   </p><p>
    You should wait an hour and rerun the dispersion report process and examine
    the report. The number of partitions missing one copy should have reduced.
    Continue to wait until this reaches zero before making any further ring
    rebalances.
   </p><div id="id-1.6.10.9.10.4.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     It is normal to see partitions missing one copy if disk drives or
     servers are down. If all servers and disk drives are mounted, and you
     did not recently perform a ring rebalance, you should investigate
     whether there are problems with the replication process. You can use the
     Operations Console to investigate replication issues.
    </p></div><div id="id-1.6.10.9.10.4.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     If there are any partitions missing two copies, you must reboot or repair
     any failed servers and disk drives as soon as possible. Do not shutdown
     any Swift nodes in this situation. Assuming a replica count of 3, if you
     are missing two copies you are in danger of losing the only remaining
     copy.
    </p></div></li></ul></div></div><div class="sect2" id="change-swift-rings"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Applying Input Model Changes to Existing Rings</span> <a title="Permalink" class="permalink" href="#change-swift-rings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>change-swift-rings</li></ul></div></div></div></div><p>
  This page describes a general approach for making changes to your existing
  Swift rings. This approach applies to actions such as adding and removing a
  server and replacing and upgrading disk drives, and must be performed as a
  series of phases, as shown below:
 </p><div class="sect3" id="change-inputmodel"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the Input Model Configuration Files</span> <a title="Permalink" class="permalink" href="#change-inputmodel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>change-inputmodel</li></ul></div></div></div></div><p>
   The first step to apply new changes to the Swift environment is to update
   the configuration files. Follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Set the weight-step attribute, as needed, for the nodes you are altering.
     (For instructions, see <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>).
    </p></li><li class="listitem "><p>
     Edit the configuration files as part of the Input Model as appropriate.
     (For general information about the Input Model, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.14 “Networks”</span>. For more specific information about
     the Swift parts of the configuration files, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”</span>)
    </p></li><li class="listitem "><p>
     Once you have completed all of the changes, commit your configuration to
     the local git repository. (For more information,
     see<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>.) :
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">root # </code>git commit -m "commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Swift playbook that will validate your configuration files and
     give you a report as an output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">root # </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div></li><li class="listitem "><p>
     Use the report to validate that the number of drives proposed to be added
     or deleted, or the weight change, is correct. Fix any errors in your input
     model. At this stage, no changes have been made to rings.
    </p></li></ol></div></div><div class="sect3" id="first-rebalance"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">First phase of Ring Rebalance</span> <a title="Permalink" class="permalink" href="#first-rebalance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>first-rebalance</li></ul></div></div></div></div><p>
   To begin the rebalancing of the Swift rings, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After going through the steps in the section above, deploy your changes to
     all of the Swift nodes in your environment by running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished or <code class="literal">min-part-hours</code>
     has elapsed (whichever is longer). For more information, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li></ol></div></div><div class="sect3" id="weight-change"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Weight Change Phase of Ring Rebalance</span> <a title="Permalink" class="permalink" href="#weight-change">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>weight-change</li></ul></div></div></div></div><p>
   At this stage, no changes have been made to the input model. However, when
   you set the <code class="literal">weight-step</code> attribute, the rings that were
   rebuilt in the previous rebalance phase have weights that are different than
   their target/final value. You gradually move to the target/final weight by
   rebalancing a number of times as described on this page. For more
   information about the weight-step attribute, see
   <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
  </p><p>
   To begin the re-balancing of the rings, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Rebalance the rings by running the playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished or <code class="literal">min-part-hours</code>
     has elapsed (whichever is longer). For more information, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem "><p>
     Run the following command and review the report:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     The following is an example of the output after executing the above
     command. In the example <span class="bold"><strong>no</strong></span> weight changes
     are proposed:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | validate-input-model | Print report] *********
ok: [padawan-ccp-c1-m1-mgmt] =&gt; {
    "var": {
        "report.stdout_lines": [
            "Need to add 0 devices",
            "Need to remove 0 devices",
            "<span class="bold"><strong>Need to set weight on 0 devices</strong></span>"
        ]
    }
}</pre></div></li><li class="listitem "><p>
     When there are no proposed weight changes, you proceed to the final phase.
    </p></li><li class="listitem "><p>
     If there are proposed weight changes repeat this phase again.
    </p></li></ol></div></div><div class="sect3" id="final-rebalance"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Final Rebalance Phase</span> <a title="Permalink" class="permalink" href="#final-rebalance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>final-rebalance</li></ul></div></div></div></div><p>
   The final rebalance phase moves all replicas to their final destination.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Rebalance the rings by running the playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml | tee /tmp/rebalance.log</pre></div><div id="id-1.6.10.9.11.6.3.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The output is saved for later reference.
     </p></div></li><li class="listitem "><p>
     Review the output from the previous step. If the output for all rings is
     similar to the following, the rebalance had no effect. That is, the rings
     are balanced and no further changes are needed. In addition, the ring
     files were not changed so you do not need to deploy them to the Swift
     nodes:
    </p><div class="verbatim-wrap"><pre class="screen">"Running: swift-ring-builder /etc/swiftlm/cloud1/cp1/builder_dir/account.builder rebalance 999",
      "NOTE: No partitions could be reassigned.",
      "Either none need to be or none can be due to min_part_hours [16]."</pre></div><p>
     The text <span class="bold"><strong>No partitions could be
     reassigned</strong></span> indicates that no further rebalances are necessary.
     If this is true for all the rings, you have completed the final phase.
    </p><div id="id-1.6.10.9.11.6.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You must have allowed enough time to elapse since the last rebalance.
      As mentioned in the above example, <code class="literal">min_part_hours [16]</code>
      means that you must wait at least 16 hours since the last rebalance. If
      not, you should wait until enough time has elapsed and repeat this
      phase.
     </p></div></li><li class="listitem "><p>
     Run the <code class="literal">swift-reconfigure.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished or <code class="literal">min-part-hours</code>
     has elapsed (whichever is longer). For more information see
     <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem "><p>
     Repeat the above steps until the ring is rebalanced.
    </p></li></ol></div></div><div class="sect3" id="system-changes"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Changes that Change Existing Rings</span> <a title="Permalink" class="permalink" href="#system-changes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>system-changes</li></ul></div></div></div></div><p>
   There are many system changes ranging from adding servers to replacing
   drives, which might require you to rebuild and rebalance your rings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th><span class="bold"><strong>Actions</strong></span>
      </th><th><span class="bold"><strong>Process</strong></span>
      </th></tr></thead><tbody><tr><td>Adding Servers(s)</td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If not already done, set the weight change attribute. For
          instructions, see <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>
         </p></li><li class="listitem "><p>
          Add servers in phases:
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            This reduces the impact of the changes on your system.
           </p></li><li class="listitem "><p>
            If your rings use Swift zones, ensure that you remove the same
            number of servers to each zone at each phase.
           </p></li></ul></div></li></ul></div>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          To add a server, see <a class="xref" href="#sec-swift-add-object-node" title="13.1.5.1.1. Adding a Swift Object Node">Section 13.1.5.1.1, “Adding a Swift Object Node”</a>.
         </p></li><li class="listitem "><p>
          Incrementally change the weights and perform the final rebalance. For
          instructions, see <a class="xref" href="#final-rebalance" title="8.5.5.4. Final Rebalance Phase">Section 8.5.5.4, “Final Rebalance Phase”</a>.
         </p></li></ul></div>
      </td></tr><tr><td>Removing Server(s)</td><td>
       <p>
        In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, when you remove servers from the input model, the
        disk drives are removed from the ring - the weight is not gradually
        reduced using the <code class="literal">weight-step</code> attribute.
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Remove servers in phases:
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            This reduces the impact of the changes on your system.
           </p></li><li class="listitem "><p>
            If your rings use Swift zones, ensure you remove the same number of
            servers for each zone at each phase.
           </p></li></ul></div></li></ul></div>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          To remove a server, see <a class="xref" href="#remove-swift-node" title="13.1.5.1.4. Removing a Swift Node">Section 13.1.5.1.4, “Removing a Swift Node”</a>.
         </p></li><li class="listitem "><p>
          To perform the final rebalance, see <a class="xref" href="#final-rebalance" title="8.5.5.4. Final Rebalance Phase">Section 8.5.5.4, “Final Rebalance Phase”</a>.
         </p></li></ul></div>
      </td></tr><tr><td>Replacing Disk Drive(s)</td><td>
       <p>
        When a drive fails, replace it as soon as possible. Do not attempt to
        remove it from the ring - this creates operator overhead. Swift will
        continue to store the correct number of replicas by handing off objects
        to other drives instead of the failed drive.
       </p>
       <p>
        If the disk drives are of the same size as the original when the
        drive is replaced, no ring changes are required. You can confirm this
        by running the
        <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
        playbook. It should report that no weight changes are needed.
       </p>
       <p>
        For a single drive replacement, even if the drive is significantly
        larger than the original drives, you do not need to rebalance the ring
        (however, the extra space on the drive will not be used).
       </p>
      </td></tr><tr><td>Upgrading Disk Drives</td><td>
       <p>
        If the drives are different size (for example, you are upgrading your
        system), you can proceed as follows:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If not already done, set the weight-step attribute
         </p></li><li class="listitem "><p>
          Replace drives in phases:
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            Avoid replacing too many drives at once.
           </p></li><li class="listitem "><p>
            If your rings use swift zones, upgrade a number of drives in the
            same zone at the same time - not drives in several zones.
           </p></li><li class="listitem "><p>
            It is also safer to upgrade one server instead of drives in several
            servers at the same time.
           </p></li><li class="listitem "><p>
            Remember that the final size of all Swift zones must be the same,
            so you may need to replace a small number of drives in one zone,
            then a small number in second zone, then return to the first zone
            and replace more drives, etc.
           </p></li></ul></div></li></ul></div>
      </td></tr></tbody></table></div></div></div><div class="sect2" id="add-storage-policy"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New Swift Storage Policy</span> <a title="Permalink" class="permalink" href="#add-storage-policy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-add_new_storage_policy.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-add_new_storage_policy.xml</li><li><span class="ds-label">ID: </span>add-storage-policy</li></ul></div></div></div></div><p>
  This page describes how to add an additional storage policy to an existing
  system. For an overview of storage policies, see
  <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.11 “Designing Storage Policies”</span>.
 </p><p>
  <span class="bold"><strong>To Add a Storage Policy</strong></span>
 </p><p>
  Perform the following steps to add the storage policy to an existing system.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Select a storage policy index and ring name.
   </p><p>
    For example, if you already have object-0 and object-1 rings in your
    ring-specifications (usually in the
    <code class="literal">~/openstack/my_cloud/definition/data/swift/rings.yml</code> file),
    the next index is 2 and the ring name is object-2.
   </p></li><li class="listitem "><p>
    Select a user-visible name so that you can see when you examine container
    metadata or when you want to specify the storage policy used when you
    create a container. The name should be a single word (hyphen and dashes are
    allowed).
   </p></li><li class="listitem "><p>
    Decide if this new policy will be the default for all new containers.
   </p></li><li class="listitem "><p>
    Decide on other attributes such as <code class="literal">partition-power</code> and
    <code class="literal">replica-count</code> if you are using a standard replication
    ring. However, if you are using an erasure coded ring, you also need to
    decide on other attributes: <code class="literal">ec-type</code>,
    <code class="literal">ec-num-data-fragments</code>,
    <code class="literal">ec-num-parity-fragments</code>, and
    <code class="literal">ec-object-segment-size</code>. For more details on the required
    attributes, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.10 “Understanding Swift Ring Specifications”</span>.
   </p></li><li class="listitem "><p>
    Edit the <code class="literal">ring-specifications</code> attribute (usually in the
    <code class="literal">~/openstack/my_cloud/definition/data/swift/rings.yml</code> file)
    and add the new ring specification. If this policy is to be the default
    storage policy for new containers, set the <code class="literal">default</code>
    attribute to <span class="bold"><strong>yes</strong></span>.
   </p><div id="id-1.6.10.9.12.5.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Ensure that only one object ring has the
       <code class="literal">default</code> attribute set to <code class="literal">yes</code>. If
       you set two rings as default, Swift processes will not start.
      </p></li><li class="listitem "><p>
       Do not specify the <code class="literal">weight-step</code> attribute for the new
       object ring. Since this is a new ring there is no need to gradually
       increase device weights.
      </p></li></ol></div></div></li><li class="listitem "><p>
    Update the appropriate disk model to use the new storage policy (for
    example, the <code class="literal">data/disks_swobj.yml</code> file). The following
    sample shows that the <span class="bold"><strong>object-2</strong></span> has been
    added to the list of existing rings that use the drives:
   </p><div class="verbatim-wrap"><pre class="screen">disk-models:
- name: SWOBJ-DISKS
  ...
  device-groups:
  - name: swobj
    devices:
       ...
    consumer:
        name: swift
        attrs:
            rings:
            - object-0
            - object-1
            - <span class="bold"><strong>object-2</strong></span>
  ...</pre></div><div id="id-1.6.10.9.12.5.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     You must use the new object ring on at least one node that runs the
     <code class="literal">swift-object</code> service. If you skip this step and
     continue to run the <code class="literal">swift-compare-model-rings.yml</code> or
     <code class="literal">swift-deploy.yml</code> playbooks, they will fail with an
     error <span class="emphasis"><em>There are no devices in this ring, or all devices have
     been deleted</em></span>, as shown below:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | build-rings | Build ring (make-delta, rebalance)] ***
failed: [padawan-ccp-c1-m1-mgmt] =&gt; {"changed": true, "cmd": ["swiftlm-ring-supervisor", "--make-delta", "--rebalance"], "delta": "0:00:03.511929", "end": "2015-10-07 14:02:03.610226", "rc": 2, "start": "2015-10-07 14:02:00.098297", "warnings": []}
...
Running: swift-ring-builder /etc/swiftlm/cloud1/cp1/builder_dir/object-2.builder rebalance 999
ERROR: -------------------------------------------------------------------------------
An error has occurred during ring validation. Common
causes of failure are rings that are empty or do not
have enough devices to accommodate the replica count.
Original exception message:
There are no devices in this ring, or all devices have been deleted
-------------------------------------------------------------------------------</pre></div></div></li><li class="listitem "><p>
    Commit your configuration:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "commit message"</pre></div></li><li class="listitem "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Create a deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Validate the changes by running the
    <code class="literal">swift-compare-model-rings.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div><p>
    If any errors occur, correct them. For instructions, see
    <a class="xref" href="#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>. Then, re-run steps
    <span class="bold"><strong>5 - 10</strong></span>.
    
   </p></li><li class="listitem "><p>
    Create the new ring (for example, object-2). Then verify the Swift service
    status and reconfigure the Swift node to use a new storage policy, by
    running these playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li></ol></div><p>
  After adding a storage policy, there is no need to rebalance the ring.
 </p></div><div class="sect2" id="min-part-hours"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing min-part-hours in Swift</span> <a title="Permalink" class="permalink" href="#min-part-hours">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_min_part_hours.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_min_part_hours.xml</li><li><span class="ds-label">ID: </span>min-part-hours</li></ul></div></div></div></div><p>
  The <code class="literal">min-part-hours</code> parameter specifies the number of
  hours you must wait before Swift will allow a given partition to be moved.
  In other words, it constrains how often you perform ring rebalance
  operations. Before changing this value, you should get some experience with
  how long it takes your system to perform replication after you make ring
  changes (for example, when you add servers).
 </p><p>
  See <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for more information about
  determining when replication has completed.
 </p><div class="sect3" id="idg-all-operations-objectstorage-swift-min-part-hours-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the min-part-hours Value</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-swift-min-part-hours-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_min_part_hours.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_min_part_hours.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-swift-min-part-hours-xml-7</li></ul></div></div></div></div><p>
   To change the <code class="literal">min-part-hours</code> value, following these
   steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your
     <code class="literal">~/openstack/my_cloud/definition/data/swift/rings.yml</code> file
     and change the value(s) of <code class="literal">min-part-hours</code> for the rings
     you desire. The value is expressed in hours and a value of zero is not
     allowed.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Apply the changes by running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li></ol></div></div></div><div class="sect2" id="changing-swift-zone"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing Swift Zone Layout</span> <a title="Permalink" class="permalink" href="#changing-swift-zone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-changing_swift_zone.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-changing_swift_zone.xml</li><li><span class="ds-label">ID: </span>changing-swift-zone</li></ul></div></div></div></div><p>
  Before changing the number of Swift zones or the assignment of servers to
  specific zones, you must ensure that your system has sufficient storage
  available to perform the operation. Specifically, if you are adding a new
  zone, you may need additional storage. There are two reasons for this:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    You cannot simply change the Swift zone number of disk drives in the ring.
    Instead, you need to remove the server(s) from the ring and then re-add
    the server(s) with a new Swift zone number to the ring. At the point where
    the servers are removed from the ring, there must be sufficient spare
    capacity on the remaining servers to hold the data that was originally
    hosted on the removed servers.
   </p></li><li class="listitem "><p>
    The total amount of storage in each Swift zone must be the same. This is
    because new data is added to each zone at the same rate. If one zone has a
    lower capacity than the other zones, once that zone becomes full, you
    cannot add more data to the system – even if there is unused space in
    the other zones.
   </p></li></ul></div><p>
  As mentioned above, you cannot simply change the Swift zone number of disk
  drives in an existing ring. Instead, you must remove and then re-add
  servers. This is a summary of the process:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Identify appropriate server groups that correspond to the desired Swift
    zone layout.
   </p></li><li class="listitem "><p>
    Remove the servers in a server group from the rings. This process may be
    protracted, either by removing servers in small batches or by using the
    weight-step attribute so that you limit the amount of replication traffic
    that happens at once.
   </p></li><li class="listitem "><p>
    Once all the targeted servers are removed, edit the
    <code class="literal">swift-zones</code> attribute in the ring specifications to add
    or remove a Swift zone.
   </p></li><li class="listitem "><p>
    Re-add the servers you had temporarily removed to the rings. Again you may
    need to do this in batches or rely on the weight-step attribute.
   </p></li><li class="listitem "><p>
    Continue removing and re-adding servers until you reach your final
    configuration.
   </p></li></ol></div><div class="sect3" id="idg-all-operations-objectstorage-changing-swift-zone-xml-5"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.5.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Process for Changing Swift Zones</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-changing-swift-zone-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-changing_swift_zone.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-changing_swift_zone.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-changing-swift-zone-xml-5</li></ul></div></div></div></div><p>
   This section describes the detailed process or reorganizing Swift zones. As
   a concrete example, we assume we start with a single Swift zone and the
   target is three Swift zones. The same general process would apply if you
   were reducing the number of zones as well.
  </p><p>
   The process is as follows:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Identify the appropriate server groups that represent the desired final
     state. In this example, we are going to change the Swift zone layout as
     follows:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Original Layout</th><th>Target Layout</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">swift-zones:
  - 1d: 1
    server-groups:
       - AZ1
       - AZ2
       - AZ3</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">swift-zones:
   - 1d: 1
     server-groups:
        - AZ1
   - id: 2
        - AZ2
   - id: 3
        - AZ3</pre></div>
        </td></tr></tbody></table></div><p>
     The plan is to move servers from server groups <code class="literal">AZ2</code> and
     <code class="literal">AZ3</code> to a new Swift zone number. The servers in
     <code class="literal">AZ1</code> will remain in Swift zone 1.
    </p></li><li class="listitem "><p>
     If you have not already done so, consider setting the weight-step
     attribute as described in <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Identify the servers in the <code class="literal">AZ2</code> server group. You may
     remove all servers at once or remove them in batches. If this is the first
     time you have performed a major ring change, we suggest you remove one or
     two servers only in the first batch. When you see how long this takes and
     the impact replication has on your system you can then use that experience
     to decide whether you can remove a larger batch of servers, or increase or
     decrease the weight-step attribute for the next server-removal cycle. To
     remove a server, use steps 2-9 as described in
     <a class="xref" href="#remove-swift-node" title="13.1.5.1.4. Removing a Swift Node">Section 13.1.5.1.4, “Removing a Swift Node”</a> ensuring that you
     do not remove the servers from the input model.
     
    </p></li><li class="listitem "><p>
     This process may take a number of ring rebalance cycles until the disk
     drives are removed from the ring files. Once this happens, you can edit
     the ring specifications and add Swift zone 2 as shown in this example:
    </p><div class="verbatim-wrap"><pre class="screen">swift-zones:
  - id: 1
    server-groups:
      - AZ1
      - AZ3
  - id: 2
       - AZ2</pre></div></li><li class="listitem "><p>
     The server removal process in step #3
     
     set the "remove" attribute in the
     <code class="literal">pass-through</code> attribute of the servers in server group
     <code class="literal">AZ2</code>. Edit the input model files and remove this
     <code class="literal">pass-through</code> attribute. This signals to the system that
     the servers should be used the next time we rebalance the rings (that is,
     the server should be added to the rings).
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Rebuild and deploy the Swift rings containing the re-added servers by
     running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished. For more details, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
    </p></li><li class="listitem "><p>
     You may need to continue to rebalance the rings. For instructions, see the
     "Final Rebalance Stage" steps at <a class="xref" href="#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li><li class="listitem "><p>
     At this stage, the servers in server group <code class="literal">AZ2</code> are
     responsible for Swift zone 2. Repeat the process in steps #3-9 to remove
     the servers in server group <code class="literal">AZ3</code> from the rings and then
     re-add them to Swift zone 3. The ring specifications for zones (step 4)
     should be as follows:
    </p><div class="verbatim-wrap"><pre class="screen">swift-zones:
  - 1d: 1
    server-groups:
      - AZ1
  - id: 2
      - AZ2
  - id: 3
      - AZ3</pre></div></li><li class="listitem "><p>
     Once complete, all data should be dispersed (that is, each replica is
     located) in the Swift zones as specified in the input model.
    </p></li></ol></div></div></div></div><div class="sect1" id="topic-el2-cqv-mv"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring your Swift System to Allow Container Sync</span> <a title="Permalink" class="permalink" href="#topic-el2-cqv-mv">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>topic-el2-cqv-mv</li></ul></div></div></div></div><p>
  Swift has a feature where all the contents of a container can be mirrored to
  another container through background synchronization. Swift operators
  configure their system to allow/accept sync requests to/from other systems,
  and the user specifies where to sync their container to along with a secret
  synchronization key. For an overview of this feature, refer to
  <a class="link" href="http://docs.openstack.org/developer/swift/overview_container_sync.html" target="_blank">OpenStack
  Swift - Container to Container Synchronization</a>.
 </p><div class="sect2" id="idg-all-operations-objectstorage-swift-container-sync-xml-5"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes and limitations</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-swift-container-sync-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-swift-container-sync-xml-5</li></ul></div></div></div></div><p>
   The container synchronization is done as a background action. When you put
   an object into the source container, it will take some time before it
   becomes visible in the destination container. Storage services will not
   necessarily copy objects in any particular order, meaning they may be
   transferred in a different order to which they were created.
  </p><p>
   Container sync may not be able to keep up with a moderate upload rate to a
   container. For example, if the average object upload rate to a container is
   greater than one object per second, then container sync may not be able to
   keep the objects synced.
  </p><p>
   If container sync is enabled on a container that already has a large number
   of objects then container sync may take a long time to sync the data. For
   example, a container with one million 1KB objects could take more than 11
   days to complete a sync.
  </p><p>
   You may operate on the destination container just like any other container
   -- adding or deleting objects -- including the objects that are in the
   destination container because they were copied from the source container. To
   decide how to handle object creation, replacement or deletion, the system
   uses timestamps to determine what to do. In general, the latest timestamp
   "wins". That is, if you create an object, replace it, delete it and the
   re-create it, the destination container will eventually contain the most
   recently created object. However, if you also create and delete objects in
   the destination container, you get some subtle behaviours as follows:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If an object is copied to the destination container and then deleted, it
     remains deleted in the destination even though there is still a copy in
     the source container. If you modify the object (replace or change its
     metadata) in the source container, it will reappear in the destination
     again.
    </p></li><li class="listitem "><p>
     The same applies to a replacement or metadata modification of an object in
     the destination container -- the object will remain as-is unless there is
     a replacement or modification in the source container.
    </p></li><li class="listitem "><p>
     If you replace or modify metadata of an object in the destination
     container and then delete it in the source container, it is
     <span class="bold"><strong>not</strong></span> deleted from the destination. This is
     because your modified object has a later timestamp than the object you
     deleted in the source.
    </p></li><li class="listitem "><p>
     If you create an object in the source container and before the system has
     a chance to copy it to the destination, you also create an object of the
     same name in the destination, then the object in the destination is
     <span class="bold"><strong>not</strong></span> overwritten by the source container's
     object.
    </p></li></ul></div><p>
   <span class="bold"><strong>Segmented objects</strong></span>
  </p><p>
   Segmented objects (objects larger than 5GB) will not work seamlessly with
   container synchronization. If the manifest object is copied to the
   destination container before the object segments, when you perform a GET
   operation on the manifest object, the system may fail to find some or all of
   the object segments. If your manifest and object segments are in different
   containers, do not forget that both containers must be synchonized and that
   the container name of the object segments must be the same on both source
   and destination.
  </p></div><div class="sect2" id="idg-all-operations-objectstorage-swift-container-sync-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-swift-container-sync-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-swift-container-sync-xml-6</li></ul></div></div></div></div><p>
   Container to container synchronization requires that SSL certificates are
   configured on both the source and destination systems. For more information
   on how to implement SSL, see <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 29 “Configuring Transport Layer Security (TLS)”</span>.
  </p></div><div class="sect2" id="configure"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring container sync</span> <a title="Permalink" class="permalink" href="#configure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>configure</li></ul></div></div></div></div><p>
   Container to container synchronization requires that both the source and
   destination Swift systems involved be configured to allow/accept this.  In
   the context of container to container synchronization, Swift uses the term
   <span class="emphasis"><em>cluster</em></span> to denote a Swift system. Swift
   <span class="emphasis"><em>clusters</em></span> correspond to <span class="emphasis"><em>Control
   Planes</em></span> in <span class="productname">OpenStack</span> terminology.
   </p><p>
   <span class="bold"><strong>Gather the public API endpoints for both Swift
   systems</strong></span>
  </p><p>
   Gather information about the external/public URL used by each system, as
   follows:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the Cloud Lifecycle Manager of one system, get the public API endpoint of the
     system by running the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack endpoint list | grep swift</pre></div><p>
     The output of the command will look similar to this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack endpoint list | grep swift
| 063a84b205c44887bc606c3ba84fa608 | region0 | swift           | object-store    | True    | admin     | https://10.13.111.176:8080/v1/AUTH_%(tenant_id)s |
| 3c46a9b2a5f94163bb5703a1a0d4d37b | region0 | swift           | object-store    | True    | public    | <span class="bold"><strong>https://10.13.120.105:8080/v1</strong></span>/AUTH_%(tenant_id)s |
| a7b2f4ab5ad14330a7748c950962b188 | region0 | swift           | object-store    | True    | internal  | https://10.13.111.176:8080/v1/AUTH_%(tenant_id)s |</pre></div><p>
     The portion that you want is the endpoint up to, but not including, the
     <code class="literal">AUTH</code> part. It is bolded in the above example,
     <code class="literal">https://10.13.120.105:8080/v1</code>.
    </p></li><li class="listitem "><p>
     Repeat these steps on the other Swift system so you have both of the
     public API endpoints for them.
    </p></li></ol></div><p>
   <span class="bold"><strong>Validate connectivity between both systems</strong></span>
  </p><p>
   The Swift nodes running the <code class="literal">swift-container</code> service must
   be able to connect to the public API endpoints of each other for the
   container sync to work. You can validate connectivity on each system using
   these steps.
  </p><p>
   For the sake of the examples, we will use the terms
   <span class="emphasis"><em>source</em></span> and <span class="emphasis"><em>destination</em></span> to notate
   the nodes doing the synchronization.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to a Swift node running the <code class="literal">swift-container</code>
     service on the source system. You can determine this by looking at the
     service list in your
     <code class="literal">~/openstack/my_cloud/info/service_info.yml</code> file for a list
     of the servers containing this service.
    </p></li><li class="listitem "><p>
     Verify the SSL certificates by running this command against the
     destination Swift server:
    </p><div class="verbatim-wrap"><pre class="screen">echo | openssl s_client -connect <em class="replaceable ">PUBLIC_API_ENDPOINT</em>:8080 -CAfile /etc/ssl/certs/ca-certificates.crt</pre></div><p>
     If the connection was successful you should see a return code of
     <code class="literal">0 (ok)</code> similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">...
Timeout   : 300 (sec)
Verify return code: 0 (ok)</pre></div></li><li class="listitem "><p>
     Also verify that the source node can connect to the destination Swift
     system using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl -k <em class="replaceable ">DESTINATION_IP OR HOSTNAME</em>:8080/healthcheck</pre></div><p>
     If the connection was successful, you should see a response of
     <code class="literal">OK</code>.
    </p></li><li class="listitem "><p>
     Repeat these verification steps on any system involved in your container
     synchronization setup.
    </p></li></ol></div><p>
   <span class="bold"><strong>Configure container to container
   synchronization</strong></span>
  </p><p>
   Both the source and destination Swift systems must be configured the same
   way, using sync realms. For more details on how sync realms work, see
   <a class="link" href="http://docs.openstack.org/developer/swift/overview_container_sync.html#configuring-container-sync" target="_blank">OpenStack
   Swift - Configuring Container Sync</a>.
  </p><p>
   To configure one of the systems, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/config/swift/container-sync-realms.conf.j2</code>
     file and uncomment the sync realm section.
    </p><p>
     Here is a sample showing this section in the file:
    </p><div class="verbatim-wrap"><pre class="screen">#Add sync realms here, for example:
# [realm1]
# key = realm1key
# key2 = realm1key2
# cluster_name1 = https://host1/v1/
# cluster_name2 = https://host2/v1/</pre></div></li><li class="listitem "><p>
     Add in the details for your source and destination systems. Each realm you
     define is a set of clusters that have agreed to allow container syncing
     between them. These values are case sensitive.
    </p><p>
     Only one <code class="literal">key</code> is required. The second key is optional
     and can be provided to allow an operator to rotate keys if desired. The
     values for the clusters must contain the prefix
     <code class="literal">cluster_</code> and will be populated with the public API
     endpoints for the systems.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update the deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Swift reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Run this command to validate that your container synchronization is
     configured:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>swift capabilities</pre></div><p>
     Here is a snippet of the output showing the container sync information.
     This should be populated with your cluster names:
    </p><div class="verbatim-wrap"><pre class="screen">...
Additional middleware: container_sync
 Options:
  realms: {u'INTRACLUSTER': {u'clusters': {u'THISCLUSTER': {}}}}</pre></div></li><li class="listitem "><p>
     Repeat these steps on any other Swift systems that will be involved in
     your sync realms.
    </p></li></ol></div></div><div class="sect2" id="intra-cluster-sync"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Intra Cluster Container Sync</span> <a title="Permalink" class="permalink" href="#intra-cluster-sync">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>intra-cluster-sync</li></ul></div></div></div></div><p>
   It is possible to use the swift container sync functionality to sync objects
   between containers within the same swift system. Swift is automatically
   configured to allow intra cluster container sync. Each swift PAC server will
   have an intracluster container sync realm defined in
   <code class="literal">/etc/swift/container-sync-realms.conf</code>.
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"># The intracluster realm facilitates syncing containers on this system
[intracluster]
key = lQ8JjuZfO
# key2 =
cluster_thiscluster = http://<em class="replaceable ">SWIFT-PROXY-VIP</em>:8080/v1/</pre></div><p>
   The keys defined in <code class="literal">/etc/swift/container-sync-realms.conf</code>
   are used by the container-sync daemon to determine trust. On top of this
   the containers that will be in sync will need a seperate shared key they
   both define in container metadata to establish their trust between each other.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create two containers, for example container-src and container-dst. In
     this example we will sync one way from container-src to container-dst.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift post container-src
<code class="prompt user">ardana &gt; </code>swift post container-dst</pre></div></li><li class="listitem "><p>
     Determine your swift account. In the following example it is AUTH_1234
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift stat
                                 Account: AUTH_1234
                              Containers: 3
                                 Objects: 42
                                   Bytes: 21692421
Containers in policy "erasure-code-ring": 3
   Objects in policy "erasure-code-ring": 42
     Bytes in policy "erasure-code-ring": 21692421
                            Content-Type: text/plain; charset=utf-8
             X-Account-Project-Domain-Id: default
                             X-Timestamp: 1472651418.17025
                              X-Trans-Id: tx81122c56032548aeae8cd-0057cee40c
                           Accept-Ranges: bytes</pre></div></li><li class="listitem "><p>
     Configure container-src to sync to container-dst using a key specified
     by both containers. Replace <em class="replaceable ">KEY</em> with your key.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift post -t '//intracluster/thiscluster/AUTH_1234/container-dst' -k '<em class="replaceable ">KEY</em>' container-src</pre></div></li><li class="listitem "><p>
     Configure container-dst to accept synced objects with this key
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift post -k '<em class="replaceable ">KEY</em>' container-dst</pre></div></li><li class="listitem "><p>
     Upload objects to container-src. Within a number of minutes the objects
     should be automatically synced to container-dst.
    </p></li></ol></div><p>
   <span class="bold"><strong>Changing the intracluster realm key</strong></span>
  </p><p>
   The intracluster realm key used by container sync to sync objects between
   containers in the same swift system is automatically generated. The process
   for changing passwords is described in
   <a class="xref" href="#servicePasswords" title="4.7. Changing Service Passwords">Section 4.7, “Changing Service Passwords”</a>.
  </p><p>
   The steps to change the intracluster realm key are as follows.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the Cloud Lifecycle Manager create a file called
     <code class="literal">~/openstack/change_credentials/swift_data_metadata.yml</code>
     with the contents included below. The <code class="literal">consuming-cp</code> and
     <code class="literal">cp</code> are the control plane name specified in
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     where the swift-container service is running.
    </p><div class="verbatim-wrap"><pre class="screen">swift_intracluster_sync_key:
 metadata:
 - clusters:
   - swpac
   component: swift-container
   consuming-cp: control-plane-1
   cp: control-plane-1
 version: '2.0'</pre></div></li><li class="listitem "><p>
     Run the following commands
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Reconfigure the swift credentials
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure-credentials-change.yml</pre></div></li><li class="listitem "><p>
     Delete
     <code class="literal">~/openstack/change_credentials/swift_data_metadata.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>rm ~/openstack/change_credentials/swift_data_metadata.yml</pre></div></li><li class="listitem "><p>
     On a swift PAC server check that the intracluster realm key has been
     updated in <code class="literal">/etc/swift/container-sync-realms.conf</code>
    </p><div class="verbatim-wrap"><pre class="screen"># The intracluster realm facilitates syncing containers on this system
[intracluster]
key = aNlDn3kWK</pre></div></li><li class="listitem "><p>
     Update any containers using the intracluster container sync to use the new
     intracluster realm key
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift post -k 'aNlDn3kWK' container-src
<code class="prompt user">ardana &gt; </code>swift post -k 'aNlDn3kWK' container-dst</pre></div></li></ol></div></div></div></div><div class="chapter " id="ops-managing-networking"><div class="titlepage"><div><div><h1 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Networking</span> <a title="Permalink" class="permalink" href="#ops-managing-networking">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_networking.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_networking.xml</li><li><span class="ds-label">ID: </span>ops-managing-networking</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic-gll-nsn-15"><span class="number">9.1 </span><span class="name">Configuring the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Firewall</span></a></span></dt><dt><span class="section"><a href="#DesignateOverview"><span class="number">9.2 </span><span class="name">DNS Service Overview</span></a></span></dt><dt><span class="section"><a href="#neutron-overview"><span class="number">9.3 </span><span class="name">Networking Service Overview</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Networking service.
 </p><div class="sect1" id="topic-gll-nsn-15"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Firewall</span> <a title="Permalink" class="permalink" href="#topic-gll-nsn-15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>topic-gll-nsn-15</li></ul></div></div></div></div><p>
  The following instructions provide information about how to identify and
  modify the overall <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall that is configured in front of the
  control services. This firewall is administered only by a cloud admin and is
  not available for tenant use for private network firewall services.
 </p><p>
  During the installation process, the configuration processor will
  automatically generate "allow" firewall rules for each server based on the
  services deployed and block all other ports. These are populated in
  <code class="literal">~/openstack/my_cloud/info/firewall_info.yml</code>, which includes
  a list of all the ports by network, including the addresses on which the
  ports will be opened. This is described in more detail in
  <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”, Section 5.2 “Concepts”, Section 5.2.10 “Networking”, Section 5.2.10.5 “Firewall Configuration”</span>.
 </p><p>
  The <code class="literal">firewall_rules.yml</code> file in the input model allows you
  to define additional rules for each network group. You can read more about
  this in <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.15 “Firewall Rules”</span>.
 </p><p>
  The purpose of this document is to show you how to make post-installation
  changes to the firewall rules if the need arises.
 </p><div id="id-1.6.11.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   This process is not to be confused with Firewall-as-a-Service (see
   <span class="intraxref">Book “User Guide”, Chapter 14 “Using Firewall as a Service (FWaaS)”</span>),
   which is a separate service that enables the ability for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tenants
   to create north-south, network-level firewalls to provide stateful
   protection to all instances in a private, tenant network. This service is
   optional and is tenant-configured.
  </p></div><div class="sect2" id="idg-all-operations-configure-firewall-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Changes to the Firewall Rules</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configure-firewall-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configure-firewall-xml-7</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your
     <code class="literal">~/openstack/my_cloud/definition/data/firewall_rules.yml</code>
     file and add the lines necessary to allow the port(s) needed through the
     firewall.
    </p><p>
     In this example we are going to open up port range 5900-5905 to allow VNC
     traffic through the firewall:
    </p><div class="verbatim-wrap"><pre class="screen">  - name: VNC
    network-groups:
  - MANAGEMENT
    rules:
     - type: allow
       remote-ip-prefix:  0.0.0.0/0
       port-range-min: 5900
       port-range-max: 5905
       protocol: tcp</pre></div><div id="id-1.6.11.3.7.2.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The example above shows a <code class="literal">remote-ip-prefix</code> of
      <code class="literal">0.0.0.0/0</code> which opens the ports up to all IP ranges.
      To be more secure you can specify your local IP address CIDR you will
      be running the VNC connect from.
     </p></div></li><li class="listitem "><p>
     Commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "firewall rule update"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create the deployment directory structure:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Change to the deployment directory and run the
     <code class="literal">osconfig-iptables-deploy.yml</code> playbook to update your
     iptable rules to allow VNC:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-iptables-deploy.yml</pre></div></li></ol></div><p>
   You can repeat these steps as needed to add, remove, or edit any of these
   firewall rules.
  </p></div></div><div class="sect1" id="DesignateOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Overview</span> <a title="Permalink" class="permalink" href="#DesignateOverview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span>DesignateOverview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS service provides multi-tenant Domain Name Service with REST
  API management for domain and records.
 </p><div id="id-1.6.11.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   The DNS Service is not intended to be used as an
   <span class="emphasis"><em>internal</em></span> or
   <span class="emphasis"><em>private</em></span> DNS service. The name records in
   DNSaaS should be treated as public information that anyone could query.
   There are controls to prevent tenants from creating records for domains they
   do not own. TSIG provides a <span class="bold"><strong>T</strong></span>ransaction
   <span class="bold"><strong>SIG</strong></span> nature to ensure integrity during zone
   transfer to other DNS servers.
  </p></div><div class="sect2" id="id-1.6.11.4.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.6.11.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For more information about Designate REST APIs, see the <span class="productname">OpenStack</span> REST
     API Documentation at
     <a class="link" href="http://docs.openstack.org/developer/designate/rest.html" target="_blank">http://docs.openstack.org/developer/designate/rest.html</a>.
    </p></li><li class="listitem "><p>
     For a glossary of terms for Designate, see the <span class="productname">OpenStack</span> glossary at
     <a class="link" href="http://docs.openstack.org/developer/designate/glossary.html" target="_blank">http://docs.openstack.org/developer/designate/glossary.html</a>.
    </p></li></ul></div></div><div class="sect2" id="DesignateInitialConfig"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designate Initial Configuration</span> <a title="Permalink" class="permalink" href="#DesignateInitialConfig">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>DesignateInitialConfig</li></ul></div></div></div></div><p>
  After the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
  has been completed, Designate requires initial configuration to operate.
 </p><div class="sect3" id="sec-designate-identify"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying Name Server Public IPs</span> <a title="Permalink" class="permalink" href="#sec-designate-identify">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-identify</li></ul></div></div></div></div><p>
   Depending on the back-end, the method used to identify the name servers'
   public IPs will differ.
  </p><div class="sect4" id="sec-designate-infoblox"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">InfoBlox</span> <a title="Permalink" class="permalink" href="#sec-designate-infoblox">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-infoblox</li></ul></div></div></div></div><p>
    InfoBlox will act as your public name servers, consult the InfoBlox
    management UI to identify the IPs.
   </p></div><div class="sect4" id="sec-designate-powerdns-bind"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">PowerDNS or BIND Back-end</span> <a title="Permalink" class="permalink" href="#sec-designate-powerdns-bind">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-powerdns-bind</li></ul></div></div></div></div><p>
    You can find the name server IPs in <code class="filename">/etc/hosts</code> by
    looking for the <code class="literal">ext-api</code> addresses, which are the
    addresses of the controllers. For example:
   </p><div class="verbatim-wrap"><pre class="screen">192.168.10.1 example-cp1-c1-m1-extapi
192.168.10.2 example-cp1-c1-m2-extapi
192.168.10.3 example-cp1-c1-m3-extapi</pre></div></div><div class="sect4" id="sec-designate-a-record"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Name Server A Records</span> <a title="Permalink" class="permalink" href="#sec-designate-a-record">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-a-record</li></ul></div></div></div></div><p>
    Each name server requires a public name, for example
    <code class="literal">ns1.example.com.</code>, to which Designate-managed domains will
    be delegated. There are two common locations where these may be registered,
    either within a zone hosted on Designate itself, or within a zone hosted on a
    external DNS service.
   </p><p>
    <span class="bold"><strong>If you are using an externally managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
      For each name server public IP, create the necessary A records in the
      external system.
     </p></li></ul></div></div><p>
    <span class="bold"><strong>If you are using a Designate-managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the zone in Designate which will contain the records:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack zone create --email hostmaster@example.com example.com.
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| action         | CREATE                               |
| created_at     | 2016-03-09T13:16:41.000000           |
| description    | None                                 |
| email          | hostmaster@example.com               |
| id             | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
| masters        |                                      |
| name           | example.com.                         |
| pool_id        | 794ccc2c-d751-44fe-b57f-8894c9f5c842 |
| project_id     | a194d740818942a8bea6f3674e0a3d71     |
| serial         | 1457529400                           |
| status         | PENDING                              |
| transferred_at | None                                 |
| ttl            | 3600                                 |
| type           | PRIMARY                              |
| updated_at     | None                                 |
| version        | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step "><p>
      For each name server public IP, create an A record. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset create --records 192.168.10.1 --type A example.com. ns1.example.com.
+-------------+--------------------------------------+
| Field       | Value                                |
+-------------+--------------------------------------+
| action      | CREATE                               |
| created_at  | 2016-03-09T13:18:36.000000           |
| description | None                                 |
| id          | 09e962ed-6915-441a-a5a1-e8d93c3239b6 |
| name        | ns1.example.com.                     |
| records     | 192.168.10.1                         |
| status      | PENDING                              |
| ttl         | None                                 |
| type        | A                                    |
| updated_at  | None                                 |
| version     | 1                                    |
| zone_id     | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
+-------------+--------------------------------------+</pre></div></li><li class="step "><p>
      When records have been added, list the record sets in the zone to
      validate:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset list example.com.
+--------------+------------------+------+---------------------------------------------------+
| id           | name             | type | records                                           |
+--------------+------------------+------+---------------------------------------------------+
| 2d6cf...655b | example.com.     | SOA  | ns1.example.com. hostmaster.example.com 145...600 |
| 33466...bd9c | example.com.     | NS   | ns1.example.com.                                  |
| da98c...bc2f | example.com.     | NS   | ns2.example.com.                                  |
| 672ee...74dd | example.com.     | NS   | ns3.example.com.                                  |
| 09e96...39b6 | ns1.example.com. | A    | 192.168.10.1                                      |
| bca4f...a752 | ns2.example.com. | A    | 192.168.10.2                                      |
| 0f123...2117 | ns3.example.com. | A    | 192.168.10.3                                      |
+--------------+------------------+------+---------------------------------------------------+</pre></div></li><li class="step "><p>
      Contact your domain registrar requesting <span class="emphasis"><em>Glue
      Records</em></span> to be registered in the
      <code class="literal">com.</code> zone for the nameserver and public
      IP address pairs above. If you are using a sub-zone of an existing
      company zone (for example, <code class="literal">ns1.cloud.mycompany.com.</code>),
      the Glue must be placed in the <code class="literal">mycompany.com.</code> zone.
     </p></li></ol></div></div></div><div class="sect4" id="sec-designate-more"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#sec-designate-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-more</li></ul></div></div></div></div><p>
    For additional DNS integration and configuration information, see the
    <span class="productname">OpenStack</span> Designate documentation at
    <a class="link" href="https://docs.openstack.org/designate/pike/index.html" target="_blank">https://docs.openstack.org/designate/pike/index.html</a>.
   </p><p>
    For more information on creating servers, domains and examples, see the
    <span class="productname">OpenStack</span> REST API documentation at
    <a class="link" href="https://developer.openstack.org/api-ref/dns/" target="_blank">https://developer.openstack.org/api-ref/dns/</a>.
   </p></div></div></div><div class="sect2" id="DesignateMonitoringSupport"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="#DesignateMonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>DesignateMonitoringSupport</li></ul></div></div></div></div><div class="sect3" id="MonitoringSupport"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="#MonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>MonitoringSupport</li></ul></div></div></div></div><p>
   Additional monitoring support for the DNS Service (Designate) has been added
   to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   In the Networking section of the Operations Console, you can see alarms for all of
   the DNS Services (Designate), such as designate-zone-manager, designate-api,
   designate-pool-manager, designate-mdns, and designate-central after running
   <code class="literal">designate-stop.yml</code>.
  </p><p>
   You can run <code class="literal">designate-start.yml</code> to start the DNS Services
   back up and the alarms will change from a red status to green and be removed
   from the <span class="bold"><strong>New Alarms</strong></span> panel of the
   Operations Console.
  </p><p>
   An example of the generated alarms from the Operations Console is provided below
   after running <code class="literal">designate-stop.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">ALARM:  STATE:  ALARM ID:  LAST CHECK:  DIMENSION:
Process Check
0f221056-1b0e-4507-9a28-2e42561fac3e 2016-10-03T10:06:32.106Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-zone-manager,
component=designate-zone-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
50dc4c7b-6fae-416c-9388-6194d2cfc837 2016-10-03T10:04:32.086Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-api,
component=designate-api,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
55cf49cd-1189-4d07-aaf4-09ed08463044 2016-10-03T10:05:32.109Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-pool-manager,
component=designate-pool-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
c4ab7a2e-19d7-4eb2-a9e9-26d3b14465ea 2016-10-03T10:06:32.105Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-mdns,
component=designate-mdns,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm
HTTP Status
c6349bbf-4fd1-461a-9932-434169b86ce5 2016-10-03T10:05:01.731Z service=dns,
cluster=cluster1,
url=http://100.60.90.3:9001/,
hostname=ardana-cp1-c1-m3-mgmt,
component=designate-api,
control_plane=control-plane-1,
api_endpoint=internal,
cloud_name=entry-scale-kvm,
monitored_host_type=instance

Process Check
ec2c32c8-3b91-4656-be70-27ff0c271c89 2016-10-03T10:04:32.082Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-central,
component=designate-central,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm</pre></div></div></div></div><div class="sect1" id="neutron-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Service Overview</span> <a title="Permalink" class="permalink" href="#neutron-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>neutron-overview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Networking is a virtual networking service that leverages the
  OpenStack Neutron service to provide network connectivity and addressing to
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Compute service devices.
 </p><p>
  The Networking service also provides an API to configure and manage a variety
  of network services.
 </p><p>
  You can use the Networking service to connect guest servers or you can define
  and configure your own virtual network topology.
 </p><div class="sect2" id="installing-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Networking service</span> <a title="Permalink" class="permalink" href="#installing-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>installing-the-networking-service</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Network Administrators are responsible for planning for the Neutron
   networking service, and once installed, to configure the service to meet the
   needs of their cloud network users.
  </p></div><div class="sect2" id="working-with-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Working with the Networking service</span> <a title="Permalink" class="permalink" href="#working-with-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>working-with-the-networking-service</li></ul></div></div></div></div><p>
   To perform tasks using the Networking service, you can use the dashboard,
   API or CLI.
  </p></div><div class="sect2" id="restarting"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring the Networking service</span> <a title="Permalink" class="permalink" href="#restarting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>restarting</li></ul></div></div></div></div><p>
   If you change any of the network configuration after installation, it is
   recommended that you reconfigure the Networking service by running the
   neutron-reconfigure playbook.
  </p><p>
   On the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></div><div class="sect2" id="for-more-information"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="#for-more-information">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>for-more-information</li></ul></div></div></div></div><p>
   For information on how to operate your cloud we suggest you read the
   <a class="link" href="http://docs.openstack.org/ops/" target="_blank">OpenStack Operations
   Guide</a>. The <span class="emphasis"><em>Architecture</em></span> section contains useful
   information about how an OpenStack Cloud is put together. However, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   takes care of these details for you. The <span class="emphasis"><em>Operations</em></span>
   section contains information on how to manage the system.
  </p></div><div class="sect2" id="neutron-external-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron External Networks</span> <a title="Permalink" class="permalink" href="#neutron-external-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>neutron-external-networks</li></ul></div></div></div></div><div class="sect3" id="id-1.6.11.5.9.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">External networks overview</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.9.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This topic explains how to create a Neutron external network.
  </p><p>
   External networks provide access to the internet.
  </p><p>
   The typical use is to provide an IP address that can be used to reach a VM
   from an external network which can be a public network like the internet or
   a network that is private to an organization.
  </p></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Ansible Playbook</span> <a title="Permalink" class="permalink" href="#idg-all-networking-neutron-external-networks-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-4</li></ul></div></div></div></div><p>
   This playbook will query the Networking service for an existing external
   network, and then create a new one if you do not already have one. The
   resulting external network will have the name <code class="literal">ext-net</code>
   with a subnet matching the CIDR you specify in the command below.
  </p><p>
   If you need to specify more granularity, for example specifying an
   allocation pool for the subnet, use the
   <a class="xref" href="#idg-all-networking-neutron-external-networks-xml-6" title="9.3.5.3. Using the NeutronClient CLI">Section 9.3.5.3, “Using the NeutronClient CLI”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&lt;CIDR&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e EXT_NET_CIDR=&lt;CIDR&gt;</code>
       </p>
      </td><td>
       <p>
        Optional. You can use this switch to specify the external network CIDR.
        If you choose not to use this switch, or use a wrong value, the VMs
        will not be accessible over the network.
       </p>
       <p>
        This CIDR will be from the <code class="literal">EXTERNAL VM</code> network.
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the NeutronClient CLI</span> <a title="Permalink" class="permalink" href="#idg-all-networking-neutron-external-networks-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-6</li></ul></div></div></div></div><p>
   For more granularity you can utilize the Neutron command line tool to create
   your external network.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the Admin creds:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
     Create the external network and then the subnet using these commands
     below.
    </p><p>
     Creating the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron net-create --router:external &lt;external-network-name&gt;</pre></div><p>
     Creating the subnet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron subnet-create <em class="replaceable ">EXTERNAL-NETWORK-NAME</em> <em class="replaceable ">CIDR</em> --gateway <em class="replaceable ">GATEWAY</em> --allocation-pool start=<em class="replaceable ">IP_START</em>,end=<em class="replaceable ">IP_END</em> [--disable-dhcp]</pre></div><p>
     Where:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>external-network-name</td><td>
         <p>
          This is the name given to your external network. This is a unique
          value that you will choose. The value <code class="literal">ext-net</code> is
          usually used.
         </p>
        </td></tr><tr><td>CIDR</td><td>
         <p>
          You can use this switch to specify the external network CIDR. If you
          choose not to use this switch, or use a wrong value, the VMs will not
          be accessible over the network.
         </p>
         <p>
          This CIDR will be from the EXTERNAL VM network.
         </p>
        </td></tr><tr><td>--gateway</td><td>
         <p>
          Optional switch to specify the gateway IP for your subnet. If this is
          not included then it will choose the first available IP.
         </p>
        </td></tr><tr><td>
         <p>
          --allocation-pool start end
         </p>
        </td><td>
         <p>
          Optional switch to specify a start and end IP address to use as the
          allocation pool for this subnet.
         </p>
        </td></tr><tr><td>--disable-dhcp</td><td>
         <p>
          Optional switch if you want to disable DHCP on this subnet. If this
          is not specified then DHCP will be enabled.
         </p>
        </td></tr></tbody></table></div></li></ol></div></div><div class="sect3" id="MultipleExternalNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple External Networks</span> <a title="Permalink" class="permalink" href="#MultipleExternalNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>MultipleExternalNetworks</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides the ability to have multiple external networks, by using
   the Network Service (Neutron) provider networks for external networks. You
   can configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to allow the use of provider VLANs as external
   networks by following these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Do NOT include the
     <code class="literal">neutron.l3_agent.external_network_bridge</code> tag in the
     network_groups definition for your cloud. This results in the
     <code class="literal">l3_agent.ini external_network_bridge</code> being set to an
     empty value (rather than the traditional br-ex).
    </p></li><li class="listitem "><p>
     Configure your cloud to use provider VLANs, by specifying the
     <code class="literal">provider_physical_network</code> tag on one of the
     network_groups defined for your cloud.
    </p><p>
     For example, to run provider VLANS over the EXAMPLE network group: (some
     attributes omitted for brevity)
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:

  - name: EXAMPLE
    tags:
      - neutron.networks.vlan:
          provider-physical-network: physnet1</pre></div></li><li class="listitem "><p>
     After the cloud has been deployed, you can create external networks using
     provider VLANs.
    </p><p>
     For example, using the Network Service CLI:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create external network 1 on vlan101
      </p><div class="verbatim-wrap"><pre class="screen">neutron net-create --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 101 ext-net1 --router:external true</pre></div></li><li class="listitem "><p>
       Create external network 2 on vlan102
      </p><div class="verbatim-wrap"><pre class="screen">neutron net-create --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 102 ext-net2 --router:external true</pre></div></li></ol></div></li></ol></div></div></div><div class="sect2" id="neutron-provider-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron Provider Networks</span> <a title="Permalink" class="permalink" href="#neutron-provider-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>neutron-provider-networks</li></ul></div></div></div></div><p>
  This topic explains how to create a Neutron provider network.
 </p><p>
  A provider network is a virtual network created in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that
  is consumed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services. The distinctive element of a provider
  network is that it does not create a virtual router; rather, it depends on
  L3 routing that is provided by the infrastructure.
 </p><p>
  A provider network is created by adding the specification to the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  input model. It consists of at least one network and one or more subnets.
 </p><div class="sect3" id="id-1.6.11.5.10.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input model</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The input model is the primary mechanism a cloud admin uses in defining a
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation. It exists as a directory with a data subdirectory that
   contains YAML files. By convention, any service that creates a Neutron
   provider network will create a subdirectory under the data directory and the
   name of the subdirectory shall be the project name. For example, the Octavia
   project will use Neutron provider networks so it will have a subdirectory
   named 'octavia' and the config file that specifies the neutron network will
   exist in that subdirectory.
  </p><div class="verbatim-wrap"><pre class="screen">├── cloudConfig.yml
    ├── data
    │   ├── control_plane.yml
    │   ├── disks_compute.yml
    │   ├── disks_controller_1TB.yml
    │   ├── disks_controller.yml
    │   ├── firewall_rules.yml
    │   ├── net_interfaces.yml
    │   ├── network_groups.yml
    │   ├── networks.yml
    │   ├── neutron
    │   │   └── neutron_config.yml
    │   ├── nic_mappings.yml
    │   ├── server_groups.yml
    │   ├── server_roles.yml
    │   ├── servers.yml
    │   ├── swift
    │   │   └── rings.yml
    │   └── octavia
    │       └── octavia_config.yml
    ├── README.html
    └── README.md</pre></div></div><div class="sect3" id="id-1.6.11.5.10.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network/Subnet specification</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The elements required in the input model for you to define a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     name
    </p></li><li class="listitem "><p>
     network_type
    </p></li><li class="listitem "><p>
     physical_network
    </p></li></ul></div><p>
   Elements that are optional when defining a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     segmentation_id
    </p></li><li class="listitem "><p>
     shared
    </p></li></ul></div><p>
   Required elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     cidr
    </p></li></ul></div><p>
   Optional elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     allocation_pools which will require start and end addresses
    </p></li><li class="listitem "><p>
     host_routes which will require a destination and nexthop
    </p></li><li class="listitem "><p>
     gateway_ip
    </p></li><li class="listitem "><p>
     no_gateway
    </p></li><li class="listitem "><p>
     enable-dhcp
    </p></li></ul></div><p>
   NOTE: Only IPv4 is supported at the present time.
  </p></div><div class="sect3" id="id-1.6.11.5.10.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network details</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the network values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Attribute</th><th>Required/optional</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>name</td><td>Required</td><td> </td><td> </td></tr><tr><td>network_type</td><td>Required</td><td>flat, vlan, vxlan</td><td>The type of desired network</td></tr><tr><td>physical_network</td><td>Required</td><td>Valid</td><td>Name of physical network that is overlayed with the virtual network</td></tr><tr><td>segmentation_id</td><td>Optional</td><td>vlan or vxlan ranges</td><td>VLAN id for vlan or tunnel id for vxlan</td></tr><tr><td>shared</td><td>Optional</td><td>True</td><td>Shared by all projects or private to a single project</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.10.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Subnet details</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the subnet values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>cidr</td><td>Required</td><td>Valid CIDR range</td><td>for example, 172.30.0.0/24</td></tr><tr><td>allocation_pools</td><td>Optional</td><td>See allocation_pools table below</td><td> </td></tr><tr><td>host_routes</td><td>Optional</td><td>See host_routes table below</td><td> </td></tr><tr><td>gateway_ip</td><td>Optional</td><td>Valid IP addr</td><td>Subnet gateway to other nets</td></tr><tr><td>no_gateway</td><td>Optional</td><td>True</td><td>No distribution of gateway</td></tr><tr><td>enable-dhcp</td><td>Optional</td><td>True</td><td>Enable dhcp for this subnet</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.10.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ALLOCATION_POOLS details</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains allocation pool settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>start</td><td>Required</td><td>Valid IP addr</td><td>First ip address in pool</td></tr><tr><td>end</td><td>Required</td><td>Valid IP addr</td><td>Last ip address in pool</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.10.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HOST_ROUTES details</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains host route settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>destination</td><td>Required</td><td>Valid CIDR</td><td>Destination subnet</td></tr><tr><td>nexthop</td><td>Required</td><td>Valid IP addr</td><td>Hop to take to destination subnet</td></tr></tbody></table></div><div id="id-1.6.11.5.10.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Multiple destination/nexthop values can be used.
   </p></div></div><div class="sect3" id="id-1.6.11.5.10.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examples</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following examples show the configuration file settings for Neutron and
   Octavia.
  </p><p>
   <span class="bold"><strong>Octavia configuration</strong></span>
  </p><p>
   This file defines the mapping. It does not need to be edited unless you want
   to change the name of your VLAN.
  </p><p>
   Path:
   <code class="literal">~/openstack/my_cloud/definition/data/octavia/octavia_config.yml</code>
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name: OCTAVIA-CONFIG-CP1
      services:
        - octavia
      data:
        amp_network_name: OCTAVIA-MGMT-NET</pre></div><p>
   <span class="bold"><strong>Neutron configuration</strong></span>
  </p><p>
   Input your network configuration information for your provider VLANs in
   <code class="literal">neutron_config.yml</code> found here:
  </p><p>
   <code class="literal">~/openstack/my_cloud/definition/data/neutron/</code>.
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  NEUTRON-CONFIG-CP1
      services:
        - neutron
      data:
        neutron_provider_networks:
        - name: OCTAVIA-MGMT-NET
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 2754
          cidr: 10.13.189.0/24
          no_gateway:  True
          enable_dhcp: True
          allocation_pools:
            - start: 10.13.189.4
              end: 10.13.189.252
          host_routes:
            # route to MANAGEMENT-NET
            - destination: 10.13.111.128/26
              nexthop:  10.13.189.5</pre></div></div><div class="sect3" id="id-1.6.11.5.10.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Implementing your changes</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.10.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "configuring provider network"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Then continue with your clean cloud installation.
    </p></li><li class="listitem "><p>
     If you are only adding a Neutron Provider network to an existing model,
     then run the neutron-deploy.yml playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-deploy.yml</pre></div></li></ol></div></div><div class="sect3" id="MultipleProviderNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Provider Networks</span> <a title="Permalink" class="permalink" href="#MultipleProviderNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>MultipleProviderNetworks</li></ul></div></div></div></div><p>
   The physical network infrastructure must be configured to convey the
   provider VLAN traffic as tagged VLANs to the cloud compute nodes and network
   service network nodes. Configuration of the physical network infrastructure
   is outside the scope of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> software.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> automates the server networking configuration and the
   Network Service configuration based on information in the cloud definition.
   To configure the system for provider VLANs, specify the<span class="emphasis"><em>
   neutron.networks.vlan</em></span> tag with a
   <span class="emphasis"><em>provider-physical-network</em></span> attribute on one or more
   network groups. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:

        - name: NET_GROUP_A
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet1

        - name: NET_GROUP_B
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet2</pre></div><p>
   A network group is associated with a server network interface via an
   interface model. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">interface-models:
        - name: INTERFACE_SET_X
        network-interfaces:
        - device:
        name: bond0
        network-groups:
        - NET_GROUP_A
        - device:
        name: eth3
        network-groups:
        - NET_GROUP_B</pre></div><p>
   A network group used for provider VLANs may contain only a single <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   network, because that VLAN must span all compute nodes and any Network
   Service network nodes/controllers (that is, it is a single L2 segment). The
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> network must be defined with tagged-vlan false, otherwise a Linux
   VLAN network interface will be created. For example:
  </p><div class="verbatim-wrap"><pre class="screen">networks:

        - name: NET_A
        tagged-vlan: false
        network-group: NET_GROUP_A

        - name: NET_B
        tagged-vlan: false
        network-group: NET_GROUP_B</pre></div><p>
   When the cloud is deployed, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> will create the appropriate
   bridges on the servers, and set the appropriate attributes in the Neutron
   configuration files (for example, bridge_mappings).
  </p><p>
   After the cloud has been deployed, create Network Service network objects
   for each provider VLAN. For example, using the Network Service CLI:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron net-create --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 101 mynet101
<code class="prompt user">ardana &gt; </code>neutron net-create --provider:network_type vlan --provider:physical_network physnet2 --provider:segmentation_id 234 mynet234</pre></div></div><div class="sect3" id="idg-all-networking-neutron-provider-networks-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#idg-all-networking-neutron-provider-networks-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-provider-networks-xml-10</li></ul></div></div></div></div><p>
   For more information on the Network Service command-line interface (CLI),
   see the OpenStack networking command-line client reference:
   <a class="link" href="http://docs.openstack.org/cli-reference/content/neutronclient_commands.html" target="_blank">http://docs.openstack.org/cli-reference/content/neutronclient_commands.html</a>
  </p></div></div><div class="sect2" id="ipam"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using IPAM Drivers in the Networking Service</span> <a title="Permalink" class="permalink" href="#ipam">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span>ipam</li></ul></div></div></div></div><p>
  This topic describes how to choose and implement an IPAM driver.
 </p><div class="sect3" id="id-1.6.11.5.11.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting and implementing an IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.11.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Beginning with the Liberty release, OpenStack networking includes a
   pluggable interface for the IP Address Management (IPAM) function. This
   interface creates a driver framework for the allocation and de-allocation of
   subnets and IP addresses, enabling the integration of alternate IPAM
   implementations or third-party IP Address Management systems.
  </p><p>
   There are three possible IPAM driver options:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Non-pluggable driver. This option is the default when the ipam_driver
     parameter is not specified in neutron.conf.
    </p></li><li class="listitem "><p>
     Pluggable reference IPAM driver. The pluggable IPAM driver interface was
     introduced in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> (OpenStack Liberty). It is a refactoring of
     the Kilo non-pluggable driver to use the new pluggable interface. The
     setting in neutron.conf to specify this driver is <code class="literal">ipam_driver =
     internal</code>.
    </p></li><li class="listitem "><p>
     Pluggable Infoblox IPAM driver. The pluggable Infoblox IPAM driver is a
     third-party implementation of the pluggable IPAM interface. the
     corresponding setting in neutron.conf to specify this driver is
     <code class="literal">ipam_driver =
     networking_infoblox.ipam.driver.InfobloxPool</code>.
    </p><div id="id-1.6.11.5.11.3.4.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can use either the non-pluggable IPAM driver or a pluggable one.
      However, you cannot use both.
     </p></div></li></ul></div></div><div class="sect3" id="id-1.6.11.5.11.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Pluggable reference IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To indicate that you want to use the Pluggable reference IPAM driver, the
   only parameter needed is "ipam_driver." You can set it by looking for the
   following commented line in the
   <code class="literal">neutron.conf.j2</code> template (ipam_driver = internal)
   uncommenting it, and committing the file. After following the standard
   steps to deploy Neutron, Neutron will be configured to run using the
   Pluggable reference IPAM driver.
  </p><p>
   As stated, the file you must edit is <code class="literal">neutron.conf.j2</code> on
   the Cloud Lifecycle Manager in the directory
   <code class="literal">~/openstack/my_cloud/config/neutron</code>. Here is the relevant
   section where you can see the <code class="literal">ipam_driver</code> parameter
   commented out:
  </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
  ...
  l3_ha_net_cidr = 169.254.192.0/18

  # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
  # ipam_driver = internal
  ...</pre></div><p>
   After uncommenting the line <code class="literal">ipam_driver = internal</code>,
   commit the file using git commit from the <code class="literal">openstack/my_cloud</code>
   directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m 'My config for enabling the internal IPAM Driver'</pre></div><p>
   Then follow the steps to deploy <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in the
   <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Preface “Installation Overview”</span> appropriate to your cloud configuration.
  </p><div id="id-1.6.11.5.11.4.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Currently there is no migration path from the non-pluggable driver to a
    pluggable IPAM driver because changes are needed to database tables and
    Neutron currently cannot make those changes.
   </p></div></div><div class="sect3" id="id-1.6.11.5.11.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As suggested above, using the Infoblox IPAM driver requires changes to
   existing parameters in <code class="literal">nova.conf</code> and
   <code class="literal">neutron.conf</code>. If you want to use the infoblox appliance,
   you will need to add the "infoblox service-component" to the service-role
   containing the neutron API server. To use the infoblox appliance for IPAM,
   both the agent <span class="emphasis"><em>and</em></span> the Infoblox IPAM driver are
   required. The <code class="literal">infoblox-ipam-agent</code> should be deployed on
   the same node where the neutron-server component is running. Usually this is
   a Controller node.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Have the Infoblox appliance running on the management network (the
     Infoblox appliance admin or the datacenter administrator should know how
     to perform this step).
    </p></li><li class="listitem "><p>
     Change the control plane definition to add
     i<code class="literal">nfoblox-ipam-agent</code> as a service in the controller node
     cluster (see change in bold). Make the changes in
     <code class="literal">control_plane.yml</code> found here:
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  control-planes:
    - name: ccp
      control-plane-prefix: ccp
 ...
      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: ARDANA-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - lifecycle-manager
        - name: cluster1
          cluster-prefix: c1
          server-role: CONTROLLER-ROLE
          member-count: 3
          allocation-policy: strict
          service-components:
            - ntp-server
...
            - neutron-server
            <span class="bold"><strong>- infoblox-ipam-agent</strong></span>
...
            - designate-client
            - powerdns
      resources:
        - name: compute
          resource-prefix: comp
          server-role: COMPUTE-ROLE
          allocation-policy: any</pre></div></li><li class="listitem "><p>
     Modify the
     <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> file
     on the controller node to comment and uncomment the lines noted below to
     enable use with the Infoblox appliance:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
            ...
            l3_ha_net_cidr = 169.254.192.0/18


            # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
            # ipam_driver = internal


            # Comment out the line below if the Infoblox IPAM Driver is to be used
            # notification_driver = messaging

            # Uncomment the lines below if the Infoblox IPAM driver is to be used
            ipam_driver = networking_infoblox.ipam.driver.InfobloxPool
            notification_driver = messagingv2


            # Modify the infoblox sections below to suit your cloud environment

            [infoblox]
            cloud_data_center_id = 1
            # This name of this section is formed by "infoblox-dc:&lt;infoblox.cloud_data_center_id&gt;"
            # If cloud_data_center_id is 1, then the section name is "infoblox-dc:1"

            [infoblox-dc:0]
            http_request_timeout = 120
            http_pool_maxsize = 100
            http_pool_connections = 100
            ssl_verify = False
            wapi_version = 2.2
            admin_user_name = admin
            admin_password = infoblox
            grid_master_name = infoblox.localdomain
            grid_master_host = 1.2.3.4


            [QUOTAS]
            ...</pre></div></li><li class="listitem "><p>
     Change <code class="literal">nova.conf.j2</code> to replace the notification driver
     "messaging" to "messagingv2"
    </p><div class="verbatim-wrap"><pre class="screen"> ...

 # Oslo messaging
 notification_driver = log

 #  Note:
 #  If the infoblox-ipam-agent is to be deployed in the cloud, change the
 #  notification_driver setting from "messaging" to "messagingv2".
 notification_driver = messagingv2
 notification_topics = notifications

 # Policy
 ...</pre></div></li><li class="listitem "><p>
     Commit the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud
<code class="prompt user">ardana &gt; </code>git commit –a –m 'My config for enabling the Infoblox IPAM driver'</pre></div></li><li class="listitem "><p>
     Deploy the cloud with the changes. Due to changes to the
     control_plane.yml, you will need to rerun the config-processor-run.yml
     playbook if you have run it already during the install process.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.11.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration parameters for using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Changes required in the notification parameters in nova.conf:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in nova.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>notify_on_state_change</td><td>DEFAULT</td><td>None</td><td>vm_and_task_state</td><td>
       <p>
        Send compute.instance.update notifications on instance state changes.
       </p>
       <p>
        Vm_and_task_state means notify on vm and task state changes.
       </p>
       <p>
        Infoblox requires the value to be vm_state (notify on vm state change).
       </p>
       <p>
        <span class="bold"><strong> Thus NO CHANGE is needed for infoblox</strong></span>
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>empty list</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>NO CHANGE is needed for infoblox.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notifications to be
        "notifications"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>None</td><td>messaging</td><td>
       <p>
        <span class="bold"><strong>Change needed.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notification driver to be
        "messagingv2".
       </p>
      </td></tr></tbody></table></div><p>
   Changes to existing parameters in neutron.conf
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>ipam_driver</td><td>DEFAULT</td><td>None</td><td>
       <p>
        None
       </p>
       <p>
        (param is undeclared in neutron.conf)
       </p>
      </td><td>
       <p>
        Pluggable IPAM driver to be used by Neutron API server.
       </p>
       <p>
        For infoblox, the value is
        "networking_infoblox.ipam.driver.InfobloxPool"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>empty list</td><td>messaging</td><td>
       <p>
        The driver used to send notifications from the Neutron API server to
        the Neutron agents.
       </p>
       <p>
        The installation guide for networking-infoblox calls for the
        notification_driver to be "messagingv2"
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>None</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>No change needed</strong></span>.
       </p>
       <p>
        The row is here show the changes in the Neutron parameters described in
        the installation guide for networking-infoblox
       </p>
      </td></tr></tbody></table></div><p>
   Parameters specific to the Networking Infoblox Driver. All the parameters
   for the Infoblox IPAM driver must be defined in neutron.conf.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>cloud_data_center_id</td><td>infoblox</td><td>0</td><td>ID for selecting a particular grid from one or more grids to serve networks in
                the Infoblox back end</td></tr><tr><td>ipam_agent_workers</td><td>infoblox</td><td>1</td><td>Number of Infoblox IPAM agent works to run</td></tr><tr><td>grid_master_host</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>IP address of the grid master. WAPI requests are sent to the
                grid_master_host</td></tr><tr><td>ssl_verify</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>False</td><td>Ensure whether WAPI requests sent over HTTPS require SSL verification</td></tr><tr><td>WAPI Version</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>1.4</td><td>The WAPI version. Value should be 2.2.</td></tr><tr><td>admin_user_name</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user name to access the grid master or cloud platform appliance</td></tr><tr><td>admin_password</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user password</td></tr><tr><td>http_pool_connections</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_pool_maxsize</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_request_timeout</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>120</td><td> </td></tr></tbody></table></div><p>
  The diagram below shows Nova compute sending notification to the
  infoblox-ipam-agent
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networking-ipam.png" target="_blank"><img src="images/media-networking-ipam.png" width="" /></a></div></div></div><div class="sect3" id="id-1.6.11.5.11.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     There is no IPAM migration path from non-pluggable to pluggable IPAM
     driver
     (<a class="link" href="https://bugs.launchpad.net/neutron/+bug/1516156" target="_blank">https://bugs.launchpad.net/neutron/+bug/1516156</a>).
     This means there is no way to reconfigure the Neutron database if you
     wanted to change Neutron to use a pluggable IPAM driver. Unless you change
     the default of non-pluggable IPAM configuration to a pluggable driver at
     install time, you will have no other opportunity to make that change
     because reconfiguration of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>from using the default
     non-pluggable IPAM configuration to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> using a pluggable IPAM
     driver is not supported.
    </p></li><li class="listitem "><p>
     Upgrade from previous versions of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> to use a
     pluggable IPAM driver is not supported.
    </p></li><li class="listitem "><p>
     The Infoblox appliance does not allow for overlapping IPs. For example,
     only one tenant can have a CIDR of 10.0.0.0/24.
    </p></li><li class="listitem "><p>
     The infoblox IPAM driver fails the creation of a subnet when a there is no
     gateway-ip supplied. For example, the command "neutron subnet-create ...
     --no-gateway ..." will fail.
    </p></li></ul></div></div></div><div class="sect2" id="HP2-0LBaaSAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Load Balancing as a Service (LBaaS)</span> <a title="Permalink" class="permalink" href="#HP2-0LBaaSAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>HP2-0LBaaSAdmin</li></ul></div></div></div></div><p>
  <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> LBaaS Configuration</strong></span>
 </p><p>
  Load Balancing as a Service (LBaaS) is an advanced networking service that
  allows load balancing of multi-node environments. It provides the ability to
  spread requests across multiple servers thereby reducing the load on any
  single server. This document describes the installation steps for LBaaS v1
  (see prerequisites) and the configuration for LBaaS v1 and v2.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> can support either LBaaS v1 or LBaaS v2 to allow for wide
  ranging customer requirements. If the decision is made to utilize LBaaS v1 it
  is highly unlikely that you will be able to perform an on-line upgrade of the
  service to v2 after the fact as the internal data structures are
  significantly different. Should you wish to attempt an upgrade, support will
  be needed from <span class="phrase"><span class="phrase">Sales Engineering</span></span> and your chosen load balancer partner.
 </p><div id="id-1.6.11.5.12.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   The LBaaS architecture is based on a driver model to support different load
   balancers. LBaaS-compatible drivers are provided by load balancer vendors
   including F5 and Citrix. A new software load balancer driver was introduced
   in the OpenStack Liberty release called "<span class="emphasis"><em>Octavia</em></span>". The
   Octavia driver deploys a software load balancer called HAProxy. Octavia is
   the default load balancing provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> for LBaaS V2.
   
   Until Octavia is configured the creation of load balancers will fail with an
   error. Please refer to <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span> document for
   information on installing Octavia.
  </p></div><div id="id-1.6.11.5.12.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   Before upgrading to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, contact F5 and
   <span class="phrase"><span class="phrase">SUSE</span></span> to determine which F5 drivers have been certified for use with
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Loading drivers not certified by <span class="phrase"><span class="phrase">SUSE</span></span> may result in
   failure of your cloud deployment.
  </p></div><p>
  LBaaS V2 offers with <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span> a software load
  balancing solution that supports both a highly available control plane and
  data plane. However, should an external hardware load balancer be selected
  the cloud operation can achieve additional performance and availability.
 </p><p>
  <span class="bold"><strong>LBaaS v1</strong></span>
 </p><p>
  Reasons to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    You must be able to configure LBaaS via Horizon.
   </p></li><li class="listitem "><p>
    Your hardware load balancer vendor does not currently support LBaaS v2.
   </p></li></ol></div><p>
  Reasons not to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    No active development is being performed on this API in the OpenStack
    community. (Security fixes are still being worked upon).
   </p></li><li class="listitem "><p>
    It does not allow for multiple ports on the same VIP (for example, to
    support both port 80 and 443 on a single VIP).
   </p></li><li class="listitem "><p>
    It will never be able to support TLS termination/re-encryption at the load
    balancer.
   </p></li><li class="listitem "><p>
    It will never be able to support L7 rules for load balancing.
   </p></li><li class="listitem "><p>
    LBaaS v1 will likely become officially deprecated by the OpenStack
    community at the Tokyo (October 2015) summit.
   </p></li></ol></div><p>
  <span class="bold"><strong>LBaaS v2</strong></span>
 </p><p>
  Reasons to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your vendor already has a driver that supports LBaaS v2. Many hardware load
    balancer vendors already support LBaaS v2 and this list is growing all the
    time.
   </p></li><li class="listitem "><p>
    You intend to script your load balancer creation and management so a UI is
    not important right now (Horizon support will be added in a future
    release).
   </p></li><li class="listitem "><p>
    You intend to support TLS termination at the load balancer.
   </p></li><li class="listitem "><p>
    You intend to use the Octavia software load balancer (adding HA and
    scalability).
   </p></li><li class="listitem "><p>
    You do not want to take your load balancers offline to perform
    subsequent LBaaS upgrades.
   </p></li><li class="listitem "><p>
    You intend in future releases to need L7 load balancing.
   </p></li></ol></div><p>
  Reasons not to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your LBaaS vendor does not have a v2 driver.
   </p></li><li class="listitem "><p>
    You must be able to manage your load balancers from Horizon.
   </p></li><li class="listitem "><p>
    You have legacy software which utilizes the LBaaS v1 API.
   </p></li></ol></div><p>
  LBaaS v1 requires configuration changes prior to installation and is not
  recommended. LBaaS v2 is installed by default with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and requires
  minimal configuration to start the service.
 </p><div id="id-1.6.11.5.12.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   Only LBaaS V2 API currently supports load balancer failover with Octavia.
   However, in LBaaS V1 and if Octavia is not deployed when a load balancer is
   deleted it will need to be manually recreated. LBaaS v2 API includes
   automatic failover of a deployed load balancer with Octavia. More
   information about this driver can be found in
   <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span>.
  </p></div><div class="sect3" id="idg-all-networking-lbaas-admin-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-networking-lbaas-admin-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-lbaas-admin-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> LBaaS v1</strong></span>
  </p><p>
   Installing LBaaS v1
  </p><div id="id-1.6.11.5.12.20.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    It is not recommended that LBaaS v1 is used in a production environment. It
    is recommended you use LBaaS v2. If you do deploy LBaaS v1, the upgrade to
    LBaaS v2 is non-trivial and may require the use of professional services.
   </p></div><div id="id-1.6.11.5.12.20.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you need to run LBaaS v1 instead of the default LBaaS v2, you should
    make appropriate installation preparations during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
    since LBaaS v2 is the default. If you have selected to install and use
    LBaaS v1 you will replace the <code class="filename">control_plane.yml</code>
    directories and <code class="filename">neutron.conf.j2</code> file to use version 1.
   </p></div><p>
   Before you modify the control_plane.yml file, it is recommended that you
   back up the original version of this file. Once you have backed them up,
   modify the control_plane.yml file.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit ~/openstack/my_cloud/definition/data/control_plane.yml - depending on
     your installation the control_plane.yml file might be in a different
     location.
    </p></li><li class="listitem "><p>
     In the section specifying the compute nodes (resources/compute) replace
     neutron-lbaasv2-agent with neutron-lbaas-agent - there will only be one
     occurrence in that file.
    </p></li><li class="listitem "><p>
     Save the modified file.
    </p></li><li class="listitem "><p>
     Follow the steps in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span> to commit and apply the
     changes.
    </p></li><li class="listitem "><p>
     To test the installation follow the steps outlined in
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span> after you have created a suitable subnet,
     see: <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 27 “UI Verification”, Section 27.4 “Creating an External Network”</span>.
    </p></li></ol></div><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> LBaaS v2</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must be installed for LBaaS v2.
    </p></li><li class="listitem "><p>
     Follow the instructions to install <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span>
    </p></li></ol></div></div></div><div class="sect2" id="OctaviaAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer: Octavia Driver Administration</span> <a title="Permalink" class="permalink" href="#OctaviaAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>OctaviaAdmin</li></ul></div></div></div></div><p>
  This document provides the instructions on how to enable and manage various
  components of the Load Balancer Octavia driver if that driver is enabled.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#Alerts" title="9.3.9.1. Monasca Alerts">Section 9.3.9.1, “Monasca Alerts”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#Tuning" title="9.3.9.2. Tuning Octavia Installation">Section 9.3.9.2, “Tuning Octavia Installation”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Homogeneous Compute Configuration
     </p></li><li class="listitem "><p>
      Octavia and Floating IP's
     </p></li><li class="listitem "><p>
      Configuration Files
     </p></li><li class="listitem "><p>
      Spare Pools
     </p></li></ul></div></li><li class="listitem "><p>
    <a class="xref" href="#Amphora" title="9.3.9.3. Managing Amphora">Section 9.3.9.3, “Managing Amphora”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Updating the Cryptographic Certificates
     </p></li><li class="listitem "><p>
      Accessing VM information in Nova
     </p></li><li class="listitem "><p>
      Initiating Failover of an Amphora VM
     </p></li></ul></div></li></ul></div><div class="sect3" id="Alerts"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Alerts</span> <a title="Permalink" class="permalink" href="#Alerts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Alerts</li></ul></div></div></div></div><p>
   The Monasca-agent has the following Octavia-related plugins:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Process checks – checks if octavia processes are running. When it
     starts, it detects which processes are running and then monitors them.
    </p></li><li class="listitem "><p>
     http_connect check – checks if it can connect to octavia api servers.
    </p></li></ul></div><p>
   Alerts are displayed in the Operations Console. For more information see
   <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span>.
  </p></div><div class="sect3" id="Tuning"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning Octavia Installation</span> <a title="Permalink" class="permalink" href="#Tuning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Tuning</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Homogeneous Compute Configuration</strong></span>
  </p><p>
   Octavia works only with homogeneous compute node configurations. Currently,
   Octavia does not support multiple nova flavors. If Octavia needs to be
   supported on multiple compute nodes, then all the compute nodes should carry
   same set of physnets (which will be used for Octavia).
  </p><p>
   <span class="bold"><strong>Octavia and Floating IPs</strong></span>
  </p><p>
   Due to a Neutron limitation Octavia will only work with CVR routers. Another
   option is to use VLAN provider networks which do not require a router.
  </p><p>
   You cannot currently assign a floating IP address as the VIP (user facing)
   address for a load balancer created by the Octavia driver if the underlying
   Neutron network is configured to support Distributed Virtual Router (DVR).
   The Octavia driver uses a Neutron function known as
   <span class="emphasis"><em>allowed address pairs</em></span>
   to support load balancer fail over.
  </p><p>
   There is currently a Neutron bug that does not support this function in a
   DVR configuration
  </p><p>
   <span class="bold"><strong>Octavia Configuration Files</strong></span>
  </p><p>
   The system comes pre-tuned and should not need any adjustments for most
   customers. If in rare instances manual tuning is needed, follow these steps:
  </p><div id="id-1.6.11.5.13.5.10" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Changes might be lost during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> upgrades.
   </p></div><p>
   Edit the Octavia configuration files in
   <code class="literal">my_cloud/config/octavia</code>. It is recommended that any
   changes be made in all of the Octavia configuration files.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     octavia-api.conf.j2
    </p></li><li class="listitem "><p>
     octavia-health-manager.conf.j2
    </p></li><li class="listitem "><p>
     octavia-housekeeping.conf.j2
    </p></li><li class="listitem "><p>
     octavia-worker.conf.j2
    </p></li></ul></div><p>
   After the changes are made to the configuration files, redeploy the service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit changes to git.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My Octavia Config"</pre></div></li><li class="listitem "><p>
     Run the configuration processor and ready deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Octavia reconfigure.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>Spare Pools</strong></span>
  </p><p>
   The Octavia driver provides support for creating spare pools of the HAProxy
   software installed in VMs. This means instead of creating a new load
   balancer when loads increase, create new load balancer calls will pull a
   load balancer from the spare pool. The spare pools feature consumes
   resources, therefore the load balancers in the spares pool has been set to
   0, which is the default and also disables the feature.
  </p><p>
   Reasons to enable a load balancing spare pool in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     You expect a large number of load balancers to be provisioned all at once
     (puppet scripts, or ansible scripts) and you want them to come up quickly.
    </p></li><li class="listitem "><p>
     You want to reduce the wait time a customer has while requesting a new
     load balancer.
    </p></li></ol></div><p>
   To increase the number of load balancers in your spares pool, edit the
   Octavia configuration files by uncommenting the
   <code class="literal">spare_amphora_pool_size</code> and adding the number of load
   balancers you would like to include in your spares pool.
  </p><div class="verbatim-wrap"><pre class="screen"># Pool size for the spare pool
# spare_amphora_pool_size = 0</pre></div><div id="id-1.6.11.5.13.5.21" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> the spare pool cannot be used to speed up fail
    overs. If a load balancer fails in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, Octavia will always provision
    a new VM to replace that failed load balancer.
   </p></div></div><div class="sect3" id="Amphora"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Amphora</span> <a title="Permalink" class="permalink" href="#Amphora">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Amphora</li></ul></div></div></div></div><p>
   Octavia starts a separate VM for each load balancing function. These VMs are
   called amphora.
  </p><p>
   <span class="bold"><strong>Updating the Cryptographic Certificates</strong></span>
  </p><p>
   Octavia uses two-way SSL encryption for communication between amphora and
   the control plane. Octavia keeps track of the certificates on the amphora
   and will automatically recycle them. The certificates on the control plane
   are valid for one year after installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   You can check on the status of the certificate by logging into the
   controller node as root and running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /opt/stack/service/octavia-<em class="replaceable ">SOME UUID</em>/etc/certs/
openssl x509 -in client.pem  -text –noout</pre></div><p>
   This prints the certificate out where you can check on the expiration dates.
  </p><p>
   To renew the certificates, reconfigure Octavia. Reconfiguring causes Octavia
   to automatically generate new certificates and deploy them to the controller
   hosts.
  </p><p>
   On the Cloud Lifecycle Manager execute octavia-reconfigure:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div><p>
   <span class="bold"><strong>Accessing VM information in Nova</strong></span>
  </p><p>
   You can use <code class="literal">openstack project list</code> as an administrative
   user to obtain information about the tenant or project-id of the Octavia
   project. In the example below, the Octavia project has a project-id of
   <code class="literal">37fd6e4feac14741b6e75aba14aea833</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack project list
+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 055071d8f25d450ea0b981ca67f7ccee | glance-swift     |
| 37fd6e4feac14741b6e75aba14aea833 | octavia          |
| 4b431ae087ef4bd285bc887da6405b12 | swift-monitor    |
| 8ecf2bb5754646ae97989ba6cba08607 | swift-dispersion |
| b6bd581f8d9a48e18c86008301d40b26 | services         |
| bfcada17189e4bc7b22a9072d663b52d | cinderinternal   |
| c410223059354dd19964063ef7d63eca | monitor          |
| d43bc229f513494189422d88709b7b73 | admin            |
| d5a80541ba324c54aeae58ac3de95f77 | demo             |
| ea6e039d973e4a58bbe42ee08eaf6a7a | backup           |
+----------------------------------+------------------+</pre></div><p>
   You can then use <code class="literal">nova list --tenant &lt;project-id&gt;</code> to
   list the VMs for the Octavia tenant. Take particular note of the IP address
   on the OCTAVIA-MGMT-NET; in the example below it is
   <code class="literal">172.30.1.11</code>. For additional nova command-line options see
   <a class="xref" href="#idg-all-networking-octavia-admin-xml-10" title="9.3.9.5. For More Information">Section 9.3.9.5, “For More Information”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.11 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div><div id="id-1.6.11.5.13.6.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    The Amphora VMs do not have SSH or any other access. In the rare case that
    there is a problem with the underlying load balancer the whole amphora will
    need to be replaced.
   </p></div><p>
   <span class="bold"><strong>Initiating Failover of an Amphora VM</strong></span>
  </p><p>
   Under normal operations Octavia will monitor the health of the amphora
   constantly and automatically fail them over if there are any issues. This
   helps to minimize any potential downtime for load balancer users. There are,
   however, a few cases a failover needs to be initiated manually:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The Loadbalancer has become unresponsive and Octavia has not detected an
     error.
    </p></li><li class="listitem "><p>
     A new image has become available and existing load balancers need to start
     using the new image.
    </p></li><li class="listitem "><p>
     The cryptographic certificates to control and/or the HMAC password to
     verify Health information of the amphora have been compromised.
    </p></li></ol></div><p>
   To minimize the impact for end users we will keep the existing load balancer
   working until shortly before the new one has been provisioned. There will be
   a short interruption for the load balancing service so keep that in mind
   when scheduling the failovers. To achieve that follow these steps (assuming
   the management ip from the previous step):
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Assign the IP to a SHELL variable for better readability.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export MGM_IP=172.30.1.11</pre></div></li><li class="listitem "><p>
     Identify the port of the vm on the management network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-list | grep $MGM_IP
| 0b0301b9-4ee8-4fb6-a47c-2690594173f4 |                                                   | fa:16:3e:d7:50:92 |
{"subnet_id": "3e0de487-e255-4fc3-84b8-60e08564c5b7", "ip_address": "172.30.1.11"} |</pre></div></li><li class="listitem "><p>
     Disable the port to initiate a failover. Note the load balancer will still
     function but cannot be controlled any longer by Octavia.
    </p><div id="id-1.6.11.5.13.6.21.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Changes after disabling the port will result in errors.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-update --admin-state-up False 0b0301b9-4ee8-4fb6-a47c-2690594173f4
Updated port: 0b0301b9-4ee8-4fb6-a47c-2690594173f4</pre></div></li><li class="listitem "><p>
     You can check to see if the amphora failed over with <code class="literal">nova list
     --tenant &lt;project-id&gt;</code>. This may take some time and in some
     cases may need to be repeated several times. You can tell that the
     failover has been successful by the changed IP on the management network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.12 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div></li></ol></div><div id="id-1.6.11.5.13.6.22" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Do not issue too many failovers at once. In a big installation you might be
    tempted to initiate several failovers in parallel for instance to speed up
    an update of amphora images. This will put a strain on the nova service and
    depending on the size of your installation you might need to throttle the
    failover rate.
   </p></div></div><div class="sect3" id="octavia-admin-delete"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing load balancers</span> <a title="Permalink" class="permalink" href="#octavia-admin-delete">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>octavia-admin-delete</li></ul></div></div></div></div><p>
     The following procedures demonstrate how to delete a load
     balancer that is in the <code class="literal">ERROR</code>,
     <code class="literal">PENDING_CREATE</code>, or
     <code class="literal">PENDING_DELETE</code> state.
   </p><div class="procedure " id="id-1.6.11.5.13.7.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 9.1: </span><span class="name">
       Manually deleting load balancers created with neutron lbaasv2
       (in an upgrade/migration scenario)
      </span><a title="Permalink" class="permalink" href="#id-1.6.11.5.13.7.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Query the Neutron service for the loadbalancer ID:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron lbaas-loadbalancer-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| id                                   | name    | tenant_id                        | vip_address  | provisioning_status | provider |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | test-lb | d62a1510b0f54b5693566fb8afeb5e33 | 192.168.1.10 | ERROR               | haproxy  |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+</pre></div></li><li class="step "><p>
         Connect to the neutron database:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use ovs_neutron</pre></div></li><li class="step "><p>
         Get the pools and healthmonitors associated with the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, healthmonitor_id, loadbalancer_id from lbaas_pools where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | healthmonitor_id                     | loadbalancer_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 26c0384b-fc76-4943-83e5-9de40dd1c78c | 323a3c4b-8083-41e1-b1d9-04e1fef1a331 | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 |
+--------------------------------------+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
         Get the members associated with the pool:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, pool_id from lbaas_members where pool_id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';
+--------------------------------------+--------------------------------------+
| id                                   | pool_id                              |
+--------------------------------------+--------------------------------------+
| 6730f6c1-634c-4371-9df5-1a880662acc9 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
| 06f0cfc9-379a-4e3d-ab31-cdba1580afc2 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
         Delete the pool members:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_members where id = '6730f6c1-634c-4371-9df5-1a880662acc9';
mysql&gt; delete from lbaas_members where id = '06f0cfc9-379a-4e3d-ab31-cdba1580afc2';</pre></div></li><li class="step "><p>
         Find and delete the listener associated with the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, loadbalancer_id, default_pool_id from lbaas_listeners where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | loadbalancer_id                      | default_pool_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 3283f589-8464-43b3-96e0-399377642e0a | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+--------------------------------------+
mysql&gt; delete from lbaas_listeners where id = '3283f589-8464-43b3-96e0-399377642e0a';</pre></div></li><li class="step "><p>
         Delete the pool associated with the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_pools where id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';</pre></div></li><li class="step "><p>
         Delete the healthmonitor associated with the pool:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_healthmonitors where id = '323a3c4b-8083-41e1-b1d9-04e1fef1a331';</pre></div></li><li class="step "><p>
         Delete the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_loadbalancer_statistics where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
mysql&gt; delete from lbaas_loadbalancers where id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';</pre></div></li></ol></div></div><div class="procedure " id="id-1.6.11.5.13.7.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 9.2: </span><span class="name">Manually Deleting Load Balancers Created With Octavia </span><a title="Permalink" class="permalink" href="#id-1.6.11.5.13.7.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Query the Octavia service for the loadbalancer ID:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer list --column id --column name --column provisioning_status
+--------------------------------------+---------+---------------------+
| id                                   | name    | provisioning_status |
+--------------------------------------+---------+---------------------+
| d8ac085d-e077-4af2-b47a-bdec0c162928 | test-lb | ERROR               |
+--------------------------------------+---------+---------------------+</pre></div></li><li class="step "><p>
         Query the Octavia service for the amphora IDs (in this
         example we use <code class="literal">ACTIVE/STANDBY</code> topology with 1 spare Amphora):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer amphora list
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| id                                   | loadbalancer_id                      | status    | role   | lb_network_ip | ha_ip       |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| 6dc66d41-e4b6-4c33-945d-563f8b26e675 | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | BACKUP | 172.30.1.7    | 192.168.1.8 |
| 1b195602-3b14-4352-b355-5c4a70e200cf | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | MASTER | 172.30.1.6    | 192.168.1.8 |
| b2ee14df-8ac6-4bb0-a8d3-3f378dbc2509 | None                                 | READY     | None   | 172.30.1.20   | None        |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+</pre></div></li><li class="step "><p>
         Query the Octavia service for the loadbalancer pools:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer pool list
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| id                                   | name      | project_id                       | provisioning_status | protocol | lb_algorithm | admin_state_up |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| 39c4c791-6e66-4dd5-9b80-14ea11152bb5 | test-pool | 86fba765e67f430b83437f2f25225b65 | ACTIVE              | TCP      | ROUND_ROBIN  | True           |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+</pre></div></li><li class="step "><p>
         Connect to the octavia database:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use octavia</pre></div></li><li class="step "><p>
         Delete any listeners, pools, health monitors, and members
         from the load balancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from listener where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
mysql&gt; delete from health_monitor where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from member where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from pool where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
         Delete the amphora entries in the database:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from amphora_health where amphora_id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; update amphora set status = 'DELETED' where id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; delete from amphora_health where amphora_id = '1b195602-3b14-4352-b355-5c4a70e200cf';
mysql&gt; update amphora set status = 'DELETED' where id = '1b195602-3b14-4352-b355-5c4a70e200cf';</pre></div></li><li class="step "><p>
         Delete the load balancer instance:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; update load_balancer set provisioning_status = 'DELETED' where id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
         The following script automates the above steps:
       </p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

if (( $# != 1 )); then
  echo "Please specify a loadbalancer ID"
  exit 1
fi

LB_ID=$1

set -u -e -x

readarray -t AMPHORAE &lt; &lt;(openstack loadbalancer amphora list \
  --format value \
  --column id \
  --column loadbalancer_id \
  | grep ${LB_ID} \
  | cut -d ' ' -f 1)

readarray -t POOLS &lt; &lt;(openstack loadbalancer show ${LB_ID} \
  --format value \
  --column pools)

mysql octavia --execute "delete from listener where load_balancer_id = '${LB_ID}';"
for p in "${POOLS[@]}"; do
  mysql octavia --execute "delete from health_monitor where pool_id = '${p}';"
  mysql octavia --execute "delete from member where pool_id = '${p}';"
done
mysql octavia --execute "delete from pool where load_balancer_id = '${LB_ID}';"
for a in "${AMPHORAE[@]}"; do
  mysql octavia --execute "delete from amphora_health where amphora_id = '${a}';"
  mysql octavia --execute "update amphora set status = 'DELETED' where id = '${a}';"
done
mysql octavia --execute "update load_balancer set provisioning_status = 'DELETED' where id = '${LB_ID}';"</pre></div></li></ol></div></div></div><div class="sect3" id="idg-all-networking-octavia-admin-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#idg-all-networking-octavia-admin-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-octavia-admin-xml-10</li></ul></div></div></div></div><p>
   For more information on the Nova command-line client, see the
   <a class="link" href="http://docs.openstack.org/cli-reference/nova.html" target="_blank">OpenStack
   Compute command-line client</a> guide.
  </p><p>
   For more information on Octavia terminology, see the
   <a class="link" href="http://docs.octavia.io/review/master/main/glossary.html" target="_blank">OpenStack
   Octavia Glossary</a>
  </p></div></div><div class="sect2" id="cha-network-rbac"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Role-based Access Control in Neutron</span> <a title="Permalink" class="permalink" href="#cha-network-rbac">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>cha-network-rbac</li></ul></div></div></div></div><p>
   This topic explains how to achieve more granular access control for your
   Neutron networks.
  </p><p>
   Previously in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, a network object was either private to a project or
   could be used by all projects. If the network's shared attribute was True,
   then the network could be used by every project in the cloud. If false, only
   the members of the owning project could use it. There was no way for the
   network to be shared by only a subset of the projects.
  </p><p>
  <span class="phrase">Neutron Role Based Access Control (RBAC) solves this problem for
   networks. Now the network owner can create RBAC policies that give network
   access to target projects. Members of a targeted project can use the
   network named in the RBAC policy the same way as if the network was owned
   by the project. Constraints are described in the
   section</span>
  <a class="xref" href="#sec-network-rbac-limitation" title="9.3.10.10. Limitations">Section 9.3.10.10, “Limitations”</a>.
 </p><p>
   With RBAC you are able to let another tenant use a network that you created,
   but as the owner of the network, you need to create the subnet and the
   router for the network.
  </p><div class="sect3" id="id-1.6.11.5.14.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Network</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create demo-net
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 9c801954-ec7f-4a65-82f8-e313120aabc4 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an RBAC Policy</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here we will create an RBAC policy where a member of the project called
   'demo' will share the network with members of project 'demo2'
  </p><p>
   To create the RBAC policy, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create  --target-project <em class="replaceable ">DEMO2-PROJECT-ID</em> --type network --action access_as_shared demo-net</pre></div><p>
   Here is an example where the <em class="replaceable ">DEMO2-PROJECT-ID</em> is
   5a582af8b44b422fafcd4545bd2b7eb5
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --target-tenant 5a582af8b44b422fafcd4545bd2b7eb5 \
  --type network --action access_as_shared demo-net</pre></div></div><div class="sect3" id="id-1.6.11.5.14.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing RBACs</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To list all the RBAC rules/policies, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac list
+--------------------------------------+-------------+--------------------------------------+
| ID                                   | Object Type | Object ID                            |
+--------------------------------------+-------------+--------------------------------------+
| 0fdec7f0-9b94-42b4-a4cd-b291d04282c1 | network     | 7cd94877-4276-488d-b682-7328fc85d721 |
+--------------------------------------+-------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing the Attributes of an RBAC</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To see the attributes of a specific RBAC policy, run
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0fd89dcb-9809-4a5e-adc1-39dd676cb386 |
| object_id     | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b |
| object_type   | network                              |
| target_tenant | 5a582af8b44b422fafcd4545bd2b7eb5     |
| tenant_id     | 75eb5efae5764682bca2fede6f4d8c6f     |
+---------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deleting an RBAC Policy</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To delete an RBAC policy, run <code class="literal">openstack network rbac delete</code> passing the policy id:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">Deleted rbac_policy: 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div></div><div class="sect3" id="id-1.6.11.5.14.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sharing a Network with All Tenants</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Either the administrator or the network owner can make a network shareable
   by all tenants.
  </p><p>
   The administrator can make a tenant's network shareable by all tenants.
   To make the network <code class="literal">demo-shareall-net</code> accessible by all
   tenants in the cloud:
  </p><p>
   To share a network with all tenants:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of all projects
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
      which produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 1be57778b61645a7a1c07ca0ac488f9e | demo             |
| 5346676226274cd2b3e3862c2d5ceadd | admin            |
| 749a557b2b9c482ca047e8f4abf348cd | swift-monitor    |
| 8284a83df4df429fb04996c59f9a314b | swift-dispersion |
| c7a74026ed8d4345a48a3860048dcb39 | demo-sharee      |
| e771266d937440828372090c4f99a995 | glance-swift     |
| f43fb69f107b4b109d22431766b85f20 | services         |
+----------------------------------+------------------+</pre></div></li><li class="step "><p>
     Get a list of networks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><p>
     This produces the following list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-------------------+----------------------------------------------------+
| id                                   | name              | subnets                                            |
+--------------------------------------+-------------------+----------------------------------------------------+
| f50f9a63-c048-444d-939d-370cb0af1387 | ext-net           | ef3873db-fc7a-4085-8454-5566fb5578ea 172.31.0.0/16 |
| 9fb676f5-137e-4646-ac6e-db675a885fd3 | demo-net          | 18fb0b77-fc8b-4f8d-9172-ee47869f92cc 10.0.1.0/24   |
| 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e | demo-shareall-net | 2bbc85a9-3ffe-464c-944b-2476c7804877 10.0.250.0/24 |
| 73f946ee-bd2b-42e9-87e4-87f19edd0682 | demo-share-subset | c088b0ef-f541-42a7-b4b9-6ef3c9921e44 10.0.2.0/24   |
+--------------------------------------+-------------------+----------------------------------------------------+</pre></div></li><li class="step "><p>
     Set the network you want to share to a shared value of True:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network set --share 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     You should see the following output:
    </p><div class="verbatim-wrap"><pre class="screen">Updated network: 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div></li><li class="step "><p>
     Check the attributes of that network by running the following command
     using the ID of the network in question:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network show 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     The output will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | None                                 |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     As the owner of the <code class="literal">demo-shareall-net</code> network, view
     the RBAC attributes for
     <code class="literal">demo-shareall-net</code>
     (<code class="literal">id=8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</code>) by first
     getting an RBAC list:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_USERNAME ; echo $OS_PROJECT_NAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network rbac list</pre></div><p>
     This produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+--------------------------------------+
| id                                   | object_id                            |
+--------------------------------------+--------------------------------------+
| ...                                                                         |
| 3e078293-f55d-461c-9a0b-67b5dae321e8 | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     View the RBAC information:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 3e078293-f55d-461c-9a0b-67b5dae321e8

+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 3e078293-f55d-461c-9a0b-67b5dae321e8 |
| object_id     | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li><li class="step "><p>
     With network RBAC, the owner of the network can also make the network
     shareable by all tenants. First create the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_PROJECT_NAME ; echo $OS_USERNAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network create test-net</pre></div><p>
     The network is created:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T18:04:25Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | test-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1073                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T18:04:25Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create the RBAC. It is important that the asterisk is surrounded by
     single-quotes to prevent the shell from expanding it to all files in the
     current directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --type network \
  --action access_as_shared --target-project '*' test-net</pre></div><p>
     Here are the resulting RBAC attributes:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0b797cc6-debc-48a1-bf9d-d294b077d0d9 |
| object_id     | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.6.11.5.14.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project (<code class="literal">demo2</code>) View of Networks and Subnets</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Note that the owner of the network and subnet is not the tenant named
   <code class="literal">demo2</code>. Both the network and subnet are owned by tenant <code class="literal">demo</code>.
   <code class="literal">Demo2</code>members cannot create subnets of the network. They also cannot
   modify or delete subnets owned by <code class="literal">demo</code>.
  </p><p>
   As the tenant <code class="literal">demo2</code>, you can get a list of neutron networks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-----------+--------------------------------------------------+
| id                                   | name      | subnets                                          |
+--------------------------------------+-----------+--------------------------------------------------+
| f60f3896-2854-4f20-b03f-584a0dcce7a6 | ext-net   | 50e39973-b2e3-466b-81c9-31f4d83d990b             |
| c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | demo-net  | d9b765da-45eb-4543-be96-1b69a00a2556 10.0.1.0/24 |
   ...
+--------------------------------------+-----------+--------------------------------------------------+</pre></div><p>
   And get a list of subnets:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet list --network c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+---------+--------------------------------------+---------------+
| ID                                   | Name    | Network                              | Subnet        |
+--------------------------------------+---------+--------------------------------------+---------------+
| a806f28b-ad66-47f1-b280-a1caa9beb832 | ext-net | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | 10.0.1.0/24   |
+--------------------------------------+---------+--------------------------------------+---------------+</pre></div><p>
To show details of the subnet:
</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet show d9b765da-45eb-4543-be96-1b69a00a2556</pre></div><div class="verbatim-wrap"><pre class="screen">+-------------------+--------------------------------------------+
| Field             | Value                                      |
+-------------------+--------------------------------------------+
| allocation_pools  | {"start": "10.0.1.2", "end": "10.0.1.254"} |
| cidr              | 10.0.1.0/24                                |
| dns_nameservers   |                                            |
| enable_dhcp       | True                                       |
| gateway_ip        | 10.0.1.1                                   |
| host_routes       |                                            |
| id                | d9b765da-45eb-4543-be96-1b69a00a2556       |
| ip_version        | 4                                          |
| ipv6_address_mode |                                            |
| ipv6_ra_mode      |                                            |
| name              | sb-demo-net                                |
| network_id        | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b       |
| subnetpool_id     |                                            |
| tenant_id         | 75eb5efae5764682bca2fede6f4d8c6f           |
+-------------------+--------------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project: Creating a Port Using demo-net</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The owner of the port is <code class="literal">demo2</code>. Members of the network owner project
   (<code class="literal">demo</code>) will not see this port.
  </p><p>
   Running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><p>
   Creates a new port:
  </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             |                                                                                                     |
| device_owner          |                                                                                                     |
| dns_assignment        | {"hostname": "host-10-0-1-10", "ip_address": "10.0.1.10", "fqdn": "host-10-0-1-10.openstacklocal."} |
| dns_name              |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.10"}                    |
| id                    | 03ef2dce-20dc-47e5-9160-942320b4e503                                                                |
| mac_address           | fa:16:3e:27:8d:ca                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | DOWN                                                                                                |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.14"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project Booting a VM Using Demo-Net</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.14.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here the tenant <code class="literal">demo2</code> boots a VM that uses the <code class="literal">demo-net</code> shared network:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor 1 --image $OS_IMAGE --nic net-id=c3d55c21-d8c9-4ee5-944b-560b7e0ea33b demo2-vm-using-demo-net-nic</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------------------------------+
| Property                             | Value                                          |
+--------------------------------------+------------------------------------------------+
| OS-EXT-AZ:availability_zone          |                                                |
| OS-EXT-STS:power_state               | 0                                              |
| OS-EXT-STS:task_state                | scheduling                                     |
| OS-EXT-STS:vm_state                  | building                                       |
| OS-SRV-USG:launched_at               | -                                              |
| OS-SRV-USG:terminated_at             | -                                              |
| accessIPv4                           |                                                |
| accessIPv6                           |                                                |
| adminPass                            | sS9uSv9PT79F                                   |
| config_drive                         |                                                |
| created                              | 2016-01-04T19:23:24Z                           |
| flavor                               | m1.tiny (1)                                    |
| hostId                               |                                                |
| id                                   | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a           |
| image                                | cirros-0.3.3-x86_64 (6ae23432-8636-4e...1efc5) |
| key_name                             | -                                              |
| metadata                             | {}                                             |
| name                                 | demo2-vm-using-demo-net-nic                    |
| os-extended-volumes:volumes_attached | []                                             |
| progress                             | 0                                              |
| security_groups                      | default                                        |
| status                               | BUILD                                          |
| tenant_id                            | 5a582af8b44b422fafcd4545bd2b7eb5               |
| updated                              | 2016-01-04T19:23:24Z                           |
| user_id                              | a0e6427b036344fdb47162987cb0cee5               |
+--------------------------------------+------------------------------------------------+</pre></div><p>
   Run openstack server list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list</pre></div><p>
   See the VM running:
  </p><div class="verbatim-wrap"><pre class="screen">+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| ID                | Name                        | Status | Task State | Power State | Networks           |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| 3a4dc...a7c2dca2a | demo2-vm-using-demo-net-nic | ACTIVE | -          | Running     | demo-net=10.0.1.11 |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+</pre></div><p>
   Run openstack port list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-list --device-id 3a4dc44a-027b-45e9-acf8-054a7c2dca2a</pre></div><p>
   View the subnet:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------------+------+-------------------+-------------------------------------------------------------------+
| id                  | name | mac_address       | fixed_ips                                                         |
+---------------------+------+-------------------+-------------------------------------------------------------------+
| 7d14ef8b-9...80348f |      | fa:16:3e:75:32:8e | {"subnet_id": "d9b765da-45...00a2556", "ip_address": "10.0.1.11"} |
+---------------------+------+-------------------+-------------------------------------------------------------------+</pre></div><p>
   Run neutron port-show:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 7d14ef8b-9d48-4310-8c02-00c74d80348f</pre></div><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a                                                                |
| device_owner          | compute:None                                                                                        |
| dns_assignment        | {"hostname": "host-10-0-1-11", "ip_address": "10.0.1.11", "fqdn": "host-10-0-1-11.openstacklocal."} |
| dns_name              |                                                                                                     |
| extra_dhcp_opts       |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.11"}                    |
| id                    | 7d14ef8b-9d48-4310-8c02-00c74d80348f                                                                |
| mac_address           | fa:16:3e:75:32:8e                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | ACTIVE                                                                                              |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="sec-network-rbac-limitation"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#sec-network-rbac-limitation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>sec-network-rbac-limitation</li></ul></div></div></div></div><p>
   Note the following limitations of RBAC in Neutron.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Neutron network is the only supported RBAC Neutron object type.
    </p></li><li class="listitem "><p>
     The "access_as_external" action is not supported – even though it is
     listed as a valid action by python-neutronclient.
    </p></li><li class="listitem "><p>
     The neutron-api server will not accept action value of
     'access_as_external'. The <code class="literal">access_as_external</code> definition
     is not found in the specs.
    </p></li><li class="listitem "><p>
     The target project users cannot create, modify, or delete subnets on
     networks that have RBAC policies.
    </p></li><li class="listitem "><p>
     The subnet of a network that has an RBAC policy cannot be added as an
     interface of a target tenant's router. For example, the command
     <code class="literal">neutron router-interface-add tgt-tenant-router &lt;sb-demo-net
     uuid&gt;</code> will error out.
    </p></li><li class="listitem "><p>
     The security group rules on the network owner do not apply to other
     projects that can use the network.
    </p></li><li class="listitem "><p>
     A user in target project can boot up VMs using a VNIC using the shared
     network. The user of the target project can assign a floating IP (FIP) to
     the VM. The target project must have SG rules that allows SSH and/or ICMP
     for VM connectivity.
    </p></li><li class="listitem "><p>
     Neutron RBAC creation and management are currently not supported in
     Horizon. For now, the Neutron CLI has to be used to manage RBAC rules.
    </p></li><li class="listitem "><p>
     A RBAC rule tells Neutron whether a tenant can access a network (Allow).
     Currently there is no DENY action.
    </p></li><li class="listitem "><p>
     Port creation on a shared network fails if <code class="literal">--fixed-ip</code>
     is specified in the <code class="literal">neutron port-create</code> command.
     
     
    </p></li></ul></div></div></div><div class="sect2" id="configureMTU"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Maximum Transmission Units in Neutron</span> <a title="Permalink" class="permalink" href="#configureMTU">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>configureMTU</li></ul></div></div></div></div><p>
  This topic explains how you can configure MTUs, what to look out for, and
  the results and implications of changing the default MTU settings. It is
  important to note that every network within a network group will have the
  same MTU.
 </p><div id="id-1.6.11.5.15.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   An MTU change will not affect existing networks that have had VMs created
   on them. It will only take effect on new networks created after the
   reconfiguration process.
  </p></div><div class="sect3" id="id-1.6.11.5.15.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.15.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A Maximum Transmission Unit, or MTU is the maximum packet size (in bytes)
   that a network device can or is configured to handle. There are a number of
   places in your cloud where MTU configuration is relevant: the physical
   interfaces managed and configured by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the virtual
   interfaces created by Neutron and Nova for Neutron networking, and the
   interfaces inside the VMs.
  </p><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces </strong></span>
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces include the physical interfaces and the
   bonds, bridges, and VLANs created on top of them. The MTU for these
   interfaces is configured via the 'mtu' property of a network group. Because
   multiple network groups can be mapped to one physical interface, there may
   have to be some resolution of differing MTUs between the untagged and tagged
   VLANs on the same physical interface. For instance, if one untagged VLAN,
   vlan101 (with an MTU of 1500) and a tagged VLAN vlan201 (with an MTU of
   9000) are both on one interface (eth0), this means that eth0 can handle
   1500, but the VLAN interface which is created on top of eth0 (that is,
   <code class="literal">vlan201@eth0</code>) wants 9000. However, vlan201 cannot have a
   higher MTU than eth0, so vlan201 will be limited to 1500 when it is brought
   up, and fragmentation will result.
  </p><p>
   In general, a VLAN interface MTU must be lower than or equal to the base
   device MTU. If they are different, as in the case above, the MTU of eth0 can
   be overridden and raised to 9000, but in any case the discrepancy will have
   to be reconciled.
  </p><p>
   <span class="bold"><strong>Neutron/Nova interfaces </strong></span>
  </p><p>
   Neutron/Nova interfaces include the virtual devices created by Neutron and
   Nova during the normal process of realizing a Neutron network/router and
   booting a VM on it (qr-*, qg-*, tap-*, qvo-*, qvb-*, etc.). There is
   currently no support in Neutron/Nova for per-network MTUs in which every
   interface along the path for a particular Neutron network has the correct
   MTU for that network. There is, however, support for globally changing the
   MTU of devices created by Neutron/Nova (see network_device_mtu below). This
   means that if you want to enable jumbo frames for any set of VMs, you will
   have to enable it for all your VMs. You cannot just enable them for a
   particular Neutron network.
  </p><p>
   <span class="bold"><strong>VM interfaces</strong></span>
  </p><p>
   VMs typically get their MTU via DHCP advertisement, which means that the
   dnsmasq processes spawned by the neutron-dhcp-agent actually advertise a
   particular MTU to the VMs. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the DHCP server advertises to
   all VMS a 1400 MTU via a forced setting in dnsmasq-neutron.conf. This is
   suboptimal for every network type (vxlan, flat, vlan, etc) but it does
   prevent fragmentation of a VM's packets due to encapsulation.
  </p><p>
   For instance, if you set the new *-mtu configuration options to a default of
   1500 and create a VXLAN network, it will be given an MTU of 1450 (with the
   remaining 50 bytes used by the VXLAN encapsulation header) and will
   advertise a 1450 MTU to any VM booted on that network. If you create a
   provider VLAN network, it will have an MTU of 1500 and will advertise 1500
   to booted VMs on the network. It should be noted that this default starting
   point for MTU calculation and advertisement is also global, meaning you
   cannot have an MTU of 8950 on one VXLAN network and 1450 on another. However,
   you can have provider physical networks with different MTUs by using the
   physical_network_mtus config option, but Nova still requires a global MTU
   option for the interfaces it creates, thus you cannot really take advantage
   of that configuration option.
  </p></div><div class="sect3" id="id-1.6.11.5.15.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network settings in the input model</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.15.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   MTU can be set as an attribute of a network group in network_groups.yml.
   Note that this applies only to KVM. That setting means that every network in
   the network group will be assigned the specified MTU. The MTU value must be
   set individually for each network group. For example:
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:
        - name: GUEST
        mtu: 9000
        ...

        - name: EXTERNAL-API
        mtu: 9000
        ...

        - name: EXTERNAL-VM
        mtu: 9000
        ...</pre></div></div><div class="sect3" id="id-1.6.11.5.15.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Infrastructure support for jumbo frames</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.15.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you want to use jumbo frames, or frames with an MTU of 9000 or more, the
   physical switches and routers that make up the infrastructure of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation must be configured to support them. To realize the advantages,
   generally all devices in the same broadcast domain must have the same MTU.
  </p><p>
   If you want to configure jumbo frames on compute and controller nodes, then
   all switches joining the compute and controller nodes must have jumbo frames
   enabled. Similarly, the "infrastructure gateway" through which the external
   VM network flows, commonly known as the default route for the external VM
   VLAN, must also have the same MTU configured.
  </p><p>
   You can also consider anything in the same broadcast domain to be anything
   in the same VLAN or anything in the same IP subnet.
  </p></div><div class="sect3" id="id-1.6.11.5.15.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling end-to-end jumbo frames for a VM</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.15.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add an 'mtu' attribute to all the network groups in your model. Note that
     adding the MTU for the network groups will only affect the configuration
     for physical network interfaces.
    </p><p>
     To add the mtu attribute, find the YAML file that contains your
     network-groups entry. We will assume it is network_groups.yml, unless you
     have changed it. Whatever the file is named, it will be found in
     ~/openstack/my_cloud/definition/data/.
    </p><p>
     To edit these files, begin by checking out the
     <span class="bold"><strong>site</strong></span> branch on the Cloud Lifecycle Manager
     node. You may already be on that branch. If so, you will remain there.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div><p>
     Then begin editing the files. In network_groups.yml, add mtu: 9000
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:
            - name: GUEST
            hostname-suffix: guest
            mtu: 9000
            tags:
            - neutron.networks.vxlan</pre></div><p>
     This will set the physical interface managed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> that has
     the GUEST network group tag assigned to it. This can be found in the
     interfaces_set.yml file under the interface-models section.
    </p></li><li class="step "><p>
     Next, edit the layer 2 agent config file, ml2_conf.ini.j2, found in
     ~/openstack/my_cloud/config/neutron/ to set the path_mtu to 0, this ensures
     that global_physnet_mtu is used.
    </p><div class="verbatim-wrap"><pre class="screen">[ml2]
...
path_mtu = 0</pre></div></li><li class="step "><p>
     Next, edit neutron.conf.j2 found in ~/openstack/my_cloud/config/neutron/ to
     set advertise_mtu (to true) and global_physnet_mtu to 9000 under
     [DEFAULT]:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
advertise_mtu = True
global_physnet_mtu = 9000</pre></div><p>
     This allows Neutron to advertise the optimal MTU to instances (based upon
     global_physnet_mtu minus the encapsulation size).
    </p></li><li class="step "><p>
     Next, remove the "dhcp-option-force=26,1400" line from
     <code class="filename">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>.
    </p></li><li class="step "><p>
     OvS will set <code class="literal">br-int</code> to the value of the lowest physical
     interface. If you are using Jumbo frames on some of your networks,
     <code class="literal">br-int</code> on the controllers may be set to 1500 instead of
     9000. Work around this condition by running:
    </p><div class="verbatim-wrap"><pre class="screen">ovs-vsctl set int br-int mtu_request=9000</pre></div></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has not been deployed yet, do normal deployment and skip to
     step 8.
    </p></li><li class="step "><p>
     Assuming it has been deployed already, continue here:
    </p><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     and ready the deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Then run the network_interface-reconfigure.yml playbook, changing
     directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div><p>
     Then run neutron-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div><p>
     Then nova-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div><p>
     Note: adding/changing network-group mtu settings will likely require a
     network restart during network_interface-reconfigure.yml.
    </p></li><li class="step "><p>
     Follow the normal process for creating a Neutron network and booting a VM
     or two. In this example, if a VXLAN network is created and a VM is booted
     on it, the VM will have an MTU of 8950, with the remaining 50 bytes used
     by the VXLAN encapsulation header.
    </p></li><li class="step "><p>
     Test and verify that the VM can send and receive jumbo frames without
     fragmentation. You can use ping. For example, to test an MTU of 9000 using
     VXLAN:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ping –M do –s 8950 <em class="replaceable ">YOUR_VM_FLOATING_IP</em></pre></div><p>
     Just substitute your actual floating IP address for the
     <em class="replaceable ">YOUR_VM_FLOATING_IP</em>.
    </p></li></ol></div></div></div><div class="sect3" id="optimal-mtu"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Optimal MTU Advertisement Feature</span> <a title="Permalink" class="permalink" href="#optimal-mtu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>optimal-mtu</li></ul></div></div></div></div><p>
   To enable the optimal MTU feature, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
     to <span class="bold"><strong>remove</strong></span> advertise_mtu variable under
     [DEFAULT]
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
advertise_mtu = False #remove this</pre></div></li><li class="step "><p>
     Remove the <code class="literal">dhcp-option-force=26,1400</code> line from
     <code class="literal">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>
    </p></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has already been deployed, follow the remaining steps,
     otherwise follow the normal deployment procedures.
    </p></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run ready deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">network_interface-reconfigure.yml</code> playbook,
     changing directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div></li><li class="step "><p>
     Run neutron-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div id="id-1.6.11.5.15.8.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If you are upgrading an existing deployment, attention must be paid to
    avoid creating MTU mismatch between network interfaces in preexisting VMs
    and that of VMs created after upgrade. If you do have an MTU mismatch, then
    the new VMs (having interface with 1500 minus the underlay protocol
    overhead) will not be able to have L2 connectivity with preexisting VMs
    (with 1400 MTU due to dhcp-option-force).
   </p></div></div></div><div class="sect2" id="topic-shy-ksv-jw"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve Network Peformance with Isolated Metadata Settings</span> <a title="Permalink" class="permalink" href="#topic-shy-ksv-jw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-isolated_metadata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-isolated_metadata.xml</li><li><span class="ds-label">ID: </span>topic-shy-ksv-jw</li></ul></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, Neutron currently sets <code class="literal">enable_isolated_metadata =
  True</code> by default in <code class="literal">dhcp_agent.ini</code> because
  several services require isolated networks (Neutron networks without a
  router). This has the effect of spawning a neutron-ns-metadata-proxy process
  on one of the controller nodes for every active Neutron network.
 </p><p>
  In environments that create many Neutron networks, these extra
  <code class="literal">neutron-ns-metadata-proxy</code> processes can quickly eat up a
  lot of memory on the controllers, which does not scale well.
 </p><p>
  For deployments that do not require isolated metadata (that is, they do not
  require the Platform Services and will always create networks with an
  attached router), you can set <code class="literal">enable_isolated_metadata =
  False</code> in dhcp_agent.ini to reduce Neutron memory usage on
  controllers, allowing a greater number of active Neutron networks.
 </p><p>
  Note that the <code class="literal">dhcp_agent.ini.j2</code> template is found in
  <code class="literal">~/openstack/my_cloud/config/neutron</code> on the Cloud Lifecycle Manager
  node. The edit can be made there and the standard deployment can be run if
  this is install time. In a deployed cloud, run the Neutron reconfiguration
  procedure outlined here:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    First check out the site branch:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/neutron
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="step "><p>
    Edit the <code class="literal">dhcp_agent.ini.j2</code> file to change the
    <code class="literal">enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</code>
    line in the <code class="literal">[DEFAULT]</code> section to read:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = False</pre></div></li><li class="step "><p>
    Commit the file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
    Run the <code class="literal">ready-deployment.yml</code> playbook from
    <code class="literal">~/openstack/ardana/ansible</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Then run the <code class="literal">neutron-reconfigure.yml</code> playbook, changing
    directories first:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="moving-from-dvr-deployments"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Moving from DVR deployments to non_DVR</span> <a title="Permalink" class="permalink" href="#moving-from-dvr-deployments">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-moving_from_dvr_deployments.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-moving_from_dvr_deployments.xml</li><li><span class="ds-label">ID: </span>moving-from-dvr-deployments</li></ul></div></div></div></div><p>
  If you have an older deployment of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which is using DVR as a default
  and you are attempting to move to non_DVR, make sure you follow these steps:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Remove all your existing DVR routers and their workloads. Make sure to
    remove interfaces, floating ips and gateways, if applicable.
   </p><div class="verbatim-wrap"><pre class="screen">neutron router-interface-delete <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">SUBNET-NAME</em>/<em class="replaceable ">SUBNET-ID</em>
neutron floatingip-disassociate <em class="replaceable ">FLOATINGIP-ID</em> <em class="replaceable ">PRIVATE-PORT-ID</em>
neutron router-gateway-clear <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">-NET-NAME</em>/<em class="replaceable ">EXT-NET-ID</em></pre></div></li><li class="listitem "><p>
    Then delete the router.
   </p><div class="verbatim-wrap"><pre class="screen">neutron router-delete <em class="replaceable ">ROUTER-NAME</em></pre></div></li><li class="listitem "><p>
    Before you create any non_DVR router make sure that l3-agents and
    metadata-agents are not running in any compute host. You can run the
    command <code class="literal">neutron agent-list</code> to see if there are any
    neutron-l3-agent running in any compute-host in your deployment.
   </p><p>
    You must disable neutron-l3-agent and neutron-metadata-agent on every
    compute host by running the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron agent-list
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | availability_zone | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| 208f6aea-3d45-4b89-bf42-f45a51b05f29 | Loadbalancerv2 agent | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-lbaasv2-agent     |
| 810f0ae7-63aa-4ee3-952d-69837b4b2fe4 | L3 agent             | ardana-cp1-comp0001-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
| 89ac17ba-2f43-428a-98fa-b3698646543d | Metadata agent       | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| f602edce-1d2a-4c8a-ba56-fa41103d4e17 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
...
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+

$ neutron agent-update 810f0ae7-63aa-4ee3-952d-69837b4b2fe4 --admin-state-down
Updated agent: 810f0ae7-63aa-4ee3-952d-69837b4b2fe4

$ neutron agent-update 89ac17ba-2f43-428a-98fa-b3698646543d --admin-state-down
Updated agent: 89ac17ba-2f43-428a-98fa-b3698646543d</pre></div><div id="id-1.6.11.5.17.3.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Only L3 and Metadata agents were disabled.
     </p></div></li><li class="listitem "><p>
    Once L3 and metadata neutron agents are stopped, follow steps 1 through 7
    in the document <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 12 “Alternative Configurations”, Section 12.2 “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR”</span> and then run the
    <code class="literal">neutron-reconfigure.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="dpdk"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVS-DPDK Support</span> <a title="Permalink" class="permalink" href="#dpdk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span>dpdk</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a version of Open vSwitch (OVS) that is built with the
  Data Plane Development Kit (DPDK) and includes a QEMU hypervisor which
  supports vhost-user.
 </p><p>
  The OVS-DPDK package modifes the OVS fast path, which is normally performed
  in kernel space, and allows it to run in userspace so there is no context
  switch to the kernel for processing network packets.
 </p><p>
  The EAL component of DPDK supports mapping the Network Interface Card (NIC)
  registers directly into userspace. The DPDK provides a Poll Mode Driver
  (PMD) that can access the NIC hardware from userspace and uses polling
  instead of interrupts to avoid the user to kernel transition.
 </p><p>
  The PMD maps the shared address space of the VM that is provided by the
  vhost-user capability of QEMU. The vhost-user mode causes Neutron to create
  a Unix domain socket that allows communication between the PMD and QEMU. The
  PMD uses this in order to acquire the file descriptors to the pre-allocated
  VM memory. This allows the PMD to directly access the VM memory space and
  perform a fast zero-copy of network packets directly into and out of the VMs
  virtio_net vring.
 </p><p>
  This yields performance improvements in the time it takes to process network
  packets.
 </p><div class="sect3" id="id-1.6.11.5.18.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage considerations</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The target for a DPDK Open vSwitch is VM performance and VMs only run on
   compute nodes so the following considerations are compute node specific.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     You are required to <a class="xref" href="#hugepages" title="9.3.14.3. Configuring Hugepages for DPDK in Neutron Networks">Section 9.3.14.3, “Configuring Hugepages for DPDK in Neutron Networks”</a> in order to use DPDK with
     VMs. The memory to be used must be allocated at boot time so you must know
     beforehand how many VMs will be scheduled on a node. Also, for NUMA
     considerations, you want those hugepages on the same NUMA node as the NIC.
     A VM maps its entire address space into a hugepage.
    </p></li><li class="listitem "><p>
     For maximum performance you must reserve logical cores for DPDK poll mode
     driver (PMD) usage and for hypervisor (QEMU) usage. This keeps the Linux
     kernel from scheduling processes on those cores. The PMD threads will go
     to 100% cpu utilization since it uses polling of the hardware instead of
     interrupts. There will be at least 2 cores dedicated to PMD threads. Each
     VM will have a core dedicated to it although for less performance VMs can
     share cores.
    </p></li><li class="listitem "><p>
     VMs can use the virtio_net or the virtio_pmd drivers. There is also a PMD
     for an emulated e1000.
    </p></li><li class="listitem "><p>
     Only VMs that use hugepages can be sucessfully launched on a DPDK enabled
     NIC. If there is a need to support both DPDK and non-DPDK based VMs an
     additional port managed by the Linux kernel must exist.
    </p></li><li class="listitem "><p>
     OVS/DPDK does not support jumbo frames. Please review
     <a class="link" href="https://github.com/openvswitch/ovs/blob/branch-2.5/INSTALL.DPDK.md#restrictions" target="_blank">https://github.com/openvswitch/ovs/blob/branch-2.5/INSTALL.DPDK.md#restrictions</a>
     for restrictions.
    </p></li><li class="listitem "><p>
     The Open vSwitch firewall driver in networking-ovs-dpdk repo is stateless
     and not a stateful one that would use iptables and conntrack. In the past,
     Neutron core has declined to pull in stateless type FW.
     <a class="link" href="https://bugs.launchpad.net/neutron/+bug/1531205" target="_blank">https://bugs.launchpad.net/neutron/+bug/1531205</a>
     The native firewall driver is stateful, which is why conntrack was added
     to Open vSwitch. But this is not supported on DPDK and will not be until OVS
     2.6.
    </p></li></ol></div></div><div class="sect3" id="id-1.6.11.5.18.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   See the following topics for more information:
  </p></div><div class="sect3" id="hugepages"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Hugepages for DPDK in Neutron Networks</span> <a title="Permalink" class="permalink" href="#hugepages">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span>hugepages</li></ul></div></div></div></div><p>
  To take advantage of DPDK and its network
  performance enhancements, enable hugepages first.
 </p><p>
  With hugepages, physical RAM is reserved at boot time and dedicated to a
  virtual machine. Only that virtual machine and Open vSwitch can use this
  specifically allocated RAM. The host OS cannot access it. This memory is
  contiguous, and because of its larger size, reduces the number of entries in
  the memory map and number of times it must be read.
 </p><p>
  The hugepage reservation is made in <code class="literal">/etc/default/grub</code>,
  but this is handled by the Cloud Lifecycle Manager.
 </p><p>
  In addition to hugepages, to use DPDK, CPU isolation is required. This is
  achieved with the 'isolcups' command in
  <code class="literal">/etc/default/grub</code>, but this is also managed by the
  Cloud Lifecycle Manager using a new input model file.
 </p><p>
  The two new input model files introduced with this release to help you
  configure the necessary settings and persist them are:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    memory_models.yml (for hugepages)
   </p></li><li class="listitem "><p>
    cpu_models.yml (for CPU isolation)
   </p></li></ul></div><div class="sect4" id="id-1.6.11.5.18.9.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">memory_models.yml</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.9.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In this file you set your huge page size along with the number of such
   huge-page allocations.
  </p><div class="verbatim-wrap"><pre class="screen"> ---
  product:
    version: 2

  memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 1G
      huge-pages:
        - size: 1G
          count: 24
          numa-node: 0
        - size: 1G
          count: 24
          numa-node: 1
        - size: 1G
          count: 48</pre></div></div><div class="sect4" id="id-1.6.11.5.18.9.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">cpu_models.yml</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.9.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  cpu-models:

    - name: COMPUTE-CPU
      assignments:
       - components:
           - nova-compute-kvm
         cpu:
           - processor-ids: 3-5,12-17
             role: vm

       - components:
           - openvswitch
         cpu:
           - processor-ids: 0
             role: eal
           - processor-ids: 1-2
             role: pmd</pre></div></div><div class="sect4" id="id-1.6.11.5.18.9.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NUMA memory allocation</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.9.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As mentioned above, the memory used for hugepages is locked down at boot
   time by an entry in <code class="literal">/etc/default/grub</code>. As an admin, you
   can specify in the input model how to arrange this memory on NUMA nodes. It
   can be spread across NUMA nodes or you can specify where you want it. For
   example, if you have only one NIC, you would probably want all the hugepages
   memory to be on the NUMA node closest to that NIC.


  </p><p>
   If you do not specify the <code class="literal">numa-node</code> settings in the
   <code class="literal">memory_models.yml</code> input model file and use only the last
   entry indicating "size: 1G" and "count: 48" then this memory is spread
   evenly across all NUMA nodes.
  </p><p>
   Also note that the hugepage service runs once at boot time and then goes to
   an inactive state so you should not expect to see it running. If you decide
   to make changes to the NUMA memory allocation, you will need to reboot the
   compute node for the changes to take effect.
  </p></div></div><div class="sect3" id="dpdk-setup"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Setup for Neutron Networking</span> <a title="Permalink" class="permalink" href="#dpdk-setup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span>dpdk-setup</li></ul></div></div></div></div><div class="sect4" id="id-1.6.11.5.18.10.2"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware requirements</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Intel-based compute node. DPDK is not available on AMD-based systems.
    </p></li><li class="listitem "><p>
     The following BIOS settings must be enabled for DL360 Gen9:
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       Virtualization Technology
      </p></li><li class="listitem "><p>
       Intel(R) VT-d
      </p></li><li class="listitem "><p>
       PCI-PT (Also see <a class="xref" href="#pcipt-gen9" title="9.3.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 9.3.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>)
      </p></li></ol></div></li><li class="listitem "><p>
     Need adequate host memory to allow for hugepages. The examples below use
     1G hugepages for the VMs
    </p></li></ul></div></div><div class="sect4" id="id-1.6.11.5.18.10.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     DPDK is supported on SLES only.
    </p></li><li class="listitem "><p>
     Applies to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> only.
    </p></li><li class="listitem "><p>
     Tenant network can be untagged vlan or untagged vxlan
    </p></li><li class="listitem "><p>
     DPDK port names must be of the form 'dpdk&lt;portid&gt;' where port id is
     sequential and starts at 0
    </p></li><li class="listitem "><p>
     No support for converting DPDK ports to non DPDK ports without rebooting
     compute node.
    </p></li><li class="listitem "><p>
     No security group support, need userspace conntrack.
    </p></li><li class="listitem "><p>
     No jumbo frame support.
    </p></li></ul></div></div><div class="sect4" id="id-1.6.11.5.18.10.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup instructions</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.18.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   These setup instructions and example model are for a three-host system.
   There is one controller with Cloud Lifecycle Manager in cloud control plane and
   two compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After initial run of site.yml all compute nodes must be rebooted to pick
     up changes in grub for hugepages and isolcpus
    </p></li><li class="listitem "><p>
     Changes to non-uniform memory access (NUMA) memory, isolcpu, or network
     devices must be followed by a reboot of compute nodes
    </p></li><li class="listitem "><p>
     Run sudo reboot to pick up libvirt change and hugepage/isocpus grub
     changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo reboot</pre></div></li><li class="listitem "><p>
     Use the bash script below to configure nova aggregates, neutron networks,
     a new flavor, etc. And then it will spin up two VMs.
    </p></li></ol></div><p>
   <span class="bold"><strong>VM spin-up instructions</strong></span>
  </p><p>
   Before running the spin up script you need to get a copy of the cirros image
   to your Cloud Lifecycle Manager node. You can manually scp a copy of the cirros
   image to the system. You can copy it locallly with wget like so
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img</pre></div><p>
   Save the following shell script in the home directory and run it. This
   should spin up two VMs, one on each compute node.
  </p><div id="id-1.6.11.5.18.10.4.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Make sure to change all network-specific information in the script to match
    your environment.
   </p></div><div class="verbatim-wrap"><pre class="screen">#!/usr/bin/env bash

source service.osrc

######## register glance image
glance image-create --name='cirros' --container-format=bare --disk-format=qcow2 &lt; ~/cirros-0.3.4-x86_64-disk.img

####### create nova aggregate and flavor for dpdk

MI_NAME=dpdk

nova aggregate-create $MI_NAME nova
nova aggregate-add-host $MI_NAME openstack-cp-comp0001-mgmt
nova aggregate-add-host $MI_NAME openstack-cp-comp0002-mgmt
nova aggregate-set-metadata $MI_NAME pinned=true

nova flavor-create $MI_NAME 6 1024 20 1
nova flavor-key $MI_NAME set hw:cpu_policy=dedicated
nova flavor-key $MI_NAME set aggregate_instance_extra_specs:pinned=true
nova flavor-key $MI_NAME set hw:mem_page_size=1048576

######## sec groups NOTE: no sec groups supported on DPDK.  This is in case we do non-DPDK compute hosts.
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0

########  nova keys
nova keypair-add mykey &gt;mykey.pem
chmod 400 mykey.pem

######## create neutron external network
neutron net-create ext-net --router:external --os-endpoint-type internalURL
neutron subnet-create ext-net 10.231.0.0/19 --gateway_ip=10.231.0.1  --ip-version=4 --disable-dhcp  --allocation-pool start=10.231.17.0,end=10.231.17.255

########  neutron network
neutron net-create mynet1
neutron subnet-create mynet1 10.1.1.0/24 --name mysubnet1
neutron router-create myrouter1
neutron router-interface-add myrouter1 mysubnet1
neutron router-gateway-set myrouter1 ext-net
export MYNET=$(neutron net-list|grep mynet|awk '{print $2}')

######## spin up 2 VMs, 1 on each compute
nova boot --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0001-mgmt vm1
nova boot --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0002-mgmt vm2

######## create floating ip and attach to instance
export MYFIP1=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm1 ${MYFIP1}

export MYFIP2=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm2 ${MYFIP2}

nova list</pre></div></div></div><div class="sect3" id="dpdk-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Configurations</span> <a title="Permalink" class="permalink" href="#dpdk-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>dpdk-config</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#base-config" title="9.3.14.5.1. Base configuration">Section 9.3.14.5.1, “Base configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#common-perf" title="9.3.14.5.2. Performance considerations common to all NIC types">Section 9.3.14.5.2, “Performance considerations common to all NIC types”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#multiqueue-config" title="9.3.14.5.3. Multiqueue configuration">Section 9.3.14.5.3, “Multiqueue configuration”</a>
   </p></li></ul></div><div class="sect4" id="base-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Base configuration</span> <a title="Permalink" class="permalink" href="#base-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>base-config</li></ul></div></div></div></div><p>
   The following is specific to DL360 Gen9 and BIOS configuration as detailed
   in <a class="xref" href="#dpdk-setup" title="9.3.14.4. DPDK Setup for Neutron Networking">Section 9.3.14.4, “DPDK Setup for Neutron Networking”</a>.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     EAL cores - 1, isolate: False in cpu-models
    </p></li><li class="listitem "><p>
     PMD cores - 1 per NIC port
    </p></li><li class="listitem "><p>
     Hugepages - 1G per PMD thread
    </p></li><li class="listitem "><p>
     Memory channels - 4
    </p></li><li class="listitem "><p>
     Global rx queues - based on needs
    </p></li></ul></div></div><div class="sect4" id="common-perf"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance considerations common to all NIC types</span> <a title="Permalink" class="permalink" href="#common-perf">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>common-perf</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Compute host core frequency</strong></span>
  </p><p>
   Host CPUs should be running at maximum performance. The following is a
   script to set that. Note that in this case there are 24 cores. This needs to
   be modified to fit your environment. For a HP DL360 Gen9, the BIOS should be
   configured to use "OS Control Mode" which can be found on the iLO Power
   Settings page.
  </p><div class="verbatim-wrap"><pre class="screen">for i in `seq 0 23`; do echo "performance" &gt; /sys/devices/system/cpu/cpu$i/cpufreq/scaling_governor; done</pre></div><p>
   <span class="bold"><strong>IO non-posted prefetch</strong></span>
  </p><p>
   The DL360 Gen9 should have the IO non-posted prefetch disabled. Experimental
   evidence shows this yields an additional 6-8% performance boost.
  </p></div><div class="sect4" id="multiqueue-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiqueue configuration</span> <a title="Permalink" class="permalink" href="#multiqueue-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>multiqueue-config</li></ul></div></div></div></div><p>
   In order to use multiqueue, a property must be applied to the Glance image
   and a setting inside the resulting VM must be applied. In this example we
   create a 4 vCPU flavor for DPDK using 1G hugepages.
  </p><div class="verbatim-wrap"><pre class="screen">MI_NAME=dpdk

nova aggregate-create $MI_NAME nova
nova aggregate-add-host $MI_NAME openstack-cp-comp0001-mgmt
nova aggregate-add-host $MI_NAME openstack-cp-comp0002-mgmt
nova aggregate-set-metadata $MI_NAME pinned=true

nova flavor-create $MI_NAME 6 1024 20 4
nova flavor-key $MI_NAME set hw:cpu_policy=dedicated
nova flavor-key $MI_NAME set aggregate_instance_extra_specs:pinned=true
nova flavor-key $MI_NAME set hw:mem_page_size=1048576</pre></div><p>
   And set the hw_vif_multiqueue_enabled property on the Glance image
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image set --property hw_vif_multiqueue_enabled=true <em class="replaceable ">IMAGE UUID</em></pre></div><p>
   Once the VM is booted using the flavor above, inside the VM, choose the
   number of combined rx and tx queues to be equal to the number of vCPUs
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ethtool -L eth0 combined 4</pre></div><p>
   On the hypervisor you can verify that multiqueue has been properly set by
   looking at the qemu process
  </p><div class="verbatim-wrap"><pre class="screen">-netdev type=vhost-user,id=hostnet0,chardev=charnet0,queues=4 -device virtio-net-pci,mq=on,vectors=10,</pre></div><p>
   Here you can see that 'mq=on' and vectors=10. The formula for vectors is
   2*num_queues+2
  </p></div></div><div class="sect3" id="dpdk-troubleshooting"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting DPDK</span> <a title="Permalink" class="permalink" href="#dpdk-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>dpdk-troubleshooting</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#hardware" title="9.3.14.6.1. Hardware configuration">Section 9.3.14.6.1, “Hardware configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#system" title="9.3.14.6.2. System configuration">Section 9.3.14.6.2, “System configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#inputModel" title="9.3.14.6.3. Input model configuration">Section 9.3.14.6.3, “Input model configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#reboot" title="9.3.14.6.4. Reboot requirements">Section 9.3.14.6.4, “Reboot requirements”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#software" title="9.3.14.6.5. Software configuration">Section 9.3.14.6.5, “Software configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#runtime" title="9.3.14.6.6. DPDK runtime">Section 9.3.14.6.6, “DPDK runtime”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#errors" title="9.3.14.6.7. Errors">Section 9.3.14.6.7, “Errors”</a>
   </p></li></ul></div><div class="sect4" id="hardware"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware configuration</span> <a title="Permalink" class="permalink" href="#hardware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>hardware</li></ul></div></div></div></div><p>
   Because there are several variations of hardware, it is up to you to verify
   that the hardware is configured properly.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only Intel based compute nodes are supported. There is no DPDK available
     for AMD-based CPUs.
    </p></li><li class="listitem "><p>
     PCI-PT must be enabled for the NIC that will be used with DPDK.
    </p></li><li class="listitem "><p>
     When using Intel Niantic and the igb_uio driver, the VT-d must be enabled
     in the BIOS.
    </p></li><li class="listitem "><p>
     For DL360 Gen9 systems, the BIOS shared-memory
     <a class="xref" href="#pcipt-gen9" title="9.3.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 9.3.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>.
    </p></li><li class="listitem "><p>
     Adequate memory must be available for <a class="xref" href="#hugepages" title="9.3.14.3. Configuring Hugepages for DPDK in Neutron Networks">Section 9.3.14.3, “Configuring Hugepages for DPDK in Neutron Networks”</a> usage.
    </p></li><li class="listitem "><p>
     Hyper-threading can be enabled but is not required for base functionality.
    </p></li><li class="listitem "><p>
     Determine the PCI slot that the DPDK NIC(s) are installed in to
     determine the associated NUMA node.
    </p></li><li class="listitem "><p>
     Only the Intel Haswell, Broadwell, and Skylake microarchitectures are
     supported.
     
     Intel Sandy Bridge is not supported.
    </p></li></ul></div></div><div class="sect4" id="system"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System configuration</span> <a title="Permalink" class="permalink" href="#system">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>system</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only SLES12-SP3 compute nodes are supported.
    </p></li><li class="listitem "><p>
     If a NIC port is used with PCI-PT, SRIOV-only, or PCI-PT+SRIOV, then it
     cannot be used with DPDK. They are mutually exclusive. This is because DPDK
     depends on an OvS bridge which does not exist if you use any combination of
     PCI-PT and SRIOV. You can use DPDK, SRIOV-only, and PCI-PT on difference
     interfaces of the same server.
    </p></li><li class="listitem "><p>
     There is an association between the PCI slot for the NIC and a NUMA node.
     Make sure to use logical CPU cores that are on the NUMA node associated to
     the NIC. Use the following to determine which CPUs are on which NUMA node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lscpu

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz
Stepping:              2
CPU MHz:               1200.000
CPU max MHz:           1800.0000
CPU min MHz:           1200.0000
BogoMIPS:              3597.06
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0-11,24-35
NUMA node1 CPU(s):     12-23,36-47</pre></div></li></ul></div></div><div class="sect4" id="inputModel"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model configuration</span> <a title="Permalink" class="permalink" href="#inputModel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>inputModel</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If you do not specify a driver for a DPDK device, the igb_uio will be
     selected as default.
    </p></li><li class="listitem "><p>
     DPDK devices must be named <code class="literal">dpdk&lt;port-id&gt;</code> where
     the port-id starts at 0 and increments sequentially.
    </p></li><li class="listitem "><p>
     Tenant networks supported are untagged VXLAN and VLAN.
    </p></li><li class="listitem "><p>
     Jumbo Frames MTU does not work with DPDK OvS. There is an upstream patch
     most likely showing up in OvS 2.6 and it cannot be backported due to
     changes this patch relies upon.
    </p></li><li class="listitem "><p>
     Sample VXLAN model
    </p></li><li class="listitem "><p>
     Sample VLAN model
    </p></li></ul></div></div><div class="sect4" id="reboot"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reboot requirements</span> <a title="Permalink" class="permalink" href="#reboot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>reboot</li></ul></div></div></div></div><p>
   A reboot of a compute node must be performed when an input model change
   causes the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After the initial <code class="filename">site.yml</code> play on a new <span class="productname">OpenStack</span>
     environment
    </p></li><li class="listitem "><p>
     Changes to an existing <span class="productname">OpenStack</span> environment that modify the
     <code class="literal">/etc/default/grub</code> file, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       hugepage allocations
      </p></li><li class="listitem "><p>
       CPU isolation
      </p></li><li class="listitem "><p>
       iommu changes
      </p></li></ul></div></li><li class="listitem "><p>
     Changes to a NIC port usage type, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       moving from DPDK to any combination of PCI-PT and SRIOV
      </p></li><li class="listitem "><p>
       moving from DPDK to kernel based eth driver
      </p></li></ul></div></li></ol></div></div><div class="sect4" id="software"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software configuration</span> <a title="Permalink" class="permalink" href="#software">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>software</li></ul></div></div></div></div><p>
   The input model is processed by the Configuration Processor which eventually
   results in changes to the OS. There are several files that should be checked
   to verify the proper settings were applied. In addition, after the inital
   site.yml play is run all compute nodes must be rebooted in order to pickup
   changes to the <code class="filename">/etc/default/grub</code> file for hugepage
   reservation, CPU isolation and iommu settings.
  </p><p>
   <span class="bold"><strong>Kernel settings</strong></span>
  </p><p>
   Check <code class="filename">/etc/default/grub</code> for the following
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     hugepages
    </p></li><li class="listitem "><p>
     CPU isolation
    </p></li><li class="listitem "><p>
     that iommu is in passthru mode if the igb_uio driver is in use
    </p></li></ol></div><p>
   <span class="bold"><strong>Open vSwitch settings</strong></span>
  </p><p>
   Check <code class="filename">/etc/default/openvswitch-switchf</code> for
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     using the <code class="literal">--dpdk</code> option
    </p></li><li class="listitem "><p>
     core 0 set aside for EAL and kernel to share
    </p></li><li class="listitem "><p>
     cores assigned to PMD drivers, at least two for each DPDK device
    </p></li><li class="listitem "><p>
     verify that memory is reserved with socket-mem option
    </p></li><li class="listitem "><p>
     Once
     <a class="link" href="https://jira.hpcloud.net/browse/VNETCORE-2509" target="_blank">VNETCORE-2509</a>
     merges also verify that the umask is 022 and the group is libvirt-qemu
    </p></li></ol></div><p>
   <span class="bold"><strong>DPDK settings</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     check <code class="filename">/etc/dpdk/interfacesf</code> for the correct DPDK devices
    </p></li></ol></div></div><div class="sect4" id="runtime"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK runtime</span> <a title="Permalink" class="permalink" href="#runtime">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>runtime</li></ul></div></div></div></div><p>
   All non-bonded DPDK devices will be added to individual OvS bridges. The
   bridges will be named <code class="literal">br-dpdk0</code>,
   <code class="literal">br-dpdk1</code>, etc. The name of the OvS bridge for bonded DPDK
   devices will be <code class="literal">br-dpdkbond0</code>,
   <code class="literal">br-dpdkbond1</code>, etc.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Since each PMD thread is in a polling loop, it will use 100% of the CPU.
     Thus for two PMDs you would expect to see the ovs-vswitchd process running
     at 200%. This can be verified by running
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>top

top - 16:45:42 up 4 days, 22:24,  1 user,  load average: 2.03, 2.10, 2.14
Tasks: 384 total,   2 running, 382 sleeping,   0 stopped,   0 zombie
%Cpu(s):  9.0 us,  0.2 sy,  0.0 ni, 90.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  13171580+total, 10356851+used, 28147296 free,   257196 buffers
KiB Swap:        0 total,        0 used,        0 free.  1085868 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 1522 root      10 -10 6475196 287780  10192 S 200.4  0.2  14250:20 ovs-vswitchd</pre></div></li><li class="listitem "><p>
     Verify that <code class="literal">ovs-vswitchd</code> is running with
    </p><div class="verbatim-wrap"><pre class="screen">--dpdk option. ps -ef | grep ovs-vswitchd</pre></div></li><li class="listitem "><p>
     PMD thread(s) are started when a DPDK port is added to an OvS bridge.
     Verify the port is on the bridge.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ovs-vsctl show</pre></div></li><li class="listitem "><p>
     A DPDK port cannot be added to an OvS bridge unless it is bound to a
     driver. Verify that the DPDK port is bound.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo dpdk_nic_bind -s</pre></div></li><li class="listitem "><p>
     Verify that the proper number of hugepages is on the correct NUMA node
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo virsh freepages --all</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo grep -R "" /sys/kernel/mm/hugepages/ /proc/sys/vm/*huge*</pre></div></li><li class="listitem "><p>
     Verify that the VM and the DPDK PMD threads have both mapped the same
     hugepage(s)
    </p><div class="verbatim-wrap"><pre class="screen"># this will yield 2 process ids, use the 2nd one
<code class="prompt user">tux &gt; </code>sudo ps -ef | grep ovs-vswitchd
<code class="prompt user">tux &gt; </code>sudo ls -l /proc/<em class="replaceable ">PROCESS-ID</em>/fd | grep huge

# if running more than 1 VM you will need to figure out which one to use
<code class="prompt user">tux &gt; </code>sudo ps -ef | grep qemu
<code class="prompt user">tux &gt; </code>sudo ls -l /proc/<em class="replaceable ">PROCESS-ID</em>/fd | grep huge</pre></div></li></ol></div></div><div class="sect4" id="errors"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Errors</span> <a title="Permalink" class="permalink" href="#errors">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>errors</li></ul></div></div></div></div><p>
   <span class="bold"><strong>VM does not get fixed IP</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     DPDK Poll Mode drivers (PMD) communicates with the VM by direct access of
     the VM hugepage. If a VM is not created using hugepages
     (see <a class="xref" href="#hugepages" title="9.3.14.3. Configuring Hugepages for DPDK in Neutron Networks">Section 9.3.14.3, “Configuring Hugepages for DPDK in Neutron Networks”</a>),
     there is no way for DPDK to communicate with the VM and the VM will never
     be connected to the network.
    </p></li><li class="listitem "><p>
     It has been observed that the DPDK communication with VM fails if the
     shared-memory is not disabled in BIOS for DL360 Gen9.
    </p></li></ol></div><p>
   <span class="bold"><strong>Vestiges of non-existent DPDK devices</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Incorrect input models that do not use the correct DPDK device name or
     do not use sequential port IDs starting at 0 may leave non-existent devices
     in the OvS database. While this does not affect proper functionality it may
     be confusing.
    </p></li></ol></div><p>
   <span class="bold"><strong>Startup issues</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Running the following will help diagnose startup issues with ovs-vswitchd:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo journalctl -u openvswitch.service --all</pre></div></li></ol></div></div></div></div><div class="sect2" id="sr-iov"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV and PCI Passthrough Support</span> <a title="Permalink" class="permalink" href="#sr-iov">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>sr-iov</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports both single-root I/O virtualization (SR-IOV) and PCI
  passthrough (PCIPT). Both technologies provide for better network
  performance.


 </p><p>
  This improves network I/O, decreases latency, and reduces processor overhead.
 </p><div class="sect3" id="id-1.6.11.5.19.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) Ethernet
   interface is a physical PCI Ethernet NIC that implements hardware-based
   virtualization mechanisms to expose multiple virtual network interfaces that
   can be used by one or more virtual machines simultaneously. With SR-IOV
   based NICs, the traditional virtual bridge is no longer required. Each
   SR-IOV port is associated with a virtual function (VF).
  </p><p>
   When compared with a PCI Passthtrough Ethernet interface, an SR-IOV Ethernet
   interface:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Provides benefits similar to those of a PCI Passthtrough Ethernet
     interface, including lower latency packet processing.
    </p></li><li class="listitem "><p>
     Scales up more easily in a virtualized environment by providing multiple
     VFs that can be attached to multiple virtual machine interfaces.
    </p></li><li class="listitem "><p>
     Shares the same limitations, including the lack of support for LAG, QoS,
     ACL, and live migration.
    </p></li><li class="listitem "><p>
     Has the same requirements regarding the VLAN configuration of the access
     switches.
    </p></li></ul></div><p>
   The process for configuring SR-IOV includes creating a VLAN provider network
   and subnet, then attaching VMs to that network.
  </p><p>
   With SR-IOV based NICs, the traditional virtual bridge is no longer
   required. Each SR-IOV port is associated with a virtual function (VF)
  </p></div><div class="sect3" id="id-1.6.11.5.19.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">PCI passthrough Ethernet interfaces</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A passthrough Ethernet interface is a physical PCI Ethernet NIC on a compute
   node to which a virtual machine is granted direct access. PCI passthrough
   allows a VM to have direct access to the hardware without being brokered by
   the hypervisor. This minimizes packet processing delays but at the same time
   demands special operational considerations. For all purposes, a PCI
   passthrough interface behaves as if it were physically attached to the
   virtual machine. Therefore any potential throughput limitations coming from
   the virtualized environment, such as the ones introduced by internal copying
   of data buffers, are eliminated. However, by bypassing the virtualized
   environment, the use of PCI passthrough Ethernet devices introduces several
   restrictions that must be taken into consideration. They include:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     no support for LAG, QoS, ACL, or host interface monitoring
    </p></li><li class="listitem "><p>
     no support for live migration
    </p></li><li class="listitem "><p>
     no access to the compute node's OVS switch
    </p></li></ul></div><p>
   A passthrough interface bypasses the compute node's OVS switch completely,
   and is attached instead directly to the provider network's access switch.
   Therefore, proper routing of traffic to connect the passthrough interface to
   a particular tenant network depends entirely on the VLAN tagging options
   configured on both the passthrough interface and the access port on the
   switch (TOR).
  </p><p>
   The access switch routes incoming traffic based on a VLAN ID, which
   ultimately determines the tenant network to which the traffic belongs. The
   VLAN ID is either explicit, as found in incoming tagged packets, or
   implicit, as defined by the access port's default VLAN ID when the incoming
   packets are untagged. In both cases the access switch must be configured to
   process the proper VLAN ID, which therefore has to be known in advance
  </p></div><div class="sect3" id="pci-passthrough"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Leveraging PCI Passthrough</span> <a title="Permalink" class="permalink" href="#pci-passthrough">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>pci-passthrough</li></ul></div></div></div></div><p>
   Two parts are necessary to leverage PCI passthrough on a SUSE <span class="productname">OpenStack</span> Cloud 8
   Compute Node: preparing the Compute Node, preparing Nova and Glance.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Preparing the Compute Node</strong></span>
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       There should be no kernel drivers or binaries with direct access to the
       PCI device. If there are kernel modules, they should be blacklisted.
      </p><p>
       For example, it is common to have a <code class="literal">nouveau</code> driver
       from when the node was installed. This driver is a graphics driver for
       Nvidia-based GPUs. It must be blacklisted as shown in this example.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/nouveau-default.conf</pre></div><p>
       The file location and its contents are important; the filename is your
       choice. Other drivers can be blacklisted in the same manner, possibly
       including Nvidia drivers.
      </p></li><li class="step "><p>
       On the host, <code class="literal">iommu_groups</code> should be enabled. To check
       if IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments)
.....</pre></div><p>
       To modify the kernel cmdline as suggested in the warning, edit the file
       <code class="filename">/etc/default/grub</code> and append
       <code class="literal">intel_iommu=on</code> to the
       <code class="literal">GRUB_CMDLINE_LINUX_DEFAULT</code> variable. Then run
       <code class="literal">update-bootloader</code>.
      </p><p>
       A reboot will be required for <code class="literal">iommu_groups</code> to be
       enabled.
      </p></li><li class="step "><p>
       After the reboot, check that IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: PASS
.....</pre></div></li><li class="step "><p>
       Confirm IOMMU groups are available by finding the group associated with
       your PCI device (for example Nvidia GPU):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lspci -nn | grep -i nvidia
08:00.0 VGA compatible controller [0300]: NVIDIA Corporation GT218 [NVS 300] [10de:10d8] (rev a2)
08:00.1 Audio device [0403]: NVIDIA Corporation High Definition Audio Controller [10de:0be3] (rev a1)</pre></div><p>
       In this example, <code class="literal">08:00.0</code> and
       <code class="literal">08:00.1</code> are addresses of the PCI device. The vendorID
       is <code class="literal">10de</code>. The productIDs are <code class="literal">10d8</code>
       and <code class="literal">0be3</code>.
      </p></li><li class="step "><p>
       Confirm that the devices are available for passthrough:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -ld /sys/kernel/iommu_groups/*/devices/*08:00.?/
drwxr-xr-x 3 root root 0 Feb 14 13:05 /sys/kernel/iommu_groups/20/devices/0000:08:00.0/
drwxr-xr-x 3 root root 0 Feb 19 16:09 /sys/kernel/iommu_groups/20/devices/0000:08:00.1/</pre></div><div id="id-1.6.11.5.19.6.3.1.2.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        With PCI passthrough, only an entire IOMMU group can be passed. Parts
        of the group cannot be passed. In this example, the IOMMU group is
        <code class="literal">20</code>.
       </p></div></li></ol></div></div></li><li class="listitem "><p>
     <span class="bold"><strong>Preparing Nova and Glance for
     passthrough</strong></span>
    </p><p>
     Information about configuring Nova and Glance is available in the
     documentation at
     <a class="link" href="https://docs.openstack.org/nova/pike/admin/pci-passthrough.html" target="_blank">https://docs.openstack.org/nova/pike/admin/pci-passthrough.html</a>.
     Both <code class="literal">nova-compute</code> and <code class="literal">nova-scheduler</code>
     must be configured.
    </p></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Intel 82599 Devices</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="table" id="intel-82599-table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 9.1: </span><span class="name">Intel 82599 devices supported with SRIOV and PCIPT </span><a title="Permalink" class="permalink" href="#intel-82599-table">#</a></h6></div><div class="table-contents"><table class="table" summary="Intel 82599 devices supported with SRIOV and PCIPT" border="1"><colgroup><col align="center" class="c1" /><col align="center" class="c2" /><col align="center" class="c3" /></colgroup><thead><tr><th align="center">Vendor</th><th align="center">Device</th><th align="center">Title</th></tr></thead><tbody><tr><td align="center">Intel Corporation</td><td align="center">10f8</td><td align="center">82599 10 Gigabit Dual Port Backplane Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10f9</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fb</td><td align="center">82599ES 10-Gigabit SFI/SFP+ Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fc</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr></tbody></table></div></div></div><div class="sect3" id="id-1.6.11.5.19.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SRIOV PCIPT configuration</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you plan to take advantage of SR-IOV support in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> you will need to
   plan in advance to meet the following requirements:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use one of the supported NIC cards:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       HP Ethernet 10Gb 2-port 560FLR-SFP+ Adapter (Intel Niantic). Product
       part number: 665243-B21 -- Same part number for the following card
       options:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         FlexLOM card
        </p></li><li class="listitem "><p>
         PCI slot adapter card
        </p></li></ul></div></li></ul></div></li><li class="listitem "><p>
     Identify the NIC ports to be used for PCI Passthrough devices and SRIOV
     devices from each compute node
    </p></li><li class="listitem "><p>
     Ensure that:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       SRIOV is enabled in the BIOS
      </p></li><li class="listitem "><p>
       HP Shared memory is disabled in the BIOS on the compute nodes.
      </p></li><li class="listitem "><p>
       The Intel boot agent is disabled on the compute
       (<a class="xref" href="#bootutil" title="9.3.15.11. Intel bootutils">Section 9.3.15.11, “Intel bootutils”</a> can be used to perform this)
      </p></li></ul></div><div id="id-1.6.11.5.19.8.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Because of Intel driver limitations, you cannot use a NIC port as an
      SRIOV NIC as well as a physical NIC. Using the physical function to carry
      the normal tenant traffic through the OVS bridge at the same time as
      assigning the VFs from the same NIC device as passthrough to the guest VM
      is not supported.
     </p></div></li></ol></div><p>
   If the above prerequisites are met, then SR-IOV or PCIPT can be reconfigured
   at any time. There is no need to do it at install time.
  </p></div><div class="sect3" id="id-1.6.11.5.19.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment use cases</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are typical use cases that should cover your particular needs:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     A device on the host needs to be enabled for both PCI-passthrough and
     PCI-SRIOV during deployment. At run time Nova decides whether to use
     physical functions or virtual function depending on vnic_type of the port
     used for booting the VM.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-passthrough.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-SRIOV virtual
     functions.
    </p></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model updates</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> provides various options for the user to configure the
   network for tenant VMs. These options have been enhanced to support SRIOV
   and PCIPT.
  </p><p>
   the Cloud Lifecycle Manager input model changes to support SRIOV and PCIPT are as follows. If
   you were familiar with the configuration settings previously, you will
   notice these changes.
  </p><p>
   <span class="bold"><strong>net_interfaces.yml:</strong></span> This file defines the
   interface details of the nodes. In it, the following fields have been added
   under the compute node interface section:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>sriov_only: </td><td>
       <p>
        Indicates that only SR-IOV be enabled on the interface. This should be
        set to true if you want to dedicate the NIC interface to support only
        SR-IOV functionality.
       </p>
      </td></tr><tr><td>pci-pt: </td><td>
       <p>
        When this value is set to true, it indicates that PCIPT should be
        enabled on the interface.
       </p>
      </td></tr><tr><td>vf-count: </td><td>
       <p>
        Indicates the number of VFs to be configured on a given interface.
       </p>
      </td></tr></tbody></table></div><p>
   In control_plane.yml, under Compute resource neutron-sriov-nic-agent has
   been added as service components
  </p><p>
   under resources:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td> Compute</td></tr><tr><td>resource-prefix:</td><td> Comp</td></tr><tr><td>server-role:</td><td>COMPUTE-ROLE</td></tr><tr><td>allocation-policy:</td><td> Any</td></tr><tr><td>min-count:</td><td> 0</td></tr><tr><td>service-components:</td><td>ntp-client</td></tr><tr><td> </td><td>nova-compute</td></tr><tr><td> </td><td>nova-compute-kvm</td></tr><tr><td> </td><td>neutron-l3-agent</td></tr><tr><td> </td><td>neutron-metadata-agent</td></tr><tr><td> </td><td>neutron-openvswitch-agent</td></tr><tr><td> </td><td>neutron-lbaasv2-agent</td></tr><tr><td> </td><td>- neutron-sriov-nic-agent*</td></tr></tbody></table></div><p>
   <span class="bold"><strong>nic_device_data.yml:</strong></span> This is the new file
   added with this release to support SRIOV and PCIPT configuration details. It
   contains information about the specifics of a nic, and is found here:
   <code class="literal">~/openstack/ardana/services/osconfig/nic_device_data.yml</code>.
   The fields in this file are as follows.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>nic-device-types:</strong></span> The nic-device-types
     section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the nic-device-types that will be referenced in
          nic_mappings.yml
         </p>
        </td></tr><tr><td>family:</td><td>
         <p>
          The name of the nic-device-families to be used with this
          nic_device_type
         </p>
        </td></tr><tr><td>device_id:</td><td>
         <p>
          Device ID as specified by the vendor for the particular NIC
         </p>
        </td></tr><tr><td>type:</td><td>
         <p>
          The value of this field can be "simple-port" or "multi-port". If a
          single bus address is assigned to more than one nic it will be
          multi-port, else if there is a one-to one mapping between bus address
          and the nic then it will be simple-port.
         </p>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     <span class="bold"><strong>nic-device-families:</strong></span> The
     nic-device-families section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the device family that can be used for reference in
          nic-device-types.
         </p>
        </td></tr><tr><td>vendor-id: </td><td>
         <p>
          Vendor ID of the NIC
         </p>
        </td></tr><tr><td>config-script:</td><td>
         <p>
          A script file used to create the virtual functions (VF) on the
          Compute node.
         </p>
        </td></tr><tr><td>driver:</td><td>
         <p>
          Indicates the NIC driver that needs to be used.
         </p>
        </td></tr><tr><td>vf-count-type:</td><td>
         <p>
          This value can be either "port" or "driver".
         </p>
        </td></tr><tr><td>“port”:</td><td>
         <p>
          Indicates that the device supports per-port virtual function (VF)
          counts.
         </p>
        </td></tr><tr><td>“driver:”</td><td>
         <p>
          Indicates that all ports using the same driver will be configured
          with the same number of VFs, whether or not the interface model
          specifies a vf-count attribute for the port. If two or more ports
          specify different vf-count values, the config processor errors out.
         </p>
        </td></tr><tr><td>Max-vf-count:</td><td>
         <p>
          This field indicates the maximum VFs that can be configured on an
          interface as defined by the vendor.
         </p>
        </td></tr></tbody></table></div></li></ol></div><p>
   <span class="bold"><strong>control_plane.yml:</strong></span> This file provides the
   information about the services to be run on a particular node. To support
   SR-IOV on a particular compute node, you must run neutron-sriov-nic-agent on
   that node.
  </p><p>
   <span class="bold"><strong>Mapping the use cases with various fields in input
   model</strong></span>
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /><col class="col7" /></colgroup><thead><tr><th> </th><th>Vf-count</th><th>SR-IOV</th><th>PCIPT</th><th>OVS bridge</th><th>Can be NIC bonded</th><th>Use case</th></tr></thead><tbody><tr><td>sriov-only: true</td><td>Mandatory</td><td>Yes</td><td>No</td><td>No</td><td>No</td><td>Dedicated to SRIOV</td></tr><tr><td>pci-pt : true</td><td>Not Specified</td><td>No</td><td>Yes</td><td>No</td><td>No</td><td>Dedicated to PCI-PT</td></tr><tr><td>pci-pt : true</td><td>Specified</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>PCI-PT or SRIOV</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Specified</td><td>Yes</td><td>No</td><td>Yes</td><td>No</td><td>SRIOV with PF used by host</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Not Specified</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td><td>Traditional/Usual use case</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.19.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Mappings between nic_mappings.yml and net_interfaces.yml</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following diagram shows which fields in nic_mappings.yml map to
   corresponding fields in net_interfaces.yml:
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-sriov_pcpit.png" target="_blank"><img src="images/media-sriov_pcpit.png" width="" /></a></div></div></div><div class="sect3" id="id-1.6.11.5.19.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Use Cases for Intel</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Nic-device-types and nic-device-families</strong></span>
     with Intel 82559 with ixgbe as the driver.
    </p><div class="verbatim-wrap"><pre class="screen">nic-device-types:
    - name: ''8086:10fb
      family: INTEL-82599
      device-id: '10fb'
      type: simple-port
nic-device-families:
    # Niantic
    - name: INTEL-82599
      vendor-id: '8086'
      config-script: intel-82599.sh
      driver: ixgbe
      vf-count-type: port
      max-vf-count: 63</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       sriov-only: true
       vf-count: 6
     network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the PCIPT-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       pci-pt: true
    network-groups:
     - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV and
     PCIPT use case
    </p><div class="verbatim-wrap"><pre class="screen"> - name: COMPUTE-INTERFACES
    - name: hed1
      device:
        name: hed1
        pci-pt: true
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for SRIOV and Normal
     Virtio use case
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
        name: hed1
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for PCI-PT
     (<code class="literal">hed1</code> and <code class="literal">hed4</code> refer to the DUAL
     ports of the PCI-PT NIC)
    </p><div class="verbatim-wrap"><pre class="screen">    - name: COMPUTE-PCI-INTERFACES
      network-interfaces:
      - name: hed3
        device:
          name: hed3
        network-groups:
          - MANAGEMENT
          - EXTERNAL-VM
        forced-network-groups:
          - EXTERNAL-API
      - name: hed1
        device:
          name: hed1
          pci-pt: true
        network-groups:
          - GUEST
      - name: hed4
        device:
          name: hed4
          pci-pt: true
        network-groups:
          - GUEST</pre></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Launching Virtual Machines</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Provisioning a VM with SR-IOV NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a Neutron port with <code class="literal">vnic_type = direct</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create $net_id --name sriov_port --binding:vnic_type direct</pre></div></li><li class="step "><p>
     Boot a VM with the created <code class="literal">port-id</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova boot --flavor m1.large --image ubuntu_14.04 --nic port-id=$port_id test-sriov</pre></div></li></ol></div></div><p>
   Provisioning a VM with PCI-PT NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create two Neutron ports with <code class="literal">vnic_type =
     direct-physical</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create net1 --name pci-port1 --vnic_type=direct-physical
neutron port-create net1 --name pci-port2  --vnic_type=direct-physical</pre></div></li><li class="step "><p>
     Boot a VM with the created ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova boot --flavor 4 --image opensuse --nic port-id pci-port1-port-id \
--nic port-id pci-port2-port-id vm1-pci-passthrough</pre></div></li></ol></div></div><p>
   If PCI-PT VM gets stuck (hangs) at boot time when using an Intel NIC, the
   boot agent should be disabled.
  </p></div><div class="sect3" id="bootutil"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Intel bootutils</span> <a title="Permalink" class="permalink" href="#bootutil">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>bootutil</li></ul></div></div></div></div><p>
   When Intel cards are used for PCI-PT, a tenant VM can get stuck at boot
   time. When this happens, you should download Intel bootutils and use it to
   should disable bootagent.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Download Preebot.tar.gz from
     <a class="link" href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers" target="_blank">https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers</a>
    </p></li><li class="listitem "><p>
     Untar the <code class="literal">Preboot.tar.gz</code> on the compute node where the
     PCI-PT VM is to be hosted.
    </p></li><li class="listitem "><p>
     Go to ~/APPS/BootUtil/Linux_x64
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/APPS/BootUtil/Linux_x64</pre></div><p>
     and run following command
    </p><div class="verbatim-wrap"><pre class="screen">./bootutil64e -BOOTENABLE disable -all</pre></div></li><li class="listitem "><p>
     Boot the PCI-PT VM and it should boot without getting stuck.
    </p><div id="id-1.6.11.5.19.14.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Here even though VM console shows VM getting stuck at PXE boot, it is not
      related to BIOS PXE settings.
     </p></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.15"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making input model changes and implementing PCI PT and SR-IOV</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To implent the configuration you require, log into the Cloud Lifecycle Manager node and update
   the Cloud Lifecycle Manager model files to enable SR-IOV or PCIPT following the relevent use
   case explained above. You will need to edit
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     net_interfaces.yml
    </p></li><li class="listitem "><p>
     nic_device_data.yml
    </p></li><li class="listitem "><p>
     control_plane.yml
    </p></li></ul></div><p>
   To make the edits,
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Check out the site branch of the local git repository and change to the
     correct directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div></li><li class="listitem "><p>
     Open each file in vim or another editor and make the necessary changes.
     Save each file, then commit to the local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="listitem "><p>
     Here you will have the Cloud Lifecycle Manager enable your changes by running the necessary
     playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div><div id="id-1.6.11.5.19.15.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    After running the site.yml playbook above, you must reboot the compute
    nodes that are configured with Intel PCI devices.
   </p></div><div id="id-1.6.11.5.19.15.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    When a VM is running on an SRIOV port on a given compute node,
    reconfiguration is not supported.
   </p></div><p>
   You can set the number of virtual functions that must be enabled on a
   compute node at install time. You can update the number of virtual functions
   after deployment. If any VMs have been spawned before you change the number
   of virtual functions, those VMs may lose connectivity. Therefore, it is
   always recommended that if any virtual function is used by any tenant VM,
   you should not reconfigure the virtual functions. Instead, you should
   delete/migrate all the VMs on that NIC before reconfiguring the number of
   virtual functions.
  </p></div><div class="sect3" id="id-1.6.11.5.19.16"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.19.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Security groups are not applicable for PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Live migration is not supported for VMs with PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Rate limiting (QoS) is not applicable on SRIOV and PCI-PT ports.
    </p></li><li class="listitem "><p>
     SRIOV/PCIPT is not supported for VxLAN network.
    </p></li><li class="listitem "><p>
     DVR is not supported with SRIOV/PCIPT.
    </p></li><li class="listitem "><p>
     For Intel cards, the same NIC cannot be used for both SRIOV and normal VM
     boot.
    </p></li><li class="listitem "><p>
     Current upstream OpenStack code does not support this hot plugin of
     SRIOV/PCIPT interface using the nova <code class="literal">attach_interface</code>
     command. See <a class="link" href="https://review.openstack.org/#/c/139910/" target="_blank">https://review.openstack.org/#/c/139910/</a>
     for more information.
    </p></li><li class="listitem "><p>
     Neutron port-update when admin state is down will not work.
    </p></li><li class="listitem "><p>
     SLES Compute Nodes with dual-port PCI-PT NICs, both ports should always be
     passed in the VM. It is not possible to split the dual port and pass
     through just a single port.
    </p></li></ul></div></div><div class="sect3" id="pcipt-gen9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling PCI-PT on HPE DL360 Gen 9 Servers</span> <a title="Permalink" class="permalink" href="#pcipt-gen9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-enabling_pcipt_on_gen9.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-enabling_pcipt_on_gen9.xml</li><li><span class="ds-label">ID: </span>pcipt-gen9</li></ul></div></div></div></div><p>
  The HPE DL360 Gen 9 and HPE ProLiant systems with Intel processors use a
  region of system memory for sideband communication of management
  information. The BIOS sets up Reserved Memory Region Reporting (RMRR) to
  report these memory regions and devices to the operating system. There is a
  conflict between the Linux kernel and RMRR which causes problems with PCI
  pass-through (PCI-PT). This is needed for IOMMU use by DPDK. Note that this
  does not affect SR-IOV.
 </p><p>
  In order to enable PCI-PT on the HPE DL360 Gen 9 you must have a version of
  firmware that supports setting this and you must change a BIOS setting.
 </p><p>
  To begin, get the latest firmware and install it on your compute nodes.
 </p><p>
  Once the firmware has been updated:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Reboot the server and press <span class="keycap">F9</span> (system utilities) during POST (power on
    self test)
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">System Configuration</span>
   </p></li><li class="listitem "><p>
    Select the NIC for which you want to enable PCI-PT
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">Device Level Configuration</span>
   </p></li><li class="listitem "><p>
    Disable the shared memory feature in the BIOS.
   </p></li><li class="listitem "><p>
    Save the changes and reboot server
   </p></li></ol></div></div></div><div class="sect2" id="vlan-aware"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up VLAN-Aware VMs</span> <a title="Permalink" class="permalink" href="#vlan-aware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>vlan-aware</li></ul></div></div></div></div><p>
  Creating a VM with a trunk port will allow a VM to gain connectivity to one
  or more networks over the same virtual NIC (vNIC) through the use VLAN
  interfaces in the guest VM. Connectivity to different networks can be added
  and removed dynamically through the use of subports. The network of the
  parent port will be presented to the VM as the untagged VLAN, and the
  networks of the child ports will be presented to the VM as the tagged VLANs
  (the VIDs of which can be chosen arbitrarily as long as they are unique to
  that trunk). The VM will send/receive VLAN-tagged traffic over the subports,
  and Neutron will mux/demux the traffic onto the subport's corresponding
  network. This is not to be confused with VLAN transparency where a VM can
  pass VLAN-tagged traffic transparently across the network without
  interference from Neutron. VLAN transparency is not supported.
 </p><div class="sect3" id="id-1.6.11.5.20.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Terminology</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.20.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Trunk</strong></span>: a resource that logically
     represents a trunked vNIC and references a parent port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Parent port</strong></span>: a Neutron port that a Trunk
     is referenced to. Its network is presented as the untagged VLAN.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Subport</strong></span>: a resource that logically
     represents a tagged VLAN port on a Trunk. A Subport references a child
     port and consists of the
     &lt;port&gt;,&lt;segmentation-type&gt;,&lt;segmentation-id&gt; tuple.
     Currently only the 'vlan' segmentation type is supported.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Child port</strong></span>: a Neutron port that a Subport
     is referenced to. Its network is presented as a tagged VLAN based upon the
     segmentation-id used when creating/adding a Subport.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy VM</strong></span>: a VM that does not use a trunk
     port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy port</strong></span>: a Neutron port that is not
     used in a Trunk.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>VLAN-aware VM</strong></span>: a VM that uses at least
     one trunk port.
    </p></li></ul></div></div><div class="sect3" id="id-1.6.11.5.20.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trunk CLI reference</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> Command</th><th>Action</th></tr></thead><tbody><tr><td>network trunk create </td><td>Create a trunk.</td></tr><tr><td>network trunk delete </td><td>Delete a given trunk.</td></tr><tr><td>network trunk list </td><td>List all trunks.</td></tr><tr><td>network trunk show </td><td>Show information of a given trunk.</td></tr><tr><td>network trunk set </td><td>Add subports to a given trunk.</td></tr><tr><td>network subport list </td><td>List all subports for a given trunk.</td></tr><tr><td>network trunk unset</td><td>Remove subports from a given trunk.</td></tr><tr><td>network trunk set</td><td>Update trunk properties.</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.20.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling VLAN-aware VM capability</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> to
     add the "trunk" service_plugin:
    </p><div class="verbatim-wrap"><pre class="screen">service_plugins = {{ neutron_service_plugins }},trunk</pre></div></li><li class="listitem "><p>
     Edit <code class="literal">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
     to enable the noop firewall driver:
    </p><div class="verbatim-wrap"><pre class="screen">[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><div id="id-1.6.11.5.20.5.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      This is a manual configuration step because it must be made apparent that
      this step disables Neutron security groups completely. The default
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall_driver is
      <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewall
      Driver</code> which does not implement security groups for trunk
      ports. Optionally, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver may still be used
      (that is, skip this step), which would provide security groups for legacy
      VMs but not for VLAN-aware VMs. However, this mixed environment is not
      recommended. For more information, see <a class="xref" href="#firewall" title="9.3.16.6. Firewall issues">Section 9.3.16.6, “Firewall issues”</a>.
     </p></div></li><li class="listitem "><p>
     Commit the configuration changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enable vlan-aware VMs"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/</pre></div></li><li class="listitem "><p>
     If this is an initial deployment, continue the rest of normal deployment
     process:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li><li class="listitem "><p>
     If the cloud has already been deployed and this is a reconfiguration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.20.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Cases</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.20.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Creating a trunk port</strong></span>
  </p><p>
   Assume that a number of Neutron networks/subnets already exist: private,
   foo-net, and bar-net. This will create a trunk with two subports allocated
   to it. The parent port will be on the "private" network, while the two child
   ports will be on "foo-net" and "bar-net", respectively:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create a port that will function as the trunk's parent port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create --name trunkparent private</pre></div></li><li class="listitem "><p>
     Create ports that will function as the child ports to be used in subports:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create --name subport1 foo-net
<code class="prompt user">ardana &gt; </code>neutron port-create --name subport2 bar-net</pre></div></li><li class="listitem "><p>
     Create a trunk port using the <code class="literal">openstack network trunk
     create</code> command, passing the parent port created in step 1 and
     child ports created in step 2:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent --subport port=subport1,segmentation-type=vlan,segmentation-id=1 --subport port=subport2,segmentation-type=vlan,segmentation-id=2 mytrunk
+-----------------+-----------------------------------------------------------------------------------------------+
| Field           | Value                                                                                         |
+-----------------+-----------------------------------------------------------------------------------------------+
| admin_state_up  | UP                                                                                            |
| created_at      | 2017-06-02T21:49:59Z                                                                          |
| description     |                                                                                               |
| id              | bd822ebd-33d5-423e-8731-dfe16dcebac2                                                          |
| name            | mytrunk                                                                                       |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358                                                          |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| revision_number | 1                                                                                             |
| status          | DOWN                                                                                          |
| sub_ports       | port_id='9d25abcf-d8a4-4272-9436-75735d2d39dc', segmentation_id='1', segmentation_type='vlan' |
|                 | port_id='e3c38cb2-0567-4501-9602-c7a78300461e', segmentation_id='2', segmentation_type='vlan' |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| updated_at      | 2017-06-02T21:49:59Z                                                                          |
+-----------------+-----------------------------------------------------------------------------------------------+

$ openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div><p>
     Optionally, a trunk may be created without subports (they can be added
     later):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent mytrunk
+-----------------+--------------------------------------+
| Field           | Value                                |
+-----------------+--------------------------------------+
| admin_state_up  | UP                                   |
| created_at      | 2017-06-02T21:45:35Z                 |
| description     |                                      |
| id              | eb8a3c7d-9f0a-42db-b26a-ca15c2b38e6e |
| name            | mytrunk                              |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358 |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9     |
| revision_number | 1                                    |
| status          | DOWN                                 |
| sub_ports       |                                      |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9     |
| updated_at      | 2017-06-02T21:45:35Z                 |
+-----------------+--------------------------------------+</pre></div><p>
     A port that is already bound (that is, already in use by a VM) cannot be
     upgraded to a trunk port. The port must be unbound to be eligible for
     use as a trunk's parent port. When adding subports to a trunk, the child
     ports must be unbound as well.
    </p></li></ol></div><p>
   <span class="bold"><strong>Checking a port's trunk details</strong></span>
  </p><p>
   Once a trunk has been created, its parent port will show the
   <code class="literal">trunk_details</code> attribute, which consists of the
   <code class="literal">trunk_id</code> and list of subport dictionaries:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-show -F trunk_details trunkparent
+---------------+-------------------------------------------------------------------------------------+
| Field         | Value                                                                               |
+---------------+-------------------------------------------------------------------------------------+
| trunk_details | {"trunk_id": "bd822ebd-33d5-423e-8731-dfe16dcebac2", "sub_ports":                   |
|               | [{"segmentation_id": 2, "port_id": "e3c38cb2-0567-4501-9602-c7a78300461e",          |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:11:90:d2"},                   |
|               | {"segmentation_id": 1, "port_id": "9d25abcf-d8a4-4272-9436-75735d2d39dc",           |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:ff:de:73"}]}                  |
+---------------+-------------------------------------------------------------------------------------+</pre></div><p>
   Ports that are not trunk parent ports will not have a
   <code class="literal">trunk_details</code> field:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-show -F trunk_details subport1
need more than 0 values to unpack</pre></div><p>
   <span class="bold"><strong>Adding subports to a trunk</strong></span>
  </p><p>
   Assuming a trunk and new child port have been created already, the
   <code class="literal">trunk-subport-add</code> command will add one or more subports
   to the trunk.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Run <code class="literal">openstack network trunk set</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk set --subport port=subport3,segmentation-type=vlan,segmentation-id=3 mytrunk</pre></div></li><li class="listitem "><p>
     Run <code class="literal">openstack network subport list</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
| bf958742-dbf9-467f-b889-9f8f2d6414ad | vlan              |               3 |
+--------------------------------------+-------------------+-----------------+</pre></div></li></ol></div><div id="id-1.6.11.5.20.6.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    The <code class="literal">--subport</code> option may be repeated multiple times in
    order to add multiple subports at a time.
   </p></div><p>
   <span class="bold"><strong>Removing subports from a trunk</strong></span>
  </p><p>
   To remove a subport from a trunk, use <code class="literal">openstack network trunk
   unset</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk unset --subport subport3 mytrunk</pre></div><p>
   <span class="bold"><strong>Deleting a trunk port</strong></span>
  </p><p>
   To delete a trunk port, use the <code class="literal">openstack network trunk
   delete</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk delete mytrunk</pre></div><p>
   Once a trunk has been created successfully, its parent port may be passed to
   the <code class="literal">nova boot</code> command, which will make the VM VLAN-aware:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova boot --image ubuntu-server --flavor 1 --nic port-id=239f8807-be2e-4732-9de6-c64519f46358 vlan-aware-vm</pre></div><div id="id-1.6.11.5.20.6.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    A trunk cannot be deleted until its parent port is unbound. Mainly, this
    means you must delete the VM using the trunk port before you are allowed to
    delete the trunk.
   </p></div></div><div class="sect3" id="id-1.6.11.5.20.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VLAN-aware VM network configuration</span> <a title="Permalink" class="permalink" href="#id-1.6.11.5.20.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This section illustrates how to configure the VLAN interfaces inside a
   VLAN-aware VM based upon the subports allocated to the trunk port being
   used.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Run <code class="literal">openstack network trunk subport list</code> to see the
     VLAN IDs in use on the trunk port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div></li><li class="listitem "><p>
     Run <code class="literal">neutron port-show</code> on the child port to get its
     mac_address:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-show -F mac_address 08848e38-50e6-4d22-900c-b21b07886fb7
+-------------+-------------------+
| Field       | Value             |
+-------------+-------------------+
| mac_address | fa:16:3e:08:24:61 |
+-------------+-------------------+</pre></div></li><li class="listitem "><p>
     Log into the VLAN-aware VM and run the following commands to set up the
     VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2
$ sudo ip link set dev ens3.2 up</pre></div></li><li class="listitem "><p>
     Note the usage of the mac_address from step 2 and VLAN ID from step 1 in
     configuring the VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2</pre></div></li><li class="listitem "><p>
     Trigger a DHCP request for the new vlan interface to verify connectivity
     and retrieve its IP address. On an Ubuntu VM, this might be:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo dhclient ens3.2
<code class="prompt user">tux &gt; </code>sudo ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:8d:77:39 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.5/24 brd 10.10.10.255 scope global ens3
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe8d:7739/64 scope link
       valid_lft forever preferred_lft forever
3: ens3.2@ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:11:90:d2 brd ff:ff:ff:ff:ff:ff
    inet 10.10.12.7/24 brd 10.10.12.255 scope global ens3.2
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe11:90d2/64 scope link
       valid_lft forever preferred_lft forever</pre></div></li></ol></div></div><div class="sect3" id="firewall"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall issues</span> <a title="Permalink" class="permalink" href="#firewall">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>firewall</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver is
   <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</code>.
   This default does not implement security groups for VLAN-aware VMs, but it
   does implement security groups for legacy VMs. For this reason, it is
   recommended to disable Neutron security groups altogether when using
   VLAN-aware VMs. To do so, set:
  </p><div class="verbatim-wrap"><pre class="screen">firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><p>
   Doing this will prevent having a mix of firewalled and non-firewalled VMs in
   the same environment, but it should be done with caution because all VMs
   would be non-firewalled.
  </p></div></div></div></div><div class="chapter " id="ops-managing-dashboards"><div class="titlepage"><div><div><h1 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing the Dashboard</span> <a title="Permalink" class="permalink" href="#ops-managing-dashboards">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_dashboards.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_dashboards.xml</li><li><span class="ds-label">ID: </span>ops-managing-dashboards</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic1564-1"><span class="number">10.1 </span><span class="name">Configuring the Dashboard Service</span></a></span></dt><dt><span class="section"><a href="#horizonTimeout"><span class="number">10.2 </span><span class="name">Changing the Dashboard Timeout Value</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Dashboard service.
 </p><div class="sect1" id="topic1564-1"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Dashboard Service</span> <a title="Permalink" class="permalink" href="#topic1564-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-configure_dashboard.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-configure_dashboard.xml</li><li><span class="ds-label">ID: </span>topic1564-1</li></ul></div></div></div></div><p>
  Horizon is the OpenStack service that serves as the basis for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  dashboards.
 </p><p>
  The dashboards provide a web-based user interface to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services
  including Compute, Volume Operations, Networking, and Identity.
 </p><p>
  Along the left side of the dashboard are sections that provide access to
  Project and Identity sections. If your login credentials have been assigned
  the 'admin' role you will also see a separate Admin section that provides
  additional system-wide setting options.
 </p><p>
  Across the top are menus to switch between projects and menus where you can
  access user settings.
 </p><div class="sect2" id="HorizonTLS"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Dashboard Service and TLS in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#HorizonTLS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-configure_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-configure_dashboard.xml</li><li><span class="ds-label">ID: </span>HorizonTLS</li></ul></div></div></div></div><p>
   By default, the Dashboard service is configured with TLS in the input model
   (ardana-input-model). You should not disable TLS in the input model for the
   Dashboard service. The normal use case for users is to have all services
   behind TLS, but users are given the freedom in the input model to take a
   service off TLS for troubleshooting or debugging. TLS should always be
   enabled for production environments.
  </p><p>
   Make sure that <code class="literal">horizon_public_protocol</code> and
   <code class="literal">horizon_private_protocol</code> are both be set to use https.
  </p></div></div><div class="sect1" id="horizonTimeout"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the Dashboard Timeout Value</span> <a title="Permalink" class="permalink" href="#horizonTimeout">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-change_horizon_timeout.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-change_horizon_timeout.xml</li><li><span class="ds-label">ID: </span>horizonTimeout</li></ul></div></div></div></div><p>
  The default session timeout for the dashboard is 1800 seconds or 30 minutes.
  This is the recommended default and best practice for those concerned with
  security.
 </p><p>
  As an administrator, you can change the session timeout by changing the
  value of the SESSION_TIMEOUT to anything less than or equal to 14400, which
  is equal to four hours. Values greater than 14400 should not be used due to
  Keystone constraints.
 </p><div id="id-1.6.12.4.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   Increasing the value of SESSION_TIMEOUT increases the risk of abuse.
  </p></div><div class="sect2" id="Steps"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to Change the Dashboard Timeout Value</span> <a title="Permalink" class="permalink" href="#Steps">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configuring-change_horizon_timeout.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-change_horizon_timeout.xml</li><li><span class="ds-label">ID: </span>Steps</li></ul></div></div></div></div><p>
   Follow these steps to change and commit the Horizon timeout value.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the Dashboard config file at
     <code class="literal">~/openstack/my_cloud/config/horizon/local_settings.py</code> and,
     if it is not already present, add a line for
     <code class="literal">SESSION_TIMEOUT</code> above the line for
     <code class="literal">SESSION_ENGINE</code>.
    </p><p>
     Here is an example snippet, in bold:
    </p><div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>SESSION_TIMEOUT = &lt;timeout value&gt;</strong></span>
SESSION_ENGINE = 'django.contrib.sessions.backends.db'</pre></div><div id="id-1.6.12.4.5.3.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      Do not exceed the maximum value of 14400.
     </p></div></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -a -m "changed Horizon timeout value"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Dashboard reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="ops-managing-orchestration"><div class="titlepage"><div><div><h1 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Orchestration</span> <a title="Permalink" class="permalink" href="#ops-managing-orchestration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_orchestration.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_orchestration.xml</li><li><span class="ds-label">ID: </span>ops-managing-orchestration</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#configure-heat"><span class="number">11.1 </span><span class="name">Configuring the Orchestration Service</span></a></span></dt><dt><span class="section"><a href="#topic-sqg-cvb-dx"><span class="number">11.2 </span><span class="name">Autoscaling using the Orchestration Service</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Orchestration service, based
  on OpenStack Heat.
 </p><div class="sect1" id="configure-heat"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Orchestration Service</span> <a title="Permalink" class="permalink" href="#configure-heat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-orchestration-configure_orchestration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-configure_orchestration.xml</li><li><span class="ds-label">ID: </span>configure-heat</li></ul></div></div></div></div><p>
  Information about configuring the Orchestration service, based on OpenStack
  Heat.
 </p><p>
  The Orchestration service, based on OpenStack Heat, does not need any
  additional configuration to be used. This documenent describes some
  configuration options as well as reasons you may want to use them.
 </p><p>
  <span class="bold"><strong>Heat Stack Tag Feature</strong></span>
 </p><p>
  Heat provides a feature called Stack Tags to allow attributing a set of
  simple string-based tags to stacks and optionally the ability to hide stacks
  with certain tags by default. This feature can be used for behind-the-scenes
  orchestration of cloud infrastructure, without exposing the cloud user to the
  resulting automatically-created stacks.
 </p><p>
  Additional details can be seen here:
  <a class="link" href="https://specs.openstack.org/openstack/heat-specs/specs/kilo/stack-tags.html" target="_blank">OpenStack
  - Stack Tags</a>.
 </p><p>
  In order to use the Heat stack tag feature, you need to use the following
  steps to define the <code class="literal">hidden_stack_tags</code> setting in the Heat
  configuration file and then reconfigure the service to enable the feature.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Edit the Heat configuration file, at this location:
   </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/heat/heat.conf.j2</pre></div></li><li class="listitem "><p>
    Under the <code class="literal">[DEFAULT]</code> section, add a line for
    <code class="literal">hidden_stack_tags</code>. Example:
   </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
hidden_stack_tags="&lt;hidden_tag&gt;"</pre></div></li><li class="listitem "><p>
    Commit the changes to your local git:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add --all
<code class="prompt user">ardana &gt; </code>git commit -m "enabling Heat Stack Tag feature"</pre></div></li><li class="listitem "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Reconfigure the Orchestration service:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li></ol></div><p>
  To begin using the feature, use these steps to create a Heat stack using the
  defined hidden tag. You will need to use credentials that have the Heat admin
  permissions. In the example steps below we are going to do this from the
  Cloud Lifecycle Manager using the <code class="literal">admin</code> credentials and a Heat
  template named <code class="literal">heat.yaml</code>:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Source the admin credentials:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
    Create a Heat stack using this feature:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack create -f heat.yaml hidden_stack_tags --tags hidden</pre></div></li><li class="listitem "><p>
    If you list your Heat stacks, your hidden one will not show unless you use
    the <code class="literal">--hidden</code> switch.
   </p><p>
    Example, not showing hidden stacks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack list</pre></div><p>
    Example, showing the hidden stacks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack list --hidden</pre></div></li></ol></div></div><div class="sect1" id="topic-sqg-cvb-dx"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Autoscaling using the Orchestration Service</span> <a title="Permalink" class="permalink" href="#topic-sqg-cvb-dx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span>topic-sqg-cvb-dx</li></ul></div></div></div></div><p>
  Autoscaling is a process that can be used to scale up and down your compute
  resources based on the load they are currently experiencing to ensure a
  balanced load.
 </p><div class="sect2" id="idg-all-operations-orchestration-autoscaling-xml-4"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is autoscaling?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-orchestration-autoscaling-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-orchestration-autoscaling-xml-4</li></ul></div></div></div></div><p>
   Autoscaling is a process that can be used to scale up and down your compute
   resources based on the load they are currently experiencing to ensure a
   balanced load across your compute environment.
  </p><div id="id-1.6.13.4.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Autoscaling is only supported for KVM.
   </p></div></div><div class="sect2" id="use"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How does autoscaling work?</span> <a title="Permalink" class="permalink" href="#use">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span>use</li></ul></div></div></div></div><p>
   The monitoring service, Monasca, monitors your infrastructure resources and
   generates alarms based on their state. The orchestration service, Heat,
   talks to the Monasca API and offers the capability to templatize the
   existing Monasca resources, which are the Monasca Notification and Monasca
   Alarm definition. Heat can configure certain alarms for the infrastructure
   resources (compute instances and block storage volumes) it creates and can
   expect Monasca to notify continuously if a certain evaluation pattern in an
   alarm definition is met.
  </p><p>
   For example, Heat can tell Monasca that it needs an alarm generated if the
   average CPU utilization of the compute instance in a scaling group goes
   beyond 90%.
  </p><p>
   As Monasca continuously monitors all the resources in the cloud, if it
   happens to see a compute instance spiking above 90% load as configured by
   Heat, it generates an alarm and in turn sends a notification to Heat. Once
   Heat is notified, it will execute an action that was preconfigured in the
   template. Commonly, this action will be a scale up to increase the number of
   compute instances to balance the load that is being taken by the compute
   instance scaling group.
  </p><p>
   Monasca sends a notification every 60 seconds while the alarm is in the
   ALARM state.
  </p></div><div class="sect2" id="id-1.6.13.4.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Autoscaling template example</span> <a title="Permalink" class="permalink" href="#id-1.6.13.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following Monasca alarm definition template snippet is an example of
   instructing Monasca to generate an alarm if the average CPU utilization in a
   group of compute instances exceeds beyond 50%. If the alarm is triggered, it
   will invoke the <code class="literal">up_notification</code> webhook once the alarm
   evaluation expression is satisfied.
  </p><div class="verbatim-wrap"><pre class="screen">cpu_alarm_high:
  type: OS::Monasca::AlarmDefinition
  properties:
    name: CPU utilization beyond 50 percent
    description: CPU utilization reached beyond 50 percent
    expression:
    str_replace:
    template: avg(cpu.utilization_perc{scale_group=scale_group_id}) &gt; 50 times 3
    params:
    scale_group_id: {get_param: "OS::stack_id"}
    severity: high
    alarm_actions:
      - {get_resource: up_notification }</pre></div><p>
   The following Monasca notification template snippet is an example of
   creating a Monasca notification resource that will be used by the alarm
   definition snippet to notify Heat.
  </p><div class="verbatim-wrap"><pre class="screen">up_notification:
  type: OS::Monasca::Notification
  properties:
    type: webhook
    address: {get_attr: [scale_up_policy, alarm_url]}</pre></div></div><div class="sect2" id="id-1.6.13.4.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Agent configuration options</span> <a title="Permalink" class="permalink" href="#id-1.6.13.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There is a Monasca Agent configuration option which controls the behavior
   around compute instance creation and the measurements being received from
   the compute instance.
  </p><p>
   The variable is <code class="literal">monasca_libvirt_vm_probation</code> which is set
   in the
   <code class="literal">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml</code>
   file. Here is a snippet of the file showing the description and variable:
  </p><div class="verbatim-wrap"><pre class="screen"># The period of time (in seconds) in which to suspend metrics from a
# newly-created VM. This is used to prevent creating and storing
# quickly-obsolete metrics in an environment with a high amount of instance
# churn (VMs created and destroyed in rapid succession).  Setting to 0
# disables VM probation and metrics will be recorded as soon as possible
# after a VM is created.  Decreasing this value in an environment with a high
# amount of instance churn can have a large effect on the total number of
# metrics collected and increase the amount of CPU, disk space and network
# bandwidth required for Monasca. This value may need to be decreased if
# Heat Autoscaling is in use so that Heat knows that a new VM has been
# created and is handling some of the load.
monasca_libvirt_vm_probation: 300</pre></div><p>
   The default value is <code class="literal">300</code>. This is the time in seconds
   that a compute instance must live before the Monasca libvirt agent plugin
   will send measurements for it. This is so that the Monasca metrics database
   does not fill with measurements from short lived compute instances. However,
   this means that the Monasca threshold engine will not see measurements from
   a newly created compute instance for at least five minutes on scale up. If
   the newly created compute instance is able to start handling the load in
   less than five minutes, then Heat autoscaling may mistakenly create another
   compute instance since the alarm does not clear.
  </p><p>
   If the default <code class="literal">monasca_libvirt_vm_probation</code> turns out to
   be an issue, it can be lowered. However, that will affect all compute
   instances, not just ones used by Heat autoscaling which can increase the
   number of measurements stored in Monasca if there are many short lived
   compute instances. You should consider how often compute instances are
   created that live less than the new value of
   <code class="literal">monasca_libvirt_vm_probation</code>. If few, if any, compute
   instances live less than the value of
   <code class="literal">monasca_libvirt_vm_probation</code>, then this value can be
   decreased without causing issues. If many compute instances live less than
   the <code class="literal">monasca_libvirt_vm_probation</code> period, then decreasing
   <code class="literal">monasca_libvirt_vm_probation</code> can cause excessive disk,
   CPU and memory usage by Monasca.
  </p><p>
   If you wish to change this value, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">monasca_libvirt_vm_probation</code> value in this
     configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml</pre></div></li><li class="listitem "><p>
     Commit your changes to the local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add --all
<code class="prompt user">ardana &gt; </code>git commit -m "changing Monasca Agent configuration option"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run this playbook to reconfigure the Nova service and enact your changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="topic-ttn-5fg-4v"><div class="titlepage"><div><div><h1 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Monitoring, Logging, and Usage Reporting</span> <a title="Permalink" class="permalink" href="#topic-ttn-5fg-4v">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_telemetry.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_telemetry.xml</li><li><span class="ds-label">ID: </span>topic-ttn-5fg-4v</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#mon"><span class="number">12.1 </span><span class="name">Monitoring</span></a></span></dt><dt><span class="section"><a href="#centralized-logging"><span class="number">12.2 </span><span class="name">Centralized Logging Service</span></a></span></dt><dt><span class="section"><a href="#ceilo-metering-overview"><span class="number">12.3 </span><span class="name">Metering Service (Ceilometer) Overview</span></a></span></dt></dl></div></div><p>
  Information about the monitoring, logging, and metering services included
  with your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect1" id="mon"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring</span> <a title="Permalink" class="permalink" href="#mon">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring.xml</li><li><span class="ds-label">ID: </span>mon</li></ul></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring service leverages OpenStack Monasca, which is a
  multi-tenant, scalable, fault tolerant monitoring service.
 </p><div class="sect2" id="monitoring-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started with Monitoring</span> <a title="Permalink" class="permalink" href="#monitoring-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring_service.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring_service.xml</li><li><span class="ds-label">ID: </span>monitoring-service</li></ul></div></div></div></div><p>
  You can use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring service to monitor the health of your
  cloud and, if necessary, to troubleshoot issues.
 </p><p>
  Monasca data can be extracted and used for a variety of legitimate purposes,
  and different purposes require different forms of data sanitization or
  encoding to protect against invalid or malicious data. Any data pulled from
  Monasca should be considered untrusted data, so users are advised to apply
  appropriate encoding and/or sanitization techniques to ensure safe and
  correct usage and display of data in a web browser, database scan, or any
  other use of the data.
 </p><div class="sect3" id="idg-all-operations-monitoring-monitoring-overview-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Service Overview</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-1</li></ul></div></div></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-2"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-2</li></ul></div></div></div></div><p>
   The monitoring service is automatically installed as part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation.
  </p><p>
   No specific configuration is required to use Monasca. However, you can
   configure the database for storing metrics as explained in
   <a class="xref" href="#configure-monitoring" title="12.1.2. Configuring the Monitoring Service">Section 12.1.2, “Configuring the Monitoring Service”</a>.
  </p></div><div class="sect4" id="differences"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Differences Between Upstream and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Implementations</span> <a title="Permalink" class="permalink" href="#differences">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>differences</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the OpenStack monitoring service, Monasca,
   is included as the monitoring solution, except for the following which are
   not included:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Transform Engine
    </p></li><li class="listitem "><p>
     Events Engine
    </p></li><li class="listitem "><p>
     Anomaly and Prediction Engine
    </p></li></ul></div><div id="id-1.6.14.3.3.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Icinga was supported in previous <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> versions but it has been
    deprecated in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
   </p></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-4"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Diagram of Monasca Service</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-4</li></ul></div></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-monasca_diagram.png" target="_blank"><img src="images/media-monasca_diagram.png" width="" /></a></div></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-5"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-5</li></ul></div></div></div></div><p>
   For more details on OpenStack Monasca, see
   <a class="link" href="http://monasca.io/" target="_blank">Monasca.io</a>
  </p></div><div class="sect4" id="id-1.6.14.3.3.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Back-end Database</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.3.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The monitoring service default metrics database is Cassandra, which is a
   highly-scalable analytics database and the recommended database for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   You can learn more about Cassandra at <a class="link" href="http://cassandra.apache.org/" target="_blank">Apache Cassandra</a>.
  </p></div></div><div class="sect3" id="working-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Working with Monasca</span> <a title="Permalink" class="permalink" href="#working-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-working_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-working_monitoring.xml</li><li><span class="ds-label">ID: </span>working-monitoring</li></ul></div></div></div></div><p>
  <span class="bold"><strong>Monasca-Agent</strong></span>
 </p><p>
  The <span class="bold"><strong>monasca-agent</strong></span> is a Python program that
  runs on the control plane nodes. It runs the defined checks and then sends
  data onto the API. The checks that the agent runs include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    System Metrics: CPU utilization, memory usage, disk I/O, network I/O, and
    filesystem utilization on the control plane and resource nodes.
   </p></li><li class="listitem "><p>
    Service Metrics: the agent supports plugins such as MySQL, RabbitMQ, Kafka,
    and many others.
   </p></li><li class="listitem "><p>
    VM Metrics: CPU utilization, disk I/O, network I/O, and memory usage of
    hosted virtual machines on compute nodes. Full details of these can be
    found
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#per-instance-metrics" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#per-instance-metrics</a>.
   </p></li></ul></div><p>
  For a full list of packaged plugins that are included <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, see
  <a class="link" href="https://github.com/stackforge/monasca-agent/blob/master/docs/Plugins.md" target="_blank">Monasca
  Plugins</a>
 </p><p>
  You can further customize the monasca-agent to suit your needs, see
  <a class="link" href="https://github.com/stackforge/monasca-agent/blob/master/docs/Customizations.md" target="_blank">Customizing
  the Agent</a>
 </p></div><div class="sect3" id="accessing-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing the Monitoring Service</span> <a title="Permalink" class="permalink" href="#accessing-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>accessing-monitoring</li></ul></div></div></div></div><p>
  Access to the Monitoring service is available through a number of different
  interfaces.
 </p><div class="sect4" id="id-1.6.14.3.3.6.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Command-Line Interface</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.3.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For users who prefer using the command line, there is the
   python-monascaclient, which is part of the default installation on your
   Cloud Lifecycle Manager node.
  </p><p>
   For details on the CLI, including installation instructions, see
   <a class="link" href="https://github.com/stackforge/python-monascaclient/blob/master/README.rst" target="_blank">Python-Monasca
   Client</a>
  </p><p>
   <span class="bold"><strong>Monasca API</strong></span>
  </p><p>
   If low-level access is desired, there is the Monasca REST API.
  </p><p>
   Full details of the Monasca API can be found
   <a class="link" href="https://github.com/stackforge/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">on
   GitHub</a>.
  </p></div><div class="sect4" id="idg-all-operations-monitoring-accessing-monitoring-xml-2"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console GUI</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-accessing-monitoring-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-accessing-monitoring-xml-2</li></ul></div></div></div></div><p>
   You can use the Operations Console (Ops Console) for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to view
   data about your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud infrastructure in a web-based graphical user
   interface (GUI) and ensure your cloud is operating correctly. By logging on
   to the console, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> administrators can manage data in the following
   ways: <span class="bold"><strong>Triage alarm notifications.</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Alarm Definitions and notifications now have their own screens and are
     collected under the <span class="bold"><strong>Alarm Explorer</strong></span> menu
     item which can be accessed from the Central Dashboard. Central Dashboard
     now allows you to customize the view in the following ways:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Rename or re-configure existing alarm cards to include services
       different from the defaults
      </p></li><li class="listitem "><p>
       Create a new alarm card with the services you want to select
      </p></li><li class="listitem "><p>
       Reorder alarm cards using drag and drop
      </p></li><li class="listitem "><p>
       View all alarms that have no service dimension now grouped in an
       <span class="bold"><strong>Uncategorized Alarms</strong></span> card
      </p></li><li class="listitem "><p>
       View all alarms that have a service dimension that does not match any of
       the other cards -now grouped in an <span class="bold"><strong>Other
       Alarms</strong></span> card
      </p></li></ul></div></li><li class="listitem "><p>
     You can also easily access alarm data for a specific component. On the
     Summary page for the following components, a link is provided to an alarms
     screen specifically for that component:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Compute Instances: <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.3 “Managing Compute Hosts”</span>
      </p></li><li class="listitem "><p>
       Object Storage: <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.4 “Managing Swift Performance”, Section 1.4.4 “Alarm Summary”</span>
      </p></li></ul></div></li></ul></div></div><div class="sect4" id="idg-all-operations-monitoring-accessing-monitoring-xml-3"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Connecting to the Operations Console</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-accessing-monitoring-xml-3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-accessing-monitoring-xml-3</li></ul></div></div></div></div><p>
   To connect to Operations Console, perform the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Ensure your login has the required access credentials:
     <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.2 “Connecting to the Operations Console”, Section 1.2.1 “Required Access Credentials”</span>
    </p></li><li class="listitem "><p>
     Connect through a browser:
     <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.2 “Connecting to the Operations Console”, Section 1.2.2 “Connect Through a Browser”</span>
    </p></li><li class="listitem "><p>
     Optionally use a Host name OR virtual IP address to access Operations Console:
     <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.2 “Connecting to the Operations Console”, Section 1.2.3 “Optionally use a Hostname OR virtual IP address to access Operations Console”</span>
    </p></li></ul></div><p>
   Operations Console will always be accessed over port 9095.
  </p></div><div class="sect4" id="id-1.6.14.3.3.6.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.1.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.3.6.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For more details about the Operations Console, see
   <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span>.
  </p></div></div><div class="sect3" id="alarms-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service Alarm Definitions</span> <a title="Permalink" class="permalink" href="#alarms-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-alarms_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-alarms_monitoring.xml</li><li><span class="ds-label">ID: </span>alarms-monitoring</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with some predefined monitoring alarms for the services
  installed.
 </p><p>
  Full details of all service alarms can be found here:
  <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a>.
 </p><p>
  Each alarm will have one of the following statuses:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="guimenu ">Critical</span> - Open alarms, identified by red indicator.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Warning</span> - Open alarms, identified by yellow indicator.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Unknown</span> - Open alarms, identified by gray indicator.
    Unknown will be the status of an alarm that has stopped receiving a metric.
    This can be caused by the following conditions:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      An alarm exists for a service or component that is not installed in the
      environment.
     </p></li><li class="listitem "><p>
      An alarm exists for a virtual machine or node that previously existed but
      has been removed without the corresponding alarms being removed.
     </p></li><li class="listitem "><p>
      There is a gap between the last reported metric and the next metric.
     </p></li></ul></div></li><li class="listitem "><p>
    <span class="guimenu ">Open</span> - Complete list of open alarms.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Total</span> - Complete list of alarms, may include
    Acknowledged and Resolved alarms.
   </p></li></ul></div><p>
  When alarms are triggered it is helpful to review the service logs.
 </p></div></div><div class="sect2" id="configure-monitoring"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Monitoring Service</span> <a title="Permalink" class="permalink" href="#configure-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-configure_monitoring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-configure_monitoring.xml</li><li><span class="ds-label">ID: </span>configure-monitoring</li></ul></div></div></div></div><p>
  The monitoring service, based on Monasca, allows you to configure an external
  SMTP server for email notifications when alarms trigger. You also have
  options for your alarm metrics database should you choose not to use the
  default option provided with the product.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> you have the option to specify a SMTP server for email
  notifications and a database platform you want to use for the metrics
  database. These steps will assist in this process.
 </p><div class="sect3" id="config-mon-notemail"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Monitoring Email Notification Settings</span> <a title="Permalink" class="permalink" href="#config-mon-notemail">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>config-mon-notemail</li></ul></div></div></div></div><p>
  The monitoring service, based on Monasca, allows you to configure an external
  SMTP server for email notifications when alarms trigger. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you
  have the option to specify a SMTP server for email notifications. These steps
  will assist in this process.
 </p><p>
  If you are going to use the email notifiication feature of the monitoring
  service, you must set the configuration options with valid email settings
  including an SMTP server and valid email addresses. The email server is not
  provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, but must be specified in the configuration file
  described below. The email server must support SMTP.
 </p><div class="sect4" id="email"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring monitoring notification settings during initial installation</span> <a title="Permalink" class="permalink" href="#email">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>email</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To change the SMTP server configuration settings edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/cloudConfig.yml</pre></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Enter your email server settings. Here is an example snippet showing the
       configuration file contents, uncomment these lines before entering your
       environment details.
      </p><div class="verbatim-wrap"><pre class="screen">    smtp-settings:
    #  server: mailserver.examplecloud.com
    #  port: 25
    #  timeout: 15
    # These are only needed if your server requires authentication
    #  user:
    #  password:</pre></div><p>
       This table explains each of these values:
      </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>Server (required)</td><td>
           <p>
            The server entry must be uncommented and set to a valid hostname or
            IP Address.
           </p>
          </td></tr><tr><td>Port (optional)</td><td>
           <p>
            If your SMTP server is running on a port other than the standard
            25, then uncomment the port line and set it your port.
           </p>
          </td></tr><tr><td>Timeout (optional)</td><td>
           <p>
            If your email server is heavily loaded, the timeout parameter can
            be uncommented and set to a larger value. 15 seconds is the
            default.
           </p>
          </td></tr><tr><td>User / Password (optional)</td><td>
           <p>
            If your SMTP server requires authentication, then you can configure
            user and password. Use double quotes around the password to avoid
            issues with special characters.
           </p>
          </td></tr></tbody></table></div></li></ol></div></li><li class="listitem "><p>
     To configure the sending email addresses, edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</pre></div><p>
     Modify the following value to add your sending email address:
    </p><div class="verbatim-wrap"><pre class="screen">email_from_addr</pre></div><div id="id-1.6.14.3.4.4.4.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The default value in the file is <code class="literal">email_from_address:
      notification@exampleCloud.com</code> which you should edit.
     </p></div></li><li class="listitem "><p>
     [optional] To configure the receiving email addresses, edit the following
     file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-default-alarms/defaults/main.yml</pre></div><p>
     Modify the following value to configure a receiving email address:
    </p><div class="verbatim-wrap"><pre class="screen">notification_address</pre></div><div id="id-1.6.14.3.4.4.4.2.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can also set the receiving email address via the Operations Console.
      Instructions for this are in the last section.
     </p></div></li><li class="listitem "><p>
     If your environment requires a proxy address then you can add that in as
     well:
    </p><div class="verbatim-wrap"><pre class="screen"># notification_environment can be used to configure proxies if needed.
# Below is an example configuration. Note that all of the quotes are required.
# notification_environment: '"http_proxy=http://&lt;your_proxy&gt;:&lt;port&gt;" "https_proxy=http://&lt;your_proxy&gt;:&lt;port&gt;"'
<span class="bold"><strong>notification_environment: ''</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Updated monitoring service email notification settings"</pre></div></li><li class="listitem "><p>
     Continue with your installation.
    </p></li></ol></div></div><div class="sect4" id="apache-commons-validate"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca and Apache Commons validator</span> <a title="Permalink" class="permalink" href="#apache-commons-validate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>apache-commons-validate</li></ul></div></div></div></div><p>
   The Monasca notification uses a standard Apache Commons validator to
   validate the configured <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> domain names before sending the
   notification over webhook. Monasca notification supports some non-standard
   domain names, but not all. See the Domain Validator documentation for more
   information:
   <a class="link" href="https://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/DomainValidator.html" target="_blank">https://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/DomainValidator.html</a>
  </p><p>
   You should ensure that any domains that you use are supported by IETF and
   IANA. As an example, <span class="bold"><strong>.local</strong></span> is not listed
   by IANA and is invalid but <span class="bold"><strong>.gov</strong></span> and
   <span class="bold"><strong>.edu</strong></span> are valid.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Internet Assigned Numbers Authority (IANA):
     <a class="link" href="https://www.iana.org/domains/root/db" target="_blank">https://www.iana.org/domains/root/db</a>
    </p></li></ul></div><p>
   Failure to use supported domains will generate an unprocessable exception in
   Monasca notification create:
  </p><div class="verbatim-wrap"><pre class="screen">HTTPException code=422 message={"unprocessable_entity":
{"code":422,"message":"Address https://myopenstack.sample:8000/v1/signal/test is not of correct format","details":"","internal_code":"c6cf9d9eb79c3fc4"}</pre></div></div><div class="sect4" id="id-1.6.14.3.4.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring monitoring notification settings after the initial installation</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.4.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you need to make changes to the email notification settings after your
   initial deployment, you can change the "From" address using the
   configuration files but the "To" address will need to be changed in the
   Operations Console. The following section will describe both of these
   processes.
  </p><p>
   <span class="bold"><strong>To change the sending email address:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To configure the sending email addresses, edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</pre></div><p>
     Modify the following value to add your sending email address:
    </p><div class="verbatim-wrap"><pre class="screen">email_from_addr</pre></div><div id="id-1.6.14.3.4.4.6.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The default value in the file is <code class="literal">email_from_address:
      notification@exampleCloud.com</code> which you should edit.
     </p></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Updated monitoring service email notification settings"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Monasca reconfigure playbook to deploy the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</pre></div><div id="id-1.6.14.3.4.4.6.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You may need to use the <code class="literal">--ask-vault-pass</code> switch if you
      opted for encryption during the initial deployment.
     </p></div></li></ol></div><p>
   <span class="bold"><strong>To change the receiving email address via the
   Operations Console:</strong></span>
  </p><p>
   To configure the "To" email address, after installation,
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Connect to and log in to the Operations Console. See
     <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.2 “Connecting to the Operations Console”</span> for assistance.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left side, click
     <span class="bold"><strong>Home</strong></span>, and then
     <span class="bold"><strong>Alarm Explorer</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Alarm Explorer</strong></span> page, at the top,
     click the <span class="bold"><strong>Notification Methods</strong></span> text.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Notification Methods</strong></span> page, find
     the row with the <span class="bold"><strong>Default Email</strong></span>
     notification.
    </p></li><li class="listitem "><p>
     In the <span class="bold"><strong>Default Email</strong></span> row, click the
     details icon (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-DetailDots.png" width="" alt="Ellipsis Icon" /></span>), then click
     <span class="bold"><strong>Edit</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Edit Notification Method: Default
     Email</strong></span> page, in <span class="bold"><strong>Name</strong></span>,
     <span class="bold"><strong>Type</strong></span>, and
     <span class="bold"><strong>Address/Key</strong></span>, type in the values you want
     to use.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Edit Notification Method: Default
     Email</strong></span> page, click <span class="bold"><strong>Update
     Notification</strong></span>.
    </p></li></ol></div><div id="id-1.6.14.3.4.4.6.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Once the notification has been added, using the procedures using the
    Ansible playbooks will not change it.
   </p></div></div></div><div class="sect3" id="manage-note-methods"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Notification Methods for Alarms</span> <a title="Permalink" class="permalink" href="#manage-note-methods">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>manage-note-methods</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#proxy" title="12.1.2.2.1. Enabling a Proxy for Webhook or Pager Duty Notifications">Section 12.1.2.2.1, “Enabling a Proxy for Webhook or Pager Duty Notifications”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#create" title="12.1.2.2.2. Creating a New Notification Method">Section 12.1.2.2.2, “Creating a New Notification Method”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#apply" title="12.1.2.2.3. Applying a Notification Method to an Alarm Definition">Section 12.1.2.2.3, “Applying a Notification Method to an Alarm Definition”</a>
   </p></li></ul></div><div class="sect4" id="proxy"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling a Proxy for Webhook or Pager Duty Notifications</span> <a title="Permalink" class="permalink" href="#proxy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>proxy</li></ul></div></div></div></div><p>
   If your environment requires a proxy in order for communications to function
   then these steps will show you how you can enable one. These steps will only
   be needed if you are utilizing the webhook or pager duty notification
   methods.
  </p><p>
   These steps will require access to the Cloud Lifecycle Manager in your cloud
   deployment so you may need to contact your Administrator. You can make these
   changes during the initial configuration phase prior to the first
   installation or you can modify your existing environment, the only
   difference being the last step.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the
     <code class="literal">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</code>
     file and edit the line below with your proxy address values:
    </p><div class="verbatim-wrap"><pre class="screen">notification_environment: '"http_proxy=http://&lt;proxy_address&gt;:&lt;port&gt;" "https_proxy=&lt;http://proxy_address&gt;:&lt;port&gt;"'</pre></div><div id="id-1.6.14.3.4.5.3.4.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      There are single quotation marks around the entire value of this entry and
      then double quotation marks around the individual proxy entries. This
      formatting must exist when you enter these values into your configuration
      file.
     </p></div></li><li class="step "><p>
     If you are making these changes prior to your initial installation then
     you are done and can continue on with the installation. However, if you
     are modifying an existing environment, you will need to continue on with
     the remaining steps below.
    </p></li><li class="step "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Generate an updated deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the Monasca reconfigure playbook to enable these changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</pre></div></li></ol></div></div></div><div class="sect4" id="create"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a New Notification Method</span> <a title="Permalink" class="permalink" href="#create">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>create</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Operations Console. For more information, see
     <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.2 “Connecting to the Operations Console”</span>.
    </p></li><li class="step "><p>
     Use the navigation menu to go to the <span class="bold"><strong>Alarm
     Explorer</strong></span> page:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod.png" width="" /></a></div></div></li><li class="step "><p>
     Select the <span class="bold"><strong>Notification Methods</strong></span> menu and
     then click the <span class="bold"><strong>Create Notification Method</strong></span>
     button:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod1.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod1.png" width="" /></a></div></div></li><li class="step "><p>
     On the <span class="bold"><strong>Create Notification Method</strong></span> window
     you will select your options and then click the
     <span class="bold"><strong>Create Notification</strong></span> button.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod3.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod3.png" width="" /></a></div></div><p>
     A description of each of the fields you use for each notification method:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>Name</td><td><p>Enter a unique name value for the notification method you are creating.</p></td></tr><tr><td>Type</td><td><p>Choose a type. Available values are <span class="bold"><strong>Webhook</strong></span>, <span class="bold"><strong>Email</strong></span>, or <span class="bold"><strong>Pager Duty</strong></span>.</p></td></tr><tr><td>Address/Key</td><td>Enter the value corresponding to the type you chose.</td></tr></tbody></table></div></li></ol></div></div></div><div class="sect4" id="apply"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Applying a Notification Method to an Alarm Definition</span> <a title="Permalink" class="permalink" href="#apply">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>apply</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Operations Console. For more informalfigure, see
     <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.2 “Connecting to the Operations Console”</span>.
    </p></li><li class="step "><p>
     Use the navigation menu to go to the <span class="bold"><strong>Alarm
     Explorer</strong></span> page:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod.png" width="" /></a></div></div></li><li class="step "><p>
     Select the <span class="bold"><strong>Alarm Definition</strong></span> menu which
     will give you a list of each of the alarm definitions in your environment.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod4.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod4.png" width="" /></a></div></div></li><li class="step "><p>
     Locate the alarm you want to change the notification method for and click
     on its name to bring up the edit menu. You can use the sorting methods for
     assistance.
    </p></li><li class="step "><p>
     In the edit menu, scroll down to the <span class="bold"><strong>Notifications
     and Severity</strong></span> section where you will select one or more
     <span class="bold"><strong>Notification Methods</strong></span> before selecting the
     <span class="bold"><strong>Update Alarm Definition</strong></span> button:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod6.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod6.png" width="" /></a></div></div></li><li class="step "><p>
     Repeat as needed until all of your alarms have the notification methods
     you desire.
    </p></li></ol></div></div></div></div><div class="sect3" id="enable-rabbitmq-admin-console"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the RabbitMQ Admin Console</span> <a title="Permalink" class="permalink" href="#enable-rabbitmq-admin-console">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-enable_mon_console.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-enable_mon_console.xml</li><li><span class="ds-label">ID: </span>enable-rabbitmq-admin-console</li></ul></div></div></div></div><p>
  The RabbitMQ Admin Console is off by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. You can turn on the
  console by following these steps:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Edit the <code class="filename">~/openstack/my_cloud/config/rabbitmq/main.yml</code>
    file. Under the <code class="literal">rabbit_plugins:</code>line, uncomment
   </p><div class="verbatim-wrap"><pre class="screen">- rabbitmq_management</pre></div></li><li class="listitem "><p>
    Commit your configuration to the local Git repository (see
    <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enabled RabbitMQ Admin Console"</pre></div></li><li class="listitem "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run the RabbitMQ reconfigure playbook to deploy the changes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-reconfigure.yml</pre></div></li></ol></div><p>
  To turn the RabbitMQ Admin Console off again, add the comment back and repeat
  steps 3 through 6.
  
 </p></div><div class="sect3" id="capacity-reporting-transform"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Capacity Reporting and Monasca Transform</span> <a title="Permalink" class="permalink" href="#capacity-reporting-transform">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-capacity_reporting_transform.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-capacity_reporting_transform.xml</li><li><span class="ds-label">ID: </span>capacity-reporting-transform</li></ul></div></div></div></div><p>
  Capacity reporting is a new feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which will provide cloud
  operators overall capacity (available, used, and remaining) information via
  the Operations Console so that the cloud operator can ensure that cloud resource pools
  have sufficient capacity to meet the demands of users.  The cloud operator is
  also able to set thresholds and set alarms to be notified when the thresholds
  are reached.
 </p><p>
  <span class="bold"><strong>For Compute</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Host Capacity - CPU/Disk/Memory: Used, Available and Remaining Capacity -
    for the entire cloud installation or by host
   </p></li><li class="listitem "><p>
    VM Capacity - CPU/Disk/Memory: Allocated, Available and Remaining - for
    the entire cloud installation, by host or by project
   </p></li></ul></div><p>
  <span class="bold"><strong>For Object Storage</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Disk Capacity - Used, Available and Remaining Capacity - for the entire
    cloud installation or by project
   </p></li></ul></div><p>
  In addition to overall capacity, roll up views with appropriate slices provide
  views by a particular project, or compute node. Graphs also show trends and
  the change in capacity over time.
 </p><div class="sect4" id="crt-features"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Transform Features</span> <a title="Permalink" class="permalink" href="#crt-features">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_features.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_features.xml</li><li><span class="ds-label">ID: </span>crt-features</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Monasca Transform is a new component in Monasca which transforms and
    aggregates metrics using Apache Spark
   </p></li><li class="listitem "><p>
    Aggregated metrics are published to Kafka and are available for other
    monasca components like monasca-threshold and are stored in monasca
    datastore
   </p></li><li class="listitem "><p>
    Cloud operators can set thresholds and set alarms to receive notifications
    when thresholds are met.
   </p></li><li class="listitem "><p>
    These aggregated metrics are made available to the cloud operators via
    Operations Console's new Capacity Summary (reporting) UI
   </p></li><li class="listitem "><p>
    Capacity reporting is a new feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which will provides
    cloud operators an overall capacity (available, used and remaining) for
    Compute and Object Storage
   </p></li><li class="listitem "><p>
    Cloud operators can look at Capacity reporting via Operations Console's Compute
    Capacity Summary and Object Storage Capacity Summary UI
   </p></li><li class="listitem "><p>
    Capacity reporting allows the cloud operators the ability to ensure that
    cloud resource pools have sufficient capacity to meet demands of users. See
    table below for Service and Capacity Types.
   </p></li><li class="listitem "><p>
    A list of aggregated metrics is provided in
    <a class="xref" href="#crt-aggregated-metrics" title="12.1.2.4.4. New Aggregated Metrics">Section 12.1.2.4.4, “New Aggregated Metrics”</a>.
   </p></li><li class="listitem "><p>
    Capacity reporting aggregated metrics are aggregated and published every
    hour
   </p></li><li class="listitem "><p>
    In addition to the overall capacity, there are graphs which show the
    capacity trends over time range (for 1 day, for 7 days, for 30 days or for
    45 days)
   </p></li><li class="listitem "><p>
    Graphs showing the capacity trends by a particular project or compute host
    are also provided.
   </p></li><li class="listitem "><p>
    Monasca Transform is integrated with centralized monitoring (Monasca) and
    centralized logging
   </p></li><li class="listitem "><p>
    Flexible Deployment
   </p></li><li class="listitem "><p>
    Upgrade &amp; Patch Support
   </p></li></ul></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Type of Capacity</th><th>Description</th></tr></thead><tbody><tr><td>Compute</td><td>Host Capacity</td><td>
      <p>
       CPU/Disk/Memory: Used, Available and Remaining Capacity - for entire
       cloud installation or by compute host
      </p>
     </td></tr><tr><td> </td><td>VM Capacity</td><td>
      <p>
       CPU/Disk/Memory: Allocated, Available and Remaining - for entire cloud
       installation, by host or by project
      </p>
     </td></tr><tr><td>Object Storage</td><td>Disk Capacity</td><td>
      <p>
       Used, Available and Remaining Disk Capacity - for entire cloud
       installation or by project
      </p>
     </td></tr><tr><td> </td><td>Storage Capacity</td><td>
      <p>
       Utilized Storage Capacity - for entire cloud installation or by project
      </p>
     </td></tr></tbody></table></div></div><div class="sect4" id="crt-arch-transform-spark"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Architecture for Monasca Transform and Spark</span> <a title="Permalink" class="permalink" href="#crt-arch-transform-spark">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_arch_transform_spark.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_arch_transform_spark.xml</li><li><span class="ds-label">ID: </span>crt-arch-transform-spark</li></ul></div></div></div></div><p>
  Monasca Transform is a new component in Monasca. Monasca Transform uses
  Spark for data aggregation. Both Monasca Transform and Spark are depicted in
  the example diagram below.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-monasca-Monasca_Service_Arch_Diagram.png" target="_blank"><img src="images/media-monasca-Monasca_Service_Arch_Diagram.png" width="" /></a></div></div><p>
  You can see that the Monasca components run on the Cloud Controller nodes,
  and the Monasca agents run on all nodes in the Mid-scale Example
  configuration.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png" target="_blank"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div></div><div class="sect4" id="crt-components"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Components for Capacity Reporting</span> <a title="Permalink" class="permalink" href="#crt-components">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span>crt-components</li></ul></div></div></div></div><div class="sect5" id="id-1.6.14.3.4.7.10.2"><div class="titlepage"><div><div><h6 class="title"><span class="number">12.1.2.4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Transform: Data Aggregation Reporting</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.4.7.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Monasca-transform is a new component which provides mechanism to aggregate
   or transform metrics and publish new aggregated metrics to Monasca.
  </p><p>
   Monasca Transform is a data driven Apache Spark based data aggregation
   engine which collects, groups and aggregates existing individual Monasca
   metrics according to business requirements and publishes new transformed
   (derived) metrics to the Monasca Kafka queue.
  </p><p>
   Since the new transformed metrics are published as any other metric in
   Monasca, alarms can be set and triggered on the transformed metric, just
   like any other metric.
  </p></div><div class="sect5" id="id-1.6.14.3.4.7.10.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">12.1.2.4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage and Compute Capacity Summary Operations Console UI</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.4.7.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A new "Capacity Summary" tab for Compute and Object Storage will displays
   all the aggregated metrics under the "Compute" and "Object Storage"
   sections.
  </p><p>
   Operations Console UI makes calls to Monasca API to retrieve and display various
   tiles and graphs on Capacity Summary tab in Compute and Object Storage
   Summary UI pages.
  </p></div><div class="sect5" id="id-1.6.14.3.4.7.10.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">12.1.2.4.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persist new metrics and Trigger Alarms</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.4.7.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   New aggregated metrics will be published to Monasca's Kafka queue and will
   be ingested by monasca-persister. If thresholds and alarms have been set on
   the aggregated metrics, Monasca will generate and trigger alarms as it
   currently does with any other metric. No new/additional change is expected
   with persisting of new aggregated metrics or setting threshold/alarms.
  </p></div></div><div class="sect4" id="crt-aggregated-metrics"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Aggregated Metrics</span> <a title="Permalink" class="permalink" href="#crt-aggregated-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_aggregated_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_aggregated_metrics.xml</li><li><span class="ds-label">ID: </span>crt-aggregated-metrics</li></ul></div></div></div></div><p>
  Following is the list of aggregated metrics produced by monasca transform in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
 </p><div class="table" id="table-ztc-yn5-3y"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.1: </span><span class="name">Aggregated Metrics </span><a title="Permalink" class="permalink" href="#table-ztc-yn5-3y">#</a></h6></div><div class="table-contents"><table class="table" summary="Aggregated Metrics" border="1"><colgroup><col align="left" class="c1" /><col align="left" class="c2" /><col align="left" class="c3" /><col align="left" class="c4" /><col align="left" class="c5" /><col align="left" class="c6" /></colgroup><thead><tr><th align="left"> </th><th align="left">Metric Name</th><th align="left">For</th><th align="left">Description</th><th align="left">Dimensions</th><th align="left">Notes</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">
      <p>
       cpu.utilized_logical_cores_agg
      </p>
     </td><td align="left">compute summary </td><td align="left">
      <p>
       utilized physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">2</td><td align="left">cpu.total_logical_cores_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">3</td><td align="left">mem.total_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">4</td><td align="left">mem.usable_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       usable physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">5</td><td align="left">disk.total_used_space_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       utilized physical host disk capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">6</td><td align="left">disk.total_space_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host disk capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">7</td><td align="left">nova.vm.cpu.total_allocated_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       cpus allocated across all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">8</td><td align="left">vcpus_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       virtual cpus allocated capacity for VMs of one or all projects by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all or &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">9</td><td align="left">nova.vm.mem.total_allocated_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory allocated to all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">10</td><td align="left">vm.mem.used_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory utilized by VMs of one or all projects by time interval (defaults
       to an hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">11</td><td align="left">vm.mem.total_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory allocated to VMs of one or all projects by time interval
       (defaults to an hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">12</td><td align="left">vm.cpu.utilization_perc_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       cpu utilized by all VMs by project by time interval (defaults to an
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">13</td><td align="left">nova.vm.disk.total_allocated_gb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       disk space allocated to all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">14</td><td align="left">vm.disk.allocation_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       disk allocation for VMs of one or all projects by time interval
       (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all or &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">15</td><td align="left">swiftlm.diskusage.val.size_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       total available object storage capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">16</td><td align="left">swiftlm.diskusage.val.avail_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       remaining object storage capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">17</td><td align="left">swiftlm.diskusage.rate_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       rate of change of object storage usage by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">18</td><td align="left">storage.objects.size_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       used object storage capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr></tbody></table></div></div></div><div class="sect4" id="crt-deployment"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment</span> <a title="Permalink" class="permalink" href="#crt-deployment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_deployment.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_deployment.xml</li><li><span class="ds-label">ID: </span>crt-deployment</li></ul></div></div></div></div><p>
  Monasca Transform and Spark will be deployed on the same control plane nodes
  along with Logging and Monitoring Service (Monasca).
 </p><p>
  <span class="bold"><strong>Security Consideration during deployment of Monasca
  Transform and Spark</strong></span>
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring system connects internally to the Kafka and Spark
  technologies without authentication. If you choose to deploy Monitoring,
  configure it to use only trusted networks such as the Management network, as
  illustrated on the network diagrams below for Entry Scale Deployment and Mid
  Scale Deployment.
 </p><p>
  <span class="bold"><strong>Entry Scale Deployment</strong></span>
 </p><p>
  In Entry Scale Deployment Monasca Transform and Spark will be deployed on
  Shared Control Plane along with other Openstack Services along with
  Monitoring and Logging
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-entryScale-Entry-ScaleAllNetworks.png" target="_blank"><img src="images/media-entryScale-Entry-ScaleAllNetworks.png" width="" /></a></div></div><p>
  <span class="bold"><strong>Mid scale Deployment</strong></span>
 </p><p>
  In a Mid Scale Deployment Monasca Transform and Spark will be deployed on
  dedicated Metering Monitoring and Logging (MML) control plane along with
  other data processing intensive services like Metering, Monitoring and
  Logging
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png" target="_blank"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div><p>
  <span class="bold"><strong>Multi Control Plane Deployment</strong></span>
 </p><p>
  In a Multi Control Plane Deployment, Monasca Transform and Spark will be
  deployed on the Shared Control plane along with rest of Monasca Components.
 </p><p>
  <span class="bold"><strong>Start, Stop and Status for Monasca Transform and Spark
  processes</strong></span>
 </p><p>
  The service management methods for monasca-transform and spark follow the
  convention for services in the <span class="productname">OpenStack</span> platform. When executing from the
  deployer node, the commands are as follows:
 </p><p>
  <span class="bold"><strong>Status</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-status.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-status.yml</pre></div><p>
  <span class="bold"><strong>Start</strong></span>
 </p><p>
  As monasca-transform depends on spark for the processing of the metrics spark
  will need to be started before monasca-transform.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-start.yml</pre></div><p>
  <span class="bold"><strong>Stop</strong></span>
 </p><p>
  As a precaution, stop the monasca-transform service before
  taking spark down. Interruption to the spark service altogether while
  monasca-transform is still running can result in a monasca-transform process
  that is unresponsive and needing to be tidied up.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-stop.yml</pre></div></div><div class="sect4" id="crt-reconfigure"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfigure</span> <a title="Permalink" class="permalink" href="#crt-reconfigure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_reconfigure.xml</li><li><span class="ds-label">ID: </span>crt-reconfigure</li></ul></div></div></div></div><p>
  The reconfigure process can be triggered again from the deployer. Presuming
  that changes have been made to the variables in the appropriate places
  execution of the respective ansible scripts will be enough to update the
  configuration. The spark reconfigure process alters the nodes serially
  meaning that spark is never down altogether, each node is stopped in turn
  and zookeeper manages the leaders accordingly. This means that
  monasca-transform may be left running even while spark is upgraded.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></div><div class="sect4" id="crt-adding-transform-spark"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Monasca Transform and Spark to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Deployment</span> <a title="Permalink" class="permalink" href="#crt-adding-transform-spark">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_adding_transform_spark.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_adding_transform_spark.xml</li><li><span class="ds-label">ID: </span>crt-adding-transform-spark</li></ul></div></div></div></div><p>
  Since Monasca Transform and Spark are optional components, the users might
  elect to not install these two components during their initial <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  install. The following instructions provide a way the users can add Monasca
  Transform and Spark to their existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Add Monasca Transform and Spark to the input model. Monasca Transform and
    Spark on a entry level cloud would be installed on the common control
    plane, for mid scale cloud which has a MML (Metering, Monitoring and
    Logging) cluster, Monasca Transform and Spark will should be added to
    MML cluster.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div><p>
    Add spark and monasca-transform to input model, control_plane.yml
   </p><div class="verbatim-wrap"><pre class="screen">clusters
       - name: core
         cluster-prefix: c1
         server-role: CONTROLLER-ROLE
         member-count: 3
         allocation-policy: strict
         service-components:

           [...]

           - zookeeper
           - kafka
           - cassandra
           - storm
           - spark
           - monasca-api
           - monasca-persister
           - monasca-notifier
           - monasca-threshold
           - monasca-client
           - monasca-transform

           [...]</pre></div></li><li class="listitem "><p>
    Run the Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Adding Monasca Transform and Spark"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Run Ready Deployment
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run Cloud Lifecycle Manager Deploy
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li></ol></div><p>
  <span class="bold"><strong>Verify Deployment</strong></span>
 </p><p>
  Login to each controller node and run
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service monasca-transform status
<code class="prompt user">tux &gt; </code>sudo service spark-master status
<code class="prompt user">tux &gt; </code>sudo service spark-worker status</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service monasca-transform status
● monasca-transform.service - Monasca Transform Daemon
  Loaded: loaded (/etc/systemd/system/monasca-transform.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:47:56 UTC; 2 days ago
Main PID: 7351 (bash)
  CGroup: /system.slice/monasca-transform.service
          ├─ 7351 bash /etc/monasca/transform/init/start-monasca-transform.sh
          ├─ 7352 /opt/stack/service/monasca-transform/venv//bin/python /opt/monasca/monasca-transform/lib/service_runner.py
          ├─27904 /bin/sh -c export SPARK_HOME=/opt/stack/service/spark/venv/bin/../current &amp;&amp; spark-submit --supervise --master spark://omega-cp1-c1-m1-mgmt:7077,omega-cp1-c1-m2-mgmt:7077,omega-cp1-c1...
          ├─27905 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -cp /opt/stack/service/spark/venv/lib/drizzle-jdbc-1.3.jar:/opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/v...
          └─28355 python /opt/monasca/monasca-transform/lib/driver.py
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.


<code class="prompt user">tux &gt; </code>sudo service spark-worker status
● spark-worker.service - Spark Worker Daemon
  Loaded: loaded (/etc/systemd/system/spark-worker.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:46:05 UTC; 2 days ago
Main PID: 63513 (bash)
  CGroup: /system.slice/spark-worker.service
          ├─ 7671 python -m pyspark.daemon
          ├─28948 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0...
          ├─63513 bash /etc/spark/init/start-spark-worker.sh &amp;
          └─63514 /usr/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0.jar:/opt/stack/service/spark/ven...
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.



<code class="prompt user">tux &gt; </code>sudo service spark-master status
● spark-master.service - Spark Master Daemon
  Loaded: loaded (/etc/systemd/system/spark-master.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:44:24 UTC; 2 days ago
Main PID: 55572 (bash)
  CGroup: /system.slice/spark-master.service
          ├─55572 bash /etc/spark/init/start-spark-master.sh &amp;
          └─55573 /usr/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0.jar:/opt/stack/service/spark/ven...
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</pre></div></div><div class="sect4" id="crt-increasing-scale"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increase Monasca Transform Scale</span> <a title="Permalink" class="permalink" href="#crt-increasing-scale">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_increasing_scale.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_increasing_scale.xml</li><li><span class="ds-label">ID: </span>crt-increasing-scale</li></ul></div></div></div></div><p>
  Monasca Transform in the default configuration can scale up to estimated
  data for 100 node cloud deployment. Estimated maximum rate of metrics from a
  100 node cloud deployment is 120M/hour.
 </p><p>
  You can further increase the processing rate to 180M/hour. Making
  the Spark configuration change will increase the CPU's being used by Spark
  and Monasca Transform from average of around 3.5 to 5.5 CPU's per control
  node over a 10 minute batch processing interval.
 </p><p>
  To increase the processing rate to 180M/hour the customer will have to make
  following spark configuration change.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Edit /var/lib/ardana/openstack/my_cloud/config/spark/spark-defaults.conf.j2 and
    set spark.cores.max to 6 and spark.executor.cores 2
   </p><p>
    <span class="bold"><strong>Set spark.cores.max to 6</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">spark.cores.max {{ spark_cores_max }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">spark.cores.max 6</pre></div><p>
    <span class="bold"><strong>Set spark.executor.cores to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">spark.executor.cores {{ spark_executor_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">spark.executor.cores 2</pre></div></li><li class="listitem "><p>
    Edit ~/openstack/my_cloud/config/spark/spark-env.sh.j2
   </p><p>
    <span class="bold"><strong>Set SPARK_WORKER_CORES to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES={{ spark_worker_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES=2</pre></div></li><li class="listitem "><p>
    Edit ~/openstack/my_cloud/config/spark/spark-worker-env.sh.j2
   </p><p>
    <span class="bold"><strong>Set SPARK_WORKER_CORES to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES={{ spark_worker_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES=2</pre></div></li><li class="listitem "><p>
    Run Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Changing Spark Config increase scale"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Run Ready Deployment
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run spark-reconfigure.yml and monasca-transform-reconfigure.yml
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></li></ol></div></div><div class="sect4" id="crt-change-compute-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change Compute Host Pattern Filter in Monasca Transform</span> <a title="Permalink" class="permalink" href="#crt-change-compute-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crt_change_compute_host.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_change_compute_host.xml</li><li><span class="ds-label">ID: </span>crt-change-compute-host</li></ul></div></div></div></div><p>
  Monasca Transform identifies compute host metrics by pattern matching on
  hostname dimension in the incoming monasca metrics. The default pattern is of
  the form <code class="literal">comp<em class="replaceable ">NNN</em></code>. For example,
  <code class="literal">comp001</code>, <code class="literal">comp002</code>, etc. To filter for it
  in the transformation specs, use the expression
  <code class="literal">-comp[0-9]+-</code>. In case the compute
  host names follow a different pattern other than the standard pattern above,
  the filter by expression when aggregating metrics will have to be changed.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the deployer: Edit
     <code class="filename">~/openstack/my_cloud/config/monasca-transform/transform_specs.json.j2</code>
   </p></li><li class="step " id="st-monasca-check-comp-reference"><p>
    Look for all references of <code class="literal">-comp[0-9]+-</code> and change the
    regular expression to the desired pattern say for example
    <code class="literal">-compute[0-9]+-</code>.
   </p><div class="verbatim-wrap"><pre class="screen">{"aggregation_params_map":{"aggregation_pipeline":{"source":"streaming", "usage":"fetch_quantity", "setters":["rollup_quantity", "set_aggregated_metric_name", "set_aggregated_period"], "insert":["prepare_data","insert_data_pre_hourly"]}, "aggregated_metric_name":"mem.total_mb_agg", "aggregation_period":"hourly", "aggregation_group_by_list": ["host", "metric_id", "tenant_id"], "usage_fetch_operation": "avg", "filter_by_list": [{"field_to_filter": "host", "filter_expression": "-comp[0-9]+", "filter_operation": "include"}], "setter_rollup_group_by_list":[], "setter_rollup_operation": "sum", "dimension_list":["aggregation_period", "host", "project_id"], "pre_hourly_operation":"avg", "pre_hourly_group_by_list":["default"]}, "metric_group":"mem_total_all", "metric_id":"mem_total_all"}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">{"aggregation_params_map":{"aggregation_pipeline":{"source":"streaming", "usage":"fetch_quantity", "setters":["rollup_quantity", "set_aggregated_metric_name", "set_aggregated_period"], "insert":["prepare_data", "insert_data_pre_hourly"]}, "aggregated_metric_name":"mem.total_mb_agg", "aggregation_period":"hourly", "aggregation_group_by_list": ["host", "metric_id", "tenant_id"],"usage_fetch_operation": "avg","filter_by_list": [{"field_to_filter": "host","filter_expression": "-compute[0-9]+", "filter_operation": "include"}], "setter_rollup_group_by_list":[], "setter_rollup_operation": "sum", "dimension_list":["aggregation_period", "host", "project_id"], "pre_hourly_operation":"avg", "pre_hourly_group_by_list":["default"]}, "metric_group":"mem_total_all", "metric_id":"mem_total_all"}</pre></div><div id="id-1.6.14.3.4.7.16.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     The filter_expression has been changed to the <span class="emphasis"><em>new</em></span>
     pattern.
    </p></div></li><li class="step "><p>
    To change all host metric transformation specs in the same
    JSON file, repeat <a class="xref" href="#st-monasca-check-comp-reference" title="Step 2">Step 2</a>.
   </p><p>
    Transformation specs will have to be changed for following metric_ids
    namely "mem_total_all", "mem_usable_all", "disk_total_all",
    "disk_usable_all", "cpu_total_all", "cpu_total_host", "cpu_util_all",
    "cpu_util_host"
   </p></li><li class="step "><p>
     Run the Configuration Processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Changing Monasca Transform specs"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run Ready Deployment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run Monasca Transform Reconfigure:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect3" id="config-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Availability of Alarm Metrics</span> <a title="Permalink" class="permalink" href="#config-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-config_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_metrics.xml</li><li><span class="ds-label">ID: </span>config-metrics</li></ul></div></div></div></div><p>
  Using the Monasca agent tuning knobs, you can choose which alarm metrics are
  available in your environment.
 </p><p>
  The addition of the libvirt and OVS plugins to the Monasca agent provides a
  number of additional metrics that can be used. Most of these metrics are
  included by default, but others are not. You have the ability to use tuning
  knobs to add or remove these metrics to your environment based on your
  individual needs in your cloud.
 </p><p>
  We will list these metrics along with the tuning knob name and instructions
  for how to adjust these.
 </p><div class="sect4" id="libvirt-tuningknobs"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Libvirt plugin metric tuning knobs</span> <a title="Permalink" class="permalink" href="#libvirt-tuningknobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-libvirt_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_tuningknobs.xml</li><li><span class="ds-label">ID: </span>libvirt-tuningknobs</li></ul></div></div></div></div><p>
  The following metrics are added as part of the libvirt plugin:
 </p><div id="id-1.6.14.3.4.8.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   For a description of each of these metrics, see
   <a class="xref" href="#libvirt-metrics" title="12.1.4.11. Libvirt Metrics">Section 12.1.4.11, “Libvirt Metrics”</a>.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="newCol2" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Tuning Knob</th><th>Default Setting</th><th>Admin Metric Name</th><th>Project Metric Name</th></tr></thead><tbody><tr><td rowspan="3">vm_cpu_check_enable</td><td rowspan="3">True</td><td>vm.cpu.time_ns</td><td>cpu.time_ns</td></tr><tr><td>vm.cpu.utilization_norm_perc</td><td>cpu.utilization_norm_perc</td></tr><tr><td>vm.cpu.utilization_perc</td><td>cpu.utilization_perc</td></tr><tr><td rowspan="10">vm_disks_check_enable</td><td rowspan="10">
      <p>
       True
      </p>
      <p>
       Creates 20 disk metrics per disk device per virtual machine.
      </p>
     </td><td>vm.io.errors</td><td>io.errors</td></tr><tr><td>vm.io.errors_sec</td><td>io.errors_sec</td></tr><tr><td>vm.io.read_bytes</td><td>io.read_bytes</td></tr><tr><td>vm.io.read_bytes_sec</td><td>io.read_bytes_sec</td></tr><tr><td>vm.io.read_ops</td><td>io.read_ops</td></tr><tr><td>vm.io.read_ops_sec</td><td>io.read_ops_sec</td></tr><tr><td>vm.io.write_bytes</td><td>io.write_bytes</td></tr><tr><td>vm.io.write_bytes_sec</td><td>io.write_bytes_sec</td></tr><tr><td>vm.io.write_ops</td><td>io.write_ops</td></tr><tr><td>vm.io.write_ops_sec</td><td> io.write_ops_sec</td></tr><tr><td rowspan="8">vm_network_check_enable</td><td rowspan="8">
      <p>
       True
      </p>
      <p>
       Creates 16 network metrics per NIC per virtual machine.
      </p>
     </td><td>vm.net.in_bytes</td><td>net.in_bytes</td></tr><tr><td>vm.net.in_bytes_sec</td><td>net.in_bytes_sec</td></tr><tr><td>vm.net.in_packets</td><td>net.in_packets</td></tr><tr><td>vm.net.in_packets_sec</td><td>net.in_packets_sec</td></tr><tr><td>vm.net.out_bytes</td><td>net.out_bytes</td></tr><tr><td>vm.net.out_bytes_sec</td><td>net.out_bytes_sec</td></tr><tr><td>vm.net.out_packets</td><td>net.out_packets</td></tr><tr><td>vm.net.out_packets_sec</td><td>net.out_packets_sec</td></tr><tr><td>vm_ping_check_enable</td><td>True</td><td>vm.ping_status</td><td>ping_status</td></tr><tr><td rowspan="6">vm_extended_disks_check_enable</td><td rowspan="3">
      <p>
       True
      </p>
      <p>
       Creates 6 metrics per device per virtual machine.
      </p>
     </td><td>vm.disk.allocation</td><td>disk.allocation</td></tr><tr><td>vm.disk.capacity</td><td>disk.capacity</td></tr><tr><td>vm.disk.physical</td><td>disk.physical</td></tr><tr><td rowspan="3">
      <p>
       True
      </p>
      <p>
       Creates 6 aggregate metrics per virtual machine.
      </p>
     </td><td>vm.disk.allocation_total</td><td>disk.allocation_total</td></tr><tr><td>vm.disk.capacity_total</td><td>disk.capacity.total</td></tr><tr><td>vm.disk.physical_total</td><td>disk.physical_total</td></tr><tr><td rowspan="10">vm_disks_check_enable vm_extended_disks_check_enable</td><td rowspan="10">
      <p>
       True
      </p>
      <p>
       Creates 20 aggregate metrics per virtual machine.
      </p>
     </td><td>vm.io.errors_total</td><td>io.errors_total</td></tr><tr><td>vm.io.errors_total_sec</td><td>io.errors_total_sec</td></tr><tr><td>vm.io.read_bytes_total</td><td>io.read_bytes_total</td></tr><tr><td>vm.io.read_bytes_total_sec</td><td>io.read_bytes_total_sec</td></tr><tr><td>vm.io.read_ops_total</td><td>io.read_ops_total</td></tr><tr><td>vm.io.read_ops_total_sec</td><td>io.read_ops_total_sec</td></tr><tr><td>vm.io.write_bytes_total</td><td>io.write_bytes_total</td></tr><tr><td>vm.io.write_bytes_total_sec</td><td>io.write_bytes_total_sec</td></tr><tr><td>vm.io.write_ops_total</td><td>io.write_ops_total</td></tr><tr><td>vm.io.write_ops_total_sec</td><td>io.write_ops_total_sec</td></tr></tbody></table></div><div class="sect5" id="configuring-libvirt-tuning-knobs"><div class="titlepage"><div><div><h6 class="title"><span class="number">12.1.2.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the libvirt metrics using the tuning knobs</span> <a title="Permalink" class="permalink" href="#configuring-libvirt-tuning-knobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-libvirt_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_tuningknobs.xml</li><li><span class="ds-label">ID: </span>configuring-libvirt-tuning-knobs</li></ul></div></div></div></div><p>
   Use the following steps to configure the tuning knobs for the libvirt plugin
   metrics.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml</pre></div></li><li class="listitem "><p>
     Change the value for each tuning knob to the desired setting,
     <code class="literal">True</code> if you want the metrics created and
     <code class="literal">False</code> if you want them removed. Refer to the table
     above for which metrics are controlled by each tuning knob.
    </p><div class="verbatim-wrap"><pre class="screen">vm_cpu_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_disks_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_extended_disks_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_network_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_ping_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "configuring libvirt plugin tuning knobs"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Nova reconfigure playbook to implement the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div><div id="id-1.6.14.3.4.8.5.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   If you modify either of the following files, then the monasca tuning
   parameters should be adjusted to handle a higher load on the system.
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml
~/openstack/my_cloud/config/neutron/monasca_ovs_plugin.yaml.j2</pre></div><p>
   Tuning parameters are located in
   <code class="filename">~/openstack/my_cloud/config/monasca/configuration.yml</code>.
   The parameter <code class="literal">monasca_tuning_selector_override</code> should be
   changed to the <code class="literal">extra-large</code> setting.
  </p></div></div></div><div class="sect4" id="ovs-tuningknobs"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVS plugin metric tuning knobs</span> <a title="Permalink" class="permalink" href="#ovs-tuningknobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-ovs_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_tuningknobs.xml</li><li><span class="ds-label">ID: </span>ovs-tuningknobs</li></ul></div></div></div></div><p>
  The following metrics are added as part of the OVS plugin:
 </p><div id="id-1.6.14.3.4.8.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   For a description of each of these metrics, see
   <a class="xref" href="#sec-metric-ovs" title="12.1.4.16. Open vSwitch (OVS) Metrics">Section 12.1.4.16, “Open vSwitch (OVS) Metrics”</a>.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="newCol2" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Tuning Knob</th><th>Default Setting</th><th>Admin Metric Name</th><th>Project Metric Name</th></tr></thead><tbody><tr><td rowspan="4">use_rate_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_bytes_sec</td><td>vrouter.in_bytes_sec</td></tr><tr><td>ovs.vrouter.in_packets_sec</td><td>vrouter.in_packets_sec</td></tr><tr><td>ovs.vrouter.out_bytes_sec</td><td>vrouter.out_bytes_sec</td></tr><tr><td>ovs.vrouter.out_packets_sec</td><td>vrouter.out_packets_sec</td></tr><tr><td rowspan="4">use_absolute_metrics</td><td rowspan="4">True</td><td>ovs.vrouter.in_bytes</td><td>vrouter.in_bytes</td></tr><tr><td>ovs.vrouter.in_packets</td><td>vrouter.in_packets</td></tr><tr><td>ovs.vrouter.out_bytes</td><td>vrouter.out_bytes</td></tr><tr><td>ovs.vrouter.out_packets</td><td>vrouter.out_packets</td></tr><tr><td rowspan="4">use_health_metrics with use_rate_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_dropped_sec</td><td>vrouter.in_dropped_sec</td></tr><tr><td>ovs.vrouter.in_errors_sec</td><td>vrouter.in_errors_sec</td></tr><tr><td>ovs.vrouter.out_dropped_sec</td><td>vrouter.out_dropped_sec</td></tr><tr><td>ovs.vrouter.out_errors_sec</td><td>vrouter.out_errors_sec</td></tr><tr><td rowspan="4">use_health_metrics with use_absolute_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_dropped</td><td>vrouter.in_dropped</td></tr><tr><td>ovs.vrouter.in_errors</td><td>vrouter.in_errors</td></tr><tr><td>ovs.vrouter.out_dropped</td><td>vrouter.out_dropped</td></tr><tr><td>ovs.vrouter.out_errors</td><td>vrouter.out_errors</td></tr></tbody></table></div><div class="sect5" id="id-1.6.14.3.4.8.6.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">12.1.2.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the OVS metrics using the tuning knobs</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.4.8.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-ovs_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_tuningknobs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Use the following steps to configure the tuning knobs for the libvirt plugin
   metrics.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/neutron/monasca_ovs_plugin.yaml.j2</pre></div></li><li class="listitem "><p>
     Change the value for each tuning knob to the desired setting,
     <code class="literal">True</code> if you want the metrics created and
     <code class="literal">False</code> if you want them removed. Refer to the table
     above for which metrics are controlled by each tuning knob.
    </p><div class="verbatim-wrap"><pre class="screen">init_config:
   use_absolute_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
   use_rate_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
   use_health_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "configuring OVS plugin tuning knobs"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Neutron reconfigure playbook to implement the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="monasca-notification-plugins"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating HipChat, Slack, and JIRA</span> <a title="Permalink" class="permalink" href="#monasca-notification-plugins">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monasca_plugins_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_plugins_overview.xml</li><li><span class="ds-label">ID: </span>monasca-notification-plugins</li></ul></div></div></div></div><p>
  Monasca, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> monitoring and notification service, includes
  three default notification methods, <span class="bold"><strong>email</strong></span>,
  <span class="bold"><strong>PagerDuty</strong></span>, and
  <span class="bold"><strong>webhook</strong></span>. Monasca also supports three other
  notification plugins which allow you to send notifications to
  <span class="bold"><strong>HipChat</strong></span>,
  <span class="bold"><strong>Slack</strong></span>, and
  <span class="bold"><strong>JIRA</strong></span>. Unlike the default notification
  methods, the additional notification plugins must be manually configured.
 </p><p>
  This guide details the steps to configure each of the three non-default
  notification plugins. This guide also assumes that your cloud
  is fully deployed and functional.
 </p><div class="sect3" id="hipchat-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the HipChat Plugin</span> <a title="Permalink" class="permalink" href="#hipchat-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monasca_hipchat_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_hipchat_plugin.xml</li><li><span class="ds-label">ID: </span>hipchat-plugin</li></ul></div></div></div></div><p>
  To configure the HipChat plugin you will need the following four pieces of
  information from your HipChat system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The URL of your HipChat system.
   </p></li><li class="listitem "><p>
    A token providing permission to send notifications to your HipChat system.
   </p></li><li class="listitem "><p>
    The ID of the HipChat room you wish to send notifications to.
   </p></li><li class="listitem "><p>
    A HipChat user account. This account will be used to authenticate any
    incoming notifications from your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
   </p></li></ul></div><p>
  <span class="bold"><strong>Obtain a token</strong></span>
 </p><p>
  Use the following instructions to obtain a token from your Hipchat system.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to HipChat as the user account that will be used to authenticate the
    notifications.
   </p></li><li class="listitem "><p>
    Navigate to the following URL:
    <code class="literal">https://&lt;your_hipchat_system&gt;/account/api</code>. Replace
    <code class="literal">&lt;your_hipchat_system&gt;</code> with the
    fully-qualified-domain-name of your HipChat system.
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Create token</strong></span> option. Ensure
    that the token has the "SendNotification" attribute.
   </p></li></ol></div><p>
  <span class="bold"><strong>Obtain a room ID</strong></span>
 </p><p>
  Use the following instructions to obtain the ID of a HipChat room.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to HipChat as the user account that will be used to authenticate the
    notifications.
   </p></li><li class="listitem "><p>
    Select <span class="bold"><strong>My account</strong></span> from the application
    menu.
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Rooms</strong></span> tab.
   </p></li><li class="listitem "><p>
    Select the room that you want your notifications sent to.
   </p></li><li class="listitem "><p>
    Look for the API ID field in the room information. This is the room ID.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create HipChat notification type</strong></span>
 </p><p>
  Use the following instructions to create a HipChat notification type.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Begin by obtaining the API URL for the HipChat room that you wish to send
    notifications to. The format for a URL used to send notifications to a room
    is as follows:
   </p><p>
    <code class="literal">/v2/room/{room_id_or_name}/notification</code>
   </p></li><li class="listitem "><p>
    Use the Monasca API to create a new notification method. The following
    example demonstrates how to create a HipChat notification type named
    <span class="bold"><strong>MyHipChatNotification</strong></span>, for room
    <span class="bold"><strong>ID 13</strong></span>, using an example API URL and auth
    token.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  <em class="replaceable ">NAME</em> <em class="replaceable ">TYPE</em> <em class="replaceable ">ADDRESS</em>
<code class="prompt user">ardana &gt; </code>monasca notification-create  MyHipChatNotification HIPCHAT https://hipchat.hpe.net/v2/room/13/notification?auth_token=1234567890</pre></div><p>
    The preceding example creates a notification type with the following
    characteristics
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      NAME: MyHipChatNotification
     </p></li><li class="listitem "><p>
      TYPE: HIPCHAT
     </p></li><li class="listitem "><p>
      ADDRESS: https://hipchat.hpe.net/v2/room/13/notification
     </p></li><li class="listitem "><p>
      auth_token: 1234567890
     </p></li></ul></div></li></ol></div><div id="id-1.6.14.3.5.4.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The Horizon dashboard can also be used to create a HipChat notification
   type.
  </p></div></div><div class="sect3" id="monasca-slack-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Slack Plugin</span> <a title="Permalink" class="permalink" href="#monasca-slack-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monasca_slack_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_slack_plugin.xml</li><li><span class="ds-label">ID: </span>monasca-slack-plugin</li></ul></div></div></div></div><p>
  Configuring a Slack notification type requires four pieces of information
  from your Slack system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Slack server URL
   </p></li><li class="listitem "><p>
    Authentication token
   </p></li><li class="listitem "><p>
    Slack channel
   </p></li><li class="listitem "><p>
    A Slack user account. This account will be used to authenticate incoming
    notifications to Slack.
   </p></li></ul></div><p>
  <span class="bold"><strong>Identify a Slack channel</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to your Slack system as the user account that will be used to
    authenticate the notifications to Slack.
   </p></li><li class="listitem "><p>
    In the left navigation panel, under the
    <span class="bold"><strong>CHANNELS</strong></span> section locate the channel that
    you wish to receive the notifications. The instructions that follow will
    use the example channel <span class="bold"><strong>#general</strong></span>.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create a Slack token</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to your Slack system as the user account that will be used to
    authenticate the notifications to Slack
   </p></li><li class="listitem "><p>
    Navigate to the following URL:
    <a class="link" href="https://api.slack.com/docs/oauth-test-tokens" target="_blank">https://api.slack.com/docs/oauth-test-tokens</a>
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Create token</strong></span> button.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create a Slack notification type</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Begin by identifying the structure of the API call to be used by your
    notification method. The format for a call to the Slack Web API is as
    follows:
   </p><p>
    <code class="literal">https://slack.com/api/METHOD</code>
   </p><p>
    You can authenticate a Web API request by using the token that you created
    in the previous <span class="bold"><strong>Create a Slack
    Token</strong></span>section. Doing so will result in an API call that looks
    like the following.
   </p><p>
    <code class="literal">https://slack.com/api/METHOD?token=auth_token</code>
   </p><p>
    You can further refine your call by specifying the channel that the message
    will be posted to. Doing so will result in an API call that looks like the
    following.
   </p><p>
    <code class="literal">https://slack.com/api/<em class="replaceable ">METHOD</em>?token=<em class="replaceable ">AUTH_TOKEN</em>&amp;channel=<em class="replaceable ">#channel</em></code>
   </p><p>
    The following example uses the <code class="literal">chat.postMessage</code> method,
    the token <code class="literal">1234567890</code>, and the channel
    <code class="literal">#general</code>.
   </p><div class="verbatim-wrap"><pre class="screen">https://slack.com/api/chat.postMessage?token=1234567890&amp;channel=#general</pre></div><p>
    Find more information on the Slack Web API here:
    <a class="link" href="https://api.slack.com/web" target="_blank">https://api.slack.com/web</a>
   </p></li><li class="listitem "><p>
    Use the CLI on your Cloud Lifecycle Manager to create a new Slack notification
    type, using the API call that you created in the preceding step. The
    following example creates a notification type named
    <span class="bold"><strong>MySlackNotification</strong></span>, using token
    <span class="bold"><strong>1234567890</strong></span>, and posting to channel
    <span class="bold"><strong>#general</strong></span>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  MySlackNotification SLACK https://slack.com/api/chat.postMessage?token=1234567890&amp;channel=#general</pre></div></li></ol></div><div id="id-1.6.14.3.5.5.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   Notification types can also be created in the Horizon dashboard.
  </p></div></div><div class="sect3" id="monasca-jira-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the JIRA Plugin</span> <a title="Permalink" class="permalink" href="#monasca-jira-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monasca_jira_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_jira_plugin.xml</li><li><span class="ds-label">ID: </span>monasca-jira-plugin</li></ul></div></div></div></div><p>
  Configuring the JIRA plugin requires three pieces of information from your
  JIRA system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The URL of your JIRA system.
   </p></li><li class="listitem "><p>
    Username and password of a JIRA account that will be used to authenticate
    the notifications.
   </p></li><li class="listitem "><p>
    The name of the JIRA project that the notifications will be sent to.
   </p></li></ul></div><p>
  <span class="bold"><strong>Create JIRA notification type</strong></span>
 </p><p>
  You will configure the Monasca service to send notifications to a particular
  JIRA project. You must also configure JIRA to create new issues for each
  notification it receives to this project, however, that configuration is
  outside the scope of this document.
 </p><p>
  The Monasca JIRA notification plugin supports only the following two JIRA
  issue fields.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <em class="replaceable ">PROJECT</em>. This is the only supported
    <span class="quote">“<span class="quote ">mandatory</span>”</span> JIRA issue field.
   </p></li><li class="listitem "><p>
    <em class="replaceable ">COMPONENT</em>. This is the only supported
    <span class="quote">“<span class="quote ">optional</span>”</span> JIRA issue field.
   </p></li></ul></div><p>
  The JIRA issue type that your notifications will create may only be
  configured with the "Project" field as mandatory. If your JIRA issue type has
  any other mandatory fields, the Monasca plugin will not function correctly.
  Currently, the Monasca plugin only supports the single optional "component"
  field.
 </p><p>
  Creating the JIRA notification type requires a few more steps than other
  notification types covered in this guide. Because the Python and YAML files
  for this notification type are not yet included in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you must
  perform the following steps to manually retrieve and place them on your
  Cloud Lifecycle Manager.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Configure the JIRA plugin by adding the following block to the
    <code class="filename">/etc/monasca/notification.yaml</code> file, under the
    <code class="literal">notification_types</code> section, and adding the username and
    password of the JIRA account used for the notifications to the respective
    sections.
   </p><div class="verbatim-wrap"><pre class="screen">    plugins:

     - monasca_notification.plugins.jira_notifier:JiraNotifier

    jira:
        user:

        password:

        timeout: 60</pre></div><p>
    After adding the necessary block, the <code class="literal">notification_types</code>
    section should look like the following example. Note that you must also add
    the username and password for the JIRA user related to the notification
    type.
   </p><div class="verbatim-wrap"><pre class="screen">notification_types:
    plugins:

     - monasca_notification.plugins.jira_notifier:JiraNotifier

    jira:
        user:

        password:

        timeout: 60

    webhook:
        timeout: 5

    pagerduty:
        timeout: 5

        url: "https://events.pagerduty.com/generic/2010-04-15/create_event.json"</pre></div></li><li class="listitem "><p>
    Create the JIRA notification type. The following command example creates a
    JIRA notification type named
    <code class="literal">MyJiraNotification</code>, in the JIRA project
    <code class="literal">HISO</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  MyJiraNotification JIRA https://jira.hpcloud.net/?project=HISO</pre></div><p>
    The following command example creates a JIRA notification type named
    <code class="literal">MyJiraNotification</code>, in the JIRA project
    <code class="literal">HISO</code>, and adds the optional
    <em class="replaceable ">component</em> field with a value of
    <code class="literal">keystone</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create MyJiraNotification JIRA https://jira.hpcloud.net/?project=HISO&amp;component=keystone</pre></div><div id="id-1.6.14.3.5.6.10.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     There is a slash (<code class="literal">/</code>) separating the URL path and the
     query string. The
     slash is required if you have a query parameter without a path parameter.
    </p></div><div id="id-1.6.14.3.5.6.10.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     Notification types may also be created in the Horizon dashboard.
    </p></div></li></ol></div></div></div><div class="sect2" id="alarm-metrics"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Metrics</span> <a title="Permalink" class="permalink" href="#alarm-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-alarm_metrics.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-alarm_metrics.xml</li><li><span class="ds-label">ID: </span>alarm-metrics</li></ul></div></div></div></div><p>
  You can use the available metrics to create custom alarms to further monitor
  your cloud infrastructure and facilitate autoscaling features.
 </p><p>
  For details on how to create customer alarms using the Operations Console,
  see <span class="intraxref">Book “Operations Console”, Chapter 1 “Alarm Definition”</span>.
 </p><div class="sect3" id="apache-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Apache Metrics</span> <a title="Permalink" class="permalink" href="#apache-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-apache_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-apache_metrics.xml</li><li><span class="ds-label">ID: </span>apache-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Apache service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>apache.net.hits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total accesses</td></tr><tr><td>apache.net.kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total Kbytes per second</td></tr><tr><td>apache.net.requests_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total accesses per second</td></tr><tr><td>apache.net.total_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total Kbytes</td></tr><tr><td>apache.performance.busy_worker_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>The number of workers serving requests</td></tr><tr><td>apache.performance.cpu_load_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>
      <p>
       The current percentage of CPU used by each worker and in total by all
       workers combined
      </p>
     </td></tr><tr><td>apache.performance.idle_worker_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>The number of idle workers</td></tr><tr><td>apache.status</td><td>
<div class="verbatim-wrap"><pre class="screen">apache_port
hostname
service=apache
component=apache</pre></div>
     </td><td>Status of Apache port</td></tr></tbody></table></div></div><div class="sect3" id="ceilometer-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metrics</span> <a title="Permalink" class="permalink" href="#ceilometer-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-ceilometer_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ceilometer_metrics.xml</li><li><span class="ds-label">ID: </span>ceilometer-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Ceilometer service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>disk.total_space_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total space of disk</td></tr><tr><td>disk.total_used_space_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total used space of disk</td></tr><tr><td>swiftlm.diskusage.rate_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>swiftlm.diskusage.val.avail_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>swiftlm.diskusage.val.size_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>image</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Existence of the image</td></tr><tr><td>image.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Delete operation on this image</td></tr><tr><td>image.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=B,
source=openstack</pre></div>
     </td><td>Size of the uploaded image</td></tr><tr><td>image.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Update operation on this image</td></tr><tr><td>image.upload</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Upload operation on this image</td></tr><tr><td>instance</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=instance,
source=openstack</pre></div>
     </td><td>Existence of instance</td></tr><tr><td>disk.ephemeral.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of ephemeral disk on this instance</td></tr><tr><td>disk.root.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of root disk on this instance</td></tr><tr><td>memory</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=MB,
source=openstack</pre></div>
     </td><td>Size of memory on this instance</td></tr><tr><td>ip.floating</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=ip,
source=openstack</pre></div>
     </td><td>Existence of IP</td></tr><tr><td>ip.floating.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=ip,
source=openstack</pre></div>
     </td><td>Create operation on this fip</td></tr><tr><td>ip.floating.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=ip,
source=openstack</pre></div>
     </td><td>Update operation on this fip</td></tr><tr><td>mem.total_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total space of memory</td></tr><tr><td>mem.usable_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Available space of memory</td></tr><tr><td>network</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=network,
source=openstack</pre></div>
     </td><td>Existence of network</td></tr><tr><td>network.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Create operation on this network</td></tr><tr><td>network.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Update operation on this network</td></tr><tr><td>network.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Delete operation on this network</td></tr><tr><td>port</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=port,
source=openstack</pre></div>
     </td><td>Existence of port</td></tr><tr><td>port.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Create operation on this port</td></tr><tr><td>port.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Delete operation on this port</td></tr><tr><td>port.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Update operation on this port</td></tr><tr><td>router</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=router,
source=openstack</pre></div>
     </td><td>Existence of router</td></tr><tr><td>router.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Create operation on this router</td></tr><tr><td>router.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Delete operation on this router</td></tr><tr><td>router.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Update operation on this router</td></tr><tr><td>snapshot</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Existence of the snapshot</td></tr><tr><td>snapshot.create.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Create operation on this snapshot</td></tr><tr><td>snapshot.delete.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Delete operation on this snapshot</td></tr><tr><td>snapshot.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of this snapshot</td></tr><tr><td>subnet</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=subnet,
source=openstack</pre></div>
     </td><td>Existence of the subnet</td></tr><tr><td>subnet.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Create operation on this subnet</td></tr><tr><td>subnet.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Delete operation on this subnet</td></tr><tr><td>subnet.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Update operation on this subnet</td></tr><tr><td>vcpus</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=vcpus,
source=openstack</pre></div>
     </td><td>Number of virtual CPUs allocated to the instance</td></tr><tr><td>vcpus_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id</pre></div>
     </td><td>Number of vcpus used by a project</td></tr><tr><td>volume</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=volume,
source=openstack</pre></div>
     </td><td>Existence of the volume</td></tr><tr><td>volume.create.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Create operation on this volume</td></tr><tr><td>volume.delete.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Delete operation on this volume</td></tr><tr><td>volume.resize.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Resize operation on this volume</td></tr><tr><td>volume.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of this volume</td></tr><tr><td>volume.update.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Update operation on this volume</td></tr><tr><td>storage.objects</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=object,
source=openstack</pre></div>
     </td><td>Number of objects</td></tr><tr><td>storage.objects.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=B,
source=openstack</pre></div>
     </td><td>Total size of stored objects</td></tr><tr><td>storage.objects.containers</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=container,
source=openstack</pre></div>
     </td><td>Number of containers</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-cinder-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cinder Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-cinder-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-cinder_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-cinder_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-cinder-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Cinder service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cinderlm.cinder.backend.physical.list</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backends
      </p>
     </td><td> List of physical backends</td></tr><tr><td>cinderlm.cinder.backend.total.avail</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backendname
      </p>
     </td><td>Total available capacity metric per backend</td></tr><tr><td>cinderlm.cinder.backend.total.size</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backendname
      </p>
     </td><td>Total capacity metric per backend</td></tr><tr><td>cinderlm.cinder.cinder_services</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component
      </p>
     </td><td>Status of a cinder-volume service</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.logical_drive</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, sub_component, logical_drive, controller_slot, array
      </p>
      <p>
       The HPE Smart Storage Administrator (HPE SSA) CLI component will have to be
       installed for SSACLI status to be reported. To download and install the
       SSACLI utility to enable management of disk controllers, please refer
       to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
      </p>
     </td><td>Status of a logical drive</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.physical_drive</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, box, bay, controller_slot
      </p>
     </td><td>Status of a logical drive</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.smart_array</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, sub_component, model
      </p>
     </td><td>Status of smart array</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.smart_array.firmware</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, model
      </p>
     </td><td>Checks firmware version</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-compute-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-compute-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-compute_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-compute_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-compute-metrics-xml-1</li></ul></div></div></div></div><div id="id-1.6.14.3.6.7.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   Compute instance metrics are listed in <a class="xref" href="#libvirt-metrics" title="12.1.4.11. Libvirt Metrics">Section 12.1.4.11, “Libvirt Metrics”</a>.
  </p></div><p>
  A list of metrics associated with the Compute service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>nova.heartbeat</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
cloud_name
hostname
component
control_plane
cluster</pre></div>
     </td><td>
      <p>
       Checks that all services are running heartbeats (uses nova user and to
       list services then sets up checks for each. For example, nova-scheduler,
       nova-conductor, nova-consoleauth, nova-compute)
      </p>
     </td></tr><tr><td>nova.vm.cpu.total_allocated</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total CPUs allocated across all VMs</td></tr><tr><td>nova.vm.disk.total_allocated_gb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total Gbytes of disk space allocated to all VMs</td></tr><tr><td>nova.vm.mem.total_allocated_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total Mbytes of memory allocated to all VMs</td></tr></tbody></table></div></div><div class="sect3" id="crash-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Crash Metrics</span> <a title="Permalink" class="permalink" href="#crash-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-crash_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crash_metrics.xml</li><li><span class="ds-label">ID: </span>crash-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Crash service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>crash.dump_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>Number of crash dumps found</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-directory-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Directory Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-directory-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-directory_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-directory_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-directory-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Directory service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>directory.files_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service
hostname
path</pre></div>
     </td><td>Total number of files under a specific directory path</td></tr><tr><td>directory.size_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service
hostname
path</pre></div>
     </td><td>Total size of a specific directory path</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-elasticsearch-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Elasticsearch Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-elasticsearch-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-elasticsearch_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-elasticsearch_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-elasticsearch-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Elasticsearch service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>elasticsearch.active_primary_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Indicates the number of primary shards in your cluster. This is an
       aggregate total across all indices.
      </p>
     </td></tr><tr><td>elasticsearch.active_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Aggregate total of all shards across all indices, which includes replica
       shards.
      </p>
     </td></tr><tr><td>elasticsearch.cluster_status</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Cluster health status.
      </p>
     </td></tr><tr><td>elasticsearch.initializing_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       The count of shards that are being freshly created.
      </p>
     </td></tr><tr><td>elasticsearch.number_of_data_nodes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Number of data nodes.
      </p>
     </td></tr><tr><td>elasticsearch.number_of_nodes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Number of nodes.
      </p>
     </td></tr><tr><td>elasticsearch.relocating_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Shows the number of shards that are currently moving from one node to
       another node.
      </p>
     </td></tr><tr><td>elasticsearch.unassigned_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       The number of unassigned shards from the master node.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-haproxy-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HAProxy Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-haproxy-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-haproxy_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-haproxy_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-haproxy-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the HAProxy service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>haproxy.backend.bytes.in_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.bytes.out_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.denied.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.denied.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.errors.con_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.errors.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.queue.current</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.1xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.2xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.3xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.4xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.5xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.other</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.current</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.limit</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.pct</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.warnings.redis_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.warnings.retr_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.bytes.in_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.bytes.out_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.denied.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.denied.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.errors.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.requests.rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.1xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.2xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.3xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.4xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.5xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.other</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.current</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.limit</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.pct</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.rate</td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect3" id="httpcheck-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HTTP Check Metrics</span> <a title="Permalink" class="permalink" href="#httpcheck-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-httpcheck_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-httpcheck_metrics.xml</li><li><span class="ds-label">ID: </span>httpcheck-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the HTTP Check service:
 </p><div class="table" id="id-1.6.14.3.6.12.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.2: </span><span class="name">HTTP Check Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.12.3">#</a></h6></div><div class="table-contents"><table class="table" summary="HTTP Check Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>http_response_time</td><td>
<div class="verbatim-wrap"><pre class="screen">url
hostname
service
component</pre></div>
     </td><td>The response time in seconds of the http endpoint call.</td></tr><tr><td>http_status</td><td>
<div class="verbatim-wrap"><pre class="screen">url
hostname
service</pre></div>
     </td><td>The status of the http endpoint call (0 = success, 1 = failure).</td></tr></tbody></table></div></div><p>
  For each component and HTTP metric name there are two separate metrics
  reported, one for the local URL and another for the virtual IP (VIP) URL:
 </p><div class="table" id="id-1.6.14.3.6.12.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.3: </span><span class="name">HTTP Metric Components </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.12.5">#</a></h6></div><div class="table-contents"><table class="table" summary="HTTP Metric Components" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>account-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=account-server
url</pre></div>
     </td><td>swift account-server http endpoint status and response time</td></tr><tr><td>barbican-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=key-manager
component=barbican-api
url</pre></div>
     </td><td>barbican-api http endpoint status and response time</td></tr><tr><td>ceilometer-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
component=ceilometer-api
url</pre></div>
     </td><td>ceilometer-api http endpoint status and response time</td></tr><tr><td>cinder-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
component=cinder-api
url</pre></div>
     </td><td>cinder-api http endpoint status and response time</td></tr><tr><td>container-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=container-server
url</pre></div>
     </td><td>swift container-server http endpoint status and response time</td></tr><tr><td>designate-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
component=designate-api
url</pre></div>
     </td><td>designate-api http endpoint status and response time</td></tr><tr><td>freezer-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=backup
component=freezer-api
url</pre></div>
     </td><td>freezer-api http endpoint status and response time</td></tr><tr><td>glance-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=image-service
component=glance-api
url</pre></div>
     </td><td>glance-api http endpoint status and response time</td></tr><tr><td>glance-registry</td><td>
<div class="verbatim-wrap"><pre class="screen">service=image-service
component=glance-registry
url</pre></div>
     </td><td>glance-registry http endpoint status and response time</td></tr><tr><td>heat-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api
url</pre></div>
     </td><td>heat-api http endpoint status and response time</td></tr><tr><td>heat-api-cfn</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api-cfn
url</pre></div>
     </td><td>heat-api-cfn http endpoint status and response time</td></tr><tr><td>heat-api-cloudwatch</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api-cloudwatch
url</pre></div>
     </td><td>heat-api-cloudwatch http endpoint status and response time</td></tr><tr><td>ardana-ux-services</td><td>
<div class="verbatim-wrap"><pre class="screen">service=ardana-ux-services
component=ardana-ux-services
url</pre></div>
     </td><td>ardana-ux-services http endpoint status and response time</td></tr><tr><td>horizon</td><td>
<div class="verbatim-wrap"><pre class="screen">service=web-ui
component=horizon
url</pre></div>
     </td><td>horizon http endpoint status and response time</td></tr><tr><td>keystone-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
component=keystone-api
url</pre></div>
     </td><td>keystone-api http endpoint status and response time</td></tr><tr><td>monasca-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
component=monasca-api
url</pre></div>
     </td><td>monasca-api http endpoint status</td></tr><tr><td>monasca-persister</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
component=monasca-persister
url</pre></div>
     </td><td>monasca-persister http endpoint status</td></tr><tr><td>neutron-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
component=neutron-server
url</pre></div>
     </td><td>neutron-server http endpoint status and response time</td></tr><tr><td>neutron-server-vip</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
component=neutron-server-vip
url</pre></div>
     </td><td>neutron-server-vip http endpoint status and response time</td></tr><tr><td>nova-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
component=nova-api
url</pre></div>
     </td><td>nova-api http endpoint status and response time</td></tr><tr><td>nova-vnc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
component=nova-vnc
url</pre></div>
     </td><td>nova-vnc http endpoint status and response time</td></tr><tr><td>object-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=object-server
url</pre></div>
     </td><td>object-server http endpoint status and response time</td></tr><tr><td>object-storage-vip</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=object-storage-vip
url</pre></div>
     </td><td>object-storage-vip http endpoint status and response time</td></tr><tr><td>octavia-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
component=octavia-api
url</pre></div>
     </td><td>octavia-api http endpoint status and response time</td></tr><tr><td>ops-console-web</td><td>
<div class="verbatim-wrap"><pre class="screen">service=ops-console
component=ops-console-web
url</pre></div>
     </td><td>ops-console-web http endpoint status and response time</td></tr><tr><td>proxy-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=proxy-server
url</pre></div>
     </td><td>proxy-server http endpoint status and response time</td></tr></tbody></table></div></div></div><div class="sect3" id="kafka-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Kafka Metrics</span> <a title="Permalink" class="permalink" href="#kafka-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-kafka_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-kafka_metrics.xml</li><li><span class="ds-label">ID: </span>kafka-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Kafka service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>kafka.consumer_lag</td><td>
<div class="verbatim-wrap"><pre class="screen">topic
service
component=kafka
consumer_group
hostname</pre></div>
     </td><td>Hostname consumer offset lag from broker offset</td></tr></tbody></table></div></div><div class="sect3" id="libvirt-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Libvirt Metrics</span> <a title="Permalink" class="permalink" href="#libvirt-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-libvirt_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_metrics.xml</li><li><span class="ds-label">ID: </span>libvirt-metrics</li></ul></div></div></div></div><div id="id-1.6.14.3.6.14.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   For information on how to turn these metrics on and off using the tuning
   knobs, see <a class="xref" href="#libvirt-tuningknobs" title="12.1.2.5.1. Libvirt plugin metric tuning knobs">Section 12.1.2.5.1, “Libvirt plugin metric tuning knobs”</a>.
  </p></div><p>
  A list of metrics associated with the Libvirt service.
 </p><div class="table" id="id-1.6.14.3.6.14.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.4: </span><span class="name">Tunable Libvirt Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.14.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Tunable Libvirt Metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>vm.cpu.time_ns</td><td>cpu.time_ns</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Cumulative CPU time (in ns)</td></tr><tr><td>vm.cpu.utilization_norm_perc</td><td>cpu.utilization_norm_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Normalized CPU utilization (percentage)</td></tr><tr><td>vm.cpu.utilization_perc</td><td>cpu.utilization_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Overall CPU utilization (percentage)</td></tr><tr><td>vm.io.errors</td><td>io.errors</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Overall disk I/O errors</td></tr><tr><td>vm.io.errors_sec</td><td>io.errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O errors per second</td></tr><tr><td>vm.io.read_bytes</td><td>io.read_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read bytes value</td></tr><tr><td>vm.io.read_bytes_sec</td><td>io.read_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read bytes per second</td></tr><tr><td>vm.io.read_ops</td><td>io.read_ops</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read operations value</td></tr><tr><td>vm.io.read_ops_sec</td><td>io.read_ops_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations per second</td></tr><tr><td>vm.io.write_bytes</td><td>io.write_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write bytes value</td></tr><tr><td>vm.io.write_bytes_sec</td><td>io.write_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write bytes per second</td></tr><tr><td>vm.io.write_ops</td><td>io.write_ops</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations value</td></tr><tr><td>vm.io.write_ops_sec</td><td> io.write_ops_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations per second</td></tr><tr><td>vm.net.in_bytes</td><td>net.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received total bytes</td></tr><tr><td>vm.net.in_bytes_sec</td><td>net.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received bytes per second</td></tr><tr><td>vm.net.in_packets</td><td>net.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received total packets</td></tr><tr><td>vm.net.in_packets_sec</td><td>net.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received packets per second</td></tr><tr><td>vm.net.out_bytes</td><td>net.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted total bytes</td></tr><tr><td>vm.net.out_bytes_sec</td><td>net.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted bytes per second</td></tr><tr><td>vm.net.out_packets</td><td>net.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted total packets</td></tr><tr><td>vm.net.out_packets_sec</td><td>net.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted packets per second</td></tr><tr><td>vm.ping_status</td><td>ping_status</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>0 for ping success, 1 for ping failure</td></tr><tr><td>vm.disk.allocation</td><td>disk.allocation</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk allocation for a device</td></tr><tr><td>vm.disk.allocation_total</td><td>disk.allocation_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk allocation across devices for instances</td></tr><tr><td>vm.disk.capacity</td><td>disk.capacity</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk capacity for a device</td></tr><tr><td>vm.disk.capacity_total</td><td>disk.capacity_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk capacity across devices for instances</td></tr><tr><td>vm.disk.physical</td><td>disk.physical</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk usage for a device</td></tr><tr><td>vm.disk.physical_total</td><td>disk.physical_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk usage across devices for instances</td></tr><tr><td>vm.io.errors_total</td><td>io.errors_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O errors across all devices</td></tr><tr><td>vm.io.errors_total_sec</td><td>io.errors_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O errors per second across all devices</td></tr><tr><td>vm.io.read_bytes_total</td><td>io.read_bytes_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read bytes across all devices</td></tr><tr><td>vm.io.read_bytes_total_sec</td><td>io.read_bytes_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read bytes per second across devices</td></tr><tr><td>vm.io.read_ops_total</td><td>io.read_ops_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read operations across all devices</td></tr><tr><td>vm.io.read_ops_total_sec</td><td>io.read_ops_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read operations across all devices per sec</td></tr><tr><td>vm.io.write_bytes_total</td><td>io.write_bytes_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write bytes across all devices</td></tr><tr><td>vm.io.write_bytes_total_sec</td><td>io.write_bytes_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O Write bytes per second across devices</td></tr><tr><td>vm.io.write_ops_total</td><td>io.write_ops_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write operations across all devices</td></tr><tr><td>vm.io.write_ops_total_sec</td><td>io.write_ops_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write operations across all devices per sec</td></tr></tbody></table></div></div><p>
  These metrics in libvirt are always enabled and cannot be disabled using the
  tuning knobs.
 </p><div class="table" id="id-1.6.14.3.6.14.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.5: </span><span class="name">Untunable Libvirt Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.14.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Untunable Libvirt Metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>vm.host_alive_status</td><td>host_alive_status</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>
      <p>
       -1 for no status, 0 for Running / OK, 1 for Idle / blocked, 2 for
       Paused,
      </p>
      <p>
       3 for Shutting down, 4 for Shut off or Nova suspend 5 for Crashed,
      </p>
      <p>
       6 for Power management suspend (S3 state)
      </p>
     </td></tr><tr><td>vm.mem.free_mb</td><td>mem.free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Free memory in Mbytes</td></tr><tr><td>vm.mem.free_perc</td><td>mem.free_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Percent of memory free</td></tr><tr><td>vm.mem.resident_mb</td><td> </td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Total memory used on host, an Operations-only metric</td></tr><tr><td>vm.mem.swap_used_mb</td><td>mem.swap_used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Used swap space in Mbytes</td></tr><tr><td>vm.mem.total_mb</td><td>mem.total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Total memory in Mbytes</td></tr><tr><td>vm.mem.used_mb</td><td>mem.used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Used memory in Mbytes</td></tr></tbody></table></div></div></div><div class="sect3" id="idg-all-operations-monitoring-monasca-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monasca-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monasca_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monasca-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Monitoring service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>alarm-state-transitions-added-to-batch-counter</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-persister</pre></div>
     </td><td> </td></tr><tr><td>jvm.memory.total.max</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Maximum JVM overall memory</td></tr><tr><td>jvm.memory.total.used</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Used JVM overall memory</td></tr><tr><td>metrics-added-to-batch-counter</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-persister</pre></div>
     </td><td> </td></tr><tr><td>metrics.published</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-api</pre></div>
     </td><td>Total number of published metrics</td></tr><tr><td>monasca.alarms_finished_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Total number of alarms received</td></tr><tr><td>monasca.checks_running_too_long</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-agent
service=monitoring
cluster</pre></div>
     </td><td>Only emitted when collection time for a check is too long</td></tr><tr><td>monasca.collection_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-agent
service=monitoring
cluster</pre></div>
     </td><td>Collection time in monasca-agent</td></tr><tr><td>monasca.config_db_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.created_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Number of notifications created</td></tr><tr><td>monasca.invalid_type_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Number of notifications with invalid type</td></tr><tr><td>monasca.log.in_bulks_rejected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs_rejected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs_lost</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs_truncated_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.processing_time_ms</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.publish_time_ms</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.thread_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name
hostname
component</pre></div>
     </td><td>Number of threads monasca is using</td></tr><tr><td>raw-sql.time.avg</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Average raw sql query time</td></tr><tr><td>raw-sql.time.max</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Max raw sql query time</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-monasca-agg-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Aggregated Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monasca-agg-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-monasca_agg_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_agg_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monasca-agg-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of the aggregated metrics associated with the Monasca Transform
  feature.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="newCol2" /><col class="newCol3" /><col class="c3" /><col class="c2" /></colgroup><thead><tr><th>Metric Name</th><th>For</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cpu.utilized_logical_cores_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Utilized physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour).
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>cpu.total_logical_cores_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Total physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>mem.total_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Total physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>mem.usable_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>Usable physical host memory capacity by time interval (defaults to a
                hour)</td></tr><tr><td>disk.total_used_space_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Utilized physical host disk capacity by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>disk.total_space_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>Total physical host disk capacity by time interval (defaults to a hour)</td></tr><tr><td>nova.vm.cpu.total_allocated_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       CPUs allocated across all virtual machines by time interval (defaults to
       a hour)
      </p>
     </td></tr><tr><td>vcpus_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Virtual CPUs allocated capacity for virtual machines of one or all
       projects by time interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>nova.vm.mem.total_allocated_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Memory allocated to all virtual machines by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>vm.mem.used_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Memory utilized by virtual machines of one or all projects by time
       interval (defaults to an hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>vm.mem.total_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Memory allocated to virtual machines of one or all projects by time
       interval (defaults to an hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>vm.cpu.utilization_perc_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       CPU utilized by all virtual machines by project by time interval
       (defaults to an hour)
      </p>
     </td></tr><tr><td>nova.vm.disk.total_allocated_gb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Disk space allocated to all virtual machines by time interval (defaults
       to an hour)
      </p>
     </td></tr><tr><td>vm.disk.allocation_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Disk allocation for virtual machines of one or all projects by time
       interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.val.size_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Total available object storage capacity by time interval (defaults to a
       hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.val.avail_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Remaining object storage capacity by time interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.rate_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Rate of change of object storage usage by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>storage.objects.size_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Used object storage capacity by time interval (defaults to a hour)
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="mysql-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MySQL Metrics</span> <a title="Permalink" class="permalink" href="#mysql-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-mysql_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-mysql_metrics.xml</li><li><span class="ds-label">ID: </span>mysql-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the MySQL service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>mysql.innodb.buffer_pool_free</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The number of free pages, in bytes. This value is calculated by
       multiplying <code class="literal">Innodb_buffer_pool_pages_free</code> and
       <code class="literal">Innodb_page_size</code> of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.buffer_pool_total</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The total size of buffer pool, in bytes. This value is calculated by
       multiplying <code class="literal">Innodb_buffer_pool_pages_total</code> and
       <code class="literal">Innodb_page_size</code> of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.buffer_pool_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The number of used pages, in bytes. This value is calculated by
       subtracting <code class="literal">Innodb_buffer_pool_pages_total</code> away from
       <code class="literal">Innodb_buffer_pool_pages_free</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.innodb.current_row_locks</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to current row locks of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.data_reads</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_data_reads</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.data_writes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_data_writes</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_os_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to the OS waits of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_spin_rounds</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to spinlock rounds of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_spin_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to the spin waits of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.os_log_fsyncs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_os_log_fsyncs</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.row_lock_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_row_lock_time</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.row_lock_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_row_lock_waits</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.net.connections</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Connections</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.net.max_connections</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Max_used_connections</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_delete</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_delete</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_delete_multi</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_delete_multi</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_insert</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_insert</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_insert_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_insert_select</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_replace_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_replace_select</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_select</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_update</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_update</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_update_multi</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_update_multi</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_disk_tables</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_disk_tables</code> of the
       server status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_files</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_files</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_tables</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_tables</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.kernel_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The kernel time for the databases performance, in seconds.
      </p>
     </td></tr><tr><td>mysql.performance.open_files</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Open_files</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.qcache_hits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Qcache_hits</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.queries</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Queries</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.questions</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Question</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.slow_queries</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Slow_queries</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.table_locks_waited</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Table_locks_waited</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.threads_connected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Threads_connected</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.user_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The CPU user time for the databases performance, in seconds.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-ntp-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NTP Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-ntp-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-ntp_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ntp_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-ntp-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the NTP service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ntp.connection_status</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
ntp_server</pre></div>
     </td><td>Value of ntp server connection status (0=Healthy)</td></tr><tr><td>ntp.offset</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
ntp_server</pre></div>
     </td><td>Time offset in seconds</td></tr></tbody></table></div></div><div class="sect3" id="sec-metric-ovs"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Open vSwitch (OVS) Metrics</span> <a title="Permalink" class="permalink" href="#sec-metric-ovs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-ovs_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_metrics.xml</li><li><span class="ds-label">ID: </span>sec-metric-ovs</li></ul></div></div></div></div><p>
  A list of metrics associated with the OVS service.
 </p><div id="id-1.6.14.3.6.19.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   For information on how to turn these metrics on and off using the tuning
   knobs, see <a class="xref" href="#ovs-tuningknobs" title="12.1.2.5.2. OVS plugin metric tuning knobs">Section 12.1.2.5.2, “OVS plugin metric tuning knobs”</a>.
  </p></div><div class="table" id="id-1.6.14.3.6.19.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.6: </span><span class="name">Per-router metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.19.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Per-router metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ovs.vrouter.in_bytes_sec</td><td>vrouter.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes per second for the router (if
       <code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vrouter.in_packets_sec</td><td>vrouter.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_bytes_sec</td><td>vrouter.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes per second for the router (if
       <code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vrouter.out_packets_sec</td><td>vrouter.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_bytes</td><td>vrouter.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes for the router (if <code class="literal">network_use_bits</code> is
       false)
      </p>
     </td></tr><tr><td>ovs.vrouter.in_packets</td><td>vrouter.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_bytes</td><td>vrouter.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes for the router (if <code class="literal">network_use_bits</code> is
       false)
      </p>
     </td></tr><tr><td>ovs.vrouter.out_packets</td><td>vrouter.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_dropped_sec</td><td>vrouter.in_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_errors_sec</td><td>vrouter.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of incoming errors per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_dropped_sec</td><td>vrouter.out_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_errors_sec</td><td>vrouter.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of outgoing errors per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_dropped</td><td>vrouter.in_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_errors</td><td>vrouter.in_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of incoming errors for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_dropped</td><td>vrouter.out_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_errors</td><td>vrouter.out_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of outgoing errors for the router
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.6.14.3.6.19.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.7: </span><span class="name">Per-DHCP port and rate metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.19.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Per-DHCP port and rate metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Tenant Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ovs.vswitch.in_bytes_sec</td><td>vswitch.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming Bytes per second on DHCP
       port(if<code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.in_packets_sec</td><td>vswitch.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_bytes_sec</td><td>vswitch.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing Bytes per second on DHCP
       port(if<code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.out_packets_sec</td><td>vswitch.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_bytes</td><td>vswitch.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes for the DHCP port (if <code class="literal">network_use_bits</code>
       is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.in_packets</td><td>vswitch.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_bytes</td><td>vswitch.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes for the DHCP port (if <code class="literal">network_use_bits</code>
       is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.out_packets</td><td>vswitch.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_dropped_sec</td><td>vswitch.in_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_errors_sec</td><td>vswitch.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming errors per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_dropped_sec</td><td>vswitch.out_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_errors_sec</td><td>vswitch.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing errors per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_dropped</td><td>vswitch.in_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_errors</td><td>vswitch.in_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Errors received for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_dropped</td><td>vswitch.out_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_errors</td><td>vswitch.out_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Errors transmitted for the DHCP port
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect3" id="process-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Process Metrics</span> <a title="Permalink" class="permalink" href="#process-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span>process-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with processes.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>process.cpu_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Percentage of cpu being consumed by a process</td></tr><tr><td>process.io.read_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of reads by a process</td></tr><tr><td>process.io.read_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Kbytes read by a process</td></tr><tr><td>process.io.write_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of writes by a process</td></tr><tr><td>process.io.write_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Kbytes written by a process</td></tr><tr><td>process.mem.rss_mbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Amount of physical memory allocated to a process, including memory from shared
                libraries in Mbytes</td></tr><tr><td>process.open_file_descriptors</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of files being used by a process</td></tr><tr><td>process.pid_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of processes that exist with this process name</td></tr><tr><td>process.thread_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of threads a process is using</td></tr></tbody></table></div><div class="sect4" id="id-1.6.14.3.6.20.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.4.17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">process.cpu_perc, process.mem.rss_mbytes, process.pid_count and process.thread_count metrics</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.6.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>apache-storm</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-thresh
process_user=storm</pre></div>
      </td><td>apache-storm process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>barbican-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=key-manager
process_name=barbican-api</pre></div>
      </td><td>barbican-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>ceilometer-agent-notification</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
process_name=ceilometer-agent-notification</pre></div>
      </td><td>ceilometer-agent-notification process info: cpu percent, momory, pid count
                  and thread count</td></tr><tr><td>ceilometer-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
process_name=ceilometer-api</pre></div>
      </td><td>ceilometer-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>ceilometer-polling</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
process_name=ceilometer-polling</pre></div>
      </td><td>ceilometer-polling process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cinder-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
process_name=cinder-api</pre></div>
      </td><td>cinder-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cinder-scheduler</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
process_name=cinder-scheduler</pre></div>
      </td><td>cinder-scheduler process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-api</pre></div>
      </td><td>designate-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-central</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-central</pre></div>
      </td><td>designate-central process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-mdns</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-mdns</pre></div>
      </td><td>designate-mdns process cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-pool-manager</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-pool-manager</pre></div>
      </td><td>designate-pool-manager process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>freezer-scheduler</td><td>
<div class="verbatim-wrap"><pre class="screen">service=backup
process_name=freezer-scheduler</pre></div>
      </td><td>freezer-scheduler process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>heat-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api</pre></div>
      </td><td>heat-api process cpu percent, momory, pid count and thread count</td></tr><tr><td>heat-api-cfn</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api-cfn</pre></div>
      </td><td>heat-api-cfn process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>heat-api-cloudwatch</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api-cloudwatch</pre></div>
      </td><td>heat-api-cloudwatch process cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>heat-engine</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-engine</pre></div>
      </td><td>heat-engine process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>ipsec/charon</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=ipsec/charon</pre></div>
      </td><td>ipsec/charon process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>keystone-admin</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
process_name=keystone-admin</pre></div>
      </td><td>keystone-admin process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>keystone-main</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
process_name=keystone-main</pre></div>
      </td><td>keystone-main process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-agent</pre></div>
      </td><td>monasca-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-api</pre></div>
      </td><td>monasca-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-notification</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-notification</pre></div>
      </td><td>monasca-notification process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-persister</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-persister</pre></div>
      </td><td>monasca-persister process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-transform</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=monasca-transform</pre></div>
      </td><td>monasca-transform process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-dhcp-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-dhcp-agent</pre></div>
      </td><td>neutron-dhcp-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-l3-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-l3-agent</pre></div>
      </td><td>neutron-l3-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-lbaasv2-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name:neutron-lbaasv2-agent</pre></div>
      </td><td>neutron-lbaasv2-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-metadata-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-metadata-agent</pre></div>
      </td><td>neutron-metadata-agent process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>neutron-openvswitch-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-openvswitch-agent</pre></div>
      </td><td>neutron-openvswitch-agent process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>neutron-rootwrap</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-rootwrap</pre></div>
      </td><td>neutron-rootwrap process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-server</pre></div>
      </td><td>neutron-server process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-vpn-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-vpn-agent</pre></div>
      </td><td>neutron-vpn-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-api</pre></div>
      </td><td>nova-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-compute</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-compute</pre></div>
      </td><td>nova-compute process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-conductor</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-conductor</pre></div>
      </td><td>nova-conductor process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-consoleauth</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-consoleauth</pre></div>
      </td><td>nova-consoleauth process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-novncproxy</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-novncproxy</pre></div>
      </td><td>nova-novncproxy process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-scheduler</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-scheduler</pre></div>
      </td><td>nova-scheduler process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-api</pre></div>
      </td><td>octavia-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-health-manager</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-health-manager</pre></div>
      </td><td>octavia-health-manager process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>octavia-housekeeping</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-housekeeping</pre></div>
      </td><td>octavia-housekeeping process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-worker</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-worker</pre></div>
      </td><td>octavia-worker process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>org.apache.spark.deploy.master.Master</td><td>
<div class="verbatim-wrap"><pre class="screen">service=spark
process_name=org.apache.spark.deploy.master.Master</pre></div>
      </td><td>org.apache.spark.deploy.master.Master process info: cpu percent, momory, pid
                  count and thread count</td></tr><tr><td>org.apache.spark.executor.CoarseGrainedExecutorBackend</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=org.apache.spark.executor.CoarseGrainedExecutorBackend</pre></div>
      </td><td>org.apache.spark.executor.CoarseGrainedExecutorBackend process info: cpu
                  percent, momory, pid count and thread count</td></tr><tr><td> pyspark</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=pyspark</pre></div>
      </td><td>pyspark process info: cpu percent, momory, pid count and thread count</td></tr><tr><td>transform/lib/driver</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=transform/lib/driver</pre></div>
      </td><td>transform/lib/driver process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cassandra</td><td>
<div class="verbatim-wrap"><pre class="screen">service=cassandra
process_name=cassandra</pre></div>
      </td><td>cassandra process info: cpu percent, momory, pid count and thread count</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.14.3.6.20.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.1.4.17.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">process.io.*, process.open_file_descriptors metrics</span> <a title="Permalink" class="permalink" href="#id-1.6.14.3.6.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>monasca-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-agent
process_user=mon-agent</pre></div>
      </td><td>monasca-agent process info: number of reads, number of writes,number of files
                  being used</td></tr></tbody></table></div></div></div><div class="sect3" id="rabbitmq-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RabbitMQ Metrics</span> <a title="Permalink" class="permalink" href="#rabbitmq-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-rabbitmq_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-rabbitmq_metrics.xml</li><li><span class="ds-label">ID: </span>rabbitmq-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the RabbitMQ service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>rabbitmq.exchange.messages.published_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "publish_out" field of "message_stats" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.published_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_out_details" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.received_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "publish_in" field of "message_stats" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.received_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_in_details" object
      </p>
     </td></tr><tr><td>rabbitmq.node.fd_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "fd_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.mem_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "mem_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.run_queue</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "run_queue" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.sockets_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "sockets_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Sum of ready and unacknowledged messages (queue depth)
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.deliver_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/deliver_details" object
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.publish_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_details" object
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.redeliver_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/redeliver_details" object
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="swift-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Metrics</span> <a title="Permalink" class="permalink" href="#swift-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-swift_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-swift_metrics.xml</li><li><span class="ds-label">ID: </span>swift-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Swift service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>swiftlm.access.host.operation.get.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes read from objects in GET requests
       processed by this host during the last minute. Only successful GET
       requests to objects are counted. GET requests to the account or
       container is not included.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.ops</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is a count of the all the API requests made to Swift that
       were processed by this host during the last minute.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.project.get.bytes</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.project.ops</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.project.put.bytes</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.put.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes written to objects in PUT or POST
       requests processed by this host during the last minute. Only successful
       requests to objects are counted. Requests to the account or container is
       not included.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.status</td><td> </td><td> </td></tr><tr><td>swiftlm.access.project.operation.status</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports whether the swiftlm-access-log-tailer program is
       running normally.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.ops</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is a count of the all the API requests made to Swift that
       were processed by this host during the last minute to a given project
       id.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.get.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes read from objects in GET requests
       processed by this host for a given project during the last minute. Only
       successful GET requests to objects are counted. GET requests to the
       account or container is not included.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.put.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes written to objects in PUT or POST
       requests processed by this host for a given project during the last
       minute. Only successful requests to objects are counted. Requests to the
       account or container is not included.
      </p>
     </td></tr><tr><td>swiftlm.async_pending.cp.total.queue_length</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the total length of all async pending queues in the
       system.
      </p>
      <p>
       When a container update fails, the update is placed on the async pending
       queue. An update may fail becuase the container server is too busy or
       because the server is down or failed. Later the system will “replay”
       updates from the queue – so eventually, the container listings will
       show all objects known to the system.
      </p>
      <p>
       If you know that container servers are down, it is normal to see the
       value of async pending increase. Once the server is restored, the value
       should return to zero.
      </p>
      <p>
       A non-zero value may also indicate that containers are too large. Look
       for “lock timeout” messages in /var/log/swift/swift.log. If you find
       such messages consider reducing the container size or enable rate
       limiting.
      </p>
     </td></tr><tr><td>swiftlm.check.failure</td><td>
<div class="verbatim-wrap"><pre class="screen">check
error
component
service=object-storage</pre></div>
     </td><td>
      <p>
       The total exception string is truncated if longer than 1919 characters
       and an ellipsis is prepended in the first three characters of the
       message. If there is more than one error reported, the list of errors is
       paired to the last reported error and the operator is expected to
       resolve failures until no more are reported. Where there are no further
       reported errors, the Value Class is emitted as ‘Ok’.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.avg.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the average utilization of all drives in the system. The value is a
       percentage (example: 30.0 means 30% of the total space is used).
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.max.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the highest utilization of all drives in the system. The value is a
       percentage (example: 80.0 means at least one drive is 80% utilized). The
       value is just as important as swiftlm.diskusage.usage.avg. For example,
       if swiftlm.diskusage.usage.avg is 70% you might think that there is
       plenty of space available. However, if swiftlm.diskusage.usage.max is
       100%, this means that some objects cannot be stored on that drive. Swift
       will store replicas on other drives. However, this will create extra
       overhead.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.min.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the lowest utilization of all drives in the system. The value is a
       percentage (example: 10.0 means at least one drive is 10% utilized)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.avail</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of available (unused) space of all drives in the
       system. Only drives used by Swift are included.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.size</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of raw size of all drives in the system.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.used</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of used space of all drives in the system. Only
       drives used by Swift are included.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.avg.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the average percent usage of all Swift filesystems
       on a host.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.max.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a Swift filesystem that is most
       used (full) on a host. The value is the max of the percentage used of
       all Swift filesystems.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.min.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a Swift filesystem that is
       least used (has free space) on a host. The value is the min of the
       percentage used of all Swift filesystems.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.avail</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the number of bytes available (free) in a Swift
       filesystem. The value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.size</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the size in bytes of a Swift filesystem. The
       value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a Swift filesystem. The value
       is a floating point number in range 0.0 to 100.0
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the number of used bytes in a Swift filesystem.
       The value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.load.cp.avg.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the averaged value of the five minutes system load average of
       all nodes in the Swift system.
      </p>
     </td></tr><tr><td>swiftlm.load.cp.max.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the five minute load average of the busiest host in the Swift
       system.
      </p>
     </td></tr><tr><td>swiftlm.load.cp.min.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the five minute load average of the least loaded host in the
       Swift system.
      </p>
     </td></tr><tr><td>swiftlm.load.host.val.five</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the 5 minute load average of a host. The value is
       derived from <code class="literal">/proc/loadavg</code>.
      </p>
     </td></tr><tr><td>swiftlm.md5sum.cp.check.ring_checksums</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       If you are in the middle of deploying new rings, it is normal for this
       to be in the failed state.
      </p>
      <p>
       However, if you are not in the middle of a deployment, you need to
       investigate the cause. Use “swift-recon –md5 -v” to identify the
       problem hosts.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.account_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the account replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.container_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the container replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.object_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the object replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.account_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the account replicator last
       completed a scan on the host that has the oldest completion time.
       Normally the replicators runs periodically and hence this value will
       decrease whenever a replicator completes. However, if a replicator is
       not completing a cycle, this value increases (by one second for each
       second that the replicator is not completing). If the value remains high
       and increasing for a long period of time, it indicates that one of the
       hosts is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.container_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the container replicator last
       completed a scan on the host that has the oldest completion time.
       Normally the replicators runs periodically and hence this value will
       decrease whenever a replicator completes. However, if a replicator is
       not completing a cycle, this value increases (by one second for each
       second that the replicator is not completing). If the value remains high
       and increasing for a long period of time, it indicates that one of the
       hosts is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.object_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the object replicator last completed
       a scan on the host that has the oldest completion time. Normally the
       replicators runs periodically and hence this value will decrease
       whenever a replicator completes. However, if a replicator is not
       completing a cycle, this value increases (by one second for each second
       that the replicator is not completing). If the value remains high and
       increasing for a long period of time, it indicates that one of the hosts
       is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.swift.drive_audit</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount_point
kernel_device</pre></div>
     </td><td>
      <p>
       If an unrecoverable read error (URE) occurs on a filesystem, the error
       is logged in the kernel log. The swift-drive-audit program scans the
       kernel log looking for patterns indicating possible UREs.
      </p>
      <p>
       To get more information, log onto the node in question and run:
      </p>
<div class="verbatim-wrap"><pre class="screen">sudoswift-drive-audit/etc/swift/drive-audit.conf</pre></div>
      <p>
       UREs are common on large disk drives. They do not necessarily indicate
       that the drive is failed. You can use the xfs_repair command to attempt
       to repair the filesystem. Failing this, you may need to wipe the
       filesystem.
      </p>
      <p>
       If UREs occur very often on a specific drive, this may indicate that the
       drive is about to fail and should be replaced.
      </p>
     </td></tr><tr><td>swiftlm.swift.file_ownership.config</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service</pre></div>
     </td><td>
      <p>
       This metric reports if a directory or file has the appropriate owner.
       The check looks at Swift configuration directories and files. It also
       looks at the top-level directories of mounted file systems (for example,
       /srv/node/disk0 and /srv/node/disk0/objects).
      </p>
     </td></tr><tr><td>swiftlm.swift.file_ownership.data</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service</pre></div>
     </td><td>
      <p>
       This metric reports if a directory or file has the appropriate owner.
       The check looks at Swift configuration directories and files. It also
       looks at the top-level directories of mounted file systems (for example,
       /srv/node/disk0 and /srv/node/disk0/objects).
      </p>
     </td></tr><tr><td>swiftlm.swiftlm_check</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This indicates of the Swiftlm Monasca Agent Plug-in is running normally.
       If the status is failed, it probable that some or all metrics are no
       longer being reported.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.account.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.container.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.object.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.swift_services</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports of the process as named in the component dimension
       and the msg value_meta is running or not.
      </p>
      <p>
       Use the <code class="literal">swift-start.yml</code> playbook to attempt to
       restart the stopped process (it will start any process that has stopped
       – you do not need to specifically name the process).
      </p>
     </td></tr><tr><td>swiftlm.swift.swift_services.check_ip_port</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
component</pre></div>
     </td><td>Reports if a service is listening to the correct ip and port.</td></tr><tr><td>swiftlm.systems.check_mounts</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the mount state of each drive that should be mounted
       on this node.
      </p>
     </td></tr><tr><td>swiftlm.systems.connectivity.connect_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
url
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if a server can connect to a VIPs. Currently the
       following VIPs are checked:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         The Keystone VIP used to validate tokens (normally port 5000)
        </p></li></ul></div>
     </td></tr><tr><td>swiftlm.systems.connectivity.memcache_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
hostname
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if memcached on the host as specified by the
       hostname dimension is accepting connections from the host running the
       check. The following value_meta.msg are used:
      </p>
      <p>
       We successfully connected to &lt;hostname&gt; on port
       &lt;target_port&gt;
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "11211"
  },
  "metric": "swiftlm.systems.connectivity.memcache_check",
  "timestamp": 1449084058,
  "value": 0,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:11211 ok"
  }
}</pre></div>
      <p>
       We failed to connect to &lt;hostname&gt; on port &lt;target_port&gt;
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "fail_message": "[Errno 111] Connection refused",
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "11211"
  },
  "metric": "swiftlm.systems.connectivity.memcache_check",
  "timestamp": 1449084150,
  "value": 2,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:11211 [Errno 111] Connection refused"
  }
}</pre></div>
     </td></tr><tr><td>swiftlm.systems.connectivity.rsync_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
hostname
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if rsyncd on the host as specified by the hostname
       dimension is accepting connections from the host running the check. The
       following value_meta.msg are used:
      </p>
      <p>
       We successfully connected to &lt;hostname&gt; on port
       &lt;target_port&gt;:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "873"
  },
  "metric": "swiftlm.systems.connectivity.rsync_check",
  "timestamp": 1449082663,
  "value": 0,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:873 ok"
  }
}</pre></div>
      <p>
       We failed to connect to &lt;hostname&gt; on port &lt;target_port&gt;:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "fail_message": "[Errno 111] Connection refused",
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "873"
  },
  "metric": "swiftlm.systems.connectivity.rsync_check",
  "timestamp": 1449082860,
  "value": 2,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:873 [Errno 111] Connection refused"
  }
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.avg.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       Reports the average value of N-iterations of the latency values recorded
       for a component.
      </p>
     </td></tr><tr><td>swiftlm.umon.target.check.state</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the state of each component after N-iterations of
       checks. If the initial check succeeds, the checks move onto the next
       component until all components are queried, then the checks sleep for
       ‘main_loop_interval’ seconds. If a check fails, it is retried every
       second for ‘retries’ number of times per component. If the check
       fails ‘retries’ times, it is reported as a fail instance.
      </p>
      <p>
       A successful state will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.check.state",
    "timestamp": 1453111805,
    "value": 0
},</pre></div>
      <p>
       A failed state will report a “fail” value and the value_meta will
       provide the http response error.
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.check.state",
    "timestamp": 1453112841,
    "value": 2,
    "value_meta": {
        "msg": "HTTPConnectionPool(host='192.168.245.9', port=8080): Max retries exceeded with url: /v1/AUTH_76538ce683654a35983b62e333001b47 (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7fd857d7f550&gt;: Failed to establish a new connection: [Errno 110] Connection timed out',))"
    }
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.max.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the maximum response time in seconds of a REST call
       from the observer to the component REST API listening on the reported
       host
      </p>
      <p>
       A response time query will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.max.latency_sec",
    "timestamp": 1453111805,
    "value": 0.2772650718688965
}</pre></div>
      <p>
       A failed query will have a much longer time value:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.max.latency_sec",
    "timestamp": 1453112841,
    "value": 127.288015127182
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.min.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the minimum response time in seconds of a REST call
       from the observer to the component REST API listening on the reported
       host
      </p>
      <p>
       A response time query will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.min.latency_sec",
    "timestamp": 1453111805,
    "value": 0.10025882720947266
}</pre></div>
      <p>
       A failed query will have a much longer time value:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.min.latency_sec",
    "timestamp": 1453112841,
    "value": 127.25378203392029
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.val.avail_day</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the average of all the collected records in the
       swiftlm.umon.target.val.avail_minute metric data. This is a walking
       average data set of these approximately per-minute states of the Swift
       Object Store. The most basic case is a whole day of successful
       per-minute records, which will average to 100% availability. If there is
       any downtime throughout the day resulting in gaps of data which are two
       minutes or longer, the per-minute availability data will be “back
       filled” with an assumption of a down state for all the per-minute
       records which did not exist during the non-reported time. Because this
       is a walking average of approximately 24 hours worth of data, any
       outtage will take 24 hours to be purged from the dataset.
      </p>
      <p>
       A 24-hour average availability report:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_day",
    "timestamp": 1453645405,
    "value": 7.894736842105263
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.val.avail_minute</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       A value of 100 indicates that swift-uptime-monitor was able to get a
       token from Keystone and was able to perform operations against the Swift
       API during the reported minute. A value of zero indicates that either
       Keystone or Swift failed to respond successfully. A metric is produced
       every minute that swift-uptime-monitor is running.
      </p>
      <p>
       An “up” minute report value will report 100 [percent]:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_minute",
    "timestamp": 1453645405,
    "value": 100.0
}</pre></div>
      <p>
       A “down” minute report value will report 0 [percent]:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_minute",
    "timestamp": 1453649139,
    "value": 0.0
}</pre></div>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.smart_array.firmware</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
model
controller_slot</pre></div>
     </td><td>
      <p>
       This metric reports the firmware version of a component of a Smart Array
       controller.
      </p>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.smart_array</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
sub_component
model
controller_slot</pre></div>
     </td><td>
      <p>
       This reports the status of various sub-components of a Smart Array
       Controller.
      </p>
      <p>
       A failure is considered to have occured if:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Controller is failed
        </p></li><li class="listitem "><p>
         Cache is not enabled or has failed
        </p></li><li class="listitem "><p>
         Battery or capacitor is not installed
        </p></li><li class="listitem "><p>
         Battery or capacitor has failed
        </p></li></ul></div>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.physical_drive</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
controller_slot
box
bay</pre></div>
     </td><td>
      <p>
       This reports the status of a disk drive attached to a Smart Array
       controller.
      </p>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.logical_drive</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
controller_slot
array
logical_drive
sub_component</pre></div>
     </td><td>
      <p>
       This reports the status of a LUN presented by a Smart Array controller.
      </p>
      <p>
       A LUN is considered failed if the LUN has failed or if the LUN cache is
       not enabled and working.
      </p>
     </td></tr></tbody></table></div><div id="id-1.6.14.3.6.22.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     HPE Smart Storage Administrator (HPE SSA) CLI component will have to be
     installed on all control nodes that are Swift nodes, in order to generate
     the following Swift metrics:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.smart_array
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.logical_drive
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.smart_array.firmware
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.physical_drive
      </p></li></ul></div></li><li class="listitem "><p>
     HPE-specific binaries that are not based on open source are distributed
     directly from and supported by HPE. To download and install the SSACLI
     utility, please refer to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
    </p></li><li class="listitem "><p>
     After the HPE SSA CLI component is installed on the Swift nodes, the
     metrics will be generated automatically during the next agent polling
     cycle. Manual reboot of the node is not required.
    </p></li></ul></div></div></div><div class="sect3" id="system-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Metrics</span> <a title="Permalink" class="permalink" href="#system-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-system_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-system_metrics.xml</li><li><span class="ds-label">ID: </span>system-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the System.
 </p><div class="table" id="id-1.6.14.3.6.23.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.8: </span><span class="name">CPU Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.23.3">#</a></h6></div><div class="table-contents"><table class="table" summary="CPU Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cpu.frequency_mhz</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Maximum MHz value for the cpu frequency.
      </p>
      <div id="id-1.6.14.3.6.23.3.2.5.1.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        This value is dynamic, and driven by CPU governor depending on current
        resource need.
       </p></div>
     </td></tr><tr><td>cpu.idle_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is idle when no I/O requests are in progress
      </p>
     </td></tr><tr><td>cpu.idle_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is idle when no I/O requests are in progress
      </p>
     </td></tr><tr><td>cpu.percent</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used in total
      </p>
     </td></tr><tr><td>cpu.stolen_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of stolen CPU time, that is, the time spent in other OS
       contexts when running in a virtualized environment
      </p>
     </td></tr><tr><td>cpu.system_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used at the system level
      </p>
     </td></tr><tr><td>cpu.system_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the system level
      </p>
     </td></tr><tr><td>cpu.time_ns</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the host level
      </p>
     </td></tr><tr><td>cpu.total_logical_cores</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Total number of logical cores available for an entire node (Includes
       hyper threading).
      </p>
      <div id="id-1.6.14.3.6.23.3.2.5.9.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note: </h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>cpu.user_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used at the user level
      </p>
     </td></tr><tr><td>cpu.user_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the user level
      </p>
     </td></tr><tr><td>cpu.wait_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is idle AND there is at least one I/O request
       in progress
      </p>
     </td></tr><tr><td>cpu.wait_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is idle AND there is at least one I/O request in progress
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.6.14.3.6.23.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.9: </span><span class="name">Disk Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.23.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Disk Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>disk.inode_used_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The percentage of inodes that are used on a device
      </p>
     </td></tr><tr><td>disk.space_used_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The percentage of disk space that is being used on a device
      </p>
     </td></tr><tr><td>disk.total_space_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The total amount of disk space in Mbytes aggregated across all the disks
       on a particular node.
      </p>
      <div id="id-1.6.14.3.6.23.4.2.5.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>disk.total_used_space_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The total amount of used disk space in Mbytes aggregated across all the
       disks on a particular node.
      </p>
      <div id="id-1.6.14.3.6.23.4.2.5.4.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>io.read_kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Kbytes/sec read by an io device
      </p>
     </td></tr><tr><td>io.read_req_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Number of read requests/sec to an io device
      </p>
     </td></tr><tr><td>io.read_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Amount of read time in seconds to an io device
      </p>
     </td></tr><tr><td>io.write_kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Kbytes/sec written by an io device
      </p>
     </td></tr><tr><td>io.write_req_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Number of write requests/sec to an io device
      </p>
     </td></tr><tr><td>io.write_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Amount of write time in seconds to an io device
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.6.14.3.6.23.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.10: </span><span class="name">Load Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.23.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Load Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>load.avg_15_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a
       15 minute period
      </p>
     </td></tr><tr><td>load.avg_1_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a 1
       minute period
      </p>
     </td></tr><tr><td>load.avg_5_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a 5
       minute period
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.6.14.3.6.23.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.11: </span><span class="name">Memory Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.23.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Memory Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>mem.free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of free memory
      </p>
     </td></tr><tr><td>mem.swap_free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Percentage of free swap memory that is free
      </p>
     </td></tr><tr><td>mem.swap_free_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of free swap memory that is free
      </p>
     </td></tr><tr><td>mem.swap_total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of total physical swap memory
      </p>
     </td></tr><tr><td>mem.swap_used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of total swap memory used
      </p>
     </td></tr><tr><td>mem.total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of memory
      </p>
     </td></tr><tr><td>mem.usable_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of usable memory
      </p>
     </td></tr><tr><td>mem.usable_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Percentage of total memory that is usable
      </p>
     </td></tr><tr><td>mem.used_buffers</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Number of buffers in Mbytes being used by the kernel for block io
      </p>
     </td></tr><tr><td>mem.used_cache</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of memory used for the page cache
      </p>
     </td></tr><tr><td>mem.used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of used memory
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.6.14.3.6.23.7"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 12.12: </span><span class="name">Network Metrics </span><a title="Permalink" class="permalink" href="#id-1.6.14.3.6.23.7">#</a></h6></div><div class="table-contents"><table class="table" summary="Network Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>net.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network bytes received per second
      </p>
     </td></tr><tr><td>net.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network errors on incoming network traffic per second
      </p>
     </td></tr><tr><td>net.in_packets_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of inbound network packets dropped per second
      </p>
     </td></tr><tr><td>net.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network packets received per second
      </p>
     </td></tr><tr><td>net.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network bytes sent per second
      </p>
     </td></tr><tr><td>net.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network errors on outgoing network traffic per second
      </p>
     </td></tr><tr><td>net.out_packets_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of outbound network packets dropped per second
      </p>
     </td></tr><tr><td>net.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network packets sent per second
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect3" id="idg-all-operations-monitoring-zookeeper-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.4.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Zookeeper Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-zookeeper-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-monitoring-zookeeper_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-zookeeper_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-zookeeper-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Zookeeper service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>zookeeper.avg_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Average latency in second</td></tr><tr><td>zookeeper.connections_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Number of connections</td></tr><tr><td>zookeeper.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Received bytes</td></tr><tr><td>zookeeper.max_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Maximum latency in second</td></tr><tr><td>zookeeper.min_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Minimum latency in second</td></tr><tr><td>zookeeper.node_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Number of nodes</td></tr><tr><td>zookeeper.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Sent bytes</td></tr><tr><td>zookeeper.outstanding_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Outstanding bytes</td></tr><tr><td>zookeeper.zxid_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Count number</td></tr><tr><td>zookeeper.zxid_epoch</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Epoch number</td></tr></tbody></table></div></div></div></div><div class="sect1" id="centralized-logging"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Centralized Logging Service</span> <a title="Permalink" class="permalink" href="#centralized-logging">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-centralized_logging.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-centralized_logging.xml</li><li><span class="ds-label">ID: </span>centralized-logging</li></ul></div></div></div></div><p>
  You can use the Centralized Logging Service to evaluate and troubleshoot your
  distributed cloud environment from a single location.
 </p><div class="sect2" id="central-log-GS"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started with Centralized Logging Service</span> <a title="Permalink" class="permalink" href="#central-log-GS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_GS.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_GS.xml</li><li><span class="ds-label">ID: </span>central-log-GS</li></ul></div></div></div></div><p>
  A typical cloud consists of multiple servers which makes locating a specific
  log from a single server difficult. The Centralized Logging feature helps the
  administrator evaluate and troubleshoot the distributed cloud deployment from
  a single location.
 </p><p>
  The Logging API is a component in the centralized logging architecture. It
  works between log producers and log storage. In most cases it works by
  default after installation with no additional configuration. To use Logging
  API with logging-as-a-service, you must
  configure an end-point. This component adds flexibility and supportability
  for features in the future.
 </p><p>
  <span class="bold"><strong>Do I need to Configure monasca-log-api?</strong></span> If
  you are only using Cloud Lifecycle Manager , then the default
  configuration is ready to use.
 </p><div id="id-1.6.14.4.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   If you are using logging in any of the following deployments, then you will
   need to query Keystone to get an end-point to use.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Logging as a Service
    </p></li><li class="listitem "><p>
     Platform as a Service
    </p></li></ul></div></div><p>
  The Logging API is protected by Keystone’s role-based access control. To
  ensure that logging is allowed and Monasca alarms can be triggered, the user
  must have the monasca-user role. <span class="bold"><strong>To get an end-point
  from Keystone:</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log on to Cloud Lifecycle Manager (deployer node).
   </p></li><li class="listitem "><p>
    To list the Identity service catalog, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ./service.osrc
<code class="prompt user">ardana &gt; </code>openstack catalog list</pre></div></li><li class="listitem "><p>
    In the output, find Kronos. For example:
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Name</th><th>Type</th><th>Endpoints</th></tr></thead><tbody><tr><td>kronos</td><td>region0</td><td>
        <p>
         public: http://myardana.test:5607/v3.0, admin:
         http://192.168.245.5:5607/v3.0, internal:
         http://192.168.245.5:5607/v3.0
        </p>
       </td></tr></tbody></table></div></li><li class="listitem "><p>
    Use the same port number as found in the output. In the example, you would
    use port 5607.
   </p></li></ol></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the logging-ansible restart playbook has been updated to manage
  the start,stop, and restart of the Centralized Logging Service in a specific
  way. This change was made to ensure the proper stop, start, and restart of
  Elasticsearch.
 </p><div id="id-1.6.14.4.3.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   It is recommended that you only use the logging playbooks to perform the
   start, stop, and restart of the Centralized Logging Service. Manually mixing
   the start, stop, and restart operations with the logging playbooks will
   result in complex failures.
  </p></div><p>
  For more information, see <a class="xref" href="#central-log-manage" title="12.2.4. Managing the Centralized Logging Feature">Section 12.2.4, “Managing the Centralized Logging Feature”</a>.
 </p><div class="sect3" id="idg-all-operations-central-log-GS-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#idg-all-operations-central-log-GS-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_GS.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_GS.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-central-log-GS-xml-4</li></ul></div></div></div></div><p>
   For more information about the centralized logging components, see the
   following sites:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="link" href="https://www.elastic.co/guide/en/logstash/current/introduction.html" target="_blank">Logstash</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/guide" target="_blank">Elasticsearch Guide</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/blog/scripting-security" target="_blank">Elasticsearch
     Scripting and Security</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://python-beaver.readthedocs.io/en/latest/" target="_blank">Beaver</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/guide/en/kibana/current/index.html" target="_blank">Kibana
     Dashboard</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://kafka.apache.org/" target="_blank">Apache Kafka</a>
    </p></li></ul></div></div></div><div class="sect2" id="central-log-concepts"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Centralized Logging Service</span> <a title="Permalink" class="permalink" href="#central-log-concepts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>central-log-concepts</li></ul></div></div></div></div><p>
  The Centralized Logging feature collects logs on a central system, rather
  than leaving the logs scattered across the network. The administrator can use
  a single Kibana interface to view log information in charts, graphs, tables,
  histograms, and other forms.
 </p><div class="sect3" id="idg-all-operations-central-log-understanding-xml-2"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What Components are Part of Centralized Logging?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-central-log-understanding-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-central-log-understanding-xml-2</li></ul></div></div></div></div><p>
   Centralized logging consists of several components, detailed below:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Administrator's Browser: </span>
      Operations Console can be used to access logging alarms or to access Kibana's
      dashboards to review logging data.
     </p></li><li class="listitem "><p><span class="formalpara-title">Apache Website for Kibana: </span>
      A standard Apache website that proxies web/REST requests to the Kibana
      NodeJS server.
     </p></li><li class="listitem "><p><span class="formalpara-title">Beaver: </span>
      A Python daemon that collects information in log files and sends it to
      the Logging API (monasca-log API) over a secure connection.
     </p></li><li class="listitem "><p><span class="formalpara-title">Cloud Auditing Data Federation (CADF): </span>
      Defines a standard, full-event model anyone can use to fill in the
      essential data needed to certify, self-manage and self-audit
      application security in cloud environments.
     </p></li><li class="listitem "><p><span class="formalpara-title">Centralized Logging and Monitoring (CLM): </span>
      Used to evaluate and troubleshoot your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> distributed cloud
      environment from a single location.
     </p></li><li class="listitem "><p>
     <span class="bold"><strong>Curator:</strong></span> a tool provided by
     Elasticsearch to manage indices.
    </p></li><li class="listitem "><p><span class="formalpara-title">Elasticsearch: </span>
      A data store offering fast indexing and querying.
     </p></li><li class="listitem "><p><span class="formalpara-title"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>: </span>
      Provides public, private, and managed cloud solutions to get you moving
      on your cloud journey.
     </p></li><li class="listitem "><p><span class="formalpara-title">JavaScript Object Notation (JSON) log file: </span>
      A file stored in the JSON format and used to exchange data. JSON uses
      JavaScript syntax, but the JSON format is text only. Text can be read
      and used as a data format by any programming language. This format is
      used by the Beaver and Logstash components.
     </p></li><li class="listitem "><p><span class="formalpara-title">Kafka: </span>
      A messaging broker used for collection of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> centralized
      logging data across nodes. It is highly available, scalable and
      performant. Kafka stores logs in disk instead of memory and is
      therefore more tolerant to consumer down times.
     </p><div id="id-1.6.14.4.4.3.3.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      Make sure not to undersize your Kafka partition or the data retention
      period may be lower than expected. If the Kafka partition capacity is
      lower than 85%, the retention period will increase to 30 minutes. Over
      time Kafka will also eject old data.
     </p></div></li><li class="listitem "><p><span class="formalpara-title">Kibana: </span>
      A client/server application with rich dashboards to visualize the data
      in Elasticsearch through a web browser. Kibana enables you to create
      charts and graphs using the log data.
     </p></li><li class="listitem "><p>
     <span class="bold"><strong>Logging API (monasca-log-api):</strong></span> <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
     API provides a standard REST interface to store logs. It uses Keystone
     authentication and role-based access control support.
    </p></li><li class="listitem "><p><span class="formalpara-title">Logstash: </span>
      A log processing system for receiving, processing and outputting logs.
      Logstash retrieves logs from Kafka, processes and enriches the data,
      then stores the data in Elasticsearch.
     </p></li><li class="listitem "><p><span class="formalpara-title">MML Service Node: </span>
      Metering, Monitoring, and Logging (MML) service node. All services
      associated with metering, monitoring, and logging run on a dedicated
      three-node cluster. Three nodes are required for high availability with
      quorum.
     </p></li><li class="listitem "><p><span class="formalpara-title">Monasca: </span>
      <span class="productname">OpenStack</span> monitoring at scale infrastructure for the cloud that supports
      alarms and reporting.
     </p></li><li class="listitem "><p><span class="formalpara-title"><span class="productname">OpenStack</span> Service. </span>
      An <span class="productname">OpenStack</span> service process that requires logging services.
     </p></li><li class="listitem "><p><span class="formalpara-title">Oslo.log. </span>
      An <span class="productname">OpenStack</span> library for log handling. The library functions automate
      configuration, deployment and scaling of complete, ready-for-work
      application platforms. Some PaaS solutions, such as Cloud Foundry,
      combine operating systems, containers, and orchestrators with developer
      tools, operations utilities, metrics, and security to create a
      developer-rich solution.
     </p></li><li class="listitem "><p><span class="formalpara-title">Text log: </span>
      A type of file used in the logging process that contains human-readable
      records.
     </p></li></ul></div><p>
   These components are configured to work out-of-the-box and the admin should
   be able to view log data using the default configurations.
  </p><p>
   In addition to each of the services, Centralized Logging also processes logs
   for the following features:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     HAProxy
    </p></li><li class="listitem "><p>
     Syslog
    </p></li><li class="listitem "><p>
     keepalived
    </p></li></ul></div><p>
   The purpose of the logging service is to provide a common logging
   infrastructure with centralized user access. Since there are numerous
   services and applications running in each node of a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud, and
   there could be hundreds of nodes, all of these services and applications can
   generate enough log files to make it very difficult to search for specific
   events in log files across all of the nodes. Centralized Logging addresses
   this issue by sending log messages in real time to a central Elasticsearch,
   Logstash, and Kibana cluster. In this cluster they are indexed and organized
   for easier and visual searches. The following illustration describes the
   architecture used to collect operational logs.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-logservice_arch.png" target="_blank"><img src="images/media-logservice_arch.png" width="" /></a></div></div><div id="id-1.6.14.4.4.3.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    The arrows come from the active (requesting) side to the passive
    (listening) side. The active side is always the one providing
    credentials, so the arrows may also be seen as coming from the credential
    holder to the application requiring authentication.
   </p></div></div><div class="sect3" id="log-arch-st1to2"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 1- 2</span> <a title="Permalink" class="permalink" href="#log-arch-st1to2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st1to2</li></ul></div></div></div></div><p>
   Services configured to generate log files record the data. Beaver listens
   for changes to the files and sends the log files to the Logging Service. The
   first step the Logging service takes is to re-format the original log file
   to a new log file with text only and to remove all network operations. In
   Step 1a, the Logging service uses the Oslo.log library to re-format the file
   to text-only. In Step 1b, the Logging service uses the Python-Logstash
   library to format the original audit log file to a JSON file.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.4.4.4.3.1"><span class="term ">Step 1a</span></dt><dd><p>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </p></dd><dt id="id-1.6.14.4.4.4.3.2"><span class="term ">Step 1b</span></dt><dd><p>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </p></dd><dt id="id-1.6.14.4.4.4.3.3"><span class="term ">Step 2a</span></dt><dd><p>
      The monascalog transport of Beaver makes a token request call to Keystone
      passing in credentials. The token returned is cached to avoid multiple
      network round-trips.
     </p></dd><dt id="id-1.6.14.4.4.4.3.4"><span class="term ">Step 2b</span></dt><dd><p>
      The monascalog transport of Beaver batches multiple logs (operational or
      audit) and posts them to the monasca-log-api VIP over a secure
      connection. Failure logs are written to the local Beaver log.
     </p></dd><dt id="id-1.6.14.4.4.4.3.5"><span class="term ">Step 2c</span></dt><dd><p>
      The REST API client for monasca-log-api makes a token-request call to
      Keystone passing in credentials. The token returned is cached to avoid
      multiple network round-trips.
     </p></dd><dt id="id-1.6.14.4.4.4.3.6"><span class="term ">Step 2d</span></dt><dd><p>
      The REST API client for monasca-log-api batches multiple logs
      (operational or audit) and posts them to the monasca-log-api VIP over a
      secure connection.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st3ab"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 3a- 3b</span> <a title="Permalink" class="permalink" href="#log-arch-st3ab">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st3ab</li></ul></div></div></div></div><p>
   The Logging API (monasca-log API) communicates with Keystone to validate the
   incoming request, and then sends the logs to Kafka.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.4.4.5.3.1"><span class="term ">Step 3a</span></dt><dd><p>
      The monasca-log-api WSGI pipeline is configured to validate incoming
      request tokens with Keystone. The keystone middleware used for this
      purpose is configured to use the monasca-log-api admin user, password and
      project that have the required keystone role to validate a token.
     </p></dd><dt id="id-1.6.14.4.4.5.3.2"><span class="term ">Step 3b</span></dt><dd><p>
      Monasca-log-api sends log messages to Kafka using a language-agnostic TCP
      protocol.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st4to8"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 4- 8</span> <a title="Permalink" class="permalink" href="#log-arch-st4to8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st4to8</li></ul></div></div></div></div><p>
   Logstash pulls messages from Kafka, identifies the log type, and transforms
   the messages into either the audit log format or operational format. Then
   Logstash sends the messages to Elasticsearch, using either an audit or
   operational indices.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.4.4.6.3.1"><span class="term ">Step 4</span></dt><dd><p>
      Logstash input workers pull log messages from the Kafka-Logstash topic
      using TCP.
     </p></dd><dt id="id-1.6.14.4.4.6.3.2"><span class="term ">Step 5</span></dt><dd><p>
      This Logstash filter processes the log message in-memory in the request
      pipeline. Logstash identifies the log type from this field.
     </p></dd><dt id="id-1.6.14.4.4.6.3.3"><span class="term ">Step 6</span></dt><dd><p>
      This Logstash filter processes the log message in-memory in the request
      pipeline. If the message is of audit-log type, Logstash transforms it
      from the monasca-log-api envelope format to the original CADF format.
     </p></dd><dt id="id-1.6.14.4.4.6.3.4"><span class="term ">Step 7</span></dt><dd><p>
      This Logstash filter determines which index should receive the log
      message. There are separate indices in Elasticsearch for operational
      versus audit logs.
     </p></dd><dt id="id-1.6.14.4.4.6.3.5"><span class="term ">Step 8</span></dt><dd><p>
      Logstash output workers write the messages read from Kafka to the daily
      index in the local Elasticsearch instance.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st9to12"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 9- 12</span> <a title="Permalink" class="permalink" href="#log-arch-st9to12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st9to12</li></ul></div></div></div></div><p>
   When an administrator who has access to the guest network accesses the
   Kibana client and makes a request, Apache forwards the request to the Kibana
   NodeJS server. Then the server uses the Elasticsearch REST API to service
   the client requests.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.4.4.7.3.1"><span class="term ">Step 9</span></dt><dd><p>
      An administrator who has access to the guest network accesses the Kibana
      client to view and search log data. The request can originate from the
      external network in the cloud through a tenant that has a pre-defined
      access route to the guest network.
     </p></dd><dt id="id-1.6.14.4.4.7.3.2"><span class="term ">Step 10</span></dt><dd><p>
      An administrator who has access to the guest network uses a web browser
      and points to the Kibana URL. This allows the user to search logs and
      view Dashboard reports.
     </p></dd><dt id="id-1.6.14.4.4.7.3.3"><span class="term ">Step 11</span></dt><dd><p>
      The authenticated request is forwarded to the Kibana NodeJS server to
      render the required dashboard, visualization, or search page.
     </p></dd><dt id="id-1.6.14.4.4.7.3.4"><span class="term ">Step 12</span></dt><dd><p>
      The Kibana NodeJS web server uses the Elasticsearch REST API in localhost
      to service the UI requests.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st13to15"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 13- 15</span> <a title="Permalink" class="permalink" href="#log-arch-st13to15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st13to15</li></ul></div></div></div></div><p>
   Log data is backed-up and deleted in the final steps.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.4.4.8.3.1"><span class="term ">Step 13</span></dt><dd><p>
      A daily cron job running in the ELK node runs curator to prune old
      Elasticsearch log indices.
     </p></dd><dt id="id-1.6.14.4.4.8.3.2"><span class="term ">Step 14</span></dt><dd><p>
      The curator configuration is done at the deployer node through the
      Ansible role logging-common. Curator is scripted to then prune or clone
      old indices based on this configuration.
     </p></dd><dt id="id-1.6.14.4.4.8.3.3"><span class="term ">Step 15</span></dt><dd><p>
      The audit logs are configured to be backed up by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Freezer
      product. For more information about Freezer (and Bura), see
      <a class="xref" href="#bura-overview" title="Chapter 14. Backup and Restore">Chapter 14, <em>Backup and Restore</em></a>.
     </p></dd></dl></div></div><div class="sect3" id="retaining-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Long are Log Files Retained?</span> <a title="Permalink" class="permalink" href="#retaining-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>retaining-logs</li></ul></div></div></div></div><p>
   The logs that are centrally stored are saved to persistent storage as
   Elasticsearch indices. These indices are stored in the partition
   <code class="literal">/var/lib/elasticsearch</code> on each of the Elasticsearch
   cluster nodes. Out of the box, logs are stored in one Elasticsearch index
   per service. As more days go by, the number of indices stored in this disk
   partition grows. Eventually the partition fills up. If they are
   <span class="bold"><strong>open</strong></span>, each of these indices takes up CPU
   and memory. If these indices are left unattended they will continue to
   consume system resources and eventually deplete them.
  </p><p>
   Elasticsearch, by itself, does not prevent this from happening.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a tool called curator that is developed by the Elasticsearch
   community to handle these situations. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installs and uses a curator
   in conjunction with several configurable settings. This curator is called by
   cron and performs the following checks:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>First Check.</strong></span> The hourly cron job checks
     to see if the currently used Elasticsearch partition size is over the
     value set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_low_watermark_percent</pre></div><p>
     If it is higher than this value, the curator deletes old indices according
     to the value set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_num_of_indices_to_keep</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>Second Check.</strong></span> Another check is made to
     verify if the partition size is below the high watermark percent. If it is
     still too high, curator will delete all indices except the current one
     that is over the size as set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_max_index_size_in_gb</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>Third Check.</strong></span> A third check verifies if
     the partition size is still too high. If it is, curator will delete all
     indices except the current one.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Final Check.</strong></span> A final check verifies if
     the partition size is still high. If it is, an error message is written to
     the log file but the current index is NOT deleted.
    </p></li></ul></div><p>
   In the case of an extreme network issue, log files can run out of disk space
   in under an hour. To avoid this <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a shell script called
   <code class="literal">logrotate_if_needed.sh</code>. The cron process runs this script
   every 5 minutes to see if the size of <code class="literal">/var/log</code> has
   exceeded the high_watermark_percent (95% of the disk, by default). If it is
   at or above this level, <code class="literal">logrotate_if_needed.sh</code> runs the
   <code class="literal">logrotate</code> script to rotate logs and to free up extra
   space. This script helps to minimize the chance of running out of disk space
   on <code class="literal">/var/log</code>.
  </p></div><div class="sect3" id="rotating-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Are Logs Rotated?</span> <a title="Permalink" class="permalink" href="#rotating-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>rotating-logs</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the cron process which in turn calls Logrotate to provide
   rotation, compression, and removal of log files. Each log file can be
   rotated hourly, daily, weekly, or monthly. If no rotation period is set then
   the log file will only be rotated when it grows too large.
  </p><p>
   Rotating a file means that the Logrotate process creates a copy of the log
   file with a new extension, for example, the .1 extension, then empties the
   contents of the original file. If a .1 file already exists, then that file
   is first renamed with a .2 extension. If a .2 file already exists, it is
   renamed to .3, etc., up to the maximum number of rotated files specified in
   the settings file. When Logrotate reaches the last possible file extension,
   it will delete the last file first on the next rotation. By the time the
   Logrotate process needs to delete a file, the results will have been copied
   to Elasticsearch, the central logging database.
  </p><p>
   The log rotation setting files can be found in the following directory
  </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/logging-common/vars</pre></div><p>
   These files allow you to set the following options:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.4.4.10.7.1"><span class="term ">Service</span></dt><dd><p>
      The name of the service that creates the log entries.
     </p></dd><dt id="id-1.6.14.4.4.10.7.2"><span class="term ">Rotated Log Files</span></dt><dd><p>
      List of log files to be rotated. These files are kept locally on the
      server and will continue to be rotated. If the file is also listed as
      Centrally Logged, it will also be copied to Elasticsearch.
     </p></dd><dt id="id-1.6.14.4.4.10.7.3"><span class="term ">Frequency</span></dt><dd><p>
      The timing of when the logs are rotated. Options include:hourly, daily,
      weekly, or monthly.
     </p></dd><dt id="id-1.6.14.4.4.10.7.4"><span class="term ">Max Size</span></dt><dd><p>
      The maximum file size the log can be before it is rotated out.
     </p></dd><dt id="id-1.6.14.4.4.10.7.5"><span class="term ">Rotation</span></dt><dd><p>
      The number of log files that are rotated.
     </p></dd><dt id="id-1.6.14.4.4.10.7.6"><span class="term ">Centrally Logged Files</span></dt><dd><p>
      These files will be indexed by Elasticsearch and will be available for
      searching in the Kibana user interface.
     </p></dd></dl></div><p>
   As an example, Freezer, the Backup and Restore (BURA) service, may be
   configured to create log files by setting the Rotated Log Files section to
   contain:
  </p><div class="verbatim-wrap"><pre class="screen">/var/log/freezer/freezer-scheduler.log</pre></div><p>
   This configuration means that in the /var/log/freezer-agent directory, in a
   live environment, there should be a file called freezer-scheduler.log. As
   the log file grows, the cron process runs every hour to check the log file
   size against the settings in the configuration files. The example
   freezer-agent settings are described below.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th>Service</th><th>Node Type</th><th>Rotated Log Files</th><th>Frequency</th><th>Max Size</th><th>Rotation</th><th>Centrally Logged Files</th></tr></thead><tbody><tr><td>
       <p>
        Freezer
       </p>
      </td><td>
       <p>
        Control
       </p>
      </td><td>
       <p>
        /var/log/freezer/freezer-scheduler.log
       </p>
       <p>
        /var/log/freezer/freezer-agent-json.log
       </p>
      </td><td>
       <p>
        Daily
       </p>
      </td><td>
       <p>
        45 MB
       </p>
      </td><td>
       <p>
        7
       </p>
      </td><td>
       <p>
        /var/log/freezer-agent/freezer-agent-json.log
       </p>
      </td></tr></tbody></table></div><p>
   For the <code class="literal">freezer-scheduler.log</code> file specifically, the
   information in the table tells the Logrotate process that the log file is to
   be rotated daily, and it can have a maximum size of 45 MB. After a week of
   log rotation, you might see something similar to this list:
  </p><div class="verbatim-wrap"><pre class="screen">freezer-scheduler.log at 10K
freezer-scheduler.log.1 at 123K
freezer-scheduler.log.2.gz at 13K
freezer-scheduler.log.3.gz at 17K
freezer-scheduler.log.4.gz at 128K
freezer-scheduler.log.5.gz at 22K
freezer-scheduler.log.6.gz at 323K
freezer-scheduler.log.7.gz at 123K</pre></div><p>
   Since the Rotation value is set to 7 for this log file, there will never be
   a <code class="literal">freezer-scheduler.log.8.gz</code>. When the cron process runs
   its checks, if the <code class="literal">freezer-scheduler.log</code> size is more
   than 45 MB, then Logrotate rotates the file.
  </p><p>
   In this example, the following log files are rotated:
  </p><div class="verbatim-wrap"><pre class="screen">/var/log/freezer/freezer-scheduler.log
/var/log/freezer/freezer-agent-json.log</pre></div><p>
   However, in this example, only the following file is centrally logged with
   Elasticsearch:
  </p><div class="verbatim-wrap"><pre class="screen">/var/log/freezer/freezer-agent-json.log</pre></div><p>
   Only files that are listed in the <span class="bold"><strong>Centrally Logged
   Files</strong></span> section are copied to Elasticsearch.
  </p><p>
   All of the variables for the Logrotate process are found in the following
   file:
  </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/logging-ansible/logging-common/defaults/main.yml</pre></div><p>
   Cron runs Logrotate hourly. Every 5 minutes another process is run called
   <span class="bold"><strong>"logrotate_if_needed"</strong></span> which uses a
   watermark value to determine if the Logrotate process needs to be run. If
   the <span class="bold"><strong>"high watermark"</strong></span> has been reached, and
   the /var/log partition is more than 95% full (by default - this can be
   adjusted), then Logrotate will be run within 5 minutes.
  </p></div><div class="sect3" id="BUElasticsearch"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Are Log Files Backed-Up To Elasticsearch?</span> <a title="Permalink" class="permalink" href="#BUElasticsearch">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>BUElasticsearch</li></ul></div></div></div></div><p>
   While centralized logging is enabled out of the box, the backup of these
   logs is not. The reason is because Centralized Logging relies on the
   Elasticsearch FileSystem Repository plugin, which in turn requires shared
   disk partitions to be configured and accessible from each of the
   Elasticsearch nodes. Since there are multiple ways to setup a shared disk
   partition, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> allows you to choose an approach that works best for
   your deployment before enabling the back-up of log files to Elasticsearch.
  </p><p>
   If you enable automatic back-up of centralized log files, then all the logs
   collected from the cloud nodes will be backed-up to Elasticsearch. Every
   hour, in the management controller nodes where Elasticsearch is setup, a
   cron job runs to check if Elasticsearch is running low on disk space. If the
   check succeeds, it further checks if the backup feature is enabled. If
   enabled, the cron job saves a snapshot of the Elasticsearch indices to the
   configured shared disk partition using curator. Next, the script starts
   deleting the oldest index and moves down from there checking each time if
   there is enough space for Elasticsearch. A check is also made to ensure that
   the backup runs only once a day.
  </p><p>
   For steps on how to enable automatic back-up, see
   <a class="xref" href="#central-log-configure-settings" title="12.2.5. Configuring Centralized Logging">Section 12.2.5, “Configuring Centralized Logging”</a>.
  </p></div></div><div class="sect2" id="central-log-access-data"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing Log Data</span> <a title="Permalink" class="permalink" href="#central-log-access-data">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>central-log-access-data</li></ul></div></div></div></div><p>
  All logging data in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is managed by the Centralized Logging Service
  and can be viewed or analyzed by Kibana. Kibana is the only graphical
  interface provided with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to search or create a report from log data.
  Operations Console provides only a link to the Kibana Logging dashboard.
 </p><p>
  The following two methods allow you to access the Kibana Logging dashboard to
  search log data:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#CL-access-OpsConsole" title="12.2.3.1. Use the Operations Console Link">Section 12.2.3.1, “Use the Operations Console Link”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-access-Kibana" title="12.2.3.2. Using Kibana to Access Log Data">Section 12.2.3.2, “Using Kibana to Access Log Data”</a>
   </p></li></ul></div><p>
  To learn more about Kibana, read the
  <a class="link" href="https://www.elastic.co/guide/en/kibana/current/getting-started.html" target="_blank">Getting
  Started with Kibana</a> guide.
 </p><div class="sect3" id="CL-access-OpsConsole"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use the Operations Console Link</span> <a title="Permalink" class="permalink" href="#CL-access-OpsConsole">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>CL-access-OpsConsole</li></ul></div></div></div></div><p>
   Operations Console allows you to access Kibana in the same tool that you use
   to manage the other <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> resources in your deployment. To use Operations Console,
   you must have the correct permissions. For more about permission
   requirements, see <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.2 “Connecting to the Operations Console”</span>.
  </p><p>
   To use Operations Console:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In a browser, open the Operations Console.
    </p></li><li class="listitem "><p>
     On the login page, enter the user name, and the
     <span class="bold"><strong>Password</strong></span>, and then click
     <span class="bold"><strong>LOG IN</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home/Central Dashboard</strong></span> page, click
     the menu represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left, select
     <span class="bold"><strong>Home</strong></span>, and then select
     <span class="bold"><strong>Logging</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home/Logging</strong></span> page, click
     <span class="bold"><strong>View Logging Dashboard</strong></span>.
    </p></li></ol></div><div id="id-1.6.14.4.5.6.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, Kibana usually runs on a different network than Operations Console.
    Due to this configuration, it is possible that using Operations Console
    to access Kibana will result in an “404 not found” error. This
    error only occurs if the user has access only to the public facing network.
   </p></div></div><div class="sect3" id="CL-access-Kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Kibana to Access Log Data</span> <a title="Permalink" class="permalink" href="#CL-access-Kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>CL-access-Kibana</li></ul></div></div></div></div><p>
   Kibana is an open-source, data-visualization plugin for Elasticsearch.
   Kibana provides visualization capabilities using the log content indexed on
   an Elasticsearch cluster. Users can create bar and pie charts, line and
   scatter plots, and maps using the data collected by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in the cloud
   log files.
  </p><p>
   While creating Kibana dashboards is beyond the scope of this document, it is
   important to know that the dashboards you create are JSON files that you can
   modify or create new dashboards based on existing dashboards.
  </p><div id="id-1.6.14.4.5.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Kibana is client-server software. To operate properly, the browser must be
    able to access port 5601 on the control plane.
   </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Field</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>user</td><td>kibana</td><td>
       <p>
        Username that will be required for logging into the Kibana UI.
       </p>
      </td></tr><tr><td>password</td><td>random password is generated</td><td>
       <p>
        Password generated during installation that is used to login to the
        Kibana UI.
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="Login-creds-Kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging into Kibana</span> <a title="Permalink" class="permalink" href="#Login-creds-Kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>Login-creds-Kibana</li></ul></div></div></div></div><p>
   To log into Kibana to view data, you must make sure you have the required
   login configuration.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Verify login credentials: <a class="xref" href="#KLogin-Creds" title="12.2.3.3.1. Verify Login Credentials">Section 12.2.3.3.1, “Verify Login Credentials”</a>
    </p></li><li class="listitem "><p>
     Find the randomized password: <a class="xref" href="#KLogin-Psswd" title="12.2.3.3.2. Find the Randomized Password">Section 12.2.3.3.2, “Find the Randomized Password”</a>
    </p></li><li class="listitem "><p>
     Access Kibana using a direct link: <a class="xref" href="#KLogin-DLink" title="12.2.3.3.3. Access Kibana Using a Direct Link:">Section 12.2.3.3.3, “Access Kibana Using a Direct Link:”</a>
    </p></li></ol></div><div class="sect4" id="KLogin-Creds"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verify Login Credentials</span> <a title="Permalink" class="permalink" href="#KLogin-Creds">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-Creds</li></ul></div></div></div></div><p>
    During the installation of Kibana, a password is automatically set and it
    is randomized. Therefore, unless an administrator has already changed it,
    you need to retrieve the default password from a file on the control plane
    node.
   </p></div><div class="sect4" id="KLogin-Psswd"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Find the Randomized Password</span> <a title="Permalink" class="permalink" href="#KLogin-Psswd">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-Psswd</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To find the Kibana password, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep kibana ~/scratch/ansible/next/my_cloud/stage/internal/CloudModel.yaml</pre></div></li></ol></div></div><div class="sect4" id="KLogin-DLink"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Kibana Using a Direct Link:</span> <a title="Permalink" class="permalink" href="#KLogin-DLink">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-DLink</li></ul></div></div></div></div><p>
    This section helps you verify the Horizon virtual IP (VIP) address that you
    should use.
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To find hostname, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep -i log-svr /etc/hosts</pre></div></li><li class="listitem "><p>
      Navigate to the following directory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/openstack/my_cloud/definition/data</pre></div><div id="id-1.6.14.4.5.8.6.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
       The file <code class="filename">network_groups.yml</code> in the
       <code class="filename">~/openstack/my_cloud/definition/data</code>
       directory is the input model file that may be copied automatically to
       other directories.
      </p></div></li><li class="listitem "><p>
      Open the following file for editing:
     </p><div class="verbatim-wrap"><pre class="screen">network_groups.yml</pre></div></li><li class="listitem "><p>
      Find the following entry:
     </p><div class="verbatim-wrap"><pre class="screen">external-name</pre></div></li><li class="listitem "><p>
      If your administrator set a hostname value in the
      <em class="replaceable ">EXTERNAL_NAME</em> field during the
      configuration process for your cloud, then Kibana will be accessed over
      port 5601 on that hostname.
     </p></li><li class="listitem "><p>
      If your administrator did not set a hostname value, then to determine
      which IP address to use, from your Cloud Lifecycle Manager, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep HZN-WEB /etc/hosts</pre></div><p>
      The output of the grep command should show you the virtual IP address for
      Kibana that you should use.
     </p><div id="id-1.6.14.4.5.8.6.3.6.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
       If nothing is returned by the grep command, you can open the following
       file to look for the IP address manually:
      </p><div class="verbatim-wrap"><pre class="screen">/etc/hosts</pre></div></div><p>
      Access to Kibana will be over port 5601 of that virtual IP address.
      Example:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable ">VIP</em>:5601</pre></div></li></ol></div></div></div></div><div class="sect2" id="central-log-manage"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing the Centralized Logging Feature</span> <a title="Permalink" class="permalink" href="#central-log-manage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>central-log-manage</li></ul></div></div></div></div><p>
  No specific configuration tasks are required to use Centralized Logging, as
  it is enabled by default after installation. However, you can configure the
  individual components as needed for your environment.
 </p><div class="sect3" id="CL-stop-start"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Do I Stop and Start the Logging Service?</span> <a title="Permalink" class="permalink" href="#CL-stop-start">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>CL-stop-start</li></ul></div></div></div></div><p>
   Although you might not need to stop and start the logging service very
   often, you may need to if, for example, one of the logging services is not
   behaving as expected or not working.
  </p><p>
   You cannot enable or disable centralized logging across all services unless
   you stop all centralized logging. Instead, it is recommended that you enable
   or disable individual log files in the &lt;service&gt;-clr.yml files and
   then reconfigure logging. You would enable centralized logging for a file
   when you want to make sure you are able to monitor those logs in Kibana.
  </p><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the logging-ansible restart playbook has been updated to manage
   the start,stop, and restart of the Centralized Logging Service in a specific
   way. This change was made to ensure the proper stop, start, and restart of
   Elasticsearch.
  </p><div id="id-1.6.14.4.6.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    It is recommended that you only use the logging playbooks to perform the
    start, stop, and restart of the Centralized Logging Service. Manually
    mixing the start, stop, and restart operations with the logging playbooks
    will result in complex failures.
   </p></div><p>
   The steps in this section only impact centralized logging. Logrotate is an
   essential feature that keeps the service log files from filling the disk and
   will not be affected.
  </p><div id="id-1.6.14.4.6.3.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    These playbooks must be run from the Cloud Lifecycle Manager.
   </p></div><p>
   <span class="bold"><strong>To stop the Logging service:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the ansible playbook, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the ansible playbook that will stop the logging service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-stop.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>To start the Logging service:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the ansible playbook, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the ansible playbook that will stop the logging service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-start.yml</pre></div></li></ol></div></div><div class="sect3" id="CL-disable"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Do I Enable or Disable Centralized Logging For a Service?</span> <a title="Permalink" class="permalink" href="#CL-disable">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>CL-disable</li></ul></div></div></div></div><p>
   To enable or disable Centralized Logging for a service you need to modify
   the configuration for the service, set the
   <span class="bold"><strong>enabled</strong></span> flag to
   <span class="bold"><strong>true</strong></span> or
   <span class="bold"><strong>false</strong></span>, and then reconfigure logging.
  </p><div id="id-1.6.14.4.6.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    There are consequences if you enable too many logging files for a service.
    If there is not enough storage to support the increased logging, the
    retention period of logs in Elasticsearch is decreased. Alternatively, if
    you wanted to increase the retention period of log files or if you did not
    want those logs to show up in Kibana, you would disable centralized logging
    for a file.
   </p></div><p>
   To enable Centralized Logging for a service:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use the documentation provided with the service to ensure it is not
     configured for logging.
    </p></li><li class="listitem "><p>
     To find the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> file to edit, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>find ~/openstack/my_cloud/config/logging/vars/ -name "*<em class="replaceable ">service-name</em>*"</pre></div></li><li class="listitem "><p>
     Edit the file for the service for which you want to enable logging.
    </p></li><li class="listitem "><p>
     To enable Centralized Logging, find the following code and change the
     enabled flag to <span class="bold"><strong>true</strong></span>, to disable, change
     the enabled flag to <span class="bold"><strong>false</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">logging_options:
 - centralized_logging:
        enabled: true
        format: json</pre></div></li><li class="listitem "><p>
     Save the changes to the file.
    </p></li><li class="listitem "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     To reconfigure logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div><p>
   Sample of a Freezer file enabled for Centralized logging:
  </p><div class="verbatim-wrap"><pre class="screen">---
sub_service:
   hosts: FRE-AGN
   name: freezer-agent
   service: freezer
   monitoring:
      enabled: true
      external_name: backup
      logging_dir: /var/log/freezer
   logging_options:
     - files:
        - /var/log/freezer/freezer-agent.log
        - /var/log/freezer/freezer-scheduler.log
        - centralized_logging:
           enabled: true
           format: json</pre></div></div></div><div class="sect2" id="central-log-configure-settings"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Centralized Logging</span> <a title="Permalink" class="permalink" href="#central-log-configure-settings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>central-log-configure-settings</li></ul></div></div></div></div><p>
  You can adjust the settings for centralized logging when you are
  troubleshooting problems with a service or to decrease log size and retention
  to save on disk space. For steps on how to configure logging settings, refer
  to the following tasks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#CL-config-files" title="12.2.5.1. Configuration Files">Section 12.2.5.1, “Configuration Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-general-config" title="12.2.5.2. Planning Resource Requirements">Section 12.2.5.2, “Planning Resource Requirements”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-BU-Elasticsearch" title="12.2.5.3. Backing Up Elasticsearch Log Indices">Section 12.2.5.3, “Backing Up Elasticsearch Log Indices”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#restore-elastic-logs" title="12.2.5.4. Restoring Logs From an Elasticsearch Backup">Section 12.2.5.4, “Restoring Logs From an Elasticsearch Backup”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#tuning-logging-parameters" title="12.2.5.5. Tuning Logging Parameters">Section 12.2.5.5, “Tuning Logging Parameters”</a>
   </p></li></ul></div><div class="sect3" id="CL-config-files"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Files</span> <a title="Permalink" class="permalink" href="#CL-config-files">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-config-files</li></ul></div></div></div></div><p>
   Centralized Logging settings are stored in the configuration files in the
   following directory on the Cloud Lifecycle Manager:
   <code class="literal">~/openstack/my_cloud/config/logging/</code>
  </p><p>
   The configuration files and their use are described below:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>File</th><th>Description</th></tr></thead><tbody><tr><td>main.yml</td><td>Main configuration file for all centralized logging components.</td></tr><tr><td>elasticsearch.yml.j2</td><td>Main configuration file for Elasticsearch.</td></tr><tr><td>elasticsearch-default.j2</td><td>Default overrides for the Elasticsearch init script.</td></tr><tr><td>kibana.yml.j2</td><td>Main configuration file for Kibana.</td></tr><tr><td>kibana-apache2.conf.j2</td><td>Apache configuration file for Kibana.</td></tr><tr><td>logstash.conf.j2</td><td>Logstash inputs/outputs configuration.</td></tr><tr><td>logstash-default.j2</td><td>Default overrides for the Logstash init script.</td></tr><tr><td>beaver.conf.j2</td><td>Main configuration file for Beaver.</td></tr><tr><td>vars</td><td>Path to logrotate configuration files.</td></tr></tbody></table></div></div><div class="sect3" id="CL-general-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planning Resource Requirements</span> <a title="Permalink" class="permalink" href="#CL-general-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-general-config</li></ul></div></div></div></div><p>
   The Centralized Logging service needs to have enough resources available to
   it to perform adequately for different scale environments. The base logging
   levels are tuned during installation according to the amount of RAM
   allocated to your control plane nodes to ensure optimum performance.
  </p><p>
   These values can be viewed and changed in the
   <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code> file, but you
   will need to run a reconfigure of the Centralized Logging service if changes
   are made.
  </p><div id="id-1.6.14.4.7.5.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    The total process memory consumption for Elasticsearch will be the above
    allocated heap value (in
    <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code>) plus any Java
    Virtual Machine (JVM) overhead.
   </p></div><p>
   <span class="bold"><strong>Setting Disk Size Requirements</strong></span>
  </p><p>
   In the entry-scale models, the disk partition sizes on your controller nodes
   for the logging and Elasticsearch data are set as a percentage of your total
   disk size. You can see these in the following file on the Cloud Lifecycle Manager
   (deployer):
   <code class="literal">~/openstack/my_cloud/definition/data/&lt;controller_disk_files_used&gt;</code>
  </p><p>
   Sample file settings:
  </p><div class="verbatim-wrap"><pre class="screen"># Local Log files.
- name: log
  size: 13%
  mount: /var/log
  fstype: ext4
  mkfs-opts: -O large_file

# Data storage for centralized logging. This holds log entries from all
# servers in the cloud and hence can require a lot of disk space.
- name: elasticsearch
  size: 30%
  mount: /var/lib/elasticsearch
  fstype: ext4</pre></div><div id="id-1.6.14.4.7.5.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    The disk size is set automatically based on the hardware configuration. If
    you need to adjust it, you can set it manually with the following steps.
   </p></div><p>
   To set disk sizes:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="listitem "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks.yml</pre></div></li><li class="listitem "><p>
     Make any desired changes.
    </p></li><li class="listitem "><p>
     Save the changes to the file.
    </p></li><li class="listitem "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A git
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To run the logging reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div><div class="sect3" id="CL-BU-Elasticsearch"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backing Up Elasticsearch Log Indices</span> <a title="Permalink" class="permalink" href="#CL-BU-Elasticsearch">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-BU-Elasticsearch</li></ul></div></div></div></div><p>
   The log files that are centrally collected in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are stored by
   Elasticsearch on disk in the <code class="literal">/var/lib/elasticsearch</code>
   partition. However, this is distributed across each of the Elasticsearch
   cluster nodes as shards. A cron job runs periodically to see if the disk
   partition runs low on space, and, if so, it runs curator to delete the old
   log indices to make room for new logs. This deletion is permanent and the
   logs are lost forever. If you want to backup old logs, for example to comply
   with certain regulations, you can configure automatic backup of
   Elasticsearch indices.
  </p><div id="id-1.6.14.4.7.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If you need to restore data that was archived prior to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> and
    used the older versions of Elasticsearch, then this data will need to be
    restored to a separate deployment of Elasticsearch.
   </p><p>
    This can be accomplished using the following steps:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Deploy a separate distinct Elasticsearch instance version matching the
      version in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li><li class="listitem "><p>
      Configure the backed-up data using NFS or some other share mechanism to
      be available to the Elasticsearch instance matching the version in
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li></ol></div></div><p>
   Before enabling automatic back-ups, make sure you understand how much disk
   space you will need, and configure the disks that will store the data. Use
   the following checklist to prepare your deployment for enabling automatic
   backups:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td>☐</td><td>
       <p>
        Add a shared disk partition to each of the Elasticsearch controller
        nodes.
       </p>
       <p>
        The default partition name used for backup is
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/lib/esbackup</pre></div>
       <p>
        You can change this by:
       </p>
       <div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
          Open the following file:
          <code class="literal">my_cloud/config/logging/main.yml</code>
         </p></li><li class="listitem "><p>
          Edit the following variable <code class="literal">curator_es_backup_partition
          </code>
         </p></li></ol></div>
      </td></tr><tr><td>☐</td><td>
       <p>
        Ensure the shared disk has enough storage to retain backups for the
        desired retention period.
       </p>
      </td></tr></tbody></table></div><p>
   To enable automatic back-up of centralized logs to Elasticsearch:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager (deployer node).
    </p></li><li class="listitem "><p>
     Open the following file in a text editor:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/main.yml</pre></div></li><li class="listitem "><p>
     Find the following variables:
    </p><div class="verbatim-wrap"><pre class="screen">curator_backup_repo_name: "es_{{host.my_dimensions.cloud_name}}"
curator_es_backup_partition: /var/lib/esbackup</pre></div></li><li class="listitem "><p>
     To enable backup, change the
     <span class="bold"><strong>curator_enable_backup</strong></span> value to
     <span class="bold"><strong>true</strong></span> in the curator section:
    </p><div class="verbatim-wrap"><pre class="screen">curator_enable_backup: true</pre></div></li><li class="listitem "><p>
     Save your changes and re-run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
# Verify the added files
<code class="prompt user">ardana &gt; </code>git status
<code class="prompt user">ardana &gt; </code>git commit -m "Enabling Elasticsearch Backup"

$ cd ~/openstack/ardana/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml
$ ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To re-configure logging:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li><li class="listitem "><p>
     To verify that the indices are backed up, check the contents of the
     partition:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls /var/lib/esbackup</pre></div></li></ol></div></div><div class="sect3" id="restore-elastic-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restoring Logs From an Elasticsearch Backup</span> <a title="Permalink" class="permalink" href="#restore-elastic-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>restore-elastic-logs</li></ul></div></div></div></div><p>
   To restore logs from an Elasticsearch backup, see
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-snapshots.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-snapshots.html</a>.
  </p><div id="id-1.6.14.4.7.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    We do not recommend restoring to the original <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Centralized Logging
    cluster as it may cause storage/capacity issues. We rather recommend setting
    up a separate ELK cluster of the same version and restoring the logs there.
   </p></div></div><div class="sect3" id="tuning-logging-parameters"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning Logging Parameters</span> <a title="Permalink" class="permalink" href="#tuning-logging-parameters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>tuning-logging-parameters</li></ul></div></div></div></div><p>
   When centralized logging is installed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, parameters for
   Elasticsearch heap size and logstash heap size are automatically configured
   based on the amount of RAM on the system. These values are typically the
   required values, but they may need to be adjusted if performance issues
   arise, or disk space issues are encountered. These values may also need to
   be adjusted if hardware changes are made after an installation.
  </p><p>
   These values are defined at the top of the following file
   <code class="literal">.../logging-common/defaults/main.yml</code>. An example of the
   contents of the file is below:
  </p><div class="verbatim-wrap"><pre class="screen">1. Select heap tunings based on system RAM
#-------------------------------------------------------------------------------
threshold_small_mb: 31000
threshold_medium_mb: 63000
threshold_large_mb: 127000
tuning_selector: " {% if ansible_memtotal_mb &lt; threshold_small_mb|int %}
demo
{% elif ansible_memtotal_mb &lt; threshold_medium_mb|int %}
small
{% elif ansible_memtotal_mb &lt; threshold_large_mb|int %}
medium
{% else %}
large
{%endif %}
"

logging_possible_tunings:
2. RAM &lt; 32GB
demo:
elasticsearch_heap_size: 512m
logstash_heap_size: 512m
3. RAM &lt; 64GB
small:
elasticsearch_heap_size: 8g
logstash_heap_size: 2g
4. RAM &lt; 128GB
medium:
elasticsearch_heap_size: 16g
logstash_heap_size: 4g
5. RAM &gt;= 128GB
large:
elasticsearch_heap_size: 31g
logstash_heap_size: 8g
logging_tunings: "{{ logging_possible_tunings[tuning_selector] }}"</pre></div><p>
   This specifies thresholds for what a <span class="bold"><strong>small</strong></span>,
   <span class="bold"><strong>medium</strong></span>, or
   <span class="bold"><strong>large</strong></span> system would look like, in terms of
   memory. To see what values will be used, see what RAM your system uses, and
   see where it fits in with the thresholds to see what values you will be
   installed with. To modify the values, you can either adjust the threshold
   values so that your system will change from a
   <span class="bold"><strong>small</strong></span> configuration to a
   <span class="bold"><strong>medium</strong></span> configuration, for example, or keep
   the threshold values the same, and modify the heap_size variables directly
   for the selector that your system is set for. For example, if your
   configuration is a <span class="bold"><strong>medium</strong></span> configuration,
   which sets heap_sizes to 16 GB for Elasticsearch and 4 GB for logstash, and
   you want twice as much set aside for logstash, then you could increase the
   4 GB for logstash to 8 GB.
  </p></div></div><div class="sect2" id="central-log-configure-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Settings for Other Services</span> <a title="Permalink" class="permalink" href="#central-log-configure-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>central-log-configure-services</li></ul></div></div></div></div><p>
  When you configure settings for the Centralized Logging Service, those
  changes impact all services that are enabled for centralized logging.
  However, if you only need to change the logging configuration for one
  specific service, you will want to modify the service's files instead of
  changing the settings for the entire Centralized Logging service. This topic
  helps you complete the following tasks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#CL-log-level-srv" title="12.2.6.1. Setting Logging Levels for Services">Section 12.2.6.1, “Setting Logging Levels for Services”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-select-central-logging" title="12.2.6.19. Selecting Files for Centralized Logging">Section 12.2.6.19, “Selecting Files for Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-space-allocation" title="12.2.6.20. Controlling Disk Space Allocation and Retention of Log Files">Section 12.2.6.20, “Controlling Disk Space Allocation and Retention of Log Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-elasticsearch-config" title="12.2.6.21. Configuring Elasticsearch for Centralized Logging">Section 12.2.6.21, “Configuring Elasticsearch for Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-safeguards" title="12.2.6.22. Safeguards for the Log Partitions Disk Capacity">Section 12.2.6.22, “Safeguards for the Log Partitions Disk Capacity”</a>
   </p></li></ul></div><div class="sect3" id="CL-log-level-srv"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Logging Levels for Services</span> <a title="Permalink" class="permalink" href="#CL-log-level-srv">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-log-level-srv</li></ul></div></div></div></div><p>
   When it is necessary to increase the logging level for a specific service to
   troubleshoot an issue, or to decrease logging levels to save disk space, you
   can edit the service's config file and then reconfigure logging. All changes
   will be made to the service's files and not to the Centralized Logging
   service files.
  </p><p>
   Messages only appear in the log files if they are the same as or more severe
   than the log level you set. The DEBUG level logs everything. Most services
   default to the INFO logging level, which lists informational events, plus
   warnings, errors, and critical errors. Some services provide other logging
   options which will narrow the focus to help you debug an issue, receive a
   warning if an operation fails, or if there is a serious issue with the
   cloud.
  </p><p>
   For more information on logging levels, see the
   <a class="link" href="http://specs.openstack.org/openstack/openstack-specs/specs/log-guidelines.html" target="_blank">OpenStack
   Logging Guidelines</a> documentation.
  </p></div><div class="sect3" id="Loglvl-intro"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Logging Level for a Service</span> <a title="Permalink" class="permalink" href="#Loglvl-intro">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-intro</li></ul></div></div></div></div><p>
   If you want to increase or decrease the amount of details that are logged by
   a service, you can change the current logging level in the configuration
   files. Most services support, at a minimum, the DEBUG and INFO logging
   levels. For more information about what levels are supported by a service,
   check the documentation or Website for the specific service.
  </p></div><div class="sect3" id="Loglvl-barb"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Barbican</span> <a title="Permalink" class="permalink" href="#Loglvl-barb">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-barb</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>Barbican</td><td>barbican-api</td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Barbican logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/barbican/barbican_deploy_config.yml</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following lines:
    </p><div class="verbatim-wrap"><pre class="screen">barbican_loglevel:  {{ openstack_loglevel | default('INFO') }}
barbican_logstash_loglevel:  {{ openstack_loglevel | default('INFO') }}</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-Cinder"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block Storage (Cinder)</span> <a title="Permalink" class="permalink" href="#Loglvl-Cinder">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-Cinder</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>Cinder</td><td>
       <p>
        cinder-local
       </p>
       <p>
        cinder-logstash
       </p>
      </td><td>
       <p>
        INFO
       </p>
       <p>
        DEBUG (default)
       </p>
      </td></tr></tbody></table></div><p>
   To enable Cinder logging:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On each Control Node, edit
     <code class="filename">/opt/stack/service/cinder-volume-<em class="replaceable ">CURRENT_VENV</em>/etc/volume-logging.conf</code>
    </p><p>
     In the <code class="literal">Writes to disk</code> section, change
     <code class="literal">WARNING</code> to <code class="literal">DEBUG</code>.
    </p><div class="verbatim-wrap"><pre class="screen"># Writes to disk
[handler_watchedfile]
class: handlers.WatchedFileHandler
args: ('/var/log/cinder/cinder-volume.log',)
formatter: context
# level: WARNING
level: DEBUG</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager (deployer) node, edit
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/cinder/volume.conf.j2</code>,
     adding a line <code class="literal">debug = TRUE</code> to the default section.
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
log_config_append={{cinder_volume_conf_dir }}/volume-logging.conf
debug = True</pre></div></li><li class="step "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>git commit -am "Enable Cinder Debug"
<code class="prompt user">ardana &gt; </code>ansible-playbook config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>sudo grep -i debug /opt/stack/service/cinder-volume-<em class="replaceable ">CURRENT_VENV</em>/etc/volume.conf</pre></div><div class="verbatim-wrap"><pre class="screen">debug = True</pre></div></li></ol></div></div><div id="id-1.6.14.4.8.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Leaving debugs enabled is not recommended. After collecting necessary logs,
    disable debug with the following steps:
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the Cloud Lifecycle Manager (deployer) node, edit
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/cinder/volume.conf.j2</code>,
     comment the line <code class="literal">debug = TRUE</code> in the default section.
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
log_config_append={{cinder_volume_conf_dir }}/volume-logging.conf
#debug = True</pre></div></li><li class="step "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>git commit -am "Disable Cinder Debug"
<code class="prompt user">ardana &gt; </code>ansible-playbook config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>sudo grep -i debug /opt/stack/service/cinder-volume-<em class="replaceable ">CURRENT_VENV</em>/etc/volume.conf</pre></div><div class="verbatim-wrap"><pre class="screen">#debug = True</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-ceilo"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer</span> <a title="Permalink" class="permalink" href="#Loglvl-ceilo">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-ceilo</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>Ceilometer</td><td>
       <p>
        ceilometer-api
       </p>
       <p>
        ceilometer-collector
       </p>
       <p>
        ceilometer-agent-notification
       </p>
       <p>
        ceilometer-agent-central
       </p>
       <p>
        ceilometer-expirer
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Ceilometer logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/_CEI-CMN/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following lines:
    </p><div class="verbatim-wrap"><pre class="screen">ceilometer_loglevel:  INFO
ceilometer_logstash_loglevel:  INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-nova"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (Nova)</span> <a title="Permalink" class="permalink" href="#Loglvl-nova">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-nova</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>nova</td><td> </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Nova logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The Neutron service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/novncproxy-logging.conf.j2
~/openstack/my_cloud/config/nova/api-logging.conf.j2
~/openstack/my_cloud/config/nova/compute-logging.conf.j2
~/openstack/my_cloud/config/nova/conductor-logging.conf.j2
~/openstack/my_cloud/config/nova/consoleauth-logging.conf.j2
~/openstack/my_cloud/config/nova/scheduler-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line in the [handler_logstash] section:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-Designate"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designate</span> <a title="Permalink" class="permalink" href="#Loglvl-Designate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-Designate</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>Designate</td><td>
       <p>
        designate-api
       </p>
       <p>
        designate-central
       </p>
       <p>
        designate-mdns
       </p>
       <p>
        designate-pool-manager
       </p>
       <p>
        designate-zone-manager
       </p>
       <p>
        designate-api-json
       </p>
       <p>
        designate-central-json
       </p>
       <p>
        designate-mdns-json
       </p>
       <p>
        designate-pool-manager-json
       </p>
       <p>
        designate-zone-manager-json
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><div id="id-1.6.14.4.8.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    To change the logging level, see the
    <a class="link" href="http://docs.openstack.org/developer/designate/" target="_blank">OpenStack
    Designate documentation</a>.
   </p></div></div><div class="sect3" id="Loglvl-freezer"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Freezer</span> <a title="Permalink" class="permalink" href="#Loglvl-freezer">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-freezer</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>Freezer</td><td>
       <p>
        freezer-agent
       </p>
       <p>
        freezer-api
       </p>
       <p>
        freezer-scheduler
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
      </td></tr></tbody></table></div><div id="id-1.6.14.4.8.11.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Currently the freezer service does not support any level other than INFO.
   </p></div></div><div class="sect3" id="Loglvl-clm-ux-services"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ARDANA-UX-Services</span> <a title="Permalink" class="permalink" href="#Loglvl-clm-ux-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-clm-ux-services</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>ARDANA-UX-Services</td><td> </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the ARDANA-UX-Services logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/HUX-SVC/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, set the desired level in the following line:
    </p><div class="verbatim-wrap"><pre class="screen">hux_svc_default_log_level: info</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-ux-services-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-keystone"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity (Keystone)</span> <a title="Permalink" class="permalink" href="#Loglvl-keystone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-keystone</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>Keystone</td><td>key-api</td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
       <p>
        WARN
       </p>
       <p>
        ERROR
       </p>
      </td></tr></tbody></table></div><p>
   To change the Keystone logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following lines:
    </p><div class="verbatim-wrap"><pre class="screen">keystone_loglevel: INFO
keystone_logstash_loglevel: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-glance"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image (Glance)</span> <a title="Permalink" class="permalink" href="#Loglvl-glance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-glance</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>Glance</td><td>
       <p>
        glance-api
       </p>
       <p>
        glance-registry
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Glance logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/glance/glance-[api,registry]-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line in the [handler_logstash] section:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-ironic"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic</span> <a title="Permalink" class="permalink" href="#Loglvl-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-ironic</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>ironic</td><td>
       <p>
        ironic-api-logging.conf.j2
       </p>
       <p>
        ironic-conductor-logging.conf.j2
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Ironic logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Change to the following directory:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/ironic</pre></div></li><li class="step "><p>
     To change the logging for one of the sub-components, open one of the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">ironic-api-logging.conf.j2
ironic-conductor-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line in the [handler_logstash] section:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-monasca"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring (Monasca)</span> <a title="Permalink" class="permalink" href="#Loglvl-monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-monasca</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>monasca</td><td>
       <p>
        monasca-persister
       </p>
       <p>
        zookeeper
       </p>
       <p>
        storm
       </p>
       <p>
        monasca-notification
       </p>
       <p>
        monasca-api
       </p>
       <p>
        kafka
       </p>
       <p>
        monasca-agent
       </p>
      </td><td>
       <p>
        WARN (default)
       </p>
       <p>
        INFO
       </p>
      </td></tr></tbody></table></div><p>
   To change the Monasca logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Monitoring service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-persister/defaults/main.yml
~/openstack/ardana/ansible/roles/zookeeper/defaults/main.yml
~/openstack/ardana/ansible/roles/storm/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-api/defaults/main.yml
~/openstack/ardana/ansible/roles/kafka/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-agent/defaults/main.yml (For this file, you will need to add the variable)</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line:
    </p><div class="verbatim-wrap"><pre class="screen">monasca_log_level: WARN</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-neutron"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking (Neutron)</span> <a title="Permalink" class="permalink" href="#Loglvl-neutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-neutron</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>neutron</td><td>
       <p>
        neutron-server
       </p>
       <p>
        dhcp-agent
       </p>
       <p>
        l3-agent
       </p>
       <p>
        lbaas-agent
       </p>
       <p>
        metadata-agent
       </p>
       <p>
        openvswitch-agent
       </p>
       <p>
        vpn-agent
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Neutron logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The Neutron service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/neutron-common/templates/dhcp-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/l3-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/lbaas-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/metadata-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/openvswitch-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/vpn-agent-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line in the [handler_logstash] section:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-swift"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage (Swift)</span> <a title="Permalink" class="permalink" href="#Loglvl-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-swift</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>swift</td><td> </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><div id="id-1.6.14.4.8.18.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Currently it is not recommended to log at any level other than INFO.
   </p></div></div><div class="sect3" id="Loglvl-octavia"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Octavia</span> <a title="Permalink" class="permalink" href="#Loglvl-octavia">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-octavia</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>octavia</td><td>
       <p>
        Octavia-api
       </p>
       <p>
        Octavia-worker
       </p>
       <p>
        Octavia-hk
       </p>
       <p>
        Octavia-hm
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Octavia logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The Octavia service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/octavia/octavia-api.conf.j2
~/openstack/my_cloud/config/octavia/octavia-worker.conf.j2
~/openstack/my_cloud/config/octavia/octavia-hk-logging.conf.j2
~/openstack/my_cloud/config/octavia/Octavia-hm-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line in the [handler_logstash] section:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-opsconsole"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console</span> <a title="Permalink" class="permalink" href="#Loglvl-opsconsole">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-opsconsole</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>opsconsole</td><td>
       <p>
        ops-web
       </p>
       <p>
        ops-mon
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Operations Console logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/OPS-WEV/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line:
    </p><div class="verbatim-wrap"><pre class="screen">ops_console_loglevel: "{{ openstack_loglevel | default('INFO') }}"</pre></div></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-heat"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Orchestration (Heat)</span> <a title="Permalink" class="permalink" href="#Loglvl-heat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-heat</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>heat</td><td>
       <p>
        api-cfn
       </p>
       <p>
        api-cloudwatch
       </p>
       <p>
        api-logging
       </p>
       <p>
        engine
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Heat logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/heat/*-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line in the [handler_logstash] section:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="CL-select-central-logging"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting Files for Centralized Logging</span> <a title="Permalink" class="permalink" href="#CL-select-central-logging">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-select-central-logging</li></ul></div></div></div></div><p>
   As you use <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you might find a need to redefine which log files are
   rotated on disk or transferred to centralized logging. These changes are all
   made in the centralized logging definition files.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the logrotate service to provide rotation, compression, and
   removal of log files. All of the tunable variables for the logrotate process
   itself can be controlled in the following file:
   <code class="literal">~/openstack/ardana/ansible/roles/logging-common/defaults/main.yml</code>
  </p><p>
   You can find the centralized logging definition files for each service in
   the following directory:
   <code class="literal">~/openstack/ardana/ansible/roles/logging-common/vars</code>
  </p><p>
   You can change log settings for a service by following these steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p><p>
     Open the *.yml file for the service or sub-component that you want to
     modify.
    </p><p>
     Using Freezer, the Backup, Restore, and Archive service as an example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/ardana/ansible/roles/logging-common/vars/freezer-agent-clr.yml</pre></div><p>
     Consider the opening clause of the file:
    </p><div class="verbatim-wrap"><pre class="screen">sub_service:
  hosts: FRE-AGN
  name: freezer-agent
  service: freezer</pre></div><p>
     The <span class="bold"><strong>hosts</strong></span> setting defines the role which
     will trigger this logrotate definition being applied to a particular host.
     It can use regular expressions for pattern matching, that is,
     <span class="bold"><strong>NEU-.*</strong></span>.
    </p><p>
     The <span class="bold"><strong>service</strong></span> setting identifies the
     high-level service name associated with this content, which will be used
     for determining log files' collective quotas for storage on disk.
    </p></li><li class="step "><p>
     Verify logging is enabled by locating the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">centralized_logging:
  enabled: true
  format: rawjson</pre></div><div id="id-1.6.14.4.8.22.6.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      When possible, centralized logging is most effective on log files
      generated using logstash-formatted JSON. These files should specify
      <span class="emphasis"><em>format: rawjson</em></span>. When only plaintext log files are
      available, <span class="emphasis"><em>format: json</em></span> is appropriate. (This will
      cause their plaintext log lines to be wrapped in a json envelope before
      being sent to centralized logging storage.)
     </p></div></li><li class="step "><p>
     Observe log files selected for rotation:
    </p><div class="verbatim-wrap"><pre class="screen">- files:
  - /var/log/freezer/freezer-agent.log
  - /var/log/freezer/freezer-scheduler.log
  log_rotate:
  - daily
  - compress
  - missingok
  - notifempty
  - copytruncate
  - maxsize 80M
  - rotate 14</pre></div><div id="id-1.6.14.4.8.22.6.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      With the introduction of dynamic log rotation, the frequency (that is,
      <span class="emphasis"><em>daily</em></span>) and file size threshold (that is,
      <span class="emphasis"><em>maxsize</em></span>) settings no longer have any effect. The
      <span class="emphasis"><em>rotate</em></span> setting may be easily overridden on a
      service-by-service basis.
     </p></div></li><li class="step "><p>
     Commit any changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the logging reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="CL-space-allocation"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Controlling Disk Space Allocation and Retention of Log Files</span> <a title="Permalink" class="permalink" href="#CL-space-allocation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-space-allocation</li></ul></div></div></div></div><p>
   Each service is assigned a weighted allocation of the
   <code class="literal">/var/log</code> filesystem's capacity. When all its log files'
   cumulative sizes exceed this allocation, a rotation is triggered for that
   service's log files according to the behavior specified in the
   <code class="literal">/etc/logrotate.d/*</code> specification.
  </p><p>
   These specification files are auto-generated based on YML sources delivered
   with the Cloud Lifecycle Manager codebase. The source files can be edited and
   reapplied to control the allocation of disk space across services or the
   behavior during a rotation.
  </p><p>
   Disk capacity is allocated as a percentage of the total weighted value of
   all services running on a particular node. For example, if 20 services run
   on the same node, all with a default weight of
   <span class="bold"><strong>100</strong></span>, they will each be granted 1/20th of
   the log filesystem's capacity. If the configuration is updated to change one
   service's weight to <span class="bold"><strong>150</strong></span>, all the services'
   allocations will be adjusted to make it possible for that one service to
   consume 150% of the space available to other individual services.
  </p><p>
   These policies are enforced by the script
   <code class="literal">/opt/kronos/rotate_if_exceeded_quota.py</code>, which will be
   executed every 5 minutes via a cron job and will rotate the log files of any
   services which have exceeded their respective quotas. When log rotation
   takes place for a service, logs are generated to describe the activity in
   <code class="literal">/var/log/kronos/check_if_exceeded_quota.log</code>.
  </p><p>
   When logrotate is performed on a service, its existing log files are
   compressed and archived to make space available for fresh log entries. Once
   the number of archived log files exceeds that service's retention
   thresholds, the oldest files are deleted. Thus, longer retention thresholds
   (that is, 10 to 15) will result in more space in the service's allocated log
   capacity being used for historic logs, while shorter retention thresholds
   (that is, 1 to 5) will keep more space available for its active plaintext log
   files.
  </p><p>
   Use the following process to make adjustments to services' log capacity
   allocations or retention thresholds:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Navigate to the following directory on your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">~/stack/scratch/ansible/next/ardana/ansible</pre></div></li><li class="step "><p>
     Open and edit the service weights file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi roles/kronos-logrotation/vars/rotation_config.yml</pre></div></li><li class="step "><p>
     Edit the service parameters to set the desired parameters. Example:
    </p><div class="verbatim-wrap"><pre class="screen">cinder:
  weight: 300
  retention: 2</pre></div><div id="id-1.6.14.4.8.23.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The retention setting of <span class="emphasis"><em>default</em></span> will use recommend
      defaults for each services' log files.
     </p></div></li><li class="step "><p>
     Run the kronos-logrotation-deploy playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-logrotation-deploy.yml</pre></div></li><li class="step "><p>
     Verify the changes to the quotas have been changed:
    </p><p>
     Login to a node and check the contents of the file
     /opt/kronos/service_info.yml to see the active quotas for that node, and
     the specifications in /etc/logrotate.d/* for rotation thresholds.
    </p></li></ol></div></div></div><div class="sect3" id="CL-elasticsearch-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Elasticsearch for Centralized Logging</span> <a title="Permalink" class="permalink" href="#CL-elasticsearch-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-elasticsearch-config</li></ul></div></div></div></div><p>
   Elasticsearch includes some tunable options exposed in its configuration.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses these options in Elasticsearch to prioritize indexing speed
   over search speed. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> also configures Elasticsearch for optimal
   performance in low RAM environments. The options that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> modifies are
   listed below along with an explanation about why they were modified.
  </p><p>
   These configurations are defined in the
   <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code> file and are
   implemented in the Elasticsearch configuration file
   <code class="literal">~/openstack/my_cloud/config/logging/elasticsearch.yml.j2</code>.
  </p></div><div class="sect3" id="CL-safeguards"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.6.22 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Safeguards for the Log Partitions Disk Capacity</span> <a title="Permalink" class="permalink" href="#CL-safeguards">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-safeguards</li></ul></div></div></div></div><p>
   Because the logging partitions are at a high risk of filling up over time, a
   condition which can cause many negative side effects on services running, it
   is important to safeguard against log files consuming 100 % of available
   capacity.
  </p><p>
   This protection is implemented by pairs of low/high
   <span class="bold"><strong>watermark</strong></span> thresholds, with values
   established in
   <code class="literal">~/stack/scratch/ansible/next/ardana/ansible/roles/logging-common/defaults/main.yml</code>
   and applied by the <code class="literal">kronos-logrotation-deploy</code> playbook.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>var_log_low_watermark_percent</strong></span> (default:
     80) sets a capacity level for the contents of the
     <code class="literal">/var/log</code> partition beyond which alarms will be
     triggered (visible to administrators in Monasca).
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_log_high_watermark_percent</strong></span> (default:
     95) defines how much capacity of the <code class="literal">/var/log</code> partition
     to make available for log rotation (in calculating weighted service
     allocations).
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_audit_low_watermark_percent</strong></span> (default:
     80) sets a capacity level for the contents of the
     <code class="literal">/var/audit</code> partition beyond which alarm notifications
     will be triggered.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_audit_high_watermark_percent</strong></span>
     (default: 95) sets a capacity level for the contents of the
     <code class="literal">/var/audit</code> partition which will cause log rotation to
     be forced according to the specification in
     <code class="literal">/etc/auditlogrotate.conf</code>.
    </p></li></ul></div></div></div><div class="sect2" id="topic-overview-audit-logs"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Logging Overview</span> <a title="Permalink" class="permalink" href="#topic-overview-audit-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_overview.xml</li><li><span class="ds-label">ID: </span>topic-overview-audit-logs</li></ul></div></div></div></div><p>
  Existing OpenStack service logging varies widely across services. Generally,
  log messages do not have enough detail about who is requesting the
  application program interface (API), or enough context-specific details about
  an action performed. Often details are not even consistently logged across
  various services, leading to inconsistent data formats being used across
  services. These issues make it difficult to integrate logging with existing
  audit tools and processes.
 </p><p>
  To help you monitor your workload and data in compliance with your corporate,
  industry or regional policies, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides auditing support as a basic
  security feature. The audit logging can be integrated with customer Security
  Information and Event Management (SIEM) tools and support your efforts to
  correlate threat forensics.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> audit logging feature uses Audit Middleware for Python services.
  This middleware service is based on OpenStack services which use the Paste
  Deploy system. Most OpenStack services use the paste deploy mechanism to find
  and configure WSGI servers and applications. Utilizing the paste deploy
  system provides auditing support in services with minimal changes.
 </p><p>
  By default, audit logging as a post-installation feature is disabled in the
  cloudConfig file on the Cloud Lifecycle Manager and it can only be enabled after
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation or upgrade.
 </p><p>
  The tasks in this section explain how to enable services for audit logging in
  your environment. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides audit logging for the following services:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Nova
   </p></li><li class="listitem "><p>
    Barbican
   </p></li><li class="listitem "><p>
    Keystone
   </p></li><li class="listitem "><p>
    Cinder
   </p></li><li class="listitem "><p>
    Ceilometer
   </p></li><li class="listitem "><p>
    Neutron
   </p></li><li class="listitem "><p>
    Glance
   </p></li><li class="listitem "><p>
    Heat
   </p></li></ul></div><p>
  For audit log backup information see <a class="xref" href="#backup-audit-logs" title="14.13. Backing up and Restoring Audit Logs">Section 14.13, “Backing up and Restoring Audit Logs”</a>
 </p><div class="sect3" id="idg-all-operations-audit-logs-checklist-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Logging Checklist</span> <a title="Permalink" class="permalink" href="#idg-all-operations-audit-logs-checklist-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-audit-logs-checklist-xml-1</li></ul></div></div></div></div><p>
  Before enabling audit logging, make sure you understand how much disk space
  you will need, and configure the disks that will store the logging data. Use
  the following table to complete these tasks:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-FAQ" title="12.2.7.1.1. Frequently Asked Questions">Section 12.2.7.1.1, “Frequently Asked Questions”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-log-est" title="12.2.7.1.2. Estimate Disk Size">Section 12.2.7.1.2, “Estimate Disk Size”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-add-disks" title="12.2.7.1.3. Add disks to the controller nodes">Section 12.2.7.1.3, “Add disks to the controller nodes”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-update-disks" title="12.2.7.1.4. Update the disk template for the controller nodes">Section 12.2.7.1.4, “Update the disk template for the controller nodes”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-save-update-disks" title="12.2.7.1.5. Save your changes">Section 12.2.7.1.5, “Save your changes”</a>
      </p>
     </td></tr></tbody></table></div><div class="sect4" id="audit-FAQ"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Frequently Asked Questions</span> <a title="Permalink" class="permalink" href="#audit-FAQ">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-FAQ</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.4.9.9.4.2.1"><span class="term ">How are audit logs generated?</span></dt><dd><p>
      The audit logs are created by services running in the cloud management
      controller nodes. The events that create auditing entries are formatted
      using a structure that is compliant with Cloud Auditing Data Federation
      (CADF) policies. The formatted audit entries are then saved to disk
      files. For more information, see the
      <a class="link" href="http://www.dmtf.org/standards/cadf" target="_blank">Cloud Auditing Data
      Federation Website.</a>
     </p></dd><dt id="id-1.6.14.4.9.9.4.2.2"><span class="term ">Where are audit logs stored?</span></dt><dd><p>
      We strongly recommend adding a dedicated disk volume for
      <code class="literal">/var/audit</code>.
     </p><p>
      If the disk templates for the controllers are not updated to create a
      separate volume for <code class="filename">/var/audit</code>,
      the audit logs will still be created in
      the root partition under the folder <code class="filename">/var/audit</code>. This
      could be problematic if the root partition does not have adequate space to
      hold the audit logs.
     </p><div id="id-1.6.14.4.9.9.4.2.2.2.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
       We recommend that you do <span class="bold"><strong>not</strong></span> store
       audit logs in the <code class="filename">/var/log</code> volume. The
       <code class="filename">/var/log</code> volume is used for storing operational logs
       and logrotation/alarms have been preconfigured for various services
       based on the size of this volume. Adding audit logs here may impact
       these causing undesired alarms. This would also impact the retention
       times for the operational logs.
      </p></div></dd><dt id="id-1.6.14.4.9.9.4.2.3"><span class="term ">Are audit logs centrally stored?</span></dt><dd><p>
      Yes. The existing operational log profiles have been configured to
      centrally log audit logs as well, once their generation has been enabled.
      The audit logs will be stored in separate Elasticsearch indices separate
      from the operational logs.
     </p></dd><dt id="id-1.6.14.4.9.9.4.2.4"><span class="term ">How long are audit log files retained?</span></dt><dd><p>
      By default, audit logs are configured to be retained for 7 days on disk.
      The audit logs are rotated each day and the rotated files are stored in a
      compressed format and retained up to 7 days (configurable). The backup
      service has been configured to back up the audit logs to a location
      outside of the controller nodes for much longer retention periods.
     </p></dd><dt id="id-1.6.14.4.9.9.4.2.5"><span class="term ">Do I lose audit data if a management controller node goes down?</span></dt><dd><p>
      Yes. For this reason, it is strongly recommended that you back up the
      audit partition in each of the management controller nodes for protection
      against any data loss.
     </p></dd></dl></div></div><div class="sect4" id="audit-log-est"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Estimate Disk Size</span> <a title="Permalink" class="permalink" href="#audit-log-est">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-log-est</li></ul></div></div></div></div><p>
   The table below provides estimates from each service of audit log size
   generated per day. The estimates are provided for environments with 100
   nodes, 300 nodes, and 500 nodes.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Service</th><th>
       <p>
        Log File Size: 100 nodes
       </p>
      </th><th>
       <p>
        Log File Size: 300 nodes
       </p>
      </th><th>
       <p>
        Log File Size: 500 nodes
       </p>
      </th></tr></thead><tbody><tr><td>Barbican</td><td>2.6 MB</td><td>4.2 MB</td><td>5.6 MB</td></tr><tr><td>Keystone</td><td>96 - 131 MB</td><td>288 - 394 MB</td><td>480 - 657 MB</td></tr><tr><td>Nova</td><td>186 (with a margin of 46) MB</td><td>557 (with a margin of 139) MB</td><td>928 (with a margin of 232) MB</td></tr><tr><td>Ceilometer</td><td>12 MB</td><td>12 MB</td><td>12 MB</td></tr><tr><td>Cinder</td><td>2 - 250 MB</td><td>2 - 250 MB</td><td>2 - 250 MB</td></tr><tr><td>Neutron</td><td>145 MB</td><td>433 MB</td><td>722 MB</td></tr><tr><td>Glance</td><td>20 (with a margin of 8) MB</td><td>60 (with a margin of 22) MB</td><td>100 (with a margin of 36) MB</td></tr><tr><td>Heat</td><td>432 MB (1 transaction per second)</td><td>432 MB (1 transaction per second)</td><td>432 MB (1 transaction per second)</td></tr><tr><td>Swift</td><td>33 GB (700 transactions per second)</td><td>102 GB (2100 transactions per second)</td><td>172 GB (3500 transactions per second)</td></tr></tbody></table></div></div><div class="sect4" id="audit-add-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add disks to the controller nodes</span> <a title="Permalink" class="permalink" href="#audit-add-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-add-disks</li></ul></div></div></div></div><p>
   You need to add disks for the audit log partition to store the data in a
   secure manner. The steps to complete this task will vary depending on the
   type of server you are running. Please refer to the manufacturer’s
   instructions on how to add disks for the type of server node used by the
   management controller cluster. If you already have extra disks in the
   controller node, you can identify any unused one and use it for the audit
   log partition.
  </p></div><div class="sect4" id="audit-update-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the disk template for the controller nodes</span> <a title="Permalink" class="permalink" href="#audit-update-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-update-disks</li></ul></div></div></div></div><p>
   Since audit logging is disabled by default, the audit volume groups in the
   disk templates are commented out. If you want to turn on audit logging, the
   template needs to be updated first. If it is not updated, there will be no
   back-up volume group. To update the disk template, you will need to copy
   templates from the examples folder to the definition folder and then edit
   the disk controller settings. Changes to the disk template used for
   provisioning cloud nodes must be made prior to deploying the nodes.
  </p><p>
   To update the disk controller template:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To copy the example templates folder, run the following command:
    </p><div id="id-1.6.14.4.9.9.7.4.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      If you already have the required templates in the definition folder, you
      can skip this step.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/entry-scale-esx/* ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     To change to the data folder, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     To edit the disks controller settings, open the file that matches your
     server model and disk model in a text editor:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Model</th><th>File</th></tr></thead><tbody><tr><td>entry-scale-kvm</td><td>
<div class="verbatim-wrap"><pre class="screen">disks_controller_1TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_controller_600GB.yml</pre></div>
        </td></tr><tr><td>mid-scale</td><td>
<div class="verbatim-wrap"><pre class="screen">disks_compute.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_control_common_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_dbmq_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_2TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_4.5TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_swobj.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_swpac.yml</pre></div>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     To update the settings and enable an audit log volume group, edit the
     appropriate file(s) listed above and remove the '#' comments from these
     lines, confirming that they are appropriate for your environment.
    </p><div class="verbatim-wrap"><pre class="screen">- name: audit-vg
  physical-volumes:
    - /dev/sdz
  logical-volumes:
    - name: audit
      size: 95%
      mount: /var/audit
      fstype: ext4
      mkfs-opts: -O large_file</pre></div></li></ol></div></div><div class="sect4" id="audit-save-update-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Save your changes</span> <a title="Permalink" class="permalink" href="#audit-save-update-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-save-update-disks</li></ul></div></div></div></div><p>
   To save your changes you will use the GIT repository to add the setup disk
   files.
  </p><p>
   To save your changes:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the openstack directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack</pre></div></li><li class="listitem "><p>
     To add the new and updated files, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A</pre></div></li><li class="listitem "><p>
     To verify the files are added, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div></li><li class="listitem "><p>
     To commit your changes, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Setup disks for audit logging"</pre></div></li></ol></div></div></div><div class="sect3" id="topic-enable-audit-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Audit Logging</span> <a title="Permalink" class="permalink" href="#topic-enable-audit-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>topic-enable-audit-logs</li></ul></div></div></div></div><p>
  To enable audit logging you must edit your cloud configuration settings, save
  your changes and re-run the configuration processor. Then you can run the
  playbooks to create the volume groups and configure them.
 </p><p>
  In the <code class="literal">~/openstack/my_cloud/definition/cloudConfig.yml</code> file,
  service names defined under enabled-services or disabled-services override
  the default setting.
 </p><p>
  The following is an example of your audit-settings section:
 </p><div class="verbatim-wrap"><pre class="screen"># Disc space needs to be allocated to the audit directory before enabling
# auditing.
# Default can be either "disabled" or "enabled". Services listed in
# "enabled-services" and "disabled-services" override the default setting.
audit-settings:
   default: disabled
   #enabled-services:
   #  - keystone
   #  - barbican
   disabled-services:
     - nova
     - barbican
     - keystone
     - cinder
     - ceilometer
     - neutron</pre></div><p>
  In this example, although the default setting for all services is set to
  <span class="bold"><strong>disabled</strong></span>, keystone and barbican may be
  explicitly enabled by removing the comments from these lines and this setting
  overrides the default.
 </p><div class="sect4" id="audit-edit-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To edit the configuration file:</span> <a title="Permalink" class="permalink" href="#audit-edit-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-edit-config</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To change to the cloud definition folder, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition</pre></div></li><li class="listitem "><p>
     To edit the auditing settings, in a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">cloudConfig.yml</pre></div></li><li class="listitem "><p>
     To enable audit logging, begin by uncommenting the "enabled-services:"
     block.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       enabled-service:
      </p></li><li class="listitem "><p>
       any service you want to enable for audit logging.
      </p></li></ul></div><p>
     For example, Keystone has been enabled in the following text:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Default cloudConfig.yml file</th><th>Enabling Keystone audit logging</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
#  - keystone</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
  - keystone</pre></div>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     To move the services you want to enable, comment out the service in the
     disabled section and add it to the enabled section. For example, Barbican
     has been enabled in the following text:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>cloudConfig.yml file</th><th>Enabling Barbican audit logging</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
  - keystone
disabled-services:
   - nova
   # - keystone
   - barbican
   - cinder</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
 - keystone
 - barbican
disabled-services:
 - nova
 # - barbican
 # - keystone
 - cinder</pre></div>
        </td></tr></tbody></table></div></li></ol></div></div><div class="sect4" id="audit-save-config2"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To save your changes and run the configuration processor:</span> <a title="Permalink" class="permalink" href="#audit-save-config2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-save-config2</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the openstack directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack</pre></div></li><li class="listitem "><p>
     To add the new and updated files, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A</pre></div></li><li class="listitem "><p>
     To verify the files are added, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div></li><li class="listitem "><p>
     To commit your changes, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Enable audit logging"</pre></div></li><li class="listitem "><p>
     To change to the directory with the ansible playbooks, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="listitem "><p>
     To rerun the configuration processor, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div><div class="sect4" id="audit-create-vgroup"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To create the volume group:</span> <a title="Permalink" class="permalink" href="#audit-create-vgroup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-create-vgroup</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the osconfig playbook, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To remove the stub file that osconfig uses to decide if the disks are
     already configured, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts KEY-API -a 'sudo rm -f /etc/hos/osconfig-ran'</pre></div><div id="id-1.6.14.4.9.10.9.2.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      The osconfig playbook uses the stub file to mark already configured disks
      as "idempotent." To stop osconfig from identifying your new disk as
      already configured, you must remove the stub file /etc/hos/osconfig-ran
      before re-running the osconfig playbook.
     </p></div></li><li class="listitem "><p>
     To run the playbook that enables auditing for a service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API</pre></div><div id="id-1.6.14.4.9.10.9.2.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      The variable KEY-API is used as an example to cover the management
      controller cluster. To enable auditing for a service that is not run on
      the same cluster, add the service to the –limit flag in the above
      command. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API:NEU-SVR</pre></div></div></li></ol></div></div><div class="sect4" id="audit-reconfig-services"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.2.7.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Reconfigure services for audit logging:</span> <a title="Permalink" class="permalink" href="#audit-reconfig-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-reconfig-services</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the service playbooks, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the playbook that reconfigures a service for audit logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts <em class="replaceable ">SERVICE_NAME</em>-reconfigure.yml</pre></div><p>
     For example, to reconfigure Keystone for audit logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Repeat steps 1 and 2 for each service you need to reconfigure.
    </p><div id="id-1.6.14.4.9.10.10.2.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      You must reconfigure each service that you changed to be enabled or
      disabled in the cloudConfig.yml file.
     </p></div></li></ol></div></div></div></div><div class="sect2" id="id-1.6.14.4.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#id-1.6.14.4.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-centralized_logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-centralized_logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For information on troubleshooting Central Logging, see
   <a class="xref" href="#sec-central-log-troubleshoot" title="15.7.1. Troubleshooting Centralized Logging">Section 15.7.1, “Troubleshooting Centralized Logging”</a>.
  </p></div></div><div class="sect1" id="ceilo-metering-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service (Ceilometer) Overview</span> <a title="Permalink" class="permalink" href="#ceilo-metering-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_overview.xml</li><li><span class="ds-label">ID: </span>ceilo-metering-overview</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> metering service collects and provides access to OpenStack
   usage data that can be used for billing reporting such as showback, and
   chargeback. The metering service can also provide general usage reporting.
   Ceilometer acts as the central collection and data access service to
   the meters provided by all the OpenStack services. The data collected
   is available both through the Monasca API and the Ceilometer V2 API.
 </p><div id="id-1.6.14.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     Ceilometer V2 API has been deprecated in Pike release upstream.
     Although the Ceilometer V2 API is still available with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, to prepare
     for eventual removal of  Ceilometer V2 API in next release we recommend that
     users switch to the Monasca API to access data.
   </p></div><div class="sect2" id="Metering-NewFunctions"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service New Functionality</span> <a title="Permalink" class="permalink" href="#Metering-NewFunctions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>Metering-NewFunctions</li></ul></div></div></div></div><div class="sect3" id="newfunct"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Metering Functionality in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span></span> <a title="Permalink" class="permalink" href="#newfunct">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>newfunct</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Ceilometer is now integrated with Monasca to use it as the datastore.
     Ceilometer API also now queries the Monasca datastore using the Monasca
     API (query) instead of the MySQL database
    </p></li><li class="listitem "><p>
     The default meters and other items configured for the Ceilometer API can
     now be modified and additional meters can be added. It is highly
     recommended that customers test overall <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> performance prior to
     deploying any Ceilometer modifications to ensure the addition of new
     notifications or polling events does not negatively affect overall system
     performance.
    </p></li><li class="listitem "><p>
     Ceilometer Central Agent (pollster) is now called Polling Agent and is
     configured to support HA (Active-Active)
    </p></li><li class="listitem "><p>
     Notification Agent has built-in HA (Active-Active) with support for
     pipeline transformers, but workload partitioning has been disabled in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    </p></li><li class="listitem "><p>
     SWIFT Poll-based account level meters will be enabled by default with an
     hourly collection cycle.
    </p></li><li class="listitem "><p>
     Integration with centralized monitoring (Monasca) and centralized logging
    </p></li><li class="listitem "><p>
     Support for upgrade and reconfigure operations
    </p></li></ul></div></div><div class="sect3" id="idg-all-metering-metering-newfunctions-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#idg-all-metering-metering-newfunctions-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-newfunctions-xml-7</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Ceilometer Post Meter API is disabled by default.
    </p></li><li class="listitem "><p>
     The Ceilometer Events and Traits API is not supported and disabled by
     default.
    </p></li><li class="listitem "><p>
     The Number of metadata attributes that can be extracted from
     resource_metadata has a maximum of 16. This is the number of fields in the
     metadata section of the
     <span class="bold"><strong>monasca_field_definitions.yaml</strong></span> file for
     any service. It is also the number that is equal to fields in
     metadata.common and fields in metadata.&lt;service.meters&gt; sections.
     The total number of these fields cannot be more than 16.
    </p></li><li class="listitem "><p>
     Several network-related attributes are accessible using a colon ":" but
     are returned as a period ".". For example, you can access a sample list
     using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>ceilometer --debug sample-list network -q "resource_id=421d50a5-156e-4cb9-b404-
d2ce5f32f18b;resource_metadata.provider.network_type=flat"</pre></div><p>
     However, in response you will see the following:
    </p><div class="verbatim-wrap"><pre class="screen">provider.network_type</pre></div><p>
     instead of
    </p><div class="verbatim-wrap"><pre class="screen">provider:network_type</pre></div><p>
     This limitation is known for the following attributes:
    </p><div class="verbatim-wrap"><pre class="screen">provider:network_type
provider:physical_network
provider:segmentation_id</pre></div></li><li class="listitem "><p>
     Ceilometer Expirer is unsupported. Data retention expiration is now
     handled by Monasca with a default retention period of 45 days.
    </p></li><li class="listitem "><p>
     Ceilometer Collector is unsupported.
    </p></li><li class="listitem "><p>
     The Ceilometer Alarms API is disabled by default. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> provides
     an alternative operations monitoring service that will provide support for
     operations monitoring, alerts, and notifications use cases.
    </p></li></ul></div></div></div><div class="sect2" id="ceilo-metering-concepts-overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Metering Service Concepts</span> <a title="Permalink" class="permalink" href="#ceilo-metering-concepts-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-metering-concepts-overview</li></ul></div></div></div></div><div class="sect3" id="ceilo-concept-intro"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Introduction</span> <a title="Permalink" class="permalink" href="#ceilo-concept-intro">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-concept-intro</li></ul></div></div></div></div><p>
   Before configuring the Ceilometer Metering Service, make sure you understand
   how it works.
  </p><div class="sect4" id="ceilo-architecture"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.3.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Architecture</span> <a title="Permalink" class="permalink" href="#ceilo-architecture">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-architecture</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> automatically configures Ceilometer to use Logging and
   Monitoring Service (Monasca) as its backend. Ceilometer is deployed on the
   same control plane nodes as Monasca.
  </p><p>
   The installation of Celiometer creates several management nodes running
   different metering components.
  </p><p>
   <span class="bold"><strong>Ceilometer Components on Controller nodes</strong></span>
  </p><p>
   This controller node is the first of the High Available (HA) cluster. In
   this node there is an instance of the Ceilometer API running under the HA
   Proxy Virtual IP address.
  </p><p>
   <span class="bold"><strong>Ceilometer Sample Polling</strong></span>
  </p><p>
   Sample Polling is part of the Polling Agent. Now that Ceilometer API uses
   Monasca API (query) instead of the MySQL database, messages are posted by
   Notification Agent directly to Monasca API.
  </p><p>
   <span class="bold"><strong>Ceilometer Polling Agent</strong></span>
  </p><p>
   The Polling Agent is responsible for coordinating the polling activity. It
   parses the <span class="bold"><strong>pipeline.yml</strong></span> configuration file
   and identifies all the sources that need to be polled. The sources are then
   evaluated using a discovery mechanism and all the sources are translated to
   resources where a dedicated pollster can retrieve and publish data. At each
   identified interval the discovery mechanism is triggered, the resource list
   is composed, and the data is polled and sent to the queue.
  </p><p>
   <span class="bold"><strong>Ceilometer Collector No Longer Required</strong></span>
  </p><p>
   In previous versions, the collector was responsible for getting the
   samples/events from the RabbitMQ service and storing it in the main
   database. The Ceilometer Collector is no longer enabled. Now that
   Notification Agent posts the data directly to Monasca API, the collector is
   no longer required
  </p></div><div class="sect4" id="ceilo-about-meters"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.3.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Meter Reference</span> <a title="Permalink" class="permalink" href="#ceilo-about-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-about-meters</li></ul></div></div></div></div><p>
   The Ceilometer API collects basic information grouped into categories known
   as meters. A meter is the unique resource-usage measurement of a particular
   OpenStack service. Each OpenStack service defines what type of data is
   exposed for metering.
  </p><p>
   Each meter has the following characteristics:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Attribute</th><th>Description</th></tr></thead><tbody><tr><td>Name</td><td>Description of the meter</td></tr><tr><td>Unit of Measurement</td><td>The method by which the data is measured. For example: storage meters are
                defined in Gigabytes (GB) and network bandwidth is measured in Gigabits
                (Gb).</td></tr><tr><td>Type</td><td><p>The origin of the meter's data. OpenStack defines the following origins: </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Cumulative - Increasing over time (instance hours)
         </p></li><li class="listitem "><p>
          Gauge - a discrete value. For example: the number of floating IP
          addresses or image uploads.
         </p></li><li class="listitem "><p>
          Delta - Changing over time (bandwidth)
         </p></li></ul></div>
      </td></tr></tbody></table></div><p>
   A meter is defined for every measurable resource. A meter can exist beyond
   the actual existence of a particular resource, such as an active instance,
   to provision long-cycle use cases such as billing.
  </p><div id="id-1.6.14.5.5.2.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    For a list of meter types and default meters installed with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, see
    <a class="xref" href="#topic3051" title="12.3.3. Ceilometer Metering Available Meter Types">Section 12.3.3, “Ceilometer Metering Available Meter Types”</a>
   </p></div><p>
   The most common meter submission method is notifications. With this method,
   each service sends the data from their respective meters on a periodic basis
   to a common notifications bus.
  </p><p>
   Ceilometer, in turn, pulls all of the events from the bus and saves the
   notifications in a Ceilometer-specific database. The period of time that the
   data is collected and saved is known as the Ceilometer expiry and is
   configured during Ceilometer installation. Each meter is collected from one
   or more samples, gathered from the messaging queue or polled by agents. The
   samples are represented by counter objects. Each counter has the following
   fields:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Attribute</th><th>Description</th></tr></thead><tbody><tr><td>counter_name</td><td>Description of the counter</td></tr><tr><td>counter_unit</td><td>The method by which the data is measured. For example: data can be
                defined in Gigabytes (GB) or for network bandwidth, measured in Gigabits
                (Gb).</td></tr><tr><td>counter_typee</td><td>
       <p>The origin of the counter's data. OpenStack defines the following origins:</p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Cumulative - Increasing over time (instance hours)
         </p></li><li class="listitem "><p>
          Gauge - a discrete value. For example: the number of floating IP
          addresses or image uploads.
         </p></li><li class="listitem "><p>
          Delta - Changing over time (bandwidth)
         </p></li></ul></div>
      </td></tr><tr><td>counter_volume</td><td>The volume of data measured (CPU ticks, bytes transmitted, etc.). Not used for gauge
                counters. Set to a default value such as 1.</td></tr><tr><td>resource_id</td><td>The identifier of the resource measured (UUID)</td></tr><tr><td>project_id</td><td>The project (tenant) ID to which the resource belongs.</td></tr><tr><td>user_id</td><td>The ID of the user who owns the resource.</td></tr><tr><td>resource_metadata</td><td>Other data transmitted in the metering notification payload.</td></tr></tbody></table></div></div><div class="sect4" id="Ceilo-rbac-ov"><div class="titlepage"><div><div><h5 class="title"><span class="number">12.3.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Role Based Access Control (RBAC)</span> <a title="Permalink" class="permalink" href="#Ceilo-rbac-ov">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>Ceilo-rbac-ov</li></ul></div></div></div></div><p>
   A user with the <span class="bold"><strong>admin role</strong></span> can access all
   API functions across all projects by default. Ceilometer also supports the
   ability to assign access to a specific API function by
   <span class="bold"><strong>project</strong></span> and
   <span class="bold"><strong>UserID</strong></span>. User access is configured in the
   Ceilometer policy file and enables you to grant specific API functions to
   specific users for a specific project.
  </p><p>
   For instructions on how to configure role-based access, see
   <a class="xref" href="#topic15050" title="12.3.7. Ceilometer Metering Setting Role-based Access Control">Section 12.3.7, “Ceilometer Metering Setting Role-based Access Control”</a>.
  </p></div></div></div><div class="sect2" id="topic3051"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Available Meter Types</span> <a title="Permalink" class="permalink" href="#topic3051">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_metertypes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_metertypes.xml</li><li><span class="ds-label">ID: </span>topic3051</li></ul></div></div></div></div><p>
  The Metering service contains three types of meters:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.5.6.3.1"><span class="term ">Cumulative</span></dt><dd><p>
     A cumulative meter measures data over time (for example, instance hours).
    </p></dd><dt id="id-1.6.14.5.6.3.2"><span class="term ">Gauge</span></dt><dd><p>
     A gauge measures discrete items (for example, floating IPs or image
     uploads) or fluctuating values (such as disk input or output).
    </p></dd><dt id="id-1.6.14.5.6.3.3"><span class="term ">Delta</span></dt><dd><p>
     A delta measures change over time, for example, monitoring bandwidth.
    </p></dd></dl></div><p>
  Each meter is populated from one or more <span class="emphasis"><em>samples</em></span>, which
  are gathered from the messaging queue (listening agent), polling agents, or
  push agents. Samples are populated by <span class="emphasis"><em>counter</em></span> objects.
 </p><p>
  Each counter contains the following <span class="emphasis"><em>fields</em></span>:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.5.6.6.1"><span class="term ">name</span></dt><dd><p>
     the name of the meter
    </p></dd><dt id="id-1.6.14.5.6.6.2"><span class="term ">type</span></dt><dd><p>
     the type of meter (cumulative, gauge, or delta)
    </p></dd><dt id="id-1.6.14.5.6.6.3"><span class="term ">amount</span></dt><dd><p>
     the amount of data measured
    </p></dd><dt id="id-1.6.14.5.6.6.4"><span class="term ">unit</span></dt><dd><p>
     the unit of measure
    </p></dd><dt id="id-1.6.14.5.6.6.5"><span class="term ">resource</span></dt><dd><p>
     the resource being measured
    </p></dd><dt id="id-1.6.14.5.6.6.6"><span class="term ">project ID</span></dt><dd><p>
     the project the resource is assigned to
    </p></dd><dt id="id-1.6.14.5.6.6.7"><span class="term ">user</span></dt><dd><p>
     the user the resource is assigned to.
    </p></dd></dl></div><p>
  <span class="bold"><strong>Note</strong></span>: The metering service shares the same
  High-availability proxy, messaging, and database clusters with the other
  Information services. To avoid unnecessarily high loads,
  <a class="xref" href="#Ceilo-optimize" title="12.3.9. Optimizing the Ceilometer Metering Service">Section 12.3.9, “Optimizing the Ceilometer Metering Service”</a>.
 </p><div class="sect3" id="openstack-default-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Default Meters</span> <a title="Permalink" class="permalink" href="#openstack-default-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_metertypes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_metertypes.xml</li><li><span class="ds-label">ID: </span>openstack-default-meters</li></ul></div></div></div></div><p>
   These meters are installed and enabled by default during an <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation.
  </p><p>
   Detailed information on the Ceilometer API can be found on the following
   page:
  </p><p>
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/webapi/v2.html" target="_blank">Ceilometer
   Web API</a>.
  </p></div><div class="sect3" id="nova-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (Nova) Meters</span> <a title="Permalink" class="permalink" href="#nova-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-nova_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-nova_meters.xml</li><li><span class="ds-label">ID: </span>nova-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>vcpus</td><td>Gauge</td><td>vcpu</td><td>Instance ID</td><td>Notification</td><td>Number of virtual CPUs allocated to the instance</td></tr><tr><td>memory</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Notification</td><td>Volume of RAM allocated to the instance</td></tr><tr><td>memory.resident</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Pollster</td><td>Volume of RAM used by the instance on the physical machine</td></tr><tr><td>memory.usage</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Pollster</td><td>Volume of RAM used by the instance from the amount of its allocated
                memory</td></tr><tr><td>cpu</td><td>Cumulative</td><td>ns</td><td>Instance ID</td><td>Pollster</td><td>CPU time used</td></tr><tr><td>cpu_util</td><td>Gauge</td><td>%</td><td>Instance ID</td><td>Pollster</td><td>Average CPU utilization</td></tr><tr><td>disk.read.requests</td><td>Cumulative</td><td>request</td><td>Instance ID</td><td>Pollster</td><td>Number of read requests</td></tr><tr><td>disk.read.requests.rate</td><td>Gauge</td><td>request/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of read requests</td></tr><tr><td>disk.write.requests</td><td>Cumulative</td><td>request</td><td>Instance ID</td><td>Pollster</td><td>Number of write requests</td></tr><tr><td>disk.write.requests.rate</td><td>Gauge</td><td>request/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of write requests</td></tr><tr><td>disk.read.bytes</td><td>Cumulative</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>Volume of reads</td></tr><tr><td>disk.read.bytes.rate</td><td>Gauge</td><td>B/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of reads</td></tr><tr><td>disk.write.bytes</td><td>Cumulative</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>Volume of writes</td></tr><tr><td>disk.write.bytes.rate</td><td>Gauge</td><td>B/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of writes</td></tr><tr><td>disk.root.size</td><td>Gauge</td><td>GB</td><td>Instance ID</td><td>Notification</td><td>Size of root disk</td></tr><tr><td>disk.ephemeral.size</td><td>Gauge</td><td>GB</td><td>Instance ID</td><td>Notification</td><td>Size of ephemeral disk</td></tr><tr><td>disk.device.read.requests</td><td>Cumulative</td><td>request</td><td>Disk ID</td><td>Pollster</td><td>Number of read requests</td></tr><tr><td>disk.device.read.requests.rate</td><td>Gauge</td><td>request/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of read requests</td></tr><tr><td>disk.device.write.requests</td><td>Cumulative</td><td>request</td><td>Disk ID</td><td>Pollster</td><td>Number of write requests</td></tr><tr><td>disk.device.write.requests.rate</td><td>Gauge</td><td>request/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of write requests</td></tr><tr><td>disk.device.read.bytes</td><td>Cumulative</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>Volume of reads</td></tr><tr><td>disk.device.read.bytes .rate</td><td>Gauge</td><td>B/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of reads</td></tr><tr><td>disk.device.write.bytes</td><td>Cumulative</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>Volume of writes</td></tr><tr><td>disk.device.write.bytes .rate</td><td>Gauge</td><td>B/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of writes</td></tr><tr><td>disk.capacity</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The amount of disk that the instance can see</td></tr><tr><td>disk.allocation</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The amount of disk occupied by the instance on the host machine</td></tr><tr><td>disk.usage</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The physical size in bytes of the image container on the host</td></tr><tr><td>disk.device.capacity</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The amount of disk per device that the instance can see</td></tr><tr><td>disk.device.allocation</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The amount of disk per device occupied by the instance on the host
                machine</td></tr><tr><td>disk.device.usage</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The physical size in bytes of the image container on the host per
                device</td></tr><tr><td>network.incoming.bytes</td><td>Cumulative</td><td>B</td><td>Interface ID</td><td>Pollster</td><td>Number of incoming bytes</td></tr><tr><td>network.outgoing.bytes</td><td>Cumulative</td><td>B</td><td>Interface ID</td><td>Pollster</td><td>Number of outgoing bytes</td></tr><tr><td>network.incoming.packets</td><td>Cumulative</td><td>packet</td><td>Interface ID</td><td>Pollster</td><td>Number of incoming packets</td></tr><tr><td>network.outgoing.packets</td><td>Cumulative</td><td>packet</td><td>Interface ID</td><td>Pollster</td><td>Number of outgoing packets</td></tr></tbody></table></div></div><div class="sect3" id="computehost-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Host Meters</span> <a title="Permalink" class="permalink" href="#computehost-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-computehost_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-computehost_meters.xml</li><li><span class="ds-label">ID: </span>computehost-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>compute.node.cpu.frequency</td><td>Gauge</td><td>MHz</td><td>Host ID</td><td>Notification</td><td>CPU frequency</td></tr><tr><td>compute.node.cpu.kernel.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU kernel time</td></tr><tr><td>compute.node.cpu.idle.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU idle time</td></tr><tr><td>compute.node.cpu.user.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU user mode time</td></tr><tr><td>compute.node.cpu.iowait.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU I/O wait time</td></tr><tr><td>compute.node.cpu.kernel.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU kernel percentage</td></tr><tr><td>compute.node.cpu.idle.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU idle percentage</td></tr><tr><td>compute.node.cpu.user.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU user mode percentage</td></tr><tr><td>compute.node.cpu.iowait.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU I/O wait percentage</td></tr><tr><td>compute.node.cpu.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU utilization</td></tr></tbody></table></div></div><div class="sect3" id="glance-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image (Glance) Meters</span> <a title="Permalink" class="permalink" href="#glance-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-glance_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-glance_meters.xml</li><li><span class="ds-label">ID: </span>glance-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>image.size</td><td>Gauge</td><td>B</td><td>Image ID</td><td>Notification</td><td>Uploaded image size</td></tr><tr><td>image.update</td><td>Delta</td><td>Image</td><td>Image ID</td><td>Notification</td><td>Number of uploads of the image</td></tr><tr><td>image.upload</td><td>Delta</td><td>Image</td><td>image ID</td><td>notification</td><td>Number of uploads of the image</td></tr><tr><td>image.delete</td><td>Delta</td><td>Image</td><td>Image ID</td><td>Notification</td><td>Number of deletes on the image</td></tr></tbody></table></div></div><div class="sect3" id="cinder-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Volume (Cinder) Meters</span> <a title="Permalink" class="permalink" href="#cinder-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-cinder_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-cinder_meters.xml</li><li><span class="ds-label">ID: </span>cinder-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>volume.size</td><td>Gauge</td><td>GB</td><td>Vol ID</td><td>Notification</td><td>Size of volume</td></tr><tr><td>snapshot.size</td><td>Gauge</td><td>GB</td><td>Snap ID</td><td>Notification</td><td>Size of snapshot's volume</td></tr></tbody></table></div></div><div class="sect3" id="swift-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage (Swift) Meters</span> <a title="Permalink" class="permalink" href="#swift-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-swift_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-swift_meters.xml</li><li><span class="ds-label">ID: </span>swift-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>storage.objects</td><td>Gauge</td><td>Object</td><td>Storage ID</td><td>Pollster</td><td>Number of objects</td></tr><tr><td>storage.objects.size</td><td>Gauge</td><td>B</td><td>Storage ID</td><td>Pollster</td><td>Total size of stored objects</td></tr><tr><td>storage.objects.containers</td><td>Gauge</td><td>Container</td><td>Storage ID</td><td>Pollster</td><td>Number of containers</td></tr></tbody></table></div><p>
  The <code class="literal">resource_id</code> for any Ceilometer query is the
  <code class="literal">tenant_id</code> for the Swift object because Swift usage is
  rolled up at the tenant level.
 </p></div></div><div class="sect2" id="topic-icq-hvc-5t"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering API Reference</span> <a title="Permalink" class="permalink" href="#topic-icq-hvc-5t">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>topic-icq-hvc-5t</li></ul></div></div></div></div><p>
  Ceilometer uses a polling agent to communicate with an API to collect
  information at a regular interval, as shown in the diagram below.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-metering-Ceilo_API_Polling.png" target="_blank"><img src="images/media-metering-Ceilo_API_Polling.png" width="" /></a></div></div><p>
  Ceilometer query APIs can put a significant load on the database leading to
  unexpected results or failures. Therefore it is important to understand how
  the Ceilometer API works and how to change the configuration to protect
  against failures.
 </p><div class="sect3" id="API-changes"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer API Changes</span> <a title="Permalink" class="permalink" href="#API-changes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>API-changes</li></ul></div></div></div></div><p>
   The following changes have been made in the latest release of Ceilometer for
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Ceilometer API</strong></span> supports a default of 100
     queries. This limit is configurable in the ceilometer.conf configuration
     file. The option is in the <code class="literal">DEFAULT</code> section and is named
     <code class="literal">default_api_return_limit</code>.
    </p></li><li class="listitem "><p>
     Flexible configuration for pollster and notifications has been added.
     Ceilometer can now list different event types differently for these
     services.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Query-sample API</strong></span> is now supported in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Meter-list API</strong></span> can now return a unique
     list of meter names with no duplicates. To create this list, when running
     the list command, use the <code class="literal">--unique</code> option.
    </p></li></ul></div><p>
   The following limitations exist in the latest release of Ceilometer for
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Event API</strong></span> is disabled by default and is
     unsupported in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Trait API</strong></span> is disabled by default and is
     unsupported in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Post Sample API</strong></span> is disabled by default
     and is unsupported in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Alarm API</strong></span> is disabled by default and is
     unsupported in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Sample-Show API</strong></span> is unsupported in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Meter-List API</strong></span> does not support filtering
     with metadata.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Query-Sample API</strong></span> (Complex query) does not
     support using the following operators in the same query:
    </p><div class="verbatim-wrap"><pre class="screen">order by argument
NOT</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>Query-Sample API</strong></span> requires you to specify
     a meter name. Complex queries will be analyzed as several simple queries
     according to the AND/OR logic. As meter-list is a constraint, each simple
     query must specify a meter name. If this condition is not met, you will
     receive a detailed 400 error.
    </p></li><li class="listitem "><p>
     Due to a Monasca API limitation, microsecond is no longer supported. In
     the <span class="bold"><strong>Resource-List API</strong></span>,
     <span class="bold"><strong>Sample-List API</strong></span>,
     <span class="bold"><strong>Statistics API</strong></span> and
     <span class="bold"><strong>Query-Samples API</strong></span>, the
     <code class="literal">timestamp</code> field now only supports measuring down to the
     millisecond.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Sample-List API</strong></span> does not support
     <code class="literal">message_id</code> as a valid search parameter. This parameter
     is also not included in the output.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Sample-List API</strong></span> now requires the meter
     name as a positional parameter.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Sample-List API</strong></span> returns a sample with an
     empty <code class="literal">message_signature</code> field.
    </p></li></ul></div></div><div class="sect3" id="disabled"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabled APIs</span> <a title="Permalink" class="permalink" href="#disabled">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>disabled</li></ul></div></div></div></div><p>
   The following Ceilometer metering APIs are disabled in this release:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Event API
    </p></li><li class="listitem "><p>
     Trait API
    </p></li><li class="listitem "><p>
     Ceilometer Alarms API
    </p></li><li class="listitem "><p>
     Post Samples API
    </p></li></ul></div><p>
   These APIs are disabled through a custom rule called
   <code class="literal">hp_disabled_rule:not_implemented</code>. This rule is added to
   each disabled API in Ceilometer's policy.json file
   <span class="bold"><strong>/etc/ceilometer/policy.json</strong></span> on controller
   nodes. Attempts to access any of the disabled APIs will result in an HTTP
   response 501 Not Implemented.
  </p><p>
   To manually enable any of the APIs, remove the corresponding rule and
   restart Apache
  </p><div class="verbatim-wrap"><pre class="screen">{
	"context_is_admin": "role:admin",
	"context_is_project": "project_id:%(target.project_id)s",
	"context_is_owner": "user_id:%(target.user_id)s",
	"segregation": "rule:context_is_admin",

	"telemetry:create_samples": "hp_disabled_rule:not_implemented",

	"telemetry:get_alarm": "hp_disabled_rule:not_implemented",
	"telemetry:change_alarm": "hp_disabled_rule:not_implemented",
	"telemetry:delete_alarm": "hp_disabled_rule:not_implemented",
	"telemetry:alarm_history": "hp_disabled_rule:not_implemented",
	"telemetry:change_alarm_state": "hp_disabled_rule:not_implemented",
	"telemetry:get_alarm_state": "hp_disabled_rule:not_implemented",
	"telemetry:create_alarm": "hp_disabled_rule:not_implemented",
	"telemetry:get_alarms": "hp_disabled_rule:not_implemented",
	"telemetry:query_sample":"hp_disabled_rule:not_implemented",
	"default": ""
}</pre></div><p>
   The following Alarm APIs are disabled
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     POST /v2/alarms
    </p></li><li class="listitem "><p>
     GET /v2/alarms
    </p></li><li class="listitem "><p>
     GET /v2/alarms/(alarm_id)
    </p></li><li class="listitem "><p>
     PUT /v2/alarms/(alarm_id)
    </p></li><li class="listitem "><p>
     DELETE /v2/alarms/(alarm_id)
    </p></li><li class="listitem "><p>
     GET /v2/alarms/(alarm_id)/history
    </p></li><li class="listitem "><p>
     PUT /v2/alarms/(alarm_id)/state
    </p></li><li class="listitem "><p>
     GET /v2/alarms/(alarm_id)/state
    </p></li><li class="listitem "><p>
     POST /v2/query/alarms
    </p></li><li class="listitem "><p>
     POST /v2/query/alarms/history
    </p></li></ul></div><p>
   In addition, these APIs are disabled:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Post Samples API: POST /v2/meters/(meter_name)
    </p></li><li class="listitem "><p>
     Query Sample API: POST /v2/query/samples
    </p></li></ul></div></div><div class="sect3" id="improve-reporting"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improving Reporting API Responsiveness</span> <a title="Permalink" class="permalink" href="#improve-reporting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>improve-reporting</li></ul></div></div></div></div><p>
   Reporting APIs are the main access to the Metering data stored in
   Ceilometer. These APIs are accessed by Horizon to provide basic usage data
   and information. However, Horizon Resources Usage Overview / Stats panel
   shows usage metrics with the following limitations:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No metric option is available until you actually create a resource (such
     as an instance, Swift container, etc).
    </p></li><li class="listitem "><p>
     Only specific meters are displayed for a selection after resources have
     been created. For example, only the Cinder volume and volume.size meters
     are displayed if only a Cinder volume has been created (for example, if no
     compute instance or Swift containers were created yet)
    </p></li><li class="listitem "><p>
     Only the top 20 meters associated with the sample query results are
     displayed.
    </p></li><li class="listitem "><p>
     Period duration selection should be much less than the default retention
     period (currently 7 days), to get statistics for multiple groups.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the Apache2 Web Server to provide API access. It is possible
   to tune performance to optimize the front end as well as the back-end
   database. Experience indicates that an excessive increase of concurrent
   access to the front-end tends to put a strain in the database.
  </p></div><div class="sect3" id="ceilo-ap-remove"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring Apache2, Horizon and Keystone</span> <a title="Permalink" class="permalink" href="#ceilo-ap-remove">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>ceilo-ap-remove</li></ul></div></div></div></div><p>
   The ceilometer-api is now running as part of the Apache2 service together
   with Horizon and Keystone. To remove them from the active list so that
   changes can be made and then re-instate them, use the following commands.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Disable the Ceilometer API on the active sites.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm /etc/apache2/vhosts.d/ceilometer_modwsgi.conf</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl reload apache2.service</pre></div></li><li class="listitem "><p>
     Perform all necessary changes. The Ceilometer API will not be served until
     it is re-enabled.
    </p></li><li class="listitem "><p>
     Re-enable the Ceilometer API on the active sites.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ln -s  /etc/apache2/vhosts.d/ceilometer_modwsgi.vhost /etc/apache2/vhosts.d/ceilometer_modwsgi.conf</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl reload apache2.service</pre></div></li><li class="listitem "><p>
     The new changes need to be picked up by Apache2. If possible, force a
     reload rather than a restart. Unlike a restart, the reload waits for
     currently active sessions to gracefully terminate or complete.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl reload apache2.service</pre></div></li></ol></div></div><div class="sect3" id="data-access"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Data Access API</span> <a title="Permalink" class="permalink" href="#data-access">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>data-access</li></ul></div></div></div></div><p>
   Ceilometer provides a complete API for data access only and not for data
   visualization or aggregation. These functions are provided by external,
   downstream applications that support various use cases like usage billing
   and software license policy adherence.
  </p><p>
   Each application calls the specific Ceilometer API needed for their use
   case. The resulting data is then aggregated and visualized based on the
   unique functions provided by each application.
  </p><p>
   For more information, see the OpenStack Developer documentation for
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/webapi/v2.html" target="_blank">V2
   Web API</a>.
  </p></div><div class="sect3" id="pipeline"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post Samples API</span> <a title="Permalink" class="permalink" href="#pipeline">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>pipeline</li></ul></div></div></div></div><p>
   The Post Sample API is disabled by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> and it
   requires a separate pipeline.yml for Ceilometer. This is because it uses a
   pipeline configuration different than the agents. Also by default, the API
   pipeline has no meters enabled. When the Post Samples API is enabled, you
   need to configure the meters.
  </p><div id="id-1.6.14.5.7.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Use caution when adding meters to the API pipeline. Ensure that only meters
    already present in the notification agent and the polling agent pipeline
    are added to the Post Sample API pipeline.
   </p></div><p>
   The Ceilometer API pipeline configuration file is located in the following
   directory:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/service/ceilometer-api/etc/pipeline-api.yml</pre></div><p>
   Sample API pipeline file:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"
      sinks:
          - meter_sink
    - name: image_source
      interval: 30
      meters:
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
      sinks:
          - meter_sink
    - name: volume_source
      interval: 30
      meters:
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
      sinks:
          - meter_sink
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div></div><div class="sect3" id="resource"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Resource API</span> <a title="Permalink" class="permalink" href="#resource">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>resource</li></ul></div></div></div></div><p>
   The Ceilometer Resource API provides a list of resources associated with
   meters that Ceilometer polls. By default, all meter links are generated for
   each resource.
  </p><div id="id-1.6.14.5.7.11.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Be aware that this functionality has a high cost. For a large deployment,
    in order to reduce the response time, it is recommended that you do not
    return meter links. You can disable links in the output using the following
    filter in your query: (for the REST API only)
   </p><div class="verbatim-wrap"><pre class="screen">meter_links=0</pre></div></div><p>
   The <code class="literal">resource-list</code> (/v2/resources) API can be filtered by
   the following parameters:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     project_id
    </p></li><li class="listitem "><p>
     user_id
    </p></li><li class="listitem "><p>
     source
    </p></li><li class="listitem "><p>
     resource_id
    </p></li><li class="listitem "><p>
     timestamp
    </p></li><li class="listitem "><p>
     metadata
    </p></li></ul></div><div id="id-1.6.14.5.7.11.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    It is highly recommended that you use one or both of the following query
    filters to get a quick response in a scaled deployment:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      project_id
     </p></li><li class="listitem "><p>
      timestamp
     </p></li></ul></div></div><p>
   <span class="bold"><strong>Example Query:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ceilometer resource-list -q "project_id=7aa0fe3f02ff4e11a70a41e97d0db5e3;timestamp&gt;=2015-10-22T15:44:00;timestamp&lt;=2015-10-23T15:44:00"</pre></div></div><div class="sect3" id="sampleAPI"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sample API</span> <a title="Permalink" class="permalink" href="#sampleAPI">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>sampleAPI</li></ul></div></div></div></div><p>
   Ceilometer Sample has two APIs:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     ceilometer sample-list(/v2/samples)
    </p></li><li class="listitem "><p>
     ceilometer query-sample (/v2/query/samples)
    </p></li></ul></div><p>
   Sample-list API allows querying based on the following values:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     meter name
    </p></li><li class="listitem "><p>
     user_id
    </p></li><li class="listitem "><p>
     project_id
    </p></li><li class="listitem "><p>
     sample source
    </p></li><li class="listitem "><p>
     resource_id
    </p></li><li class="listitem "><p>
     sample timestamp (range)
    </p></li><li class="listitem "><p>
     sample message_id
    </p></li><li class="listitem "><p>
     resource metadata attributes
    </p></li></ul></div><p>
   Sample-list API uses the AND operator implicitly. However, the query-sample
   API allows for finer control over the filter expression. This is because
   query-sample API allows the use of AND, OR, and NOT operators over any of
   the sample, meter or resource attributes.
  </p><p>
   <span class="bold"><strong>Limitations:</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Ceilometer query-sample API does not support the JOIN operator for
     stability of the system. This is due to the fact that query-sample API
     uses an anonymous/alias table to cache the JOIN query results and
     concurrent requests to this API. This can use up the disk space quickly
     and cause service interruptions.
    </p></li><li class="listitem "><p>
     Ceilometer sample-list API uses the AND operator implicitly for all
     queries. However, sample-list API does allow you to query on resource
     metadata field of samples.
    </p></li></ul></div><p>
   <span class="bold"><strong>Sample queries from the command line:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ceilometer sample-list -m METER_NAME -q '&lt;field1&gt;&lt;operator1&gt;&lt;value1&gt;;...;&lt;field_n&gt;&lt;operator_n&gt;&lt;value_n&gt;'</pre></div><p>
   where operators can be:<span class="bold"><strong> &lt;, &lt;=, =, !=, &gt;=
   &gt;</strong></span>
  </p><div id="id-1.6.14.5.7.12.12" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    All the key value pairs will be combined with the implicit AND operator.
   </p></div><p>
   <span class="bold"><strong>Example usage for the sample-list API</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ceilometer sample-list --meter image.serve -q 'resource_id=a1ec2585'</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ceilometer sample-list --meter instance -q 'resource_id=&lt;ResourceID&gt;;metadata.event_type=&lt;eventType&gt;'</pre></div></div><div class="sect3" id="statAPI"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Statistics API</span> <a title="Permalink" class="permalink" href="#statAPI">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_apis.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_apis.xml</li><li><span class="ds-label">ID: </span>statAPI</li></ul></div></div></div></div><p>
   Ceilometer Statistics is an open-ended query API that performs queries on
   the table of data collected from a meter. The Statistics API obtains the
   minimum and maximum timestamp for the meter that is being queried.
  </p><p>
   The Statistics API also provides a set of statistical functions. These
   functions perform basic aggregation for meter-specific data over a period of
   time. Statistics API includes the following functions:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.5.7.13.4.1"><span class="term ">Count</span></dt><dd><p>
      the number of discrete samples collected in each period
     </p></dd><dt id="id-1.6.14.5.7.13.4.2"><span class="term ">Maximum</span></dt><dd><p>
      the sample with the maximum value in a selected time period
     </p></dd><dt id="id-1.6.14.5.7.13.4.3"><span class="term ">Minimum</span></dt><dd><p>
      the sample with the minimum value in a selected time period
     </p></dd><dt id="id-1.6.14.5.7.13.4.4"><span class="term ">Average</span></dt><dd><p>
      the average value of a samples within a selected time period
     </p></dd><dt id="id-1.6.14.5.7.13.4.5"><span class="term ">Sum</span></dt><dd><p>
      the total value of all samples within a selected time period added
      together
     </p></dd></dl></div><div id="id-1.6.14.5.7.13.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    The Statistics API can put a significant load on the database leading to
    unexpected results and or failures. Therefore, you should be careful about
    restricting your queries.
   </p></div><p>
   <span class="bold"><strong>Limitations of
   <span class="bold"><strong>Statistics-list</strong></span> API</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     filtering with metadata is not supported
    </p></li><li class="listitem "><p>
     the <code class="literal">groupby</code> option is only supported with only one
     parameter. That single parameter has to be one of the following:
    </p><div class="verbatim-wrap"><pre class="screen">user_id
project_id
resource_id
source</pre></div></li><li class="listitem "><p>
     only the following are supported as aggregate functions:
     <span class="bold"><strong>average</strong></span>,
     <span class="bold"><strong>minimum</strong></span>,
     <span class="bold"><strong>maximum</strong></span>,
     <span class="bold"><strong>sum</strong></span>, and
     <span class="bold"><strong>count</strong></span>
    </p></li><li class="listitem "><p>
     when no time period is specified in the query, a default period of 300
     seconds is used to aggregate measurements (samples)
    </p></li><li class="listitem "><p>
     the <span class="bold"><strong>meter name</strong></span> is a required positional
     parameter
    </p></li><li class="listitem "><p>
     when a closed time range is specified, results may contain an extra row
     with <span class="bold"><strong>duration</strong></span>,
     <span class="bold"><strong>duration start</strong></span>,
     <span class="bold"><strong>duration end</strong></span> assigned with a value of
     <span class="bold"><strong>None</strong></span>. This row has a start and end time
     period that fall outside the requested time range and can be ignored.
     Ceilometer does not remove this row because it is by design inside the
     back-end Monasca.
    </p></li></ul></div><p>
   <span class="bold"><strong>Statistical Query Best Practices</strong></span>
  </p><p>
   By default, the Statistics API will return a limited number of statistics.
   You can control the output using the <span class="bold"><strong>period "."
   </strong></span> parameter.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.5.7.13.10.1"><span class="term ">Without a period parameter</span></dt><dd><p>
      only a few statistics: <span class="bold"><strong>minimum</strong></span>,
      <span class="bold"><strong>maximum</strong></span>,
      <span class="bold"><strong>avgerage</strong></span> and
      <span class="bold"><strong>sum</strong></span>
     </p></dd><dt id="id-1.6.14.5.7.13.10.2"><span class="term ">With a period parameter "."</span></dt><dd><p>
      the range is divided into equal periods and Statistics API finds the
      <span class="bold"><strong>count</strong></span>,
      <span class="bold"><strong>minimum</strong></span>,
      <span class="bold"><strong>maximum</strong></span>,
      <span class="bold"><strong>average</strong></span>, and
      <span class="bold"><strong>sum</strong></span> for each of the periods
     </p></dd></dl></div><div id="id-1.6.14.5.7.13.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    It is recommended that you provide a <code class="literal">timestamp</code> parameter
    with every query, regardless of whether a period paramter is used. For
    example:
   </p><div class="verbatim-wrap"><pre class="screen">timestamp&gt;={$start-timestamp} and timestamp&lt;{$end-timestamp}</pre></div><p>
    It is also recommended that you query a period of time that covers at most
    1 day (24 hours).
   </p></div><p>
   <span class="bold"><strong>Examples</strong></span>
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.5.7.13.13.1"><span class="term ">Without period parameter</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ceilometer statistics -q "timestamp&gt;=2014-12-11T00:00:10;timestamp&lt;2014-12-11T23:00:00" -m "instance"</pre></div></dd><dt id="id-1.6.14.5.7.13.13.2"><span class="term ">With the period parameter "."</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ceilometer statistics -q "timestamp&gt;=2014-12-11T00:00:10;timestamp&lt;2014-12-11T23:00:00" -m "instance" -p 3600</pre></div></dd></dl></div><p>
   If the <span class="bold"><strong>query</strong></span> and
   <span class="bold"><strong>timestamp</strong></span> parameters are not provided, all
   records in the database will be queried. This is not recommended. Use the
   following recommended values for <span class="bold"><strong>query (-q)</strong></span>
   parameter and <span class="bold"><strong>period (-p)</strong></span> parameters:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.5.7.13.15.1"><span class="term ">-q</span></dt><dd><p>
      Always provide a <span class="bold"><strong>timestamp</strong></span> range, with
      the following guidelines:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        recommended maximum time period to query is one day (24 hours)
       </p></li><li class="listitem "><p>
        do not set the timestamp range to greater than a day
       </p></li><li class="listitem "><p>
        it is better to provide no time stamp range than to set the time period
        for more than 1 day
       </p></li><li class="listitem "><p>
        example of an acceptable range:
       </p><div class="verbatim-wrap"><pre class="screen">-q "timestamp&gt;=2014-12-11T00:00:10;timestamp&lt;2014-12-11T23:00:00"</pre></div></li></ul></div></dd><dt id="id-1.6.14.5.7.13.15.2"><span class="term ">-p</span></dt><dd><p>
      Provide a large number in seconds, with the following guidelines:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        recommended minimum value is 3600 or more (1 hour or more)
       </p></li><li class="listitem "><p>
        providing a period of less than 3600 is not recommended
       </p></li><li class="listitem "><p>
        Use this parameter to divide the overall time range into smaller
        intervals. A small period parameter value will translate into a very
        large number of queries against the database.
       </p></li><li class="listitem "><p>
        Example of an acceptable range:
       </p><div class="verbatim-wrap"><pre class="screen">-p 3600</pre></div></li></ul></div></dd></dl></div></div></div><div class="sect2" id="reconfig-metering"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the Ceilometer Metering Service</span> <a title="Permalink" class="permalink" href="#reconfig-metering">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>reconfig-metering</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> automatically deploys Ceilometer to use the Monasca database.
  Ceilometer is deployed on the same control plane nodes along with other
  OpenStack services such as Keystone, Nova, Neutron, Glance, and Swift.
 </p><p>
  The Metering Service can be configured using one of the procedures described
  below.
 </p><div class="sect3" id="idg-all-metering-metering-reconfig-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Run the Upgrade Playbook</span> <a title="Permalink" class="permalink" href="#idg-all-metering-metering-reconfig-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-reconfig-xml-7</li></ul></div></div></div></div><p>
   Follow Standard Service upgrade mechanism available in the Cloud Lifecycle Manager
   distribution. For Ceilometer, the playbook included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is
   <span class="bold"><strong>ceilometer-upgrade.yml</strong></span>
  </p></div><div class="sect3" id="apache"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Apache2 for the Ceilometer API</span> <a title="Permalink" class="permalink" href="#apache">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>apache</li></ul></div></div></div></div><p>
   Reporting APIs provide access to the metering data stored in Ceilometer.
   These APIs are accessed by Horizon to provide basic usage data and
   information.<span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses Apache2 Web Server to provide the API access.
  </p><div id="id-1.6.14.5.8.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    To improve API responsiveness you can increase the number of threads and
    processes in the Ceilometer configuration file. The Ceilometer API runs as
    an WSGI processes. Each process can have a certain amount of threads
    managing the filters and applications, which can comprise the processing
    pipeline.
   </p></div><p>
   To configure Apache:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit the Ceilometer configuration files.
    </p></li><li class="listitem "><p>
     Reload and verify Apache2.
    </p></li></ol></div><p>
   <span class="bold"><strong>Edit the Ceilometer Configuration Files</strong></span>
  </p><p>
   To create a working file for Ceilometer with the correct settings:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To add the configuration file to the correct folder, copy the following
     file:
    </p><div class="verbatim-wrap"><pre class="screen">ceilometer.conf</pre></div><p>
     to the following directory:
    </p><div class="verbatim-wrap"><pre class="screen">/etc/apache2/vhosts.d/</pre></div></li><li class="listitem "><p>
     To verify the settings, in a text editor, open the
     <code class="literal">ceilometer_modwsgi.vhost</code> file.
    </p></li><li class="listitem "><p>
     The ceilometer_modwsgi.conf file should have the following data. If it does not
     exist, add it to the file.
    </p><div class="verbatim-wrap"><pre class="screen">Listen &lt;ipaddress&gt;:8777
&lt;VirtualHost *:8777&gt;
  WSGIScriptAlias / /srv/www/ceilometer/ceilometer-api
  WSGIDaemonProcess ceilometer user=ceilometer group=ceilometer processes=4 threads=5 socket-timeout=600 python-path=/opt/stack/service/ceilometer-api/venv:/opt/stack/service/ceilometer-api/venv/lib/python2.7/site-packages/ display-name=ceilometer-api
  WSGIApplicationGroup %{GLOBAL}
  WSGIProcessGroup ceilometer

  ErrorLog /var/log/ceilometer/ceilometer_modwsgi.log
  LogLevel INFO
  CustomLog /var/log/ceilometer/ceilometer_access.log combined

  &lt;Directory /opt/stack/service/ceilometer-api/venv/lib/python2.7/site-packages/ceilometer&gt;
    Options Indexes FollowSymLinks MultiViews
    Require all granted
    AllowOverride None
    Order allow,deny
    allow from all
    LimitRequestBody 102400
  &lt;/Directory&gt;
&lt;/VirtualHost&gt;</pre></div><div id="id-1.6.14.5.8.5.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The WSGIDaemon Recommended Settings are to use four processes running in
      parallel:
     </p><div class="verbatim-wrap"><pre class="screen">processes=4</pre></div><p>
      Five threads for each process is also recommended:
     </p><div class="verbatim-wrap"><pre class="screen">threads=5</pre></div></div></li><li class="listitem "><p>
     To add a softlink for the ceilometer.conf, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ln -s /etc/apache2/vhosts.d/ceilometer_modwsgi.vhost /etc/apache2/vhosts.d/ceilometer_modwsgi.conf</pre></div></li></ol></div><p>
   <span class="bold"><strong>Reload and Verify Apache2</strong></span>
  </p><p>
   For the changes to take effect, the Apache2 service needs to be reloaded.
   This ensures that all the configuration changes are saved and the service
   has applied them. The system administrator can change the configuration of
   processes and threads and experiment if alternative settings are necessary.
  </p><p>
   Once the Apache2 service has been reloaded you can verify that the
   Ceilometer APIs are running and able to receive incoming traffic. The
   Ceilometer APIs are listening on port 8777.
  </p><p>
   To reload and verify the Apache2 service:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To reload Apache2, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl reload apache2.service</pre></div></li><li class="listitem "><p>
     To verify the service is running, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl status apache2.service</pre></div><div id="id-1.6.14.5.8.5.13.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      In a working environment, the list of entries in the output should match
      the number of processes in the configuration file. In the example
      configuration file, the recommended number of 4 is used, and the number
      of Running Instances is also 4.
     </p></div></li></ol></div><p>
   You can also verify that Apache2 is accepting incoming traffic using the
   following procedure:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To verify traffic on port 8777, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo netstat -tulpn | grep 8777</pre></div></li><li class="listitem "><p>
     Verify your output is similar to the following example:
    </p><div class="verbatim-wrap"><pre class="screen">tcp6 0 0 :::8777 :::* LISTEN 8959/apache2</pre></div></li></ol></div><div id="id-1.6.14.5.8.5.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If Ceilometer fails to deploy:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      check the proxy setting
     </p></li><li class="listitem "><p>
      unset the https_proxy, for example:
     </p><div class="verbatim-wrap"><pre class="screen">unset http_proxy HTTP_PROXY HTTPS_PROXY</pre></div></li></ul></div></div></div><div class="sect3" id="metering-services"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Services for Messaging Notifications</span> <a title="Permalink" class="permalink" href="#metering-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>metering-services</li></ul></div></div></div></div><p>
   After installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the following services are enabled
   by default to send notifications:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Nova
    </p></li><li class="listitem "><p>
     Cinder
    </p></li><li class="listitem "><p>
     Glance
    </p></li><li class="listitem "><p>
     Neutron
    </p></li><li class="listitem "><p>
     Swift
    </p></li></ul></div><p>
   The list of meters for these services are specified in the Notification
   Agent or Polling Agent's pipeline configuration file.
  </p><p>
   For steps on how to edit the pipeline configuration files, see:
   <a class="xref" href="#notifications" title="12.3.6. Ceilometer Metering Service Notifications">Section 12.3.6, “Ceilometer Metering Service Notifications”</a>
  </p></div><div class="sect3" id="Ceilo-StopStart"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Polling Agent</span> <a title="Permalink" class="permalink" href="#Ceilo-StopStart">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>Ceilo-StopStart</li></ul></div></div></div></div><p>
   The Polling Agent is responsible for coordinating the polling activity. It
   parses the <span class="bold"><strong>pipeline.yml</strong></span> configuration file
   and identifies all the sources where data is collected. The sources are then
   evaluated and are translated to resources that a dedicated pollster can
   retrieve. The Polling Agent follows this process:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     At each identified interval, the
     <span class="bold"><strong>pipeline.yml</strong></span> configuration file is
     parsed.
    </p></li><li class="listitem "><p>
     The resource list is composed.
    </p></li><li class="listitem "><p>
     The pollster collects the data.
    </p></li><li class="listitem "><p>
     The pollster sends data to the queue.
    </p></li></ol></div><p>
   Metering processes should normally be operating at all times. This need is
   addressed by the Upstart event engine which is designed to run on any Linux
   system. Upstart creates events, handles the consequences of those events,
   and starts and stops processes as required. Upstart will continually attempt
   to restart stopped processes even if the process was stopped manually. To
   stop or start the Polling Agent and avoid the conflict with Upstart, using
   the following steps.
  </p><p>
   <span class="bold"><strong>To restart the Polling Agent:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To determine whether the process is running, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl status ceilometer-agent-notification
#SAMPLE OUTPUT:
ceilometer-agent-notification.service - ceilometer-agent-notification Service
   Loaded: loaded (/etc/systemd/system/ceilometer-agent-notification.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-06-12 05:07:14 UTC; 2 days ago
 Main PID: 31529 (ceilometer-agen)
    Tasks: 69
   CGroup: /system.slice/ceilometer-agent-notification.service
           ├─31529 ceilometer-agent-notification: master process [/opt/stack/service/ceilometer-agent-notification/venv/bin/ceilometer-agent-notification --config-file /opt/stack/service/ceilometer-agent-noti...
           └─31621 ceilometer-agent-notification: NotificationService worker(0)

Jun 12 05:07:14 ardana-qe201-cp1-c1-m2-mgmt systemd[1]: Started ceilometer-agent-notification Service.</pre></div></li><li class="step "><p>
     To stop the process, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop ceilometer-agent-notification</pre></div></li><li class="step "><p>
     To start the process, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl start ceilometer-agent-notification</pre></div></li></ol></div></div></div><div class="sect3" id="ceilo-replace-cntrler"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replace a Logging, Monitoring, and Metering Controller</span> <a title="Permalink" class="permalink" href="#ceilo-replace-cntrler">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>ceilo-replace-cntrler</li></ul></div></div></div></div><p>
   In a medium-scale environment, if a metering controller has to be replaced
   or rebuilt, use the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     <a class="xref" href="#replacing-controller" title="13.1.2.1. Replacing a Controller Node">Section 13.1.2.1, “Replacing a Controller Node”</a>.
    </p></li><li class="step "><p>
     If the Ceilometer nodes are not on the shared control plane, to implement
     the changes and replace the controller, you must reconfigure Ceilometer.
     To do this, run the ceilometer-reconfigure.yml ansible playbook
     <span class="bold"><strong>without</strong></span> the limit option
    </p></li></ol></div></div></div><div class="sect3" id="ceilo-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Monitoring</span> <a title="Permalink" class="permalink" href="#ceilo-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>ceilo-monitoring</li></ul></div></div></div></div><p>
   The Monasca HTTP Process monitors the Ceilometer API service. Ceilometer's
   notification and polling agents are also monitored. If these agents are
   down, Monasca monitoring alarms are triggered. You can use the notification
   alarms to debug the issue and restart the notifications agent. However, for
   Central-Agent (polling) and Collector the alarms need to be deleted. These
   two processes are not started after an upgrade so when the monitoring
   process checks the alarms for these components, they will be in UNDETERMINED
   state. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not monitor these processes anymore so the best option to
   resolve this issue is to manually delete alarms that are no longer used but
   are installed.
  </p><p>
   To resolve notification alarms, first check the
   <span class="bold"><strong>ceilometer-agent-notification</strong></span> logs for
   errors in the <span class="bold"><strong>/var/log/ceilometer</strong></span>
   directory. You can also use the Operations Console to access Kibana and
   check the logs. This will help you understand and debug the error.
  </p><p>
   To restart the service, run the
   <span class="bold"><strong>ceilometer-start.yml</strong></span>. This playbook starts
   the ceilometer processes that has stopped and only restarts during install,
   upgrade or reconfigure which is what is needed in this case. Restarting the
   process that has stopped will resolve this alarm because this Monasca alarm
   means that ceilometer-agent-notification is no longer running on certain
   nodes.
  </p><p>
   You can access Ceilometer data through Monasca. Ceilometer publishes samples
   to Monasca with credentials of the following accounts:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>ceilometer</strong></span> user
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>services</strong></span>
    </p></li></ul></div><p>
   Data collected by Ceilometer can also be retrieved by the Monasca REST API.
   Make sure you use the following guidelines when requesting data from the
   Monasca REST API:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Verify you have the monasca-admin role. This role is configured in the
     monasca-api configuration file.
    </p></li><li class="listitem "><p>
     Specify the <code class="literal">tenant id</code> of the
     <span class="bold"><strong>services</strong></span> project.
    </p></li></ul></div><p>
   For more details, read the
   <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">Monasca
   API Specification</a>.
  </p><p>
   To run Monasca commands at the command line, you must be have the
   <span class="bold"><strong>admin</strong></span> role. This allows you to use the
   Ceilometer account credentials to replace the default admin account
   credentials defined in the <span class="bold"><strong>service.osrc</strong></span>
   file. When you use the Ceilometer account credentials, Monasca commands will
   only return data collected by Ceilometer. At this time, Monasca command line
   interface (CLI) does not support the data retrieval of other tenants or
   projects.
  </p></div></div><div class="sect2" id="notifications"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Service Notifications</span> <a title="Permalink" class="permalink" href="#notifications">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>notifications</li></ul></div></div></div></div><p>
  Ceilometer uses the notification agent to listen to the message queue,
  convert notifications to Events and Samples, and apply pipeline actions.
 </p><div class="sect3" id="whitelist"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manage Whitelisting and Polling</span> <a title="Permalink" class="permalink" href="#whitelist">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>whitelist</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is designed to reduce the amount of data that is stored.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>'s use of a SQL-based cluster, which is not recommended for big data,
   means you must control the data that Ceilometer collects. You can do this by
   filtering (whitelisting) the data or by using the configuration files for
   the Ceilometer Polling Agent and the Ceilometer Notificfoation Agent.
  </p><p>
   Whitelisting is used in a rule specification as a positive filtering
   parameter. Whitelist is only included in rules that can be used in direct
   mappings, for identity service issues such as service discovery,
   provisioning users, groups, roles, projects, domains as well as user
   authentication and authorization.
  </p><p>
   You can run tests against specific scenarios to see if filtering reduces the
   amount of data stored. You can create a test by editing or creating a run
   filter file (whitelist). For steps on how to do this, see:
   <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 26 “Cloud Verification”, Section 26.1 “API Verification”</span>.
  </p><p>
   Ceilometer Polling Agent (polling agent) and Ceilometer Notification Agent
   (notification agent) use different pipeline.yaml files to configure meters
   that are collected. This prevents accidentally polling for meters which can
   be retrieved by the polling agent as well as the notification agent. For
   example, glance image and image.size are meters which can be retrieved both
   by polling and notifications.
  </p><p>
   In both of the separate configuration files, there is a setting for
   <code class="literal">interval</code>. The interval attribute determines the
   frequency, in seconds, of how often data is collected. You can use this
   setting to control the amount of resources that are used for notifications
   and for polling. For example, you want to use more resources for
   notifications and less for polling. To accomplish this you would set the
   <code class="literal">interval</code> in the polling configuration file to a large
   amount of time, such as 604800 seconds, which polls only once a week. Then
   in the notifications configuration file, you can set the
   <code class="literal">interval</code> to a higher amount, such as collecting data
   every 30 seconds.
  </p><div id="id-1.6.14.5.9.3.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Swift account data will be collected using the polling mechanism in an
    hourly interval.
   </p></div><p>
   Setting this interval to manage both notifications and polling is the
   recommended procedure when using a SQL cluster back-end.
  </p><p>
   <span class="bold"><strong>Sample Ceilometer Polling Agent file:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">#File: ~/opt/stack/service/ceilometer-polling/etc/pipeline-polling.yaml
---
sources:
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
   <span class="bold"><strong>Sample Ceilometer Notification Agent(notification
   agent) file:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">#File:    ~/opt/stack/service/ceilometer-agent-notification/etc/pipeline-agent-notification.yaml
---
sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"
resources:
discovery:
sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
   Both of the pipeline files have two major sections:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.14.5.9.3.14.1"><span class="term ">Sources</span></dt><dd><p>
      represents the data that is collected either from notifications posted by
      services or through polling. In the Sources section there is a list of
      meters. These meters define what kind of data is collected. For a full
      list refer to the Ceilometer documentation available at:
      <a class="link" href="http://docs.openstack.org/admin-guide/telemetry-measurements.html" target="_blank">Telemetry
      Measurements</a>
     </p></dd><dt id="id-1.6.14.5.9.3.14.2"><span class="term ">Sinks</span></dt><dd><p>
      represents how the data is modified before it is published to the
      internal queue for collection and storage.
     </p></dd></dl></div><p>
   You will only need to change a setting in the Sources section to control the
   data collection interval.
  </p><p>
   For more information, see
   <a class="link" href="http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html" target="_blank">Telemetry
   Measurements</a>
  </p><p>
   <span class="bold"><strong>To change the Ceilometer Polling Agent interval
   setting:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To find the polling agent configuration file, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/opt/stack/service/ceilometer-polling/etc</pre></div></li><li class="step "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">pipeline-polling.yaml</pre></div></li><li class="step "><p>
     In the following section, change the value of <code class="literal">interval</code>
     to the desired amount of time:
    </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
     In the sample code above, the polling agent will collect data every 600
     seconds, or 10 minutes.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>To change the Ceilometer Notification Agent
   (notification agent) interval setting:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To find the notification agent configuration file, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd /opt/stack/service/ceilometer-agent-notification</pre></div></li><li class="step "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">pipeline-agent-notification.yaml</pre></div></li><li class="step "><p>
     In the following section, change the value of <code class="literal">interval</code>
     to the desired amount of time:
    </p><div class="verbatim-wrap"><pre class="screen">sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"</pre></div><p>
     In the sample code above, the notification agent will collect data every
     30 seconds.
    </p></li></ol></div></div><div id="id-1.6.14.5.9.3.21" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    The <code class="literal">pipeline-agent-notification.yaml</code> file needs to be changed on all
    controller nodes to change the white-listing and polling strategy.
   </p></div></div><div class="sect3" id="idg-all-metering-metering-notifications-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Edit the List of Meters</span> <a title="Permalink" class="permalink" href="#idg-all-metering-metering-notifications-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-notifications-xml-7</li></ul></div></div></div></div><p>
   The number of enabled meters can be reduced or increased by editing the
   pipeline configuration of the notification and polling agents. To deploy
   these changes you must then restart the agent. If pollsters and
   notifications are both modified, then you will have to restart both the
   Polling Agent and the Notification Agent. Ceilometer Collector will also
   need to be restarted. The following code is an example of a compute-only
   Ceilometer Notification Agent (notification agent)
   <span class="bold"><strong>pipeline-agent-notification.yaml </strong></span>file:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><div id="id-1.6.14.5.9.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If you enable meters at the container level in this file, every time the
    polling interval triggers a collection, at least 5 messages per existing
    container in Swift are collected.
   </p></div><p>
   The following table illustrates the amount of data produced hourly in
   different scenarios:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td>Swift Containers</td><td>Swift Objects per container</td><td>Samples per Hour</td><td>Samples stored per 24 hours</td></tr><tr><td>10</td><td>10</td><td>500</td><td>12000</td></tr><tr><td>10</td><td>100</td><td>5000</td><td>120000</td></tr><tr><td>100</td><td>100</td><td>50000</td><td>1200000</td></tr><tr><td>100</td><td>1000</td><td>500000</td><td>12000000</td></tr></tbody></table></div><p>
   The data in the table shows that even a very small Swift storage with 10
   containers and 100 files will store 120,000 samples in 24 hours, generating
   a total of 3.6 million samples.
  </p><div id="id-1.6.14.5.9.4.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    The size of each file does not have any impact on the number of samples
    collected. As shown in the table above, the smallest number of samples
    results from polling when there are a small number of files and a small
    number of containers. When there are a lot of small files and containers,
    the number of samples is the highest.
   </p></div></div><div class="sect3" id="meters-add"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add Resource Fields to Meters</span> <a title="Permalink" class="permalink" href="#meters-add">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>meters-add</li></ul></div></div></div></div><p>
   By default, not all the resource metadata fields for an event are recorded
   and stored in Ceilometer. If you want to collect metadata fields for a
   consumer application, for example, it is easier to add a field to an
   existing meter rather than creating a new meter. If you create a new meter,
   you must also reconfigure Ceilometer.
  </p><div id="id-1.6.14.5.9.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    Consider the following information before you add or edit a meter:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      You can add a maximum of 12 new fields.
     </p></li><li class="listitem "><p>
      Adding or editing a meter causes all non-default meters to STOP receiving
      notifications. You will need to restart Ceilometer.
     </p></li><li class="listitem "><p>
      New meters added to the <code class="literal">pipeline-polling.yaml.j2</code> file
      must also be added to the
      <code class="literal">pipeline-agent-notification.yaml.j2</code> file. This is due
      to the fact that polling meters are drained by the notification agent and
      not by the collector.
     </p></li><li class="listitem "><p>
      After <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is installed, services like compute, cinder, glance, and
      neutron are configured to publish Ceilometer meters by default. Other
      meters can also be enabled after the services are configured to start
      publishing the meter. The only requirement for publishing a meter is that
      the <code class="literal">origin</code> must have a value of
      <code class="literal">notification</code>. For a complete list of meters, see the
      OpenStack documentation on
      <a class="link" href="http://docs.openstack.org/admin-guide/telemetry-measurements.html" target="_blank">Measurements</a>.
     </p></li><li class="listitem "><p>
      Not all meters are supported. Meters collected by Ceilometer Compute
      Agent or any agent other than Ceilometer Polling are not supported or
      tested with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li><li class="listitem "><p>
      Identity meters are disabled by Keystone.
     </p></li><li class="listitem "><p>
      To enable Ceilometer to start collecting meters, some services require
      you enable the meters you need in the service first before enabling them
      in Ceilometer. Refer to the documentation for the specific service before
      you add new meters or resource fields.
     </p></li></ul></div></div><p>
   <span class="bold"><strong>To add Resource Metadata fields:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log on to the Cloud Lifecycle Manager (deployer node).
    </p></li><li class="step "><p>
     To change to the Ceilometer directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/ceilometer</pre></div></li><li class="step "><p>
     In a text editor, open the target configuration file (for example,
     monasca-field-definitions.yaml.j2).
    </p></li><li class="step "><p>
     In the metadata section, either add a new meter or edit an existing one
     provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="step "><p>
     Include the metadata fields you need. You can use the <code class="literal">instance
     meter</code> in the file as an example.
    </p></li><li class="step "><p>
     Save and close the configuration file.
    </p></li><li class="step "><p>
     To save your changes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config"</pre></div></li><li class="step "><p>
     If you added a new meter, reconfigure Ceilometer:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
# To run the config-processor playbook:
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
#To run the ready-deployment playbook:
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="update-pollSwift"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the Polling Strategy and Swift Considerations</span> <a title="Permalink" class="permalink" href="#update-pollSwift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>update-pollSwift</li></ul></div></div></div></div><p>
   Polling can be very taxing on the system due to the sheer volume of data
   thtyat the system may have to process. It also has a severe impact on
   queries since the database will now have a very large amount of data to scan
   to respond to the query. This consumes a great amount of cpu and memory.
   This can result in long wait times for query responses, and in extreme cases
   can result in timeouts.
  </p><p>
   There are 3 polling meters in Swift:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     storage.objects
    </p></li><li class="listitem "><p>
     storage.objects.size
    </p></li><li class="listitem "><p>
     storage.objects.containers
    </p></li></ul></div><p>
   Here is an example of <code class="filename">pipeline.yml</code> in which
   Swift polling is set to occur hourly.
  </p><div class="verbatim-wrap"><pre class="screen">---
      sources:
      - name: swift_source
      interval: 3600
      meters:
      - "storage.objects"
      - "storage.objects.size"
      - "storage.objects.containers"
      resources:
      discovery:
      sinks:
      - meter_sink
      sinks:
      - name: meter_sink
      transformers:
      publishers:
      - notifier://</pre></div><p>
   With this configuration above, we did not enable polling of container based
   meters and we only collect 3 messages for any given tenant, one for each
   meter listed in the configuration files. Since we have 3 messages only per
   tenant, it does not create a heavy load on the MySQL database as it would
   have if container-based meters were enabled. Hence, other APIs are not
   hit because of this data collection configuration.
  </p></div></div><div class="sect2" id="topic15050"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Setting Role-based Access Control</span> <a title="Permalink" class="permalink" href="#topic15050">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>topic15050</li></ul></div></div></div></div><p>
  Role Base Access Control (RBAC) is a technique that limits access to
  resources based on a specific set of roles associated with each user's
  credentials.
 </p><p>
  Keystone has a set of users that are associated with each project. Each user
  has one or more roles. After a user has authenticated with Keystone using a
  valid set of credentials, Keystone will augment that request with the Roles
  that are associated with that user. These roles are added to the Request
  Header under the X-Roles attribute and are presented as a comma-separated
  list.
 </p><div class="sect3" id="display-users"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Displaying All Users</span> <a title="Permalink" class="permalink" href="#display-users">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>display-users</li></ul></div></div></div></div><p>
   To discover the list of users available in the system, an administrator can
   run the following command using the Keystone command-line interface:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack user list</pre></div><p>
   The output should resemble this response, which is a list of all the users
   currently available in this system.
  </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-----------------------------------------+----+
|                id                |    name      | enabled |       email        |
+----------------------------------+-----------------------------------------+----+
| 1c20d327c92a4ea8bb513894ce26f1f1 |   admin      |   True  | admin.example.com  |
| 0f48f3cc093c44b4ad969898713a0d65 | ceilometer   |   True  | nobody@example.com |
| 85ba98d27b1c4c8f97993e34fcd14f48 |   cinder     |   True  | nobody@example.com |
| d2ff982a0b6547d0921b94957db714d6 |    demo      |   True  |  demo@example.com  |
| b2d597e83664489ebd1d3c4742a04b7c |    ec2       |   True  | nobody@example.com |
| 2bd85070ceec4b608d9f1b06c6be22cb |   glance     |   True  | nobody@example.com |
| 0e9e2daebbd3464097557b87af4afa4c |    heat      |   True  | nobody@example.com |
| 0b466ddc2c0f478aa139d2a0be314467 |  neutron     |   True  | nobody@example.com |
| 5cda1a541dee4555aab88f36e5759268 |    nova      |   True  | nobody@example.com ||
| 5cda1a541dee4555aab88f36e5759268 |    nova      |   True  | nobody@example.com |
| 1cefd1361be8437d9684eb2add8bdbfa |   swift      |   True  | nobody@example.com |
| f05bac3532c44414a26c0086797dab23 | user20141203213957|True| nobody@example.com |
| 3db0588e140d4f88b0d4cc8b5ca86a0b | user20141205232231|True| nobody@example.com |
+----------------------------------+-----------------------------------------+----+</pre></div></div><div class="sect3" id="display-roles"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Displaying All Roles</span> <a title="Permalink" class="permalink" href="#display-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>display-roles</li></ul></div></div></div></div><p>
   To see all the roles that are currently available in the deployment, an
   administrator (someone with the admin role) can run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack role list</pre></div><p>
   The output should resemble the following response:
  </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-------------------------------------+
|                id                |                 name                |
+----------------------------------+-------------------------------------+
| 507bface531e4ac2b7019a1684df3370 |            ResellerAdmin            |
| 9fe2ff9ee4384b1894a90878d3e92bab |               _member_              |
| e00e9406b536470dbde2689ce1edb683 |                admin                |
| aa60501f1e664ddab72b0a9f27f96d2c |           heat_stack_user           |
| a082d27b033b4fdea37ebb2a5dc1a07b |               service               |
| 8f11f6761534407585feecb5e896922f |            swiftoperator            |
+----------------------------------+-------------------------------------+</pre></div></div><div class="sect3" id="assign-role"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assigning a Role to a User</span> <a title="Permalink" class="permalink" href="#assign-role">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>assign-role</li></ul></div></div></div></div><p>
   In this example, we want to add the role
   <span class="bold"><strong>ResellerAdmin</strong></span> to the demo user who has the
   ID <span class="bold"><strong>d2ff982a0b6547d0921b94957db714d6</strong></span>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Determine which Project/Tenant the user belongs to.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack user show d2ff982a0b6547d0921b94957db714d6</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
|    id               | d2ff982a0b6547d0921b94957db714d6 |
| name                | admin                            |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+</pre></div></li><li class="step "><p>
     We need to link the ResellerAdmin Role to a Project/Tenant. To start,
     determine which tenants are available on this deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-------------------------------+--+
|                id                |        name       | enabled |
+----------------------------------+-------------------------------+--+
| 4a8f4207a13444089a18dc524f41b2cf |       admin       |   True  |
| 00cbaf647bf24627b01b1a314e796138 |        demo       |   True  |
| 8374761f28df43b09b20fcd3148c4a08 |        gf1        |   True  |
| 0f8a9eef727f4011a7c709e3fbe435fa |        gf2        |   True  |
| 6eff7b888f8e470a89a113acfcca87db |        gf3        |   True  |
| f0b5d86c7769478da82cdeb180aba1b0 |        jaq1       |   True  |
| a46f1127e78744e88d6bba20d2fc6e23 |        jaq2       |   True  |
| 977b9b7f9a6b4f59aaa70e5a1f4ebf0b |        jaq3       |   True  |
| 4055962ba9e44561ab495e8d4fafa41d |        jaq4       |   True  |
| 33ec7f15476545d1980cf90b05e1b5a8 |        jaq5       |   True  |
| 9550570f8bf147b3b9451a635a1024a1 |      service      |   True  |
+----------------------------------+-------------------------------+--+</pre></div></li><li class="step "><p>
     Now that we have all the pieces, we can assign the ResellerAdmin role to
     this User on the Demo project.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138 507bface531e4ac2b7019a1684df3370</pre></div><p>
     This will produce no response if everything is correct.
    </p></li><li class="step "><p>
     Validate that the role has been assigned correctly. Pass in the user and
     tenant ID and request a list of roles assigned.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role list --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138</pre></div><p>
     Note that all members have the <span class="emphasis"><em>_member_</em></span> role as a
     default role in addition to any other roles that have been assigned.
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------------+----------------------------------+----------------------------------+
|                id                |      name     |             user_id              | tenant_id             |
+----------------------------------+---------------+----------------------------------+----------------------------------+
| 507bface531e4ac2b7019a1684df3370 | ResellerAdmin | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 9fe2ff9ee4384b1894a90878d3e92bab |    _member_   | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
+----------------------------------+---------------+----------------------------------+----------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="create-role"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a New Role</span> <a title="Permalink" class="permalink" href="#create-role">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>create-role</li></ul></div></div></div></div><p>
   In this example, we will create a Level 3 Support role called
   <span class="bold"><strong>L3Support</strong></span>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add the new role to the list of roles.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role create L3Support</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|    id    | 7e77946db05645c4ba56c6c82bf3f8d2 |
|   name   |            L3Support             |
+----------+----------------------------------+</pre></div></li><li class="step "><p>
     Now that we have the new role's ID, we can add that role to the Demo user
     from the previous example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add --user d2ff982a0b6547d0921b94957db714d6  --project 00cbaf647bf24627b01b1a314e796138 7e77946db05645c4ba56c6c82bf3f8d2</pre></div><p>
     This will produce no response if everything is correct.
    </p></li><li class="step "><p>
     Verify that the user Demo has both the ResellerAdmin and L3Support roles.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role list --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138</pre></div></li><li class="step "><p>
     The response should resemble the following output. Note that this user has
     the L3Support role, the ResellerAdmin role, and the default member role.
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------------+----------------------------------+----------------------------------+
|                id                |      name     |             user_id              |            tenant_id             |
+----------------------------------+---------------+----------------------------------+----------------------------------+
| 7e77946db05645c4ba56c6c82bf3f8d2 |   L3Support   | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 507bface531e4ac2b7019a1684df3370 | ResellerAdmin | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 9fe2ff9ee4384b1894a90878d3e92bab |    _member_   | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
+----------------------------------+---------------+----------------------------------+----------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="access-policies"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Policies</span> <a title="Permalink" class="permalink" href="#access-policies">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>access-policies</li></ul></div></div></div></div><p>
   Before introducing RBAC, Ceilometer had very simple access control. There
   were two types of user: admins and users. Admins will be able to access any
   API and perform any operation. Users will only be able to access non-admin
   APIs and perform operations only on the Project/Tenant where they belonged.
  </p></div><div class="sect3" id="newfile"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New RBAC Policy File</span> <a title="Permalink" class="permalink" href="#newfile">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>newfile</li></ul></div></div></div></div><p>
   This is the policy file for Ceilometer without RBAC
   (<span class="bold"><strong>etc/ceilometer/policy.json</strong></span>)
  </p><div class="verbatim-wrap"><pre class="screen">{
  "context_is_admin": "role:admin"
}</pre></div><p>
   With the RBAC-enhanced code it is possible to control access to each API
   command. The new policy file (<span class="emphasis"><em>rbac_policy.json</em></span>) looks
   like this.
  </p><div class="verbatim-wrap"><pre class="screen">{
    "context_is_admin": "role:admin",
    "telemetry:get_samples": "rule:context_is_admin",
    "telemetry:get_sample": "rule:context_is_admin",
    "telemetry:query_sample": "rule:context_is_admin",
    "telemetry:create_samples": "rule:context_is_admin",
    "telemetry:compute_statistics": "rule:context_is_admin",
    "telemetry:get_meters": "rule:context_is_admin",
    "telemetry:get_resource": "rule:context_is_admin",
    "telemetry:get_resources": "rule:context_is_admin",
    "telemetry:get_alarm": "rule:context_is_admin",
    "telemetry:query_alarm": "rule:context_is_admin",
    "telemetry:get_alarm_state": "rule:context_is_admin",
    "telemetry:get_alarms": "rule:context_is_admin",
    "telemetry:create_alarm": "rule:context_is_admin",
    "telemetry:set_alarm": "rule:service_role",
    "telemetry:delete_alarm": "rule:context_is_admin",
    "telemetry:alarm_history": "rule:context_is_admin",
    "telemetry:change_alarm_state": "rule:context_is_admin",
    "telemetry:query_alarm_history": "rule:context_is_admin"
}</pre></div><p>
   Note that the API action names are namespaced using the
   <span class="bold"><strong>telemetry:</strong></span> prefix. This avoids potential
   confusion if other services have policies with the same name.
  </p></div><div class="sect3" id="apply-policy"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Applying Policies to Roles</span> <a title="Permalink" class="permalink" href="#apply-policy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>apply-policy</li></ul></div></div></div></div><p>
   Copy the <span class="bold"><strong>rbac_policy.json</strong></span> file over the
   <span class="bold"><strong>policy.json</strong></span> file and make any required
   changes.
  </p></div><div class="sect3" id="apply-a-policy-to-multiple-roles"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Apply a policy to multiple roles</span> <a title="Permalink" class="permalink" href="#apply-a-policy-to-multiple-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>apply-a-policy-to-multiple-roles</li></ul></div></div></div></div><p>
   For example, the ResellerAdmin role could also be permitted to access
   <span class="bold"><strong>compute_statistics</strong></span>. This change would
   require the following changes in the
   <span class="bold"><strong>rbac_policy.json</strong></span> policy file:
  </p><div class="verbatim-wrap"><pre class="screen">{
    "context_is_admin": "role:admin",
    "i_am_reseller": "role:ResellerAdmin",
    "telemetry:get_samples": "rule:context_is_admin",
    "telemetry:get_sample": "rule:context_is_admin",
    "telemetry:query_sample": "rule:context_is_admin",
    "telemetry:create_samples": "rule:context_is_admin",
    "telemetry:compute_statistics": "rule:context_is_admin or rule:i_am_reseller",
    ...
}</pre></div><p>
   After a policy change has been made all the API services will need to be

   restarted

   .
  </p></div><div class="sect3" id="apply-a-policy-to-a-non-default-role-only"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Apply a policy to a non-default role only</span> <a title="Permalink" class="permalink" href="#apply-a-policy-to-a-non-default-role-only">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>apply-a-policy-to-a-non-default-role-only</li></ul></div></div></div></div><p>
   Another example: assign the L3Support role to the
   <span class="bold"><strong>get_meters</strong></span> API and exclude all other roles.
  </p><div class="verbatim-wrap"><pre class="screen">{
    "context_is_admin": "role:admin",
    "i_am_reseller": "role:ResellerAdmin",
    "l3_support": "role:L3Support",
    "telemetry:get_samples": "rule:context_is_admin",
    "telemetry:get_sample": "rule:context_is_admin",
    "telemetry:query_sample": "rule:context_is_admin",
    "telemetry:create_samples": "rule:context_is_admin",
    "telemetry:compute_statistics": "rule:context_is_admin or rule:i_am_reseller",
    "telemetry:get_meters": "rule:l3_support",
    ...
}</pre></div></div><div class="sect3" id="write-policy"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.7.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Writing a Policy</span> <a title="Permalink" class="permalink" href="#write-policy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>write-policy</li></ul></div></div></div></div><p>
   The Policy Engine capabilities are as expressible using a set of rules and
   guidelines. For a complete reference, please see the
   <a class="link" href="https://github.com/openstack/oslo.policy/blob/master/oslo_policy/policy.py" target="_blank">OSLO
   policy documentation</a>.
  </p><p>
   Policies can be expressed in one of two forms: A list of lists, or a string
   written in the new policy language.
  </p><p>
   In the list-of-lists representation, each check inside the innermost list is
   combined with an <span class="bold"><strong>and</strong></span> conjunction - for that
   check to pass, <span class="bold"><strong>all</strong></span> the specified checks
   must pass. These innermost lists are then combined as with an
   <span class="bold"><strong>or</strong></span> conjunction.
  </p><p>
   As an example, take the following rule, expressed in the list-of-lists
   representation:
  </p><div class="verbatim-wrap"><pre class="screen">[["role:admin"], ["project_id:%(project_id)s", "role:projectadmin"]]</pre></div><p>
   In the policy language, each check is specified the same way as in the
   list-of-lists representation: a simple [a:b] pair that is matched to the
   correct class to perform that check.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     User's Role
    </p><div class="verbatim-wrap"><pre class="screen">role:admin</pre></div></li><li class="listitem "><p>
     Rules already defined on policy
    </p><div class="verbatim-wrap"><pre class="screen">rule:admin_required</pre></div></li><li class="listitem "><p>
     Against a URL (URL checking must return TRUE to be valid)
    </p><div class="verbatim-wrap"><pre class="screen">http://my-url.org/check</pre></div></li><li class="listitem "><p>
     User attributes (obtained through the token: user_id, domain_id,
     project_id)
    </p><div class="verbatim-wrap"><pre class="screen">project_id:%(target.project.id)s</pre></div></li><li class="listitem "><p>
     Strings
    </p><div class="verbatim-wrap"><pre class="screen">&lt;variable&gt;:'xpto2035abc'
'myproject':&lt;variable&gt;</pre></div></li><li class="listitem "><p>
     Literals
    </p><div class="verbatim-wrap"><pre class="screen">project_id:xpto2035abc
domain_id:20
True:%(user.enabled)s</pre></div></li></ul></div><p>
   Conjunction operators are also available, allowing for more flexibility in
   crafting policies. So, in the policy language, the previous check in
   list-of-lists becomes:
  </p><div class="verbatim-wrap"><pre class="screen">role:admin or (project_id:%(project_id)s and role:projectadmin)</pre></div><p>
   The policy language also has the NOT operator, allowing for richer policy
   rules:
  </p><div class="verbatim-wrap"><pre class="screen">project_id:%(project_id)s and not role:dunce</pre></div><p>
   Attributes sent along with API calls can be used by the policy engine (on
   the right side of the expression), by using the following syntax:
  </p><div class="verbatim-wrap"><pre class="screen">&lt;some_value&gt;:%(user.id)s</pre></div><p>
   <span class="bold"><strong>Note</strong></span>: two special policy checks should be
   mentioned; the policy check <span class="bold"><strong>@</strong></span> will
   <span class="bold"><strong>always accept</strong></span> an access, and the policy
   check <span class="bold"><strong>!</strong></span> will <span class="bold"><strong>always
   reject</strong></span> an access.
  </p></div></div><div class="sect2" id="topic-zx2-mmd-5t"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Failover HA Support</span> <a title="Permalink" class="permalink" href="#topic-zx2-mmd-5t">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_failover_ha.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_failover_ha.xml</li><li><span class="ds-label">ID: </span>topic-zx2-mmd-5t</li></ul></div></div></div></div><p>
  In the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment, the Ceilometer metering service supports native
  Active-Active high-availability (HA) for the notification and polling agents.
  Implementing HA support includes workload-balancing, workload-distribution
  and failover.
 </p><p>
  Tooz is the coordination engine that is used to coordinate workload among
  multiple active agent instances. It also maintains the knowledge of
  active-instance-to-handle failover and group membership using hearbeats
  (pings).
 </p><p>
  Zookeeper is used as the coordination backend. Zookeeper uses Tooz to expose
  the APIs that manage group membership and retrieve workload specific to each
  agent.
 </p><p>
  The following section in the configuration file is used to implement
  high-availability (HA):
 </p><div class="verbatim-wrap"><pre class="screen">[coordination]
backend_url = &lt;IP address of Zookeeper host: port&gt; (port is usually 2181 as a zookeeper default)
heartbeat = 1.0
check_watchers = 10.0</pre></div><p>
  For the notification agent to be configured in HA mode, additional
  configuration is needed in the configuration file:
 </p><div class="verbatim-wrap"><pre class="screen">[notification]
workload_partitioning = true</pre></div><p>
  The HA notification agent distributes workload among multiple queues that are
  created based on the number of unique source:sink combinations. The
  combinations are configured in the notification agent pipeline configuration
  file. If there are additional services to be metered using notifications,
  then the recommendation is to use a separate source for those events. This is
  recommended especially if the expected load of data from that source is
  considered high. Implementing HA support should lead to better workload
  balancing among multiple active notification agents.
 </p><p>
  Ceilometer-expirer is also an Active-Active HA. Tooz is used to pick an
  expirer process that acquires a lock when there are multiple contenders and
  the winning process runs. There is no failover support, as expirer is not a
  daemon and is scheduled to run at pre-determined intervals.
 </p><div id="id-1.6.14.5.11.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   You must ensure that a single expirer process runs when multiple processes
   are scheduled to run at the same time. This must be done using cron-based
   scheduling. on multiple controller nodes
  </p></div><p>
  The following configuration is needed to enable expirer HA:
 </p><div class="verbatim-wrap"><pre class="screen">[coordination]
backend_url = &lt;IP address of Zookeeper host: port&gt; (port is usually 2181 as a zookeeper default)
heartbeat = 1.0
check_watchers = 10.0</pre></div><p>
  The notification agent HA support is mainly designed to coordinate among
  notification agents so that correlated samples can be handled by the same
  agent. This happens when samples get transformed from other samples. The
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Ceilometer pipeline has no transformers, so this task of coordination
  and workload partitioning does not need to be enabled. The notification agent
  is deployed on multiple controller nodes and they distribute workload among
  themselves by randomly fetching the data from the queue.
 </p><p>
  To disable coordination and workload partitioning by OpenStack, set the
  following value in the configuration file:
 </p><div class="verbatim-wrap"><pre class="screen">        [notification]
        workload_partitioning = False</pre></div><div id="id-1.6.14.5.11.17" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   When a configuration change is made to an API running under the HA Proxy,
   that change needs to be replicated in <span class="bold"><strong>all</strong></span>
   controllers.
  </p></div></div><div class="sect2" id="Ceilo-optimize"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optimizing the Ceilometer Metering Service</span> <a title="Permalink" class="permalink" href="#Ceilo-optimize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>Ceilo-optimize</li></ul></div></div></div></div><p>
  You can improve API and database responsiveness by configuring metering to
  store only the data you are require. This topic provides strategies for
  getting the most out of metering while not overloading your resources.
 </p><div class="sect3" id="changing-meter-list"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change the List of Meters</span> <a title="Permalink" class="permalink" href="#changing-meter-list">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>changing-meter-list</li></ul></div></div></div></div><p>
   The list of meters can be easily reduced or increased by editing the
   pipeline.yaml file and restarting the polling agent.
  </p><p>
   Sample compute-only pipeline.yaml file with the daily poll interval:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><div id="id-1.6.14.5.12.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    This change will cause all non-default meters to stop receiving
    notifications.
   </p></div></div><div class="sect3" id="ceilometer-nova"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Nova Notifications</span> <a title="Permalink" class="permalink" href="#ceilometer-nova">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>ceilometer-nova</li></ul></div></div></div></div><p>
   You can configure Nova to send notifications by enabling the setting in the
   configuration file. When enabled, Nova will send information to Ceilometer
   related to its usage and VM status. You must restart Nova for these changes
   to take effect.
  </p><p>
   The Openstack notification daemon, also known as a polling agent, monitors
   the message bus for data being provided by other OpenStack components such
   as Nova. The notification daemon loads one or more listener plugins, using
   the <code class="literal">ceilometer.notification</code> namespace. Each plugin can
   listen to any topic, but by default it will listen to the
   <code class="literal">notifications.info</code> topic. The listeners grab messages off
   the defined topics and redistribute them to the appropriate plugins
   (endpoints) to be processed into Events and Samples. After the Nova service
   is restarted, you should verify that the notification daemons are receiving
   traffic.
  </p><p>
   For a more in-depth look at how information is sent over
   <span class="emphasis"><em>openstack.common.rpc</em></span>, refer to the
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/measurements.html" target="_blank">OpenStack
   Ceilometer documentation</a>.
  </p><p>
   Nova can be configured to send following data to Ceilometer:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><tbody><tr><td><span class="bold"><strong>Name</strong></span>
      </td><td><span class="bold"><strong>Unit</strong></span>
      </td><td><span class="bold"><strong>Type</strong></span>
      </td><td><span class="bold"><strong>Resource</strong></span>
      </td><td><span class="bold"><strong>Note</strong></span>
      </td></tr><tr><td>instance</td><td>g</td><td>instance</td><td> inst ID</td><td>Existence of instance</td></tr><tr><td>instance: <code class="varname">type</code>
      </td><td>g</td><td>instance</td><td> inst ID</td><td>Existence of instance of <code class="varname">type</code> (Where
                                    <code class="varname">type</code> is a valid OpenStack type.) </td></tr><tr><td>memory</td><td>g</td><td>MB</td><td> inst ID</td><td>Amount of allocated RAM. Measured in MB.</td></tr><tr><td>vcpus</td><td>g</td><td>vcpu</td><td> inst ID</td><td>Number of VCPUs</td></tr><tr><td>disk.root.size</td><td>g</td><td>GB</td><td> inst ID</td><td>Size of root disk. Measured in GB.</td></tr><tr><td>disk.ephemeral.size</td><td>g</td><td>GB</td><td> inst ID</td><td>Size of ephemeral disk. Measured in GB.</td></tr></tbody></table></div><p>
   To enable Nova to publish notifications:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">nova.conf</pre></div></li><li class="listitem "><p>
     Compare the example of a working configuration file with the necessary
     changes to your configuration file. If there is anything missing in your
     file, add it, and then save the file.
    </p><div class="verbatim-wrap"><pre class="screen">notification_driver=messaging
notification_topics=notifications
notify_on_state_change=vm_and_task_state
instance_usage_audit=True
instance_usage_audit_period=hour</pre></div><div id="id-1.6.14.5.12.4.8.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      The <code class="literal">instance_usage_audit_period</code> interval can be set to
      check the instance's status every hour, once a day, once a week or once a
      month. Every time the audit period elapses, Nova sends a notification to
      Ceilometer to record whether or not the instance is alive and running.
      Metering this statistic is critical if billing depends on usage.
     </p></div></li><li class="listitem "><p>
     To restart Nova service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl restart nova-api.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-conductor.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-scheduler.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-novncproxy.service</pre></div><div id="id-1.6.14.5.12.4.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      Different platforms may use their own unique command to restart
      nova-compute services. If the above command does not work, please refer
      to the documentation for your specific platform.
     </p></div></li><li class="listitem "><p>
     To verify successful launch of each process, list the service components:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>nova service-list
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host       | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | controller | internal | enabled | up    | 2014-09-16T23:54:02.000000 | -               |
| 2  | nova-consoleauth | controller | internal | enabled | up    | 2014-09-16T23:54:04.000000 | -               |
| 3  | nova-scheduler   | controller | internal | enabled | up    | 2014-09-16T23:54:07.000000 | -               |
| 4  | nova-cert        | controller | internal | enabled | up    | 2014-09-16T23:54:00.000000 | -               |
| 5  | nova-compute     | compute1   | nova     | enabled | up    | 2014-09-16T23:54:06.000000 | -               |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+</pre></div></li></ol></div></div><div class="sect3" id="webserverapi"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve Reporting API Responsiveness</span> <a title="Permalink" class="permalink" href="#webserverapi">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>webserverapi</li></ul></div></div></div></div><p>
   Reporting APIs are the main access to the metering data stored in
   Ceilometer. These APIs are accessed by Horizon to provide basic usage data
   and information.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses Apache2 Web Server to provide the API access. This topic
   provides some strategies to help you optimize the front-end and back-end
   databases.
  </p><p>
   To improve the responsiveness you can increase the number of threads and
   processes in the ceilometer configuration file. The Ceilometer API runs as
   an WSGI processes. Each process can have a certain amount of threads
   managing the filters and applications, which can comprise the processing
   pipeline.
  </p><p>
   <span class="bold"><strong>To configure Apache2 to use increase the number of
   threads</strong></span>, use the steps in <a class="xref" href="#reconfig-metering" title="12.3.5. Configure the Ceilometer Metering Service">Section 12.3.5, “Configure the Ceilometer Metering Service”</a>
  </p><div id="id-1.6.14.5.12.5.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    The resource usage panel could take some time to load depending on the
    number of metrics selected.
   </p></div></div><div class="sect3" id="update-polling-strategy"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the Polling Strategy and Swift Considerations</span> <a title="Permalink" class="permalink" href="#update-polling-strategy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>update-polling-strategy</li></ul></div></div></div></div><p>
   Polling can put an excessive amount of strain on the system due to the
   amount of data the system may have to process. Polling also has a severe
   impact on queries since the database can have very large amount of data to
   scan before responding to the query. This process usually consumes a large
   amount of CPU and memory to complete the requests. Clients can also
   experience long waits for queries to come back and, in extreme cases, even
   timeout.
  </p><p>
   There are 3 polling meters in Swift:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     storage.objects
    </p></li><li class="listitem "><p>
     storage.objects.size
    </p></li><li class="listitem "><p>
     storage.objects.containers
    </p></li></ul></div><p>
   Sample section of the pipeline.yaml configuration file with Swift polling on
   an hourly interval:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: swift_source
      interval: 3600
      sources:
            meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><p>
   Every time the polling interval occurs, at least 3 messages per existing
   object/container in Swift are collected. The following table illustrates the
   amount of data produced hourly in different scenarios:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td>Swift Containers</td><td>Swift Objects per container</td><td>Samples per Hour</td><td>Samples stored per 24 hours</td></tr><tr><td>10</td><td>10</td><td>500</td><td>12000</td></tr><tr><td>10</td><td>100</td><td>5000</td><td>120000</td></tr><tr><td>100</td><td>100</td><td>50000</td><td>1200000</td></tr><tr><td>100</td><td>1000</td><td>500000</td><td>12000000</td></tr></tbody></table></div><p>
   Looking at the data we can see that even a very small Swift storage with 10
   containers and 100 files will store 120K samples in 24 hours, bringing it to
   a total of 3.6 million samples.
  </p><div id="id-1.6.14.5.12.6.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    The file size of each file does not have any impact on the number of samples
    collected. In fact the smaller the number of containers or files, the
    smaller the sample size. In the scenario where there a large number of small
    files and containers, the sample size is also large and the performance is
    at its worst.
   </p></div></div></div><div class="sect2" id="ceilo-samples"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service Samples</span> <a title="Permalink" class="permalink" href="#ceilo-samples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/metering-metering_samples.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_samples.xml</li><li><span class="ds-label">ID: </span>ceilo-samples</li></ul></div></div></div></div><p>
  Samples are discrete collections of a particular meter or the actual usage
  data defined by a meter description. Each sample is time-stamped and includes
  a variety of data that varies per meter but usually includes the project ID
  and UserID of the entity that consumed the resource represented by the meter
  and sample.
 </p><p>
  In a typical deployment, the number of samples can be in the tens of
  thousands if not higher for a specific collection period depending on overall
  activity.
 </p><p>
  Sample collection and data storage expiry settings are configured in
  Ceilometer. Use cases that include collecting data for monthly billing cycles
  are usually stored over a period of 45 days and require a large, scalable,
  back-end database to support the large volume of samples generated by
  production OpenStack deployments.
 </p><p>
  <span class="bold"><strong>Example configuration:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">[database]
metering_time_to_live=-1</pre></div><p>
  In our example use case, to construct a complete billing record, an external
  billing application must collect all pertinent samples. Then the results must
  be sorted, summarized, and combine with the results of other types of metered
  samples that are required. This function is known as aggregation and is
  external to the Ceilometer service.
 </p><p>
  Meter data, or samples, can also be collected directly from the service APIs
  by individual Ceilometer polling agents. These polling agents directly access
  service usage by calling the API of each service.
 </p><p>
  OpenStack services such as Swift currently only provide metered data through
  this function and some of the other OpenStack services provide specific
  metrics only through a polling action.
 </p></div></div></div><div class="chapter " id="system-maintenance"><div class="titlepage"><div><div><h1 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Maintenance</span> <a title="Permalink" class="permalink" href="#system-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-system_maintenance.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-system_maintenance.xml</li><li><span class="ds-label">ID: </span>system-maintenance</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#planned-maintenance"><span class="number">13.1 </span><span class="name">Planned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#unplanned-maintenance"><span class="number">13.2 </span><span class="name">Unplanned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#maintenance-update"><span class="number">13.3 </span><span class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span></a></span></dt><dt><span class="section"><a href="#deploy-ptf"><span class="number">13.4 </span><span class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span></a></span></dt><dt><span class="section"><a href="#database-maintenance"><span class="number">13.5 </span><span class="name">Periodic OpenStack Maintenance Tasks</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring your cloud as well as procedures
  for performing node maintenance.
 </p><p>
  This section contains the following sections to help you manage, configure,
  and maintain your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
 </p><div class="sect1" id="planned-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned System Maintenance</span> <a title="Permalink" class="permalink" href="#planned-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-planned_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-planned_maintenance.xml</li><li><span class="ds-label">ID: </span>planned-maintenance</li></ul></div></div></div></div><p>
  Planned maintenance tasks for your cloud. See sections below for:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#cont-planned" title="13.1.2. Planned Control Plane Maintenance">Section 13.1.2, “Planned Control Plane Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#comp-planned" title="13.1.3. Planned Compute Maintenance">Section 13.1.3, “Planned Compute Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#planned-maintenance-task-for-networking-nodes" title="13.1.4. Planned Network Maintenance">Section 13.1.4, “Planned Network Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#storage-maintenance" title="13.1.5. Planned Storage Maintenance">Section 13.1.5, “Planned Storage Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#mariadb-manual-update" title="13.1.6. Updating MariaDB with Galera">Section 13.1.6, “Updating MariaDB with Galera”</a>
   </p></li></ul></div><div class="sect2" id="sysmn-gen"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Whole Cloud Maintenance</span> <a title="Permalink" class="permalink" href="#sysmn-gen">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-general_procedures.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-general_procedures.xml</li><li><span class="ds-label">ID: </span>sysmn-gen</li></ul></div></div></div></div><p>
  Planned maintenance procedures for your whole cloud.
 </p><div class="sect3" id="stop-restart"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bringing Down Your Cloud: Services Down Method</span> <a title="Permalink" class="permalink" href="#stop-restart">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_down.xml</li><li><span class="ds-label">ID: </span>stop-restart</li></ul></div></div></div></div><div id="id-1.6.15.4.4.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   If you have a planned maintenance and need to bring down your entire cloud,
   update and reboot all nodes in the cloud one by one. Start with the deployer
   node, then follow the order recommended in <a class="xref" href="#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>. This method will bring down all of your services.
  </p></div><p>
  If you wish to use a method utilizing rolling reboots where your cloud
  services will continue running then see <a class="xref" href="#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>.
 </p><p>
  To perform backups prior to these steps, visit the backup and
  restore pages first at <a class="xref" href="#bura-overview" title="Chapter 14. Backup and Restore">Chapter 14, <em>Backup and Restore</em></a>.
 </p><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-down-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Gracefully Bringing Down and Restarting Your Cloud Environment</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-down-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_down.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-down-xml-7</li></ul></div></div></div></div><p>
   You will do the following steps from your Cloud Lifecycle Manager.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Gracefully shut down your cloud by running the
     <code class="literal">ardana-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml</pre></div></li><li class="step "><p>
     Shut down your nodes. You should shut down your controller nodes last and
     bring them up first after the maintenance.
    </p><p>
     There are multiple ways you can do this:
    </p><ol type="a" class="substeps "><li class="step "><p>
       You can SSH to each node and use <code class="literal">sudo reboot -f</code> to
       reboot the node.
      </p></li><li class="step "><p>
       From the Cloud Lifecycle Manager, you can use the
       <code class="literal">bm-power-down.yml</code> and
       <code class="literal">bm-power-up.yml</code> playbooks.
      </p></li><li class="step "><p>
       You can shut down the nodes and then physically restart them either via a
       power button or the IPMI.
      </p></li></ol></li><li class="step "><p>
     Perform the necessary maintenance.
    </p></li><li class="step "><p>
     After the maintenance is complete, power your Cloud Lifecycle Manager back up
     and then SSH to it.
    </p></li><li class="step "><p>
     Determine the current power status of the nodes in your environment:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-status.yml</pre></div></li><li class="step "><p>
     If necessary, power up any nodes that are not already powered up, ensuring
     that you power up your controller nodes first. You can target specific
     nodes with the <code class="literal">-e nodelist=&lt;node_name&gt;</code> switch.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-up.yml [-e nodelist=&lt;node_name&gt;]</pre></div><div id="id-1.6.15.4.4.3.5.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     Obtain the <code class="literal">&lt;node_name&gt;</code> by using the
     <code class="command">sudo cobbler system list</code> command from the Cloud Lifecycle Manager.
    </p></div></li><li class="step "><p>
     Bring the databases back up:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step "><p>
     Gracefully bring up your cloud services by running the
     <code class="literal">ardana-start.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml</pre></div></li><li class="step "><p>
     Pause for a few minutes and give the cloud environment time to come up
     completely and then verify the status of the individual services using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li><li class="step "><p>
     If any services did not start properly, you can run playbooks for the
     specific services having issues.
    </p><p>
     For example:
    </p><p>
     If RabbitMQ fails, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-start.yml</pre></div><p>
     You can check the status of RabbitMQ afterwards with this:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div><p>
     If the recovery had failed, you can run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</pre></div><p>
     Each of the other services have playbooks in the
     <code class="literal">~/scratch/ansible/next/ardana/ansible</code> directory in the
     format of <code class="literal">&lt;service&gt;-start.yml</code> that you can run.
     One example, for the compute service, is
     <code class="literal">nova-start.yml</code>.
    </p></li><li class="step "><p>
     Continue checking the status of your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> cloud services until
     there are no more failed or unreachable nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></div></div><div class="sect3" id="rebootNodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rolling Reboot of the Cloud</span> <a title="Permalink" class="permalink" href="#rebootNodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>rebootNodes</li></ul></div></div></div></div><p>
  If you have a planned maintenance and need to bring down your entire cloud
  and restart services while minimizing downtime, follow the steps here to
  safely restart your cloud. If you do not mind your services being down, then
  another option for planned maintenance can be found at
  <a class="xref" href="#stop-restart" title="13.1.1.1. Bringing Down Your Cloud: Services Down Method">Section 13.1.1.1, “Bringing Down Your Cloud: Services Down Method”</a>.
 </p><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended node reboot order</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-5</li></ul></div></div></div></div><p>
   To ensure that rebooted nodes reintegrate into the cluster, the key is
   having enough time between controller reboots.
  </p><p>
   The recommended way to achieve this is as follows:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Reboot controller nodes one-by-one with a suitable interval in between. If
     you alternate between controllers and compute nodes you will gain more
     time between the controller reboots.
    </p></li><li class="step "><p>
     Reboot of compute nodes (if present in your cloud).
    </p></li><li class="step "><p>
     Reboot of Swift nodes (if present in your cloud).
    </p></li><li class="step "><p>
     Reboot of ESX nodes (if present in your cloud).
    </p></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting controller nodes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Turn off the Keystone Fernet Token-Signing Key
   Rotation</strong></span>
  </p><p>
   Before rebooting any controller node, you need to ensure that the Keystone
   Fernet token-signing key rotation is turned off. Run the following command:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-stop-fernet-auto-rotation.yml</pre></div><p>
   <span class="bold"><strong>Migrate singleton services first</strong></span>
  </p><div id="id-1.6.15.4.4.4.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you have previously rebooted your Cloud Lifecycle Manager for any reason, ensure that
    the <code class="systemitem">apache2</code> service is running before
    continuing. To start the <code class="systemitem">apache2</code> service, use
    this command:
   </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start apache2</pre></div></div><p>
   The first consideration before rebooting any controller nodes is that there
   are a few services that run as singletons (non-HA), thus they will be
   unavailable while the controller they run on is down. Typically this is a
   very small window, but if you want to retain the service during the reboot
   of that server you should take special action to maintain service, such as
   migrating the service.
  </p><p>
   For these steps, if your singleton services are running on controller1 and
   you move them to controller2, then ensure you move them back to controller1
   before proceeding to reboot controller2.
  </p><p>
   <span class="bold"><strong>For the <code class="literal">cinder-volume</code> singleton
   service:</strong></span>
  </p><p>
   Execute the following command on each controller node to determine which
   node is hosting the cinder-volume singleton. It should be running on only
   one node:
  </p><div class="verbatim-wrap"><pre class="screen">ps auxww | grep cinder-volume | grep -v grep</pre></div><p>
   Run the <code class="literal">cinder-migrate-volume.yml</code> playbook - details
   about the Cinder volume and backup migration instructions can be found in
   <a class="xref" href="#sec-operation-manage-block-storage" title="7.1.3. Managing Cinder Volume and Backup Services">Section 7.1.3, “Managing Cinder Volume and Backup Services”</a>.
  </p><p>
   <span class="bold"><strong>For the <code class="literal">nova-consoleauth</code> singleton
   service:</strong></span>
  </p><p>
   The <code class="literal">nova-consoleauth</code> component runs by default on the
   first controller node, that is, the host with
   <code class="literal">consoleauth_host_index=0</code>. To move it to another
   controller node before rebooting controller 0, run the ansible playbook
   <code class="literal">nova-start.yml</code> and pass it the index of the next
   controller node. For example, to move it to controller 2 (index of 1), run:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</pre></div><p>
   After you run this command you may now see two instances of the
   <code class="literal">nova-consoleauth</code> service, which will show as being in
   <code class="literal">disabled</code> state, when you run the <code class="literal">nova
   service-list</code> command. You can then delete the service using these
   steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Obtain the service ID for the duplicated nova-consoleauth service:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| Id | Binary           | Host                      | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 10 | nova-conductor   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:47.000000 | -               |
| 13 | nova-conductor   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 16 | nova-scheduler   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:39.000000 | -               |
| 19 | nova-scheduler   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 22 | nova-scheduler   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:44.000000 | -               |
| 25 | nova-consoleauth | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:45.000000 | -               |
| 49 | nova-compute     | ...a-cp1-comp0001-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 52 | nova-compute     | ...a-cp1-comp0002-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 55 | nova-compute     | ...a-cp1-comp0003-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:43.000000 | -               |
<span class="bold"><strong>| 70 | nova-consoleauth | ...a-cp1-c1-m3-mgmt    | internal | disabled | down  | 2016-08-25T12:10:40.000000 | -               |</strong></span>
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+</pre></div></li><li class="step "><p>
     Delete the disabled duplicate service with this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete &lt;service_ID&gt;</pre></div><p>
     Given the example in the previous step, the command could be:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete 70</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>For the SNAT namespace singleton service:</strong></span>
  </p><p>
   If you reboot the controller node hosting the SNAT namespace service on it,
   Compute instances without floating IPs will lose network connectivity when
   that controller is rebooted. To prevent this from happening, you can use
   these steps to determine which controller node is hosting the SNAT namespace
   service and migrate it to one of the other controller nodes while that node
   is rebooted.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Locate the SNAT node where the router is providing the active
     <code class="literal">snat_service</code>:
    </p><ol type="a" class="substeps "><li class="step "><p>
       From the Cloud Lifecycle Manager, list out your ports to determine which port
       is serving as the router gateway:
      </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
neutron port-list --device_owner network:router_gateway</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-list --device_owner network:router_gateway
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                           |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| 287746e6-7d82-4b2c-914c-191954eba342 |      | fa:16:3e:2e:26:ac | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"} |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
       Look at the details of this port to determine what the
       <code class="literal">binding:host_id</code> value is, which will point to the
       host in which the port is bound to:
      </p><div class="verbatim-wrap"><pre class="screen">neutron port-show &lt;port_id&gt;</pre></div><p>
       Example, with the value you need in bold:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m2-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div><p>
       In this example, the <code class="literal">ardana-cp1-c1-m2-mgmt</code> is the
       node hosting the SNAT namespace service.
      </p></li></ol></li><li class="step "><p>
     SSH to the node hosting the SNAT namespace service and check the SNAT
     namespace, specifying the router_id that has the interface with the subnet
     that you are interested in:
    </p><div class="verbatim-wrap"><pre class="screen">ssh &lt;IP_of_SNAT_namespace_host&gt;
sudo ip netns exec snat-&lt;router_ID&gt; bash</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns exec snat-e122ea3f-90c5-4662-bf4a-3889f677aacf bash</pre></div></li><li class="step "><p>
     Obtain the ID for the L3 Agent for the node hosting your SNAT namespace:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
neutron agent-list</pre></div><p>
     Example, with the entry you need given the examples above:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron agent-list
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 0126bbbf-5758-4fd0-84a8-7af4d93614b8 | DHCP agent           | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| 33dec174-3602-41d5-b7f8-a25fd8ff6341 | Metadata agent       | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-metadata-agent    |
| 3bc28451-c895-437b-999d-fdcff259b016 | L3 agent             | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 4af1a941-61c1-4e74-9ec1-961cebd6097b | L3 agent             | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-l3-agent          |
| 58f01f34-b6ca-4186-ac38-b56ee376ffeb | Loadbalancerv2 agent | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-lbaasv2-agent     |
| 65bcb3a0-4039-4d9d-911c-5bb790953297 | Open vSwitch agent   | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| 6981c0e5-5314-4ccd-bbad-98ace7db7784 | L3 agent             | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 7df9fa0b-5f41-411f-a532-591e6db04ff1 | Metadata agent       | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-metadata-agent    |
| 92880ab4-b47c-436c-976a-a605daa8779a | Metadata agent       | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-metadata-agent    |
<span class="bold"><strong>| a209c67d-c00f-4a00-b31c-0db30e9ec661 | L3 agent             | ardana-cp1-c1-m2-mgmt</strong></span>    | :-)   | True           | neutron-vpn-agent         |
| a9467f7e-ec62-4134-826f-366292c1f2d0 | DHCP agent           | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| b13350df-f61d-40ec-b0a3-c7c647e60f75 | Open vSwitch agent   | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| d4c07683-e8b0-4a2b-9d31-b5b0107b0b31 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-openvswitch-agent |
| e91d7f3f-147f-4ad2-8751-837b936801e3 | Open vSwitch agent   | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| f33015c8-f4e4-4505-b19b-5a1915b6e22a | DHCP agent           | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| fe43c0e9-f1db-4b67-a474-77936f7acebf | Metadata agent       | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-metadata-agent    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+</pre></div></li><li class="step "><p>
     Also obtain the ID for the L3 Agent of the node you are going to move the
     SNAT namespace service to using the same commands as the previous step.
    </p></li><li class="step "><p>
     Use these commands to move the SNAT namespace service, with the
     <code class="literal">router_id</code> being the same value as the ID for router:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Remove the L3 Agent for the old host:
      </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-remove &lt;agent_id_of_snat_namespace_host&gt; &lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron l3-agent-router-remove a209c67d-c00f-4a00-b31c-0db30e9ec661 e122ea3f-90c5-4662-bf4a-3889f677aacf
Removed router e122ea3f-90c5-4662-bf4a-3889f677aacf from L3 agent</pre></div></li><li class="step "><p>
       Remove the SNAT namespace:
      </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns delete snat-&lt;router_id&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ sudo ip netns delete snat-e122ea3f-90c5-4662-bf4a-3889f677aacf</pre></div></li><li class="step "><p>
       Create a new L3 Agent for the new host:
      </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-add &lt;agent_id_of_new_snat_namespace_host&gt; &lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron l3-agent-router-add 3bc28451-c895-437b-999d-fdcff259b016 e122ea3f-90c5-4662-bf4a-3889f677aacf
Added router e122ea3f-90c5-4662-bf4a-3889f677aacf to L3 agent</pre></div></li></ol><p>
     Confirm that it has been moved by listing the details of your port from step
     1b above, noting the value of <code class="literal">binding:host_id</code> which
     should be updated to the host you moved your SNAT namespace to:
    </p><div class="verbatim-wrap"><pre class="screen">neutron port-show &lt;port_ID&gt;</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m1-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Reboot the controllers</strong></span>
  </p><p>
   In order to reboot the controller nodes, you must first retrieve a list of
   nodes in your cloud running control plane services.
  </p><div class="verbatim-wrap"><pre class="screen">for i in $(grep -w cluster-prefix ~/openstack/my_cloud/definition/data/control_plane.yml | awk '{print $2}'); do grep $i ~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts | grep ansible_ssh_host | awk '{print $1}'; done</pre></div><p>
   Then perform the following steps from your Cloud Lifecycle Manager for each of
   your controller nodes:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If any singleton services are active on this node, they will be
     unavailable while the node is down. If you want to retain the service
     during the reboot, you should take special action to maintain the service,
     such as migrating the service as appropriate as noted above.
    </p></li><li class="step "><p>
     Stop all services on the controller node that you are rebooting first:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     Reboot the controller node, e.g. run the following command on the
     controller itself:
    </p><div class="verbatim-wrap"><pre class="screen">sudo reboot</pre></div><p>
     Note that the current node being rebooted could be hosting the lifecycle
     manager.
    </p></li><li class="step "><p>
     Wait for the controller node to become ssh-able and allow an additional
     minimum of five minutes for the controller node to settle. Start all
     services on the controller node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     Verify that the status of all services on that is OK on the controller
     node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml --limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     When above start operation has completed successfully, you may proceed to
     the next controller node. Ensure that you migrate your singleton services
     off the node first.
    </p></li></ol></div></div><div id="id-1.6.15.4.4.4.4.26" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    It is important that you not begin the reboot procedure for a new
    controller node until the reboot of the previous controller node has been
    completed successfully (that is, the ardana-status playbook has completed
    without error).
   </p></div><p>
   <span class="bold"><strong>
    Reenable the Keystone Fernet Token-Signing Key Rotation
   </strong></span>
  </p><p>
   After all the controller nodes are successfully updated and back online, you
   need to re-enable the Keystone Fernet token-signing key rotation job by
   running the <code class="filename">keystone-reconfigure.yml</code> playbook. On the
   deployer, run:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-9"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting compute nodes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-9</li></ul></div></div></div></div><p>
   To reboot a compute node the following operations will need to be performed:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Disable provisioning of the node to take the node offline to prevent
     further instances being scheduled to the node during the reboot.
    </p></li><li class="listitem "><p>
     Identify instances that exist on the compute node, and then either:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Live migrate the instances off the node before actioning the reboot. OR
      </p></li><li class="listitem "><p>
       Stop the instances
      </p></li></ul></div></li><li class="listitem "><p>
     Reboot the node
    </p></li><li class="listitem "><p>
     Restart the Nova services
    </p></li></ul></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Disable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "&lt;describe reason&gt;" &lt;node name&gt; nova-compute</pre></div><p>
     If the node has existing instances running on it these instances will need
     to be migrated or stopped prior to re-booting the node.
    </p></li><li class="step "><p>
     Live migrate existing instances. Identify the instances on the compute
     node. Note: The following command must be run with nova admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="step "><p>
     Migrate or Stop the instances on the compute node.
    </p><p>
     Migrate the instances off the node by running one of the following
     commands for each of the instances:
    </p><p>
     If your instance is booted from a volume and has any number of Cinder
     volume attached, use the nova live-migration command:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only, you can use the
     --block-migrate option:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     Note: The [&lt;target compute host&gt;] option is optional. If you do not
     specify a target host then the nova scheduler will choose a node for you.
    </p><p>
     OR
    </p><p>
     Stop the instances on the node by running the following command for each
     of the instances:
    </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance-uuid&gt;</pre></div></li><li class="step "><p>
     Stop all services on the Compute node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;compute node&gt;</pre></div></li><li class="step "><p>
     SSH to your Compute nodes and reboot them:
    </p><div class="verbatim-wrap"><pre class="screen">sudo reboot</pre></div><p>
     The operating system cleanly shuts down services and then automatically
     reboots. If you want to be very thorough, run your backup jobs just before
     you reboot.
    </p></li><li class="step "><p>
     Run the ardana-start.yml playbook from the Cloud Lifecycle Manager. If needed, use
     the bm-power-up.yml playbook to restart the node. Specify just the node(s)
     you want to start in the 'nodelist' parameter arguments, that is,
     nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node&gt;</pre></div></li><li class="step "><p>
     Execute the <span class="bold"><strong>ardana-start.yml </strong></span>playbook.
     Specifying the node(s) you want to start in the 'limit' parameter
     arguments. This parameter accepts wildcard arguments and also
     '@&lt;filename&gt;' to process all hosts listed in the file.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;compute node&gt;</pre></div></li><li class="step "><p>
     Re-enable provisioning on the node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;node-name&gt; nova-compute</pre></div></li><li class="step "><p>
     Restart any instances you stopped.
    </p><div class="verbatim-wrap"><pre class="screen">nova start &lt;instance-uuid&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-10"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting Swift nodes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-10</li></ul></div></div></div></div><p>
   If your Swift services are on controller node, please follow the controller
   node reboot instructions above.
  </p><p>
   For a dedicated Swift PAC cluster or Swift Object resource node:
  </p><p>
   For each Swift host
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Stop all services on the Swift node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;Swift node&gt;</pre></div></li><li class="step "><p>
     Reboot the Swift node by running the following command on the Swift node
     itself:
    </p><div class="verbatim-wrap"><pre class="screen">sudo reboot</pre></div></li><li class="step "><p>
     Wait for the node to become ssh-able and then start all services on the
     Swift node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;swift node&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Get list of status playbooks</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-14</li></ul></div></div></div></div><p>
   Running the following command will yield a list of status playbooks:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ls *status*</pre></div><p>
   Here is the list:
  </p><div class="verbatim-wrap"><pre class="screen">ls *status*
bm-power-status.yml          heat-status.yml      logging-producer-status.yml
ceilometer-status.yml        FND-AP2-status.yml   ardana-status.yml
FND-CLU-status.yml           horizon-status.yml   logging-status.yml
cinder-status.yml            freezer-status.yml   ironic-status.yml
cmc-status.yml               glance-status.yml    keystone-status.yml
galera-status.yml            memcached-status.yml nova-status.yml
logging-server-status.yml    monasca-status.yml   ops-console-status.yml
monasca-agent-status.yml     neutron-status.yml   rabbitmq-status.yml
swift-status.yml             zookeeper-status.yml</pre></div></div></div></div><div class="sect2" id="cont-planned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Control Plane Maintenance</span> <a title="Permalink" class="permalink" href="#cont-planned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-cont_planned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-cont_planned.xml</li><li><span class="ds-label">ID: </span>cont-planned</li></ul></div></div></div></div><p>
  Planned maintenance tasks for controller nodes such as full cloud reboots and
  replacing controller nodes.
 </p><div class="sect3" id="replacing-controller"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Controller Node</span> <a title="Permalink" class="permalink" href="#replacing-controller">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-replace_controller.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_controller.xml</li><li><span class="ds-label">ID: </span>replacing-controller</li></ul></div></div></div></div><p>
  This section outlines steps for replacing a controller node in your
  environment.
 </p><p>
  For <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you must have three controller nodes.
  Therefore, adding or removing nodes is not an option. However, if you need to
  repair or replace a controller node, you may do so by following the steps
  outlined here. Note that to run any playbooks whatsoever for cloud
  maintenance, you will always run the steps from the Cloud Lifecycle Manager.
 </p><p>
  These steps will depend on whether you need to replace a shared lifecycle
  manager/controller node or whether this is a standalone controller node.
 </p><p>Keep in mind while performing the following tasks:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Do not add entries for a new server. Instead, update the entries for
    the broken one.
   </p></li><li class="listitem "><p>
    Be aware that all management commands are run on the node where the
    Cloud Lifecycle Manager is running.
   </p></li></ul></div><div class="sect4" id="replace-shared-lm"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Shared Cloud Lifecycle Manager/Controller Node</span> <a title="Permalink" class="permalink" href="#replace-shared-lm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-replace_shared_lm.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_shared_lm.xml</li><li><span class="ds-label">ID: </span>replace-shared-lm</li></ul></div></div></div></div><p>
  If the controller node you need to replace was also being used as your
  Cloud Lifecycle Manager then use these steps below. If this is not a shared
  controller then skip to the next section.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that you previously had
    loaded on your Cloud Lifecycle Manager, you will need to download and install the
    lifecycle management software using the instructions from the installation
    guide:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 3 “Installing the Cloud Lifecycle Manager server”, Section 3.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span>
     </p></li><li class="listitem "><p>
      To restore your data, see <a class="xref" href="#pit-lifecyclemanager-recovery" title="13.2.2.2.3. Point-in-time Cloud Lifecycle Manager Recovery">Section 13.2.2.2.3, “Point-in-time Cloud Lifecycle Manager Recovery”</a>
     </p></li></ol></div></li><li class="step "><p>
    On the new node, update your cloud model with the new
    <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields to
    reflect the attributes of the node. Do not change the
    <code class="literal">id</code>, <code class="literal">ip-addr</code>, <code class="literal">role</code>,
    or <code class="literal">server-group</code> settings.
   </p><div id="id-1.6.15.4.5.3.7.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div></li><li class="step "><p>
    Get the <span class="bold"><strong>servers.yml</strong></span> file stored in git:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div><p>
    then change, as necessary, the <code class="literal">mac-addr</code>,
    <code class="literal">ilo-ip</code>, <code class="literal">ilo-password</code>, and
    <code class="literal">ilo-user</code> fields of this existing controller node. Save
    and commit the change
   </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "repaired node X"</pre></div></li><li class="step "><p>
    Run the configuration processor as follows:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
    Then run ready-deployment:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Deploy Cobbler:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div><div id="id-1.6.15.4.5.3.7.3.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    After this step you may see failures because MariaDB has not finished
    syncing. If so, rerun this step.
   </p></div></li><li class="step "><p>
    Delete the haproxy user:
   </p><div class="verbatim-wrap"><pre class="screen">sudo userdel haproxy</pre></div></li><li class="step "><p>
    Install the software on your new Cloud Lifecycle Manager/controller node with
    these three playbooks:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller-hostname&gt;,&lt;first-proxy-hostname&gt;</pre></div></li><li class="step "><p>
    During the replacement of the node there will be alarms that show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></div><div class="sect4" id="replace-dedicated-lm"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Standalone Controller Node</span> <a title="Permalink" class="permalink" href="#replace-dedicated-lm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-replace_dedicated_lm.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_dedicated_lm.xml</li><li><span class="ds-label">ID: </span>replace-dedicated-lm</li></ul></div></div></div></div><p>
  If the controller node you need to replace is not also being used as the
  Cloud Lifecycle Manager, follow the steps below.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Update your cloud model, specifically the <code class="literal">servers.yml</code>
    file, with the new <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields
    where these have changed. Do not change the <code class="literal">id</code>,
    <code class="literal">ip-addr</code>, <code class="literal">role</code>, or
    <code class="literal">server-group</code> settings.
   </p></li><li class="step "><p>
    Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, as follows:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Remove the old controller node(s) from Cobbler. You can list out the
    systems in Cobbler currently with this command:
   </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div><p>
    and then remove the old controller nodes with this command:
   </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name &lt;node&gt;</pre></div></li><li class="step "><p>
    Remove the SSH key of the old controller node from the known hosts file.
    You will specify the <code class="literal">ip-addr</code> value:
   </p><div class="verbatim-wrap"><pre class="screen">ssh-keygen -f "~/.ssh/known_hosts" -R &lt;ip_addr&gt;</pre></div><p>
    You should see a response similar to this one:
   </p><div class="verbatim-wrap"><pre class="screen">ardana@ardana-cp1-c1-m1-mgmt:~/openstack/ardana/ansible$ ssh-keygen -f "~/.ssh/known_hosts" -R 10.13.111.135
# Host 10.13.111.135 found: line 6 type ECDSA
~/.ssh/known_hosts updated.
Original contents retained as ~/.ssh/known_hosts.old</pre></div></li><li class="step "><p>
    Run the cobbler-deploy playbook to add the new controller node:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
    Image the new node(s) by using the bm-reimage playbook. You will specify
    the name for the node in Cobbler in the command:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node-name&gt;</pre></div><div id="id-1.6.15.4.5.3.8.3.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     You must ensure that the old controller node is powered off before
     completing this step. This is because the new controller node will re-use
     the original IP address.
    </p></div></li><li class="step "><p>
    Configure the necessary keys used for the database etc:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml</pre></div></li><li class="step "><p>
    Run osconfig on the replacement controller node. For example:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;</pre></div></li><li class="step "><p>
    If the controller being replaced is the Swift ring builder (see
    <a class="xref" href="#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a>) you need to restore the Swift ring
    builder files to the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory.
    See <a class="xref" href="#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a> for details.
   </p></li><li class="step "><p>
    Run the ardana-deploy playbook on the replacement controller.
   </p><p>
    If the node being replaced is the Swift ring builder server then you only
    need to use the <code class="literal">--limit</code> switch for that node, otherwise
    you need to specify the hostname of your Swift ringer builder server and
    the hostname of the node being replaced.
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True
--limit=&lt;controller-hostname&gt;,&lt;swift-ring-builder-hostname&gt;</pre></div><div id="id-1.6.15.4.5.3.8.3.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     If you receive a Keystone failure when running this playbook, it is
     likely due to Fernet keys being out of sync. This problem can be corrected
     by running the <code class="filename">keystone-reconfigure.yml</code> playbook to
     re-sync the Fernet keys.
    </p><p>
     In this situation, do not use the <code class="literal">--limit</code> option when
     running <code class="filename">keystone-reconfigure.yml</code>. In order to re-sync
     Fernet keys, all the controller nodes must be in the play.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></div><div id="id-1.6.15.4.5.3.8.3.13.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     If you receive a RabbitMQ failure when running this playbook, review
     <a class="xref" href="#recoverrabbit" title="15.2.1. Understanding and Recovering RabbitMQ after Failure">Section 15.2.1, “Understanding and Recovering RabbitMQ after Failure”</a> for how to resolve the issue and then
     re-run the ardana-deploy playbook.
    </p></div></li><li class="step "><p>
    During the replacement of the node there will be alarms that show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="comp-planned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Compute Maintenance</span> <a title="Permalink" class="permalink" href="#comp-planned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-comp_planned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-comp_planned.xml</li><li><span class="ds-label">ID: </span>comp-planned</li></ul></div></div></div></div><p>
  Planned maintenance tasks for compute nodes.
 </p><div class="sect3" id="planned-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Maintenance for a Compute Node</span> <a title="Permalink" class="permalink" href="#planned-computenode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-planned_computenode.xml</li><li><span class="ds-label">ID: </span>planned-computenode</li></ul></div></div></div></div><p>
  If one or more of your compute nodes needs hardware maintenance and you can
  schedule a planned maintenance then this procedure should be followed.
 </p><div class="sect4" id="id-1.6.15.4.6.3.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing planned maintenance on a compute node</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.6.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-planned_computenode.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you have planned maintenance to perform on a compute node, you have to
   take it offline, repair it, and restart it. To do so, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-list | grep compute</pre></div><p>
     The following example shows two compute nodes:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova host-list | grep compute
| ardana-cp1-comp0001-mgmt | compute     | AZ1      |
| ardana-cp1-comp0002-mgmt | compute     | AZ2      |</pre></div></li><li class="step "><p>
     Disable provisioning on the compute node, which will prevent additional
     instances from being spawned on it:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "Maintenance mode" &lt;hostname&gt; nova-compute</pre></div><div id="id-1.6.15.4.6.3.3.3.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Make sure you re-enable provisioning after the maintenance is complete
      if you want to continue to be able to spawn instances on the node. You
      can do this with the command:
     </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;hostname&gt; nova-compute</pre></div></div></li><li class="step "><p>
     At this point you have two choices:
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       <span class="bold"><strong>Live migration</strong></span>: This option enables you
       to migrate the instances off the compute node with minimal downtime so
       you can perform the maintenance without risk of losing data.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Stop/start the instances</strong></span>: Issuing
       <code class="literal">nova stop</code> commands to each of the instances will halt
       them. This option lets you do maintenance and then start the instances
       back up, as long as no disk failures occur on the compute node data
       disks. This method involves downtime for the length of the maintenance.
      </p></li></ol></div><p>
     If you choose the live migration route, See
     <a class="xref" href="#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a> for more details. Skip to step #6
     after you finish live migration.
    </p><p>
     If you choose the stop start method, continue on.
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       List all of the instances on the node so you can issue stop commands to
       them:
      </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
       Issue the <code class="literal">nova stop</code> command against each of the
       instances:
      </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance uuid&gt;</pre></div></li><li class="listitem "><p>
       Confirm that the instances are stopped. If stoppage was successful you
       should see the instances in a <code class="literal">SHUTOFF</code> state, as shown
       here:
      </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status  | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>SHUTOFF</strong></span> | -          | Shutdown    | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+</pre></div></li><li class="listitem "><p>
       Do your required maintenance. If this maintenance does not take down the
       disks completely then you should be able to list the instances again
       after the repair and confirm that they are still in their
       <code class="literal">SHUTOFF</code> state:
      </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
       Start the instances back up using this command:
      </p><div class="verbatim-wrap"><pre class="screen">nova start &lt;instance uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ nova start ef31c453-f046-4355-9bd3-11e774b1772f
Request to start server ef31c453-f046-4355-9bd3-11e774b1772f has been accepted.</pre></div></li><li class="listitem "><p>
       Confirm that the instances started back up. If restarting is successful
       you should see the instances in an <code class="literal">ACTIVE</code> state, as
       shown here:
      </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>ACTIVE</strong></span> | -          | Running     | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="listitem "><p>
       If the <code class="literal">nova start</code> fails, you can try doing a hard
       reboot:
      </p><div class="verbatim-wrap"><pre class="screen">nova reboot --hard &lt;instance uuid&gt;</pre></div><p>
       If this does not resolve the issue you may want to contact support.
      </p></li></ol></div></li><li class="step "><p>
     Reenable provisioning when the node is fixed:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;hostname&gt; nova-compute</pre></div></li></ol></div></div></div></div><div class="sect3" id="reboot-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting a Compute Node</span> <a title="Permalink" class="permalink" href="#reboot-computenode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-reboot_computenode.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-reboot_computenode.xml</li><li><span class="ds-label">ID: </span>reboot-computenode</li></ul></div></div></div></div><p>
  If all you need to do is reboot a Compute node, the following steps can be
  used.
 </p><p>
  You can choose to live migrate all Compute instances off the node prior to
  the reboot. Any instances that remain will be restarted when the node is
  rebooted. This playbook will ensure that all services on the Compute node are
  restarted properly.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Reboot the Compute node(s) with the following playbook.
   </p><p>
    You can specify either single or multiple Compute nodes using the
    <code class="literal">--limit</code> switch.
   </p><p>
    An optional reboot wait time can also be specified. If no reboot wait time
    is specified it will default to 300 seconds.
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-compute-reboot.yml --limit [compute_node_or_list] [-e nova_reboot_wait_timeout=(seconds)]</pre></div><div id="id-1.6.15.4.6.4.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     If the Compute node fails to reboot, you should troubleshoot this issue
     separately as this playbook will not attempt to recover after a failed
     reboot.
    </p></div></li></ol></div></div></div><div class="sect3" id="liveInstMigration"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Live Migration of Instances</span> <a title="Permalink" class="permalink" href="#liveInstMigration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>liveInstMigration</li></ul></div></div></div></div><p>
  Live migration allows you to move active compute instances between compute
  nodes, allowing for less downtime during maintenance.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Nova offers a set of commands that allow you to move compute
  instances between compute hosts. Which command you use will depend on the
  state of the host, what operating system is on the host, what type of storage
  the instances are using, and whether you want to migrate a single instance or
  all of the instances off of the host. We will describe these options on this
  page as well as give you step-by-step instructions for performing them.
 </p><div class="sect4" id="options"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migration Options</span> <a title="Permalink" class="permalink" href="#options">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>options</li></ul></div></div></div></div><p>
   <span class="bold"><strong>If your compute node has failed</strong></span>
  </p><p>
   A compute host failure could be caused by hardware failure, such as the data
   disk needing to be replaced, power has been lost, or any other type of
   failure which requires that you replace the baremetal host. In this
   scenario, the instances on the compute node are unrecoverable and any data
   on the local ephemeral storage is lost. If you are utilizing block storage
   volumes, either as a boot device or as additional storage, they should be
   unaffected.
  </p><p>
   In these cases you will want to use one of the Nova evacuate commands, which
   will cause Nova to rebuild the instances on other hosts.
  </p><p>
   This table describes each of the evacuate options for failed compute nodes:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">nova evacuate &lt;instance&gt; &lt;hostname&gt;</code>
       </p>
      </td><td>
       <p>
        This command is used to evacuate a single instance from a failed host.
        You specify the compute instance UUID and the target host you want to
        evacuate it to. If no host is specified then the Nova scheduler will
        choose one for you.
       </p>
       <p>
        See <code class="literal">nova help evacuate</code> for more information and
        syntax. Further details can also be seen in the OpenStack documentation
        at
        <a class="link" href="http://docs.openstack.org/admin-guide/cli_nova_evacuate.html" target="_blank">http://docs.openstack.org/admin-guide/cli_nova_evacuate.html</a>.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate &lt;hostname&gt; --target_host
        &lt;target_hostname&gt;</code>
       </p>
      </td><td>
       <p>
        This command is used to evacuate all instances from a failed host. You
        specify the hostname of the compute host you want to evacuate.
        Optionally you can specify a target host. If no target host is
        specified then the Nova scheduler will choose a target host for each
        instance.
       </p>
       <p>
        See <code class="literal">nova help host-evacuate</code> for more information and
        syntax.
       </p>
      </td></tr></tbody></table></div><p>
   If your compute host is active, powered on and the data disks are in working order
   you can utilize the migration commands to migrate your compute instances.
   There are two migration features, "cold" migration (also referred to simply
   as "migration") and live migration. Migration and live migration are two
   different functions.
  </p><p>
   <span class="bold"><strong>Cold migration</strong></span> is used to copy an instances
   data in a <code class="literal">SHUTOFF</code> status from one compute host to
   another. It does this using passwordless SSH access which has security
   concerns associated with it. For this reason, the <code class="literal">nova
   migrate</code> function has been disabled by default but you have the
   ability to enable this feature if you would like. Details on how to do this
   can be found in <a class="xref" href="#enabling-the-nova-resize" title="5.4. Enabling the Nova Resize and Migrate Features">Section 5.4, “Enabling the Nova Resize and Migrate Features”</a>.
  </p><p>
   <span class="bold"><strong>Live migration</strong></span> can be performed on
   instances in either an <code class="literal">ACTIVE</code> or
   <code class="literal">PAUSED</code> state and uses the QEMU hypervisor to manage the
   copy of the running processes and associated resources to the destination
   compute host using the hypervisors own protocol and thus is a more secure
   method and allows for less downtime. There may be a short network outage,
   usually a few milliseconds but could be up to a few seconds if your compute
   instances are busy, during a live migration. Also there may be some
   performance degredation during the process.
  </p><p>
   The compute host must remain powered on during the migration process.
  </p><p>
   Both the cold migration and live migration options will honor Nova group
   policies, which includes affinity settings. There is a limitation to keep in
   mind if you use group policies and that is discussed in the
   <a class="xref" href="#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a> section.
  </p><p>
   This table describes each of the migration options for active compute nodes:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Command</th><th>Description</th><th>SLES</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">nova migrate &lt;instance_uuid&gt;</code>
       </p>
      </td><td>
       <p>
        Used to cold migrate a single instance from a compute host. The
        <code class="literal">nova-scheduler</code> will choose the new host.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td> </td></tr><tr><td>
       <p>
        <code class="literal">nova host-servers-migrate &lt;hostname&gt;</code>
       </p>
      </td><td>
       <p>
        Used to cold migrate all instances off a specified host to other
        available hosts, chosen by the <code class="literal">nova-scheduler</code>.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td> </td></tr><tr><td>
       <p>
        <code class="literal">nova live-migration &lt;instance_uuid&gt; [&lt;target
        host&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova live-migration --block-migrate &lt;instance_uuid&gt;
        [&lt;target host&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate-live &lt;hostname&gt; [--target-host
        &lt;target_hostname&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate-live --block-migrate &lt;hostname&gt;
        [--target-host &lt;target_hostname&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr></tbody></table></div></div><div class="sect4" id="idg-all-operations-maintenance-live-migration-xml-10"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations of these Features</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-live-migration-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-live-migration-xml-10</li></ul></div></div></div></div><p>
   There are limitations that may impact your use of this feature:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     To use live migration, your compute instances must be in either an
     <code class="literal">ACTIVE</code> or <code class="literal">PAUSED</code> state on the
     compute host. If you have instances in a <code class="literal">SHUTOFF</code> state
     then cold migration should be used.
    </p></li><li class="listitem "><p>
     Instances in a <code class="literal">Paused</code> state cannot be live migrated
     using the Horizon dashboard. You will need to utilize the NovaClient CLI
     to perform these.
    </p></li><li class="listitem "><p>
     Both cold migration and live migration honor an instance's group policies.
     If you are utilizing an affinity policy and are migrating multiple
     instances you may run into an error stating no hosts are available to
     migrate to. To work around this issue you should specify a target host
     when migrating these instances, which will bypass the
     <code class="literal">nova-scheduler</code>. You should ensure that the target host
     you choose has the resources available to host the instances.
    </p></li><li class="listitem "><p>
     The <code class="literal">nova host-evacuate-live</code> command will produce an
     error if you have a compute host that has a mix of instances that use
     local ephemeral storage and instances that are booted from a block storage
     volume or have any number of block storage volumes attached. If you have a
     mix of these instance types, you may need to run the command twice,
     utilizing the <code class="literal">--block-migrate</code> option. This is described
     in further detail in <a class="xref" href="#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a>.
    </p></li><li class="listitem "><p>
     Instances on KVM hosts can only be live migrated to other KVM hosts.
    </p></li><li class="listitem "><p>
     The migration options described in this document are not available on ESX
     compute hosts.
    </p></li><li class="listitem "><p>
     Ensure that you read and take into account any other limitations that
     exist in the release notes. See the release notes for
     more details.
    </p></li></ul></div></div><div class="sect4" id="liveMigrate"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing a Live Migration</span> <a title="Permalink" class="permalink" href="#liveMigrate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>liveMigrate</li></ul></div></div></div></div><p>
   Cloud administrators can perform a migration on an instance using either the
   Horizon dashboard, API, or CLI. Instances in a <code class="literal">Paused</code>
   state cannot be live migrated using the Horizon GUI. You will need to
   utilize the CLI to perform these.
  </p><p>
   We have documented different scenarios:
  </p></div><div class="sect4" id="failed-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating instances off of a failed compute host</span> <a title="Permalink" class="permalink" href="#failed-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>failed-host</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     If the compute node is not already powered off, do so with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.6.15.4.6.5.7.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The value for <code class="literal">&lt;node_name&gt;</code> will be the name that
      Cobbler has when you run <code class="command">sudo cobbler system list</code> from
      the Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     Source the admin credentials necessary to run administrative commands
     against the Nova API:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Force the <code class="literal">nova-compute</code> service to go down on the
     compute node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-force-down <em class="replaceable ">HOSTNAME</em> nova-compute</pre></div><div id="id-1.6.15.4.6.5.7.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The value for <em class="replaceable ">HOSTNAME</em> can be obtained by
      using <code class="command">nova host-list</code> from the Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     Evacuate the instances off of the failed compute node. This will cause the
     nova-scheduler to rebuild the instances on other valid hosts. Any local
     ephemeral data on the instances is lost.
    </p><p>
     For single instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova evacuate &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
     For all instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate &lt;hostname&gt; [--target_host &lt;target_hostname&gt;]</pre></div></li><li class="step "><p>
     When you have repaired the failed node and start it back up again, when
     the <code class="command">nova-compute</code> process starts again, it will clean
     up the evacuated instances.
    </p></li></ol></div></div></div><div class="sect4" id="active-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating instances off of an active compute host</span> <a title="Permalink" class="permalink" href="#active-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>active-host</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Migrating instances using the Horizon
   dashboard</strong></span>
  </p><p>
   The Horizon dashboard offers a GUI method for performing live migrations.
   Instances in a <code class="literal">Paused</code> state will not provide you the live
   migration option in Horizon so you will need to use the CLI instructions in
   the next section to perform these.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Horizon dashboard with admin credentials.
    </p></li><li class="step "><p>
     Navigate to the menu <span class="guimenu ">Admin</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Instances</span>.
    </p></li><li class="step "><p>
     Next to the instance you want to migrate, select the drop down menu and
     choose the <span class="guimenu ">Live Migrate Instance</span> option.
    </p></li><li class="step "><p>
     In the Live Migrate wizard you will see the compute host the instance
     currently resides on and then a drop down menu that allows you to choose
     the compute host you want to migrate the instance to. Select a destination
     host from that menu. You also have two checkboxes for additional options,
     which are described below:
    </p><p>
     <span class="guimenu ">Disk Over Commit</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will allow you to override the check that occurs to ensure the
     destination host has the available disk space to host the instance.
    </p><p>
     <span class="guimenu ">Block Migration</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will migrate the local disks by using block migration. Use this
     option if you are only using ephemeral storage on your instances. If you
     are using block storage for your instance then ensure this box is not
     checked.
    </p></li><li class="step "><p>
     To begin the live migration, click <span class="guimenu ">Submit</span>.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Migrating instances using the NovaClient
   CLI</strong></span>
  </p><p>
   To perform migrations from the command-line, use the NovaClient.
   The Cloud Lifecycle Manager node in your cloud environment should have
   the NovaClient already installed. If you will be accessing your environment
   through a different method, ensure that the NovaClient is
   installed. You can do so using Python's <code class="command">pip</code> package
   manager.
   
   
  </p><p>
   To run the commands in the steps below, you need administrator
   credentials. From the Cloud Lifecycle Manager, you can source the
   <code class="filename">service.osrc</code> file which is provided that has the
   necessary credentials:
  </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div><p>
   Here are the steps to perform:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Identify the instances on the compute node you wish to migrate:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --all-tenants --host &lt;hostname&gt;</pre></div><p>
     Example showing a host with a single compute instance on it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> nova list --host ardana-cp1-comp0001-mgmt --all-tenants
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| 553ba508-2d75-4513-b69a-f6a2a08d04e3 | test | 193548a949c146dfa1f051088e141f0b | ACTIVE | -          | Running     | adminnetwork=10.0.0.5 |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="step "><p>
     When using live migration you can either specify a target host that the
     instance will be migrated to or you can omit the target to allow the
     nova-scheduler to choose a node for you. If you want to get a list of
     available hosts you can use this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-list</pre></div></li><li class="step "><p>
     Migrate the instance(s) on the compute node using the notes below.
    </p><p>
     If your instance is booted from a block storage volume or has any number
     of block storage volumes attached, use the <code class="literal">nova
     live-migration</code> command with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only or if your instance
     has a mix of ephemeral disk(s) and block storage volume(s), you should use
     the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><div id="id-1.6.15.4.6.5.8.10.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">[&lt;target compute host&gt;]</code> option is
      optional. If you do not specify a target host then the nova scheduler
      will choose a node for you.
     </p></div><p>
     <span class="bold"><strong>Multiple instances</strong></span>
    </p><p>
     If you want to live migrate all of the instances off a single compute host
     you can utilize the <code class="literal">nova host-evacuate-live</code> command.
    </p><p>
     Issue the host-evacuate-live command, which will begin the live migration
     process.
    </p><p>
     If all of the instances on the host are using at least one local
     (ephemeral) disk, you should use this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live --block-migrate &lt;hostname&gt;</pre></div><p>
     Alternatively, if all of the instances are only using block storage
     volumes then omit the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt;</pre></div><div id="id-1.6.15.4.6.5.8.10.4.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can either let the nova-scheduler choose a suitable target host or
      you can specify one using the
      <code class="literal">--target-host &lt;hostname&gt;</code> switch. See
      <code class="command">nova help host-evacuate-live</code> for details.
     </p></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-live-migration-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting migration or host evacuate issues</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-live-migration-xml-14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-live-migration-xml-14</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                                                                                        |
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 95a7ded8-ebfc-4848-9090-2df378c88a4c | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-9fd79670-a780-40ed-a515-c14e28e0a0a7)     |
| 13ab4ef7-0623-4d00-bb5a-5bb2f1214be4 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration cannot be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-26834267-c3ec-4f8b-83cc-5193d6a394d6)     |
+--------------------------------------+-------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from local storage and
   you are not specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live <span class="bold"><strong>--block-migrate</strong></span> &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live --block-migrate ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| e9874122-c5dc-406f-9039-217d9258c020 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-60b1196e-84a0-4b71-9e49-96d6f1358e1a)     |
| 84a02b42-9527-47ac-bed9-8fde1f98e3fe | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-0cdf1198-5dbd-40f4-9e0c-e94aa1065112)     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from a block storage
   volume and you are specifying <code class="literal">--block-migrate</code> in your
   command. Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration 2a13ffe6-e269-4d75-8e46-624fec7a5da0 ardana-cp1-comp0002-mgmt
ERROR (BadRequest): ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-158dd415-0bb7-4613-8529-6689265387e7)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from local storage and you are not
   specifying <code class="literal">--block-migrate</code> in your command. Re-attempt
   the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration <span class="bold"><strong>--block-migrate</strong></span> &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration --block-migrate 84a02b42-9527-47ac-bed9-8fde1f98e3fe ardana-cp1-comp0001-mgmt
ERROR (BadRequest): ardana-cp1-comp0002-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-51fee8d6-6561-4afc-b0c9-7afa7dc43a5b)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from a block storage volume and you
   are specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div></div></div><div class="sect3" id="adding-compute-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Compute Node</span> <a title="Permalink" class="permalink" href="#adding-compute-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-adding_compute_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-adding_compute_nodes.xml</li><li><span class="ds-label">ID: </span>adding-compute-nodes</li></ul></div></div></div></div><p>
  Adding a Compute Node allows you to add capacity.
 </p><div class="sect4" id="add-sles-compute"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a SLES Compute Node</span> <a title="Permalink" class="permalink" href="#add-sles-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>add-sles-compute</li></ul></div></div></div></div><p>
  Adding a SLES compute node allows you to add additional capacity for more
  virtual machines.
 </p><p>
  You may have a need to add additional SLES compute hosts for more virtual
  machine capacity or another purpose and these steps will help you achieve
  this.
 </p><p>
  There are two methods you can use to add SLES compute hosts to your
  environment:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Adding SLES pre-installed compute hosts. This method does not require the
    SLES ISO be on the Cloud Lifecycle Manager to complete.
   </p></li><li class="listitem "><p>
    Using the provided Ansible playbooks and Cobbler, SLES will be installed on
    your new compute hosts. This method requires that you provided a SUSE Linux Enterprise Server 12 SP3
    ISO during the initial installation of your cloud, following the
    instructions at <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”, Section 19.1 “SLES Compute Node Installation Overview”</span>.
   </p><p>
    If you want to use the provided Ansible playbooks and Cobbler to setup and
    configure your SLES hosts and you did not have the SUSE Linux Enterprise Server 12 SP3 ISO on your
    Cloud Lifecycle Manager during your initial installation then ensure you look at
    the note at the top of that section before proceeding.
   </p></li></ol></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.3.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-add-sles-compute-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-5</li></ul></div></div></div></div><p>
   You need to ensure your input model files are properly setup for SLES
   compute host clusters. This must be done during the installation process of
   your cloud and is discussed further at <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”, Section 19.3 “Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes”</span> and
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
  </p></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.3.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a SLES compute node</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-add-sles-compute-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-6</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Adding pre-installed SLES compute hosts</strong></span>
  </p><p>
   This method requires that you have SUSE Linux Enterprise Server 12 SP3 pre-installed on the
   baremetal host prior to beginning these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure you have SUSE Linux Enterprise Server 12 SP3 pre-installed on your baremetal host.
    </p></li><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in the
     format. Note that we left out the IPMI details because they will not be
     needed since you pre-installed the SLES OS on your host(s).
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.6.15.4.6.6.3.7.4.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem "><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See for <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> more details.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation.
    </p><div id="id-1.6.15.4.6.6.3.7.4.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="filename">wipe_disks.yml</code> playbook is only meant to be run
      on systems immediately after running
      <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
      not wipe all of the expected partitions.
     </p></div><p>
     The location of <code class="literal">hostname</code> is
     <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts</code>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the compute host deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Adding SLES compute hosts with Ansible playbooks and
   Cobbler</strong></span>
  </p><p>
   These steps will show you how to add the new SLES compute host to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><p>
   If you did not have the SUSE Linux Enterprise Server 12 SP3 ISO available on your Cloud Lifecycle Manager
   during your initial installation, it must be installed before proceeding
   further. Instructions can be found in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”</span>.
  </p><p>
   When you are prepared to continue, use these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="step "><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in this
     format:
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1
  mac-addr: e8:39:35:21:32:4e
  ilo-ip: 10.1.192.36
  ilo-password: password
  ilo-user: admin
  distro-id: sles12sp3-x86_64</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.6.15.4.6.6.3.7.9.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="step "><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     The following playbook confirms that your servers are accessible over their IPMI ports.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml -e nodelist=compute4</pre></div></li><li class="step "><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
     Run the following playbook, ensuring that you specify only your UEFI
     SLES nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div><div id="id-1.6.15.4.6.6.3.7.9.10.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
       If you do not know the <code class="literal">&lt;node name&gt;</code>, you can
       get it by using <code class="command">sudo cobbler system list</code>.
      </p></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your hosts are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div><div id="id-1.6.15.4.6.6.3.7.9.12.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can obtain the <code class="literal">&lt;hostname&gt;</code> from the file
      <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.
     </p></div></li><li class="step "><p>
     You should verify that the netmask, bootproto, and other necessary
     settings are correct and if they are not then re-do them. See
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”</span> for details.
    </p></li><li class="step "><p>
     Complete the compute host deployment with these playbooks. For the last
     one, ensure you specify the compute hosts you are added with the
     <code class="literal">--limit</code> switch:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-8"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.3.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a new SLES compute node to monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-add-sles-compute-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-8</li></ul></div></div></div></div><p>
   If you want to add a new Compute node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></div></div></div><div class="sect3" id="remove-compute-node"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Compute Node</span> <a title="Permalink" class="permalink" href="#remove-compute-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-compute-node</li></ul></div></div></div></div><p>
  Removing a Compute node allows you to remove capacity.
 </p><p>
  You may have a need to remove a Compute node and these steps will help you
  achieve this.
 </p><div class="sect4" id="disable-provisioning"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable Provisioning on the Compute Host</span> <a title="Permalink" class="permalink" href="#disable-provisioning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>disable-provisioning</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of the Nova services running which will provide us with the
     details we need to disable the provisioning on the Compute host you are
     wanting to remove:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -               |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -               |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -               |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | -               |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</pre></div></li><li class="step "><p>
     Disable the Nova service on the Compute node you are wanting to remove
     which will ensure it is taken out of the scheduling rotation:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "&lt;enter reason here&gt;" &lt;node hostname&gt; nova-compute</pre></div><p>
     Here is an example if I wanted to remove the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> in the output above:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-disable --reason "hardware reallocation" ardana-cp1-comp0002-mgmt nova-compute
+--------------------------+--------------+----------+-----------------------+
| Host                     | Binary       | Status   | Disabled Reason       |
+--------------------------+--------------+----------+-----------------------+
| ardana-cp1-comp0002-mgmt | nova-compute | disabled | hardware reallocation |
+--------------------------+--------------+----------+-----------------------+</pre></div></li></ol></div></div></div><div class="sect4" id="remove-az"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from its Availability Zone</span> <a title="Permalink" class="permalink" href="#remove-az">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-az</li></ul></div></div></div></div><p>
   If you configured the Compute host to be part of an availability zone, these
   steps will show you how to remove it.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of the Nova services running which will provide us with the
     details we need to remove a Compute node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+</pre></div></li><li class="step "><p>
     You can remove the Compute host from the availability zone it was a part
     of with this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova aggregate-remove-host &lt;availability zone&gt; &lt;nova hostname&gt;</pre></div><p>
     So for the same example as the previous step, the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> host was in the
     <code class="literal">AZ2</code> availability zone so I would use this command to
     remove it:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova aggregate-remove-host AZ2 ardana-cp1-comp0002-mgmt
Host ardana-cp1-comp0002-mgmt has been successfully removed from aggregate 4
+----+------+-------------------+-------+-------------------------+
| Id | Name | Availability Zone | Hosts | Metadata                |
+----+------+-------------------+-------+-------------------------+
| 4  | AZ2  | AZ2               |       | 'availability_zone=AZ2' |
+----+------+-------------------+-------+-------------------------+</pre></div></li><li class="step "><p>
     You can confirm the last two steps completed successfully by running
     another <code class="literal">nova service-list</code>.
    </p><p>
     Here is an example which confirms that the node has been disabled and that
     it has been removed from the availability zone. I have highlighted these:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div></li></ol></div></div></div><div class="sect4" id="live-migration"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Live Migration to Move Any Instances on this Host to Other Hosts</span> <a title="Permalink" class="permalink" href="#live-migration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>live-migration</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     You will need to verify if the Compute node is currently hosting any
     instances on it. You can do this with the command below:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host=&lt;nova hostname&gt; --all_tenants=1</pre></div><p>
     Here is an example below which shows that we have a single running
     instance on this node currently:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | ACTIVE | -          | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+</pre></div></li><li class="step "><p>
     You will likely want to migrate this instance off of this node before
     removing it. You can do this with the live migration functionality within
     Nova. The command will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;nova instance ID&gt;</pre></div><p>
     Here is an example using the instance in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration --block-migrate 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9</pre></div><p>
     You can check the status of the migration using the same command from the
     previous step:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status    | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | MIGRATING | migrating  | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+</pre></div></li><li class="step "><p>
     Run nova list again
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1</pre></div><p>
     to see that the running instance has been migrated:
    </p><div class="verbatim-wrap"><pre class="screen">+----+------+-----------+--------+------------+-------------+----------+
| ID | Name | Tenant ID | Status | Task State | Power State | Networks |
+----+------+-----------+--------+------------+-------------+----------+
+----+------+-----------+--------+------------+-------------+----------+</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.4.6.7.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable Neutron Agents on Node to be Removed</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.6.7.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You should also locate and disable or remove neutron agents. To see the
   neutron agents running:
  </p><div class="verbatim-wrap"><pre class="screen">$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-update --admin-state-down 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-update --admin-state-down dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-update --admin-state-down f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></div><div class="sect4" id="shutdown-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Shut down or Stop the Nova and Neutron Services on the Compute Host</span> <a title="Permalink" class="permalink" href="#shutdown-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>shutdown-node</li></ul></div></div></div></div><p>
   To perform this step you have a few options. You can SSH into the Compute
   host and run the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop nova-compute</pre></div><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop neutron-*</pre></div><p>
   Because the Neutron agent self-registers against Neutron server, you may
   want to prevent the following services from coming back online. Here is how
   you can get the list:
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl list-units neutron-* --all</pre></div><p>
   Here are the results:
  </p><div class="verbatim-wrap"><pre class="screen">UNIT                                  LOAD        ACTIVE     SUB      DESCRIPTION
neutron-common-rundir.service         loaded      inactive   dead     Create /var/run/neutron
•neutron-dhcp-agent.service         not-found     inactive   dead     neutron-dhcp-agent.service
neutron-l3-agent.service              loaded      inactive   dead     neutron-l3-agent Service
neutron-lbaasv2-agent.service         loaded      inactive   dead     neutron-lbaasv2-agent Service
neutron-metadata-agent.service        loaded      inactive   dead     neutron-metadata-agent Service
•neutron-openvswitch-agent.service    loaded      failed     failed   neutron-openvswitch-agent Service
neutron-ovs-cleanup.service           loaded      inactive   dead     Neutron OVS Cleanup Service

        LOAD   = Reflects whether the unit definition was properly loaded.
        ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
        SUB    = The low-level unit activation state, values depend on unit type.

        7 loaded units listed.
        To show all installed unit files use 'systemctl list-unit-files'.</pre></div><p>
   For each loaded service issue the command
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl disable &lt;service-name&gt;</pre></div><p>
   In the above example that would be each service, <span class="emphasis"><em>except neutron-dhcp-agent.service
   </em></span>
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl disable neutron-common-rundir neutron-l3-agent neutron-lbaasv2-agent neutron-metadata-agent neutron-openvswitch-agent</pre></div><p>
   Now you can shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen">sudo shutdown now</pre></div><p>
   OR
  </p><p>
   From the Cloud Lifecycle Manager you can use the
   <code class="literal">bm-power-down.yml</code> playbook to shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&lt;node name&gt;</pre></div><div id="id-1.6.15.4.6.7.8.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The <code class="literal">&lt;node name&gt;</code> value will be the value
   corresponding to this node in Cobbler. You can run
   <code class="command">sudo cobbler system list</code> to retrieve these names.
  </p></div></div><div class="sect4" id="delete-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete the Compute Host from Nova</span> <a title="Permalink" class="permalink" href="#delete-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>delete-node</li></ul></div></div></div></div><p>
   Retrieve the list of Nova services:
  </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
   Here is an example highlighting the Compute host we're going to remove:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div><p>
   Delete the host from Nova using the command below:
  </p><div class="verbatim-wrap"><pre class="screen">nova service-delete &lt;service ID&gt;</pre></div><p>
   Following our example above, you would use:
  </p><div class="verbatim-wrap"><pre class="screen">nova service-delete 37</pre></div><p>
   Use the command below to confirm that the Compute host has been completely
   removed from Nova:
  </p><div class="verbatim-wrap"><pre class="screen">nova hypervisor-list</pre></div></div><div class="sect4" id="deletefromneutron"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete the Compute Host from Neutron</span> <a title="Permalink" class="permalink" href="#deletefromneutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>deletefromneutron</li></ul></div></div></div></div><p>
   Multiple Neutron agents are running on the compute node. You have to remove
   all of the agents running on the node using the "neutron agent-delete"
   command. In the example below, the l3-agent, openvswitch-agent and
   metadata-agent are running:
  </p><div class="verbatim-wrap"><pre class="screen">$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-delete AGENT_ID

$ neutron agent-delete 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-delete dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-delete f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></div><div class="sect4" id="remove-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from the servers.yml File and Run the Configuration Processor</span> <a title="Permalink" class="permalink" href="#remove-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-node</li></ul></div></div></div></div><p>
   Complete these steps from the Cloud Lifecycle Manager to remove the Compute node:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="step "><p>
     Edit your <code class="literal">servers.yml</code> file in the location below to
     remove references to the Compute node(s) you want to remove:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div></li><li class="step "><p>
     You may also need to edit your <code class="literal">control_plane.yml</code> file
     to update the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code> if you used
     those to ensure they reflect the proper number of nodes you are using.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "Remove node &lt;name&gt;"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     To free up the resources when running the configuration processor, use
     the switches <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code>. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div></div><div class="sect4" id="remove-cobbler"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from Cobbler</span> <a title="Permalink" class="permalink" href="#remove-cobbler">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-cobbler</li></ul></div></div></div></div><p>
   Complete these steps to remove the node from Cobbler:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Confirm the system name in Cobbler with this command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="step "><p>
     Remove the system from Cobbler using this command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name=&lt;node&gt;</pre></div></li><li class="step "><p>
     Run the <code class="literal">cobbler-deploy.yml</code> playbook to complete the
     process:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-compute-remove-compute-node-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from Monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-remove-compute-node-xml-14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-remove-compute-node-xml-14</li></ul></div></div></div></div><p>
   Once you have removed the Compute nodes, the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
    To find all Monasca API servers 
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cat /etc/haproxy/haproxy.cfg | grep MON
listen ardana-cp1-vip-public-MON-API-extapi-8070
    bind ardana-cp1-vip-public-MON-API-extapi:8070  ssl crt /etc/ssl/private//my-public-cert-entry-scale                                          
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5        
listen ardana-cp1-vip-MON-API-mgmt-8070
    bind ardana-cp1-vip-MON-API-mgmt:8070  ssl crt /etc/ssl/private//ardana-internal-cert                                          
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5</pre></div><p>In above example <code class="literal">ardana-cp1-c1-m1-mgmt</code>,<code class="literal">ardana-cp1-c1-m2-mgmt</code>,
  <code class="literal">ardana-cp1-c1-m3-mgmt</code> are Monasa API servers</p><p>
   You will want to SSH to each of the Monasca API servers and edit the
   <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to remove
   references to the Compute node you removed. This will require
   <code class="literal">sudo</code> access. The entries will look similar to the one
   below:
  </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: ardana-cp1-comp0001-mgmt
  name: ardana-cp1-comp0001-mgmt ping</pre></div><p>
   Once you have removed the references on each of your Monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the Compute node references removed and the monasca-agent restarted,
   you can then delete the corresponding alarm to finish this process. To do so
   we recommend using the Monasca CLI which should be installed on each of your
   Monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=&lt;compute node deleted&gt;</pre></div><p>
   For example, if your Compute node looked like the example above then you
   would use this command to get the alarm ID:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div></div></div></div><div class="sect2" id="planned-maintenance-task-for-networking-nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Network Maintenance</span> <a title="Permalink" class="permalink" href="#planned-maintenance-task-for-networking-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-networking_nodes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking_nodes.xml</li><li><span class="ds-label">ID: </span>planned-maintenance-task-for-networking-nodes</li></ul></div></div></div></div><p>
  Planned maintenance task for networking nodes.
 </p><div class="sect3" id="add-network-node"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Neutron Network Node</span> <a title="Permalink" class="permalink" href="#add-network-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>add-network-node</li></ul></div></div></div></div><p>
  Adding an additional Neutron networking node allows you to increase the
  performance of your cloud.
 </p><p>
  You may have a need to add an additional Neutron network node for increased
  performance or another purpose and these steps will help you achieve this.
 </p><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-networking-add-network-node-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-6</li></ul></div></div></div></div><p>
   If you are using the mid-scale model then your networking nodes are already
   separate and the roles are defined. If you are not already using this model
   and wish to add separate networking nodes then you need to ensure that those
   roles are defined. You can look in the <code class="literal">~/openstack/examples</code>
   folder on your Cloud Lifecycle Manager for the mid-scale example model files which
   show how to do this. We have also added the basic edits that need to be made
   below:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In your <code class="literal">server_roles.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/server_roles.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-ROLE
  interface-model: NEUTRON-INTERFACES
  disk-model: NEUTRON-DISKS</pre></div></li><li class="listitem "><p>
     In your <code class="literal">net_interfaces.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-INTERFACES</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/net_interfaces.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-INTERFACES
  network-interfaces:
  - device:
      name: hed3
    name: hed3
    network-groups:
    - EXTERNAL-VM
    - GUEST
    - MANAGEMENT</pre></div></li><li class="listitem "><p>
     Create a <code class="literal">disks_neutron.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-DISKS</code> defined in it.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_neutron.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  product:
    version: 2

  disk-models:
  - name: NEUTRON-DISKS
    volume-groups:
      - name: ardana-vg
        physical-volumes:
         - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></li><li class="listitem "><p>
     Modify your <code class="literal">control_plane.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined as well as the Neutron services
     added.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/control_plane.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  - allocation-policy: strict
    cluster-prefix: neut
    member-count: 1
    name: neut
    server-role: NEUTRON-ROLE
    service-components:
    - ntp-client
    - neutron-vpn-agent
    - neutron-dhcp-agent
    - neutron-metadata-agent
    - neutron-openvswitch-agent</pre></div></li></ol></div><p>
   You should also have one or more baremetal servers that meet the minimum
   hardware requirements for a network node which are documented in the
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span>.
  </p></div><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a network node</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-networking-add-network-node-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-7</li></ul></div></div></div></div><p>
   These steps will show you how to add the new network node to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="listitem "><p>
     In the same directory, edit your <code class="literal">servers.yml</code> file to
     include the details about your new network node(s).
    </p><p>
     For example, if you already had a cluster of three network nodes and
     needed to add a fourth one you would add your details to the bottom of the
     file in this format:
    </p><div class="verbatim-wrap"><pre class="screen"># network nodes
- id: neut3
  ip-addr: 10.13.111.137
  role: NEUTRON-ROLE
  server-group: RACK2
  mac-addr: "5c:b9:01:89:b6:18"
  nic-mapping: HP-DL360-6PORT
  ip-addr: 10.243.140.22
  ilo-ip: 10.1.12.91
  ilo-password: password
  ilo-user: admin</pre></div><div id="id-1.6.15.4.7.3.5.3.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this node does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem "><p>
     In your <code class="literal">control_plane.yml</code> file you will need to check
     the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code>, if you
     specified them, to ensure that they match up with your new total node
     count. So for example, if you had previously specified
     <code class="literal">member-count: 3</code> and are adding a fourth network node,
     you will need to change that value to <code class="literal">member-count: 4</code>.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "Add new networking node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;hostname&gt;</pre></div><div id="id-1.6.15.4.7.3.5.3.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      If you do not know the <code class="literal">&lt;hostname&gt;</code>, you can
      get it by using <code class="command">sudo cobbler system list</code>.
     </p></div></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Configure the operating system on the new networking node with this
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the networking node deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">site.yml</code> playbook with the required tag so that
     all other services become aware of the new node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li></ol></div></div><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-8"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New Network Node to Monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-networking-add-network-node-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-8</li></ul></div></div></div></div><p>
   If you want to add a new networking node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></div></div></div><div class="sect2" id="storage-maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Storage Maintenance</span> <a title="Permalink" class="permalink" href="#storage-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-storage_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-storage_maintenance.xml</li><li><span class="ds-label">ID: </span>storage-maintenance</li></ul></div></div></div></div><p>
  Planned maintenance procedures for Swift storage nodes.
 </p><div class="sect3" id="planned-maintenance-for-swift-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Maintenance Tasks for Swift Nodes</span> <a title="Permalink" class="permalink" href="#planned-maintenance-for-swift-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift_nodes.xml</li><li><span class="ds-label">ID: </span>planned-maintenance-for-swift-nodes</li></ul></div></div></div></div><p>
  Planned maintenance tasks including recovering, adding, and removing Swift
  nodes.
 </p><div class="sect4" id="sec-swift-add-object-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Swift Object Node</span> <a title="Permalink" class="permalink" href="#sec-swift-add-object-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_object_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-add-object-node</li></ul></div></div></div></div><p>
  Adding additional object nodes allows you to increase capacity.
 </p><p>
  This topic describes how to add additional Swift object server nodes to an
  existing system.
 </p><div class="sect5" id="id-1.6.15.4.8.3.3.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To add a new node</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.8.3.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_object_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="literal">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="literal">servers.yml
   file</code> by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager node.
    </p></li><li class="listitem "><p>
     Get the <code class="literal">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem "><p>
     If not already done, set the <code class="literal">weight-step</code> attribute. For
     instructions, see <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Add the details of new nodes to the <code class="literal">servers.yml</code> file.
     In the following example only one new server
     <span class="bold"><strong>swobj4</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="literal">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swobj4
  role: SWOBJ_ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div></li><li class="listitem "><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.6.15.4.8.3.3.4.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 18 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Configure Cobbler to include the new node, and then reimage the node (if
     you are adding several nodes, use a comma-separated list with the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swobj4</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj4</pre></div><div id="id-1.6.15.4.8.3.3.4.4.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">server-id</code>.
     </p></div></li><li class="listitem "><p>
     Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     The hostname of the newly added server can be found in the list generated
     from the output of the following command:
    </p><div class="verbatim-wrap"><pre class="screen">grep hostname ~/openstack/my_cloud/info/server_info.yml</pre></div><p>
     For example, for <span class="bold"><strong>swobj4</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-swobj0004-mgmt</strong></span>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-swobj0004-mgmt</pre></div></li><li class="listitem "><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem "><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swobj4</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem "><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml</pre></div></li></ol></div></div></div><div class="sect4" id="adding-proxy"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Swift Proxy, Account, Container (PAC) Node</span> <a title="Permalink" class="permalink" href="#adding-proxy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_pac_node.xml</li><li><span class="ds-label">ID: </span>adding-proxy</li></ul></div></div></div></div><p>
  Steps for adding additional PAC nodes to your Swift system.
 </p><p>
  This topic describes how to add additional Swift proxy, account, and
  container (PAC) servers to an existing system.
 </p><div class="sect5" id="id-1.6.15.4.8.3.4.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a new node</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.8.3.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_pac_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="literal">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="literal">servers.yml
   file</code> by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Get the <code class="literal">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem "><p>
     If not already done, set the weight-step attribute. For instructions, see
     <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Add details of new nodes to the <code class="literal">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swpac6
  role: SWPAC-ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div><p>
     In the above example, only one new server
     <span class="bold"><strong>swpac6</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="literal">servers.yml</code> file.
    </p><p>
     In the entry-scale configurations there is no dedicated Swift PAC cluster.
     Instead, there is a cluster using servers that have a role of
     <code class="literal">CONTROLLER-ROLE</code>. You cannot add
     <code class="literal">swpac4</code> to this cluster because that would change the
     <code class="literal">member-count</code>. If your system does not already have a
     dedicated Swift PAC cluster you will need to add it to the configuration
     files. For details on how to do this, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.7 “Creating a Swift Proxy, Account, and Container (PAC) Cluster”</span>.
    </p><p>
     If using a new PAC nodes you must add the PAC node's configuration details
     in the following yaml files:
    </p><div class="verbatim-wrap"><pre class="screen">control_plane.yml
disks_pac.yml
net_interfaces.yml
servers.yml
server_roles.yml</pre></div><p>
     You can see a good example of this in the example configurations for the
     mid-scale model in the <code class="literal">~/openstack/examples/mid-scale-kvm</code>
     directory.
    </p><p>
     The following steps assume that you have already created a dedicated Swift
     PAC cluster and that it has two members
     (<span class="bold"><strong>swpac4</strong></span> and
     <span class="bold"><strong>swpac5</strong></span>).
    </p></li><li class="listitem "><p>
     Increase the member count of the Swift PAC cluster, as appropriate. For
     example, if you are adding <span class="bold"><strong>swpac6</strong></span> and you
     previously had two Swift PAC nodes, the increased member count should be 3
     as shown in the following example:
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1

  . . .
  clusters:
  . . .
     - name: ....
       cluster-prefix: c2
       server-role: SWPAC-ROLE
       member-count: 3
   . . .</pre></div></li><li class="listitem "><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.6.15.4.8.3.4.4.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 18 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Configure Cobbler to include the new node and reimage the node (if you are
     adding several nodes, use a comma-separated list for the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swpac6</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swpac6</pre></div><div id="id-1.6.15.4.8.3.4.4.4.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">server-id</code>.
     </p></div></li><li class="listitem "><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     For example, for <span class="bold"><strong>swpac6</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-c2-m3-mgmt</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-c2-m3-mgmt</pre></div></li><li class="listitem "><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem "><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swpac6</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem "><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div></div><div class="sect4" id="add-swift-disk"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Additional Disks to a Swift Node</span> <a title="Permalink" class="permalink" href="#add-swift-disk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_disk.xml</li><li><span class="ds-label">ID: </span>add-swift-disk</li></ul></div></div></div></div><p>
  Steps for adding additional disks to any nodes hosting Swift services.
 </p><p>
  You may have a need to add additional disks to a node for Swift usage and we
  can show you how. These steps work for adding additional disks to Swift
  object or proxy, account, container (PAC) nodes. It can also apply to adding
  additional disks to a controller node that is hosting the Swift service, like
  you would see if you are using one of the entry-scale example models.
 </p><p>
  Read through the notes below before beginning the process.
 </p><p>
  You can add multiple disks at the same time, there is no need to do it one at
  a time.
 </p><div id="id-1.6.15.4.8.3.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Add the Same Number of Disks</h6><p>
   You must add the <span class="emphasis"><em>same</em></span> number of disks to each server
   that the disk model applies to. For example, if you have a single cluster of
   three Swift servers and you want to increase capacity and decide to add two
   additional disks, you must add two to each of your three Swift servers.
  </p></div><div class="sect5" id="id-1.6.15.4.8.3.5.7"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding additional disks to your Swift servers</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.8.3.5.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_disk.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Verify the general health of the Swift system and that it is safe to
     rebalance your rings. See <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details
     on how to do this.
    </p></li><li class="step "><p>
     Perform the disk maintenance.
    </p><ol type="a" class="substeps "><li class="step " id="st-swift-add-disk-shutdown"><p>
       Shut down the first Swift server you wish to add disks to.
      </p></li><li class="step "><p>
       Add the additional disks to the physical server. The disk drives that
       are added should be clean. They should either contain no partitions or a
       single partition the size of the entire disk. It should not contain a
       file system or any volume groups. Failure to comply will cause errors
       and the disk will not be added.
      </p><p>
       For more details, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.
      </p></li><li class="step "><p>
       Power the server on.
      </p></li><li class="step "><p>
       While the server was shutdown, data that normally would have been placed
       on the server is placed elsewhere. When the server is rebooted, the
       Swift replication process will move that data back onto the server.
       Monitor the replication process to determine when it is complete. See
       <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details on how to do this.
      </p></li><li class="step "><p>
       Repeat the steps from <a class="xref" href="#st-swift-add-disk-shutdown" title="Step 2.a">Step 2.a</a> for
       each of the Swift servers you are adding the disks to, one at a time.
      </p><div id="id-1.6.15.4.8.3.5.7.2.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         If the additional disks can be added to the Swift servers online
         (for example, via hotplugging) then there is no need to perform the
         last two steps.
         
        </p></div></li></ol></li><li class="step "><p>
     On the Cloud Lifecycle Manager, update your cloud configuration with the details
     of your additional disks.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Edit the disk configuration file that correlates to the type of server
       you are adding your new disks to.
      </p><p>
       Path to the typical disk configuration files:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_swobj.yml
~/openstack/my_cloud/definition/data/disks_swpac.yml
~/openstack/my_cloud/definition/data/disks_controller_*.yml</pre></div><p>
       Example showing the addition of a single new disk, indicated by the
       <code class="literal">/dev/sdd</code>, in bold:
      </p><div class="verbatim-wrap"><pre class="screen">device-groups:
  - name: SwiftObject
    devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"
      <span class="bold"><strong>- name: "/dev/sdd"</strong></span>
    consumer:
      name: swift
      ...</pre></div><div id="id-1.6.15.4.8.3.5.7.2.3.2.1.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         For more details on how the disk model works, see
         <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”</span>.
        </p></div></li><li class="step "><p>
       Configure the Swift weight-step value in the
       <code class="literal">~/openstack/my_cloud/definition/data/swift/rings.yml</code>
       file. See <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for details on how to do
       this.
      </p></li><li class="step "><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "adding additional Swift disks"</pre></div></li><li class="step "><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step "><p>
     Run the <code class="literal">osconfig-run.yml</code> playbook against the Swift
     nodes you have added disks to. Use the <code class="literal">--limit</code> switch
     to target the specific nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostnames&gt;</pre></div><p>
     You can use a wildcard when specifying the hostnames with the
     <code class="literal">--limit</code> switch. If you added disks to all of the Swift
     servers in your environment and they all have the same prefix (for
     example, <code class="literal">ardana-cp1-swobj...</code>) then you can use a
     wildcard like <code class="literal">ardana-cp1-swobj*</code>. If you only added
     disks to a set of nodes but not all of them, you can use a comma
     deliminated list and enter the hostnames of each of the nodes you added
     disks to.
    </p></li><li class="step "><p>
     Validate your Swift configuration with this playbook which will also
     provide details of each drive being added:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div></li><li class="step "><p>
     Verify that Swift services are running on all of your servers:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li><li class="step "><p>
     If everything looks okay with the Swift status, then apply the changes to
     your Swift rings with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="step "><p>
     At this point your Swift rings will begin rebalancing. You should wait
     until replication has completed or min-part-hours has elapsed (whichever
     is longer), as described in <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> and then
     follow the "Weight Change Phase of Ring Rebalance" process as described in
     <a class="xref" href="#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div></div></div><div class="sect4" id="remove-swift-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Swift Node</span> <a title="Permalink" class="permalink" href="#remove-swift-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>remove-swift-node</li></ul></div></div></div></div><p>
  Removal process for both Swift Object and PAC nodes.
 </p><p>
  You can use this process when you want to remove one or more Swift nodes
  permanently. This process applies to both Swift Proxy, Account, Container
  (PAC) nodes and Swift Object nodes.
 </p><div class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting the Pass-through Attributes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-swift-removing-swift-node-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-removing-swift-node-xml-6</li></ul></div></div></div></div><p>
   This process will remove the Swift node's drives from the rings and move it
   to the remaining nodes in your cluster.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Ensure that the weight-step attribute is set. See
     <a class="xref" href="#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for more details.
    </p></li><li class="listitem "><p>
     Add the pass-through definition to your input model, specifying the server
     ID (as opposed to the server name). It is easiest to include in your
     <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file
     since your server IDs are already listed in that file. For more
     information about pass-through, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.17 “Pass Through”</span>.
    </p><p>
     Here is the general format:
    </p><div class="verbatim-wrap"><pre class="screen">pass-through:
  servers:
    - id: &lt;server-id&gt;
      data:
          &lt;subsystem&gt;:
                &lt;subsystem-attributes&gt;</pre></div><p>
     Here is an example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  <span class="bold"><strong>pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            drain: yes</strong></span></pre></div><p>
     By setting this pass-through attribute, you indicate that the system
     should reduce the weight of the server's drives. The weight reduction is
     determined by the weight-step attribute as described in the previous step.
     This process is known as "draining", where you remove the Swift data from
     the node in preparation for removing the node.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Swift deploy playbook to perform the first ring rebuild. This will
     remove some of the partitions from all drives on the node you are
     removing:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until the replication has completed. For further details, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem "><p>
     Determine whether all of the partitions have been removed from all drives
     on the Swift node you are removing. You can do this by SSH'ing into the
     first account server node and using these commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/swiftlm/cloud1/cp1/builder_dir/
sudo swift-ring-builder &lt;ring_name&gt;.builder</pre></div><p>
     For example, if the node you are removing was part of the object-o ring
     the command would be:
    </p><div class="verbatim-wrap"><pre class="screen">sudo swift-ring-builder object-0.builder</pre></div><p>
     Check the output. You will need to know the IP address of the server being
     drained. In the example below, the number of partitions of the drives on
     192.168.245.3 has reached zero for the object-0 ring:
    </p><div class="verbatim-wrap"><pre class="screen">$ cd /etc/swiftlm/cloud1/cp1/builder_dir/
$ sudo swift-ring-builder object-0.builder
account.builder, build version 6
4096 partitions, 3.000000 replicas, 1 regions, 1 zones, 6 devices, 0.00 balance, 0.00 dispersion
The minimum number of hours before a partition can be reassigned is 16
The overload factor is 0.00% (0.000000)
Devices:    id  region  zone      ip address  port  replication ip  replication port      name weight partitions balance meta
             0       1     1   192.168.245.3  6002   192.168.245.3              6002     disk0   0.00          0   -0.00 padawan-ccp-c1-m1:disk0:/dev/sdc
             1       1     1   192.168.245.3  6002   192.168.245.3              6002     disk1   0.00          0   -0.00 padawan-ccp-c1-m1:disk1:/dev/sdd
             2       1     1   192.168.245.4  6002   192.168.245.4              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m2:disk0:/dev/sdc
             3       1     1   192.168.245.4  6002   192.168.245.4              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m2:disk1:/dev/sdd
             4       1     1   192.168.245.5  6002   192.168.245.5              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m3:disk0:/dev/sdc
             5       1     1   192.168.245.5  6002   192.168.245.5              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m3:disk1:/dev/sdd</pre></div></li><li class="listitem "><p>
     If the number of partitions is zero for the server on all rings, you can
     move to the next step, otherwise continue the ring rebalance cycle by
     repeating steps 7-9 until the weight has reached zero.
    </p></li><li class="listitem "><p>
     If the number of partitions is zero for the server on all rings, you can
     remove the Swift nodes' drives from all rings. Edit the pass-through data
     you created in step #3 and set the <code class="literal">remove</code> attribute as
     shown in this example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            <span class="bold"><strong>remove: yes</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Swift deploy playbook to rebuild the rings by removing the server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     At this stage, the server has been removed from the rings and the data
     that was originally stored on the server has been replicated in a balanced
     way to the other servers in the system. You can proceed to the next phase.
    </p></li></ol></div></div><div class="sect5" id="sec-swift-disable-node"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Disable Swift on a Node</span> <a title="Permalink" class="permalink" href="#sec-swift-disable-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-disable-node</li></ul></div></div></div></div><p>
   The next phase in this process will disable the Swift service on the node.
   In this example, <span class="bold"><strong>swobj4</strong></span> is the node being
   removed from Swift.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Stop Swift services on the node using the
     <code class="literal">swift-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong>&lt;hostname&gt;</strong></span></pre></div><div id="id-1.6.15.4.8.3.6.5.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      When using the <code class="literal">--limit</code> argument, you must specify the
      full hostname (for example: <span class="emphasis"><em>ardana-cp1-swobj0004</em></span>) or
      use the wild card <code class="literal">*</code> (for example,
      <span class="emphasis"><em>*swobj4*</em></span>).
     </p></div><p>
     The following example uses the <code class="literal">swift-stop.yml</code> playbook
     to stop Swift services on
     <span class="bold"><strong>ardana-cp1-swobj0004</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong>ardana-cp1-swobj0004</strong></span></pre></div></li><li class="listitem "><p>
     Remove the configuration files.
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-swobj4-mgmt sudo rm -R /etc/swift</pre></div><div id="id-1.6.15.4.8.3.6.5.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Do not run any other playbooks until you have finished the process
      described in <a class="xref" href="#sec-swift-remove-node-model" title="13.1.5.1.4.3. To Remove a Node from the Input Model">Section 13.1.5.1.4.3, “To Remove a Node from the Input Model”</a>. Otherwise,
      these playbooks may recreate <code class="filename">/etc/swift</code> and
      restart Swift on <span class="emphasis"><em>swobj4</em></span>. If you accidentally run a
      playbook, repeat the process in <a class="xref" href="#sec-swift-disable-node" title="13.1.5.1.4.2. To Disable Swift on a Node">Section 13.1.5.1.4.2, “To Disable Swift on a Node”</a>.
     </p></div></li></ol></div></div><div class="sect5" id="sec-swift-remove-node-model"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Remove a Node from the Input Model</span> <a title="Permalink" class="permalink" href="#sec-swift-remove-node-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-remove-node-model</li></ul></div></div></div></div><p>
   Use the following steps to finish the process of removing the Swift node.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file and remove the entry for the node
     (<span class="bold"><strong>swobj4</strong></span> in this example).
    </p></li><li class="listitem "><p>
     If this was a SWPAC node, reduce the member-count attribute by 1 in the
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file. For SWOBJ nodes, no such action is needed.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     You may want to use the <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> switches to free up the resources
     when running the configuration processor. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Validate the changes you have made to the configuration files using the
     playbook below before proceeding further:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them in your configuration files and repeat
     steps 3-5 again until no more errors occur before going to the next step.
    </p><p>
     For more details on how to interpret and resolve errors, see
     <a class="xref" href="#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>
    </p></li><li class="listitem "><p>
     Remove the node from Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name=swobj4</pre></div></li><li class="listitem "><p>
     Run the Cobbler deploy playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     The final step will depend on what type of Swift node you are removing.
    </p><p>
     If the node was a SWPAC node, run the <code class="literal">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div><p>
     If the node was a SWOBJ node, run the <code class="literal">swift-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished. For more details, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
    </p></li><li class="listitem "><p>
     You may need to continue to rebalance the rings. For instructions, see
     <span class="bold"><strong>Final Rebalance Phase </strong></span> at
     <a class="xref" href="#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div><div class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-9"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Swift Node from Monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-swift-removing-swift-node-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-removing-swift-node-xml-9</li></ul></div></div></div></div><p>
   Once you have removed the Swift node(s), the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
   You will want to SSH to each of the Monasca API servers and edit the
   <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to remove
   references to the Swift node(s) you removed. This will require
   <code class="literal">sudo</code> access.
  </p><p>
   Once you have removed the references on each of your Monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the Swift node references removed and the monasca-agent restarted, you
   can then delete the corresponding alarm to finish this process. To do so we
   recommend using the Monasca CLI which should be installed on each of your
   Monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=&lt;swift node deleted&gt;</pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div></div></div><div class="sect4" id="replace-swift-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Swift Node</span> <a title="Permalink" class="permalink" href="#replace-swift-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_swift_node.xml</li><li><span class="ds-label">ID: </span>replace-swift-node</li></ul></div></div></div></div><p>
  Maintenance steps for replacing a failed Swift node in your environment.
 </p><p>
  This process is used when you want to replace a failed Swift node in your
  cloud.
 </p><div id="id-1.6.15.4.8.3.7.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   If it applies to the server, do not skip step 10. If you do, the system will
   overwrite the existing rings with new rings. This will not cause data loss,
   but, potentially, will move most objects in your system to new locations and
   may make data unavailable until the replication process has completed.
  </p></div><div class="sect5" id="id-1.6.15.4.8.3.7.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace a Swift node in your environment</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.8.3.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update your cloud configuration with the details of your replacement Swift
     node.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Edit your <code class="literal">servers.yml</code> file to include the details
       (MAC address, IPMI user, password, and IP address (IPME) if these
       have changed) about your replacement Swift node.
      </p><div id="id-1.6.15.4.8.3.7.5.2.2.2.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        Do not change the server's IP address (that is,
        <code class="literal">ip-addr</code>).
       </p></div><p>
       Path to file:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div><p>
       Example showing the fields to edit, in bold:
      </p><div class="verbatim-wrap"><pre class="screen"> - id: swobj5
   role: SWOBJ-ROLE
   server-group: rack2
   <span class="bold"><strong>mac-addr: 8c:dc:d4:b5:cb:bd</strong></span>
   nic-mapping: HP-DL360-6PORT
   ip-addr: 10.243.131.10
   <span class="bold"><strong>ilo-ip: 10.1.12.88
   ilo-user: iLOuser
   ilo-password: iLOpass</strong></span>
   ...</pre></div></li><li class="listitem "><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "replacing a Swift node"</pre></div></li><li class="listitem "><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></li><li class="listitem "><p>
     Update Cobbler and reimage your replacement Swift node:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Obtain the name in Cobbler for your node you wish to remove. You will
       use this value to replace <code class="literal">&lt;node name&gt;</code> in future
       steps.
      </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
       Remove the replaced Swift node from Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name &lt;node name&gt;</pre></div></li><li class="listitem "><p>
       Re-run the <code class="literal">cobbler-deploy.yml</code> playbook to add the
       replaced node:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
       Reimage the node using this playbook:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div></li><li class="listitem "><p>
     Complete the deployment of your replacement Swift node.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Obtain the hostname for your new Swift node. You will use this value to
       replace <code class="literal">&lt;hostname&gt;</code> in future steps.
      </p><div class="verbatim-wrap"><pre class="screen">cat ~/openstack/my_cloud/info/server_info.yml</pre></div></li><li class="listitem "><p>
       Configure the operating system on your replacement Swift node:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
       If this is the Swift ring builder server, restore the Swift ring builder
       files to the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For
       more information and instructions, see
       <a class="xref" href="#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a> and
       <a class="xref" href="#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
      </p></li><li class="listitem "><p>
       Configure services on the node using the
       <code class="literal">ardana-deploy.yml</code> playbook. If you have used an
       encryption password when running the configuration processor, include
       the <code class="literal">--ask-vault-pass</code> argument.
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit &lt;hostname&gt;</pre></div></li></ol></div></li></ol></div></div></div><div class="sect4" id="replacing-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing Drives in a Swift Node</span> <a title="Permalink" class="permalink" href="#replacing-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span>replacing-disks</li></ul></div></div></div></div><p>
  Maintenance steps for replacing drives in a Swift node.
 </p><p>
  This process is used when you want to remove a failed hard drive
  from Swift node and replace it with a new one.
 </p><p>
  There are two different classes of drives in a Swift node that needs to be
  replaced; the operating system disk drive (generally
  <span class="bold"><strong>/dev/sda</strong></span>) and storage disk drives. There are
  different procedures for the replacement of each class of drive to bring the
  node back to normal.
 </p><div class="sect5" id="id-1.6.15.4.8.3.8.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Replace the Operating System Disk Drive</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.8.3.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After the operating system disk drive is replaced, the node must be
   reimaged.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update your Cobbler profile:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     Reimage the node using this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server name&gt;</pre></div><p>
     In the example below <span class="bold"><strong>swobj2</strong></span> server is
     reimaged:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj2</pre></div></li><li class="listitem "><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     In the following example, for <span class="bold"><strong>swobj2</strong></span>, the
     hostname is <span class="bold"><strong>ardana-cp1-swobj0002</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit ardana-cp1-swobj0002*</pre></div></li><li class="listitem "><p>
     If this is the first server running the swift-proxy service, restore the
     Swift Ring Builder files to the
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For more
     information and instructions, see <a class="xref" href="#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a> and
     <a class="xref" href="#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
    </p></li><li class="listitem "><p>
     Configure services on the node using the <code class="literal">ardana-deploy.yml</code>
     playbook. If you have used an encryption password when running the
     configuration processor include the <code class="literal">--ask-vault-pass</code>
     argument.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass \
  --limit &lt;hostname&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit ardana-cp1-swobj0002*</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.4.8.3.8.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Replace a Storage Disk Drive</span> <a title="Permalink" class="permalink" href="#id-1.6.15.4.8.3.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After a storage drive is replaced, there is no need to reimage the server.
   Instead, run the <code class="literal">swift-reconfigure.yml</code> playbook.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log onto the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit &lt;hostname&gt;</pre></div><p>
     In following example, the server used is
     <span class="bold"><strong>swobj2</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit ardana-cp1-swobj0002-mgmt</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="mariadb-manual-update"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating MariaDB with Galera</span> <a title="Permalink" class="permalink" href="#mariadb-manual-update">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-mariadb-manual-update.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-mariadb-manual-update.xml</li><li><span class="ds-label">ID: </span>mariadb-manual-update</li></ul></div></div></div></div><p>
   Updating MariaDB with Galera must be done manually. Updates are not
   installed automatically. In particular, this situation applies to upgrades
   to MariaDB 10.2.17 or higher from MariaDB 10.2.16 or earlier. See <a class="link" href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/" target="_blank">MariaDB
   10.2.22 Release Notes - Notable Changes</a>.
  </p><p>
   Using the CLI, update MariaDB with the following procedure:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Mark Galera as unmanaged:
    </p><div class="verbatim-wrap"><pre class="screen">crm resource unmanage galera</pre></div><p>
     Or put the whole cluster into maintenance mode:
    </p><div class="verbatim-wrap"><pre class="screen">crm configure property maintenance-mode=true</pre></div></li><li class="step "><p>
     Pick a node other than the one currently targeted by the loadbalancer and
     stop MariaDB on that node:
    </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-demote -r galera -V</pre></div></li><li class="step "><p>
     Perform updates:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Uninstall the old versions of MariaDB and the Galera wsrep provider.
      </p></li><li class="step "><p>
       Install the new versions of MariaDB and the Galera wsrep
       provider. Select the appropriate instructions at <a class="link" href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/" target="_blank">Installing
       MariaDB with zypper</a>.
      </p></li><li class="step "><p>
       Change configuration options if necessary.
      </p></li></ol></li><li class="step "><p>
       Start MariaDB on the node.
      </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-promote -r galera -V</pre></div></li><li class="step "><p>
       Run <code class="command">mysql_upgrade</code> with the
       <code class="literal">--skip-write-binlog</code> option.
      </p></li><li class="step "><p>
       On the other nodes, repeat the process detailed above: stop MariaDB,
       perform updates, start MariaDB, run <code class="command">mysql_upgrade</code>.
      </p></li><li class="step "><p>
       Mark Galera as managed:
      </p><div class="verbatim-wrap"><pre class="screen">crm resource manage galera</pre></div><p>
       Or take the cluster out of maintenance mode.
      </p></li></ol></div></div></div></div><div class="sect1" id="unplanned-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned System Maintenance</span> <a title="Permalink" class="permalink" href="#unplanned-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-unplanned_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-unplanned_maintenance.xml</li><li><span class="ds-label">ID: </span>unplanned-maintenance</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for your cloud.
 </p><div class="sect2" id="whole-unplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Whole Cloud Recovery Procedures</span> <a title="Permalink" class="permalink" href="#whole-unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-whole_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-whole_unplanned.xml</li><li><span class="ds-label">ID: </span>whole-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance procedures for your whole cloud.
 </p><div class="sect3" id="full-recovery"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Full Disaster Recovery</span> <a title="Permalink" class="permalink" href="#full-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span>full-recovery</li></ul></div></div></div></div><p>
  In this disaster scenario, you have lost everything in the cloud, including
  Swift.
 </p><div class="sect4" id="id-1.6.15.5.3.3.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup:</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Restoring from a Swift backup is not possible because Swift is gone.
  </p></div><div class="sect4" id="id-1.6.15.5.3.3.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup:</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file so it contains the same information as it had
     previously:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div></li><li class="listitem "><p>
     On the Cloud Lifecycle Manager copy the following files:
    </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/hp-ci/openstack/* ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     Run this playbook to restore the Cloud Lifecycle Manager helper:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="listitem "><p>
     Run as root, and change directories:
    </p><div class="verbatim-wrap"><pre class="screen">sudo su
cd /root/deployer_restore_helper/</pre></div></li><li class="listitem "><p>
     Execute the restore:
    </p><div class="verbatim-wrap"><pre class="screen">./deployer_restore_script.sh</pre></div></li><li class="listitem "><p>
     Run this playbook to deploy your cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml -e '{ "freezer_backup_jobs_upload": false }'</pre></div></li><li class="listitem "><p>
     You can now perform the procedures to restore MySQL and Swift. Once
     everything is restored, re-enable the backups from the Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div><div class="sect3" id="full-recovery-test"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Full Disaster Recovery Test</span> <a title="Permalink" class="permalink" href="#full-recovery-test">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span>full-recovery-test</li></ul></div></div></div></div><p>
  Full Disaster Recovery Test
 </p><div class="sect4" id="id-1.6.15.5.3.4.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> platform</p><p>An external server to store backups to via SSH</p></div><div class="sect4" id="id-1.6.15.5.3.4.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Goals</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Here is a high level view of how we expect to test the disaster recovery of the platform.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the control plane using Freezer to an SSH target</p></li><li class="listitem "><p>Backup the Cassandra Database</p></li><li class="listitem "><p>Re-install Controller 1 with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO</p></li><li class="listitem "><p>Use Freezer to recover deployment data (model …)</p></li><li class="listitem "><p>Re-install <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> on Controller 1, 2, 3</p></li><li class="listitem "><p>Recover the Cassandra Database</p></li><li class="listitem "><p>Recover the backup of the MariaDB database</p></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Description of the testing environment</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The testing environment is very similar to the Entry Scale model.</p><p>It used 5 servers: 3 Controllers and 2 computes.</p><p>The controller node have three disks. The first one is reserved for the system, while others are used for swift.</p><div id="id-1.6.15.5.3.4.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>During this Disaster Recovery exercise, we have saved the data on disk 2 and 3 of the swift controllers.</p><p>This allow to restore the swift objects after the recovery.</p><p>If these disks were to be wiped as well, swift data would be lost but the procedure would not change.</p><p>The only difference is that Glance images would be lost and they will have to be re-uploaded.</p></div></div><div class="sect4" id="id-1.6.15.5.3.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disaster recovery test note</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>If it is not specified otherwise, all the commands should be executed on controller 1, which is also the deployer node.</p></div><div class="sect4" id="id-1.6.15.5.3.4.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Disaster testing</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In order to validate the procedure after recovery, we need to create some workloads.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
         Source the service credential file
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
         Copy an image to the platform and create a Glance image with it.
         In this example, Cirros is used
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image create --disk-format raw --container-format bare --public --file ~/cirros-0.3.5-x86_64-disk.img cirros</pre></div></li><li class="listitem "><p>Create a network</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create test_net</pre></div></li><li class="listitem "><p>Create a subnet</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron subnet-create 07c35d11-13f9-41d4-8289-fa92147b1d44 192.168.42.0/24 --name test_subnet</pre></div></li><li class="listitem "><p>Create some instances</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create server_1 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_2 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_3 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_4 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_5 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server list</pre></div></li><li class="listitem "><p>Create containers and objects</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift upload container_1 ~/service.osrc
var/lib/ardana/service.osrc

<code class="prompt user">ardana &gt; </code>swift upload container_1 ~/backup.osrc
swift upload container_1 ~/backup.osrc

<code class="prompt user">ardana &gt; </code>swift list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation of the backup server</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Preparation of the backup server</p><div class="sect5" id="id-1.6.15.5.3.4.8.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation to store Freezer backups</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.8.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In this example, we want to store the backups on the server 192.168.69.132</p><p>Freezer will connect with the user backupuser on port 22 and store the backups in the <code class="filename">/mnt/backups/</code> directory.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Connect to the backup server</p></li><li class="listitem "><p>Create the user</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>useradd backupuser --create-home --home-dir /mnt/backups/</pre></div></li><li class="listitem "><p>Switch to that user</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>su backupuser</pre></div></li><li class="listitem "><p>Create the SSH keypair</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ssh-keygen -t rsa
&gt; # Just leave the default for the first question and do not set any passphrase
&gt; Generating public/private rsa key pair.
&gt; Enter file in which to save the key (/mnt/backups//.ssh/id_rsa):
&gt; Created directory '/mnt/backups//.ssh'.
&gt; Enter passphrase (empty for no passphrase):
&gt; Enter same passphrase again:
&gt; Your identification has been saved in /mnt/backups//.ssh/id_rsa
&gt; Your public key has been saved in /mnt/backups//.ssh/id_rsa.pub
&gt; The key fingerprint is:
&gt; a9:08:ae:ee:3c:57:62:31:d2:52:77:a7:4e:37:d1:28 backupuser@padawan-ccp-c0-m1-mgmt
&gt; The key's randomart image is:
&gt; +---[RSA 2048]----+
&gt; |          o      |
&gt; |   . . E + .     |
&gt; |  o . . + .      |
&gt; | o +   o +       |
&gt; |  + o o S .      |
&gt; | . + o o         |
&gt; |  o + .          |
&gt; |.o .             |
&gt; |++o              |
&gt; +-----------------+</pre></div></li><li class="listitem "><p>Add the public key to the list of the keys authorized to connect to that user on this server</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa.pub &gt;&gt; /mnt/backups/.ssh/authorized_keys</pre></div></li><li class="listitem "><p>Print the private key. This is what we will use for the backup configuration (ssh_credentials.yml file)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa

&gt; -----BEGIN RSA PRIVATE KEY-----
&gt; MIIEogIBAAKCAQEAvjwKu6f940IVGHpUj3ffl3eKXACgVr3L5s9UJnb15+zV3K5L
&gt; BZuor8MLvwtskSkgdXNrpPZhNCsWSkryJff5I335Jhr/e5o03Yy+RqIMrJAIa0X5
&gt; ...
&gt; ...
&gt; ...
&gt; iBKVKGPhOnn4ve3dDqy3q7fS5sivTqCrpaYtByJmPrcJNjb2K7VMLNvgLamK/AbL
&gt; qpSTZjicKZCCl+J2+8lrKAaDWqWtIjSUs29kCL78QmaPOgEvfsw=
&gt; -----END RSA PRIVATE KEY-----</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.8.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation to store Cassandra backups</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In this example, we want to store the backups on the server 192.168.69.132. We will store the backups in the <code class="filename">/mnt/backups/cassandra_backups/</code> directory.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a directory on the backup server to store cassandra backups</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>mkdir /mnt/backups/cassandra_backups</pre></div></li><li class="listitem "><p>Copy private ssh key from backupserver to all controller nodes</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>scp /mnt/backups/.ssh/id_rsa ardana@<em class="replaceable ">CONTROLLER</em>:~/.ssh/id_rsa_backup
         Password:
         id_rsa     100% 1675     1.6KB/s   00:00</pre></div><p><em class="replaceable ">Replace CONTROLLER with each control node e.g. doc-cp1-c1-m1-mgmt, doc-cp1-c1-m2-mgmt etc</em></p></li><li class="listitem "><p>Login to each controller node and copy private ssh key to the root user's .ssh directory</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp /var/lib/ardana/.ssh/id_rsa_backup /root/.ssh/</pre></div></li><li class="listitem "><p>Verify that you can ssh to backup server as backup user using the private key</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ssh -i ~/.ssh/id_rsa_backup backupuser@doc-cp1-comp0001-mgmt</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.5.3.4.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Perform Backups for disaster recovery test</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Perform Backups for disaster recovery</p><div class="sect5" id="id-1.6.15.5.3.4.9.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Execute backup of Cassandra</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.9.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Execute backup of Cassandra</p><p>Create cassandra-backup-extserver.sh script on all
     controller nodes where Cassandra runs, which can be determined
     by running this command on deployer</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible FND-CDB --list-hosts</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &gt; ~/cassandra-backup-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra/data/data
NODETOOL=/usr/bin/nodetool

# e.g. cassandra-snp-2018-06-26-1003
SNAPSHOT_NAME=cassandra-snp-\$(date +%F-%H%M)
HOST_NAME=\$(/bin/hostname)_

# Take a snapshot of cassandra database
\$NODETOOL snapshot -t \$SNAPSHOT_NAME monasca

# Collect a list of directories that make up the snapshot
SNAPSHOT_DIR_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)
for d in \$SNAPSHOT_DIR_LIST
  do
    # copy snapshot directories to external server
    rsync -avR -e "ssh -i /root/.ssh/id_rsa_backup" \$d \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME
  done

\$NODETOOL clearsnapshot monasca
EOF</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod +x ~/cassandra-backup-extserver.sh</pre></div><p>Execute following steps on all the controller nodes</p><div id="id-1.6.15.5.3.4.9.3.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>/usr/local/sbin/cassandra-backup-extserver.sh should be
       executed on all the three controller nodes at the same time
       (within seconds of each other) for a successful backup
       </p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit /usr/local/sbin/cassandra-backup-extserver.sh script</p><p>
          Set
          <code class="literal">BACKUP_USER</code> and <code class="literal">BACKUP_SERVER</code>
          to the desired backup user (for example,
          <code class="systemitem">backupuser</code>) and desired
          backup server (for example, <code class="literal">192.168.68.132</code>),
          respectively.
         </p><div class="verbatim-wrap"><pre class="screen">BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</pre></div></li><li class="listitem "><p>Execute ~/cassandra-backup-extserver.sh</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>~/cassandra-backup-extserver.sh (on all controller nodes which are also cassandra nodes)

Requested creating snapshot(s) for [monasca] with snapshot name [cassandra-snp-2018-06-28-0251] and options {skipFlush=false}
Snapshot directory: cassandra-snp-2018-06-28-0251
sending incremental file list
created directory /mnt/backups/cassandra_backups//doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
/var/
/var/cassandra/
/var/cassandra/data/
/var/cassandra/data/data/
/var/cassandra/data/data/monasca/

...
...
...

/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-Summary.db
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-TOC.txt
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/schema.cql
sent 173,691 bytes  received 531 bytes  116,148.00 bytes/sec
total size is 171,378  speedup is 0.98
Requested clearing snapshot(s) for [monasca]</pre></div></li><li class="listitem "><p>Verify cassandra backup directory on backup server</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306
drwxr-xr-x 3 backupuser users 4096 Jun 28 02:51 doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
drwxr-xr-x 8 backupuser users 4096 Jun 27 20:56 ..

$backupuser@backupserver&gt; du -shx /mnt/backups/cassandra_backups/*
6.2G    /mnt/backups/cassandra_backups/doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
6.3G    /mnt/backups/cassandra_backups/doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.9.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Execute backup of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.9.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Execute backup of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit the configuration file for SSH backups (be careful to format the private key as requested: pipe on the first line and two spaces indentation). The private key is the key we created on the backup server earlier.</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/my_cloud/config/freezer/ssh_credentials.yml

$ cat ~/openstack/my_cloud/config/freezer/ssh_credentials.yml
freezer_ssh_host: 192.168.69.132
freezer_ssh_port: 22
freezer_ssh_username: backupuser
freezer_ssh_base_dir: /mnt/backups
freezer_ssh_private_key: |
  -----BEGIN RSA PRIVATE KEY-----
  MIIEowIBAAKCAQEAyzhZ+F+sXQp70N8zCDDb6ORKAxreT/qD4zAetjOTuBoFlGb8
  pRBY79t9vNp7qvrKaXHBfb1OkKzhqyUwEqNcC9bdngABbb8KkCq+OkfDSAZRrmja
  wa5PzgtSaZcSJm9jQcF04Fq19mZY2BLK3OJL4qISp1DmN3ZthgJcpksYid2G3YG+
  bY/EogrQrdgHfcyLaoEkiBWQSBTEENKTKFBB2jFQYdmif3KaeJySv9cJqihmyotB
  s5YTdvB5Zn/fFCKG66THhKnIm19NftbJcKc+Y3Z/ZX4W9SpMSj5dL2YW0Y176mLy
  gMLyZK9u5k+fVjYLqY7XlVAFalv9+HZsvQ3OQQIDAQABAoIBACfUkqXAsrrFrEDj
  DlCDqwZ5gBwdrwcD9ceYjdxuPXyu9PsCOHBtxNC2N23FcMmxP+zs09y+NuDaUZzG
  vCZbCFZ1tZgbLiyBbiOVjRVFLXw3aNkDSiT98jxTMcLqTi9kU5L2xN6YSOPTaYRo
  IoSqge8YjwlmLMkgGBVU7y3UuCmE/Rylclb1EI9mMPElTF+87tYK9IyA2QbIJm/w
  4aZugSZa3PwUvKGG/TCJVD+JfrZ1kCz6MFnNS1jYT/cQ6nzLsQx7UuYLgpvTMDK6
  Fjq63TmVg9Z1urTB4dqhxzpDbTNfJrV55MuA/z9/qFHs649tFB1/hCsG3EqWcDnP
  mcv79nECgYEA9WdOsDnnCI1bamKA0XZxovb2rpYZyRakv3GujjqDrYTI97zoG+Gh
  gLcD1EMLnLLQWAkDTITIf8eurkVLKzhb1xlN0Z4xCLs7ukgMetlVWfNrcYEkzGa8
  wec7n1LfHcH5BNjjancRH0Q1Xcc2K7UgGe2iw/Iw67wlJ8i5j2Wq3sUCgYEA0/6/
  irdJzFB/9aTC8SFWbqj1DdyrpjJPm4yZeXkRAdn2GeLU2jefqPtxYwMCB1goeORc
  gQLspQpxeDvLdiQod1Y1aTAGYOcZOyAatIlOqiI40y3Mmj8YU/KnL7NMkaYBCrJh
  aW//xo+l20dz52pONzLFjw1tW9vhCsG1QlrCaU0CgYB03qUn4ft4JDHUAWNN3fWS
  YcDrNkrDbIg7MD2sOIu7WFCJQyrbFGJgtUgaj295SeNU+b3bdCU0TXmQPynkRGvg
  jYl0+bxqZxizx1pCKzytoPKbVKCcw5TDV4caglIFjvoz58KuUlQSKt6rcZMHz7Oh
  BX4NiUrpCWo8fyh39Tgh7QKBgEUajm92Tc0XFI8LNSyK9HTACJmLLDzRu5d13nV1
  XHDhDtLjWQUFCrt3sz9WNKwWNaMqtWisfl1SKSjLPQh2wuYbqO9v4zRlQJlAXtQo
  yga1fxZ/oGlLVe/PcmYfKT91AHPvL8fB5XthSexPv11ZDsP5feKiutots47hE+fc
  U/ElAoGBAItNX4jpUfnaOj0mR0L+2R2XNmC5b4PrMhH/+XRRdSr1t76+RJ23MDwf
  SV3u3/30eS7Ch2OV9o9lr0sjMKRgBsLZcaSmKp9K0j/sotwBl0+C4nauZMUKDXqg
  uGCyWeTQdAOD9QblzGoWy6g3ZI+XZWQIMt0pH38d/ZRbuSUk5o5v
  -----END RSA PRIVATE KEY-----</pre></div></li><li class="listitem "><p>Save the modifications in the GIT repository</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "SSH backup configuration"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>Create the Freezer jobs</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li><li class="listitem "><p>Wait until all the SSH backup jobs have finished running</p><p>Freezer backup jobs are scheduled at interval specified in job specification</p><p>You will have to wait for the scheduled time interval for the backup job to run</p><p>To find the interval:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep SSH

| 34c1364692f64a328c38d54b95753844 | Ardana Default: deployer backup to SSH      |         7 | success | scheduled |       |            |
| 944154642f624bb7b9ff12c573a70577 | Ardana Default: swift backup to SSH         |         1 | success | scheduled |       |            |
| 22c6bab7ac4d43debcd4f5a9c4c4bb19 | Ardana Default: mysql backup to SSH         |         1 | success | scheduled |       |            |

<code class="prompt user">ardana &gt; </code>freezer job-show 944154642f624bb7b9ff12c573a70577
+-------------+---------------------------------------------------------------------------------+
| Field       | Value                                                                           |
+-------------+---------------------------------------------------------------------------------+
| Job ID      | 944154642f624bb7b9ff12c573a70577                                                |
| Client ID   | ardana-qe201-cp1-c1-m1-mgmt                                                     |
| User ID     | 33a6a77adc4b4799a79a4c3bd40f680d                                                |
| Session ID  |                                                                                 |
| Description | Ardana Default: swift backup to SSH                                             |
| Actions     | [{u'action_id': u'e8373b03ca4b41fdafd83f9ba7734bfa',                            |
|             |   u'freezer_action': {u'action': u'backup',                                     |
|             |                       u'backup_name': u'freezer_swift_builder_dir_backup',      |
|             |                       u'container': u'/mnt/backups/freezer_rings_backups',      |
|             |                       u'log_config_append': u'/etc/freezer/agent-logging.conf', |
|             |                       u'max_level': 14,                                         |
|             |                       u'path_to_backup': u'/etc/swiftlm/',                      |
|             |                       u'remove_older_than': 90,                                 |
|             |                       u'snapshot': True,                                        |
|             |                       u'ssh_host': u'192.168.69.132',                           |
|             |                       u'ssh_key': u'/etc/freezer/ssh_key',                      |
|             |                       u'ssh_port': u'22',                                       |
|             |                       u'ssh_username': u'backupuser',                           |
|             |                       u'storage': u'ssh'},                                      |
|             |   u'max_retries': 5,                                                            |
|             |   u'max_retries_interval': 60,                                                  |
|             |   u'user_id': u'33a6a77adc4b4799a79a4c3bd40f680d'}]                             |
| Start Date  |                                                                                 |
| End Date    |                                                                                 |
| Interval    | 24 hours                                                                        |
+-------------+---------------------------------------------------------------------------------+</pre></div><p>Swift SSH backup job has Interval of 24 hours, so the next backup would run after 24 hours.</p><p>In the default installation Interval for various backup jobs are:</p><div class="table" id="FreezerBackupJobScheduleTable"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.1: </span><span class="name">Default Interval for Freezer backup jobs </span><a title="Permalink" class="permalink" href="#FreezerBackupJobScheduleTable">#</a></h6></div><div class="table-contents"><table class="table" summary="Default Interval for Freezer backup jobs" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Job Name</th><th>Interval</th></tr></thead><tbody><tr><td>Ardana Default: deployer backup to SSH</td><td>48 hours</td></tr><tr><td>Ardana Default: mysql backup to SSH</td><td>12 hours</td></tr><tr><td>Ardana Default: swift backup to SSH</td><td>24 hours</td></tr></tbody></table></div></div><p>You will have to wait for as long as 48 hours for all the backup jobs to run</p></li><li class="listitem "><p>On the backup server, you can verify that the backup files are present</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ls -lah  /mnt/backups/
total 16
drwxr-xr-x 2 backupuser users 4096 Jun 27  2017 bin
drwxr-xr-x 2 backupuser users 4096 Jun 29 14:04 freezer_database_backups
drwxr-xr-x 2 backupuser users 4096 Jun 29 14:05 freezer_lifecycle_manager_backups
drwxr-xr-x 2 backupuser users 4096 Jun 29 14:05 freezer_rings_backups</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>du -shx *
4.0K    bin
509M    freezer_audit_logs_backups
2.8G    freezer_database_backups
24G     freezer_lifecycle_manager_backups
160K    freezer_rings_backups</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.5.3.4.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore of the first controller</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Restore of the first controller</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit the SSH backup configuration (re-enter the same information as earlier)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div></li><li class="listitem "><p>Execute the restore helper. When prompted, enter the hostname the first controller had. In this example: <code class="literal">doc-cp1-c1-m1-mgmt</code></p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="listitem "><p>Execute the restore. When prompted, leave the first value empty (none) and validate the restore by typing 'yes'.</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo su
cd /root/deployer_restore_helper/
./deployer_restore_script.sh</pre></div></li><li class="listitem "><p>Create a restore file for Swift rings</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nano swift_rings_restore.ini
<code class="prompt user">ardana &gt; </code>cat swift_rings_restore.ini</pre></div><p>Help:</p><div class="verbatim-wrap"><pre class="screen">[default]
action = restore
storage = ssh
# backup server ip
ssh_host = <em class="replaceable ">192.168.69.132</em>
# username to connect to the backup server
ssh_username = <em class="replaceable ">backupuser</em>
ssh_key = /etc/freezer/ssh_key
# base directory for backups on the backup server 
container = <em class="replaceable ">/mnt/backups/freezer_ring_backups</em>
backup_name = freezer_swift_builder_dir_backup
restore_abs_path = /etc/swiftlm
log_file = /var/log/freezer-agent/freezer-agent.log
# hostname that the controller
hostname = <em class="replaceable ">doc-cp1-c1-m1-mgmt</em>
overwrite = True</pre></div></li><li class="listitem "><p>Execute the restore of the swift rings</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer-agent --config ./swift_rings_restore.ini</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-deployment of controllers 1, 2 and 3</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Re-deployment of controllers 1, 2 and 3</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Change back to the default ardana user</p></li><li class="listitem "><p>Deactivate the freezer backup jobs (otherwise empty backups would be added on top of the current good backups)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nano ~/openstack/my_cloud/config/freezer/activate_jobs.yml
<code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/config/freezer/activate_jobs.yml

# If set to false, We wont create backups jobs.
freezer_create_backup_jobs: false

# If set to false, We wont create restore jobs.
freezer_create_restore_jobs: true</pre></div></li><li class="listitem "><p>Save the modification in the GIT repository</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "De-Activate SSH backup jobs during re-deployment"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>Run the cobbler-deploy.yml playbook</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.xml</pre></div></li><li class="listitem "><p>Run the bm-reimage.yml playbook limited to the second and third controller</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2,controller3</pre></div><p>controller2 and controller3 names can vary. You can use the bm-power-status.yml playbook in order to check the cobbler names of these nodes.</p></li><li class="listitem "><p>Run the site.yml playbook limited to the three controllers and localhost. In this example, this means: doc-cp1-c1-m1-mgmt, doc-cp1-c1-m2-mgmt, doc-cp1-c1-m3-mgmt and localhost</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cassandra database restore</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Cassandra database restore</p><p>Create a script cassandra-restore-extserver.sh on all
       controller nodes</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &gt; ~/cassandra-restore-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra
NODETOOL=/usr/bin/nodetool

HOST_NAME=\$(/bin/hostname)_

#Get snapshot name from command line.
if [ -z "\$*"  ]
then
  echo "usage \$0 &lt;snapshot to restore&gt;"
  exit 1
fi
SNAPSHOT_NAME=\$1

# restore
rsync -av -e "ssh -i /root/.ssh/id_rsa_backup" \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME/ /

# set ownership of newley restored files
chown -R cassandra:cassandra \$DATA_DIR

# Get a list of snapshot directories that have files to be restored.
RESTORE_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)

# use RESTORE_LIST to move snapshot files back into place of database.
for d in \$RESTORE_LIST
do
  cd \$d
  mv * ../..
  KEYSPACE=\$(pwd | rev | cut -d '/' -f4 | rev)
  TABLE_NAME=\$(pwd | rev | cut -d '/' -f3 |rev | cut -d '-' -f1)
  \$NODETOOL refresh \$KEYSPACE \$TABLE_NAME
done
cd
# Cleanup snapshot directories
\$NODETOOL clearsnapshot \$KEYSPACE
EOF</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod +x ~/cassandra-restore-extserver.sh</pre></div><p>Execute following steps on all the controller nodes</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit ~/cassandra-restore-extserver.sh script</p><p>
               Set
               <em class="replaceable ">BACKUP_USER</em>,<em class="replaceable ">BACKUP_SERVER</em>
               to the desired backup user (for example,
               <code class="systemitem">backupuser</code>) and the
               desired backup server (for example,
               <code class="literal">192.168.68.132</code>), respectively.
              </p><div class="verbatim-wrap"><pre class="screen">BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</pre></div></li><li class="listitem "><p>Execute ~/cassandra-restore-extserver.sh <em class="replaceable ">SNAPSHOT_NAME</em></p><p>You will have to find out <em class="replaceable ">SNAPSHOT_NAME</em> from listing of /mnt/backups/cassandra_backups.
              All the directories are of format <em class="replaceable ">HOST</em>_<em class="replaceable ">SNAPSHOT_NAME</em></p><div class="verbatim-wrap"><pre class="screen">ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>~/cassandra-restore-extserver.sh cassandra-snp-2018-06-28-0306

receiving incremental file list
./
var/
var/cassandra/
var/cassandra/data/
var/cassandra/data/data/
var/cassandra/data/data/monasca/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/manifest.json
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-CompressionInfo.db
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-Data.db
...
...
...
/usr/bin/nodetool clearsnapshot monasca</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Databases restore</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Databases restore</p><div class="sect5" id="id-1.6.15.5.3.4.13.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MariaDB database restore</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.13.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>MariaDB database restore</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Source the backup credentials file</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/backup.osrc</pre></div></li><li class="listitem "><p>List Freezer jobs</p><p>Gather the id of the job corresponding to the first controller and with the description. For example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep "mysql restore from SSH"
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| Job ID                           | Description                                 | # Actions | Result  | Status    | Event | Session ID |
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| 64715c6ce8ed40e1b346136083923260 | Ardana Default: mysql restore from SSH      |         1 |         | stop      |       |            |

<code class="prompt user">ardana &gt; </code>freezer job-show 64715c6ce8ed40e1b346136083923260
+-------------+---------------------------------------------------------------------------------+
| Field       | Value                                                                           |
+-------------+---------------------------------------------------------------------------------+
| Job ID      | 64715c6ce8ed40e1b346136083923260                                                |
| Client ID   | doc-cp1-c1-m1-mgmt                                                     |
| User ID     | 33a6a77adc4b4799a79a4c3bd40f680d                                                |
| Session ID  |                                                                                 |
| Description | Ardana Default: mysql restore from SSH                                          |
| Actions     | [{u'action_id': u'19dfb0b1851e41c682716ecc6990b25b',                            |
|             |   u'freezer_action': {u'action': u'restore',                                    |
|             |                       u'backup_name': u'freezer_mysql_backup',                  |
|             |                       u'container': u'/mnt/backups/freezer_database_backups',   |
|             |                       u'hostname': u'doc-cp1-c1-m1-mgmt',              |
|             |                       u'log_config_append': u'/etc/freezer/agent-logging.conf', |
|             |                       u'restore_abs_path': u'/tmp/mysql_restore/',              |
|             |                       u'ssh_host': u'192.168.69.132',                           |
|             |                       u'ssh_key': u'/etc/freezer/ssh_key',                      |
|             |                       u'ssh_port': u'22',                                       |
|             |                       u'ssh_username': u'backupuser',                           |
|             |                       u'storage': u'ssh'},                                      |
|             |   u'max_retries': 5,                                                            |
|             |   u'max_retries_interval': 60,                                                  |
|             |   u'user_id': u'33a6a77adc4b4799a79a4c3bd40f680d'}]                             |
| Start Date  |                                                                                 |
| End Date    |                                                                                 |
| Interval    |                                                                                 |
+-------------+---------------------------------------------------------------------------------+</pre></div></li><li class="listitem "><p>Start the job using its id</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start 64715c6ce8ed40e1b346136083923260
Start request sent for job 64715c6ce8ed40e1b346136083923260</pre></div></li><li class="listitem "><p>Wait for the job result to be success</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep "mysql restore from SSH"
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| Job ID                           | Description                                 | # Actions | Result  | Status    | Event | Session ID |
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| 64715c6ce8ed40e1b346136083923260 | Ardana Default: mysql restore from SSH      |         1 |         | running      |       |            |</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep "mysql restore from SSH"
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| Job ID                           | Description                                 | # Actions | Result  | Status    | Event | Session ID |
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| 64715c6ce8ed40e1b346136083923260 | Ardana Default: mysql restore from SSH      |         1 | success | completed |       |            |</pre></div></li><li class="listitem "><p>Verify that the files have been restored on the controller</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*

16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li><li class="listitem "><p>Repeat steps 2-5 on the other two controllers where the 
               MariaDB/Galera database is running, which can be 
               determined by running below command on deployer</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible FND-MDB --list-hosts</pre></div></li><li class="listitem "><p>Stop <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers (replace the hostnames of the controllers in the command)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="listitem "><p>Clean the mysql directory and copy the restored backup on all three controllers where MariaDB/Galera database is running</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cd /var/lib/mysql/
<code class="prompt user">root # </code>rm -rf ./*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* ./</pre></div><p>Switch back to the ardana user once the copy is finished</p></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.13.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.13.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Restart the MariaDB Database</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div><p>
             On the deployer node, execute the
             <code class="filename">galera-bootstrap.yml</code> playbook which will
             automatically determine the log sequence number, bootstrap the main node,
             and start the database cluster.
           </p><p>
             If this process fails to recover the database cluster, please refer to
             <a class="xref" href="#mysql" title="13.2.2.1.2. Recovering the MariaDB Database">Section 13.2.2.1.2, “Recovering the MariaDB Database”</a>. There Scenario 3 covers the process of manually
             starting the database.
           </p></li><li class="listitem "><p>
            Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services limited to the three controllers (replace the
            the hostnames of the controllers in the command).
          </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-start.yml \
 --limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="listitem "><p>Re-configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.13.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-enable SSH backups</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.13.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Re-enable SSH backups</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Re-activate Freezer backup jobs</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/my_cloud/config/freezer/activate_jobs.yml
<code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/config/freezer/activate_jobs.yml

# If set to false, We wont create backups jobs.
freezer_create_backup_jobs: true

# If set to false, We wont create restore jobs.
freezer_create_restore_jobs: true</pre></div></li><li class="listitem "><p>Save the modifications in the GIT repository</p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/
git add -A
git commit -a -m “Re-Activate SSH backup jobs”
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>Create Freezer jobs</p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.5.3.4.14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post restore testing</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.3.4.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Post restore testing</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Source the service credential file</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>Swift</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift list
container_1
volumebackups

<code class="prompt user">ardana &gt; </code>swift list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc

<code class="prompt user">ardana &gt; </code>swift download container_1 /tmp/backup.osrc</pre></div></li><li class="listitem "><p>Neutron</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list
+--------------------------------------+---------------------+--------------------------------------+
| ID                                   | Name                | Subnets                              |
+--------------------------------------+---------------------+--------------------------------------+
| 07c35d11-13f9-41d4-8289-fa92147b1d44 | test-net             | 02d5ca3b-1133-4a74-a9ab-1f1dc2853ec8|
+--------------------------------------+---------------------+--------------------------------------+</pre></div></li><li class="listitem "><p>Glance</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image list
+--------------------------------------+----------------------+--------+
| ID                                   | Name                 | Status |
+--------------------------------------+----------------------+--------+
| 411a0363-7f4b-4bbc-889c-b9614e2da52e | cirros-0.4.0-x86_64  | active |
+--------------------------------------+----------------------+--------+
<code class="prompt user">ardana &gt; </code>openstack image save --file /tmp/cirros f751c39b-f1e3-4f02-8332-3886826889ba
<code class="prompt user">ardana &gt; </code>ls -lah /tmp/cirros
-rw-r--r-- 1 ardana ardana 12716032 Jul  2 20:52 /tmp/cirros</pre></div></li><li class="listitem "><p>Nova</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list

<code class="prompt user">ardana &gt; </code>openstack server list

<code class="prompt user">ardana &gt; </code>openstack server create server_6 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e  --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
+-------------------------------------+------------------------------------------------------------+
| Field                               | Value                                                      |
+-------------------------------------+------------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                     |
| OS-EXT-AZ:availability_zone         |                                                            |
| OS-EXT-SRV-ATTR:host                | None                                                       |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                                       |
| OS-EXT-SRV-ATTR:instance_name       |                                                            |
| OS-EXT-STS:power_state              | NOSTATE                                                    |
| OS-EXT-STS:task_state               | scheduling                                                 |
| OS-EXT-STS:vm_state                 | building                                                   |
| OS-SRV-USG:launched_at              | None                                                       |
| OS-SRV-USG:terminated_at            | None                                                       |
| accessIPv4                          |                                                            |
| accessIPv6                          |                                                            |
| addresses                           |                                                            |
| adminPass                           | iJBoBaj53oUd                                               |
| config_drive                        |                                                            |
| created                             | 2018-07-02T21:02:01Z                                       |
| flavor                              | m1.small (2)                                               |
| hostId                              |                                                            |
| id                                  | ce7689ff-23bf-4fe9-b2a9-922d4aa9412c                       |
| image                               | cirros-0.4.0-x86_64 (f751c39b-f1e3-4f02-8332-3886826889ba) |
| key_name                            | None                                                       |
| name                                | server_6                                                   |
| progress                            | 0                                                          |
| project_id                          | cca416004124432592b2949a5c5d9949                           |
| properties                          |                                                            |
| security_groups                     | name='default'                                             |
| status                              | BUILD                                                      |
| updated                             | 2018-07-02T21:02:01Z                                       |
| user_id                             | 8cb1168776d24390b44c3aaa0720b532                           |
| volumes_attached                    |                                                            |
+-------------------------------------+------------------------------------------------------------+

<code class="prompt user">ardana &gt; </code>openstack server list
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ID                                   | Name     | Status | Networks                        | Image               | Flavor    |
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ce7689ff-23bf-4fe9-b2a9-922d4aa9412c | server_6 | ACTIVE | n1=1.1.1.8                      | cirros-0.4.0-x86_64 | m1.small  |

<code class="prompt user">ardana &gt; </code>openstack server delete ce7689ff-23bf-4fe9-b2a9-922d4aa9412c</pre></div></li></ol></div></div></div></div><div class="sect2" id="cont-ungplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Control Plane Maintenance</span> <a title="Permalink" class="permalink" href="#cont-ungplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-cont_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-cont_unplanned.xml</li><li><span class="ds-label">ID: </span>cont-ungplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for controller nodes such as recovery from power
  failure.
 </p><div class="sect3" id="recover-downed-cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting Controller Nodes After a Reboot</span> <a title="Permalink" class="permalink" href="#recover-downed-cluster">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>recover-downed-cluster</li></ul></div></div></div></div><p>
  Steps to follow if one or more of your controller nodes lose network
  connectivity or power, which includes if the node is either rebooted or needs
  hardware maintenance.
 </p><p>
  When a controller node is rebooted, needs hardware maintenance, loses
  network connectivity or loses power, these steps will help you recover the
  node.
 </p><p>
  These steps may also be used if the Host Status (ping) alarm is triggered
  for one or more of your controller nodes.
 </p><div class="sect4" id="idg-all-operations-maintenance-controller-restart-controller-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-controller-restart-controller-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-controller-restart-controller-xml-7</li></ul></div></div></div></div><p>
   The following conditions must be true in order to perform these steps
   successfully:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Each of your controller nodes should be powered on.
    </p></li><li class="listitem "><p>
     Each of your controller nodes should have network connectivity, verified
     by SSH connectivity from the Cloud Lifecycle Manager to them.
    </p></li><li class="listitem "><p>
     The operator who performs these steps will need access to the lifecycle
     manager.
    </p></li></ul></div></div><div class="sect4" id="mysql"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering the MariaDB Database</span> <a title="Permalink" class="permalink" href="#mysql">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>mysql</li></ul></div></div></div></div><p>
   The recovery process for your MariaDB database cluster will depend on how
   many of your controller nodes need to be recovered. We will cover two
   scenarios:
  </p><p>
   <span class="bold"><strong>Scenario 1: Recovering one or two of your controller
   nodes but not the entire cluster</strong></span>
  </p><p>
   Follow these steps to recover one or two of your controller nodes but not the
   entire cluster, then use these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Ensure the controller nodes have power and are booted to the command
     prompt.
    </p></li><li class="step "><p>
     If the MariaDB service is not started, start it with this command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo service mysql start</pre></div></li><li class="step "><p>
     If MariaDB fails to start, proceed to the next section which covers the
     bootstrap process.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Scenario 2: Recovering the entire controller cluster
   with the bootstrap playbook</strong></span>
  </p><p>
   If the scenario above failed or if you need to recover your entire control
   plane cluster, use the process below to recover the MariaDB database.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Make sure no <code class="literal">mysqld</code> daemon is running on any node in
     the cluster before you continue with the steps in this procedure. If there
     is a <code class="literal">mysqld</code> daemon running, then use the command below
     to shut down the daemon.
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop mysql</pre></div><p>
     If the mysqld daemon does not go down following the service stop, then
     kill the daemon using <code class="literal">kill -9</code> before continuing.
    </p></li><li class="step "><p>
     On the deployer node, execute the
     <code class="filename">galera-bootstrap.yml</code> playbook which will
     automatically determine the log sequence number, bootstrap the main node,
     and start the database cluster.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li></ol></div></div></div><div class="sect4" id="hlm"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting Services on the Controller Nodes</span> <a title="Permalink" class="permalink" href="#hlm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>hlm</li></ul></div></div></div></div><p>
   From the Cloud Lifecycle Manager you should execute the
   <code class="literal">ardana-start.yml</code> playbook for each node that was brought
   down so the services can be started back up.
  </p><p>
   If you have a dedicated (separate) Cloud Lifecycle Manager node you can use this
   syntax:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;</pre></div><p>
   If you have a shared Cloud Lifecycle Manager/controller setup and need to restart
   services on this shared node, you can use <code class="literal">localhost</code> to
   indicate the shared node, like this:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;,localhost</pre></div><div id="id-1.6.15.5.4.3.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you leave off the <code class="literal">--limit</code> switch, the playbook will
    be run against all nodes.
   </p></div></div><div class="sect4" id="monasca"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Monitoring Agents</span> <a title="Permalink" class="permalink" href="#monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>monasca</li></ul></div></div></div></div><p>
   As part of the recovery process, you should also restart the
   <code class="literal">monasca-agent</code> and these steps will show you how:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Stop the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml</pre></div></li><li class="listitem "><p>
     Restart the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml</pre></div></li><li class="listitem "><p>
     You can then confirm the status of the <code class="literal">monasca-agent</code>
     with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div></div><div class="sect3" id="recovering-controller-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering the Control Plane</span> <a title="Permalink" class="permalink" href="#recovering-controller-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-recovering_controller_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-recovering_controller_nodes.xml</li><li><span class="ds-label">ID: </span>recovering-controller-nodes</li></ul></div></div></div></div><p>
  If one or more of your controller nodes has experienced data or disk
  corruption due to power loss or hardware failure and you need perform
  disaster recovery then we provide different scenarios for how to resolve them
  to get your cloud recovered.
 </p><p>
  If one or more of your controller nodes has experienced data or disk
  corruption due to power-loss or hardware failure and you need perform
  disaster recovery then we provide different scenarios for how to resolve
  them to get your cloud recovered.
 </p><div id="id-1.6.15.5.4.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   You should have backed up <code class="filename">/etc/group</code> of the Cloud Lifecycle Manager
   manually after installation. While recovering a Cloud Lifecycle Manager node, manually copy
   the <code class="filename">/etc/group</code> file from a backup of the old Cloud Lifecycle Manager.
  </p></div><div class="sect4" id="pit-database-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time MariaDB Database Recovery</span> <a title="Permalink" class="permalink" href="#pit-database-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span>pit-database-recovery</li></ul></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, cloud controller nodes,
  and compute nodes) but you want to restore the MariaDB database to a
  previous state.
 </p><div class="sect5" id="id-1.6.15.5.4.4.5.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Determine which node is the first host member in the
     <code class="literal">FND-MDB</code> group, which will be the first node hosting the
     MariaDB service in your cloud. You can do this by using these commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>grep -A1 FND-MDB--first-member hosts/verb_hosts</pre></div><p>The result will be similar to the following example:
</p><div class="verbatim-wrap"><pre class="screen">[FND-MDB--first-member:children]
ardana002-cp1-c1-m1</pre></div><p>
     In this example, the host name of the node is
     <code class="literal">ardana002-cp1-c1-m1</code>
    </p></li><li class="step " id="mariadb-nodes"><p>
     Find the host IP address which will be used to log in.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana002-cp1-c1-m1
10.84.43.82      ardana002-cp1-c1-m1-extapi ardana002-cp1-c1-m1-extapi
192.168.24.21    ardana002-cp1-c1-m1-mgmt ardana002-cp1-c1-m1-mgmt
10.1.2.1         ardana002-cp1-c1-m1-guest ardana002-cp1-c1-m1-guest
10.84.65.3       ardana002-cp1-c1-m1-EXTERNAL-VM ardana002-cp1-c1-m1-external-vm</pre></div><p>
     In this example, <code class="literal">192.168.24.21</code> is the IP address for
     the host.
    </p></li><li class="step "><p>
     SSH into the host.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh ardana@192.168.24.21</pre></div></li><li class="step "><p>
     Source the backup file.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source /var/lib/ardana/backup.osrc</pre></div></li><li class="step "><p>
     Find the <code class="literal">Client ID</code> for the host name from the beginning
     of this procedure ( <code class="literal">ardana002-cp1-c1-m1</code> ) in this
     example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
     In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
     ID</code> are the same: <code class="literal">ardana002-cp1-c1-m1-mgmt</code>.
    </p></li><li class="step "><p>
     List the jobs
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
     Using the example in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C ardana002-cp1-c1-m1-mgmt</pre></div></li><li class="step "><p>
     Get the corresponding job id for <code class="literal">Ardana Default: mysql restore
     from Swift</code>.
    </p></li><li class="step "><p>
     Launch the restore process with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB-ID</em></pre></div></li><li class="step "><p>
     This will take some time. You can follow the progress by running
     <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code>. Wait
     until the restore job is finished before doing the next step.
    </p></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step "><p>
     Log back in to the first node running the MariaDB service, the same node
     as in <a class="xref" href="#mariadb-nodes" title="Step 3">Step 3</a>.
    </p></li><li class="step "><p>
     Clean the MariaDB directory using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm -r /var/lib/mysql/*</pre></div></li><li class="step "><p>
     Copy the restored files back to the MariaDB directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</pre></div></li><li class="step "><p>
     Log in to each of the other nodes in your MariaDB cluster, which were
     determined in <a class="xref" href="#mariadb-nodes" title="Step 3">Step 3</a>. Remove the
     <code class="literal">grastate.dat</code> file from each of them.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm /var/lib/mysql/grastate.dat</pre></div><div id="id-1.6.15.5.4.4.5.3.2.16.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
      Do not remove this file from the first node in your MariaDB cluster.
      Ensure you only do this from the other cluster nodes.
     </p></div></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Start the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.5.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Follow the same procedure as the one for Swift but select the job
   <code class="literal">Ardana Default: mysql restore from SSH</code>.
  </p></div><div class="sect5" id="id-1.6.15.5.4.4.5.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore MariaDB manually</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.5.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If restoring MariaDB fails during the procedure outlined above, you can
   follow this procedure to manually restore MariaDB:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the MariaDB cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step "><p>
     On all of the nodes running the MariaDB service, which should be all of
     your controller nodes, run the following command to purge the old
     database:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm -r /var/lib/mysql/*</pre></div></li><li class="step "><p>
     On the first node running the MariaDB service restore the backup with
     the command below. If you have already restored to a temporary directory,
     copy the files again.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</pre></div></li><li class="step "><p>
     If you need to restore the files manually from SSH, follow these steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Create the <code class="literal">/root/mysql_restore.ini</code> file with the
       contents below. Be careful to substitute the <code class="literal">{{ values
       }}</code>. Note that the SSH information refers to the SSH server you
       configured for backup before installing.
      </p><div class="verbatim-wrap"><pre class="screen">[default]
action = restore
storage = ssh
ssh_host = {{ freezer_ssh_host }}
ssh_username = {{ freezer_ssh_username }}
container = {{ freezer_ssh_base_dir }}/freezer_mysql_backup
ssh_key = /etc/freezer/ssh_key
backup_name = freezer_mysql_backup
restore_abs_path = /var/lib/mysql/
log_file = /var/log/freezer-agent/freezer-agent.log
hostname = {{ hostname of the first MariaDB node }}</pre></div></li><li class="step "><p>
       Execute the restore job:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer-agent --config /root/mysql_restore.ini</pre></div></li></ol></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Start the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step "><p>
     After approximately 10-15 minutes, the output of the
     <code class="literal">percona-status.yml</code> playbook should show all the
     MariaDB nodes in sync. MariaDB cluster status can be checked using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div><p>
     An example output is as follows:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] *************
  ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m2-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m3-mgmt] =&gt; {
  "msg": "mysql is synced."
  }</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.5.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time Cassandra Recovery</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.5.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A node may have been removed either due to an intentional action in the
   Cloud Lifecycle Manager Admin UI or as a result of a fatal hardware event that requires a
   server to be replaced. In either case, the entry for the failed or deleted
   node should be removed from Cassandra before a new node is brought up.
  </p><p>
   The following steps should be taken before enabling and deploying the
   replacement node.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Determine the IP address of the node that was removed or is being replaced.
    </p></li><li class="step "><p>
     On one of the functional Cassandra control plane nodes, log in as the
     <code class="literal">ardana</code> user.
    </p></li><li class="step "><p>
     Run the command <code class="command">nodetool status</code> to display a list of
     Cassandra nodes.
    </p></li><li class="step "><p>
     If the node that has been removed (no IP address matches that of the
     removed node) is not in the list, skip the next step.
    </p></li><li class="step "><p>
     If the node that was removed is still in the list, copy its node
     <em class="replaceable ">ID</em>.
    </p></li><li class="step "><p>
     Run the command <code class="command">nodetool removenode
     <em class="replaceable ">ID</em></code>.
    </p></li></ol></div></div><p>
   After any obsolete node entries have been removed, the replacement node can
   be deployed as usual (for more information, see <a class="xref" href="#cont-planned" title="13.1.2. Planned Control Plane Maintenance">Section 13.1.2, “Planned Control Plane Maintenance”</a>). The new Cassandra node will be able to join the
   cluster and replicate data.
  </p><p>
   For more information, please consult <a class="link" href="http://cassandra.apache.org/doc/latest/operating/topo_changes.html" target="_blank">the
   Cassandra documentation</a>.
   </p></div></div><div class="sect4" id="pit-swiftrings-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time Swift Rings Recovery</span> <a title="Permalink" class="permalink" href="#pit-swiftrings-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>pit-swiftrings-recovery</li></ul></div></div></div></div><p>
  In this situation, everything is still running (Cloud Lifecycle Manager, control plane nodes,
  and compute nodes) but you want to restore your Swift rings to a previous
  state.
 </p><div id="id-1.6.15.5.4.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   Freezer backs up and restores Swift rings only, not Swift data.
  </p></div><div class="sect5" id="restore-swift-bu"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup</span> <a title="Permalink" class="permalink" href="#restore-swift-bu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>restore-swift-bu</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="swift-nodes"><p>
     Log in to the first Swift Proxy (<code class="literal">SWF-PRX[0]</code>) node.
    </p><p>
     To find the first Swift Proxy node:
    </p><ol type="a" class="substeps "><li class="step "><p>
       On the Cloud Lifecycle Manager
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd  ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit SWF-PRX[0]</pre></div><p>
       At the end of the output, you will see something like the following
       example:
      </p><div class="verbatim-wrap"><pre class="screen">...
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:max-latency: 0.679254770279 (at 1529352109.66)'
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:avg-latency: 0.679254770279 (at 1529352109.66)'

PLAY RECAP ********************************************************************
ardana-qe102-cp1-c1-m1 : ok=12 changed=0 unreachable=0 failed=0```</pre></div></li><li class="step "><p>
       Find the first node name and its IP address. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana-qe102-cp1-c1-m1</pre></div></li></ol></li><li class="step "><p>
     Source the backup environment file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source /var/lib/ardana/backup.osrc</pre></div></li><li class="step "><p>
     Find the client id.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
     In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
     ID</code> are the same: <code class="literal">ardana002-cp1-c1-m1-mgmt</code>.
    </p></li><li class="step "><p>
     List the jobs
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
     Using the example in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C ardana002-cp1-c1-m1-mgmt</pre></div></li><li class="step "><p>
     Get the corresponding job id for <code class="literal">Ardana Default: swift restore
     from Swift</code> in the <code class="literal">Description</code> column.
    </p></li><li class="step "><p>
     Launch the restore job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB-ID</em></pre></div></li><li class="step "><p>
     This will take some time. You can follow the progress by running
     <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code> Wait
     until the restore job is finished before doing the next step.
    </p></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the Swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml</pre></div></li><li class="step "><p>
     Log back in to the first Swift Proxy (<code class="literal">SWF-PRX[0]</code>)
     node, which was determined in <a class="xref" href="#swift-nodes" title="Step 1">Step 1</a>.
    </p></li><li class="step "><p>
     Copy the restored files.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Reconfigure the Swift service:\
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.6.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Follow almost the same procedure as for Swift in the section immediately
   preceding this one: <a class="xref" href="#restore-swift-bu" title="13.2.2.2.2.1. Restore from a Swift backup">Section 13.2.2.2.2.1, “Restore from a Swift backup”</a>. The only change is
   that the restore job uses a different job id. Get the corresponding job id
   for <code class="literal">Ardana Default: Swift restore from SSH</code> in the
   <code class="literal">Description</code> column.
  </p></div></div><div class="sect4" id="pit-lifecyclemanager-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-time Cloud Lifecycle Manager Recovery</span> <a title="Permalink" class="permalink" href="#pit-lifecyclemanager-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-pit_lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span>pit-lifecyclemanager-recovery</li></ul></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, controller nodes, and
  compute nodes) but you want to restore the Cloud Lifecycle Manager to a previous state.
 </p><div class="procedure " id="restore-swift-ssh-bu"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 13.1: </span><span class="name">Restoring from a Swift or SSH Backup </span><a title="Permalink" class="permalink" href="#restore-swift-ssh-bu">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Source the backup environment file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>source /var/lib/ardana/backup.osrc</pre></div></li><li class="step "><p>
    Find the <code class="literal">Client ID</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
    In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
    ID</code> are the same: <code class="literal">ardana002-cp1-c1-m1-mgmt</code>.
   </p></li><li class="step "><p>
    List the jobs
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
    Using the example in the previous step:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer job-list -C ardana002-cp1-c1-m1-mgmt</pre></div></li><li class="step "><p>
    Find the correct job ID:
   </p><p><span class="formalpara-title">SSH Backups: </span>
     Get the id corresponding to the job id for <code class="literal">Ardana Default:
     deployer restore from SSH</code>.
    </p><p>
    or
   </p><p><span class="formalpara-title">Swift Backups. </span>
     Get the id corresponding to the job id for <code class="literal">Ardana Default:
     deployer restore from Swift</code>.
    </p></li><li class="step "><p>
    Stop the Dayzero UI:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop dayzero</pre></div></li><li class="step "><p>
    Launch the restore job:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer job-start <em class="replaceable ">JOB ID</em></pre></div></li><li class="step "><p>
    This will take some time. You can follow the progress by running
    <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code>. Wait
    until the restore job is finished before doing the next step.
   </p></li><li class="step "><p>
    Start the Dayzero UI:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl start dayzero</pre></div></li></ol></div></div></div><div class="sect4" id="lifecyclemanager-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Disaster Recovery</span> <a title="Permalink" class="permalink" href="#lifecyclemanager-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span>lifecyclemanager-recovery</li></ul></div></div></div></div><p>
  In this scenario everything is still running (controller nodes and compute
  nodes) but you have lost either a dedicated Cloud Lifecycle Manager or a shared
  Cloud Lifecycle Manager/controller node.
 </p><p>
  To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that you previously had
  loaded on your Cloud Lifecycle Manager, you will need to download and install the
  lifecycle management software using the instructions from the
  <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 3 “Installing the Cloud Lifecycle Manager server”, Section 3.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span> before proceeding further.
 </p><div class="sect5" id="id-1.6.15.5.4.4.8.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Install the freezer-agent using the following playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="step "><p>
     Access one of the other controller or compute nodes in your environment to
     perform the following steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Retrieve the <code class="filename">/var/lib/ardana/backup.osrc</code> file and
       copy it to the <code class="filename">/var/lib/ardana/</code> directory on the
       Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       Copy all the files in the
       <code class="literal">/opt/stack/service/freezer-api/etc/</code> directory to
       the same directory on the Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       Copy all the files in the <code class="literal">/var/lib/ca-certificates</code>
       directory to the same directory on the Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       Retrieve the <code class="literal">/etc/hosts</code> file and replace the one
       found on the Cloud Lifecycle Manager.
      </p></li></ol></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the value for <code class="literal">client_id</code> in the following file to
     contain the hostname of your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/service/freezer-api/etc/freezer-api.conf</pre></div></li><li class="step "><p>
     Update your ca-certificates:
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step "><p>
     Edit the <code class="literal">/etc/hosts</code> file, ensuring you edit the
     127.0.0.1 line so it points to <code class="literal">ardana</code>:
    </p><div class="verbatim-wrap"><pre class="screen">127.0.0.1       localhost ardana
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, source the backup user credentials:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/backup.osrc</pre></div></li><li class="step "><p>
     Find the <code class="literal">Client ID</code>
     (<code class="literal">ardana002-cp1-c0-m1-mgmt</code>) for the host name as done in
     previous procedures (see <a class="xref" href="#restore-swift-ssh-bu" title="Restoring from a Swift or SSH Backup">Procedure 13.1, “Restoring from a Swift or SSH Backup”</a>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
     In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
     ID</code> are the same: <code class="literal">ardana002-cp1-c0-m1-mgmt</code>.
    </p></li><li class="step "><p>
     List the Freezer jobs
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
     Using the example in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C ardana002-cp1-c0-m1-mgmt</pre></div></li><li class="step "><p>
     Get the id of the job corresponding to <code class="literal">Ardana Default: deployer
     backup to Swift</code>. Stop that job so the freezer scheduler does not
     begin making backups when started.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-stop <em class="replaceable ">JOB-ID</em></pre></div><p>
     If it is present, also stop the Cloud Lifecycle Manager's SSH backup.
    </p></li><li class="step "><p>
     Start the freezer scheduler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start openstack-freezer-scheduler</pre></div></li><li class="step "><p>
     Get the id of the job corresponding to <code class="literal">Ardana Default: deployer
     restore from Swift</code> and launch that job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB-ID</em></pre></div><p>
     This will take some time. You can follow the progress by running
     <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code>. Wait
     until the restore job is finished before doing the next step.
    </p></li><li class="step "><p>
     When the job completes, the previous Cloud Lifecycle Manager contents should be
     restored to your home directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~
<code class="prompt user">ardana &gt; </code>ls</pre></div></li><li class="step "><p>
     If you are using Cobbler, restore your Cobbler configuration with these
     steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Remove the following files:
      </p><div class="verbatim-wrap"><pre class="screen">sudo rm -rf /var/lib/cobbler
sudo rm -rf /srv/www/cobbler</pre></div></li><li class="step "><p>
       Deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
       Set the <code class="literal">netboot-enabled</code> flag for each of your nodes
       with this command:
      </p><div class="verbatim-wrap"><pre class="screen">for h in $(sudo cobbler system list)
do
  sudo cobbler system edit --name=$h --netboot-enabled=0
done</pre></div></li></ol></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready_deployment.yml</pre></div></li><li class="step "><p>
     If you are using a dedicated Cloud Lifecycle Manager, follow these steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       re-run the deployment to ensure the Cloud Lifecycle Manager is in the correct
       state:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</pre></div></li></ol></li><li class="step "><p>
     If you are using a shared Cloud Lifecycle Manager/controller, follow these
     steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       If the node is also a Cloud Lifecycle Manager hypervisor, run the following commands to
       recreate the virtual machines that were lost:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-hypervisor-setup.yml --limit &lt;this node&gt;</pre></div></li><li class="step "><p>
       If the node that was lost (or one of the VMs that it hosts) was a member
       of the RabbitMQ cluster then you need to remove the record of the old
       node, by running the following command <span class="bold"><strong>on any one
       of the other cluster members</strong></span>. In this example the nodes are
       called <code class="literal">cloud-cp1-rmq-mysql-m*-mgmt</code> but you need to
       use the correct names for your system, which you can find in
       <code class="literal">/etc/hosts</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ssh cloud-cp1-rmq-mysql-m3-mgmt sudo rabbitmqctl forget_cluster_node \
rabbit@cloud-cp1-rmq-mysql-m1-mgmt</pre></div></li><li class="step "><p>
       Run the <code class="literal">site.yml</code> against the complete cloud to
       reinstall and rebuild the services that were lost. If you replaced one
       of the RabbitMQ cluster members then you will need to add the
       <code class="literal">-e</code> flag shown below, to nominate a new master node
       for the cluster, otherwise you can omit it.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml -e \
rabbit_primary_hostname=cloud-cp1-rmq-mysql-m3</pre></div></li></ol></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.8.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the Cloud Lifecycle Manager, edit the following file so it contains the same
     information as it did previously:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, copy the following files, change directories,
     and run the playbook _deployer_restore_helper.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/hp-ci/openstack/* ~/openstack/my_cloud/definition/
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="step "><p>
     Perform the restore. First become root and change directories:
    </p><div class="verbatim-wrap"><pre class="screen">sudo su
<code class="prompt user">root # </code>cd /root/deployer_restore_helper/</pre></div></li><li class="step "><p>
     Execute the restore job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./deployer_restore_script.sh</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready_deployment.yml</pre></div></li><li class="step "><p>
     When the Cloud Lifecycle Manager is restored, re-run the deployment to ensure
     the Cloud Lifecycle Manager is in the correct state:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</pre></div></li></ol></div></div></div></div><div class="sect4" id="onetwo-controller-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">One or Two Controller Node Disaster Recovery</span> <a title="Permalink" class="permalink" href="#onetwo-controller-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-onetwo_controller_recovery.xml</li><li><span class="ds-label">ID: </span>onetwo-controller-recovery</li></ul></div></div></div></div><p>
  This scenario makes the following assumptions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Your Cloud Lifecycle Manager is still intact and working.
   </p></li><li class="listitem "><p>
    One or two of your controller nodes went down, but not the entire cluster.
   </p></li><li class="listitem "><p>
    The node needs to be rebuilt from scratch, not simply rebooted.
   </p></li></ul></div><div class="sect5" id="id-1.6.15.5.4.4.9.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps to recovering one or two controller nodes</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.9.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-onetwo_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that your node has power and all of the hardware is functioning.
    </p></li><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Verify that all of the information in your
     <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file is
     correct for your controller node. You may need to replace the existing
     information if you had to either replacement your entire controller node
     or just pieces of it.
    </p></li><li class="listitem "><p>
     If you made changes to your <code class="literal">servers.yml</code> file then
     commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "editing controller information"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Ensure that Cobbler has the correct system information:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       If you replaced your controller node with a completely new machine, you
       need to verify that Cobbler has the correct list of controller nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem "><p>
       Remove any controller nodes from Cobbler that no longer exist:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name=&lt;node&gt;</pre></div></li><li class="listitem "><p>
       Add the new node into Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></li><li class="listitem "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.6.15.5.4.4.9.4.2.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      If you do not know the <code class="literal">&lt;node name&gt;</code> already,
      you can get it by using <code class="command">sudo cobbler system list</code>.
     </p></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See the Persisted Server Allocations section in for
     information on how this works.
    </p></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;controller_node_hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the rebuilding of your controller node with the two playbooks
     below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="three-controller-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Three Control Plane Node Disaster Recovery</span> <a title="Permalink" class="permalink" href="#three-controller-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span>three-controller-recovery</li></ul></div></div></div></div><p>
  In this scenario, all control plane nodes are destroyed which need to be
  rebuilt or replaced.
 </p><div class="sect5" id="id-1.6.15.5.4.4.10.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup:</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Restoring from a Swift backup is not possible because Swift is gone.
  </p></div><div class="sect5" id="id-1.6.15.5.4.4.10.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Disable the default backup job(s) by editing the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible/roles/freezer-jobs/defaults/activate.yml</pre></div><p>
     Set the value for <code class="literal">freezer_create_backup_jobs</code> to
     <code class="literal">false</code>:
    </p><div class="verbatim-wrap"><pre class="screen"># If set to false, We won't create backups jobs.
freezer_create_backup_jobs: false</pre></div></li><li class="listitem "><p>
     Deploy the control plane nodes, using the values for your control plane
     node hostnames:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit \
  <em class="replaceable ">CONTROL_PLANE_HOSTNAME1</em>,<em class="replaceable ">CONTROL_PLANE_HOSTNAME2</em>, \
  <em class="replaceable ">CONTROL_PLANE_HOSTNAME3</em> -e rebuild=True</pre></div><p>
     For example, if you were using the default values from the example model
     files your command would look like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml \
    --limit ardana-ccp-c1-m1-mgmt,ardana-ccp-c1-m2-mgmt,ardana-ccp-c1-m3-mgmt \
    -e rebuild=True</pre></div><div id="id-1.6.15.5.4.4.10.4.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">-e rebuild=True</code> is only used on a single control
      plane node when there are other controllers available to pull
      configuration data
      from. This will cause the MariaDB database to be reinitialized, which is
      the only choice if there are no additional control nodes.
     </p></div></li><li class="listitem "><p>
     Restore the MariaDB backup on the first controller node.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       List the Freezer jobs:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">FIRST_CONTROLLER_NODE</em></pre></div></li><li class="listitem "><p>
       Run the <code class="literal">Ardana Default: mysql restore from SSH</code> job for your first
       controller node, replacing the <code class="literal">JOB_ID</code> for that job:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB_ID</em></pre></div></li></ol></div></li><li class="listitem "><p>
     You can monitor the restore job by connecting to your first controller
     node via SSH and running the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh <em class="replaceable ">FIRST_CONTROLLER_NODE</em>
<code class="prompt user">ardana &gt; </code>sudo su
<code class="prompt user">root # </code>tail -n 100 /var/log/freezer/freezer-scheduler.log</pre></div></li><li class="listitem "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Stop MySQL:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="listitem "><p>
     Log back in to the first controller node and move the following files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh <em class="replaceable ">FIRST_CONTROLLER_NODE</em>
<code class="prompt user">ardana &gt; </code>sudo su
<code class="prompt user">root # </code>rm -rf /var/lib/mysql/*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* /var/lib/mysql/</pre></div></li><li class="listitem "><p>
     Log back in to the Cloud Lifecycle Manager and bootstrap MySQL:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="listitem "><p>
     Verify the status of MySQL:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div></li><li class="listitem "><p>
     Re-enable the default backup job(s) by editing the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/freezer-jobs/defaults/activate.yml</pre></div><p>
     Set the value for <code class="literal">freezer_create_backup_jobs</code> to
     <code class="literal">true</code>:
    </p><div class="verbatim-wrap"><pre class="screen"># If set to false, We won't create backups jobs.
freezer_create_backup_jobs: true</pre></div></li><li class="listitem "><p>
     Run this playbook to deploy the backup jobs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div><div class="sect4" id="swiftrings-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Rings Recovery</span> <a title="Permalink" class="permalink" href="#swiftrings-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>swiftrings-recovery</li></ul></div></div></div></div><p>
  To recover your Swift rings in the event of a disaster, follow the procedure
  that applies to your situation: either recover the rings from one Swift node
  if possible, or use the SSH backup that you have set up.
 </p><p>
  To recover your Swift rings in the event of a disaster, follow the procedure
  that applies to your situation: either recover the rings from one Swift node
  if possible, or use the SSH backup that you have set up.
 </p><div class="sect5" id="id-1.6.15.5.4.4.11.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from the Swift deployment backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   See <a class="xref" href="#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
  </p></div><div class="sect5" id="id-1.6.15.5.4.4.11.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from the SSH Freezer backup</span> <a title="Permalink" class="permalink" href="#id-1.6.15.5.4.4.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In the very specific use case where you lost all system disks of all object
   nodes, and Swift proxy nodes are corrupted, you can recover the rings
   because a copy of the Swift rings is stored in Freezer. This means that
   Swift data is still there (the disks used by Swift needs to be still
   accessible).
  </p><p>
   Recover the rings with these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to a node that has the freezer-agent installed.
    </p></li><li class="listitem "><p>
     Become root:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo su</pre></div></li><li class="listitem "><p>
     Create the temporary directory to restore your files to:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /tmp/swift_builder_dir_restore/</pre></div></li><li class="listitem "><p>
     Create a restore file with the following content:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &lt;&lt; EOF &gt; ./restore_config.ini
[default]
action = restore
storage = ssh
compression = bzip2
restore_abs_path = /tmp/swift_builder_dir_restore/
ssh_key = /etc/freezer/ssh_key
ssh_host = &lt;freezer_ssh_host&gt;
ssh_port = &lt;freezer_ssh_port&gt;
ssh_user name = &lt;freezer_ssh_user name&gt;
container = &lt;freezer_ssh_base_rid&gt;/freezer_swift_backup_name = freezer_swift_builder_backup
hostname = &lt;hostname of the old first Swift-Proxy (SWF-PRX[0])&gt;
EOF</pre></div></li><li class="listitem "><p>
     Edit the file and replace all &lt;tags&gt; with the right information.
    </p><div class="verbatim-wrap"><pre class="screen">vim ./restore_config.ini</pre></div><p>
     You will also need to put the SSH key used to do the backups in
     /etc/freezer/ssh_key and remember to set the right permissions: 600.
    </p></li><li class="listitem "><p>
     Execute the restore job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>freezer-agent --config ./restore_config.ini</pre></div><p>
     You now have the Swift rings in
     <code class="literal">/tmp/swift_builder_dir_restore/</code>
    </p></li><li class="listitem "><p>
     If the SWF-PRX[0] is already deployed, copy the contents of the restored
     directory (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>) to
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code> on the SWF-PRX[0] Then from
     the Cloud Lifecycle Manager run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     If the SWF-ACC[0] is<span class="bold"><strong> not </strong></span>deployed, from
     the Cloud Lifecycle Manager run these playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;SWF-ACC[0]-hostname&gt;</pre></div></li><li class="listitem "><p>
     Copy the contents of the restored directory
     (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>) to
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code> on the SWF-ACC[0] You will
     have to create the directories :
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="listitem "><p>
     From the Cloud Lifecycle Manager, run the <code class="filename">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="unplanned-compute-maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Compute Maintenance</span> <a title="Permalink" class="permalink" href="#unplanned-compute-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute_maintenance.xml</li><li><span class="ds-label">ID: </span>unplanned-compute-maintenance</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks including recovering compute nodes.
 </p><div class="sect3" id="recover-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering a Compute Node</span> <a title="Permalink" class="permalink" href="#recover-computenode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>recover-computenode</li></ul></div></div></div></div><p>
  If one or more of your compute nodes has experienced an issue such as power
  loss or hardware failure, then you need to perform disaster recovery. Here we
  provide different scenarios and how to resolve them to get your cloud
  repaired.
 </p><p>
  Typical scenarios in which you will need to recover a compute node include
  the following:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The node has failed, either because it has shut down has a hardware
    failure, or for another reason.
   </p></li><li class="listitem "><p>
    The node is working but the <code class="literal">nova-compute</code> process is not
    responding, thus instances are working but you cannot manage them (for
    example to delete, reboot, and attach/detach volumes).
   </p></li><li class="listitem "><p>
    The node is fully operational but monitoring indicates a potential issue
    (such as disk errors) that require down time to fix.
   </p></li></ul></div><div class="sect4" id="idg-all-operations-maintenance-compute-recover-compute-node-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What to do if your compute node is down</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-recover-compute-node-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-recover-compute-node-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Compute node has power but is not powered
   on</strong></span>
  </p><p>
   If your compute node has power but is not powered on, use these steps to
   restore the node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Obtain the name for your compute node in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Compute node is powered on but services are not
   running on it</strong></span>
  </p><p>
   If your compute node is powered on but you are unsure if services are
   running, you can use these steps to ensure that they are running:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Confirm the status of the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-status.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     You can start the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div><div class="sect4" id="unplanned"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scenarios involving disk failures on your compute nodes</span> <a title="Permalink" class="permalink" href="#unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>unplanned</li></ul></div></div></div></div><p>
   Your compute nodes should have a minimum of two disks, one that is used for
   the operating system and one that is used as the data disk. These are
   defined during the installation of your cloud, in the
   <code class="literal">~/openstack/my_cloud/definition/data/disks_compute.yml</code> file
   on the Cloud Lifecycle Manager. The data disk(s) are where the
   <code class="literal">nova-compute</code> service lives. Recovery scenarios will
   depend on whether one or the other, or both, of these disks experienced
   failures.
  </p><p>
   <span class="bold"><strong>If your operating system disk failed but the data
   disk(s) are okay</strong></span>
  </p><p>
   If you have had issues with the physical volume that nodes your operating
   system you need to ensure that your physical volume is restored and then you
   can use the following steps to restore the operating system:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="listitem "><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-list | grep compute</pre></div></li><li class="listitem "><p>
     Obtain the status of the <code class="literal">nova-compute</code> service on that
     node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list --host &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     You will likely want to disable provisioning on that node to ensure that
     <code class="literal">nova-scheduler</code> does not attempt to place any additional
     instances on the node while you are repairing it:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "node is being rebuilt" &lt;hostname&gt; nova-compute</pre></div></li><li class="listitem "><p>
     Obtain the status of the instances on the compute node:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
     Before continuing, you should either evacuate all of the instances off
     your compute node or shut them down. If the instances are booted from
     volumes, then you can use the <code class="literal">nova evacuate</code> or
     <code class="literal">nova host-evacuate</code> commands to do this. See
     <a class="xref" href="#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a> for more details on how to do this.
    </p><p>
     If your instances are not booted from volumes, you will need to stop the
     instances using the <code class="literal">nova stop</code> command. Because the
     <code class="literal">nova-compute</code> service is not running on the node you
     will not see the instance status change, but the <code class="literal">Task
     State</code> for the instance should change to
     <code class="literal">powering-off</code>.
    </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance_uuid&gt;</pre></div><p>
     Verify the status of each of the instances using these commands, verifying
     the <code class="literal">Task State</code> states <code class="literal">powering-off</code>:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova show &lt;instance_uuid&gt;</pre></div></li><li class="listitem "><p>
     At this point you should be ready with a functioning hard disk in the node
     that you can use for the operating system. Follow these steps:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Obtain the name for your compute node in Cobbler, which you will use in
       subsequent commands when <code class="literal">&lt;node_name&gt;</code> is
       requested:
      </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
       Reimage the compute node with this playbook:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div></li><li class="listitem "><p>
     Once reimaging is complete, use the following playbook to configure the
     operating system and start up services:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     You should then ensure any instances on the recovered node are in an
     <code class="literal">ACTIVE</code> state. If they are not then use the
     <code class="literal">nova start</code> command to bring them to the
     <code class="literal">ACTIVE</code> state:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova start &lt;instance_uuid&gt;</pre></div></li><li class="listitem "><p>
     Reenable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;hostname&gt; nova-compute</pre></div></li><li class="listitem "><p>
     Start any instances that you had stopped previously:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova start &lt;instance_uuid&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>If your data disk(s) failed but the operating system
   disk is okay OR if all drives failed</strong></span>
  </p><p>
   In this scenario your instances on the node are lost. First, follow steps 1
   to 5 and 8 to 9 in the previous scenario.
  </p><p>
   After that is complete, use the <code class="literal">nova rebuild</code> command to
   respawn your instances, which will also ensure that they receive the same IP
   address:
  </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova rebuild &lt;instance_uuid&gt;</pre></div></div></div></div><div class="sect2" id="storage-unplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Storage Maintenance</span> <a title="Permalink" class="permalink" href="#storage-unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-storage_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-storage_unplanned.xml</li><li><span class="ds-label">ID: </span>storage-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for storage nodes.
 </p><div class="sect3" id="swift-storage-unplanned"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Swift Storage Maintenance</span> <a title="Permalink" class="permalink" href="#swift-storage-unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-swift_storage_unplanned.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-swift_storage_unplanned.xml</li><li><span class="ds-label">ID: </span>swift-storage-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for Swift storage nodes.
 </p><div class="sect4" id="recover-swiftnode"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering a Swift Node</span> <a title="Permalink" class="permalink" href="#recover-swiftnode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>recover-swiftnode</li></ul></div></div></div></div><p>
  If one or more of your Swift Object or PAC nodes has experienced an issue,
  such as power loss or hardware failure, and you need to perform disaster
  recovery then we provide different scenarios and how to resolve them to get
  your cloud repaired.
 </p><p>
  Typical scenarios in which you will need to repair a Swift object or PAC
  node include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The node has either shut down or been rebooted.
   </p></li><li class="listitem "><p>
    The entire node has failed and needs to be replaced.
   </p></li><li class="listitem "><p>
    A disk drive has failed and must be replaced.
   </p></li></ul></div><div class="sect5" id="idg-all-operations-maintenance-swift-recover-swift-node-xml-5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.4.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What to do if your Swift host has shut down or rebooted</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-swift-recover-swift-node-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-recover-swift-node-xml-5</li></ul></div></div></div></div><p>
   If your Swift host has power but is not powered on, from the lifecycle
   manager you can run this playbook:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Obtain the name for your Swift host in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div><p>
   Once the node is booted up, Swift should start automatically. You can verify
   this with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div><p>
   Any alarms that have triggered due to the host going down should clear
   within 10 minutes. See <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> if further
   assistance is needed with the alarms.
  </p></div><div class="sect5" id="replace"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.4.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace your Swift node</span> <a title="Permalink" class="permalink" href="#replace">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>replace</li></ul></div></div></div></div><p>
   If your Swift node has irreparable damage and you need to replace the entire
   node in your environment, see <a class="xref" href="#replace-swift-node" title="13.1.5.1.5. Replacing a Swift Node">Section 13.1.5.1.5, “Replacing a Swift Node”</a> for
   details on how to do this.
  </p></div><div class="sect5" id="disk-replacement"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.4.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace a hard disk in your Swift node</span> <a title="Permalink" class="permalink" href="#disk-replacement">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>disk-replacement</li></ul></div></div></div></div><p>
   If you need to do a hard drive replacement in your Swift node, see
   <a class="xref" href="#replacing-disks" title="13.1.5.1.6. Replacing Drives in a Swift Node">Section 13.1.5.1.6, “Replacing Drives in a Swift Node”</a> for details on how to do this.
  </p></div></div></div></div></div><div class="sect1" id="maintenance-update"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span> <a title="Permalink" class="permalink" href="#maintenance-update">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span>maintenance-update</li></ul></div></div></div></div><div class="procedure " id="id-1.6.15.6.2"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 13.2: </span><span class="name">Preparing for Update </span><a title="Permalink" class="permalink" href="#id-1.6.15.6.2">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Ensure that the update repositories have been properly set up on all nodes.
    The easiest way to provide the required repositories on the Cloud Lifecycle Manager Server is
    to set up an SMT server as described in
    <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 4 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”</span>. Alternatives to setting up an
    SMT server are described in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 5 “Software Repository Setup”</span>.
   </p></li><li class="step "><p>
    Read the Release Notes for the security and maintenance updates that will
    be installed.
   </p></li><li class="step "><p>
    Have a backup strategy in place. For further information, see
    <a class="xref" href="#bura-overview" title="Chapter 14. Backup and Restore">Chapter 14, <em>Backup and Restore</em></a>.
   </p></li><li class="step "><p>
    Ensure that you have a known starting state by resolving any unexpected
    alarms.
   </p></li><li class="step "><p>
    Determine if you need to reboot your cloud after updating the software.
    Rebooting is highly recommended to ensure that all affected services are
    restarted. Reboot may be required after installing Linux kernel updates,
    but it can be skipped if the impact on running services is non-existent or
    well understood.
   </p></li><li class="step "><p>
    Review steps in <a class="xref" href="#add-network-node" title="13.1.4.1. Adding a Neutron Network Node">Section 13.1.4.1, “Adding a Neutron Network Node”</a> and
    <a class="xref" href="#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a> to minimize the impact on existing
    workloads. These steps are critical when the Neutron services are not
    provided via external SDN controllers.
   </p></li><li class="step "><p>
    Before the update, prepare your working loads by consolidating all of your
    instances to one or more Compute Nodes. After the update is complete on the
    324
    evacuated Compute Nodes, reboot them and move the images from the remaining
    Compute Nodes to the newly booted ones. Then, update the remaining
    Compute Nodes.
   </p></li></ol></div></div><div class="sect2" id="perform-update"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing the Update</span> <a title="Permalink" class="permalink" href="#perform-update">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span>perform-update</li></ul></div></div></div></div><p>
     Before you proceed, get the status of all your services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div><p>
     If status check returns an error for a specific service, run the
     <code class="filename"><em class="replaceable ">SERVICE</em>-reconfigure.yml</code>
     playbook. Then run the
     <code class="filename"><em class="replaceable ">SERVICE</em>-status.yml</code>
     playbook to check that the issue has been resolved.
    </p><p>
     Update and reboot all nodes in the cloud one by one. Start with the
     deployer node, then follow the order recommended in
     <a class="xref" href="#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>.
    </p><div id="id-1.6.15.6.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The described workflow also covers cases in which the deployer node is
      also provisioned as an active cloud node.
     </p></div><p>
     To minimize the impact on the existing workloads, the node should first be
     prepared for an update and a subsequent reboot by following the steps
     leading up to stopping services listed in
     <a class="xref" href="#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>, such as migrating singleton agents on
     Control Nodes and evacuating Compute Nodes. Do not stop services running on
     the node, as they need to be running during the update.
    </p><div class="procedure " id="id-1.6.15.6.3.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 13.3: </span><span class="name">Update Instructions </span><a title="Permalink" class="permalink" href="#id-1.6.15.6.3.8">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Install all available security and maintenance updates on the deployer
       using the <code class="command">zypper patch</code> command.
      </p></li><li class="step "><p>
       Initialize the Cloud Lifecycle Manager and prepare the update playbooks.
      </p><ol type="a" class="substeps "><li class="step "><p>
         Run the <code class="systemitem">ardana-init</code> initialization script to
         update the deployer.
        </p></li><li class="step "><p>
         Redeploy cobbler:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
         Run the configuration processor:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
         Update your deployment directory:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step "><p>
       Installation and management of updates can be automated with the
       following playbooks:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <code class="filename">ardana-update-pkgs.yml</code>
        </p></li><li class="listitem "><p>
         <code class="filename">ardana-update.yml</code>
        </p></li><li class="listitem "><p>
         <code class="filename">ardana-update-status.yml</code>
        </p><div id="id-1.6.15.6.3.8.4.2.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
          Some playbooks are being deprecated. To determine how your system is
          affected, run:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>rpm -qa ardana-ansible</pre></div><p>
          The result will be <code class="literal">ardana-ansible-8.0+git.</code>
          followed by a version number string.
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            If the first part of the version number string is greater than or
            equal to 1553878455 (for example, ardana-ansible-8.0+git.<span class="bold"><strong>1553878455</strong></span>.7439e04), use the newly
            introduced parameters:
           </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <code class="literal">pending_clm_update</code>
             </p></li><li class="listitem "><p>
              <code class="literal">pending_service_update</code>
             </p></li><li class="listitem "><p>
              <code class="literal">pending_system_reboot</code>
             </p></li></ul></div></li><li class="listitem "><p>
            If the first part of the version number string is less than 1553878455
            (for example, ardana-ansible-8.0+git.<span class="bold"><strong>1552032267</strong></span>.5298d45), use the following
            parameters:
           </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <code class="literal">update_status_var</code>
             </p></li><li class="listitem "><p>
              <code class="literal">update_status_set</code>
             </p></li><li class="listitem "><p>
              <code class="literal">update_status_reset</code>
             </p></li></ul></div></li></ul></div></div></li><li class="listitem "><p>
         <code class="filename">ardana-reboot.yml</code>
        </p></li></ul></div></li><li class="step "><p>
       Confirm version changes by running <code class="literal">hostnamectl</code>
       before and after running the <code class="literal">ardana-update-pkgs</code>
       playbook on each node.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>hostnamectl</pre></div><p>
       Notice that the <code class="literal">Boot ID:</code> and
       <code class="literal">Kernel:</code> information has changed.
      </p></li><li class="step "><p>
       By default, the <code class="filename">ardana-update-pkgs.yml</code> playbook
       will install patches and updates that do not require a system
       reboot. Patches and updates that <span class="bold"><strong>do</strong></span>
       require a system reboot will be installed later in this process.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div><p>
       There may be a delay in the playbook output at the following task while
       updates are pulled from the deployer.
      </p><div class="verbatim-wrap"><pre class="screen">TASK: [ardana-upgrade-tools | pkg-update | Download and install
package updates] ***</pre></div></li><li class="step "><p>
       After running the <code class="filename">ardana-update-pkgs.yml</code> playbook
       to install patches and updates not requiring reboot, check the status of
       remaining tasks.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       To install patches that require reboot, run the
       <code class="filename">ardana-update-pkgs.yml</code> playbook with the parameter
       <code class="literal">-e zypper_update_include_reboot_patches=true</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit  <em class="replaceable ">TARGET_NODE_NAME</em> \
-e zypper_update_include_reboot_patches=true</pre></div><p>
	If the output of <code class="filename">ardana-update-pkgs.yml</code> indicates 
	that a reboot is required, run <code class="filename">ardana-reboot.yml</code> 
	<span class="emphasis"><em>after</em></span> completing the <code class="filename">ardana-update.yml</code> 
	step below. Running <code class="filename">ardana-reboot.yml</code>
	will cause cloud service interruption.
      </p><div id="id-1.6.15.6.3.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        To update a single package (for example, apply a PTF on a single node
        or on all nodes), run <code class="command">zypper update
        <em class="replaceable ">PACKAGE</em></code>.
       </p><p>
        To install all package updates using <code class="command">zypper update</code>.
       </p></div></li><li class="step "><p>
       Update services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       If indicated by the <code class="filename">ardana-update-status.yml</code>
       playbook, reboot the node.
      </p><p>
       There may also be a warning to reboot after running the
       <code class="filename">ardana-update-pkgs.yml</code>.
      </p><p>
       This check can be overridden by setting the
       <code class="literal">SKIP_UPDATE_REBOOT_CHECKS</code> environment variable or the
       <code class="literal">skip_update_reboot_checks</code> Ansible variable.
        </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-reboot.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       To recheck pending system reboot status at a later time, run the
       following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2</pre></div></li><li class="step "><p>
       The pending system reboot status can be reset by running:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2 \
-e pending_system_reboot=off</pre></div></li><li class="step "><p>
       Multiple servers can be patched at the same time with
       <code class="filename">ardana-update-pkgs.yml</code> by setting the option
       <code class="literal">-e skip_single_host_checks=true</code>.
      </p><div id="id-1.6.15.6.3.8.13.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
        When patching multiple servers at the same time, take care not to
        compromise HA capability by updating an entire cluster (controller,
        database, monitor, logging) at the same time.
       </p></div><p>
       If multiple nodes are specified on the command line (with
       <code class="literal">--limit</code>), services on those servers will experience
       outages as the packages are shutdown and updated.  On Compute Nodes (or
       group of Compute Nodes) migrate the workload off if you plan to update
       it. The same applies to Control Nodes: move singleton services off of the
       control plane node that will be updated.
      </p><div id="id-1.6.15.6.3.8.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
        Do not reboot all of your controllers at the same time.
       </p></div></li><li class="step "><p>
       When the node comes up after the reboot, run the
      <code class="filename">spark-start.yml</code> file:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml</pre></div></li><li class="step "><p>
       Verify that Spark is running on all Control Nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-status.yml</pre></div></li><li class="step "><p>
       After all nodes have been updated, check the status of all services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.6.15.6.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Summary of the Update Playbooks</span> <a title="Permalink" class="permalink" href="#id-1.6.15.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.15.6.4.2.1"><span class="term ">ardana-update-pkgs.yml</span></dt><dd><p>
      Top-level playbook automates the installation of package updates on
      a single node. It also works for multiple nodes, if the single-node
      restriction is overridden by setting the SKIP_SINGLE_HOST_CHECKS
      environment variable <code class="literal">ardana-update-pkgs.yml -e
      skip_single_host_checks=true</code>.
     </p><p>
      Provide the following <code class="literal">-e</code> options to modify default
      behavior:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">zypper_update_method</code> (default: patch)
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">patch</code> will install all patches for the
          system. Patches are intended for specific bug and security fixes.
         </p></li><li class="listitem "><p>
          <code class="literal">update</code> will install all packages that have a
          higher version number than the installed packages.
         </p></li><li class="listitem "><p>
          <code class="literal">dist-upgrade</code> replaces each package installed with
          the version from the repository and deletes packages not available in
          the repositories.
         </p></li></ul></div></li><li class="listitem "><p>
        <code class="literal">zypper_update_repositories</code> (default: all) restricts
        the list of repositories used
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_gpg_checks</code> (default: true) enables GPG
        checks. If set to <code class="literal">true</code>, checks if packages are
        correctly signed.
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_licenses_agree</code> (default: false)
        automatically agrees with licenses. If set to <code class="literal">true</code>,
        zypper automatically accepts third party licenses.
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_include_reboot_patches</code> (default:
        false) includes patches that require reboot. Setting this to
        <code class="literal">true</code> installs patches that require a reboot (such as
        kernel or glibc updates).
       </p></li></ul></div></dd><dt id="id-1.6.15.6.4.2.2"><span class="term ">ardana-update.yml</span></dt><dd><p>
      Top level playbook that automates the update of all the services. Runs
      on all nodes by default, or can be limited to a single node by adding
      <code class="literal">--limit <em class="replaceable ">nodename</em></code>.
     </p></dd><dt id="id-1.6.15.6.4.2.3"><span class="term ">ardana-reboot.yml</span></dt><dd><p>
      Top-level playbook that automates the steps required to reboot a node. It
      includes pre-boot and post-boot phases, which can be extended to include
      additional checks.
     </p></dd><dt id="id-1.6.15.6.4.2.4"><span class="term ">ardana-update-status.yml</span></dt><dd><p>
      This playbook can be used to check or reset the update-related status
      variables maintained by the update playbooks. The main reason for having
      this mechanism is to allow the update status to be checked at any point
      during the update procedure. It is also used heavily by the automation
      scripts to orchestrate installing maintenance updates on multiple nodes.
     </p></dd></dl></div></div></div><div class="sect1" id="deploy-ptf"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span> <a title="Permalink" class="permalink" href="#deploy-ptf">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-deploy-ptf.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-deploy-ptf.xml</li><li><span class="ds-label">ID: </span>deploy-ptf</li></ul></div></div></div></div><p>
  Occasionally, in order to fix a given issue, SUSE will provide a set of
  packages known as a Program Temporary Fix (PTF). Such a PTF is fully
  supported by SUSE until the Maintenance Update containing a permanent fix has
  been released via the regular Update repositories. Customers running PTF
  fixes will be notified through the related Service Request when a permanent
  patch for a PTF has been released.
 </p><p>
  Use the following steps to deploy a PTF:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    When SUSE has developed a PTF, you will receive a URL for that PTF. You
    should download the packages from the location provided by SUSE Support
    to a temporary location on the Cloud Lifecycle Manager. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>tmpdir=`mktemp -d`
<code class="prompt user">ardana &gt; </code>cd $tmpdir
<code class="prompt user">ardana &gt; </code>sudo wget --no-directories --recursive --reject "index.html*"\
--user=<em class="replaceable ">USER_NAME</em> \
--password=<em class="replaceable ">PASSWORD</em> \
--no-parent https://ptf.suse.com/54321aaaa...dddd12345/cloud8/042171/x86_64/20181030</pre></div></li><li class="step "><p>
    Remove any old data from the PTF repository, such as a listing for a PTF
    repository from a migration or when previous product patches were
    installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -rf /srv/www/suse-12.3/x86_64/repos/PTF/*</pre></div></li><li class="step "><p>
    Move packages from the temporary download location to the PTF repository
    directory on the CLM Server. This example is for a Neutron PTF.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo mkdir -p /srv/www/suse-12.3/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo mv $tmpdir/*
   /srv/www/suse-12.3/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo chown --recursive root:root /srv/www/suse-12.3/x86_64/repos/PTF/*
<code class="prompt user">ardana &gt; </code>rmdir $tmpdir</pre></div></li><li class="step "><p>
    Create or update the repository metadata:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo /usr/local/sbin/createrepo-cloud-ptf
Spawning worker 0 with 2 pkgs
Workers Finished
Saving Primary metadata
Saving file lists metadata
Saving other metadata</pre></div></li><li class="step "><p>
    Refresh the PTF repository before installing package updates on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper refresh --force --repo PTF
Forcing raw metadata refresh
Retrieving repository 'PTF' metadata
..........................................[d
one]
Forcing building of repository cache
Building repository 'PTF' cache ..........................................[done]
Specified repositories have been refreshed.</pre></div></li><li class="step "><p>
    The PTF shows as available on the deployer.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --repo PTF
Loading repository data...
Reading installed packages...

S | Name                          | Summary                                 | Type
--+-------------------------------+-----------------------------------------+--------
  | python-neutronclient          | Python API and CLI for OpenStack Neutron | package
i | venv-openstack-neutron-x86_64 | Python virtualenv for OpenStack Neutron | package</pre></div></li><li class="step "><p>
    Install the PTF venv packages on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper dup  --from PTF
Refreshing service
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

The following package is going to be upgraded:
  venv-openstack-neutron-x86_64

The following package has no support information from its vendor:
  venv-openstack-neutron-x86_64

1 package to upgrade.
Overall download size: 64.2 MiB. Already cached: 0 B. After the operation, additional 6.9 KiB will be used.
Continue? [y/n/...? shows all options] (y): y
Retrieving package venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ... (1/1),  64.2 MiB ( 64.6 MiB unpacked)
Retrieving: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm ....[done]
Checking for file conflicts: ..............................................................[done]
(1/1) Installing: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ....[done]
Additional rpm output:
warning
warning: /var/cache/zypp/packages/PTF/noarch/venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm: Header V3 DSA/SHA1 Signature, key ID b37b98a9: NOKEY</pre></div></li><li class="step "><p>
    Validate the venv tarball has been installed into the deployment directory:(note:the packages file under that dir shows the registered tarballs that will be used for the services, which should align with the installed venv RPM)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/ardana_packager/ardana-8/sles_venv/x86_64
total 898952
drwxr-xr-x 2 root root     4096 Oct 30 16:10 .
...
-rw-r--r-- 1 root root 67688160 Oct 30 12:44 neutron-20181030T124310Z.tgz &lt;&lt;&lt;
-rw-r--r-- 1 root root 64674087 Aug 14 16:14 nova-20180814T161306Z.tgz
-rw-r--r-- 1 root root 45378897 Aug 14 16:09 octavia-20180814T160839Z.tgz
-rw-r--r-- 1 root root     1879 Oct 30 16:10 packages
-rw-r--r-- 1 root root 27186008 Apr 26  2018 swift-20180426T230541Z.tgz</pre></div></li><li class="step "><p>
    Install the non-venv PTF packages on the Compute Node
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml --extra-vars '{"zypper_update_method": "update", "zypper_update_repositories": ["PTF"]}' --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that the upgraded package
    has been installed on <code class="literal">comp0001-mgmt</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --detail python-neutronclient
Loading repository data...
Reading installed packages...

S | Name                 | Type     | Version                         | Arch   | Repository
--+----------------------+----------+---------------------------------+--------+--------------------------------------
i | python-neutronclient | package  | 6.5.1-4.361.042171.0.PTF.102473 | noarch | PTF
  | python-neutronclient | package  | 6.5.0-4.361                     | noarch | SUSE-OPENSTACK-CLOUD-x86_64-GM-DVD1</pre></div></li><li class="step "><p>
    Running the ardana update playbook will distribute the PTF venv packages to
    the cloud server. Then you can find them loaded in the virtual environment
    directory with the other venvs.
   </p><p>
    The Compute Node before running the update playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 24
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step "><p>
    Run the update.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that an additional virtual environment
    has been installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 28
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x  9 root root 4096 Oct 30 12:43 neutron-20181030T124310Z &lt;&lt;&lt; New venv installed
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step "><p>
    The PTF may also have <code class="literal">RPM</code> package updates in addition to
    venv updates. To complete the update, follow the instructions at <a class="xref" href="#perform-update" title="13.3.1. Performing the Update">Section 13.3.1, “Performing the Update”</a>.
   </p></li></ol></div></div></div><div class="sect1" id="database-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Periodic OpenStack Maintenance Tasks</span> <a title="Permalink" class="permalink" href="#database-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-maintenance-database_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-database_maintenance.xml</li><li><span class="ds-label">ID: </span>database-maintenance</li></ul></div></div></div></div><p>
    Heat-manage helps manage Heat specific database operations. The associated
    database should be periodically purged to save space. The following should
    be setup as a cron job on the servers where the heat service is running at
    <code class="literal">/etc/cron.weekly/local-cleanup-heat</code>
    with the following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su heat -s /bin/bash -c "/usr/bin/heat-manage purge_deleted -g days 14" || :</pre></div><p>
     nova-manage db archive_deleted_rows command will move deleted rows
     from production tables to shadow tables. Including
     <code class="literal">--until-complete</code> will make the command run continuously
     until all deleted rows are archived. It is recommended to setup this task
     as <code class="literal">/etc/cron.weekly/local-cleanup-nova</code>
     on the servers where the nova service is running, with the
     following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su nova -s /bin/bash -c "/usr/bin/nova-manage db archive_deleted_rows --until-complete" || :</pre></div></div></div><div class="chapter " id="bura-overview"><div class="titlepage"><div><div><h1 class="title"><span class="number">14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup and Restore</span> <a title="Permalink" class="permalink" href="#bura-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-bura_overview.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-bura_overview.xml</li><li><span class="ds-label">ID: </span>bura-overview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.6.16.10"><span class="number">14.1 </span><span class="name">Architecture</span></a></span></dt><dt><span class="section"><a href="#idg-all-bura-bura-overview-xml-9"><span class="number">14.2 </span><span class="name">Architecture of the Backup/Restore Service</span></a></span></dt><dt><span class="section"><a href="#topic-bxm-gxr-st"><span class="number">14.3 </span><span class="name">Default Automatic Backup Jobs</span></a></span></dt><dt><span class="section"><a href="#topic-jsc-qps-qt"><span class="number">14.4 </span><span class="name">Enabling Default Backups of the Control Plane to an SSH Target</span></a></span></dt><dt><span class="section"><a href="#topic-pth-st3-mw"><span class="number">14.5 </span><span class="name">Changing Default Jobs</span></a></span></dt><dt><span class="section"><a href="#freezerUI"><span class="number">14.6 </span><span class="name">Backup/Restore Via the Horizon UI</span></a></span></dt><dt><span class="section"><a href="#previous-backups"><span class="number">14.7 </span><span class="name">Restore from a Specific Backup</span></a></span></dt><dt><span class="section"><a href="#topic-mlh-wtn-rt"><span class="number">14.8 </span><span class="name">Backup/Restore Scheduler</span></a></span></dt><dt><span class="section"><a href="#topic-pgq-mnw-dt"><span class="number">14.9 </span><span class="name">Backup/Restore Agent</span></a></span></dt><dt><span class="section"><a href="#bu-limitations"><span class="number">14.10 </span><span class="name">Backup and Restore Limitations</span></a></span></dt><dt><span class="section"><a href="#topic-i4l-xhn-tt"><span class="number">14.11 </span><span class="name">Disabling Backup/Restore before Deployment</span></a></span></dt><dt><span class="section"><a href="#topic-dsm-fbs-st"><span class="number">14.12 </span><span class="name">Enabling, Disabling and Restoring Backup/Restore Services</span></a></span></dt><dt><span class="section"><a href="#backup-audit-logs"><span class="number">14.13 </span><span class="name">Backing up and Restoring Audit Logs</span></a></span></dt></dl></div></div><p>
  Information about how to back up and restore your cloud.
 </p><p>
  Freezer is a Backup and Restore as a Service platform that helps you automate
  the backup and restore process for your data. This backup and restore
  component (Freezer) executes backups and restores as jobs, and executes these
  jobs independently and/or as managed sessions (multiple jobs in multiple
  machines sharing a state).
 </p><p>
  There are a number of things you must do before installing your cloud so that
  you achieve the backup plan you need.
 </p><p>
  First, you should consider <a class="xref" href="#topic-jsc-qps-qt" title="14.4. Enabling Default Backups of the Control Plane to an SSH Target">Section 14.4, “Enabling Default Backups of the Control Plane to an SSH Target”</a> in case you
  lose cloud servers that the Freezer backup and restore service uses by
  default.
 </p><p>
  Second, you can prevent the Freezer backup and restore service from being
  installed completely, or designate which services should be backed up by
  default. <a class="xref" href="#topic-i4l-xhn-tt" title="14.11. Disabling Backup/Restore before Deployment">Section 14.11, “Disabling Backup/Restore before Deployment”</a>.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports backup and restore of control plane services. It
  comes with playbooks and procedures to recover the control plane from various
  disaster scenarios.
 </p><p>
  The following features are supported:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Backup of your file system using a point-in-time snapshot.
   </p></li><li class="listitem "><p>
    Strong encryption: AES-256-CFB.
   </p></li><li class="listitem "><p>
    Backup of your MySQL database with LVM snapshot.
   </p></li><li class="listitem "><p>
    Restoring your data from a specific date automatically to your file system.
   </p></li><li class="listitem "><p>
    Low storage consumption: the backups are uploaded as a stream.
   </p></li><li class="listitem "><p>
    Flexible backup policy (both incremental and differential).
   </p></li><li class="listitem "><p>
    Data archived in GNU Tar format for file-based incremental.
   </p></li><li class="listitem "><p>
    Multiple compression algorithm support (zlib, bzip2, xz).
   </p></li><li class="listitem "><p>
    Removal old backup automatically according the provided parameters.
   </p></li><li class="listitem "><p>
    Multiple storage media support (Swift, local file system, SSH).
   </p></li><li class="listitem "><p>
    Management of multiple jobs (multiple backups on the same node).
   </p></li><li class="listitem "><p>
    Synchronization of backup and restore on multiple nodes.
   </p></li><li class="listitem "><p>
    Execution of scripts/commands before or after a job execution.
   </p></li></ul></div><div class="sect1" id="id-1.6.16.10"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Architecture</span> <a title="Permalink" class="permalink" href="#id-1.6.16.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-bura_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-bura_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The backup and restore service/Freezer uses GNU Tar under the hood to
   execute incremental backup and restore. When a key is provided, it uses Open
   SSL to encrypt data (AES-256-CFB).
  </p><p>
   The architecture consists of the following components:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="123pt" align="left" class="c1" /><col width="372.75pt" align="left" class="c2" /></colgroup><thead><tr><th align="left">Component</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">Freezer Scheduler</td><td align="left">
       <p>
        A client-side component running on the node from where the data backup
        is executed. It consists of a daemon that retrieves the data from the
        freezer API and executes jobs (that is, backups, restore, admin
        actions, info actions, and pre- and/or post- job scripts) by running
        the Freezer Agent. The metrics and exit codes returned by the Freezer
        Agent are captured and sent to the Freezer API.
       </p>
       <p>
        The scheduler manages the execution and synchronization of multiple
        jobs executed on a single node or multiple nodes. The status of the
        execution of all the nodes is saved through the API.
       </p>
       <p>
        The Freezer scheduler takes care of uploading jobs to the API by
        reading job files on the file system. It also has its own configuration
        file where job sessions or other settings such as the Freezer API
        polling interval can be configured.
       </p>
      </td></tr><tr><td align="left">Freezer Agent </td><td align="left">
       <p>
        Multiprocessing Python software that runs on the client side where the
        data backup is executed. It can be executed as a standalone or by the
        Freezer Scheduler. The freezer-agent provides a flexible way to execute
        backup, restore, and perform other actions on a running system.
       </p>
       <p>
        To provide flexibility in terms of data integrity, speed, performance,
        resource usage, and so on, the Freezer Agent offers a wide range of
        options to execute optimized backup according the available resources,
        such as:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Segments size (the amount of memory used)
         </p></li><li class="listitem "><p>
          Queues size (optimize backups where I/O, bandwidth, memory, or CPU is
          a constraint)
         </p></li><li class="listitem "><p>
          I/O affinity and process priority (can be used with real time I/O and
          maximum user-level process priority)
         </p></li><li class="listitem "><p>
          Bandwidth limitation
         </p></li><li class="listitem "><p>
          Client-side Encryption (AES-256-CFB)
         </p></li><li class="listitem "><p>
          Compression (multiple algorithms supported as zlib, bzip2, xz/lzma)
         </p></li><li class="listitem "><p>
          Parallel upload to pluggable storage media (that is, upload backup to
          Swift and to a remote node by SSH, or upload to two or more
          independent Swift instances with different credentials, and so on)
         </p></li><li class="listitem "><p>
          Execute file-based incremental (such as tar), block-based incremental
          (such as rsync algorithm), and differential-based backup and restore
         </p></li><li class="listitem "><p>
          Multi platform: you can run it on SUSE Linux, Windows, *BSD, and OSX
         </p></li><li class="listitem "><p>
          Automatic removal of old backups
         </p></li></ul></div>
      </td></tr><tr><td align="left">Freezer API </td><td align="left">Stores and provides metadata to the  Freezer Scheduler. Also stores session
       information for multi node backup synchronization. Workload data is not stored in the API .
      </td></tr><tr><td align="left">DB Elasticsearch</td><td align="left">API uses the backend to store and retrieve metrics metadata sessions information job
       status, and so on.</td></tr></tbody></table></div></div><div class="sect1" id="idg-all-bura-bura-overview-xml-9"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Architecture of the Backup/Restore Service</span> <a title="Permalink" class="permalink" href="#idg-all-bura-bura-overview-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-bura_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-bura_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-bura-bura-overview-xml-9</li></ul></div></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-bura-bura_architecture.png" target="_blank"><img src="images/media-bura-bura_architecture.png" width="" /></a></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component</th><th>Description</th><th>Runs on</th></tr></thead><tbody><tr><td>API</td><td>API service to add / fetch Freezer jobs</td><td>Controller nodes with Elasticsearch</td></tr><tr><td>Scheduler</td><td>Daemon that stores and retrieves backup/restore jobs and executes them</td><td>Nodes needing backup/restore (controllers, Cloud Lifecycle Manager)</td></tr><tr><td>Agent</td><td>The agent that backs up and restores to and from targets. Invoked from scheduler or
       manually.</td><td>Nodes needing backup/restore (controllers, Cloud Lifecycle Manager)</td></tr></tbody></table></div></div><div class="sect1" id="topic-bxm-gxr-st"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Default Automatic Backup Jobs</span> <a title="Permalink" class="permalink" href="#topic-bxm-gxr-st">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-supported_services.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-supported_services.xml</li><li><span class="ds-label">ID: </span>topic-bxm-gxr-st</li></ul></div></div></div></div><p>
  By default, the following are <span class="bold"><strong>automatically</strong></span>
  backed up. You do not have to do anything for these backup jobs to run.
  However if you want to back up to somewhere outside the cluster, you do need
  to <a class="xref" href="#topic-jsc-qps-qt" title="14.4. Enabling Default Backups of the Control Plane to an SSH Target">Section 14.4, “Enabling Default Backups of the Control Plane to an SSH Target”</a>.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Cloud Lifecycle Manager Data. </span>
     All important information on the Cloud Lifecycle Manager
    </p></li><li class="listitem "><p><span class="formalpara-title">MariaDB Database. </span>
     The MariaDB database contains most of the data needed to restore
     services. While the MariaDB database only allows for an incomplete
     recovery of ESX data, for other services it allows full recovery.
     Logging data in Elasticsearch is not backed up. Swift objects are
     not backed up because of the redundant nature of Swift.
    </p></li><li class="listitem "><p><span class="formalpara-title">Swift Rings. </span>
     Swift rings are backed up so that you can recover more quickly even
     though Swift can rebuild the rings without this data. However
     automatically rebuilding the rings is slower than restoring via a backup.
    </p></li></ul></div><p>
  The following services will be effectively backed up. In other words, the
  data needed to restore the services is backed up. The critical data that will
  be backed up are the databases and the configuration-related files. Note the
  data that is not backed up per service:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Ceilometer. However, there is no backup of metrics data
   </p></li><li class="listitem "><p>
    Cinder. However, there is no backup of the volumes
   </p></li><li class="listitem "><p>
    Glance. However, there is no backup of the images
   </p></li><li class="listitem "><p>
    Heat
   </p></li><li class="listitem "><p>
    Horizon
   </p></li><li class="listitem "><p>
    Keystone
   </p></li><li class="listitem "><p>
    Neutron
   </p></li><li class="listitem "><p>
    Nova. However, there is no backup of the images
   </p></li><li class="listitem "><p>
    Swift. However, there is no backup of the objects. Swift has its own high
    availability/redundancy. Swift rings are backed up. Although Swift will
    rebuild the rings itself, restoring from backup is faster.
   </p></li><li class="listitem "><p>
    Operations Console
   </p></li><li class="listitem "><p>
    Monasca. However, there is no backup of the metrics
   </p></li></ul></div><div class="sect2" id="freezer-backup-limitations"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#freezer-backup-limitations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-supported_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-supported_services.xml</li><li><span class="ds-label">ID: </span>freezer-backup-limitations</li></ul></div></div></div></div><p>
   The following limitations apply to backups created by the Freezer backup and
   restore service in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Recovery of the following services (or cloud topologies) will be partially
     backed up. They will need additional data (other than the data stored in
     MariaDB) to return to fully functional.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       ESX Cloud
      </p></li><li class="listitem "><p>
       Network services - LBaaS and VPNaaS
      </p></li></ul></div></li><li class="listitem "><p>
     Logging data (that is, log files).
    </p></li><li class="listitem "><p>
     VMs and volumes are not currently backed up.
    </p></li></ul></div></div></div><div class="sect1" id="topic-jsc-qps-qt"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Default Backups of the Control Plane to an SSH Target</span> <a title="Permalink" class="permalink" href="#topic-jsc-qps-qt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span>topic-jsc-qps-qt</li></ul></div></div></div></div><p>
  This topic describes how you can set up an external server as a backup server
  in case you lose access to your cloud servers that store the default backups.
 </p><div class="sect2" id="id-1.6.16.13.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Default Backup and Restore</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As part of the installation procedure in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, automatic
   backup/restore jobs are set up to back up to Swift via the Freezer scheduler
   component of the backup and restore service. The backup jobs perform
   scheduled backups of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> control plane data
   (files/directories/db). The restore jobs can be used to restore appropriate
   control plane data. Additional automatic jobs can be added to backup/restore
   from the secure shell (SSH) server that you set up/designate. It is
   recommended that you set up SSH backups so that in the event that you lose
   all of your control plane nodes at once, you have a backup on remote servers
   that you can use to restore the control plane nodes. Note that you do not
   have to restore from the SSH location if only one or two control plane nodes
   are lost. In that case, they can be recovered from the data on the remaining
   control plane node following the restore procedures in
   <a class="xref" href="#recovering-controller-nodes" title="13.2.2.2. Recovering the Control Plane">Section 13.2.2.2, “Recovering the Control Plane”</a>. That document also explains
   how to recover using your remote server SSH backups.
  </p><p>
   While control plane backups to Swift are set up automatically, you must use
   the following procedure to set up SSH backups.
  </p></div><div class="sect2" id="id-1.6.16.13.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up SSH backups</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   By default, during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> deployment, backup jobs are automatically
   deployed to Swift, the MySQL database, the Cloud Lifecycle Manager, and Swift
   rings. Restore jobs are also deployed for convenience. It is more secure to
   store those backups also outside of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> infrastructure. If you
   provide all the values required in the following file, jobs will also be
   deployed to backup and restore to/from an SSH server of your choice:
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div></div><div class="sect2" id="id-1.6.16.13.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backing up your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> control plane to an SSH server</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You must provide the following connection information to this server:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The SSH server's IP address
    </p></li><li class="listitem "><p>
     The SSH server's port to connect to (usually port 22). You may want to
     confirm how to open the port on the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall.
    </p></li><li class="listitem "><p>
     The user to connect to the SSH server as
    </p></li><li class="listitem "><p>
     The SSH private key authorized to connect to that user (see below for
     details of how to set up one if it is not already done)
    </p></li><li class="listitem "><p>
     The directory where you wish to store the backup on that server
    </p></li></ul></div></div><div class="sect2" id="id-1.6.16.13.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up SSH for backups before deployment</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Before running the configuration processor, edit the following file:
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div><p>
   All parameters are mandatory. Take care in providing the SSH private key.
  </p></div><div class="sect2" id="id-1.6.16.13.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparing the server that will store the backup</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In this example, the information is as follows:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     IP: 192.168.100.42
    </p></li><li class="listitem "><p>
     Port: 22
    </p></li><li class="listitem "><p>
     User: backupuser
    </p></li><li class="listitem "><p>
     Target directory for backups: /mnt/backups/
    </p></li></ul></div><p>
   Please replace these values to meet your own requirements, as appropriate.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Connect to the server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh -p 22 root@192.168.100.42</pre></div></li><li class="listitem "><p>
     Create the user:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo useradd backupuser --create-home --home-dir /mnt/backups/</pre></div></li><li class="listitem "><p>
     Switch to that user:
    </p><div class="verbatim-wrap"><pre class="screen">su backupuser</pre></div></li><li class="listitem "><p>
     Create the SSH keypair:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ssh-keygen -t rsa
# Just leave the default for the first question and do not set any passphrase
&gt; Generating public/private rsa key pair.
&gt; Enter file in which to save the key (/mnt/backups//.ssh/id_rsa):
&gt; Created directory '/mnt/backups//.ssh'.
&gt; Enter passphrase (empty for no passphrase):
&gt; Enter same passphrase again:
&gt; Your identification has been saved in /mnt/backups//.ssh/id_rsa
&gt; Your public key has been saved in /mnt/backups//.ssh/id_rsa.pub
&gt; The key fingerprint is:
&gt; a9:08:ae:ee:3c:57:62:31:d2:52:77:a7:4e:37:d1:28 backupuser@padawan-ccp-c0-m1-mgmt
&gt; The key's randomart image is:
&gt; +---[RSA 2048]----+
&gt; |          o      |
&gt; |   . . E + .     |
&gt; |  o . . + .      |
&gt; | o +   o +       |
&gt; |  + o o S .      |
&gt; | . + o o         |
&gt; |  o + .          |
&gt; |.o .             |
&gt; |++o              |
&gt; +-----------------+</pre></div></li><li class="listitem "><p>
     Add the public key to the list of the keys authorized to connect to that
     user on this server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa.pub &gt;&gt; /mnt/backups/.ssh/authorized_keys</pre></div></li><li class="listitem "><p>
     View the private key. This is what you will use for the backup
     configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa
-----BEGIN RSA PRIVATE KEY-----
MIIEogIBAAKCAQEAvjwKu6f940IVGHpUj3ffl3eKXACgVr3L5s9UJnb15+zV3K5L
BZuor8MLvwtskSkgdXNrpPZhNCsWSkryJff5I335Jhr/e5o03Yy+RqIMrJAIa0X5
...
iBKVKGPhOnn4ve3dDqy3q7fS5sivTqCrpaYtByJmPrcJNjb2K7VMLNvgLamK/AbL
qpSTZjicKZCCl+J2+8lrKAaDWqWtIjSUs29kCL78QmaPOgEvfsw=
-----END RSA PRIVATE KEY-----</pre></div></li></ol></div><p>
   Your server is now ready to receive backups. If you wish, you can check our advice on how
   to secure it in <a class="xref" href="#secure-ssh-bu" title="14.4.8. Securing your SSH backup server">Section 14.4.8, “Securing your SSH backup server”</a>.
  </p></div><div class="sect2" id="id-1.6.16.13.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up SSH for backups after deployment</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you already deployed your cloud and forgot to configure SSH backups, or
   if you wish to modify the settings for where the backups are stored, follow
   the following instructions:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div><p>
     Please be advised that all parameters are mandatory, and take care in
     providing the SSH private key.
    </p></li><li class="listitem "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     This will deploy the SSH key and configure SSH backup and restore jobs for
     you. It may take some time before the backups occur.
    </p></li></ol></div></div><div class="sect2" id="id-1.6.16.13.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Opening ports in the cloud firewall</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There is a strict policy of firewalling deployed with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. If you use a
   non-standard SSH port, you may need to specifically open it by using the
   following process:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     When creating your model, edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/firewall_rules.yml</pre></div></li><li class="listitem "><p>
     You must add a new element in the firewall-rules list, such as:
    </p><div class="verbatim-wrap"><pre class="screen">- name: BACKUP
  # network-groups is a list of all the network group names
  # that the rules apply to
  network-groups:
  - MANAGEMENT
  rules:
  - type: allow
    # range of remote addresses in CIDR format that this
    # rule applies to
    remote-ip-prefix:
    port-range-min:
    port-range-max:
    # protocol must be one of: null, tcp, udp or icmp
    protocol: tcp</pre></div></li></ol></div></div><div class="sect2" id="secure-ssh-bu"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Securing your SSH backup server</span> <a title="Permalink" class="permalink" href="#secure-ssh-bu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span>secure-ssh-bu</li></ul></div></div></div></div><p>
   You can do the following to harden an SSH server (these techniques are well
   documented on the internet):
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Disable root login
    </p></li><li class="listitem "><p>
     Move SSH to a non-default port (that is, something other than 22)
    </p></li><li class="listitem "><p>
     Disable password login (only allow RSA keys)
    </p></li><li class="listitem "><p>
     Disable SSH v1
    </p></li><li class="listitem "><p>
     Authorize Secure File Transfer Protocol (SFTP) only for that user (disable
     SSH shell)
    </p></li><li class="listitem "><p>
     Firewall SSH traffic to ensure it comes from the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> address range
    </p></li><li class="listitem "><p>
     Install a Fail2Ban solution
    </p></li><li class="listitem "><p>
     Restrict users that are allowed to SSH
    </p></li></ul></div><p>
   Remove the key pair generated earlier on the backup server: the only thing
   needed is the .ssh/authorized_keys. You can remove the .ssh/id_rsa and
   .ssh/id_rsa.pub files. Be sure to save a backup of them somewhere.
  </p></div><div class="sect2" id="id-1.6.16.13.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Finish Firewall Configuration</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Run the following commands to finish configuring the firewall.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></div><div class="sect2" id="id-1.6.16.13.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General tips</span> <a title="Permalink" class="permalink" href="#id-1.6.16.13.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Take care when sizing the directory that will receive the backup.
    </p></li><li class="listitem "><p>
     Monitor the space left on that directory.
    </p></li><li class="listitem "><p>
     Keep the system up to date on that server.
    </p></li></ul></div></div></div><div class="sect1" id="topic-pth-st3-mw"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing Default Jobs</span> <a title="Permalink" class="permalink" href="#topic-pth-st3-mw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-change_default_backup_jobs.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-change_default_backup_jobs.xml</li><li><span class="ds-label">ID: </span>topic-pth-st3-mw</li></ul></div></div></div></div><p>
  The procedure to make changes to jobs created by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is to
  edit the model file, <code class="literal">my_cloud/config/freezer/jobs.yml</code> and
  then re-run the <code class="literal">_freezer-manage-jobs.yml</code> playbook. (Note
  that the backup/restore component is called "Freezer" so you may see commands
  by that name.)
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Open jobs.yml in an editor, then change and save the file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>nano my_cloud/config/freezer/jobs.yml</pre></div></li><li class="step "><p>
    Commit the file to the local git repository:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Backup job changes"</pre></div></li><li class="step "><p>
    Next, run the configuration processor followed by the ready-deployment
    playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Run _freezer_manage_jobs.yml:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div><div class="sect1" id="freezerUI"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup/Restore Via the Horizon UI</span> <a title="Permalink" class="permalink" href="#freezerUI">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_ui.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_ui.xml</li><li><span class="ds-label">ID: </span>freezerUI</li></ul></div></div></div></div><p>
  A number of backup and restore tasks can be performed using the Horizon UI.
  This topic lists the available tasks, the access requirements, and
  limitations of using Horizon for backup and restore.
 </p><div class="sect2" id="access-ui"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing the UI</span> <a title="Permalink" class="permalink" href="#access-ui">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_ui.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_ui.xml</li><li><span class="ds-label">ID: </span>access-ui</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.16.15.3.2.1"><span class="term ">User name</span></dt><dd><p>
      The only supported user in this version is "ardana_backup". The login
      credentials are available in <code class="literal">backup.osrc</code> located at
      <code class="literal">~/backup.osrc/</code>
     </p></dd><dt id="id-1.6.16.15.3.2.2"><span class="term ">UI access</span></dt><dd><p>
      To access the Horizon UI, follow the instructions shown in
      <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>. Once logged in as "ardana_backup",
      navigate to "Disaster Recovery" panel located in the left-hand menu where
      you should see "Backup and Restore."
     </p></dd></dl></div></div><div class="sect2" id="idg-all-bura-freezer-ui-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup and Restore Operations Supported in the UI</span> <a title="Permalink" class="permalink" href="#idg-all-bura-freezer-ui-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_ui.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_ui.xml</li><li><span class="ds-label">ID: </span>idg-all-bura-freezer-ui-xml-7</li></ul></div></div></div></div><p>
   The following Operations are supported via the UI
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Ability to create new jobs to Backup/Restore files
    </p></li><li class="listitem "><p>
     List the freezer jobs that have completed
    </p></li><li class="listitem "><p>
     Create sessions to link multiple jobs
    </p></li><li class="listitem "><p>
     List the various nodes ( hosts/servers) on which the freezer scheduler and
     freezer agent are installed
    </p></li></ul></div></div><div class="sect2" id="id-1.6.16.15.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.6.16.15.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_ui.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_ui.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following limitations apply to Freezer backups in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The UI for backup and restore is supported only if you log in as
     "ardana_backup". All other users will see the UI panel but the UI will not
     work.
    </p></li><li class="listitem "><p>
     If Backup/Restore action fails via the UI, you must check the Freezer logs
     for details of the failure.
    </p></li><li class="listitem "><p>
     Job Status and Job Result on the UI and backend (CLI) are not in sync.
    </p></li><li class="listitem "><p>
     For a given "Action" the following modes are not supported from the UI:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Microsoft SQL Server
      </p></li><li class="listitem "><p>
       Cinder
      </p></li><li class="listitem "><p>
       Nova
      </p></li></ul></div></li><li class="listitem "><p>
     There is a known issue which will be fixed in future releases while using
     Start and End dates and times in creating a job. Please refrain from using
     those fields.
    </p></li></ul></div></div></div><div class="sect1" id="previous-backups"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Specific Backup</span> <a title="Permalink" class="permalink" href="#previous-backups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-restore_previous_backup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-restore_previous_backup.xml</li><li><span class="ds-label">ID: </span>previous-backups</li></ul></div></div></div></div><p>
  This topic describes how you can get a list of previous backups and how to
  restore from them.
 </p><div id="id-1.6.16.16.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   Note that the existing contents of the directory to which you will restore
   your data (and its children) will be completely overwritten. You must take
   that into account if there is data in that directory that you want to
   survive the restore by either copying that data somewhere else or changing
   the directory to which you will restore.
  </p></div><p>
  By default, freezer-agent restores only the latest (most recent) backup. Here
  is a manual procedure to restore from a list of backups
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Obtain the list of backups:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer backup-list
  [--limit]
  [--offset]</pre></div><p>
    <code class="literal">--limit</code> limit results to this limit.
   </p><p>
    <code class="literal">--offset</code> return results from this offset.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer backup-list
+----------------------------------+-------------+-----------------------------+-------------------------------------------------------------------------+---------------------+-------+
| Backup ID                        | Backup UUID | Hostname                    | Path                                                                    | Created at          | Level |
+----------------------------------+-------------+-----------------------------+-------------------------------------------------------------------------+---------------------+-------+
| 75f8312788fa4e95bf975807905287f8 |             | ardana-qe202-cp1-c1-m3-mgmt | /var/lib/freezer/mount_94e03f120c9e4ae78ad50328d782cea6/.               | 2018-07-06 08:26:00 |     0 |
| 4229d71c840e4ee1b78680131695a330 |             | ardana-qe202-cp1-c1-m2-mgmt | /var/lib/freezer/mount_77d3c7a76b16435181bcaf41837cc7fe/.               | 2018-07-06 08:26:01 |     0 |
| 6fe59b58924e43f88729dc0a1fe1290b |             | ardana-qe202-cp1-c1-m1-mgmt | /var/lib/freezer/mount_4705ac61026c4e77b6bf59b7bcfc286a/.               | 2018-07-06 16:38:50 |     0 |</pre></div></li><li class="step "><p>
    Use the "restore-from-date" option to restore a backup based on
    data/timestamp. The restore-from-data is an option available in
    freezer-agent. When using the parameter
    <code class="literal">--restore-from-date</code>, Freezer searches the available
    backups and selects the nearest older backup relative to the provided date.
    To use this option, the following parameters of the backup must be provided
    - storage target details (example, <code class="literal">target-name</code>,
    <code class="literal">container-name</code>), backup_name, hostname. Usually these
    parameters can be obtained from the backup_job.
   </p><p>
    For example, take the following simple backup job:
   </p><div class="verbatim-wrap"><pre class="screen">[default]
 action = backup
 backup_name = mystuffbackup
 storage = local
 container = /home/me/mystorage
 max_level = 7
     path_to_backup = ~/mydata</pre></div><p>
    Suppose you schedule that every day and you end up with backups that
    happened at:
   </p><div class="verbatim-wrap"><pre class="screen">1) 2015-12-10T02:00:00
     2) 2015-12-11T02:00:00
     3) 2015-12-12T02:00:00
     3) 2015-12-13T02:00:00</pre></div><p>
    Now, if you restore using the following parameters:
   </p><div class="verbatim-wrap"><pre class="screen">[default]
     action = restore
     backup_name = mystuffbackup
     storage = local
     container = /home/me/mystorage
     restore_abs_path = ~/mydata_restore_dir
     restore_from_date = 2015-12-11T23:00:00</pre></div><p>
    The nearest oldest backup will be number 2, taken at 2015-12-11T02:00:00.
   </p></li></ol></div></div></div><div class="sect1" id="topic-mlh-wtn-rt"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup/Restore Scheduler</span> <a title="Permalink" class="permalink" href="#topic-mlh-wtn-rt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span>topic-mlh-wtn-rt</li></ul></div></div></div></div><div class="sect2" id="id-1.6.16.17.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Freezer (backup/restore service) Scheduler Overview</span> <a title="Permalink" class="permalink" href="#id-1.6.16.17.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This document explains, through examples, how to set up backup and restore
   jobs using the backup/restore service scheduler (referred to as Freezer
   Scheduler).
  </p><p>
   The scheduler is a long running process that executes the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Interact with the Freezer API
    </p></li><li class="listitem "><p>
     Generate a client_id to register the client on to the API (to identify the
     node during the next executions)
    </p></li><li class="listitem "><p>
     Execute the freezer-agent according the jobs information retrieved from
     the API
    </p></li><li class="listitem "><p>
     Write to the freezer API the outcome of the freezer-agent execution
    </p></li></ul></div><div id="id-1.6.16.17.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Freezer API maintains information about jobs in the Elasticsearch
    Database.
   </p></div><div id="id-1.6.16.17.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    You must run as root to perform any tasks using the Freezer backup/restore
    service.
   </p></div></div><div class="sect2" id="id-1.6.16.17.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Freezer (backup/restore service) Scheduler Client-ID</span> <a title="Permalink" class="permalink" href="#id-1.6.16.17.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, Freezer Scheduler is automatically installed on the
   Cloud Lifecycle Manager and controller nodes.
  </p><p>
   There is a client_id for each node and its corresponds to the hostname. The
   client_id is created at registration time. The registration is done
   automatically when the scheduler executes any request to the API.

  </p><p>
   The following command lists all the freezer scheduler clients:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list</pre></div><p>
   Here is an example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list
+--------------------------------+----------------------------------+--------------------------------+-------------+
| Client ID                      | uuid                             | hostname                       | description |
+--------------------------------+----------------------------------+--------------------------------+-------------+
| ardana-qe202-cp1-c1-m3-mgmt    | 7869340f2efc4fb9b29e94397385ac39 | ardana-qe202-cp1-c1-m3-mgmt    |             |
| ardana-qe202-cp1-c1-m2-mgmt    | 18041c2b12054802bdaf8cc458abc35d | ardana-qe202-cp1-c1-m2-mgmt    |             |
| ardana-qe202-cp1-c1-m1-mgmt    | 884045a72026425dbcea754806d1022d | ardana-qe202-cp1-c1-m1-mgmt    |             |
| ardana-qe202-cp1-comp0001-mgmt | e404b34e5f7844ed957ca5dd90e6446f | ardana-qe202-cp1-comp0001-mgmt |             |
+--------------------------------+----------------------------------+--------------------------------+-------------+</pre></div></div><div class="sect2" id="id-1.6.16.17.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Scheduler Job</span> <a title="Permalink" class="permalink" href="#id-1.6.16.17.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to a controller node and create the job.
    </p></li><li class="step "><p>
     Source the operating system variables and use the correct client_id. (The
     client-id corresponds to the node where the backup
     files/directory/database resides.) In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> the sourcing of the variable
     should be done like this when you need to use ardana_backup user and backup
     tenant (used for infrastructure backup): Note that when you perform these
     actions you must be <span class="bold"><strong>running as root</strong></span>. The
     following command will provide the necessary credentials to run the job.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/backup.osrc</pre></div><p>
     And with the following when you need to use admin user and admin tenant.
     The following file will contain the admin user credentials. These are not
     for jobs that were created automatically; they are only used for jobs
     created manually to be created/executed under the admin account. Jobs
     created automatically use the credentials stored in the backup.osrc file
     noted above.
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div><div class="verbatim-wrap"><pre class="screen">  {
                        "job_actions": [
                        {
                        "freezer_action": {
                        "action": "backup",
                        "mode": "fs",
                        "backup_name": "backup1",
                        "path_to_backup": "/home/user/tmp",
                        "container": "tmp_backups"
                        },
                        "max_retries": 3,
                        "max_retries_interval": 60
                        }
                        ],
                        "job_schedule": {
                        "schedule_interval": "24 hours"
                        },
                        "description": "backup for tmp dir"
                        }</pre></div></li><li class="step "><p>
     Upload it into the api using the correct client_id:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-create -C <em class="replaceable ">CLIENT-ID</em> --file <em class="replaceable ">FREEZER-FILE</em></pre></div><div id="id-1.6.16.17.4.2.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Freezer file examples can be found in
      <a class="xref" href="#idg-all-bura-freezer-scheduler-xml-6" title="14.8.6. Example Backup Job File">Section 14.8.6, “Example Backup Job File”</a>.
     </p></div></li><li class="step "><p>
     The status of the jobs can be checked with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT-ID</em></pre></div></li><li class="step "><p>
     If no scheduling information is provided, the job will be executed as soon
     as possible so its status will go into a "running" state, then
     "completed".
    </p></li></ol></div></div><p>
   You can find information about the scheduling and backup-execution in
   /var/log/freezer/freezer-scheduler.log and /var/log/freezer-api/freezer-api.log, respectively.
  </p><div id="id-1.6.16.17.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Recurring jobs never go into a "completed" state, as they go back into
    "scheduled" state.
   </p></div></div><div class="sect2" id="id-1.6.16.17.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Different Node</span> <a title="Permalink" class="permalink" href="#id-1.6.16.17.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The scheduler can be used to restore from a different node using the
   hostname parameter that you see in the JSON below. Here is an example conf
   file.

  </p><div class="verbatim-wrap"><pre class="screen">{
                    "job_actions": [
                    {
                    "freezer_action": {
                    "action": "restore",
                    "restore_abs_path": "/var/lib/mysql",
                    "hostname": "test_machine_1",
                    "backup_name": "freezer-db-mysql",
                    "container": "freezer_backup_devstack_1"
                    },
                    "max_retries": 5,
                    "max_retries_interval": 60,
                    "mandatory": true
                    }
                    ],
                    "description": "mysql test restore"
                    }</pre></div><p>
   Create the job like so:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-create -C <em class="replaceable ">CLIENT-ID</em> --file job-restore-mysql.conf</pre></div></div><div class="sect2" id="id-1.6.16.17.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Differential Backup and Restore</span> <a title="Permalink" class="permalink" href="#id-1.6.16.17.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The difference is in the use of the parameter
   <code class="literal">always_level: 1</code>. We also specify a different container,
   so it is easier to spot the files created in the Swift container:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift list freezer_backup_devstack_1_alwayslevel</pre></div></div><div class="sect2" id="idg-all-bura-freezer-scheduler-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Backup Job File</span> <a title="Permalink" class="permalink" href="#idg-all-bura-freezer-scheduler-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span>idg-all-bura-freezer-scheduler-xml-6</li></ul></div></div></div></div><p>
   Here is a sample backup file:
  </p><div class="verbatim-wrap"><pre class="screen">{
                "job_actions": [
                {
                "freezer_action": {
                "mode" : "mysql",
                "mysql_conf" : "/etc/mysql/debian.cnf",
                "path_to_backup": "/var/lib/mysql/",
                "backup_name": "freezer-db-mysql",
                "snapshot": true,
                "always_level": 1,
                "max_priority": true,
                "remove_older_than": 90,
                "container": "freezer_backup_devstack_1_alwayslevel"
                },
                "max_retries": 5,
                "max_retries_interval": 60,
                "mandatory": true
                }
                ],
                "job_schedule" : {
                },
                "description": "mysql backup"
                }</pre></div><p>
   To create the job:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-create -C client_node_1 --file job-backup.conf</pre></div></div><div class="sect2" id="id-1.6.16.17.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Restore Job File</span> <a title="Permalink" class="permalink" href="#id-1.6.16.17.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_scheduler.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_scheduler.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here is an example of <code class="filename">job-restore.conf</code>
  </p><div class="verbatim-wrap"><pre class="screen">{
                    "job_actions": [
                    {
                    "freezer_action": {
                    "action": "restore",
                    "restore_abs_path": "/var/lib/mysql",
                    "hostname": "test_machine_1",
                    "backup_name": "freezer-db-mysql",
                    "container": "freezer_backup_devstack_1_alwayslevel"
                    },
                    "max_retries": 5,
                    "max_retries_interval": 60,
                    "mandatory": true
                    }
                    ],
                    "description": "mysql test restore"
                    }</pre></div><p>
   To create the job:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-create -C client_node_1 --file job-restore.conf</pre></div></div></div><div class="sect1" id="topic-pgq-mnw-dt"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup/Restore Agent</span> <a title="Permalink" class="permalink" href="#topic-pgq-mnw-dt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_agent.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_agent.xml</li><li><span class="ds-label">ID: </span>topic-pgq-mnw-dt</li></ul></div></div></div></div><p>
  This topic describes how to configure backup jobs and restore jobs.
 </p><div class="sect2" id="id-1.6.16.18.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#id-1.6.16.18.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_agent.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The backup/restore service agent (Freezer Agent) is a tool that is used to
   manually back up and restore your data. It can be run from any place you
   want to take a backup (or do a restore) because all <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes have the
   freezer-agent installed on them. To use it, you should run as root. The
   agent runs in conjunction with the <a class="xref" href="#topic-mlh-wtn-rt" title="14.8. Backup/Restore Scheduler">Section 14.8, “Backup/Restore Scheduler”</a>. The
   following explains their relationship:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The backup/restore scheduler (openstack-freezer-scheduler, also see
     <a class="xref" href="#topic-mlh-wtn-rt" title="14.8. Backup/Restore Scheduler">Section 14.8, “Backup/Restore Scheduler”</a>) takes
     JSON-style config files, and can run them automatically according to a
     schedule in the job_schedule field of the scheduler's JSON config file. It
     takes anything you pass in via the job_actions field and translates those
     requirements into an INI-style config file. Then it runs freezer-agent. As
     a user, you could also run the freezer agent using <code class="literal">freezer-agent
     --config file.ini</code>, which is exactly how the scheduler runs it.
    </p></li><li class="listitem "><p>
     The agent (freezer-agent) actually performs the jobs. Whenever any backup
     or restore action happens, the agent is the one doing the actual work. It
     can be run directly by the user, as noted above, or by the scheduler. It
     accepts either command-line flags (such as <code class="literal">--action
     backup</code>) or INI-style config files.
    </p><div id="id-1.6.16.18.3.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can run <code class="literal">freezer-agent --help</code> to view a definitive
      list of all possible flags that can be used (with the transform rules
      mentioned) in these configuration files.
     </p></div></li></ul></div><p>
   For <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you must follow these steps to perform backups:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Define what you want to back up.
    </p></li><li class="step "><p>
     Define a mode for that backup. The following modes are available:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       fs (filesystem) (default)
      </p></li><li class="listitem "><p>
       mysql
      </p></li><li class="listitem "><p>
       sqlserver
      </p></li></ul></div><div id="id-1.6.16.18.3.5.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      It is recommended that you use snapshots if the mode is mysql or
      sqlserver.
     </p></div></li><li class="step "><p>
     Define whether to use a snapshot in the file system for the backup:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       In Unix systems LVM is used (when available).
      </p></li><li class="listitem "><p>
       In Windows systems virtual shadow copies are used.
      </p></li></ul></div></li><li class="step "><p>
     Define a storage media in a job from the following list:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Swift (requires OpenStack credentials)(default)
      </p></li><li class="listitem "><p>
       Local (no credentials required)
      </p></li><li class="listitem "><p>
       SSH (no credentials required) (not implemented on Windows)
      </p></li></ul></div></li></ol></div></div></div><div class="sect2" id="id-1.6.16.18.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Basic Configuration for Backups</span> <a title="Permalink" class="permalink" href="#id-1.6.16.18.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_agent.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There are several mandatory parameters you need to specify in order to
   execute a backup. Note storage is optional:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     action (backup by default)
    </p></li><li class="listitem "><p>
     mode (fs by default)
    </p></li><li class="listitem "><p>
     path-to-backup
    </p></li><li class="listitem "><p>
     backup-name
    </p></li><li class="listitem "><p>
     container (Swift container or local path)
    </p></li><li class="listitem "><p>
     storage is not mandatory. It is Swift by default.
    </p></li></ul></div><p>
   For <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you can create a backup using only mandatory values, as
   in the following example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer-agent --action backup --mode fs --storage swift --path-to-backup /home/user/tmp --container tmp_backups --backup-name backup1</pre></div><p>
   Running the above command from the command line will cause this backup to
   execute once. To create a configuration file for this same backup, in case
   you want to run it manually another time, create a configuration file like
   the one below. Note that in the config file, the parameter names such as
   backup-name will use underscores instead of dashes. Thus backup-name as used
   in the CLI will be backup_name when used in the config file. Note also that
   where you use -- in the CLI, such as --mode, you do not use the -- in the
   config file.
  </p><div class="verbatim-wrap"><pre class="screen">[default]
                action = backup
                mode = fs
                backup_name = backup1
                path_to_backup = /home/user/tmp
                container = tmp_backups</pre></div><p>
   A configuration file similar to the one above will be generated if you
   create a JSON configuration file for automated jobs to be run by the
   scheduler. Instructions on how to do that are found on the
   <a class="xref" href="#topic-mlh-wtn-rt" title="14.8. Backup/Restore Scheduler">Section 14.8, “Backup/Restore Scheduler”</a> page.
  </p></div><div class="sect2" id="id-1.6.16.18.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restoring your Data</span> <a title="Permalink" class="permalink" href="#id-1.6.16.18.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_agent.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you must do the following in order to restore data
   after a backup:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Select a backup to restore.
    </p></li><li class="step "><p>
     Define a mode for the restore: The following modes are available:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       fs (filesystem) (default)
      </p></li><li class="listitem "><p>
       mysql
      </p></li><li class="listitem "><p>
       sqlserver
      </p></li></ul></div></li><li class="step "><p>
     If the restore involves an application (such as MariaDB) remember to shut
     down the application or service and start it again after the restore.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.6.16.18.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Basic Configuration for Restoring</span> <a title="Permalink" class="permalink" href="#id-1.6.16.18.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-freezer_agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-freezer_agent.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To restore from a backup, note that in some cases you must stop the service
   (for instance, MariaDB) before the restore.
  </p><p>
   There are several parameters that are required and there are some optional
   parameters used to execute a restore:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     action (backup by default)
    </p></li><li class="listitem "><p>
     mode (fs by default)
    </p></li><li class="listitem "><p>
     restore-abs-path
    </p></li><li class="listitem "><p>
     backup-name
    </p></li><li class="listitem "><p>
     container (Swift container or local path)
    </p></li><li class="listitem "><p>
     restore-from-host
    </p></li><li class="listitem "><p>
     restore-from-date (optional)
    </p></li><li class="listitem "><p>
     storage is not mandatory. It is Swift by default
    </p></li></ul></div><p>
   You can create a restore using mandatory values, as in the following
   example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer-agent --action restore --mode fs --storage swift --restore-abs-path /home/user/tmp --container tmp_backups --backup-name backup1 --restore-from-host ubuntu</pre></div><p>
   To create a configuration file for this same restore, the file would look
   like the one below. Note that in the config file, the parameter names such
   as backup-name will use underscores instead of dashes. Thus backup-name as
   used in the CLI will be backup_name when used in the config file. Note also
   that where you use -- in the CLI, such as --mode, you do not use the -- in
   the config file. This is the same format as used above for backup
   configuration.
  </p><div class="verbatim-wrap"><pre class="screen">  {
                    "job_actions": [
                    {
                    "freezer_action": {
                    "action": "restore",
                    "mode": "fs",
                    "backup_name": "backup1",
                    "restore_abs_path": "/home/user/tmp",
                    "container": "tmp_backups",
                    "hostname": "ubuntu"
                    },
                    "max_retries": 3,
                    "max_retries_interval": 60
                    }
                    ],
                    "description": "backup for tmp dir"
                    }</pre></div></div></div><div class="sect1" id="bu-limitations"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup and Restore Limitations</span> <a title="Permalink" class="permalink" href="#bu-limitations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-backup_limitations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-backup_limitations.xml</li><li><span class="ds-label">ID: </span>bu-limitations</li></ul></div></div></div></div><p>
  The following limitations apply to backups created by the Freezer backup and
  restore service in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Recovery of the following services (or cloud topologies) will be partially
    supported as they need additional data (other than MariaDB) to return to
    fully functional.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      ESX Cloud
     </p></li><li class="listitem "><p>
      Network services - LBaaS and VPNaaS
     </p></li></ul></div></li><li class="listitem "><p>
    Logging data (that is, log files)
   </p></li></ul></div></div><div class="sect1" id="topic-i4l-xhn-tt"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabling Backup/Restore before Deployment</span> <a title="Permalink" class="permalink" href="#topic-i4l-xhn-tt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-disable_bura_before_deployment.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-disable_bura_before_deployment.xml</li><li><span class="ds-label">ID: </span>topic-i4l-xhn-tt</li></ul></div></div></div></div><p>
   Backups are enabled by default. Therefore, you must take action if you want
   backups to be disabled for any reason. This topic explains how to disable
   default backup jobs before completing the installation of your cloud.
  </p><div id="id-1.6.16.20.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    You should make modifications in the <code class="literal">~/openstack/my_cloud/</code>
    directory before running the configuration processor and ready-deployment
    steps.
   </p></div><div class="sect2" id="DisableFreezerDeploy"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable backups before installation:</span> <a title="Permalink" class="permalink" href="#DisableFreezerDeploy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-disable_bura_before_deployment.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-disable_bura_before_deployment.xml</li><li><span class="ds-label">ID: </span>DisableFreezerDeploy</li></ul></div></div></div></div><p>
   To disable deployment of the Freezer backup and restore service, remove the
   following lines in <code class="literal">control_plane.yml</code>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     freezer-agent
    </p></li><li class="listitem "><p>
     freezer-api
    </p></li></ul></div><div id="id-1.6.16.20.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    This action is required even if you already removed Freezer lines from your
    model (<code class="filename">control_plane.yml</code>).
   </p></div></div><div class="sect2" id="DisableFreezerCompletely"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy Freezer but disable backup/restore job creation:</span> <a title="Permalink" class="permalink" href="#DisableFreezerCompletely">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-disable_bura_before_deployment.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-disable_bura_before_deployment.xml</li><li><span class="ds-label">ID: </span>DisableFreezerCompletely</li></ul></div></div></div></div><p>
   It is also possible to allow Freezer deployment yet prevent the lifecycle
   manager from creating automatic backup jobs. By default, the lifecycle
   manager deployment automatically creates jobs for the backup and restore of
   the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Lifecycle-manager node
    </p></li><li class="listitem "><p>
     MySQL database
    </p></li><li class="listitem "><p>
     Swift rings
    </p></li></ul></div><p>
   Before running the configuration processor, you can prevent Freezer from
   automatically creating backup and restore jobs by changing the variables
   <code class="literal">freezer_create_backup_jobs</code> and
   <code class="literal">freezer_create_restore_jobs</code> to <code class="literal">false</code>
   in:
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/freezer/activate_jobs.yml</pre></div><p>
   Alternatively, you can disable the creation of those jobs while launching
   the deployment process, as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml -e '{ "freezer_create_backup_jobs": false }' -e '{ "freezer_create_restore_jobs": false }'</pre></div><p>
   When using these options, the Freezer infrastructure will still be deployed
   but will not execute any backups.
  </p></div><div class="sect2" id="id-1.6.16.20.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable backup and restore jobs for a specific service</span> <a title="Permalink" class="permalink" href="#id-1.6.16.20.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-disable_bura_before_deployment.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-disable_bura_before_deployment.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To manage which jobs will be enabled, set the appropriate paramters in the
   <code class="literal">jobs.yml</code> freezer jobs configuration file:
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/freezer/jobs.yml</pre></div><p>
   You can completely disable the backup of a component by changing the
   <code class="literal">enabled</code> field that corresponds to that service to false
   in <code class="literal">jobs.yml</code>.
  </p><p>
   You can specify where a job will store its backup by setting
   <code class="literal">store_in_swift, store_in_ssh</code>,
   <code class="literal">store_in_local</code> to true or false. Note that these are not
   mutually exclusive. You can set true for all of these backup targets.
   Setting SSH, Swift, and local to true will cause one backup job (and one
   restore job) per storage target to be created.
  </p><p>
   Note also that even if <code class="literal">store_in_ssh</code> is set to true, the
   SSH backup job will not be created unless SSH credentials are provided in
   <code class="filename">/openstack/my_cloud/config/freezer/ssh_credentials.yml</code>.
  </p><p>
   When setting <code class="literal">store_in_local</code> to <code class="literal">true</code>,
   the backup job will store backups on the server executing the backup. This
   option is useful, for example, if you plan to mount an NFS share and want
   your backup stored on it. You need to provide the path where the backup will
   be stored by setting the <code class="literal">local_storage_base_dir</code>
   parameter.
  </p><p>
   By default, one backup job per storage medium per component will be created.
   A corresponding restore job for each of those backup jobs will also be
   created by default. These jobs can be used to quickly restore the
   corresponding backup. To disable the creation of these restore jobs, change
   <code class="literal">also_create_restore_job</code> to <code class="literal">false</code>.
  </p></div><div class="sect2" id="id-1.6.16.20.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Activating and deactivating jobs after cloud deployment</span> <a title="Permalink" class="permalink" href="#id-1.6.16.20.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-disable_bura_before_deployment.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-disable_bura_before_deployment.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Make modifications similar to those discussed above in <code class="literal">
     /openstack/my_cloud/config/freezer/jobs.yml</code>.
    </p></li><li class="step "><p>
     Commit modifications to the git repo
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "A message that explains what modifications have been made"</pre></div></li><li class="step "><p>
     Run the configuration processor
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run the ready deployment playbook. (This will update the scratch/...
     directories with all of the above modifications).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Change directories to scratch
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="step "><p>
     Run _freezer_manage_jobs.yml
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div></div><div class="sect1" id="topic-dsm-fbs-st"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling, Disabling and Restoring Backup/Restore Services</span> <a title="Permalink" class="permalink" href="#topic-dsm-fbs-st">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-start_stop_freezer_services.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-start_stop_freezer_services.xml</li><li><span class="ds-label">ID: </span>topic-dsm-fbs-st</li></ul></div></div></div></div><div class="sect2" id="idg-all-bura-start-stop-freezer-services-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stop, Start and Restart the Backup Services</span> <a title="Permalink" class="permalink" href="#idg-all-bura-start-stop-freezer-services-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-start_stop_freezer_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-start_stop_freezer_services.xml</li><li><span class="ds-label">ID: </span>idg-all-bura-start-stop-freezer-services-xml-6</li></ul></div></div></div></div><p>
   To stop the Freezer backup and restore service globally, launch the
   following playbook from the Cloud Lifecycle Manager (this will stop all
   freezer-api and all freezer-agent running on your clusters):
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts freezer-stop.yml</pre></div><p>
   To start the Freezer backup and restore service globally, launch the
   following playbook from the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts freezer-start.yml</pre></div><p>
   To restart the Freezer backup and restore services use the ansible playbooks
   from above.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts freezer-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts freezer-start.yml</pre></div><p>
   It is possible to target only specific nodes using the ansible --limit
   parameter.
  </p></div><div class="sect2" id="manual"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manually</span> <a title="Permalink" class="permalink" href="#manual">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/bura-start_stop_freezer_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-start_stop_freezer_services.xml</li><li><span class="ds-label">ID: </span>manual</li></ul></div></div></div></div><p>
   <span class="emphasis"><em>For the freezer-agent: </em></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Connect to the concerned host.
    </p></li><li class="step "><p>
     Run the following command to stop the freezer agent:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop openstack-freezer-scheduler</pre></div><p>
     or run the following command to start the freezer-agent:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl start openstack-freezer-scheduler</pre></div><p>
     or run the following command to restart the freezer-agent:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl restart openstack-freezer-scheduler</pre></div></li></ol></div></div><p>
   <span class="emphasis"><em>For the freezer-api:</em></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Connect to the concerned host.
    </p></li><li class="step "><p>
     Run the following commands to stop the freezer-api:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm /etc/apache2/vhosts.d/freezer-modwsgi.conf</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl reload apache2</pre></div><p>
     or run the following commands to start the freezer-api:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ln -s /etc/apache2/vhosts.d/freezer-modwsgi.vhost /etc/apache2/vhosts.d/freezer-modwsgi.conf</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl reload apache2</pre></div></li></ol></div></div></div></div><div class="sect1" id="backup-audit-logs"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backing up and Restoring Audit Logs</span> <a title="Permalink" class="permalink" href="#backup-audit-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-audit_logs_backup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_backup.xml</li><li><span class="ds-label">ID: </span>backup-audit-logs</li></ul></div></div></div></div><p>
  To enable backup of the audit log directory, follow these steps. Before
  performing the following steps, run through
  <a class="xref" href="#topic-enable-audit-logs" title="12.2.7.2. Enable Audit Logging">Section 12.2.7.2, “Enable Audit Logging”</a> .
 </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
    First, from the Cloud Lifecycle Manager node, run the following playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div><p>
    This will create a job to back up the audit log directory on any node where
    that directory exists. In order to limit this to only specific nodes, use
    the --limit option of Ansible
   </p></li></ul></div></div><p>
  In order to restore the logs, follow one of the following procedures:
 </p><p>
  To restore from the node that made the backup to the same node directly in
  the audit log directory (for example, the folder has been deleted):
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Connect to the node
   </p></li><li class="step "><p>
    Source OpenStack credentials
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/backup.osrc</pre></div></li><li class="step "><p>
    List pre-configured jobs
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C `hostname`</pre></div></li><li class="step "><p>
    Note the id corresponding to the job: "Ardana Default: Audit log restore from
    ..."
   </p></li><li class="step "><p>
    Schedule the restore
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB-ID</em></pre></div></li></ol></div></div><p>
  To restore the backup in another directory, or from another host,
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Connect to the node
   </p></li><li class="step "><p>
    Source the OpenStack credentials
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/backup.osrc</pre></div></li><li class="step "><p>
    Choose from where you will restore (from Swift or from and SSH backup)
   </p><ol type="a" class="substeps "><li class="step "><p>
      Swift: Create a restore config file (for example,
      <code class="filename">restore.ini</code>) with the following
      content to restore from a swift backup (make sure to fill in
      &lt;value&gt; )
     </p><div class="verbatim-wrap"><pre class="screen">[default]
action = restore
backup_name = freezer_audit_log_backup
container = freezer_audit_backup
log_file = /freezer-agent/freezer-agent.log
restore_abs_path = <em class="replaceable ">PATH TO THE DIRECTORY WHERE YOU WANT TO RESTORE</em>
hostname = <em class="replaceable ">HOSTNAME OF THE HOST YOU WANT TO RESTORE THE BACKUP FROM</em></pre></div></li><li class="step "><p>
      Or: Create a restore configuration file (for example,
      <code class="filename">restore.ini</code>) with the following
      content to restore from an SSH backup (make sure to fill in
      <em class="replaceable ">VALUE</em>) SSH information is available in
      <code class="literal">openstack/my_cloud/config/freezer/ssh_credentials.yml</code>
     </p><div class="verbatim-wrap"><pre class="screen">[default]
action = restore
storage = ssh
backup_name = freezer_audit_log_backup
log_file = /freezer-agent/freezer-agent.log
ssh_key = /etc/freezer/ssh_key
restore_abs_path = <em class="replaceable ">PATH TO THE DIRECTORY WHERE YOU WANT TO RESTORE</em>
hostname = <em class="replaceable ">HOSTNAME OF THE HOST YOU WANT TO RESTORE THE BACKUP FROM</em>
ssh_host = <em class="replaceable ">YOUR SSH BACKUP HOST</em>
ssh_port = <em class="replaceable ">YOUR SSH BACKUP PORT</em>
ssh_username = <em class="replaceable ">YOUR SSH BACKUP USERNAME</em>
container = <em class="replaceable ">YOUR SSH BACKUP BASEDIR</em>/freezer_audit_backup</pre></div></li></ol></li><li class="step "><p>
    Run the freezer-agent to restore
   </p><div class="verbatim-wrap"><pre class="screen">freezer-agent --config restore.ini</pre></div></li></ol></div></div></div></div><div class="chapter " id="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1"><div class="titlepage"><div><div><h1 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Issues</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-troubleshooting-issues-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-troubleshooting_issues.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_issues.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-issues-xml-1</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#general-troubleshooting"><span class="number">15.1 </span><span class="name">General Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-controlplane"><span class="number">15.2 </span><span class="name">Control Plane Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#ts-compute"><span class="number">15.3 </span><span class="name">Troubleshooting Compute Service</span></a></span></dt><dt><span class="section"><a href="#neutron-troubleshooting"><span class="number">15.4 </span><span class="name">Network Service Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-glance"><span class="number">15.5 </span><span class="name">Troubleshooting the Image (Glance) Service</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-storage"><span class="number">15.6 </span><span class="name">Storage Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#monitoring-logging-usage-reporting"><span class="number">15.7 </span><span class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#topic-ly3-yyr-st"><span class="number">15.8 </span><span class="name">Backup and Restore Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-orchestration"><span class="number">15.9 </span><span class="name">Orchestration Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-tools"><span class="number">15.10 </span><span class="name">Troubleshooting Tools</span></a></span></dt></dl></div></div><p>
  Troubleshooting and support processes for solving issues in your environment.
 </p><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
 </p><div class="sect1" id="general-troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General Troubleshooting</span> <a title="Permalink" class="permalink" href="#general-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-general_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-general_troubleshooting.xml</li><li><span class="ds-label">ID: </span>general-troubleshooting</li></ul></div></div></div></div><p>
  General troubleshooting procedures for resolving your cloud issues including
  steps for resolving service alarms and support contact information.
 </p><p>
  Before contacting support to help you with a problem on SUSE <span class="productname">OpenStack</span> Cloud, we recommend
  gathering as much information as possible about your system and the
  problem. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with a tool called
  <code class="command">supportconfig</code>. It gathers system information such as the
  current kernel version being used, the hardware, RPM database, partitions,
  and other items. <code class="command">supportconfig</code> also collects the most
  important log files. This information assists support staff to identify and
  solve your problem.
 </p><p>
  Always run <code class="command">supportconfig</code> on the Cloud Lifecycle Manager and on the
  Control Node(s). If a Compute Node or a Storage Node is part of the problem, run
  <code class="command">supportconfig</code> on the affected node as well. For details on
  how to run <code class="command">supportconfig</code>, see
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support</a>.
 </p><div class="sect2" id="alarmdefinitions"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Resolution Procedures</span> <a title="Permalink" class="permalink" href="#alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-alarm_resolutions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-alarm_resolutions.xml</li><li><span class="ds-label">ID: </span>alarmdefinitions</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a monitoring solution based on OpenStack’s Monasca
  service. This service provides monitoring and metrics for all OpenStack
  components, as well as much of the underlying system. By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  comes with a set of alarms that provide coverage of the primary systems. In
  addition, you can define alarms based on threshold values for any metrics
  defined in the system. You can view alarm information in the Operations
  Console. You can also receive or deliver this information to others by
  configuring email or other mechanisms. Alarms provide information about
  whether a component failed and is affecting the system, and also what
  condition triggered the alarm.
 </p><p>
  Here is a list of the included service-specific alarms and the recommended
  troubleshooting steps. We have organized these alarms by the section of the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console, they are organized in as well as the
  <code class="literal">service</code> dimension defined.
 </p><div class="sect3" id="compute-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Alarms</span> <a title="Permalink" class="permalink" href="#compute-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>compute-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Compute section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.4.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: COMPUTE</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> This is a <code class="literal">nova-api</code> health check.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
        </p>
       </td><td valign="top">Restart the <code class="literal">nova-api</code> process on the affected
     node. Review the <code class="literal">nova-api.log</code> files. Try to connect
     locally to the http port that is found in the dimension field of the alarm
     to see if the connection is accepted.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Host Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Alarms when the specified host is down or not reachable.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The host is down, has been rebooted, or has network
         connectivity issues.
        </p>
       </td><td valign="top">If it is a single host, attempt to restart the system. If it is
     multiple hosts, investigate networking issues.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Bound Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: <code class="literal">process_name=nova-api</code> This alarm
         checks that the number of processes found is in a predefined range.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process crashed or too many processes running
        </p>
       </td><td valign="top">Stop all the processes and restart the nova-api process on the
       affected host.  Review the system and nova-api logs.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Separate alarms for each of these Nova services,
         specified by the <code class="literal">component</code> dimension:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           nova-api
          </p></li><li class="listitem "><p>
           nova-cert
          </p></li><li class="listitem "><p>
           nova-compute
          </p></li><li class="listitem "><p>
           nova-consoleauth
          </p></li><li class="listitem "><p>
           nova-conductor
          </p></li><li class="listitem "><p>
           nova-scheduler
          </p></li><li class="listitem "><p>
           nova-novncproxy
          </p></li></ul></div>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process specified by the <code class="literal">component</code>
         dimension has crashed on the host specified by the
         <code class="literal">hostname</code> dimension.
        </p>
       </td><td valign="top">
        <p>
         Restart the process on the affected node using these steps:
        </p>
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
           Log in to the Cloud Lifecycle Manager.
          </p></li><li class="step "><p>
           Use the Nova start playbook against the affected node:
          </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
        <p>
         Review the associated logs. The logs will be in the format of
         <code class="literal">&lt;service&gt;.log</code>, such as
         <code class="literal">nova-compute.log</code> or
         <code class="literal">nova-scheduler.log</code>.
        </p>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: nova.heartbeat</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Check that all services are sending heartbeats.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process for service specified in the alarm has crashed
         or is hung and not reporting its status to the database. Alternatively
         it may be the service is fine but an issue with messaging or the
         database which means the status is not being updated correctly.
        </p>
       </td><td valign="top">Restart the affected service. If the service is reporting OK the
     issue may be with RabbitMQ or MySQL. In that case, check the alarms for
     those services.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Service log directory consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a service set to
         <code class="literal">DEBUG</code> instead of <code class="literal">INFO</code> level.
         Another reason could be due to a repeating error message filling up
         the log files. Finally, it could be due to log rotate not configured
         properly so old log files are not being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at the
     logs. If <code class="literal">DEBUG</code> log entries exist, set the logging level
     to <code class="literal">INFO</code>. If the logs are repeatedly logging an error
     message, do what is needed to resolve the error. If old log files exist,
     configure log rotate to remove them. You could also choose to remove old
     log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.4.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: IMAGE-SERVICE in Compute section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Separate alarms for
         each of these Glance services, specified by the
         <code class="literal">component</code> dimension:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          glance-api
         </p></li><li class="listitem "><p>
          glance-registry
         </p></li></ul></div>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> API is unresponsive.
        </p>
       </td><td valign="top">
        <p>
         Restart the process on the affected node using these steps:
        </p>
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Glance start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p></td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Service log directory consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a service set to
         <code class="literal">DEBUG</code> instead of <code class="literal">INFO</code>
         level. Another reason could be due to a repeating error message
         filling up the log files. Finally, it could be due to log rotate not
         configured properly so old log files are not being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.4.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BAREMETAL in Compute section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified process is not running: <code class="literal">process_name = ironic-api</code>
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The Ironic API is unresponsive.
        </p>
       </td><td valign="top">
        <p>
        Restart the <code class="literal">ironic-api</code> process with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the affected host via SSH.
         </p></li><li class="step "><p>
          Restart the <code class="literal">ironic-api</code> process with this command:
         </p><div class="verbatim-wrap"><pre class="screen">sudo service ironic-api restart</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified process is not running: <code class="literal">process_name = ironic-conductor</code>
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The
         <code class="literal">ironic-conductor</code> process has crashed.
        </p>
       </td><td valign="top">
        <p>
        Restart the <code class="literal">ironic-conductor</code> process with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Source your <code class="literal">admin</code> user credentials:
         </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
          Locate the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants | grep mess</pre></div></li><li class="step "><p>
          SSH to the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">sudo -u ardana ssh &lt;IP_ADDRESS&gt;</pre></div></li><li class="step "><p>
          Stop the <code class="literal">ironic-conductor</code> process by using this
          playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</pre></div></li><li class="step "><p>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified HTTP endpoint is down or not reachable.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The API is unresponsive.
        </p>
       </td><td valign="top">
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Source your <code class="literal">admin</code> user credentials:
         </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
          Locate the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants | grep mess</pre></div></li><li class="step "><p>
          SSH to the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">sudo -u ardana ssh &lt;IP_ADDRESS&gt;</pre></div></li><li class="step "><p>
          Stop the <code class="literal">ironic-api</code> process by using this
          playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</pre></div></li><li class="step "><p>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Service log directory
         consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
         service set to <code class="literal">DEBUG</code> instead of
         <code class="literal">INFO</code> level. Another reason could be due to a
         repeating error message filling up the log files. Finally, it could be
         due to log rotate not configured properly so old log files are not
         being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="storage-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Alarms</span> <a title="Permalink" class="permalink" href="#storage-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>storage-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Storage section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.5.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OBJECT-STORAGE</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swiftlm-scan monitor</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        <code class="literal">swiftlm-scan</code> cannot execute a monitoring task.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">swiftlm-scan</code> program is used to monitor and measure
        a number of metrics. If it is unable to monitor or measure something,
        it raises this alarm.
       </p>
      </td><td valign="top">
       <p>
        Click on the alarm to examine the <code class="literal">Details</code> field and
        look for a <code class="literal">msg</code> field. The text may explain the error
        problem. To view/confirm this, you can also log into the host specified
        by the <code class="literal">hostname</code> dimension, and then run this
        command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo swiftlm-scan | python -mjson.tool</pre></div>
       <p>
        The <code class="literal">msg</code> field is contained in the
        <code class="literal">value_meta</code> item.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift account replicator last</strong></span>
        completed in 12 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if an
        <code class="literal">account-replicator</code> process did not complete a
        replication cycle within the last 12 hours.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the <code class="literal">account-replication</code> process is stuck.
       </p>
      </td><td valign="top">
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift container replicator last</strong></span>
        completed in 12 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a
        container-replicator process did not complete a replication cycle
        within the last 12 hours
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the container-replication process is stuck.
       </p>
      </td><td valign="top">
       <p>
        SSH to the affected host and restart the process with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo systemctl restart swift-container-replicator</pre></div>
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift object replicator last</strong></span>
        completed in 24 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if an
        object-replicator process did not complete a replication cycle within
        the last 24 hours
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the object-replication process is stuck.
       </p>
      </td><td valign="top">
       <p>
        SSH to the affected host and restart the process with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo systemctl restart swift-account-replicator</pre></div>
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift configuration file</strong></span>
        ownership
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        files/directories in <code class="literal">/etc/swift</code> are not owned by
        Swift.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> For files in
        <code class="literal">/etc/swift</code>, somebody may have manually edited or
        created a file.
       </p>
      </td><td valign="top">
       <p>
        For files in <code class="literal">/etc/swift</code>, use this command to change
        the file ownership:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo chown swift.swift /etc/swift/, /etc/swift/*</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift data filesystem ownership</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if files or
        directories in <code class="literal">/srv/node</code> are not owned by Swift.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> For directories in
        <code class="literal">/srv/node/*</code>, it may happen that the root partition
        was reimaged or reinstalled and the UID assigned to the Swift user
        change. The directories and files would then not be owned by the UID
        assigned to the Swift user.
       </p>
      </td><td valign="top">
       <p>
        For directories and files in <code class="filename">/srv/node/*</code>, compare
        the swift UID of this system and other systems and the UID of the owner
        of <code class="filename">/srv/node/*</code>. If possible, make the UID of the
        Swift user match the directories or files. Otherwise, change the
        ownership of all files and directories under the
        <code class="filename">/srv/node</code> path using a similar <code class="command">chown
        swift.swift</code> command as above.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Drive URE errors detected</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        <code class="literal">swift-drive-audit</code> reports an unrecoverable read
        error on a drive used by the Swift service.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> An unrecoverable read
        error occurred when Swift attempted to access a directory.
       </p>
      </td><td valign="top">
       <p>
        The UREs reported only apply to file system metadata (that is,
        directory structures). For UREs in object files, the Swift system
        automatically deletes the file and replicates a fresh copy from one of
        the other replicas.
       </p>
       <p>
        UREs are a normal feature of large disk drives. It does not mean that
        the drive has failed. However, if you get regular UREs on a specific
        drive, then this may indicate that the drive has indeed failed and
        should be replaced.
       </p>
       <p>
        You can use standard XFS repair actions to correct the UREs in the file
        system.
       </p>
       <p>
        If the XFS repair fails, you should wipe the GPT table as follows
        (where &lt;drive_name&gt; is replaced by the actual drive name):
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo dd if=/dev/zero of=/dev/sd&lt;drive_name&gt; \
bs=$((1024*1024)) count=1</pre></div>
       <p>
        Then follow the steps below which will reformat the drive, remount it,
        and restart Swift services on the affected node.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift reconfigure playbook, specifying the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _swift-configure.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        It is safe to reformat drives containing Swift data because Swift
        maintains other copies of the data (usually, Swift is configured to
        have three replicas of all data).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift service</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a Swift
        process, specified by the <code class="literal">component</code> field, is not
        running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A daemon specified by
        the <code class="literal">component</code> dimension on the host specified by the
        <code class="literal">hostname</code> dimension has stopped running.
       </p>
      </td><td valign="top">
       <p>
        Examine the <code class="filename">/var/log/swift/swift.log</code> file for
        possible error messages related the Swift process. The process in
        question is listed in the alarm dimensions in the
        <code class="literal">component</code> dimension.
       </p>
       <p>
        Restart Swift processes by running the
        <code class="filename">swift-start.yml</code> playbook, with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift filesystem mount point</strong></span>
        status
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a file
        system/drive used by Swift is not correctly mounted.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The device specified by
        the <code class="literal">device</code> dimension is not correctly mounted at the
        mountpoint specified by the <code class="literal">mount</code> dimension.
       </p>
       <p>
        The most probable cause is that the drive has failed or that it had a
        temporary failure during the boot process and remained unmounted.
       </p>
       <p>
        Other possible causes are a file system corruption that prevents the
        device from being mounted.
       </p>
      </td><td valign="top">
       <p>
        Reboot the node and see if the file system remains unmounted.
       </p>
       <p>
        If the file system is corrupt, see the process used for the "Drive URE
        errors" alarm to wipe and reformat the drive.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift uptime-monitor status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the
        swiftlm-uptime-monitor has errors using Keystone (<code class="literal">keystone-get-token</code>),
        Swift (<code class="literal">rest-api</code>) or Swift's healthcheck.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        swiftlm-uptime-monitor cannot get a token from Keystone or cannot get a
        successful response from the Swift Object-Storage API.
       </p>
      </td><td valign="top">
       <p>
        Check that the Keystone service is running:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check the status of the Keystone service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-status.yml</pre></div></li><li class="step "><p>
          If it is not running, start the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-start.yml</pre></div></li><li class="step "><p>
          Contact the support team if further assistance troubleshooting the
          Keystone service is needed.
         </p></li></ol></div></div>
       <p>
        Check that Swift is running:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check the status of the Keystone service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li><li class="step "><p>
          If it is not running, start the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml</pre></div></li></ol></div></div>
       <p>
        Restart the swiftlm-uptime-monitor as follows:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log into the first server running the swift-proxy-server service. Use
          this playbook below to determine whcih host this is:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml
--limit SWF-PRX[0]</pre></div></li><li class="step "><p>
          Restart the swiftlm-uptime-monitor with this command:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart swiftlm-uptime-monitor</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift Keystone server connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the Keystone service (used for token validation)
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Identity service
        (Keystone) server may be down. Another possible cause is that the
        network between the host reporting the problem and the Keystone server
        or the <code class="literal">haproxy</code> process is not forwarding requests to
        Keystone.
       </p>
      </td><td valign="top">
       <p>
        The <code class="literal">URL</code> dimension contains the name of the virtual
        IP address. Use cURL or a similar program to confirm that a connection
        can or cannot be made to the virtual IP address. Check that
        <code class="literal">haproxy</code> is running. Check that the Keystone service
        is working.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift service listening on ip</strong></span>
        and port
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a Swift
        service is not listening on the correct port or ip.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Swift service may be
        down.
       </p>
      </td><td valign="top">
       <p>
        Verify the status of the Swift service on the affected host, as
        specified by the <code class="literal">hostname</code> dimension.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift status playbook to confirm status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If an issue is determined, you can stop and restart the Swift service
        with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the Swift service on the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts Swift-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Restart the Swift service on the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift rings checksum</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the Swift rings
        checksums do not match on all hosts.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Swift ring files
        must be the same on every node. The files are located in
        <code class="filename">/etc/swift/*.ring.gz</code>.
       </p>
       <p>
        If you have just changed any of the rings and you are still deploying
        the change, it is normal for this alarm to trigger.
       </p>
      </td><td valign="top">
       <p>
        If you have just changed any of your Swift rings, if you wait until the
        changes complete then this alarm will likely clear on its own. If it
        does not, then continue with these steps.
       </p>
       <p>
        Use <code class="command">sudo swift-recon --md5</code> to find which node has
        outdated rings.
       </p>
       <p>
        Run the <code class="filename">swift-reconfigure.yml</code> playbook, using the
        steps below. This deploys the same set of rings to every node.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift memcached server connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the specified memcached server.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The server may be down.
        The memcached daemon running the server may have stopped.
       </p>
      </td><td valign="top">
       <p>
        If the server is down, restart it.
       </p>
       <p>
        If memcached has stopped, you can restart it by using the
        <code class="filename">memcached-start.yml</code> playbook, using the steps
        below. If this fails, rebooting the node will restart the process.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the memcached start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts memcached-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If the server is running and memcached is running, there may be a
        network problem blocking port 11211.
       </p>
       <p>
        If you see sporadic alarms on different servers, the system may be
        running out of resources. Contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift individual disk usage
        exceeds 80%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a disk drive
        used by Swift exceeds 80% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Generally all disk
        drives will fill roughly at the same rate. If an individual disk drive
        becomes filled faster than other drives it can indicate a problem with
        the replication process.
       </p>
      </td><td valign="top">
       <p>
        If many or most of your disk drives are 80% full, you need to add more
        nodes to your system or delete existing objects.
       </p>
       <p>
        If one disk drive is noticeably (more than 30%) more utilized than the
        average of other disk drives, check that Swift processes are working on
        the server (use the steps below) and also look for alarms related to
        the host. Otherwise continue to monitor the situation.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift individual disk usage exceeds
        90%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a disk drive
        used by Swift exceeds 90% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Generally all disk
        drives will fill roughly at the same rate. If an individual disk drive
        becomes filled faster than other drives it can indicate a problem with
        the replication process.
       </p>
      </td><td valign="top">
       <p>
        If one disk drive is noticeably (more than 30%) more utilized than the
        average of other disk drives, check that Swift processes are working on
        the server, using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li></ol></div></div>
       <p>
        Also look for alarms related to the host. An individual disk drive
        filling can indicate a problem with the replication process.
       </p>
       <p>
        Restart Swift on that host using the <code class="literal">--limit</code>
        argument to target the host:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the Swift service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Start the Swift service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If the utilization does not return to similar values as other disk
        drives, you can reformat the disk drive. You should only do this if the
        average utilization of all disk drives is less than 80%. To format a
        disk drive contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for instructions.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift total disk usage exceeds
        80%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the average
        disk utilization of Swift disk drives exceeds 80% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number and size of
        objects in your system is beginning to fill the available disk space.
        Account and container storage is included in disk utilization. However,
        this generally consumes 1-2% of space compared to objects, so object
        storage is the dominate consumer of disk space.
       </p>
      </td><td valign="top">
       <p>
        You need to add more nodes to your system or delete existing objects to
        remain under 80% utilization.
       </p>
       <p>
        If you delete a project/account, the objects in that account are not
        removed until a week later by the <code class="literal">account-reaper</code>
        process, so this is not a good way of quickly freeing up space.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift total disk usage exceeds
        90%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the average
        disk utilization of Swift disk drives exceeds 90% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number and size of
        objects in your system is beginning to fill the available disk space.
        Account and container storage is included in disk utilization. However,
        this generally consumes 1-2% of space compared to objects, so object
        storage is the dominate consumer of disk space.
       </p>
      </td><td valign="top">
       <p>
        If your disk drives are 90% full, you must immediately stop all
        applications that put new objects into the system. At that point you
        can either delete objects or add more servers.
       </p>
       <p>
        Using the steps below, set the <code class="literal">fallocate_reserve</code>
        value to a value higher than the currently available space on disk
        drives. This will prevent more objects being created.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Edit the configuration files below and change the value for
          <code class="literal">fallocate_reserve</code> to a value higher than the
          currently available space on the disk drives:
         </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/swift/account-server.conf.j2
~/openstack/my_cloud/config/swift/container-server.conf.j2
~/openstack/my_cloud/config/swift/object-server.conf.j2</pre></div></li><li class="step "><p>
          Commit the changes to git:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "changing Swift fallocate_reserve value"</pre></div></li><li class="step "><p>
          Run the configuration processor:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
          Update your deployment directory:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
          Run the Swift reconfigure playbook to deploy the change:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div>
       <p>
        If you allow your file systems to become full, you will be unable to
        delete objects or add more nodes to the system. This is because the
        system needs some free space to handle the replication process when
        adding nodes. With no free space, the replication process cannot work.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift service per-minute
        availability</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the Swift
        service reports unavailable for the previous minute.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">swiftlm-uptime-monitor</code> service runs on the first
        proxy server. It monitors the Swift endpoint and reports latency data.
        If the endpoint stops reporting, it generates this alarm.
       </p>
      </td><td valign="top">
       <p>
        There are many reasons why the endpoint may stop running. Check:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Is <code class="literal">haproxy</code> running on the control nodes?
         </p></li><li class="listitem "><p>
          Is <code class="literal">swift-proxy-server</code> running on the Swift proxy
          servers?
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift rsync connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the specified rsync server
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The rsync daemon on the
        specified node cannot be contacted. The most probable cause is that the
        node is down. The rsync service might also have been stopped on the
        node.
       </p>
      </td><td valign="top">
       <p>
        Reboot the server if it is down.
       </p>
       <p>
        Attempt to restart rsync with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">systemctl restart rsync.service</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift smart array controller
        status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Smart Array.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Smart Array or Smart
        HBA controller has a fault or a component of the controller (such as a
        battery) is failed or caching is disabled.
       </p>
       <p>
        The HPE Smart Storage Administrator (HPE SSA) CLI component will have
        to be installed for SSACLI status to be reported. HPE-specific binaries
        that are not based on open source are distributed directly from and
        supported by HPE. To download and install the SSACLI utility, please
        refer to:
        <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the controllers:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; controller show all detail</pre></div>
       <p>
        For hardware failures (such as failed battery), replace the failed
        component. If the cache is disabled, reenable the cache.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift physical drive status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Physical Drive.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>A disk drive on the
        server has failed or has warnings.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported and run these commands to find out the status of
        the drive:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift logical drive status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Logical Drive.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A LUN on the server is
        degraded or has failed.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the LUN:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> If the
        <code class="literal">service</code> dimension is
        <code class="literal">object-store</code>, see the description of the "Swift
        Service" alarm for possible causes.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">service</code> dimension is
        <code class="literal">object-storage</code>, see the description of the "Swift
        Service" alarm for possible mitigation tasks.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> If the
        <code class="literal">service</code> dimension is
        <code class="literal">object-store</code>, see the description of the "Swift host
        socket connect" alarm for possible causes.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">service</code> dimension is
        <code class="literal">object-storage</code>, see the description of the "Swift
        host socket connect" alarm for possible mitigation tasks.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">
       <p>
        Find the service that is consuming too much disk space. Look at the
        logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
        level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
        error message, do what is needed to resolve the error. If old log files
        exist, configure log rotate to remove them. You could also choose to
        remove old log files by hand after backing them up if needed.
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.5.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BLOCK-STORAGE in Storage section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these Cinder services, specified by the <code class="literal">component</code>
        dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          cinder-api
         </p></li><li class="listitem "><p>
          cinder-backup
         </p></li><li class="listitem "><p>
          cinder-scheduler
         </p></li><li class="listitem "><p>
          cinder-volume
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node. Review the associated logs.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the <code class="filename">cinder-start.yml</code> playbook to start the
          process back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-start.yml
--limit &lt;hostname&gt;</pre></div><div id="id-1.6.17.4.5.5.4.2.1.4.1.2.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
           The <code class="literal">--limit &lt;hostname&gt;</code> switch is optional.
           If it is included, then the <code class="literal">&lt;hostname&gt;</code> you
           should use is the host where the alarm was raised.
          </p></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name=cinder-backup</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Alert may be incorrect if the service has migrated. Validate that the
        service is intended to be running on this node before restarting the
        service. Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name=cinder-scheduler</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node. Review the associated logs.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the <code class="filename">cinder-start.yml</code> playbook to start the
          process back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-start.yml \
--limit &lt;hostname&gt;</pre></div><div id="id-1.6.17.4.5.5.4.2.1.4.3.2.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
           The <code class="literal">--limit &lt;hostname&gt;</code> switch is optional.
           If it is included, then the <code class="literal">&lt;hostname&gt;</code> you
           should use is the host where the alarm was raised.
          </p></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name=cinder-volume</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Alert may be incorrect if the service has migrated. Validate that the
        service is intended to be running on this node before restarting the
        service. Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Cinder backup running
        &lt;hostname&gt; check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Cinder backup singleton
        check.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Backup process is one of
        the following:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          It is running on a node it should not be on
         </p></li><li class="listitem "><p>
          It is not running on a node it should be on
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Run the <code class="filename">cinder-migrate-volume.yml</code> playbook to
        migrate the volume and back up to the correct node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook to migrate the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Cinder volume running
        &lt;hostname&gt; check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Cinder volume singleton
        check.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">cinder-volume</code> process is either:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          running on a node it should not be on, or
         </p></li><li class="listitem "><p>
          not running on a node it should be on
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Run the <code class="filename">cinder-migrate-volume.yml</code> playbook to
        migrate the volume and backup to correct node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook to migrate the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Storage faulty lun check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if local LUNs on
        your HPE servers using smartarray are not OK.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A LUN on the server is
        degraded or has failed.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the LUN:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Storage faulty drive check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the local disk
        drives on your HPE servers using smartarray are not OK.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A disk drive on the
        server has failed or has warnings.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported and run these commands to find out the status of
        the drive:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">
       <p>
        Find the service that is consuming too much disk space. Look at the
        logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
        level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
        error message, do what is needed to resolve the error. If old log files
        exist, configure log rotate to remove them. You could also choose to
        remove old log files by hand after backing them up if needed.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="networking-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Alarms</span> <a title="Permalink" class="permalink" href="#networking-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>networking-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Networking section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.6.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: NETWORKING</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. Separate alarms for each of these Neutron
        services, specified by the <code class="literal">component</code> dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         ipsec/charon
        </p></li><li class="listitem "><p>
         neutron-openvswitch-agent
        </p></li><li class="listitem "><p>
         neutron-l3-agent
        </p></li><li class="listitem "><p>
         neutron-dhcp-agent
        </p></li><li class="listitem "><p>
         neutron-metadata-agent
        </p></li><li class="listitem "><p>
         neutron-server
        </p></li><li class="listitem "><p>
         neutron-vpn-agent
        </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Check the status of the networking status:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li><li class="step "><p>
         Make note of the failed service names and the affected hosts which you
         will use to review the logs later.
        </p></li><li class="step "><p>
         Using the affected hostname(s) from the previous output, run the
         Neutron start playbook to restart the services:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-start.yml \
--limit &lt;hostname&gt;</pre></div><div id="id-1.6.17.4.5.6.3.2.1.4.1.2.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
          You can pass multiple hostnames with
          <code class="literal">--limit</code> option by separating them with a colon
          <code class="literal">:</code>.
         </p></div></li><li class="step "><p>
         Check the status of the networking service again:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li><li class="step "><p>
         Once all services are back up, you can SSH to the affected host(s) and
         review the logs in the location below for any errors around the time
         that the alarm triggered:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/&lt;service_name&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = neutron-rootwrap</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Currently <code class="literal">neutron-rootwrap</code> is only used to run
       <code class="literal">ovsdb-client</code>. To restart this process, use these
       steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         SSH to the affected host(s).
        </p></li><li class="step "><p>
         Restart the process:
        </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart neutron-openvswitch-agent</pre></div></li><li class="step "><p>
         Review the logs at the location below for errors:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/neutron-openvswitch-agent.log</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> neutron api health check
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process is stuck if the
        <code class="literal">neutron-server</code> Process Check is not OK.
       </p>
      </td><td valign="top">
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         SSH to the affected host(s).
        </p></li><li class="step "><p>
         Run this command to restart the <code class="literal">neutron-server</code>
         process:
        </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart neutron-server</pre></div></li><li class="step "><p>
         Review the logs at the location below for errors:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/neutron-server.log</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> neutron api health check
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The node
        crashed. Alternatively, only connectivity might have been lost if the
        local node HTTP Status is OK or UNKNOWN.
       </p>
      </td><td valign="top">Reboot the node if it crashed or diagnose the networking
      connectivity failures between the local and remote nodes. Review the
      logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Directory Log Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.6.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: DNS in Networking section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-zone-manager</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-ZMG'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-zone-manager.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-pool-manager</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-PMG'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-pool-manager.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-central</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-CEN'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-central.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-api</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-API'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-api.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-mdns</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen">         <code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-MDN'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-mdns.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        designate-api</code> This alarm will also have the
        <code class="literal">api_endpoint</code> and
        <code class="literal">monitored_host_types</code> dimensions defined. The likely
        cause and mitigation steps are the same for both.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The API is unresponsive.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-API,DES-CEN'</pre></div></li></ol></div></div>
      <p>
       Review the logs located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-api.log
/var/log/designate/designate-central.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Directory Log Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.6.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BIND in Networking section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = pdns_server</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the PowerDNS start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts bind-start.yml</pre></div></li></ol></div></div>
      <p>
       Review the log located at, querying against <code class="literal">process =
       pdns_server</code>:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/syslog</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = named</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Bind start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts bind-start.yml</pre></div></li></ol></div></div>
      <p>
       Review the log located at, querying against <code class="literal">process =
       named</code>:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/syslog</pre></div>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="identity-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity Alarms</span> <a title="Permalink" class="permalink" href="#identity-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-identity_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-identity_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>identity-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Identity section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.7.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: IDENTITY-SERVICE</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-identity_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-identity_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the Keystone public endpoint directly.
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
api_endpoint=public</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Keystone service is
        down on the affected node.
       </p>
      </td><td valign="top">
       <p>
        Restart the Keystone service on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Keystone start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the Keystone admin endpoint directly
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
api_endpoint=admin</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Keystone service is
        down on the affected node.
       </p>
      </td><td valign="top">
       <p>
        Restart the Keystone service on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Keystone start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the Keystone admin endpoint via the virtual IP address (HAProxy)
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
monitored_host_type=vip</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Keystone service is
        unreachable via the virtual IP address.
       </p>
      </td><td valign="top">
       <p>
        If neither the <code class="literal">api_endpoint=public</code> or
        <code class="literal">api_endpoint=admin</code> alarms are triggering at the same
        time then there is likely a problem with haproxy.
       </p>
       <p>
        You can restart the haproxy service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use this playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these Glance services, specified by the <code class="literal">component</code>
        dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          keystone-main
         </p></li><li class="listitem "><p>
          keystone admin
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        You can restart the Keystone service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use this playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the logs in <code class="literal">/var/log/keystone</code> on the affected
        node.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="telemetry-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Telemetry Alarms</span> <a title="Permalink" class="permalink" href="#telemetry-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>telemetry-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Telemetry section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.8.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: TELEMETRY</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.8.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the
        <code class="literal">ceilometer-agent-notification</code> process is not
        running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs on the alarming host in the following location for the
        cause:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/ceilometer/ceilometer-agent-notification-json.log</pre></div>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Ceilometer start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the
        <code class="literal">ceilometer-polling</code> process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs on the alarming host in the following location for the
        cause:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/ceilometer/ceilometer-polling-json.log</pre></div>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Ceilometer start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: METERING in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: KAFKA in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Persister Metric Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the Persister
        consumer group is not keeping up with the incoming messages on the
        metric topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Verify that all of the monasca-persister services are up with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager
         </p></li><li class="step "><p>
          Verify that all of the <code class="literal">monasca-persister</code> services
          are up with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Determining which alarms are
        firing can help diagnose likely causes. For example, if the alarm is
        alerting all on one machine it could be the machine. If one topic
        across multiple machines it is likely the consumers of that topic, etc.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Alarm Transition Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        consumer group is not keeping up with the incoming messages on the
        alarm state transition topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Check that monasca-thresh and monasca-notification are up.
       </p>
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Which alarms are firing can help
        diagnose likely causes. For example:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If all alarms are on the same machine, the machine could be at fault.
         </p></li><li class="listitem "><p>
          If one topic is shared across multiple machines, the consumers of
          that topic are likely at fault.
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Kronos Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the Kronos
        consumer group is not keeping up with the incoming messages on the
        metric topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Which alarms are firing can help
        diagnose likely causes. For example:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If all alarms are on the same machine, the machine could be at fault.
         </p></li><li class="listitem "><p>
          If one topic is shared across multiple machines, the consumers of
          that topic are likely at fault.
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = kafka.Kafka</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the kafka service with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Start the kafka service back up with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li></ol></div></div>
       <p>
        Review the logs in <code class="filename">/var/log/kafka/server.log</code>
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: LOGGING in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Beaver Memory Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Beaver is using more
        memory than expected. This may indicate that it cannot forward messages
        and its queue is filling up. If you continue to see this, see the
        troubleshooting guide.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Overloaded system or
        services with memory leaks.
       </p>
      </td><td valign="top">Log on to the reporting host to investigate high memory users.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Audit Log Partition Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="literal">/var/audit</code> disk space usage has crossed low
        watermark. If the high watermark is reached, logrotate will be run to
        free up disk space. If needed, adjust:
       </p>
       <div class="verbatim-wrap"><pre class="screen">var_audit_low_watermark_percent</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to DEBUG instead of INFO level. Another reason could be due
        to a repeating error message filling up the log files. Finally, it
        could be due to log rotate not configured properly so old log files are
        not being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If DEBUG log entries exist, set the logging level to INFO. If
       the logs are repeatedly logging an error message, do what is needed to
       resolve the error. If old log files exist, configure log rotate to
       remove them. You could also choose to remove old log files by hand after
       backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Audit Log Partition High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="literal">/var/audit</code> volume is running low on disk space.
        Logrotate will be run now to free up space. If needed, adjust:
       </p>
       <div class="verbatim-wrap"><pre class="screen">var_audit_high_watermark_percent</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to DEBUG instead of INFO level. Another reason could be due
        to a repeating error message filling up the log files. Finally, it
        could be due to log rotate not configured properly so old log files are
        not being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If DEBUG log entries exist, set the logging level to INFO. If
       the logs are repeatedly logging an error message, do what is needed to
       resolve the error. If old log files exist, configure log rotate to
       remove them. You could also choose to remove old log files by hand after
       backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Unassigned Shards</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> component =
        elasticsearch; Elasticsearch unassigned shards count is greater than
        0.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Environment could be
        misconfigured.
       </p>
      </td><td valign="top">
       <p>
        To find the unassigned shards, run the following command on the Cloud Lifecycle Manager
        from the <code class="filename">~/scratch/ansible/next/ardana/ansible</code>
        directory:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a \
"curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</pre></div>
       <p>
        This shows which shards are unassigned, like this:
       </p>
<div class="verbatim-wrap"><pre class="screen">logstash-2015.10.21 4 p UNASSIGNED ... 10.240.75.10 NodeName</pre></div>
       <p>
        The last column shows the name that Elasticsearch uses for the node
        that the unassigned shards are on. To find the actual host name, run:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a \
"curl localhost:9200/_nodes/_all/name?pretty -s"</pre></div>
       <p>
        When you find the host name, take the following steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Make sure the node is not out of disk space, and free up space if
          needed.
         </p></li><li class="step "><p>
          Restart the node (use caution, as this may affect other services as
          well).
         </p></li><li class="step "><p>
          Make sure all versions of Elasticsearch are the same:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR -m shell -a \
"curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</pre></div></li><li class="step "><p>
          Contact customer support.
         </p></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Number of Log Entries</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Elasticsearch Number of
        Log Entries: <code class="literal">component = elasticsearch;</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number of log
        entries may get too large.
       </p>
      </td><td valign="top">Older versions of Kibana (version 3 and earlier) may hang if the
       number of log entries is too large (for example, above 40,000), and the
       page size would need to be small enough (about 20,000 results), because
       if it is larger (for example, 200,000), it may hang the browser, but
       Kibana 4 should not have this issue.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Field Data Evictions</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Elasticsearch Field
        Data Evictions count is greater than 0: <code class="literal">component =
        elasticsearch</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Field Data Evictions may
        be found even though it is nowhere near the limit set.
       </p>
      </td><td valign="top">
       <p>
        The <code class="literal">elasticsearch_indices_fielddata_cache_size</code> is
        set to <code class="literal">unbounded</code> by default. If this is set by the
        user to a value that is insufficient, you may need to increase this
        configuration parameter or set it to <code class="literal">unbounded</code> and
        run a reconfigure using the steps below:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Edit the configuration file below and change the value for
          <code class="literal">elasticsearch_indices_fielddata_cache_size</code> to your
          desired value:
         </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/main.yml</pre></div></li><li class="step "><p>
          Commit the changes to git:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Elasticsearch fielddata cache size"</pre></div></li><li class="step "><p>
          Run the configuration processor:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
          Update your deployment directory:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
          Run the Logging reconfigure playbook to deploy the change:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these logging services, specified by the
        <code class="literal">process_name</code> dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          elasticsearch
         </p></li><li class="listitem "><p>
          logstash
         </p></li><li class="listitem "><p>
          beaver
         </p></li><li class="listitem "><p>
          apache2
         </p></li><li class="listitem "><p>
          kibana
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        On the affected node, attempt to restart the process.
       </p>
       <p>
        If the <code class="command">elasticsearch</code> process has crashed, use:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart elasticsearch</pre></div>
       <p>
        If the logstash process has crashed, use:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart logstash</pre></div>
       <p>
        The rest of the processes can be restarted using similar commands,
        listed here:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart beaver
<code class="prompt user">ardana &gt; </code>sudo systemctl restart apache2
<code class="prompt user">ardana &gt; </code>sudo systemctl restart kibana</pre></div>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MONASCA-TRANSFORM in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.8.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">process_name =
        pyspark</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart process on affected node. Review logs.
       </p>
       <p>
        Child process of <code class="literal">spark-worker</code> but created once the
        <code class="literal">monasca-transform</code> process begins processing streams.
        If the process fails on one node only, along with the pyspark process,
        it is likely that the <code class="literal">spark-worker</code> has failed to
        connect to the elected leader of the <code class="literal">spark-master</code>
        service. In this case the <code class="literal">spark-worker</code> service
        should be started on the affected node. If on multiple nodes check the
        <code class="literal">spark-worker</code>, <code class="literal">spark-master</code> and
        <code class="literal">monasca-transform</code> services and logs. If the
        <code class="literal">monasca-transform</code> or <code class="literal">spark</code>
        services have been interrupted this process may not re-appear for up to
        ten minutes (the stream processing interval).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name =
org.apache.spark.executor.CoarseGrainedExecutorBackend</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart process on affected node. Review logs.
       </p>
       <p>
        Child process of <code class="literal">spark-worker</code> but created once the
        <code class="literal">monasca-transform</code> process begins processing streams.
        If the process fails on one node only, along with the pyspark process,
        it is likely that the <code class="literal">spark-worker</code> has failed to
        connect to the elected leader of the <code class="literal">spark-master</code>
        service. In this case the <code class="literal">spark-worker</code> service
        should be started on the affected node. If on multiple nodes check the
        <code class="literal">spark-worker</code>, <code class="literal">spark-master</code> and
        <code class="literal">monasca-transform</code> services and logs. If the
        <code class="literal">monasca-transform</code> or <code class="literal">spark</code>
        services have been interrupted this process may not re-appear for up to
        ten minutes (the stream processing interval).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">process_name =
        monasca-transform</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">Restart the service on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MONITORING in Telemetery section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.8.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Persister Health Check
        <code class="literal">component = monasca-persister</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed
        or a dependency is out.
       </p>
      </td><td valign="top">
       <p>
        If the process has crashed, restart it using the steps below. If a
        dependent service is down, address that issue.
       </p>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags persister</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> API Health Check
        <code class="literal">component = monasca-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed
        or a dependency is out.
       </p>
      </td><td valign="top">
       <p>
        If the process has crashed, restart it using the steps below. If a
        dependent service is down, address that issue.
       </p>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Monasca Agent Collection Time</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the elapsed
        time the <code class="literal">monasca-agent</code> takes to collect metrics is
        high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy load on the box or
        a stuck agent plug-in.
       </p>
      </td><td valign="top">
       <p>
        Address the load issue on the machine. If needed, restart the agent
        using the steps below:
       </p>
       <p>
        Restart the agent on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-agent</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">component = kafka</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if Kafka is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Verify that Kafka is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = monasca-notification</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags notification</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags notification</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags notification</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = monasca-agent</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the agent on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-agent</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = monasca-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        &gt;Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name =
        monasca-persister</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags persister</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.nimbus
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="filename">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.6.17.4.5.8.8.2.1.4.9.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart <code class="literal">monasca-thresh</code>, if necessary, with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-thresh</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.supervisor
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="literal">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.6.17.4.5.8.8.2.1.4.10.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart monasca-thresh with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the monasca-thresh service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Start the monasca-thresh service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.worker
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="literal">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.6.17.4.5.8.8.2.1.4.11.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart <code class="literal">monasca-thresh</code> with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the <code class="literal">monasca-thresh</code> service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Start the <code class="literal">monasca-thresh</code> service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal"></code>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = monasca-thresh
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-thresh</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="console-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Console Alarms</span> <a title="Permalink" class="permalink" href="#console-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-console_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-console_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>console-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Console section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: HTTP Status</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span>
       <code class="literal">service=ops-console</code>
      </p>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> The Operations Console is
       unresponsive
      </p>
     </td><td valign="top">
      <p>
       Review logs in <code class="filename">/var/log/ops-console</code> and logs in
       <code class="filename">/var/log/apache2</code>. Restart ops-console by running
       the following commands on the Cloud Lifecycle Manager:
      </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-start.yml</pre></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: Process Check</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when the specified
       process is not running:
       <code class="literal">process_name=leia-leia_monitor</code>
      </p>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Process crashed or
       unresponsive.
      </p>
     </td><td valign="top">
      <p>
       Review logs in <code class="filename">/var/log/ops-console</code>. Restart
       ops-console by running the following commands on the Cloud Lifecycle Manager:
      </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-start.yml</pre></div>
     </td></tr></tbody></table></div></div><div class="sect3" id="system-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Alarms</span> <a title="Permalink" class="permalink" href="#system-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-system_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-system_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>system-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the System section and are set up per
  <code class="literal">hostname</code> and/or <code class="literal">mount_point</code>.
 </p><div class="sect4" id="id-1.6.17.4.5.10.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: SYSTEM</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-system_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-system_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: CPU Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on high CPU usage.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy load or runaway
        processes.
       </p>
      </td><td valign="top">Log onto the reporting host and diagnose the heavy CPU usage.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        elasticsearch</code> Elasticsearch disk low watermark. Backup
        indices. If high watermark is reached, indices will be deleted. Adjust
        curator_low_watermark_percent, curator_high_watermark_percent, and
        elasticsearch_max_total_indices_size_in_bytes if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Running out of disk
        space for <code class="filename">/var/lib/elasticsearch</code>.
       </p>
      </td><td valign="top">
       <p>
        Free up space by removing indices (backing them up first if desired).
        Alternatively, adjust <code class="literal">curator_low_watermark_percent</code>,
        <code class="literal">curator_high_watermark_percent</code>, and/or
        <code class="literal">elasticsearch_max_total_indices_size_in_bytes</code> if
        needed.
       </p>
       <p>
        For more information about how to back up your centralized logs, see
        <a class="xref" href="#central-log-configure-settings" title="12.2.5. Configuring Centralized Logging">Section 12.2.5, “Configuring Centralized Logging”</a>.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        elasticsearch</code> Elasticsearch disk high watermark. Attempting
        to delete indices to free disk space. Adjust
        <code class="literal">curator_low_watermark_percent</code>,
        <code class="literal">curator_high_watermark_percent</code>, and
        <code class="literal">elasticsearch_max_total_indices_size_in_bytes</code> if
        needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Running out of disk
        space for <code class="filename">/var/lib/elasticsearch</code>
       </p>
      </td><td valign="top">
       <p>
        Verify that disk space was freed up by the curator. If needed, free up
        additional space by removing indices (backing them up first if
        desired). Alternatively, adjust curator_low_watermark_percent,
        curator_high_watermark_percent, and/or
        elasticsearch_max_total_indices_size_in_bytes if needed.
       </p>
       <p>
        For more information about how to back up your centralized logs, see
        <a class="xref" href="#central-log-configure-settings" title="12.2.5. Configuring Centralized Logging">Section 12.2.5, “Configuring Centralized Logging”</a>.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Log Partition Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="filename">/var/log</code> disk space usage has crossed the low
        watermark. If the high watermark is reached,
        <code class="literal">logrotate</code> will be run to free up disk space. Adjust
        <code class="literal">var_log_low_watermark_percent</code> if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Log Partition High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="filename">/var/log</code> volume is running low on disk space.
        Logrotate will be run now to free up space. Adjust
        <code class="literal">var_log_high_watermark_percent</code> if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Crash Dump Count</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if it receives any
        metrics with <code class="literal">crash.dump_count</code> &gt; 0
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> When a crash dump is
        generated by kdump, the crash dump file is put into the
        <code class="filename">/var/crash</code> directory by default. Any crash dump
        files in this directory will cause the
        <code class="literal">crash.dump_count</code> metric to show a value greater than
        0.
       </p>
      </td><td valign="top">
       <p>
        Analyze the crash dump file(s) located in
        <code class="filename">/var/crash</code> on the host that generated the alarm to
        try to determine if a service or hardware caused the crash.
       </p>
       <p>
        Move the file to a new location so that a developer can take a look at
        it. Make sure all of the processes are back up after the crash (run the
        <code class="literal">&lt;service&gt;-status.yml</code> playbooks). When the
        <code class="filename">/var/crash</code> directory is empty, the <code class="literal">Crash
        Dump Count</code> alarm should transition back to OK.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Disk Inode Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Nearly out of inodes for
        a partition, as indicated by the <code class="literal">mount_point</code>
        reported.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Many files on the disk.
       </p>
      </td><td valign="top">Investigate cleanup of data or migration to other partitions.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Disk Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> High disk usage, as
        indicated by the <code class="literal">mount_point</code> reported.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Large files on the disk.
       </p>
      </td><td valign="top">
       <p>
        Investigate cleanup of data or migration to other partitions.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Host Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alerts when a host is
        unreachable. <code class="literal">test_type = ping</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Host or network is down.
       </p>
      </td><td valign="top">If a single host, attempt to restart the system. If multiple
      hosts, investigate network issues.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Memory Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> High memory usage.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Overloaded system or
        services with memory leaks.
       </p>
      </td><td valign="top">Log onto the reporting host to investigate high memory users.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Network Errors</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on a high network
        error rate.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Bad network or cabling.
       </p>
      </td><td valign="top">Take this host out of service until the network can be fixed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: NTP Time Sync</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the NTP time
        offset is high.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and check if the ntp service is running.
       </p>
       <p>
        If it is running, then use these steps:
       </p>
       <div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
          Stop the service:
         </p><div class="verbatim-wrap"><pre class="screen">service ntpd stop</pre></div></li><li class="listitem "><p>
          Resynchronize the node's time:
         </p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/ntpdate -b  &lt;ntp-server&gt;</pre></div></li><li class="listitem "><p>
          Restart the ntp service:
         </p><div class="verbatim-wrap"><pre class="screen">service ntp start</pre></div></li><li class="listitem "><p>
          Restart rsyslog:
         </p><div class="verbatim-wrap"><pre class="screen">service rsyslog restart</pre></div></li></ol></div>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="other-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Services Alarms</span> <a title="Permalink" class="permalink" href="#other-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>other-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Other Services section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.11.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: APACHE</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Apache Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on failure to
        reach the Apache status endpoint.
       </p>
      </td><td valign="top"> </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = apache2</code>
       </p>
      </td><td valign="top">If the Apache process goes down, connect to the affected node via
      SSH and restart it with this command: <code class="command">sudo systemctl restart
      apache2</code>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Apache Idle Worker Count</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when there are no
        idle workers in the Apache server.
       </p>
      </td><td valign="top"> </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BACKUP in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = freezer-scheduler</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">Restart the process on the affected node. Review the associated logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable: <code class="literal">process_name =
        freezer-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> see
        <code class="literal">Description</code>
       </p>
      </td><td valign="top">see <code class="literal">Description</code></td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: HAPROXY in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = haproxy</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> HA Proxy is not running
        on this machine.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook on the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ARDANA-UX-SERVICES in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
      </td><td valign="top"> </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: KEY-MANAGER in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal"></code>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = barbican-api</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Barbican start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
<div class="verbatim-wrap"><pre class="screen">component = barbican-api
api_endpoint = public or internal</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The endpoint is not
        responsive, it may be down.
       </p>
      </td><td valign="top">
       <p>
        For the HTTP Status alarms for the public and internal endpoints,
        restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the barbican service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Restart the barbican service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Examine the logs in <code class="filename">/var/log/barbican/</code> for
        possible error messages.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
<div class="verbatim-wrap"><pre class="screen">component = barbican-api
monitored_host_type = vip</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Barbican API on the
        admin virtual IP is down.
       </p>
      </td><td valign="top">This alarm is verifying access to the Barbican API via the virtual
      IP address (HAProxy). If this check is failing but the other two HTTP
      Status alarms for the key-manager service are not then the issue is
      likely with HAProxy so you should view the alarms for that service. If
      the other two HTTP Status alarms are alerting as well then restart
      Barbican using the steps listed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MYSQL in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: MySQL Slow Query Rate</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the slow
        query rate is high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The system load is too
        high.
       </p>
      </td><td valign="top">This could be an indication of near capacity limits or an exposed
      bad query. First, check overall system load and then investigate MySQL
      details.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> MySQL crashed.
       </p>
      </td><td valign="top">Restart MySQL on the affected node.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OCTAVIA in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. There are individual alarms for each of these
        processes:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          octavia-worker
         </p></li><li class="listitem "><p>
          octavia-housekeeping
         </p></li><li class="listitem "><p>
          octavia-api
         </p></li><li class="listitem "><p>
          octavia-health-manager
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Octavia start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The <code class="literal">octavia-api
        </code>process could be down or you could be experiencing an issue
        with either haproxy or another network related issue.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">octavia-api</code> process is down, restart it on
        the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Octavia start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If it is not the <code class="literal">octavia-process</code> that is the issue,
        then check if there is an issue with <code class="literal">haproxy</code> or
        possibly a network issue and troubleshoot accordingly.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ORCHESTRATION in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. There are individual alarms for each of these
        processes:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          heat-api
         </p></li><li class="listitem "><p>
          heat-api-cfn
         </p></li><li class="listitem "><p>
          heat-api-cloudwatch
         </p></li><li class="listitem "><p>
          heat-engine
         </p></li></ul></div>
       <p>
        heat-api process check on each node
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop all the Heat processes:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li><li class="step "><p>
          Start the Heat processes back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li></ol></div></div>
       <p>
        Review the relevant log at the following locations on the affected
        node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log
/var/log/heat/heat-engine.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          heat-api
         </p></li><li class="listitem "><p>
          heat-api-cfn
         </p></li><li class="listitem "><p>
          heat-api-cloudwatch
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Restart the Heat service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop all the Heat processes:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li><li class="step "><p>
          Start the Heat processes back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li></ol></div></div>
       <p>
        Review the relevant log at the following locations on the affected
        node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OVSVAPP-SERVICEVM in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span>Alarms when the specified
        process is not running:
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = ovs-vswitchd
process_name = neutron-ovsvapp-agent
process_name = ovsdb-server</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: RABBITMQ in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = rabbitmq
process_name = epmd</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: SPARK in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = org.apache.spark.deploy.master.Master
process_name = org.apache.spark.deploy.worker.Worker</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.14"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: WEB-UI in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Apache is not running or
        there is a misconfiguration.
       </p>
      </td><td valign="top">Check that Apache is running; investigate Horizon logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.15"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ZOOKEEPER in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.5.11.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name =
        org.apache.zookeeper.server</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">Restart the process on the affected node. Review the associated
      logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: ZooKeeper Latency</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the ZooKeeper
        latency is high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy system load.
       </p>
      </td><td valign="top">Check the individual system as well as activity across the entire
      service.</td></tr></tbody></table></div></div></div><div class="sect3" id="esx-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX vCenter Plugin Alarms</span> <a title="Permalink" class="permalink" href="#esx-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-esx_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-esx_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>esx-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms relate to your ESX cluster, if you are utilizing one.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster CPU Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when average of CPU
       usage for a particular cluster exceeds 90% continuously for 3 polling
       cycles.
      </p>
      <p>
       Alarm will have the following dimension:
      </p>
<div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Virtual machines are
       consuming more than 90% of allocated vCPUs.
      </p>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Reduce the load on virtual machines with high consumption by
         restarting/stopping one or more services.
        </p></li><li class="listitem "><p>
         Add more vCPUs to the host(s) attached to the cluster.
        </p></li></ul></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster Disk Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span>
      </p>
      <div class="itemizedlist " id="ul-ctc-lpy-5x"><ul class="itemizedlist"><li class="listitem "><p>
         Alarms when the total size of the all shared datastores attached to
         the cluster exceeds 90% of their total allocated capacity.
        </p></li></ul></div>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Or in the case of cluster having a single host, the size of non-shared
         datastore exceeds 90% of its allocated capacity.
        </p></li><li class="listitem "><p>
         Alarm will have the following dimension:
        </p><div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div></li></ul></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span>
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Virtual machines occupying the storage.
        </p></li><li class="listitem "><p>
         Large file or image being copied on the datastore(s).
        </p></li></ul></div>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Check the virtual machines that are consuming more disk space. Delete
         unnecessary files.
        </p></li><li class="listitem "><p>
         Delete unnecessary files and images from database(s).
        </p></li><li class="listitem "><p>
         Add storage to the datastore(s).
        </p></li></ul></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster Memory Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when average of RAM
       memory usage for a particular cluster, exceeds 90% continuously for 3
       polling cycles.
      </p>
      <p>
       Alarm will have the following dimension:
      </p>
<div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Virtual machines are
       consuming more than 90% of their total allocated memory.
      </p>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Reduce the load on virtual machines with high consumption by
         restarting or stopping one or more services.
        </p></li><li class="listitem "><p>
         Add more memory to the host(s) attached to the cluster.
        </p></li></ul></div>
     </td></tr></tbody></table></div></div></div><div class="sect2" id="topic-ask-vgb-dv"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support Resources</span> <a title="Permalink" class="permalink" href="#topic-ask-vgb-dv">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span>topic-ask-vgb-dv</li></ul></div></div></div></div><p>
  To solve issues in your cloud, consult the Knowledge Base or contact
  <span class="phrase"><span class="phrase">Sales Engineering</span></span>.
 </p><div class="sect3" id="kb"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Knowledge Base</span> <a title="Permalink" class="permalink" href="#kb">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span>kb</li></ul></div></div></div></div><p>
   Support information is available at the SUSE Support page <a class="link" href="https://www.suse.com/products/suse-openstack-cloud/" target="_blank">https://www.suse.com/products/suse-openstack-cloud/</a>. This page
   offers access to the Knowledge Base, forums and documentation.
  </p></div><div class="sect3" id="id-1.6.17.4.6.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Contacting SUSE Support</span> <a title="Permalink" class="permalink" href="#id-1.6.17.4.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The central location for information about accessing and using SUSE
   Technical Support is available at <a class="link" href="https://www.suse.com/support/handbook/" target="_blank">https://www.suse.com/support/handbook/</a>. This page has
   guidelines and links to many online support services, such as support
   account management, incident reporting, issue reporting, feature requests,
   training, consulting.
  </p></div></div></div><div class="sect1" id="troubleshooting-controlplane"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-controlplane">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_controlplane.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_controlplane.xml</li><li><span class="ds-label">ID: </span>troubleshooting-controlplane</li></ul></div></div></div></div><p>
  Troubleshooting procedures for control plane services.
 </p><div class="sect2" id="recoverrabbit"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding and Recovering RabbitMQ after Failure</span> <a title="Permalink" class="permalink" href="#recoverrabbit">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>recoverrabbit</li></ul></div></div></div></div><p>
  RabbitMQ is the message queue service that runs on each of your controller
  nodes and brokers communication between multiple services in your
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud environment. It is important for cloud operators to
  understand how different troubleshooting scenarios affect RabbitMQ so they
  can minimize downtime in their environments. We are going to discuss multiple
  scenarios and how it affects RabbitMQ. We will also explain how you can
  recover from them if there are issues.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-recover-rabbit-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How upgrades affect RabbitMQ</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-recover-rabbit-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-recover-rabbit-xml-7</li></ul></div></div></div></div><p>
   There are two types of upgrades within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> -- major and minor. The
   effect that the upgrade process has on RabbitMQ depends on these types.
  </p><p>
   A major upgrade is defined by an erlang change or major version upgrade of
   RabbitMQ. A minor upgrade would be an upgrade where RabbitMQ stays within
   the same version, such as v3.4.3 to v.3.4.6.
  </p><p>
   During both types of upgrades there may be minor blips in the authentication
   process of client services as the accounts are recreated.
  </p><p>
   <span class="bold"><strong>RabbitMQ during a major upgrade</strong></span>
  </p><p>
   There will be a RabbitMQ service outage while the upgrade is performed.
  </p><p>
   During the upgrade, high availability consistency is compromised -- all but
   the primary node will go down and will be reset, meaning their database
   copies are deleted. The primary node is not taken down until the last step
   and then it is upgrade. The database of users and permissions is maintained
   during this process. Then the other nodes are brought back into the cluster
   and resynchronized.
  </p><p>
   <span class="bold"><strong>RabbitMQ during a minor upgrade</strong></span>
  </p><p>
   Minor upgrades are performed node by node. This "rolling" process means
   there should be no overall service outage because each node is taken out of
   its cluster in turn, its database is reset, and then it is added back to the
   cluster and resynchronized.
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-recover-rabbit-xml-8"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How RabbitMQ is affected by other operational processes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-recover-rabbit-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-recover-rabbit-xml-8</li></ul></div></div></div></div><p>
   There are operational tasks, such as <a class="xref" href="#stop-restart" title="13.1.1.1. Bringing Down Your Cloud: Services Down Method">Section 13.1.1.1, “Bringing Down Your Cloud: Services Down Method”</a>, where
   you use the <code class="filename">ardana-stop.yml</code> and
   <code class="filename">ardana-start.yml</code> playbooks to gracefully restart your cloud.
   If you use these playbooks, and there are no errors associated with them
   forcing you to troubleshoot further, then RabbitMQ is brought down
   gracefully and brought back up. There is nothing special to note regarding
   RabbitMQ in these normal operational processes.
  </p><p>
   However, there are other scenarios where an understanding of RabbitMQ is
   important when a graceful shutdown did not occur.
  </p><p>
   These examples that follow assume you are using one of the entry-scale
   models where RabbitMQ is hosted on your controller node cluster. If you are
   using a mid-scale model or have a dedicated cluster that RabbitMQ lives on
   you may need to alter the steps accordingly. To determine which nodes
   RabbitMQ is on you can use the <code class="filename">rabbit-status.yml</code> playbook
   from your Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div><p>
   <span class="bold"><strong>Your entire control plane cluster goes down</strong></span>
  </p><p>
   If you have a scenario where all of your controller nodes went down, either
   manually or via another process such as a power outage, then an
   understanding of how RabbitMQ should be brought back up is important. Follow
   these steps to recover RabbitMQ on your controller node cluster in these
   cases:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The order in which the nodes went down is key here. Locate the last node
     to go down as this will be used as the primary node when bringing the
     RabbitMQ cluster back up. You can review the timestamps in the
     <code class="filename">/var/log/rabbitmq</code> log file to determine what the last
     node was.
    </p><div id="id-1.6.17.5.3.4.8.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">primary</code> status of a node is transient, it only applies for the
      duration that this process is running. There is no long-term distinction
      between any of the nodes in your cluster. The primary node is simply
      the one that owns the RabbitMQ configuration database that will be
      synchronized across the cluster.
     </p></div></li><li class="step "><p>
     Run the <code class="filename">ardana-start.yml</code> playbook specifying the primary
     node (aka the last node down determined in the first step):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;hostname&gt;</pre></div><div id="id-1.6.17.5.3.4.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">&lt;hostname&gt;</code> value will be the "shortname" for
      your node, as found in the <code class="filename">/etc/hosts</code> file.
     </p></div></li></ol></div></div><p>
   <span class="bold"><strong>If one of your controller nodes goes down</strong></span>
  </p><p>
   First step here is to determine whether the controller that went down is the
   primary RabbitMQ host or not. The primary host is going to be the first host
   member in the <code class="literal">FND-RMQ</code> group in the file below on your
   Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</pre></div><p>
   In this example below, <code class="literal">ardana-cp1-c1-m1-mgmt</code> would be the
   primary:
  </p><div class="verbatim-wrap"><pre class="screen">[FND-RMQ-ccp-cluster1:children]
ardana-cp1-c1-m1-mgmt
ardana-cp1-c1-m2-mgmt
ardana-cp1-c1-m3-mgmt</pre></div><p>
   If your primary RabbitMQ controller node has gone down and you need to bring
   it back up, you can follow these steps. In this playbook you are using the
   <code class="literal">rabbit_primary_hostname</code> parameter to specify the hostname
   for one of the other controller nodes in your environment hosting RabbitMQ,
   which will service as the primary node in the recovery. You will also use
   the <code class="literal">--limit</code> parameter to specify the controller node you
   are attempting to bring back up.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_bringing_up&gt;</pre></div><p>
   If the node you need to bring back is <span class="bold"><strong>not</strong></span>
   the primary RabbitMQ node then you can just run the
   <code class="filename">ardana-start.yml</code> playbook with the
   <code class="literal">--limit</code> parameter and your node should recover:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_bringing_up&gt;</pre></div><p>
   <span class="bold"><strong>If you are replacing one or more of your controller
   nodes</strong></span>
  </p><p>
   The same general process noted above is used if you are removing or
   replacing one or more of your controller nodes.
  </p><p>
   If your node needs minor hardware repairs, but does not need to be replaced
   with a new node, you should use the <code class="filename">ardana-stop.yml</code> playbook
   with the <code class="literal">--limit</code> parameter to stop services on that node
   prior to removing it from the cluster.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the <code class="filename">rabbitmq-stop.yml</code> playbook, specifying the
     hostname of the node you are removing, which will remove the node from the
     RabbitMQ cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</pre></div></li><li class="step "><p>
     Run the <code class="filename">ardana-stop.yml</code> playbook, again specifying the
     hostname of the node you are removing, which will stop the rest of the
     services and prepare it to be removed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</pre></div></li></ol></div></div><p>
   If your node cannot be repaired and needs to be replaced with another
   baremetal node, any references to the replaced node must be removed from the
   RabbitMQ cluster. This is because RabbitMQ associates a cookie with each
   node in the cluster which is derived, in part, by the specific hardware.  So
   it is possible to replace a hard drive in a node. However changing a
   motherboard or replacing the node with another node entirely may cause
   RabbitMQ to stop working. When this happens, the running RabbitMQ cluster
   must be edited from a running RabbitMQ node. The following steps show how to
   do this.
  </p><p>
   In this example, controller 3 is the node being replaced with the following
   steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="step "><p>
     SSH to a running RabbitMQ cluster node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh cloud-cp1-rmq-mysql-m1-mgmt</pre></div></li><li class="step "><p>
     Force the cluster to forget the node you are removing (in this example,
     the controller 3 node).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl forget_cluster_node \
rabbit@cloud-cp1-rmq-mysql-m3-mgmt</pre></div></li><li class="step "><p>
     Confirm that the node has been removed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl cluster_status</pre></div></li><li class="step "><p>
     On the replacement node, information and services related to RabbitMQ must be removed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl stop rabbitmq-server
<code class="prompt user">ardana &gt; </code>sudo systemctl stop epmd.socket&gt;</pre></div></li><li class="step "><p>
     Verify that the epmd service has stopped (kill it if it is still running).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ps -eaf | grep epmd.</pre></div></li><li class="step "><p>
     Remove the Mnesia database directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -rf /var/lib/rabbitmq/mnesia</pre></div></li><li class="step "><p>
     Restart the RabbitMQ server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl start rabbitmq-server</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, run the <code class="filename">ardana-start.yml</code> playbook.
    </p></li></ol></div></div><p>
   If the node you are removing/replacing is your primary host then when you
   are adding it to your cluster then you will want to ensure that you specify
   a new primary host when doing so, as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_adding&gt;</pre></div><p>
   If the node you are removing/replacing is not your primary host then you can
   add it as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_adding&gt;</pre></div><p>
   <span class="bold"><strong>If one of your controller nodes has rebooted or
   temporarily lost power</strong></span>
  </p><p>
   After a single reboot, RabbitMQ will not automatically restart. This is by
   design to protect your RabbitMQ cluster. To restart RabbitMQ, you should
   follow the process below.
  </p><p>
   If the rebooted node was your primary RabbitMQ host, you will specify a
   different primary hostname using one of the other nodes in your cluster:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_rebooted&gt;</pre></div><p>
   If the rebooted node was not the primary RabbitMQ host then you can just
   start it back up with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_that_rebooted&gt;</pre></div></div><div class="sect3" id="recovery"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering RabbitMQ</span> <a title="Permalink" class="permalink" href="#recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>recovery</li></ul></div></div></div></div><p>
   In this section we will show you how to check the status of RabbitMQ and how
   to do a variety of disaster recovery procedures.
  </p><p>
   <span class="bold"><strong>Verifying the status of RabbitMQ</strong></span>
  </p><p>
   You can verify the status of RabbitMQ on each of your controller nodes by
   using the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the <code class="filename">rabbitmq-status.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div></li><li class="step "><p>
     If all is well, you should see an output similar to the following:
    </p><div class="verbatim-wrap"><pre class="screen">PLAY RECAP ********************************************************************
rabbitmq | status | Check RabbitMQ running hosts in cluster ------------- 2.12s
rabbitmq | status | Check RabbitMQ service running ---------------------- 1.69s
rabbitmq | status | Report status of RabbitMQ --------------------------- 0.32s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 4.36s
ardana-cp1-c1-m1-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m2-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m3-mgmt  : ok=2    changed=0    unreachable=0    failed=0</pre></div></li></ol></div></div><p>
   If one or more of your controller nodes are having RabbitMQ issues then
   continue reading, looking for the scenario that best matches yours.
  </p><p>
   <span class="bold"><strong>RabbitMQ recovery after a small network
   outage</strong></span>
  </p><p>
   In the case of a transient network outage, the version of RabbitMQ included
   with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is likely to recover automatically without any further
   action needed. However, if yours does not and the
   <code class="filename">rabbitmq-status.yml</code> playbook is reporting an issue then
   use the scenarios below to resolve your issues.
  </p><p>
   <span class="bold"><strong>All of your controller nodes have gone down and using
   other methods have not brought RabbitMQ back up</strong></span>
  </p><p>
   If your RabbitMQ cluster is irrecoverable and you need rapid service
   recovery because other methods either cannot resolve the issue or you do not
   have time to investigate more nuanced approaches then we provide a disaster
   recovery playbook for you to use. This playbook will tear down and reset any
   RabbitMQ services. This does have an extreme effect on your services. The
   process will ensure that the RabbitMQ cluster is recreated.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the RabbitMQ disaster recovery playbook. This generally takes around
     two minutes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</pre></div></li><li class="step "><p>
     Run the reconfigure playbooks for both Cinder (Block Storage) and Heat
     (Orchestration), if those services are present in your cloud. These
     services are affected when the fan-out queues are not recovered correctly.
     The reconfigure generally takes around five minutes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-server-configure.yml</pre></div></li><li class="step "><p>
     If you need to do a safe recovery of all the services in your environment
     then you can use this playbook. This is a more lengthy process as all
     services are inspected.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>One of your controller nodes has gone down and using
   other methods have not brought RabbitMQ back up</strong></span>
  </p><p>
   This disaster recovery procedure has the same caveats as the preceding one,
   but the steps differ.
  </p><p>
   If your primary RabbitMQ controller node has gone down and you need to
   perform a disaster recovery, use this playbook from your Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_needs_recovered&gt;</pre></div><p>
   If the controller node is not your primary, you can use this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml --limit &lt;hostname_of_node_that_needs_recovered&gt;</pre></div><p>
   No reconfigure playbooks are needed because all of the fan-out exchanges are
   maintained by the running members of your RabbitMQ cluster.
  </p></div></div></div><div class="sect1" id="ts-compute"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Compute Service</span> <a title="Permalink" class="permalink" href="#ts-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>ts-compute</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Nova service.
 </p><p>
  Nova offers scalable, on-demand, self-service access to compute resources.
  You can use this guide to help with known issues and troubleshooting of Nova
  services.
 </p><div class="sect2" id="idg-all-operations-troubleshooting-ts-compute-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How can I reset the state of a compute instance?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-compute-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-compute-xml-6</li></ul></div></div></div></div><p>
   If you have an instance that is stuck in a non-Active state, such as
   <code class="literal">Deleting</code> or <code class="literal">Rebooting</code> and you want to
   reset the state so you can interact with the instance again, there is a way
   to do this.
  </p><p>
   The Nova command-line tool (also known as the Nova CLI or python-novaclient)
   has a command, <code class="literal">nova reset-state</code>, that allows you to reset
   the state of a server.
  </p><p>
   Here is the content of the help information about the command which shows
   the syntax:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova help reset-state
        usage: nova reset-state [--active] &lt;server&gt; [&lt;server&gt; ...]

        Reset the state of a server.

        Positional arguments:
        &lt;server&gt;  Name or ID of server(s).

        Optional arguments:
        --active  Request the server be reset to "active" state instead of "error"
        state (the default).</pre></div><p>
   If you had an instance that was stuck in a <code class="literal">Rebooting</code>
   state you would use this command to reset it back to
   <code class="literal">Active</code>:
  </p><div class="verbatim-wrap"><pre class="screen">nova reset-state --active &lt;instance_id&gt;</pre></div></div><div class="sect2" id="nova-consoleauth-troubleshoot"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting nova-consoleauth</span> <a title="Permalink" class="permalink" href="#nova-consoleauth-troubleshoot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>nova-consoleauth-troubleshoot</li></ul></div></div></div></div><p>
   The nova-consoleauth service runs by default on the first controller node,
   that is, the host with <code class="literal">consoleauth_host_index=0</code>. If
   nova-consoleauth fails on the first controller node, you can switch it to
   another controller node by running the ansible playbook nova-start.yml and
   passing it the index of the next controller node.
  </p><p>
   The command to switch nova-consoleauth to another controller node
   (controller 2 for instance) is:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</pre></div><p>
   After you run this command you may now see two instances of the
   <code class="literal">nova-consoleauth</code> service, which will show as being in
   <code class="literal">disabled</code> state, when you run the <code class="literal">nova
   service-list</code> command. You can then delete the service using these
   steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Obtain the service ID for the duplicated nova-consoleauth service:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| Id | Binary           | Host                      | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 10 | nova-conductor   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:47.000000 | -               |
| 13 | nova-conductor   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 16 | nova-scheduler   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:39.000000 | -               |
| 19 | nova-scheduler   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 22 | nova-scheduler   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:44.000000 | -               |
| 25 | nova-consoleauth | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:45.000000 | -               |
| 49 | nova-compute     | ...a-cp1-comp0001-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 52 | nova-compute     | ...a-cp1-comp0002-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 55 | nova-compute     | ...a-cp1-comp0003-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:43.000000 | -               |
<span class="bold"><strong>| 70 | nova-consoleauth | ...a-cp1-c1-m3-mgmt    | internal | disabled | down  | 2016-08-25T12:10:40.000000 | -               |</strong></span>
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+</pre></div></li><li class="step "><p>
     Delete the disabled duplicate service with this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete &lt;service_ID&gt;</pre></div><p>
     Given the example in the previous step, the command could be:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete 70</pre></div></li></ol></div></div></div><div class="sect2" id="ts-resize"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the migrate or resize functions in Nova post-installation when using encryption</span> <a title="Permalink" class="permalink" href="#ts-resize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>ts-resize</li></ul></div></div></div></div><p>
   If you have used encryption for your data when running the configuration
   processor during your cloud deployment and are enabling the Nova resize and
   migrate functionality after the initial installation, there is an issue that
   arises if you have made additional configuration changes that required you to
   run the configuration processor before enabling these features.
  </p><p>
   You will only experience an issue if you have enabled encryption. If you
   haven't enabled encryption, then there is no need to follow the procedure
   below. If you are using encryption and you have made a configuration change
   and run the configuration processor after your initial install or upgrade,
   and you have run the <code class="filename">ready-deployment.yml</code> playbook, and
   you want to enable migrate or resize in Nova, then the following steps will
   allow you to proceed. Note that the ansible vault key referred to below is
   the encryption key that you have provided to the configuration processor.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Checkout the ansible branch of your local git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git checkout ansible</pre></div></li><li class="step "><p>
     Do a git log, and pick the previous commit:
    </p><div class="verbatim-wrap"><pre class="screen">git log</pre></div><p>
     In this example below, the commit is
     <code class="literal">ac54d619b4fd84b497c7797ec61d989b64b9edb3</code>:
    </p><div class="verbatim-wrap"><pre class="screen">$ git log

              commit 69f95002f9bad0b17f48687e4d97b2a791476c6a
              Merge: 439a85e ac54d61
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 09:08:55 2016 +0000

              Merging promotion of saved output

              commit 439a85e209aeeca3ab54d1a9184efb01604dbbbb
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 09:08:24 2016 +0000

              Saved output from CP run on 1d3976dac4fd7e2e78afad8d23f7b64f9d138778

              commit <span class="bold"><strong>ac54d619b4fd84b497c7797ec61d989b64b9edb3</strong></span>
              Merge: a794083 66ffe07
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 08:32:04 2016 +0000

              Merging promotion of saved output</pre></div></li><li class="step "><p>
     Checkout the commit:
    </p><div class="verbatim-wrap"><pre class="screen">git checkout &lt;commit_ID&gt;</pre></div><p>
     Using the same example above, here is the command:
    </p><div class="verbatim-wrap"><pre class="screen">$ git checkout ac54d619b4fd84b497c7797ec61d989b64b9edb3
              Note: checking out 'ac54d619b4fd84b497c7797ec61d989b64b9edb3'.

              You are in 'detached HEAD' state. You can look around, make experimental
              changes and commit them, and you can discard any commits you make in this
              state without impacting any branches by performing another checkout.

              If you want to create a new branch to retain commits you create, you may
              do so (now or later) by using -b with the checkout command again. Example:

              git checkout -b new_branch_name

              HEAD is now at ac54d61... Merging promotion of saved output</pre></div></li><li class="step "><p>
     Change to the ansible output directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/stage/ansible/group_vars/</pre></div></li><li class="step "><p>
     View the <code class="literal">group_vars</code> file from the ansible vault - it
     will be of the form below, with your compute cluster name being the
     indicator:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;cloud name&gt;-&lt;control plane name&gt;-&lt;compute cluster name&gt;</pre></div><p>
     View this group_vars file from the ansible vault with this command which
     will prompt you for your vault password:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-vault view &lt;group_vars_file&gt;</pre></div></li><li class="step "><p>
     Search the contents of this file for the <code class="literal">nova_ssh_key</code>
     section which will contain both the private and public SSH keys which you
     should then save into a temporary file so you can use it in a later step.
    </p><p>
     Here is an example snippet, with the bold part being what you need to
     save:
    </p><div class="verbatim-wrap"><pre class="screen">NOV_KVM:
                vars:
                <span class="bold"><strong>              nova_ssh_key:
                  private: '-----BEGIN RSA PRIVATE KEY-----
                  MIIEpAIBAAKCAQEAv/hhekzykD2K8HnVNBKZcJWYrVlUyb6gR8cvE6hbh2ISzooA
                  jQc3xgglIwpt5TuwpTY3LL0C4PEHObxy9WwqXTHBZp8jg/02RzD02bEcZ1WT49x7
                  Rj8f5+S1zutHlDv7PwEIMZPAHA8lihfGFG5o+QHUmsUHgjShkWPdHXw1+6mCO9V/
                  eJVZb3nDbiunMOBvyyk364w+fSzes4UDkmCq8joDa5KkpTgQK6xfw5auEosyrh8D
                  zocN/JSdr6xStlT6yY8naWziXr7p/QhG44RPD9SSD7dhkyJh+bdCfoFVGdjmF8yA
                  h5DlcLu9QhbJ/scb7yMP84W4L5GwvuWCCFJTHQIDAQABAoIBAQCCH5O7ecMFoKG4
                  JW0uMdlOJijqf93oLk2oucwgUANSvlivJX4AGj9k/YpmuSAKvS4cnqZBrhDwdpCG
                  Q0XNM7d3mk1VCVPimNWc5gNiOBpftPNdBcuNryYqYq4WBwdq5EmGyGVMbbFPk7jH
                  ZRwAJ2MCPoplKl7PlGtcCMwNu29AGNaxCQEZFmztXcEFdMrfpTh3kuBI536pBlEi
                  Srh23mRILn0nvLXMAHwo94S6bI3JOQSK1DBCwtA52r5YgX0nkZbi2MvHISY1TXBw
                  SiWgzqW8dakzVu9UNif9nTDyaJDpU0kr0/LWtBQNdcpXnDSkHGjjnIm2pJVBC+QJ
                  SM9o8h1lAoGBANjGHtG762+dNPEUUkSNWVwd7tvzW9CZY35iMR0Rlux4PO+OXwNq
                  agldHeUpgG1MPl1ya+rkf0GD62Uf4LHTDgaEkUfiXkYtcJwHbjOnj3EjZLXaYMX2
                  LYBE0bMKUkQCBdYtCvZmo6+dfC2DBEWPEhvWi7zf7o0CJ9260aS4UHJzAoGBAOK1
                  P//K7HBWXvKpY1yV2KSCEBEoiM9NA9+RYcLkNtIy/4rIk9ShLdCJQVWWgDfDTfso
                  sJKc5S0OtOsRcomvv3OIQD1PvZVfZJLKpgKkt20/w7RwfJkYC/jSjQpzgDpZdKRU
                  vRY8P5iryptleyImeqV+Vhf+1kcH8t5VQMUU2XAvAoGATpfeOqqIXMpBlJqKjUI2
                  QNi1bleYVVQXp43QQrrK3mdlqHEU77cYRNbW7OwUHQyEm/rNN7eqj8VVhi99lttv
                  fVt5FPf0uDrnVhq3kNDSh/GOJQTNC1kK/DN3WBOI6hFVrmZcUCO8ewJ9MD8NQG7z
                  4NXzigIiiktayuBd+/u7ZxMCgYEAm6X7KaBlkn8KMypuyIsssU2GwHEG9OSYay9C
                  Ym8S4GAZKGyrakm6zbjefWeV4jMZ3/1AtXg4tCWrutRAwh1CoYyDJlUQAXT79Phi
                  39+8+6nSsJimQunKlmvgX7OK7wSp24U+SPzWYPhZYzVaQ8kNXYAOlezlquDfMxxv
                  GqBE5QsCgYA8K2p/z2kGXCNjdMrEM02reeE2J1Ft8DS/iiXjg35PX7WVIZ31KCBk
                  wgYTWq0Fwo2W/EoJVl2o74qQTHK0Bs+FTnR2nkVF3htEOAW2YXQTTN2rEsHmlQqE
                  A9iGTNwm9hvzbvrWeXtx8Zk/6aYfsXCoxq193KglS40shOCaXzWX0w==
                  -----END RSA PRIVATE KEY-----'
                  public: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/+GF6TPKQPYrwedU0Epl
                  wlZitWVTJvqBHxy8TqFuHYhLOigCNBzfGCCUjCm3lO7ClNjcsvQLg8Qc5vHL1bCpdMc
                  FmnyOD/TZHMPTZsRxnVZPj3HtGPx/n5LXO60eUO/s/AQgxk8AcDyWKF8YUbmj5Ad
                  SaxQeCNKGRY90dfDX7qYI71X94lVlvecNuK6cw4G/LKTfrjD59LN6zhQOSYKryOgNrkq
                  SlOBArrF/Dlq4SizKuHwPOhw38lJ2vrFK2VPrJjydpbOJevun9CEbjhE8P1JIPt2GTImH5t0
                  J+gVUZ2OYXzICHkOVwu71CFsn+xxvvIw/zhbgvkbC+5YIIUlMd
                  Generated Key for Nova User</strong></span>
                NTP_CLI:</pre></div></li><li class="step "><p>
     Switch back to the <code class="literal">site</code> branch by checking it out:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git checkout site</pre></div></li><li class="step "><p>
     Navigate to your group_vars directory in this branch:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/group_vars</pre></div></li><li class="step "><p>
     Edit your compute group_vars file, which will prompt you for your vault
     password:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-vault edit &lt;group_vars_file&gt;
              Vault password:
              Decryption successful</pre></div></li><li class="step "><p>
     Search the contents of this file for the <code class="literal">nova_ssh_key</code>
     section and replace the private and public keys with the contents that you
     had saved in a temporary file in step #7 earlier.
    </p></li><li class="step "><p>
     Remove the temporary file that you created earlier. You are now ready to
     run the deployment. For information about enabling Nova resizing and
     migration, see <a class="xref" href="#enabling-the-nova-resize" title="5.4. Enabling the Nova Resize and Migrate Features">Section 5.4, “Enabling the Nova Resize and Migrate Features”</a>.
    </p></li></ol></div></div></div><div class="sect2" id="idg-all-operations-troubleshooting-ts-compute-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (ESX)</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-compute-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-compute-xml-9</li></ul></div></div></div></div><p>

   <span class="bold"><strong>Unable to Create Instance Snapshot when Instance is
   Active</strong></span>
  </p><p>
   There is a known issue with VMWare vCenter where if you have a compute
   instance in <code class="literal">Active</code> state you will receive the error below
   when attempting to take a snapshot of it:
  </p><div class="verbatim-wrap"><pre class="screen">An error occurred while saving the snapshot: Failed to quiesce the virtual machine</pre></div><p>
   The workaround for this issue is to stop the instance. Here are steps to
   achieve this using the command line tool:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Stop the instance using the NovaClient:
    </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance UUID&gt;</pre></div></li><li class="step "><p>
     Take the snapshot of the instance.
    </p></li><li class="step "><p>
     Start the instance back up:
    </p><div class="verbatim-wrap"><pre class="screen">nova start &lt;instance UUID&gt;</pre></div></li></ol></div></div></div></div><div class="sect1" id="neutron-troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Service Troubleshooting</span> <a title="Permalink" class="permalink" href="#neutron-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>neutron-troubleshooting</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Networking service.
 </p><div class="sect2" id="network-fail-troublshoot"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Network failures</span> <a title="Permalink" class="permalink" href="#network-fail-troublshoot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>network-fail-troublshoot</li></ul></div></div></div></div><p>
   <span class="bold"><strong>CVR HA - Split-brain result of failover of L3 agent
   when master comes back up</strong></span> This situation is specific to when L3
   HA is configured and a network failure occurs to the node hosting the
   currently active l3 agent. L3 HA is intended to provide HA in situations
   where the l3-agent crashes or the node hosting an l3-agent crashes/restarts.
   In the case of a physical networking issue which isolates the active l3
   agent, the stand-by l3-agent takes over but when the physical networking
   issue is resolved, traffic to the VMs is disrupted due to a "split-brain"
   situation in which traffic is split over the two L3 agents. The solution is
   to restart the L3-agent that was originally the master.
  </p><p>
   <span class="bold"><strong>OVSvApp loses connectivity with vCenter</strong></span> If
   the OVSvApp loses connectivity with the vCenter cluster, you will receive
   the following errors:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The OVSvApp VM will go into ERROR state
    </p></li><li class="step "><p>
     The OVSvApp VM will not get IP address
    </p></li></ol></div></div><p>
   When you see these symptoms:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Restart the OVSvApp agent on the OVSvApp VM.
    </p></li><li class="step "><p>
     Execute the following command to restart the Network (Neutron) service:
    </p><div class="verbatim-wrap"><pre class="screen">sudo service neutron-ovsvapp-agent restart</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Fail over a plain CVR router because the node became
   unavailable:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of l3 agent UUIDs which can be used in the commands that follow
    </p><div class="verbatim-wrap"><pre class="screen"> neutron agent-list | grep l3</pre></div></li><li class="step "><p>
     Determine the current host
    </p><div class="verbatim-wrap"><pre class="screen"> neutron l3-agent-list-hosting-router &lt;router uuid&gt;</pre></div></li><li class="step "><p>
     Remove the router from the current host
    </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-remove &lt;current l3 agent uuid&gt; &lt;router uuid&gt;</pre></div></li><li class="step "><p>
     Add the router to a new host
    </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-add &lt;new l3 agent uuid&gt; &lt;router uuid&gt;</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Trouble setting maximum transmission units
   (MTU)</strong></span> <a class="xref" href="#configureMTU" title="9.3.11. Configuring Maximum Transmission Units in Neutron">Section 9.3.11, “Configuring Maximum Transmission Units in Neutron”</a>
  </p><p>
   <span class="bold"><strong>Floating IP on allowed_address_pair port with
   DVR-routed networks</strong></span> <code class="literal">allowed_address_pair</code>
  </p><p>
   <span class="bold"><strong>You may notice this issue:</strong></span> If you have an
   <code class="literal">allowed_address_pair</code> associated with multiple virtual
   machine (VM) ports, and if all the VM ports are ACTIVE, then the
   <code class="literal">allowed_address_pair</code> port binding will have the last
   ACTIVE VM's binding host as its bound host.
  </p><p>
   <span class="bold"><strong>In addition, you may notice</strong></span> that if the
   floating IP is assigned to the <code class="literal">allowed_address_pair</code> that
   is bound to multiple VMs that are ACTIVE, then the floating IP will not work
   with DVR routers. This is different from the centralized router behavior
   where it can handle unbound <code class="literal">allowed_address_pair</code> ports
   that are associated with floating IPs.
  </p><p>
   Currently we support <code class="literal">allowed_address_pair</code> ports with DVR
   only if they have floating IPs enabled, and have just one ACTIVE port.
  </p><p>
   Using the CLI, you can follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a network to add the host to:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron net-create vrrp-net</pre></div></li><li class="step "><p>
     Attach a subnet to that network with a specified allocation-pool range:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron subnet-create  --name vrrp-subnet --allocation-pool start=10.0.0.2,end=10.0.0.200 vrrp-net 10.0.0.0/24</pre></div></li><li class="step "><p>
     Create a router, uplink the vrrp-subnet to it, and attach the router to an
     upstream network called public:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron router-create router1
$ neutron router-interface-add router1 vrrp-subnet
$ neutron router-gateway-set router1 public</pre></div><p>
     Create a security group called vrrp-sec-group and add ingress rules to
     allow ICMP and TCP port 80 and 22:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron security-group-create vrrp-sec-group
$ neutron security-group-rule-create  --protocol icmp vrrp-sec-group
$ neutron security-group-rule-create  --protocol tcp  --port-range-min80 --port-range-max80 vrrp-sec-group
$ neutron security-group-rule-create  --protocol tcp  --port-range-min22 --port-range-max22 vrrp-sec-group</pre></div></li><li class="step "><p>
     Next, boot two instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova boot --num-instances 2 --image ubuntu-12.04 --flavor 1 --nic net-id=24e92ee1-8ae4-4c23-90af-accb3919f4d1 vrrp-node --security_groups vrrp-sec-group</pre></div></li><li class="step "><p>
     When you create two instances, make sure that both the instances are not
     in ACTIVE state before you associate the
     <code class="literal">allowed_address_pair</code>. The instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| ID                                   | Name                                            | Status | Task State | Power State | Networks                                               |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| 15b70af7-2628-4906-a877-39753082f84f | vrrp-node-15b70af7-2628-4906-a877-39753082f84f | ACTIVE  | -          | Running     | vrrp-net=10.0.0.3                                      |
| e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | vrrp-node-e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | DOWN    | -          | Running     | vrrp-net=10.0.0.4                                      |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+</pre></div></li><li class="step "><p>
     Create a port in the VRRP IP range that was left out of the ip-allocation
     range:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-create --fixed-ip ip_address=10.0.0.201 --security-group vrrp-sec-group vrrp-net
Created a new port:
+-----------------------+-----------------------------------------------------------------------------------+
| Field                 | Value                                                                             |
+-----------------------+-----------------------------------------------------------------------------------+
| admin_state_up        | True                                                                              |
| allowed_address_pairs |                                                                                   |
| device_id             |                                                                                   |
| device_owner          |                                                                                   |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                              |
| mac_address           | fa:16:3e:20:67:9f                                                                 |
| name                  |                                                                                   |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                              |
| port_security_enabled | True                                                                              |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                              |
| status                | DOWN                                                                              |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                  |
+-----------------------+-----------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Another thing to cross check after you associate the allowed_address_pair
     port to the VM port, is whether the
     <code class="literal">allowed_address_pair</code> port has inherited the VM's host
     binding:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron --os-username admin --os-password ZIy9xitH55 --os-tenant-name admin port-show f5a252b2-701f-40e9-a314-59ef9b5ed7de
+-----------------------+--------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                  |
+-----------------------+--------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                   |
| allowed_address_pairs |                                                                                                        |
| {color:red}binding:host_id{color} | ...-cp1-comp0001-mgmt                                                                      |
| binding:profile       | {}                                                                                                     |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                         |
| binding:vif_type      | ovs                                                                                                    |
| binding:vnic_type     | normal                                                                                                 |
| device_id             |                                                                                                        |
| device_owner          | compute:None                                                                                           |
| dns_assignment        | {"hostname": "host-10-0-0-201", "ip_address": "10.0.0.201", "fqdn": "host-10-0-0-201.openstacklocal."} |
| dns_name              |                                                                                                        |
| extra_dhcp_opts       |                                                                                                        |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"}                      |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                                                   |
| mac_address           | fa:16:3e:20:67:9f                                                                                      |
| name                  |                                                                                                        |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                                                   |
| port_security_enabled | True                                                                                                   |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                                                   |
| status                | DOWN                                                                                                   |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                                       |
+-----------------------+--------------------------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Note that you were allocated a port with the IP address 10.0.0.201 as
     requested. Next, associate a floating IP to this port to be able to access
     it publicly:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron floatingip-create --port-id=6239f501-e902-4b02-8d5c-69062896a2dd public
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.0.0.201                           |
| floating_ip_address | 10.36.12.139                         |
| floating_network_id | 3696c581-9474-4c57-aaa0-b6c70f2529b0 |
| id                  | a26931de-bc94-4fd8-a8b9-c5d4031667e9 |
| port_id             | 6239f501-e902-4b02-8d5c-69062896a2dd |
| router_id           | 178fde65-e9e7-4d84-a218-b1cc7c7b09c7 |
| tenant_id           | d4e4332d5f8c4a8eab9fcb1345406cb0     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Now update the ports attached to your VRRP instances to include this IP
     address as an allowed-address-pair so they will be able to send traffic
     out using this address. First find the ports attached to these instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-list -- --network_id=24e92ee1-8ae4-4c23-90af-accb3919f4d1
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                         |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d |      | fa:16:3e:7a:7b:18 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"}   |
| 14f57a85-35af-4edb-8bec-6f81beb9db88 |      | fa:16:3e:2f:7e:ee | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.2"}   |
| 6239f501-e902-4b02-8d5c-69062896a2dd |      | fa:16:3e:20:67:9f | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| 87094048-3832-472e-a100-7f9b45829da5 |      | fa:16:3e:b3:38:30 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.1"}   |
| c080dbeb-491e-46e2-ab7e-192e7627d050 |      | fa:16:3e:88:2e:e2 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.3"}   |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Add this address to the ports c080dbeb-491e-46e2-ab7e-192e7627d050 and
     12bf9ea4-4845-4e2c-b511-3b8b1ad7291d which are 10.0.0.3 and 10.0.0.4 (your
     vrrp-node instances):
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-update  c080dbeb-491e-46e2-ab7e-192e7627d050 --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201
$ neutron port-update  12bf9ea4-4845-4e2c-b511-3b8b1ad7291d --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201</pre></div></li><li class="step "><p>
     The allowed-address-pair 10.0.0.201 now shows up on the port:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-show12bf9ea4-4845-4e2c-b511-3b8b1ad7291d
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs | {"ip_address": "10.0.0.201", "mac_address": "fa:16:3e:7a:7b:18"}                |
| device_id             | e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6                                            |
| device_owner          | compute:None                                                                    |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"} |
| id                    | 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d                                            |
| mac_address           | fa:16:3e:7a:7b:18                                                               |
| name                  |                                                                                 |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                            |
| port_security_enabled | True                                                                            |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                |</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>OpenStack traffic that must traverse VXLAN tunnel
   dropped when using HPE 5930 switch</strong></span> Cause: UDP destination port
   4789 is conflicting with OpenStack VXLAN traffic.
  </p><p>
   There is a configuration setting you can use in the switch to configure the
   port number the HPN kit will use for its own VXLAN tunnels. Setting this to
   a port number other than the one Neutron will use by default (4789) will
   keep the HPN kit from absconding with Neutron's VXLAN traffic. Specifically:
  </p><p>
   <span class="bold"><strong>Parameters: </strong></span>
  </p><p>
   port-number: Specifies a UDP port number in the range of 1 to 65535. As a
   best practice, specify a port number in the range of 1024 to 65535 to avoid
   conflict with well-known ports.
  </p><p>
   <span class="bold"><strong>Usage guidelines:</strong></span>
  </p><p>
   You must configure the same destination UDP port number on all VTEPs in a
   VXLAN.
  </p><p>
   Examples
  </p><div class="verbatim-wrap"><pre class="screen"># Set the destination UDP port number to 6666 for VXLAN packets.
&lt;Sysname&gt; system-view
[Sysname] vxlan udp-port 6666</pre></div><p>
   Use vxlan udp-port to configure the destination UDP port number of VXLAN
   packets.   Mandatory for all VXLAN packets to specify a UDP port Default
   The destination UDP port number is 4789 for VXLAN packets.
  </p><p>
   OVS can be configured to use a different port number itself:
  </p><div class="verbatim-wrap"><pre class="screen"># (IntOpt) The port number to utilize if tunnel_types includes 'vxlan'. By
# default, this will make use of the Open vSwitch default value of '4789' if
# not specified.
#
# vxlan_udp_port =
# Example: vxlan_udp_port = 8472
#</pre></div><div class="sect3" id="DOCS-3584"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: PCI-PT virtual machine gets stuck at boot</span> <a title="Permalink" class="permalink" href="#DOCS-3584">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>DOCS-3584</li></ul></div></div></div></div><p>
    If you are using a machine that uses Intel NICs, if the PCI-PT virtual
    machine gets stuck at boot, the boot agent should be disabled.
   </p><p>
    When Intel cards are used for PCI-PT, sometimes the tenant virtual machine
    gets stuck at boot. If this happens, you should download Intel bootutils
    and use it to disable the bootagent.
   </p><p>
    Use the following steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download <code class="literal">preebot.tar.gz</code> from the
      <a class="link" href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers" target="_blank">Intel
      website</a>.
     </p></li><li class="step "><p>
      Untar the <code class="literal">preboot.tar.gz</code> file on the compute host
      where the PCI-PT virtual machine is to be hosted.
     </p></li><li class="step "><p>
      Go to path <code class="literal">~/APPS/BootUtil/Linux_x64</code> and then run
      following command:
     </p><div class="verbatim-wrap"><pre class="screen">./bootutil64e -BOOTENABLE disable -all</pre></div></li><li class="step "><p>
      Now boot the PCI-PT virtual machine and it should boot without getting
      stuck.
     </p></li></ol></div></div></div></div></div><div class="sect1" id="troubleshooting-glance"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting the Image (Glance) Service</span> <a title="Permalink" class="permalink" href="#troubleshooting-glance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_image.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_image.xml</li><li><span class="ds-label">ID: </span>troubleshooting-glance</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Glance service. We have
  gathered some of the common issues and troubleshooting steps that will help
  when resolving issues that occur with the Glance service.
 </p><div class="sect2" id="image-upload"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Images Created in Horizon UI Get Stuck in a Queued State</span> <a title="Permalink" class="permalink" href="#image-upload">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_image.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_image.xml</li><li><span class="ds-label">ID: </span>image-upload</li></ul></div></div></div></div><p>
   When creating a new image in the Horizon UI you will see the option for
   <code class="literal">Image Location</code> which allows you to enter a HTTP source to
   use when creating a new image for your cloud. However, this option is
   disabled by default for security reasons. This results in any new images
   created via this method getting stuck in a <code class="literal">Queued</code> state.
  </p><p>
   We cannot guarantee the security of any third party sites you use as image
   sources and the traffic goes over HTTP (non-SSL) traffic.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> You will need your cloud
   administrator to enable the HTTP store option in Glance for your cloud.
  </p><p>
   Here are the steps to enable this option:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the file below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/GLA-API/templates/glance-api.conf.j2</pre></div></li><li class="step "><p>
     Locate the Glance store options and add the <code class="literal">http</code> value
     in the <code class="literal">stores</code> field. It will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}</pre></div><p>
     Change this to:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}<span class="bold"><strong>,http</strong></span></pre></div></li><li class="step "><p>
     Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "adding HTTP option to Glance store list"</pre></div></li><li class="step "><p>
     Run the configuration processor with this command:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the Glance service reconfigure playbook which will update these
     settings:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-storage"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-storage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_storage.xml</li><li><span class="ds-label">ID: </span>troubleshooting-storage</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for Swift services.
 </p><div class="sect2" id="troubleshooting-blockstorage"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-blockstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>troubleshooting-blockstorage</li></ul></div></div></div></div><p>
  The block storage service utilizes <span class="productname">OpenStack</span> Cinder and can integrate with
  multiple back-ends including 3Par. Failures may exist at the Cinder API level,
  an operation may fail, or you may see an alarm trigger in the monitoring
  service. These may be caused by configuration problems, network issues, or
  issues with your servers or storage back-ends. The purpose of this page and
  section is to describe how the service works, where to find additional
  information, some of the common problems that come up, and how to address
  them.
 </p><div class="sect3" id="logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Where to find information</span> <a title="Permalink" class="permalink" href="#logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>logs</li></ul></div></div></div></div><p>
   When debugging block storage issues it is helpful to understand the
   deployment topology and know where to locate the logs with additional
   information.
  </p><p>
   The Cinder service consists of:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     An API service, typically deployed and active on the controller nodes.
    </p></li><li class="listitem "><p>
     A scheduler service, also typically deployed and active on the controller
     nodes.
    </p></li><li class="listitem "><p>
     A volume service, which is deployed on all of the controller nodes but
     only active on one of them.
    </p></li><li class="listitem "><p>
     A backup service, which is deployed on the same controller node as the
     volume service.
    </p></li></ul></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-troubleshooting-cinder_topology.png" target="_blank"><img src="images/media-hos.docs-troubleshooting-cinder_topology.png" width="" /></a></div></div><p>
   You can refer to your configuration files (usually located in
   <code class="literal">~/openstack/my_cloud/definition/</code> on the Cloud Lifecycle Manager) for
   specifics about where your services are located. They will usually be
   located on the controller nodes.
  </p><p>
   Cinder uses a MariaDB database and communicates between components by
   consuming messages from a RabbitMQ message service.
  </p><p>
   The Cinder API service is layered underneath a HAProxy service and accessed
   using a virtual IP address maintained using keepalived.
  </p><p>
   If any of the Cinder components is not running on its intended host then an
   alarm will be raised. Details on how to resolve these alarms can be found on
   our <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> page. You should check the logs for
   the service on the appropriate nodes. All Cinder logs are stored in
   <code class="literal">/var/log/cinder/</code> and all log entries above
   <code class="literal">INFO</code> level are also sent to the centralized logging
   service. For details on how to change the logging level of the Cinder
   service, see <a class="xref" href="#central-log-configure-services" title="12.2.6. Configuring Settings for Other Services">Section 12.2.6, “Configuring Settings for Other Services”</a>.
  </p><p>
   In order to get the full context of an error you may need to examine the
   full log files on individual nodes. Note that if a component runs on more
   than one node you will need to review the logs on each of the nodes that
   component runs on. Also remember that as logs rotate that the time interval
   you are interested in may be in an older log file.
  </p><p>
   <span class="bold"><strong>Log locations:</strong></span>
  </p><p>
   <code class="literal">/var/log/cinder/cinder-api.log</code> - Check this log if you
   have endpoint or connectivity issues
  </p><p>
   <code class="literal">/var/log/cinder/cinder-scheduler.log</code> - Check this log if
   the system cannot assign your volume to a back-end
  </p><p>
   <code class="literal">/var/log/cinder/cinder-backup.log</code> - Check this log if you
   have backup or restore issues
  </p><p>
   <code class="literal">/var/log/cinder-cinder-volume.log</code> - Check here for
   failures during volume creation
  </p><p>
   <code class="literal">/var/log/nova/nova-compute.log</code> - Check here for failures
   with attaching volumes to compute instances
  </p><p>
   You can also check the logs for the database and/or the RabbitMQ service if
   your cloud exhibits database or messaging errors.
  </p><p>
   If the API servers are up and running but the API is not reachable then
   checking the HAProxy logs on the active keepalived node would be the place
   to look.
  </p><p>
   If you have errors attaching volumes to compute instances using the Nova API
   then the logs would be on the compute node associated with the instance. You
   can use the following command to determine which node is hosting the
   instance:
  </p><div class="verbatim-wrap"><pre class="screen">nova show &lt;instance_uuid&gt;</pre></div><p>
   Then you can check the logs located at
   <code class="literal">/var/log/nova/nova-compute.log</code> on that compute node.
  </p></div><div class="sect3" id="volume-states"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Cinder volume states</span> <a title="Permalink" class="permalink" href="#volume-states">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>volume-states</li></ul></div></div></div></div><p>
   Once the topology is understood, if the issue with the Cinder service
   relates to a specific volume then you should have a good understanding of
   what the various states a volume can be in are. The states are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     attaching
    </p></li><li class="listitem "><p>
     available
    </p></li><li class="listitem "><p>
     backing-up
    </p></li><li class="listitem "><p>
     creating
    </p></li><li class="listitem "><p>
     deleting
    </p></li><li class="listitem "><p>
     downloading
    </p></li><li class="listitem "><p>
     error
    </p></li><li class="listitem "><p>
     error attaching
    </p></li><li class="listitem "><p>
     error deleting
    </p></li><li class="listitem "><p>
     error detaching
    </p></li><li class="listitem "><p>
     error extending
    </p></li><li class="listitem "><p>
     error restoring
    </p></li><li class="listitem "><p>
     in-use
    </p></li><li class="listitem "><p>
     extending
    </p></li><li class="listitem "><p>
     restoring
    </p></li><li class="listitem "><p>
     restoring backup
    </p></li><li class="listitem "><p>
     retyping
    </p></li><li class="listitem "><p>
     uploading
    </p></li></ul></div><p>
   The common states are <code class="literal">in-use</code> which indicates a volume is
   currently attached to a compute instance and <code class="literal">available</code>
   means the volume is created on a back-end and is free to be attached to an
   instance. All <code class="literal">-ing</code> states are transient and represent a
   transition. If a volume stays in one of those states for too long indicating
   it is stuck, or if it fails and goes into an error state, you should check
   for failures in the logs.
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-blockstorage-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initial troubleshooting steps</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-blockstorage-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-blockstorage-xml-7</li></ul></div></div></div></div><p>
   These should be the initial troubleshooting steps you go through.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If you have noticed an issue with the service, you should check your
     monitoring system for any alarms that may have triggered. See
     <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> for resolution steps for those alarms.
    </p></li><li class="step "><p>
     Check if the Cinder API service is active by listing the available volumes
     from the Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
openstack volume list</pre></div></li><li class="step "><p>
     Run a basic diagnostic from the Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts _cinder_post_check.yml</pre></div><p>
     This ansible playbook will list all volumes, create a 1 GB volume and then
     delete it using the v1 and v2 APIs, which will exercise basic Cinder
     capability.
    </p></li></ol></div></div></div><div class="sect3" id="common-issues"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Common failures</span> <a title="Permalink" class="permalink" href="#common-issues">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>common-issues</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Alerts from the Cinder service</strong></span>
  </p><p>
   Check for alerts associated with the block storage service, noting that
   these could include alerts related to the server nodes being down, alerts
   related to the messaging and database services, or the HAProxy and
   keepalived services, as well as alerts directly attributed to the block
   storage service.
  </p><p>
   The Operations Console provides a web UI method for checking
   alarms. See <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span> for details on how to connect to
   the Operations Console.
  </p><p>
   <span class="bold"><strong>Cinder volume service is down</strong></span>
  </p><p>
   The Cinder volume service could be down if the server hosting the
   volume service fails. (Running the command <code class="command">cinder
   service-list</code> will show the state of the volume service.) In this
   case you should follow the documented procedure linked below to start the
   volume service on another controller node. See <a class="xref" href="#sec-operation-manage-block-storage" title="7.1.3. Managing Cinder Volume and Backup Services">Section 7.1.3, “Managing Cinder Volume and Backup Services”</a> for details.
  </p><p>
   <span class="bold"><strong>Creating a Cinder bootable volume fails</strong></span>
  </p><p>
   When creating a bootable volume from an image, your Cinder volume must be
   larger than the Virtual Size (raw size) of your image or creation will fail
   with an error.
  </p><p>
   An error like this error would appear in
   <code class="literal">cinder-volume.log</code> file:
  </p><div class="verbatim-wrap"><pre class="screen">'2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher ImageCopyFailure: Failed to copy image to volume: qemu-img: /dev/disk/by-path/ip-192.168.92.5:3260-iscsi-iqn.2003-10.com.lefthandnetworks:mg-ses:146:volume-c0e75c66-a20a-4368-b797-d70afedb45cc-lun-0: error while converting raw: Device is too small
2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher'</pre></div><p>
   In an example where creating a 1GB bootable volume fails, your image may
   look like this:
  </p><div class="verbatim-wrap"><pre class="screen">$ qemu-img info /tmp/image.qcow2
image: /tmp/image.qcow2
file format: qcow2
virtual size: 1.5G (1563295744 bytes)
disk size: 354M
cluster_size: 65536
...</pre></div><p>
   In this case, note that the image format is qcow2 and hte virtual size is
   1.5GB, which is greater than the size of the bootable volume. Even though
   the compressed image size is less than 1GB, this bootable volume creation
   will fail.
  </p><p>
   When creating your disk model for nodes that will have the cinder volume
   role make sure that there is sufficient disk space allocated for a temporary
   space for image conversion if you will be creating bootable volumes. You
   should allocate enough space to the filesystem as would be needed to cater
   for the raw size of images to be used for bootable volumes - for example
   Windows images can be quite large in raw format.
  </p><p>
   By default, Cinder uses <code class="literal">/var/lib/cinder</code> for image
   conversion and this will be on the root filesystem unless it is explicitly
   separated. You can ensure there is enough space by ensuring that the root
   file system is sufficiently large, or by creating a logical volume mounted
   at <code class="literal">/var/lib/cinder</code> in the disk model when installing the
   system.
  </p><p>
   If your system is already installed, use these steps to update this:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit the configuration item <code class="literal">image_conversion_dir</code> in
     <code class="literal">cinder.conf.j2</code> to point to another location with more
     disk space. Make sure that the new directory location has the same
     ownership and permissions as <code class="literal">/var/lib/cinder</code>
     (owner:cinder group:cinder. mode 0750).
    </p></li><li class="listitem "><p>
     Then run this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>API-level failures</strong></span>
  </p><p>
   If the API is inaccessible, determine if the API service is running on the
   target node. If it is not, check to see why the API service is not running in
   the log files. If it is running okay, check if the HAProxy service is
   functioning properly.
  </p><div id="id-1.6.17.9.3.6.20" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    After a controller node is rebooted, you must make sure to run the
    <code class="literal">ardana-start.yml</code> playbook to ensure all the services are
    up and running. For more information, see
    <a class="xref" href="#recover-downed-cluster" title="13.2.2.1. Restarting Controller Nodes After a Reboot">Section 13.2.2.1, “Restarting Controller Nodes After a Reboot”</a>.
   </p></div><p>
   If the API service is returning an error code, look for the error message in
   the API logs on all API nodes. Successful completions would be logged like
   this:
  </p><div class="verbatim-wrap"><pre class="screen">2016-04-25 10:09:51.107 30743 INFO eventlet.wsgi.server [<span class="bold"><strong>req-a14cd6f3-6c7c-4076-adc3-48f8c91448f6</strong></span>
dfb484eb00f94fb39b5d8f5a894cd163 7b61149483ba4eeb8a05efa92ef5b197 - - -] 192.168.186.105 - - [25/Apr/2016
10:09:51] "GET /v2/7b61149483ba4eeb8a05efa92ef5b197/volumes/detail HTTP/1.1" <span class="bold"><strong>200</strong></span> 13915 0.235921</pre></div><p>
   where <code class="literal">200</code> represents HTTP status 200 for a successful
   completion. Look for a line with your status code and then examine all
   entries associated with the request id. The request ID in the successful
   completion is highlighted in bold above.
  </p><p>
   The request may have failed at the scheduler or at the volume or backup
   service and you should also check those logs at the time interval of
   interest, noting that the log file of interest may be on a different node.
  </p><p>
   <span class="bold"><strong>Operations that do not complete</strong></span>
  </p><p>
   If you have started an operation, such as creating or deleting a volume,
   that does not complete, the Cinder volume may be stuck in a state. You
   should follow the procedures for detaling with stuck volumes.
  </p><p>
   There are six transitory states that a volume can get stuck in:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>State</th><th>Description</th></tr></thead><tbody><tr><td>creating</td><td>The Cinder volume manager has sent a request
                                                  to a back-end driver to create a volume, but has
                                                  not received confirmation that the volume is
                                                  available.</td></tr><tr><td>attaching</td><td>Cinder has received a request from Nova to
                                                  make a volume available for attaching to an
                                                  instance but has not received confirmation from
                                                  Nova that the attachment is complete.</td></tr><tr><td>detaching</td><td>Cinder has received notification from Nova
                                                  that it will detach a volume from an instance but
                                                  has not received notification that the detachment
                                                  is complete.</td></tr><tr><td>deleting</td><td>Cinder has received a request to delete a
                                                  volume but has not completed the
                                                  operation.</td></tr><tr><td>backing-up</td><td>Cinder backup manager has started to back a
                                                  volume up to Swift, or some other backup target,
                                                  but has not completed the operation.</td></tr><tr><td>restoring</td><td>Cinder backup manager has started to restore
                                                  a volume from Swift, or some other backup target,
                                                  but has not completed the operation.</td></tr></tbody></table></div><p>
   At a high level, the steps that you would take to address any of these
   states are similar:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Confirm that the volume is actually stuck, and not just temporarily
     blocked.
    </p></li><li class="listitem "><p>
     Where possible, remove any resources being held by the volume. For
     example, if a volume is stuck detaching it may be necessary to remove
     associated iSCSI or DM devices on the compute node.
    </p></li><li class="listitem "><p>
     Reset the state of the volume to an appropriate state, for example to
     <code class="literal">available</code> or <code class="literal">error</code>.
    </p></li><li class="listitem "><p>
     Do any final cleanup. For example, if you reset the state to
     <code class="literal">error</code> you can then delete the volume.
    </p></li></ol></div><p>
   The next sections will describe specific steps you can take for volumes
   stuck in each of the transitory states.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Creating</strong></span>
  </p><p>
   Broadly speaking, there are two possible scenarios where a volume would get
   stuck in <code class="literal">creating</code>. The <code class="literal">cinder-volume</code>
   service could have thrown an exception while it was attempting to create the
   volume, and failed to handle the exception correctly. Or the volume back-end
   could have failed, or gone offline, after it received the request from
   Cinder to create the volume.
  </p><p>
   These two cases are different in that for the second case you will need to
   determine the reason the back-end is offline and restart it. Often, when the
   back-end has been restarted, the volume will move from
   <code class="literal">creating</code> to <code class="literal">available</code> so your issue
   will be resolved.
  </p><p>
   If you can create volumes successfully on the same back-end as the volume
   stuck in <code class="literal">creating</code> then the back-end is not down. So you
   will need to reset the state for the volume and then delete it.
  </p><p>
   To reset the state of a volume you can use the <code class="literal">cinder
   reset-state</code> command. You can use either the UUID or the volume
   name of the stuck volume.
  </p><p>
   For example, here is a volume list where we have a stuck volume:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | creating  | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</pre></div><p>
   You can reset the state by using the <code class="literal">cinder reset-state</code>
   command, like this:
  </p><div class="verbatim-wrap"><pre class="screen">cinder reset-state --state error 14b76133-e076-4bd3-b335-fa67e09e51f6</pre></div><p>
   Confirm that with another listing:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | error     | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</pre></div><p>
   You can then delete the volume:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder delete 14b76133-e076-4bd3-b335-fa67e09e51f6
Request to delete volume 14b76133-e076-4bd3-b335-fa67e09e51f6 has been accepted.</pre></div><p>
   <span class="bold"><strong>Volumes stuck in Deleting</strong></span>
  </p><p>
   If a volume is stuck in the deleting state then the request to delete the
   volume may or may not have been sent to and actioned by the back-end. If you
   can identify volumes on the back-end then you can examine the back-end to
   determine whether the volume is still there or not. Then you can decide
   which of the following paths you can take. It may also be useful to
   determine whether the back-end is responding, either by checking for recent
   volume create attempts, or creating and deleting a test volume.
  </p><p>
   The first option is to reset the state of the volume to
   <code class="literal">available</code> and then attempt to delete the volume again.
  </p><p>
   The second option is to reset the state of the volume to
   <code class="literal">error</code> and then delete the volume.
  </p><p>
   If you have reset the volume state to <code class="literal">error</code> then the volume
   may still be consuming storage on the back-end. If that is the case then you
   will need to delete it from the back-end using your back-end's specific tool.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Attaching</strong></span>
  </p><p>
   The most complicated situation to deal with is where a volume is stuck
   either in attaching or detaching, because as well as dealing with the state
   of the volume in Cinder and the back-end, you have to deal with exports from
   the back-end, imports to the compute node, and attachments to the compute
   instance.
  </p><p>
   The two options you have here are to make sure that all exports and imports
   are deleted and to reset the state of the volume to
   <code class="literal">available</code> or to make sure all of the exports and imports
   are correct and to reset the state of the volume to
   <code class="literal">in-use</code>.
  </p><p>
   A volume that is in attaching state should never have been made available to
   a compute instance and therefore should not have any data written to it, or
   in any buffers between the compute instance and the volume back-end. In that
   situation, it is often safe to manually tear down the devices exported on
   the back-end and imported on the compute host and then reset the volume state
   to <code class="literal">available</code>.
  </p><p>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Detaching</strong></span>
  </p><p>
   The steps in dealing with a volume stuck in <code class="literal">detaching</code>
   state are very similar to those for a volume stuck in
   <code class="literal">attaching</code>. However, there is the added consideration that
   the volume was attached to, and probably servicing, I/O from a compute
   instance. So you must take care to ensure that all buffers are properly
   flushed before detaching the volume.
  </p><p>
   When a volume is stuck in <code class="literal">detaching</code>, the output from a
   <code class="literal">cinder list</code> command will include the UUID for the
   instance to which the volume was attached. From that you can identify the
   compute host that is running the instance using the <code class="literal">nova
   show</code> command.
  </p><p>
   For example, here are some snippets:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder list
+--------------------------------------+-----------+-----------------------+-----------------+
|                  ID                  |   Status  |       Name            |   Attached to   |
+--------------------------------------+-----------+-----------------------+-----------------+
| 85384325-5505-419a-81bb-546c69064ec2 | detaching |        vol1           | 4bedaa76-78ca-… |
+--------------------------------------+-----------+-----------------------+-----------------+</pre></div><div class="verbatim-wrap"><pre class="screen">$ nova show 4bedaa76-78ca-4fe3-806a-3ba57a9af361|grep host
| OS-EXT-SRV-ATTR:host                 | mycloud-cp1-comp0005-mgmt
| OS-EXT-SRV-ATTR:hypervisor_hostname  | mycloud-cp1-comp0005-mgmt
| hostId                               | 61369a349bd6e17611a47adba60da317bd575be9a900ea590c1be816</pre></div><p>
   The first thing to check in this case is whether the instance is still
   importing the volume. Use <code class="literal">virsh list</code> and <code class="literal">virsh
   dumpxml</code> as described in the section above. If the XML for the
   instance has a reference to the device, then you should reset the volume
   state to <code class="literal">in-use</code> and attempt the <code class="literal">cinder
   detach</code> operation again.
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder reset-state --state in-use --attach-status attached 85384325-5505-419a-81bb-546c69064ec2</pre></div><p>
   If the volume gets stuck detaching again, there may be a more fundamental
   problem, which is outside the scope of this document and you should contact
   the Support team.
  </p><p>
   If the volume is not referenced in the XML for the instance then you should
   remove any devices on the compute node and back-end and then reset the state
   of the volume to <code class="literal">available</code>.
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder reset-state --state available --attach-status detached 85384325-5505-419a-81bb-546c69064ec2</pre></div><p>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </p><p>
   <span class="bold"><strong>Volumes stuck in restoring</strong></span>
  </p><p>
   Restoring a Cinder volume from backup will be as slow as backing it up. So
   you must confirm that the volume is actually stuck by examining the
   <code class="literal">cinder-backup.log</code>. For example:
  </p><div class="verbatim-wrap"><pre class="screen"># tail -f cinder-backup.log |grep 162de6d5-ba92-4e36-aba4-e37cac41081b
2016-04-27 12:39:14.612 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - -
2016-04-27 12:39:15.533 6689 DEBUG cinder.backup.chunkeddriver [req-0c65ec42-8f9d-430a-b0d5-
2016-04-27 12:39:15.566 6689 DEBUG requests.packages.urllib3.connectionpool [req-0c65ec42-
2016-04-27 12:39:15.567 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - - -</pre></div><p>
   If you determine that the volume is genuinely stuck in
   <code class="literal">detaching</code> then you must follow the procedure described in
   the detaching section above to remove any volumes that remain exported from
   the back-end and imported on the controller node. Remember that in this case
   the volumes will be imported and mounted on the controller node running
   <code class="literal">cinder-backup</code>. So you do not have to search for the
   correct compute host. Also remember that no instances are involved so you do
   not need to confirm that the volume is not imported to any instances.
  </p></div><div class="sect3" id="debugging-attachment"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Debugging volume attachment</span> <a title="Permalink" class="permalink" href="#debugging-attachment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>debugging-attachment</li></ul></div></div></div></div><p>
   In an error case, it is possible for a Cinder volume to fail to complete an
   operation and revert back to its initial state. For example, attaching a
   Cinder volume to a Nova instance, so you would follow the steps above to
   examine the Nova compute logs for the attach request.
  </p></div><div class="sect3" id="errors-creating"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Errors creating volumes</span> <a title="Permalink" class="permalink" href="#errors-creating">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>errors-creating</li></ul></div></div></div></div><p>
   If you are creating a volume and it goes into the <code class="literal">ERROR</code>
   state, a common error to see is <code class="literal">No valid host was found</code>.
   This means that the scheduler could not schedule your volume to a back-end.
   You should check that the volume service is up and running. You can use this
   command:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo cinder-manage service list
Binary           Host                                 Zone             Status     State Updated At
cinder-scheduler ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:30
cinder-volume    ha-volume-manager@ses1               nova             enabled    XXX   2016-04-25 11:27:26
cinder-backup    ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:28</pre></div><p>
   In this example, the state of <code class="literal">XXX</code> indicates that the
   service is down.
  </p><p>
   If the service is up, next check that the back-end has sufficient space. You
   can use this command to show the available and total space on each back-end:
  </p><div class="verbatim-wrap"><pre class="screen">cinder get-pools –detail</pre></div><p>
   If your deployment is using volume types, verify that the
   <code class="literal">volume_backend_name</code> in your
   <code class="literal">cinder.conf</code> file matches the
   <code class="literal">volume_backend_name</code> for the volume type you selected.
  </p><p>
   You can verify the back-end name on your volume type by using this command:
  </p><div class="verbatim-wrap"><pre class="screen">openstack volume type list</pre></div><p>
   Then list the details about your volume type. For example:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume type show dfa8ecbd-8b95-49eb-bde7-6520aebacde0
+---------------------------------+--------------------------------------+
| Field                           | Value                                |
+---------------------------------+--------------------------------------+
| description                     | None                                 |
| id                              | dfa8ecbd-8b95-49eb-bde7-6520aebacde0 |
| is_public                       | True                                 |
| name                            | my3par                               |
| os-volume-type-access:is_public | True                                 |
| properties                      | volume_backend_name='3par'           |
+---------------------------------+--------------------------------------+</pre></div></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-blockstorage-xml-12"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Diagnosing back-end issues</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-blockstorage-xml-12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-blockstorage-xml-12</li></ul></div></div></div></div><p>
   You can find further troubleshooting steps for specific back-end types by
   vising these pages:
  </p></div></div><div class="sect2" id="troubleshooting-objectstorage"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-objectstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_swift.xml</li><li><span class="ds-label">ID: </span>troubleshooting-objectstorage</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Swift service. You can use
  these guides to help you identify and resolve basic problems you may
  experience while deploying or using the Object Storage service. It contains
  the following troubleshooting scenarios:
 </p><div class="sect3" id="topic-shs-j2b-kt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Fails With <span class="quote">“<span class="quote ">MSDOS Disks Labels Do Not Support Partition Names</span>”</span></span> <a title="Permalink" class="permalink" href="#topic-shs-j2b-kt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-deploy_fails_with_msdos_disks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-deploy_fails_with_msdos_disks.xml</li><li><span class="ds-label">ID: </span>topic-shs-j2b-kt</li></ul></div></div></div></div><p>
 Description
</p><p>
 If a disk drive allocated to Swift uses the MBR partition table type, the
 deploy process refuses to label and format the drive. This is to prevent
 potential data loss. (For more information, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.5 “Allocating Disk Drives for Object Storage”</span>. If you intend to use the disk drive for
 Swift, you must convert the MBR partition table to GPT on the drive using
 <code class="literal">/sbin/sgdisk</code>.
</p><div id="id-1.6.17.9.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   This process only applies to Swift drives. It does not apply to the
   operating system or boot drive.
  </p></div><p>
 Resolution
</p><p>
 You must install <code class="literal">gdisk</code>, before using
 <code class="literal">sgdisk</code>:
</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
   Run the following command to install <code class="literal">gdisk</code>:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper install gdisk</pre></div></li><li class="step "><p>
   Convert to the GPT partition type. Following is an example for
   converting <code class="literal">/dev/sdd</code> to the GPT partition type:
  </p><div class="verbatim-wrap"><pre class="screen">sudo sgdisk -g /dev/sdd</pre></div></li><li class="step "><p>
   Reboot the node to take effect. You may then resume the deployment
   (repeat the playbook that reported the error).
  </p></li></ol></div></div></div><div class="sect3" id="topic-ev4-q2b-kt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining Planned Ring Changes</span> <a title="Permalink" class="permalink" href="#topic-ev4-q2b-kt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-examine_details_planned_ring_changes_prior_deploy.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-examine_details_planned_ring_changes_prior_deploy.xml</li><li><span class="ds-label">ID: </span>topic-ev4-q2b-kt</li></ul></div></div></div></div><p>
  Before making major changes to your rings, you can see the planned layout of
  Swift rings using the following steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Run the <code class="literal">swift-compare-model-rings.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div></li><li class="step "><p>
    Validate the following in the output:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Drives are being added to all rings in the ring specifications.
     </p></li><li class="listitem "><p>
      Servers are being used as expected (for example, you may have a different
      set of servers for the account/container rings than the object rings.)
     </p></li><li class="listitem "><p>
      The drive size is the expected size.
     </p></li></ul></div></li></ol></div></div></div><div class="sect3" id="sec-input-swift-error"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interpreting Swift Input Model Validation Errors</span> <a title="Permalink" class="permalink" href="#sec-input-swift-error">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-interpreting_swift_validate_input_model.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-interpreting_swift_validate_input_model.xml</li><li><span class="ds-label">ID: </span>sec-input-swift-error</li></ul></div></div></div></div><p>
  The following examples provide an error message, description, and resolution.
 </p><div id="id-1.6.17.9.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   To resolve an error, you must first modify the input model and re-run the
   configuration processor. (For instructions, see
   <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>.) Then, continue with the deployment.
  </p></div><div class="orderedlist " id="ol-vnh-g2b-kt"><ol class="orderedlist" type="1"><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Model Mismatch: Cannot find drive
    /dev/sdt on padawan-ccp-c1-m2 (192.168.245.3))</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The disk model used for node <span class="bold"><strong>padawan-ccp-c1-m2</strong></span> has drive
                                    <code class="literal">/dev/sdt</code> listed in the devices list of a
                                device-group where Swift is the consumer. However, the
                                    <code class="literal">dev/sdt</code> device does not exist on that
                                node.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>
        <p>
         If a drive or controller is failed on a node, the operating system
         does not see the drive and so the corresponding block device may not
         exist. Sometimes this is transitory and a reboot may resolve the
         problem. The problem may not be with <code class="literal">/dev/sdt</code>, but
         with another drive. For example, if <code class="literal">/dev/sds</code> is
         failed, when you boot the node, the drive that you expect to be called
         <code class="literal">/dev/sdt</code> is actually called
         <code class="literal">/dev/sds</code>.
        </p>
        <p>
         Alternatively, there may not be enough drives installed in the server.
         You can add drives. Another option is to remove
         <code class="literal">/dev/sdt</code> from the appropriate disk model. However,
         this removes the drive for all servers using the disk model.
        </p>
       </td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Model Mismatch: Cannot find drive
    /dev/sdd2 on padawan-ccp-c1-m2 (192.168.245.3)</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The disk model used for node<span class="bold"><strong>
                                    padawan-ccp-c1-m2</strong></span> has drive
                                    <code class="literal">/dev/sdt</code> listed in the devices list of a
                                device-group where Swift is the consumer. However, the partition
                                number (2) has been specified in the model. This is not supported -
                                only specify the block device name (for example
                                    <code class="literal">/dev/sdd</code>), not partition names in disk
                                models.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Remove the partition number from the disk model.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Cannot find IP address of
    padawan-ccp-c1-m3-swift for ring: account host:
    padawan-ccp-c1-m3-mgmt</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The service (in this example, swift-account) is running on the
                                node <span class="bold"><strong>padawan-ccp-c1-m3</strong></span>. However,
                                this node does not have a connection to the network designated for
                                the <code class="literal">swift-account</code> service (that is, the SWIFT
                                network).</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Check the input model for which networks are configured for each
                                node type.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Ring: object-2 has specified
    replication_policy and erasure_coding_policy. Only one may be specified.
    </strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>Only either <code class="literal">replication-policy</code> or
                                    <code class="literal">erasure-coding-policy</code> may be used in
                                    <code class="literal">ring-specifications</code>.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Remove one of the policy types.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Ring: object-3 is missing a policy
    type (replication-policy or erasure-coding-policy) </strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>There is no <code class="literal">replication-policy</code> or
                                    <code class="literal">erasure-coding-policy</code> section in
                                    <code class="literal">ring-specifications</code> for the object-0
                                ring.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Add a policy type to the input model file. </td></tr></tbody></table></div></li></ol></div></div><div class="sect3" id="topic-rtc-s3t-mt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying the Swift Ring Building Server</span> <a title="Permalink" class="permalink" href="#topic-rtc-s3t-mt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-identify_ring_builder.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-identify_ring_builder.xml</li><li><span class="ds-label">ID: </span>topic-rtc-s3t-mt</li></ul></div></div></div></div><div class="sect4" id="idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identify the Swift Ring Building server</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-identify_ring_builder.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-identify_ring_builder.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6</li></ul></div></div></div></div><p>
   Perform the following steps to identify the Swift ring building server:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the following command:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml --limit SWF-ACC[0]</pre></div></li><li class="step "><p>
     Examine the output of this playbook. The last line underneath the play
     recap will give you the server name which is your Swift ring building
     server.
    </p><div class="verbatim-wrap"><pre class="screen">PLAY RECAP ********************************************************************
_SWF_CMN | status | Check systemd service running ----------------------- 1.61s
_SWF_CMN | status | Check systemd service running ----------------------- 1.16s
_SWF_CMN | status | Check systemd service running ----------------------- 1.09s
_SWF_CMN | status | Check systemd service running ----------------------- 0.32s
_SWF_CMN | status | Check systemd service running ----------------------- 0.31s
_SWF_CMN | status | Check systemd service running ----------------------- 0.26s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 7.88s
<span class="bold"><strong>ardana-cp1-c1-m1-mgmt</strong></span>      : ok=7    changed=0    unreachable=0    failed=0</pre></div><p>
     In the above example, the first swift proxy server is
     <code class="literal">ardana-cp1-c1-m1-mgmt</code>.
    </p></li></ol></div></div><div id="id-1.6.17.9.4.6.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    For the purposes of this document, any errors you see in the output of this
    playbook can be ignored if all you are looking for is the server name for
    your Swift ring builder server.
   </p></div></div></div><div class="sect3" id="verify-partition-label"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying a Swift Partition Label</span> <a title="Permalink" class="permalink" href="#verify-partition-label">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-label_on_partition.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-label_on_partition.xml</li><li><span class="ds-label">ID: </span>verify-partition-label</li></ul></div></div></div></div><div id="id-1.6.17.9.4.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   For a system upgrade do NOT clear the label before starting the upgrade.
  </p></div><p>
  This topic describes how to check whether a device has a label on a
  partition.
 </p><div class="sect4" id="label-partition"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Check Partition Label</span> <a title="Permalink" class="permalink" href="#label-partition">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-label_on_partition.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-label_on_partition.xml</li><li><span class="ds-label">ID: </span>label-partition</li></ul></div></div></div></div><p>
   To check whether a device has label on a partition, perform the following
   step:
  </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
     Log on to the node and use the <code class="literal">parted</code> command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo parted -l</pre></div><p>
     The output lists all of the block devices. Following is an example output
     for <code class="literal">/dev/sdc</code> with a single partition and a label of
     <span class="bold"><strong>c0a8f502h000</strong></span>.
     Because the partition has a label, if you are about to install and deploy
     the system, you must clear this label before starting the deployment. As
     part of the deployment process, the system will label the partition.
    </p><div class="verbatim-wrap"><pre class="screen">.
.
.
Model: QEMU QEMU HARDDISK (scsi)
Disk /dev/sdc: 20.0GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system  Name           Flags
1       1049kB  20.0GB  20.0GB  xfs          <span class="bold"><strong>c0a8f502h000</strong></span>

.
.
.</pre></div></li></ul></div></div></div></div><div class="sect3" id="verify-the-filesystem-label"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying a Swift File System Label</span> <a title="Permalink" class="permalink" href="#verify-the-filesystem-label">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-filesystem_label.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_label.xml</li><li><span class="ds-label">ID: </span>verify-the-filesystem-label</li></ul></div></div></div></div><div id="id-1.6.17.9.4.8.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   For a system upgrade do NOT clear the label before starting the upgrade.
  </p></div><p>
  This topic describes how to check whether a file system in a partition has a
  label.
 </p><p>
  To check whether a file system in a partition has a label, perform the
  following step:
 </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
    Log on to the server and execute the <code class="literal">xfs_admin</code> command
    (where <code class="literal">/dev/sdc1</code> is the partition where the file system
    is located):
   </p><div class="verbatim-wrap"><pre class="screen">sudo xfs_admin -l /dev/sdc1</pre></div><p>
    The output shows if a file system has a label. For example, this shows a
    label of <span class="bold"><strong>c0a8f502h000</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">$ sudo xfs_admin -l /dev/sdc1
label = "<span class="bold"><strong>c0a8f502h000</strong></span>"</pre></div><p>
    If no file system exists, the result is as follows:
   </p><div class="verbatim-wrap"><pre class="screen">$ sudo xfs_admin -l /dev/sde1
xfs_admin: /dev/sde is not a valid XFS file system (unexpected SB magic number 0x00000000)</pre></div><p>
    If you are about to install and deploy the system, you must delete the
    label before starting the deployment. As part of the deployment process,
    the system will label the partition.
   </p></li></ul></div></div></div><div class="sect3" id="topic-gbz-13t-mt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering Swift Builder Files</span> <a title="Permalink" class="permalink" href="#topic-gbz-13t-mt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-recovering_builder_file.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-recovering_builder_file.xml</li><li><span class="ds-label">ID: </span>topic-gbz-13t-mt</li></ul></div></div></div></div><p>
  When you execute the deploy process for a system, a copy of the builder files
  are stored on the following nodes and directories:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    On the Swift ring building node, the primary reference copy is stored in
    the
    <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
    directory.
   </p></li><li class="step "><p>
    On the next node after the Swift ring building node, a backup copy is
    stored in the
    <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
    directory.
   </p></li><li class="step "><p>
    In addition, in the deploy process, the builder files are also copied to
    the <code class="literal">/etc/swiftlm/deploy_dir/&lt;cloud-name&gt;</code> directory
    on every Swift node.
   </p></li></ol></div></div><p>
  If a copy of the builder files are found in the
  <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
  then no further recover action is needed. However, if all nodes running the
  Swift account (SWF-ACC) are lost, then you need to copy the files from the
  <code class="literal">/etc/swiftlm/deploy_dir/&lt;cloud-name&gt;</code> directory from
  an intact Swift node to the
  <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
  directory on the primary Swift ring building node.
 </p><p>
  If you have no intact <code class="literal">/etc/swiftlm</code> directory on any Swift
  node, you may be able to restore from Freezer. See
  <a class="xref" href="#recovering-controller-nodes" title="13.2.2.2. Recovering the Control Plane">Section 13.2.2.2, “Recovering the Control Plane”</a>.
 </p><p>
  To restore builder files from the <code class="literal">/etc/swiftlm/deploy_dir</code>
  directory, use the following process:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Swift ring building server (To identify the Swift ring
    building server, see <a class="xref" href="#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a>).
   </p></li><li class="step "><p>
    Create the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir</code> directory structure
    with these commands:
   </p><p>
    Replace <em class="replaceable ">CLOUD_NAME</em> with the name of your cloud
    and <em class="replaceable ">CONTROL_PLANE_NAME</em> with the name of your
    control plane.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo mkdir -p /etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/
<code class="prompt user">tux &gt; </code>sudo chown -R ardana.ardana /etc/swiftlm/</pre></div></li><li class="step "><p>
    Log in to a Swift node where an intact
    <code class="literal">/etc/swiftlm/deploy_dir</code> directory exists.
   </p></li><li class="step "><p>
    Copy the builder files to the Swift ring building node. In the example
    below we use scp to transfer the files, where
    <code class="literal">swpac-c1-m1-mgmt</code> is the ring building node,
    <code class="literal">cloud1</code> is the cloud, and <code class="literal">cp1</code> is the
    control plane name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp /etc/swiftlm//cloud1/cp1/* swpac-ccp-c1-m1-mgmt:/etc/swiftlm/cloud1/cp1/builder_dir/</pre></div></li><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Run the Swift reconfigure playbook to make sure every Swift node has the
    same rings:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="sec-trouble-restart-storeage-deployment"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting the Object Storage Deployment</span> <a title="Permalink" class="permalink" href="#sec-trouble-restart-storeage-deployment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>sec-trouble-restart-storeage-deployment</li></ul></div></div></div></div><p>
  This page describes the various operational procedures performed by Swift.
 </p><div class="sect4" id="restart-swift-depl"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Swift Object Storage Deployment</span> <a title="Permalink" class="permalink" href="#restart-swift-depl">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>restart-swift-depl</li></ul></div></div></div></div><p>
   The structure of ring is built in an incremental stages. When you modify a
   ring, the new ring uses the state of the old ring as a basis for the new
   ring. Rings are stored in the builder file. The
   <code class="literal">swiftlm-ring-supervisor</code> stores builder files in the
   <code class="literal">/etc/swiftlm/cloud1/cp1/builder_dir/</code>
   directory on the Ring-Builder node. The builder files are named
   &lt;ring-name&gt; builder. Prior versions of the builder files are stored in
   the
   <code class="literal">/etc/swiftlm/cloud1/cp1/builder_dir/backups</code>
   directory.
  </p><p>
   Generally, you use an existing builder file as the basis for changes to a
   ring. However, at initial deployment, when you create a ring there will be
   no builder file. Instead, the first step in the process is to build a
   builder file. The deploy playbook does this as a part of the deployment
   process. If you have successfully deployed some of the system, the ring
   builder files will exist.
  </p><p>
   If you change your input model (for example, by adding servers) now, the
   process assumes you are <span class="emphasis"><em>modifying</em></span> a ring and behaves
   differently than while creating a ring from scratch. In this case, the ring
   is not balanced. So, if the cloud model contains an error or you decide to
   make substantive changes, it is a best practice to start from scratch and
   build rings using the steps below.
  </p></div><div class="sect4" id="reset-builder-files"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reset Builder Files</span> <a title="Permalink" class="permalink" href="#reset-builder-files">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>reset-builder-files</li></ul></div></div></div></div><p>
   You must reset the builder files during the initial deployment process
   (only). This process should be used only when you want to restart a
   deployment from scratch. If you reset the builder files after completing
   your initial deployment, then you are at a risk of losing critical system
   data.
  </p><p>
   Delete the builder files in the
   <code class="filename">/etc/swiftlm/cloud1/cp1/builder-dir/</code>
   directory. For example, for the region0 Keystone region (the default single
   region designation), do the following:
  </p><div class="verbatim-wrap"><pre class="screen">sudo rm /etc/swiftlm/cloud1/cp1/builder_dir/*.builder</pre></div><div id="id-1.6.17.9.4.10.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you have successfully deployed a system and accidentally delete the
    builder files, you can recover to the correct state. For instructions, see
    <a class="xref" href="#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
   </p></div></div></div><div class="sect3" id="increase-timeout"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increasing the Swift Node Timeout Value</span> <a title="Permalink" class="permalink" href="#increase-timeout">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-increase_timeout.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-increase_timeout.xml</li><li><span class="ds-label">ID: </span>increase-timeout</li></ul></div></div></div></div><p>
  On a heavily loaded Object Storage system timeouts may occur when
  transferring data to or from Swift, particularly large objects.
 </p><p>
  The following is an example of a timeout message in the log
  (<code class="literal">/var/log/swift/swift.log</code>) on a Swift proxy server:
 </p><div class="verbatim-wrap"><pre class="screen">Jan 21 16:55:08 ardana-cp1-swpaco-m1-mgmt proxy-server: ERROR with Object server 10.243.66.202:6000/disk1 re: Trying to write to
/v1/AUTH_1234/testcontainer/largeobject: ChunkWriteTimeout (10s)</pre></div><p>
  If this occurs, it may be necessary to increase the
  <code class="literal">node_timeout</code> parameter in the
  <code class="literal">proxy-server.conf</code> configuration file.
 </p><p>
  The <code class="literal">node_timeout</code> parameter in the Swift
  <code class="literal">proxy-server.conf</code> file is the maximum amount of time the
  proxy server will wait for a response from the account, container, or object
  server. The default value is 10 seconds.
 </p><p>
  In order to modify the timeout you can use these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Edit the
    <code class="literal">~/openstack/my_cloud/config/swift/proxy-server.conf.j2</code> file
    and add a line specifying the <code class="literal">node_timeout</code> into the
    <code class="literal">[app:proxy-server]</code> section of the file.
   </p><p>
    Example, in bold, increasing the timeout to 30 seconds:
   </p><div class="verbatim-wrap"><pre class="screen">[app:proxy-server]
use = egg:swift#proxy
.
.
<span class="bold"><strong>node_timeout = 30</strong></span></pre></div></li><li class="step "><p>
    Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, as follows:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Use the playbook below to create a deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Change to the deployment directory and run the Swift reconfigure playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="swift-filesystem-ts"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Swift File System Usage Issues</span> <a title="Permalink" class="permalink" href="#swift-filesystem-ts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml</li><li><span class="ds-label">ID: </span>swift-filesystem-ts</li></ul></div></div></div></div><p>
  If you have recycled your environment to do a re-installation and you haven't
  run the <code class="literal">wipe_disks.yml</code> playbook in the process, you may
  experience an issue where your file system usage continues to grow
  exponentially even though you are not adding any files to your Swift system.
  This is likely occurring because the quarantined directory is getting filled
  up. You can find this directory at
  <code class="literal">/srv/node/disk0/quarantined</code>.
 </p><p>
  You can resolve this issue by following these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    SSH to each of your Swift nodes and stop the replication processes on each
    of them. The following commands must be executed on each of your Swift
    nodes. Make note of the time that you performed this action as you will
    reference it in step three.
   </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop swift-account-replicator
sudo systemctl stop swift-container-replicator
sudo systemctl stop swift-object-replicator</pre></div></li><li class="step "><p>
    Examine the <code class="literal">/var/log/swift/swift.log</code> file for events
    that indicate when the auditor processes have started and completed audit
    cycles. For more details, see <a class="xref" href="#swift-filesystem-ts" title="15.6.2.10. Troubleshooting Swift File System Usage Issues">Section 15.6.2.10, “Troubleshooting Swift File System Usage Issues”</a>.
   </p></li><li class="step "><p>
    Wait until you see that the auditor processes have finished two complete
    cycles since the time you stopped the replication processes (from step
    one). You must check every Swift node, which on a lightly loaded system
    that was recently installed this should take less than two hours.
   </p></li><li class="step "><p>
    At this point you should notice that your quarantined directory has stopped
    growing. You may now delete the files in that directory on each of your
    nodes.
   </p></li><li class="step "><p>
    Restart the replication processes using the Swift start playbook:
   </p><ol type="a" class="substeps "><li class="step "><p>
      Log in to the Cloud Lifecycle Manager.
     </p></li><li class="step "><p>
      Run the Swift start playbook:
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-start.yml</pre></div></li></ol></li></ol></div></div><div class="sect4" id="swift-log"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining the Swift Log for Audit Event Cycles</span> <a title="Permalink" class="permalink" href="#swift-log">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml</li><li><span class="ds-label">ID: </span>swift-log</li></ul></div></div></div></div><p>
   Below is an example of the <code class="literal">object-server</code> start and end
   cycle details. They were taken by using the following command on a Swift
   node:
  </p><div class="verbatim-wrap"><pre class="screen">sudo grep object-auditor /var/log/swift/swift.log|grep ALL</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo grep object-auditor /var/log/swift/swift.log|grep ALL
...
Apr  1 13:31:18 padawan-ccp-c1-m1-mgmt object-auditor: Begin object audit "forever" mode (ALL)
Apr  1 13:31:18 padawan-ccp-c1-m1-mgmt object-auditor: Object audit (ALL). Since Fri Apr  1 13:31:18 2016: Locally: 0 passed, 0 quarantined, 0 errors files/sec: 0.00 , bytes/sec: 0.00, Total time: 0.00, Auditing time: 0.00, Rate: 0.00
Apr  1 13:51:32 padawan-ccp-c1-m1-mgmt object-auditor: Object audit (ALL) "forever" mode completed: 1213.78s. Total quarantined: 0, Total errors: 0, Total files/sec: 7.02, Total bytes/sec: 9999722.38, Auditing time: 1213.07, Rate: 1.00</pre></div><p>
   In this example, the auditor started at <code class="literal">13:31</code> and ended
   at <code class="literal">13:51</code>.
  </p><p>
   In this next example, the <code class="literal">account-auditor</code> and
   <code class="literal">container-auditor</code> use similar message structure, so we
   only show the container auditor. You can substitute
   <code class="literal">account</code> for <code class="literal">container</code> as well:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo grep container-auditor /var/log/swift/swift.log
...
Apr  1 14:07:00 padawan-ccp-c1-m1-mgmt container-auditor: Begin container audit pass.
Apr  1 14:07:00 padawan-ccp-c1-m1-mgmt container-auditor: Since Fri Apr  1 13:07:00 2016: Container audits: 42 passed audit, 0 failed audit
Apr  1 14:37:00 padawan-ccp-c1-m1-mgmt container-auditor: Container audit pass completed: 0.10s</pre></div><p>
   In the example, the container auditor started a cycle at
   <code class="literal">14:07</code> and the cycle finished at <code class="literal">14:37</code>.
  </p></div></div></div></div><div class="sect1" id="monitoring-logging-usage-reporting"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span> <a title="Permalink" class="permalink" href="#monitoring-logging-usage-reporting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_telemetry.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_telemetry.xml</li><li><span class="ds-label">ID: </span>monitoring-logging-usage-reporting</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Monitoring, Logging, and
  Usage Reporting services.
 </p><div class="sect2" id="sec-central-log-troubleshoot"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Centralized Logging</span> <a title="Permalink" class="permalink" href="#sec-central-log-troubleshoot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-troubleshoot</li></ul></div></div></div></div><p>
  This section contains the following scenarios:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-review-logs" title="15.7.1.1. Reviewing Log Files">Section 15.7.1.1, “Reviewing Log Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-monitor" title="15.7.1.2. Monitoring Centralized Logging">Section 15.7.1.2, “Monitoring Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-log-collection" title="15.7.1.3. Situations In Which Logs Might Not Be Collected">Section 15.7.1.3, “Situations In Which Logs Might Not Be Collected”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-error-kibana" title="15.7.1.4. Error When Creating a Kibana Visualization">Section 15.7.1.4, “Error When Creating a Kibana Visualization”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-store-log" title="15.7.1.5. After Deploying Logging-API, Logs Are Not Centrally Stored">Section 15.7.1.5, “After Deploying Logging-API, Logs Are Not Centrally Stored”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-slow-log" title="15.7.1.6. Re-enabling Slow Logging">Section 15.7.1.6, “Re-enabling Slow Logging”</a>
   </p></li></ul></div><div class="sect3" id="sec-central-log-review-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reviewing Log Files</span> <a title="Permalink" class="permalink" href="#sec-central-log-review-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-review-logs</li></ul></div></div></div></div><p>
   You can troubleshoot service-specific issues by reviewing the logs. After
   logging into Kibana, follow these steps to load the logs for viewing:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Settings</strong></span> menu to
     configure an index pattern to search for.
    </p></li><li class="listitem "><p>
     In the <span class="bold"><strong>Index name or pattern</strong></span> field, you
     can enter <code class="literal">logstash-*</code> to query all Elasticsearch
     indices.
    </p></li><li class="listitem "><p>
     Click the green <span class="bold"><strong>Create</strong></span> button to create
     and load the index.
    </p></li><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Discover</strong></span> menu to load the
     index and make it available to search.
    </p></li></ol></div><div id="id-1.6.17.10.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you want to search specific Elasticsearch indices, you can run the
    following command from the control plane to get a full list of available
    indices:
   </p><div class="verbatim-wrap"><pre class="screen">curl localhost:9200/_cat/indices?v</pre></div></div><p>
   Once the logs load you can change the timeframe from the dropdown in the
   upper-righthand corner of the Kibana window. You have the following options
   to choose from:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Quick</strong></span> - a variety of time frame choices
     will be available here
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Relative</strong></span> - allows you to select a start
     time relative to the current time to show this range
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Absolute</strong></span> - allows you to select a date
     range to query
    </p></li></ul></div><p>
   When searching there are common fields you will want to use, such as:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>type</strong></span> - this will include the service
     name, such as <code class="literal">keystone</code> or <code class="literal">ceilometer</code>
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>host </strong></span>- you can specify a specific host to
     search for in the logs
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>file</strong></span> - you can specify a specific log
     file to search
    </p></li></ul></div><p>
   For more details on using Kibana and Elasticsearch to query logs, see
   <a class="link" href="https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html" target="_blank">https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html</a>
  </p></div><div class="sect3" id="sec-central-log-monitor"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Centralized Logging</span> <a title="Permalink" class="permalink" href="#sec-central-log-monitor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-monitor</li></ul></div></div></div></div><p>
   To help keep ahead of potential logging issues and resolve issues before
   they affect logging, you may want to monitor the Centralized Logging Alarms.
  </p><p>
   <span class="bold"><strong>To monitor logging alarms:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Operations Console.
    </p></li><li class="listitem "><p>
     From the menu button in the upper left corner, navigate to the
     <span class="bold"><strong>Alarm Definitions</strong></span> page.
    </p></li><li class="listitem "><p>
     Find the alarm definitions that are applied to the various hosts. See the
     <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> for the Centralized Logging Alarm
     Definitions.
    </p></li><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Alarms</strong></span> page
    </p></li><li class="listitem "><p>
     Find the alarm definitions applied to the various hosts. These should
     match the alarm definitions in the <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a>.
    </p></li><li class="listitem "><p>
     See if the alarm is green (good) or is in a bad state. If any are in a bad
     state, see the possible actions to perform in the
     <a class="xref" href="#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a>.
    </p></li></ol></div><p>
   You can use this filtering technique in the "Alarms" page to look for the
   following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To look for processes that may be down, filter for
     <span class="bold"><strong>"Process"</strong></span> then make sure the process are
     up:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Elasticsearch
      </p></li><li class="listitem "><p>
       Logstash
      </p></li><li class="listitem "><p>
       Beaver
      </p></li><li class="listitem "><p>
       Apache (Kafka)
      </p></li><li class="listitem "><p>
       Kibana
      </p></li><li class="listitem "><p>
       Monasca
      </p></li></ul></div></li><li class="listitem "><p>
     To look for sufficient disk space, filter for
     <span class="bold"><strong>"Disk"</strong></span>
    </p></li><li class="listitem "><p>
     To look for sufficient RAM memory, filter for
     <span class="bold"><strong>"Memory"</strong></span>
    </p></li></ol></div></div><div class="sect3" id="sec-central-log-log-collection"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Situations In Which Logs Might Not Be Collected</span> <a title="Permalink" class="permalink" href="#sec-central-log-log-collection">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-log-collection</li></ul></div></div></div></div><p>
   Centralized logging might not collect log data under the following
   circumstances:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If the Beaver service is not running on one or more of the nodes
     (controller or compute), logs from these nodes will not be collected.
    </p></li></ul></div></div><div class="sect3" id="sec-central-log-error-kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error When Creating a Kibana Visualization</span> <a title="Permalink" class="permalink" href="#sec-central-log-error-kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-error-kibana</li></ul></div></div></div></div><p>
   When creating a visualization in Kibana you may get an error similiar to
   this:
  </p><div class="verbatim-wrap"><pre class="screen">"logstash-*" index pattern does not contain any of the following field types: number</pre></div><p>
   To resolve this issue:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Kibana.
    </p></li><li class="listitem "><p>
     Navigate to the <code class="literal">Settings</code> page.
    </p></li><li class="listitem "><p>
     In the left panel, select the <code class="literal">logstash-*</code> index.
    </p></li><li class="listitem "><p>
     Click the <span class="bold"><strong>Refresh</strong></span> button. You may see a
     mapping conflict warning after refreshing the index.
    </p></li><li class="listitem "><p>
     Re-create the visualization.
    </p></li></ol></div></div><div class="sect3" id="sec-central-log-store-log"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">After Deploying Logging-API, Logs Are Not Centrally Stored</span> <a title="Permalink" class="permalink" href="#sec-central-log-store-log">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-store-log</li></ul></div></div></div></div><p>
   If you are using the Logging-API and logs are not being centrally stored,
   use the following checklist to troubleshoot Logging-API.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       <p>
        Ensure Monasca is running.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Check any alarms Monasca has triggered.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Check to see if the Logging-API (monasca-log-api) process alarm has
        triggered.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Run an Ansible playbook to get status of the Cloud Lifecycle Manager:
       </p>
<div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div>
      </td></tr><tr><td> </td><td>
       <p>
        Troubleshoot all specific tasks that have failed on the Cloud Lifecycle Manager.
       </p>
      </td></tr><tr><td> </td><td>Ensure that the Logging-API daemon is up.</td></tr><tr><td> </td><td>
       <p>
        Run an Ansible playbook to try and bring the Logging-API daemon up:
       </p>
<div class="verbatim-wrap"><pre class="screen">ansible-playbook –I hosts/verb_hosts logging-start.yml</pre></div>
      </td></tr><tr><td> </td><td>
       <p>
        If you get errors trying to bring up the daemon, resolve them.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Verify the Logging-API configuration settings are correct in the
        configuration file:
       </p>
<div class="verbatim-wrap"><pre class="screen">roles/kronos-api/templates/kronos-apache2.conf.j2</pre></div>
      </td></tr></tbody></table></div><p>
   The following is a sample Logging-API configuration file:
  </p><div class="verbatim-wrap"><pre class="screen">{#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
#}
Listen {{ kronos_api_host }}:{{ kronos_api_port }}
&lt;VirtualHost *:{{ kronos_api_port }}&gt;
    WSGIDaemonProcess log-api processes=4 threads=4 socket-timeout=300  user={{ kronos_user }} group={{ kronos_group }} python-path=/opt/stack/service/kronos/venv:/opt/stack/service/kronos/venv/bin/../lib/python2.7/site-packages/ display-name=monasca-log-api
    WSGIProcessGroup log-api
    WSGIApplicationGroup log-api
    WSGIScriptAlias / {{ kronos_wsgi_dir }}/app.wsgi
    ErrorLog /var/log/kronos/wsgi.log
    LogLevel info
    CustomLog /var/log/kronos/wsgi-access.log combined

    &lt;Directory /opt/stack/service/kronos/venv/bin/../lib/python2.7/site-packages/monasca_log_api&gt;
      Options Indexes FollowSymLinks MultiViews
      Require all granted
      AllowOverride None
      Order allow,deny
      allow from all
      LimitRequestBody 102400
    &lt;/Directory&gt;

    SetEnv no-gzip 1
&lt;/VirtualHost&gt;</pre></div></div><div class="sect3" id="sec-central-log-slow-log"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-enabling Slow Logging</span> <a title="Permalink" class="permalink" href="#sec-central-log-slow-log">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-slow-log</li></ul></div></div></div></div><p>
   MariaDB slow logging was enabled by default in earlier versions. Slow
   logging logs slow MariaDB queries to
   <code class="filename">/var/log/mysql/mysql-slow.log</code> on
   FND-MDB hosts.
  </p><p>
   As it is possible for temporary tokens to be logged to the slow log, we have
   disabled slow log in this version for security reasons.
  </p><p>
   To re-enable slow logging follow the following procedure:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to the Cloud Lifecycle Manager and set a mariadb service configurable to
     enable slow logging.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud</pre></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Check slow_query_log is currently disabled with a value of 0:
      </p><div class="verbatim-wrap"><pre class="screen">grep slow ./config/percona/my.cfg.j2
slow_query_log          = 0
slow_query_log_file     = /var/log/mysql/mysql-slow.log</pre></div></li><li class="listitem "><p>
       Enable slow logging in the server configurable template file and confirm
       the new value:
      </p><div class="verbatim-wrap"><pre class="screen">sed -e 's/slow_query_log = 0/slow_query_log = 1/' -i ./config/percona/my.cfg.j2
grep slow ./config/percona/my.cfg.j2
slow_query_log          = 1
slow_query_log_file     = /var/log/mysql/mysql-slow.log</pre></div></li><li class="listitem "><p>
       Commit the changes:
      </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Enable Slow Logging"</pre></div></li></ol></div></li><li class="listitem "><p>
     Run the configuration procesor.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     You will be prompted for an encryption key, and also asked if you want to
     change the encryption key to a new value, and it must be a different key.
     You can turn off encryption by typing the following:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</pre></div></li><li class="listitem "><p>
     Create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Reconfigure Percona (note this will restart your mysqld server on your
     cluster hosts).
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts percona-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="topic1976"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage Reporting Troubleshooting</span> <a title="Permalink" class="permalink" href="#topic1976">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>topic1976</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Ceilometer service.
 </p><p>
  This page describes troubleshooting scenarios for Ceilometer.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-ts-metering-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-metering-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-metering-xml-6</li></ul></div></div></div></div><p>
   Logs for the various running components in the Overcloud Controllers can be
   found at <span class="emphasis"><em>/var/log/ceilometer.log</em></span>
  </p><p>
   The Upstart for the services also logs data at
   <span class="bold"><strong>/var/log/upstart</strong></span>
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-metering-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-metering-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-metering-xml-7</li></ul></div></div></div></div><p>
   Change the level of debugging in Ceilometer by editing the
   <span class="bold"><strong>ceilometer.conf</strong></span>
   file located at
   <span class="bold"><strong>/etc/ceilometer/ceilometer.conf</strong></span>.
   To log the maximum amount of information, change the
   <span class="bold"><strong>level</strong></span>
   entry to <span class="bold"><strong>DEBUG</strong></span>.
  </p><p>
   <span class="bold"><strong>Note</strong></span>: When the logging level for a service
   is changed, that service must be re-started before the change will take
   effect.
  </p><p>
   This is an excerpt of the <span class="bold"><strong>ceilometer.conf</strong></span>
   configuration file showing where to make changes:
  </p><div class="verbatim-wrap"><pre class="screen">[loggers]
 keys: root

[handlers]
 keys: watchedfile, logstash

[formatters]
 keys: context, logstash

[logger_root]
 qualname: root
 handlers: watchedfile, logstash
 level: NOTSET</pre></div></div><div class="sect3" id="qerrors"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Messaging/Queuing Errors</span> <a title="Permalink" class="permalink" href="#qerrors">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>qerrors</li></ul></div></div></div></div><p>
   Ceilometer relies on a message bus for passing data between the various
   components. In high-availability scenarios, RabbitMQ servers are used for
   this purpose. If these servers are not available, the Ceilometer log will
   record errors during "Connecting to AMQP" attempts.
  </p><p>
   These errors may indicate that the RabbitMQ messaging nodes are not running
   as expected and/or the RPC publishing pipeline is stale. When these errors
   occur, re-start the instances.
  </p><p>
   Example error:
  </p><div class="verbatim-wrap"><pre class="screen">Error: unable to connect to node 'rabbit@xxxx-rabbitmq0000': nodedown</pre></div><p>
   Use the RabbitMQ CLI to re-start the instances and then the host.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Restart the downed cluster node.
    </p><div class="verbatim-wrap"><pre class="screen">sudo invoke-rc.d rabbitmq-server start</pre></div></li><li class="listitem "><p>
     Restart the RabbitMQ host
    </p><div class="verbatim-wrap"><pre class="screen">sudo rabbitmqctl start_app</pre></div></li></ol></div></div></div></div><div class="sect1" id="topic-ly3-yyr-st"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup and Restore Troubleshooting</span> <a title="Permalink" class="permalink" href="#topic-ly3-yyr-st">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_bura.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_bura.xml</li><li><span class="ds-label">ID: </span>topic-ly3-yyr-st</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Backup and Restore
  service.
 </p><p>
  The following logs will help you troubleshoot Freezer functionality:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><tbody><tr><td>Component</td><td>Description</td></tr><tr><td>Freezer Client</td><td>
      <p>
       /var/log/freezer-agent/freezer-agent.log
      </p>
     </td></tr><tr><td>Freezer Scheduler</td><td>/var/log/freezer-agent/freezer-scheduler.log</td></tr><tr><td>Freezer API</td><td>/var/log/freezer-api/freezer-api-access.log/var/log/freezer-api/freezer-api-modwsgi.log
            /var/log/freezer-api/freezer-api.log</td></tr></tbody></table></div><p>
  The following issues apply to the Freezer UI and the backup and restore
  process:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The UI for backup and restore is supported only if you log in as
    "ardana_backup". All other users will see the UI panel but the UI will not
    work.
   </p></li><li class="listitem "><p>
    If a backup or restore action fails via the UI, you must check the Freezer
    logs for details of the failure.
   </p></li><li class="listitem "><p>
    Job Status and Job Result on the UI and backend (CLI) are not in sync.
   </p></li><li class="listitem "><p>
    For a given "Action" the following modes are not supported from the UI:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Microsoft SQL Server
     </p></li><li class="listitem "><p>
      Cinder
     </p></li><li class="listitem "><p>
      Nova
     </p></li></ul></div></li><li class="listitem "><p>
    Start and end dates and times available for job creation should not be used
    due to a known issue. Please refrain from using those fields.
   </p></li><li class="listitem "><p>
    Once a backup is created. A listing of the contents is needed to verify if
    the backup of any single item was done.
   </p></li></ul></div></div><div class="sect1" id="troubleshooting-orchestration"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Orchestration Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-orchestration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_orchestration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_orchestration.xml</li><li><span class="ds-label">ID: </span>troubleshooting-orchestration</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Orchestration services.
  Troubleshooting scenarios with resolutions for the Orchestration services.
 </p><div class="sect2" id="troubleshootingHeat"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Heat Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshootingHeat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>troubleshootingHeat</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Heat service. This page
  describes troubleshooting scenarios for Heat.
 </p><div class="sect3" id="rpc-timeout-heat"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RPC timeout on Heat stack creation</span> <a title="Permalink" class="permalink" href="#rpc-timeout-heat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>rpc-timeout-heat</li></ul></div></div></div></div><p>
   If you exerience a remote procedure call (RPC) timeout failure when
   attempting heat stack-create, you can work around the issue by increasing
   the timeout value and purging records of deleted stacks from the database.
   To do so, follow the steps below. An example of the error is:
  </p><div class="verbatim-wrap"><pre class="screen">MessagingTimeout: resources.XXX-LCP-Pair01.resources[0]: Timed out waiting for a reply to message ID e861c4e0d9d74f2ea77d3ec1984c5cb6</pre></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Increase the timeout value.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/config/heat</pre></div></li><li class="step "><p>
     Make changes to heat config files. In heat.conf.j2 add this timeout value:
    </p><div class="verbatim-wrap"><pre class="screen">rpc_response_timeout=300</pre></div><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "some message"</pre></div></li><li class="step "><p>
     Move to ansible directory and run the following playbooks:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Change to the scratch directory and run heat-reconfigure:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li><li class="step "><p>
     Purge records of deleted stacks from the database. First delete all stacks
     that are in failed state. Then execute the following
    </p><div class="verbatim-wrap"><pre class="screen">sudo /opt/stack/venv/heat-20151116T000451Z/bin/python2
/opt/stack/service/heat-engine/venv/bin/heat-manage
--config-file /opt/stack/service/heat-engine-20151116T000451Z/etc/heat/heat.conf
--config-file /opt/stack/service/heat-engine-20151116T000451Z/etc/heat/engine.conf purge_deleted 0</pre></div></li></ol></div></div></div><div class="sect3" id="hrat-stack-create-errors"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General Heat stack creation errors</span> <a title="Permalink" class="permalink" href="#hrat-stack-create-errors">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>hrat-stack-create-errors</li></ul></div></div></div></div><p>
   In Heat, in general when a timeout occurs it means that the underlying
   resource service such as Nova, Neutron, or Cinder, fails to complete the
   required action. No matter what error this underlying service reports, Heat
   simply reports it back. So in the case of time-out in Heat stack create, you
   should look at the logs of the underlying services, most importantly the
   Nova service, to understand the reason for the timeout.
  </p></div><div class="sect3" id="heat-stack-create-failure"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Heat stack create failure</span> <a title="Permalink" class="permalink" href="#heat-stack-create-failure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>heat-stack-create-failure</li></ul></div></div></div></div><p>
   The Monasca AlarmDefinition resource,
   <code class="literal">OS::Monasca::AlarmDefinition</code> used for Heat autoscaling,
   consists of an optional property
   <span class="bold"><strong>name</strong></span> for
   defining the alarm name. In case this optional property being specified in
   the Heat template, this name must be unique in the same project of the
   system. Otherwise, multiple heat stack create using this heat template will
   fail with the following conflict:
  </p><div class="verbatim-wrap"><pre class="screen">| cpu_alarm_low  | 5fe0151b-5c6a-4a54-bd64-67405336a740 | HTTPConflict: resources.cpu_alarm_low: An alarm definition already exists for project / tenant: 835d6aeeb36249b88903b25ed3d2e55a named: CPU utilization less than 15 percent  | CREATE_FAILED  | 2016-07-29T10:28:47 |</pre></div><p>
   This is due to the fact that the Monasca registers the alarm definition name
   using this name property when it is defined in the Heat template. This name
   must be unique.
  </p><p>
   To avoid this problem, if you want to define an alarm name using this
   property in the template, you must be sure this name is unique within a
   project in the system. Otherwise, you can leave this optional property
   undefined in your template. In this case, the system will create an unique
   alarm name automatically during heat stack create.
  </p></div><div class="sect3" id="id-1.6.17.12.3.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unable to Retrieve QOS Policies</span> <a title="Permalink" class="permalink" href="#id-1.6.17.12.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Launching the Orchestration Template Generator may trigger the message:
   <code class="literal">Unable to retrieve resources Qos Policies</code>. This is a
   known <a class="link" href="https://storyboard.openstack.org/#!/story/2003523" target="_blank">upstream
   bug</a>. This information message can be ignored.
  </p></div></div><div class="sect2" id="ts-magnum"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Magnum Service</span> <a title="Permalink" class="permalink" href="#ts-magnum">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_magnum.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_magnum.xml</li><li><span class="ds-label">ID: </span>ts-magnum</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Magnum service. Magnum
  Service provides container orchestration engines such as Docker Swarm,
  Kubernetes, and Apache Mesos available as first class resources. You can use
  this guide to help with known issues and troubleshooting of Magnum services.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-ts-magnum-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum cluster fails to create</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-magnum-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_magnum.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_magnum.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-magnum-xml-6</li></ul></div></div></div></div><p>
   Typically, small size clusters need about 3-5 minutes to stand up. If
   cluster stand up takes longer, you may proceed with troubleshooting, not
   waiting for status to turn to CREATE_FAILED after timing out.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use <code class="literal">heat resource-list -n2</code> to identify which Heat stack
     resource is stuck in <span class="bold"><strong>CREATE_IN_PROGRESS</strong></span>.
    </p><div id="id-1.6.17.12.4.3.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The main Heat stack has nested stacks, one for kubemaster(s) and one
      for kubeminion(s). These stacks are visible as resources of type
      <span class="emphasis"><em>OS::Heat::ResourceGroup</em></span> (in parent stack) and
      <span class="emphasis"><em>file:///...</em></span> in nested stack. If any resource
      remains in <span class="emphasis"><em>CREATE_IN_PROGRESS</em></span> state within the
      nested stack, the overall state of the resource will be
      <span class="emphasis"><em>CREATE_IN_PROGRESS</em></span>.
     </p></div><div class="verbatim-wrap"><pre class="screen">$ heat resource-list -n2 22385a42-9e15-49d9-a382-f28acef36810
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| resource_name                 | physical_resource_id                 | resource_type                        | resource_status    | updated_time         | stack_name                                                       |
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| api_address_floating_switch   | 06b2cc0d-77f9-4633-8d96-f51e2db1faf3 | Magnum::FloatingIPAddressSwitcher    | CREATE_COMPLETE    | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
. . .

| fixed_subnet                  | d782bdf2-1324-49db-83a8-6a3e04f48bb9 | OS::Neutron::Subnet                  | CREATE_COMPLETE    | 2017-04-10T21:25:11Z | my-cluster-z4aquda2mgpv                                          |
| kube_masters                  | f0d000aa-d7b1-441a-a32b-17125552d3e0 | OS::Heat::ResourceGroup              | CREATE_IN_PROGRESS | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
| 0                             | b1ff8e2c-23dc-490e-ac7e-14e9f419cfb6 | file:///opt/s...ates/kubemaster.yaml | CREATE_IN_PROGRESS | 2017-04-10T21:25:41Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb                |
| kube_master                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5 | OS::Nova::Server                     | CREATE_IN_PROGRESS | 2017-04-10T21:25:48Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb-0-saafd5k7l7im |
. . .</pre></div></li><li class="listitem "><p>
     If stack creation failed on some native OpenStack resource, like
     <span class="bold"><strong>OS::Nova::Server</strong></span> or
     <span class="bold"><strong>OS::Neutron::Router</strong></span>, proceed with
     respective service troubleshooting. This type of error usually does not
     cause time out, and cluster turns into status
     <span class="bold"><strong>CREATE_FAILED</strong></span> quickly. The underlying
     reason of the failure, reported by Heat, can be checked via the
     <code class="literal">magnum cluster-show</code> command.
    </p></li><li class="listitem "><p>
     If stack creation stopped on resource of type OS::Heat::WaitCondition,
     Heat is not receiving notification from cluster VM about bootstrap
     sequence completion. Locate corresponding resource of type
     <span class="bold"><strong>OS::Nova::Server</strong></span> and use its
     <span class="bold"><strong>physical_resource_id</strong></span> to get information
     about the VM (which should be in status
     <span class="bold"><strong>CREATE_COMPLETE</strong></span>)
    </p><div class="verbatim-wrap"><pre class="screen">$ nova show 4d96510e-c202-4c62-8157-c0e3dddff6d5
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                                         |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                                        |
| OS-EXT-AZ:availability_zone          | nova                                                                                                          |
| OS-EXT-SRV-ATTR:host                 | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000025                                                                                             |
| OS-EXT-STS:power_state               | 1                                                                                                             |
| OS-EXT-STS:task_state                | -                                                                                                             |
| OS-EXT-STS:vm_state                  | active                                                                                                        |
| OS-SRV-USG:launched_at               | 2017-04-10T22:10:40.000000                                                                                    |
| OS-SRV-USG:terminated_at             | -                                                                                                             |
| accessIPv4                           |                                                                                                               |
| accessIPv6                           |                                                                                                               |
| config_drive                         |                                                                                                               |
| created                              | 2017-04-10T22:09:53Z                                                                                          |
| flavor                               | m1.small (2)                                                                                                  |
| hostId                               | eb101a0293a9c4c3a2d79cee4297ab6969e0f4ddd105f4d207df67d2                                                      |
| id                                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5                                                                          |
| image                                | fedora-atomic-26-20170723.0.x86_64 (4277115a-f254-46c0-9fb0-fffc45d2fd38)                                     |
| key_name                             | testkey                                                                                                       |
| metadata                             | {}                                                                                                            |
| name                                 | my-zaqshggwge-0-sqhpyez4dig7-kube_master-wc4vv7ta42r6                                                         |
| os-extended-volumes:volumes_attached | [{"id": "24012ce2-43dd-42b7-818f-12967cb4eb81"}]                                                              |
| private network                      | 10.0.0.14, 172.31.0.6                                                                                         |
| progress                             | 0                                                                                                             |
| security_groups                      | my-cluster-z7ttt2jvmyqf-secgroup_base-gzcpzsiqkhxx, my-cluster-z7ttt2jvmyqf-secgroup_kube_master-27mzhmkjiv5v |
| status                               | ACTIVE                                                                                                        |
| tenant_id                            | 2f5b83ab49d54aaea4b39f5082301d09                                                                              |
| updated                              | 2017-04-10T22:10:40Z                                                                                          |
| user_id                              | 7eba6d32db154d4790e1d3877f6056fb                                                                              |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     Use the floating IP of the master VM to log into first master node. Use
     the appropriate username below for your VM type. Passwords should not be
     required as the VMs should have public ssh key installed.
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th align="center">VM Type</th><th align="center">Username</th></tr></thead><tbody><tr><td>Kubernetes or Swarm on Fedora Atomic</td><td align="center">fedora</td></tr><tr><td>Kubernetes on CoreOS</td><td align="center">core</td></tr><tr><td>Mesos on Ubuntu</td><td align="center">ubuntu</td></tr></tbody></table></div></li><li class="listitem "><p>
     Useful dianostic commands
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Kubernetes cluster on Fedora Atomic
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u etcd.service
sudo journalctl -u docker.service
sudo journalctl -u kube-apiserver.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Kubernetes cluster on CoreOS
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u oem-cloudinit.service
sudo journalctl -u etcd2.service
sudo journalctl -u containerd.service
sudo journalctl -u flanneld.service
sudo journalctl -u docker.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Swarm cluster on Fedora Atomic
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u docker.service
sudo journalctl -u swarm-manager.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Mesos cluster on Ubuntu
      </p><div class="verbatim-wrap"><pre class="screen">sudo less /var/log/syslog
sudo less /var/log/cloud-init.log
sudo less /var/log/cloud-init-output.log
sudo less /var/log/os-collect-config.log
sudo less /var/log/marathon.log
sudo less /var/log/mesos-master.log</pre></div></li></ul></div></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-tools"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Tools</span> <a title="Permalink" class="permalink" href="#troubleshooting-tools">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-ts_tools.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_tools.xml</li><li><span class="ds-label">ID: </span>troubleshooting-tools</li></ul></div></div></div></div><p>
  Tools to assist with troubleshooting issues in your cloud. Additional
  troubleshooting information is available at <a class="xref" href="#general-troubleshooting" title="15.1. General Troubleshooting">Section 15.1, “General Troubleshooting”</a>.
 </p><div class="sect2" id="idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the SOS Report</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-troubleshooting_sosreport.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_sosreport.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1</li></ul></div></div></div></div><p>
  The SOS report provides debug level information about your environment to
  assist in troubleshooting issues. When troubleshooting and debugging issues
  in your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment you can run an ansible playbook that will
  provide you with a full debug report, referred to as a SOS report. These
  reports can be sent to the support team when seeking assistance.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the SOS Report</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-troubleshooting-troubleshooting_sosreport.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_sosreport.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the SOS report ansible playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts sosreport-run.yml</pre></div></li><li class="step "><p>
     Retrieve the SOS report tarballs, which will be in the following
     directories on your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">/tmp
/tmp/sosreport-report-archives/</pre></div></li><li class="step "><p>
     You can then use these reports to troubleshoot issues further or provide
     to the support team when you reach out to them.
    </p></li></ol></div></div><div id="id-1.6.17.13.3.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    The SOS Report may contain sensitive information because service
    configuration file data is included in the report. Please remove any
    sensitive information before sending the SOSReport tarball externally.
   </p></div></div></div></div></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>