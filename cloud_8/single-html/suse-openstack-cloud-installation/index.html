<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SUSE OpenStack Cloud 8 | Installing with Cloud Lifecycle Manager</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Installing with Cloud Lifecycle Manager | SUSE OpenSta…"/>
<meta name="description" content="Before beginning your installation, you should prepare thoroughly. Several resources are available to assist you."/>
<meta name="product-name" content="SUSE OpenStack Cloud"/>
<meta name="product-number" content="8"/>
<meta name="book-title" content="Installing with Cloud Lifecycle Manager"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8"/>
<meta property="og:title" content="Installing with Cloud Lifecycle Manager | SUSE OpenSta…"/>
<meta property="og:description" content="Before beginning your installation, you should prepare thoroughly. Several resources are available to assist you."/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Installing with Cloud Lifecycle Manager | SUSE OpenSta…"/>
<meta name="twitter:description" content="Before beginning your installation, you should prepare thoroughly. Several resources are available to assist you."/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/book.installation.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-installation">Installing with Cloud Lifecycle Manager</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-installation" data-id-title="Installing with Cloud Lifecycle Manager"><div class="titlepage"><div><div class="big-version-info"><span class="productname"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber"><span class="phrase"><span class="phrase">8</span></span></span></div><div class="title-container"><div class="title-container"><h1 class="title"><em class="citetitle">Installing with Cloud Lifecycle Manager</em> <a title="Permalink" class="permalink" href="#book-installation">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/book.installation.xml" title="Edit source document"> </a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/book.installation.xml" title="Edit source document"> </a></div></div><div class="date"><span class="imprint-label">Publication Date: </span>02 Nov 2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#install-overview"><span class="title-name">Installation Overview</span></a></span></li><li><span class="part"><a href="#preinstall"><span class="title-number">I </span><span class="title-name">Pre-Installation</span></a></span><ul><li><span class="chapter"><a href="#preinstall-overview"><span class="title-number">1 </span><span class="title-name">Overview</span></a></span></li><li><span class="chapter"><a href="#preinstall-checklist"><span class="title-number">2 </span><span class="title-name">Pre-Installation Checklist</span></a></span><ul><li><span class="section"><a href="#id-1.4.4.3.4"><span class="title-number">2.1 </span><span class="title-name">BIOS and IPMI Settings</span></a></span></li><li><span class="section"><a href="#id-1.4.4.3.5"><span class="title-number">2.2 </span><span class="title-name">Network Setup and Configuration</span></a></span></li><li><span class="section"><a href="#id-1.4.4.3.6"><span class="title-number">2.3 </span><span class="title-name">Cloud Lifecycle Manager</span></a></span></li><li><span class="section"><a href="#id-1.4.4.3.7"><span class="title-number">2.4 </span><span class="title-name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span></a></span></li><li><span class="section"><a href="#id-1.4.4.3.8"><span class="title-number">2.5 </span><span class="title-name">Control Plane</span></a></span></li><li><span class="section"><a href="#id-1.4.4.3.9"><span class="title-number">2.6 </span><span class="title-name">Compute Hosts</span></a></span></li><li><span class="section"><a href="#id-1.4.4.3.10"><span class="title-number">2.7 </span><span class="title-name">Storage Hosts</span></a></span></li><li><span class="section"><a href="#id-1.4.4.3.11"><span class="title-number">2.8 </span><span class="title-name">Additional Comments</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-depl-dep-inst"><span class="title-number">3 </span><span class="title-name">Installing the Cloud Lifecycle Manager server</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-adm-inst-online-update"><span class="title-number">3.1 </span><span class="title-name">Registration and Online Updates</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-os"><span class="title-number">3.2 </span><span class="title-name">Starting the Operating System Installation</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-partitioning"><span class="title-number">3.3 </span><span class="title-name">Partitioning</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-user"><span class="title-number">3.4 </span><span class="title-name">Creating a User</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-settings"><span class="title-number">3.5 </span><span class="title-name">Installation Settings</span></a></span></li></ul></li><li><span class="chapter"><a href="#app-deploy-smt-lcm"><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></span><ul><li><span class="sect1"><a href="#app-deploy-smt-install"><span class="title-number">4.1 </span><span class="title-name">SMT Installation</span></a></span></li><li><span class="sect1"><a href="#app-deploy-smt-config"><span class="title-number">4.2 </span><span class="title-name">SMT Configuration</span></a></span></li><li><span class="sect1"><a href="#app-deploy-smt-repos"><span class="title-number">4.3 </span><span class="title-name">Setting up Repository Mirroring on the SMT Server</span></a></span></li><li><span class="sect1"><a href="#app-deploy-smt-info"><span class="title-number">4.4 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-depl-repo-conf-lcm"><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-adm-conf-repos-product"><span class="title-number">5.1 </span><span class="title-name">Copying the Product Media Repositories</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-conf-repos-scc"><span class="title-number">5.2 </span><span class="title-name">Update and Pool Repositories</span></a></span></li><li><span class="sect1"><a href="#sec-deploy-repo-locations"><span class="title-number">5.3 </span><span class="title-name">Repository Locations</span></a></span></li></ul></li><li><span class="chapter"><a href="#multipath-boot-from-san"><span class="title-number">6 </span><span class="title-name">Boot from SAN and Multipath Configuration</span></a></span><ul><li><span class="section"><a href="#multipath-overview"><span class="title-number">6.1 </span><span class="title-name">Introduction</span></a></span></li><li><span class="section"><a href="#id-1.4.4.7.3"><span class="title-number">6.2 </span><span class="title-name">Install Phase Configuration</span></a></span></li><li><span class="section"><a href="#restriction2"><span class="title-number">6.3 </span><span class="title-name">QLogic FCoE restrictions and additional configurations</span></a></span></li><li><span class="section"><a href="#install-boot-from-san"><span class="title-number">6.4 </span><span class="title-name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> ISO for Nodes That Support Boot From SAN</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#cloudinstallation"><span class="title-number">II </span><span class="title-name">Cloud Installation</span></a></span><ul><li><span class="chapter"><a href="#cloudinstallation-overview"><span class="title-number">7 </span><span class="title-name">Overview</span></a></span></li><li><span class="chapter"><a href="#preparing-standalone"><span class="title-number">8 </span><span class="title-name">Preparing for Stand-Alone Deployment</span></a></span><ul><li><span class="section"><a href="#id-1.4.5.3.2"><span class="title-number">8.1 </span><span class="title-name">Cloud Lifecycle Manager Installation Alternatives</span></a></span></li><li><span class="section"><a href="#id-1.4.5.3.3"><span class="title-number">8.2 </span><span class="title-name">Installing a Stand-Alone Deployer</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-gui"><span class="title-number">9 </span><span class="title-name">Installing with the Install UI</span></a></span><ul><li><span class="section"><a href="#id-1.4.5.4.8"><span class="title-number">9.1 </span><span class="title-name">Before You Start</span></a></span></li><li><span class="section"><a href="#id-1.4.5.4.9"><span class="title-number">9.2 </span><span class="title-name">Preparing to Run the Install UI</span></a></span></li><li><span class="section"><a href="#create-csv-file"><span class="title-number">9.3 </span><span class="title-name">Optional: Creating a CSV File to Import Server Data</span></a></span></li><li><span class="section"><a href="#discover-servers"><span class="title-number">9.4 </span><span class="title-name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span></a></span></li><li><span class="section"><a href="#id-1.4.5.4.12"><span class="title-number">9.5 </span><span class="title-name">Running the Install UI</span></a></span></li></ul></li><li><span class="chapter"><a href="#using-git"><span class="title-number">10 </span><span class="title-name">Using Git for Configuration Management</span></a></span><ul><li><span class="section"><a href="#id-1.4.5.5.4"><span class="title-number">10.1 </span><span class="title-name">Initialization on a new deployment</span></a></span></li><li><span class="section"><a href="#updating-configuration-including-default-config"><span class="title-number">10.2 </span><span class="title-name">Updating any configuration, including the default configuration</span></a></span></li><li><span class="section"><a href="#git-merge"><span class="title-number">10.3 </span><span class="title-name">Resolving Git merge conflicts</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-standalone"><span class="title-number">11 </span><span class="title-name">Installing a Stand-Alone Cloud Lifecycle Manager</span></a></span><ul><li><span class="section"><a href="#id-1.4.5.6.2"><span class="title-number">11.1 </span><span class="title-name">Important Notes</span></a></span></li><li><span class="section"><a href="#id-1.4.5.6.3"><span class="title-number">11.2 </span><span class="title-name">Before You Start</span></a></span></li><li><span class="section"><a href="#id-1.4.5.6.4"><span class="title-number">11.3 </span><span class="title-name">Configuring Your Environment</span></a></span></li><li><span class="section"><a href="#id-1.4.5.6.5"><span class="title-number">11.4 </span><span class="title-name">Running the Configuration Processor</span></a></span></li><li><span class="section"><a href="#id-1.4.5.6.6"><span class="title-number">11.5 </span><span class="title-name">Configuring TLS</span></a></span></li><li><span class="section"><a href="#id-1.4.5.6.7"><span class="title-number">11.6 </span><span class="title-name">Deploying the Cloud</span></a></span></li><li><span class="section"><a href="#id-1.4.5.6.8"><span class="title-number">11.7 </span><span class="title-name">Installing <span class="productname">OpenStack</span> Assets on the Stand-alone Deployer</span></a></span></li><li><span class="section"><a href="#id-1.4.5.6.9"><span class="title-number">11.8 </span><span class="title-name">Post-Installation Verification and Administration</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-kvm"><span class="title-number">12 </span><span class="title-name">Installing Mid-scale and Entry-scale KVM</span></a></span><ul><li><span class="section"><a href="#sec-kvm-important-notes"><span class="title-number">12.1 </span><span class="title-name">Important Notes</span></a></span></li><li><span class="section"><a href="#sec-kvm-prereqs"><span class="title-number">12.2 </span><span class="title-name">Before You Start</span></a></span></li><li><span class="section"><a href="#sec-kvm-configuration"><span class="title-number">12.3 </span><span class="title-name">Configuring Your Environment</span></a></span></li><li><span class="section"><a href="#sec-kvm-provision"><span class="title-number">12.4 </span><span class="title-name">Provisioning Your Baremetal Nodes</span></a></span></li><li><span class="section"><a href="#sec-kvm-config-processor"><span class="title-number">12.5 </span><span class="title-name">Running the Configuration Processor</span></a></span></li><li><span class="section"><a href="#sec-kvm-security"><span class="title-number">12.6 </span><span class="title-name">Configuring TLS</span></a></span></li><li><span class="section"><a href="#sec-kvm-deploy"><span class="title-number">12.7 </span><span class="title-name">Deploying the Cloud</span></a></span></li><li><span class="section"><a href="#sec-kvm-configure-backend"><span class="title-number">12.8 </span><span class="title-name">Configuring a Block Storage Backend (Optional)</span></a></span></li><li><span class="section"><a href="#sec-kvm-post-installation"><span class="title-number">12.9 </span><span class="title-name">Post-Installation Verification and Administration</span></a></span></li></ul></li><li><span class="chapter"><a href="#DesignateInstallOverview"><span class="title-number">13 </span><span class="title-name">DNS Service Installation Overview</span></a></span><ul><li><span class="section"><a href="#DesignateBIND"><span class="title-number">13.1 </span><span class="title-name">Installing the DNS Service with BIND</span></a></span></li><li><span class="section"><a href="#DesignatePowerDNS"><span class="title-number">13.2 </span><span class="title-name">Install the DNS Service with PowerDNS</span></a></span></li><li><span class="section"><a href="#DNS-NS"><span class="title-number">13.3 </span><span class="title-name">Configure DNS Domain and NS Records</span></a></span></li></ul></li><li><span class="chapter"><a href="#MagnumOverview"><span class="title-number">14 </span><span class="title-name">Magnum Overview</span></a></span><ul><li><span class="section"><a href="#MagnumArchitecture"><span class="title-number">14.1 </span><span class="title-name">Magnum Architecture</span></a></span></li><li><span class="section"><a href="#MagnumInstall"><span class="title-number">14.2 </span><span class="title-name">Install the Magnum Service</span></a></span></li><li><span class="section"><a href="#MagnumIntegrateDNS"><span class="title-number">14.3 </span><span class="title-name">Integrate Magnum with the DNS Service</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-esx-ovsvapp"><span class="title-number">15 </span><span class="title-name">Installing ESX Computes and OVSvAPP</span></a></span><ul><li><span class="section"><a href="#sec-ironic-prereqs"><span class="title-number">15.1 </span><span class="title-name">Before You Start</span></a></span></li><li><span class="section"><a href="#sec-ironic-setup-deployer"><span class="title-number">15.2 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></a></span></li><li><span class="section"><a href="#esxi-overview"><span class="title-number">15.3 </span><span class="title-name">Overview of ESXi and OVSvApp</span></a></span></li><li><span class="section"><a href="#id-1.4.5.10.6"><span class="title-number">15.4 </span><span class="title-name">VM Appliances Used in OVSvApp Implementation</span></a></span></li><li><span class="section"><a href="#id-1.4.5.10.7"><span class="title-number">15.5 </span><span class="title-name">Prerequisites for Installing ESXi and Managing with vCenter</span></a></span></li><li><span class="section"><a href="#id-1.4.5.10.8"><span class="title-number">15.6 </span><span class="title-name">ESXi/vCenter System Requirements</span></a></span></li><li><span class="section"><a href="#create-esx-cluster"><span class="title-number">15.7 </span><span class="title-name">Creating an ESX Cluster</span></a></span></li><li><span class="section"><a href="#config-dvs-pg"><span class="title-number">15.8 </span><span class="title-name">Configuring the Required Distributed vSwitches and Port Groups</span></a></span></li><li><span class="section"><a href="#create-vapp-template"><span class="title-number">15.9 </span><span class="title-name">Create a SUSE-based Virtual Appliance Template in vCenter</span></a></span></li><li><span class="section"><a href="#id-1.4.5.10.12"><span class="title-number">15.10 </span><span class="title-name">ESX Network Model Requirements</span></a></span></li><li><span class="section"><a href="#create-vms-vapp-template"><span class="title-number">15.11 </span><span class="title-name">Creating and Configuring Virtual Machines Based on Virtual Appliance
 Template</span></a></span></li><li><span class="section"><a href="#collect-vcenter-credentials"><span class="title-number">15.12 </span><span class="title-name">Collect vCenter Credentials and UUID</span></a></span></li><li><span class="section"><a href="#edit-input-models"><span class="title-number">15.13 </span><span class="title-name">Edit Input Models to Add and Configure Virtual Appliances</span></a></span></li><li><span class="section"><a href="#run-config-processor"><span class="title-number">15.14 </span><span class="title-name">Running the Configuration Processor With Applied Changes</span></a></span></li><li><span class="section"><a href="#test-esx-environment"><span class="title-number">15.15 </span><span class="title-name">Test the ESX-OVSvApp Environment</span></a></span></li></ul></li><li><span class="chapter"><a href="#integrate-nsx-vsphere"><span class="title-number">16 </span><span class="title-name">Integrating NSX for vSphere</span></a></span><ul><li><span class="section"><a href="#nsx-vsphere-vm"><span class="title-number">16.1 </span><span class="title-name">Integrating with NSX for vSphere</span></a></span></li><li><span class="section"><a href="#nsx-vsphere-baremetal"><span class="title-number">16.2 </span><span class="title-name">Integrating with NSX for vSphere on Baremetal</span></a></span></li><li><span class="section"><a href="#nsx-verification"><span class="title-number">16.3 </span><span class="title-name">Verifying the NSX-v Functionality After Integration</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-ironic-overview"><span class="title-number">17 </span><span class="title-name">Installing Baremetal (Ironic)</span></a></span><ul><li><span class="section"><a href="#install-ironic"><span class="title-number">17.1 </span><span class="title-name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span></a></span></li><li><span class="section"><a href="#ironic-multi-control-plane"><span class="title-number">17.2 </span><span class="title-name">Ironic in Multiple Control Plane</span></a></span></li><li><span class="section"><a href="#ironic-provisioning"><span class="title-number">17.3 </span><span class="title-name">Provisioning Bare-Metal Nodes with Flat Network Model</span></a></span></li><li><span class="section"><a href="#ironic-provisioning-multi-tenancy"><span class="title-number">17.4 </span><span class="title-name">Provisioning Baremetal Nodes with Multi-Tenancy</span></a></span></li><li><span class="section"><a href="#ironic-system-details"><span class="title-number">17.5 </span><span class="title-name">View Ironic System Details</span></a></span></li><li><span class="section"><a href="#ironic-toubleshooting"><span class="title-number">17.6 </span><span class="title-name">Troubleshooting Ironic Installation</span></a></span></li><li><span class="section"><a href="#ironic-node-cleaning"><span class="title-number">17.7 </span><span class="title-name">Node Cleaning</span></a></span></li><li><span class="section"><a href="#ironic-oneview"><span class="title-number">17.8 </span><span class="title-name">Ironic and HPE OneView</span></a></span></li><li><span class="section"><a href="#ironic-raid-config"><span class="title-number">17.9 </span><span class="title-name">RAID Configuration for Ironic</span></a></span></li><li><span class="section"><a href="#ironic-audit-support"><span class="title-number">17.10 </span><span class="title-name">Audit Support for Ironic</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-swift"><span class="title-number">18 </span><span class="title-name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></span><ul><li><span class="section"><a href="#sec-swift-important-notes"><span class="title-number">18.1 </span><span class="title-name">Important Notes</span></a></span></li><li><span class="section"><a href="#sec-swift-prereqs"><span class="title-number">18.2 </span><span class="title-name">Before You Start</span></a></span></li><li><span class="section"><a href="#sec-swift-setup-deployer"><span class="title-number">18.3 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></a></span></li><li><span class="section"><a href="#id-1.4.5.13.6"><span class="title-number">18.4 </span><span class="title-name">Configure Your Environment</span></a></span></li><li><span class="section"><a href="#sec-swift-provision"><span class="title-number">18.5 </span><span class="title-name">Provisioning Your Baremetal Nodes</span></a></span></li><li><span class="section"><a href="#sec-swift-config-processor"><span class="title-number">18.6 </span><span class="title-name">Running the Configuration Processor</span></a></span></li><li><span class="section"><a href="#sec-swift-deploy"><span class="title-number">18.7 </span><span class="title-name">Deploying the Cloud</span></a></span></li><li><span class="section"><a href="#sec-swift-post-installation"><span class="title-number">18.8 </span><span class="title-name">Post-Installation Verification and Administration</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-sles-compute"><span class="title-number">19 </span><span class="title-name">Installing SLES Compute</span></a></span><ul><li><span class="section"><a href="#sles-overview"><span class="title-number">19.1 </span><span class="title-name">SLES Compute Node Installation Overview</span></a></span></li><li><span class="section"><a href="#sles-support"><span class="title-number">19.2 </span><span class="title-name">SLES Support</span></a></span></li><li><span class="section"><a href="#install-sles"><span class="title-number">19.3 </span><span class="title-name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span></a></span></li><li><span class="section"><a href="#provisioning-sles"><span class="title-number">19.4 </span><span class="title-name">Provisioning SLES Yourself</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-ardana-manila"><span class="title-number">20 </span><span class="title-name">Installing Manila and Creating Manila Shares</span></a></span><ul><li><span class="section"><a href="#id-1.4.5.15.2"><span class="title-number">20.1 </span><span class="title-name">Installing Manila</span></a></span></li><li><span class="section"><a href="#id-1.4.5.15.3"><span class="title-number">20.2 </span><span class="title-name">Adding Manila to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span></a></span></li><li><span class="section"><a href="#configure-manila-backend"><span class="title-number">20.3 </span><span class="title-name">Configure Manila Backend</span></a></span></li><li><span class="section"><a href="#id-1.4.5.15.5"><span class="title-number">20.4 </span><span class="title-name">Creating Manila Shares</span></a></span></li><li><span class="section"><a href="#id-1.4.5.15.6"><span class="title-number">20.5 </span><span class="title-name">Troubleshooting</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-heat-templates"><span class="title-number">21 </span><span class="title-name">Installing SUSE CaaS Platform Heat Templates</span></a></span><ul><li><span class="section"><a href="#sec-heat-templates-install"><span class="title-number">21.1 </span><span class="title-name">SUSE CaaS Platform Heat Installation Procedure</span></a></span></li><li><span class="section"><a href="#id-1.4.5.16.4"><span class="title-number">21.2 </span><span class="title-name">Installing SUSE CaaS Platform with Multiple Masters</span></a></span></li><li><span class="section"><a href="#id-1.4.5.16.5"><span class="title-number">21.3 </span><span class="title-name">Deploy SUSE CaaS Platform Stack Using Heat SUSE CaaS Platform Playbook</span></a></span></li><li><span class="section"><a href="#id-1.4.5.16.6"><span class="title-number">21.4 </span><span class="title-name">Deploy SUSE CaaS Platform Cluster with Multiple Masters Using Heat CaasP
  Playbook</span></a></span></li><li><span class="section"><a href="#id-1.4.5.16.7"><span class="title-number">21.5 </span><span class="title-name">SUSE CaaS Platform <span class="productname">OpenStack</span> Image for Heat SUSE CaaS Platform Playbook</span></a></span></li><li><span class="section"><a href="#id-1.4.5.16.8"><span class="title-number">21.6 </span><span class="title-name">Enabling the Cloud Provider Integration (CPI) Feature</span></a></span></li><li><span class="section"><a href="#id-1.4.5.16.9"><span class="title-number">21.7 </span><span class="title-name">More Information about SUSE CaaS Platform</span></a></span></li></ul></li><li><span class="chapter"><a href="#integrations"><span class="title-number">22 </span><span class="title-name">Integrations</span></a></span><ul><li><span class="section"><a href="#config-3par"><span class="title-number">22.1 </span><span class="title-name">Configuring for 3PAR Block Storage Backend</span></a></span></li><li><span class="section"><a href="#ironic-oneview-integration"><span class="title-number">22.2 </span><span class="title-name">Ironic HPE OneView Integration</span></a></span></li><li><span class="section"><a href="#ses-integration"><span class="title-number">22.3 </span><span class="title-name">SUSE Enterprise Storage Integration</span></a></span></li></ul></li><li><span class="chapter"><a href="#troubleshooting-installation"><span class="title-number">23 </span><span class="title-name">Troubleshooting the Installation</span></a></span><ul><li><span class="section"><a href="#sec-trouble-deployer-setup"><span class="title-number">23.1 </span><span class="title-name">Issues during Cloud Lifecycle Manager Setup</span></a></span></li><li><span class="section"><a href="#sec-trouble-config-processor"><span class="title-number">23.2 </span><span class="title-name">Issues while Updating Configuration Files</span></a></span></li><li><span class="section"><a href="#sec-trouble-deploy-cloud"><span class="title-number">23.3 </span><span class="title-name">Issues while Deploying the Cloud</span></a></span></li></ul></li><li><span class="chapter"><a href="#esx-troubleshooting-installation"><span class="title-number">24 </span><span class="title-name">Troubleshooting the ESX</span></a></span><ul><li><span class="section"><a href="#id-1.4.5.19.3"><span class="title-number">24.1 </span><span class="title-name">Issue: ardana-service.service is not running</span></a></span></li><li><span class="section"><a href="#id-1.4.5.19.4"><span class="title-number">24.2 </span><span class="title-name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span></a></span></li><li><span class="section"><a href="#id-1.4.5.19.5"><span class="title-number">24.3 </span><span class="title-name">Issue: Unable to view the VM console in Horizon UI</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#post-install"><span class="title-number">III </span><span class="title-name">Post-Installation</span></a></span><ul><li><span class="chapter"><a href="#post-install-overview"><span class="title-number">25 </span><span class="title-name">Overview</span></a></span></li><li><span class="chapter"><a href="#cloud-verification"><span class="title-number">26 </span><span class="title-name">Cloud Verification</span></a></span><ul><li><span class="section"><a href="#api-verification"><span class="title-number">26.1 </span><span class="title-name">API Verification</span></a></span></li></ul></li><li><span class="chapter"><a href="#ui-verification"><span class="title-number">27 </span><span class="title-name">UI Verification</span></a></span><ul><li><span class="section"><a href="#sec-verify-block-storage-volume"><span class="title-number">27.1 </span><span class="title-name">Verifying Your Block Storage Backend</span></a></span></li><li><span class="section"><a href="#sec-verify-block-storage-swift"><span class="title-number">27.2 </span><span class="title-name">Verify the Object Storage (Swift) Operations</span></a></span></li><li><span class="section"><a href="#upload-image"><span class="title-number">27.3 </span><span class="title-name">Uploading an Image for Use</span></a></span></li><li><span class="section"><a href="#create-extnet"><span class="title-number">27.4 </span><span class="title-name">Creating an External Network</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-openstack-clients"><span class="title-number">28 </span><span class="title-name">Installing OpenStack Clients</span></a></span></li><li><span class="chapter"><a href="#tls30"><span class="title-number">29 </span><span class="title-name">Configuring Transport Layer Security (TLS)</span></a></span><ul><li><span class="section"><a href="#id-1.4.6.6.11"><span class="title-number">29.1 </span><span class="title-name">Configuring TLS in the input model</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.12"><span class="title-number">29.2 </span><span class="title-name">User-provided certificates and trust chains</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.13"><span class="title-number">29.3 </span><span class="title-name">Edit the input model to include your certificate files</span></a></span></li><li><span class="section"><a href="#sec-generate-certificate"><span class="title-number">29.4 </span><span class="title-name">Generate a self-signed CA</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.15"><span class="title-number">29.5 </span><span class="title-name">Generate a certificate signing request</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.16"><span class="title-number">29.6 </span><span class="title-name">Generate a server certificate</span></a></span></li><li><span class="section"><a href="#sec-upload-toclm"><span class="title-number">29.7 </span><span class="title-name">Upload to the Cloud Lifecycle Manager</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.18"><span class="title-number">29.8 </span><span class="title-name">Configuring the cipher suite</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.19"><span class="title-number">29.9 </span><span class="title-name">Testing</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.20"><span class="title-number">29.10 </span><span class="title-name">Verifying that the trust chain is correctly deployed</span></a></span></li><li><span class="section"><a href="#id-1.4.6.6.21"><span class="title-number">29.11 </span><span class="title-name">Turning TLS on or off</span></a></span></li></ul></li><li><span class="chapter"><a href="#config-availability-zones"><span class="title-number">30 </span><span class="title-name">Configuring Availability Zones</span></a></span></li><li><span class="chapter"><a href="#OctaviaInstall"><span class="title-number">31 </span><span class="title-name">Configuring Load Balancer as a Service</span></a></span><ul><li><span class="section"><a href="#id-1.4.6.8.10"><span class="title-number">31.1 </span><span class="title-name">Prerequisites</span></a></span></li><li><span class="section"><a href="#id-1.4.6.8.11"><span class="title-number">31.2 </span><span class="title-name">Octavia Load Balancing Provider</span></a></span></li><li><span class="section"><a href="#id-1.4.6.8.12"><span class="title-number">31.3 </span><span class="title-name">Setup of prerequisites</span></a></span></li><li><span class="section"><a href="#id-1.4.6.8.13"><span class="title-number">31.4 </span><span class="title-name">Create Load Balancers</span></a></span></li><li><span class="section"><a href="#id-1.4.6.8.14"><span class="title-number">31.5 </span><span class="title-name">Create Floating IPs for Load Balancer</span></a></span></li><li><span class="section"><a href="#id-1.4.6.8.15"><span class="title-number">31.6 </span><span class="title-name">Testing the Octavia Load Balancer</span></a></span></li></ul></li><li><span class="chapter"><a href="#postinstall-checklist"><span class="title-number">32 </span><span class="title-name">Other Common Post-Installation Tasks</span></a></span><ul><li><span class="section"><a href="#id-1.4.6.9.2"><span class="title-number">32.1 </span><span class="title-name">Determining Your User Credentials</span></a></span></li><li><span class="section"><a href="#id-1.4.6.9.3"><span class="title-number">32.2 </span><span class="title-name">Configure your Cloud Lifecycle Manager to use the command-line tools</span></a></span></li><li><span class="section"><a href="#id-1.4.6.9.4"><span class="title-number">32.3 </span><span class="title-name">Protect home directory</span></a></span></li><li><span class="section"><a href="#id-1.4.6.9.5"><span class="title-number">32.4 </span><span class="title-name">Back up Your SSH Keys</span></a></span></li><li><span class="section"><a href="#id-1.4.6.9.6"><span class="title-number">32.5 </span><span class="title-name">Retrieving Service Endpoints</span></a></span></li><li><span class="section"><a href="#id-1.4.6.9.7"><span class="title-number">32.6 </span><span class="title-name">Other Common Post-Installation Tasks</span></a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#cha-inst-trouble"><span class="title-number">33 </span><span class="title-name">Support</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-trouble-faq"><span class="title-number">33.1 </span><span class="title-name">FAQ</span></a></span></li><li><span class="sect1"><a href="#sec-installation-trouble-support"><span class="title-number">33.2 </span><span class="title-name">Support</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#magnum-service-arch-diagram"><span class="number">14.1 </span><span class="name">Service Architecture Diagram for Kubernetes</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.12.4.3.8"><span class="number">17.1 </span><span class="name">Architecture of Multiple Control Plane with Ironic</span></a></span></li></ul></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><ul><li><span class="table"><a href="#cp-1"><span class="number">2.1 </span><span class="name">Control Plane 1</span></a></span></li><li><span class="table"><a href="#cp-2"><span class="number">2.2 </span><span class="name">Control Plane 2</span></a></span></li><li><span class="table"><a href="#cp-3"><span class="number">2.3 </span><span class="name">Control Plane 3</span></a></span></li><li><span class="table"><a href="#id-1.4.4.6.5.4"><span class="number">5.1 </span><span class="name">Local Product Repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></a></span></li><li><span class="table"><a href="#tab-smt-repos-local"><span class="number">5.2 </span><span class="name">SMT Repositories Hosted on the Administration Server</span></a></span></li><li><span class="table"><a href="#tab-depl-adm-conf-local-repos"><span class="number">5.3 </span><span class="name">Repository Locations on the Cloud Lifecycle Manager server</span></a></span></li><li><span class="table"><a href="#DNSBackendTable"><span class="number">13.1 </span><span class="name">DNS Backends</span></a></span></li><li><span class="table"><a href="#table-ebc-x5v-jz"><span class="number">14.1 </span><span class="name">Data</span></a></span></li><li><span class="table"><a href="#table-fst-gxv-jz"><span class="number">14.2 </span><span class="name">Interfaces</span></a></span></li><li><span class="table"><a href="#security-groups-table"><span class="number">14.3 </span><span class="name">Security Groups</span></a></span></li><li><span class="table"><a href="#network-ports-table"><span class="number">14.4 </span><span class="name">Network Ports</span></a></span></li><li><span class="table"><a href="#nsx-hw-reqs-vm"><span class="number">16.1 </span><span class="name">NSX Hardware Requirements for Virtual Machine Integration</span></a></span></li><li><span class="table"><a href="#nsx-hw-reqs-bm"><span class="number">16.2 </span><span class="name">NSX Hardware Requirements for Baremetal Integration</span></a></span></li><li><span class="table"><a href="#nsx-interface-reqs"><span class="number">16.3 </span><span class="name">NSX Interface Requirements</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#sec-tls-private-metadata"><span class="number">29.1 </span><span class="name">Certificate request file</span></a></span></li></ul></div><div><div class="legalnotice" id="id-1.4.2.1"><p>
  Copyright © 2006–
2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under
  <span class="bold"><strong>Creative Commons Attribution 3.0 License
  </strong></span>:
  <a class="link" href="http://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">


  </a>
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be held
  liable for possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="install-overview" data-id-title="Installation Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">Installation Overview</span></span> <a title="Permalink" class="permalink" href="#install-overview">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installation_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Before beginning your installation, you should prepare thoroughly. Several
  resources are available to assist you.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Work through the information in the <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”</span>. <em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em> covers prerequisites,
    considerations, and choices that are necessary to install your cloud
    successfully.
   </p></li><li class="listitem"><p>
    Study the example configurations in the table below offered by
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. These example configurations are a key aspect of
    installing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with the least amount of time and effort.
   </p></li><li class="listitem"><p>
    Familiarize yourself with Git. Git is a version control system for tracking
    and managing changes to your cloud configuration, even with installation
    methods (such as the Install UI) that do not require direct interaction
    with version control. For more information about Git, see <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>.
   </p></li><li class="listitem"><p>
    Additional information that will help with your cloud installation.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#troubleshooting-installation" title="Chapter 23. Troubleshooting the Installation">Chapter 23, <em>Troubleshooting the Installation</em></a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#cloud-verification" title="Chapter 26. Cloud Verification">Chapter 26, <em>Cloud Verification</em></a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#postinstall-checklist" title="Chapter 32. Other Common Post-Installation Tasks">Chapter 32, <em>Other Common Post-Installation Tasks</em></a>
     </p></li></ul></div></li></ul></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Name</th><th style="border-bottom: 1px solid ; ">Location</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.1 “Entry-Scale Cloud”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.2 “Entry Scale Cloud with Metering and Monitoring Services”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.4 “ESX Examples”, Section 9.4.1 “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</span>
     </td><td style="border-bottom: 1px solid ; "><code class="filename">~/openstack/examples/entry-scale-esx-kvm</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.4 “ESX Examples”, Section 9.4.2 “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-esx-kvm-mml</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.5 “Swift Examples”, Section 9.5.1 “Entry-scale Swift Model”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.6 “Ironic Examples”, Section 9.6.1 “Entry-Scale Cloud with Ironic Flat Network”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.6 “Ironic Examples”, Section 9.6.2 “Entry-Scale Cloud with Ironic Multi-Tenancy”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td style="border-right: 1px solid ; "><span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.3 “Single-Region Mid-Size Model”</span>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div></section><div class="part" id="preinstall" data-id-title="Pre-Installation"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part I </span><span class="title-name">Pre-Installation </span></span><a title="Permalink" class="permalink" href="#preinstall">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_overview.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#preinstall-overview"><span class="title-number">1 </span><span class="title-name">Overview</span></a></span></li><dd class="toc-abstract"><p>
   To ensure that your environment meets the requirements of the cloud model you
   choose, see the check list in <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a>.
  </p></dd><li><span class="chapter"><a href="#preinstall-checklist"><span class="title-number">2 </span><span class="title-name">Pre-Installation Checklist</span></a></span></li><dd class="toc-abstract"><p>
   The formatting of this page facilitates printing it out and using it to
   record details of your setup.
  </p></dd><li><span class="chapter"><a href="#cha-depl-dep-inst"><span class="title-number">3 </span><span class="title-name">Installing the Cloud Lifecycle Manager server</span></a></span></li><dd class="toc-abstract"><p>
    This chapter will show how to install the Cloud Lifecycle Manager from scratch. It will run
    on SUSE Linux Enterprise Server 12 SP3, include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> extension, and, optionally, the
    Subscription Management Tool (SMT) server.
   </p></dd><li><span class="chapter"><a href="#app-deploy-smt-lcm"><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></span></li><dd class="toc-abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is to install a Subscription Management Tool (SMT) server on the Cloud Lifecycle Manager server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Cloud Lifecycle Manager server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Cloud Lifecycle Manager server, skip this step.
   </p></dd><li><span class="chapter"><a href="#cha-depl-repo-conf-lcm"><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></a></span></li><dd class="toc-abstract"><p>Software repositories containing products, extensions, and the respective updates for all software need to be available to all nodes in SUSE OpenStack Cloud in order to complete the deployment. These can be managed manually, or they can be hosted on the Cloud Lifecycle Manager server. In this config…</p></dd><li><span class="chapter"><a href="#multipath-boot-from-san"><span class="title-number">6 </span><span class="title-name">Boot from SAN and Multipath Configuration</span></a></span></li><dd class="toc-abstract"><p>
   For information about supported hardware for multipathing, see
   <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”, Section 2.2 “Supported Hardware Configurations”</span>.
  </p></dd></ul></div><section class="chapter" id="preinstall-overview" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#preinstall-overview">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To ensure that your environment meets the requirements of the cloud model you
   choose, see the check list in <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a>.
  </p><p>
   After you have decided on a configuration to choose for your cloud
   and you have gone through the pre-installation steps, you will have two options
   for installation:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     You can use a graphical user interface (GUI) that runs in your Web browser.
    </p></li><li class="listitem"><p>
     You can install via the command line that gives you the flexibility and
     full control of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
    </p></li></ul></div><p>
   <span class="bold"><strong>Using the GUI</strong></span>
  </p><p>
   You should use the GUI if:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     You are not planning to deploy availability zones or use L3 segmentation
     in your initial deployment.
    </p></li><li class="listitem"><p>
     You are satisfied with the tuned <span class="phrase"><span class="phrase">SUSE</span></span>-default <span class="productname">OpenStack</span> configuration.
    </p></li></ul></div><p>
   Instructions for GUI installation are in <a class="xref" href="#install-gui" title="Chapter 9. Installing with the Install UI">Chapter 9, <em>Installing with the Install UI</em></a>.
  </p><div id="id-1.4.4.2.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Reconfiguring your cloud can only be done via the command line. The GUI
    installer is for initial installation only.
   </p></div><p>
   <span class="bold"><strong>Using the Command Line</strong></span>
  </p><p>
   You should use the command line if:
  </p><div class="itemizedlist" id="idg-installation-installation-installation-overview-xml-7"><ul class="itemizedlist"><li class="listitem"><p>
     You are installing a complex or large-scale cloud.
    </p></li><li class="listitem"><p>
     You need to use availability zones or the server groups functionality of
     the cloud model. For more information, see the <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”</span>.
    </p></li><li class="listitem"><p>
     You want to customize the cloud configuration beyond the tuned defaults
     that <span class="phrase"><span class="phrase">SUSE</span></span> provides out of the box.
    </p></li><li class="listitem"><p>
     You need more extensive customizations than are possible using the GUI.
    </p></li></ul></div><p>
   Instructions for installing via the command line are in <a class="xref" href="#install-kvm" title="Chapter 12. Installing Mid-scale and Entry-scale KVM">Chapter 12, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p><div id="id-1.4.4.2.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Ardana is an open-source project and a generalized lifecycle management
    framework.  Cloud Lifecycle Manager is based on Ardana, and delivers the lifecycle management
    functionality required by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. Due to this
    relationship, some Cloud Lifecycle Manager commands refer to Ardana.
   </p></div></section><section class="chapter" id="preinstall-checklist" data-id-title="Pre-Installation Checklist"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Pre-Installation Checklist</span></span> <a title="Permalink" class="permalink" href="#preinstall-checklist">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.4.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   The formatting of this page facilitates printing it out and using it to
   record details of your setup.
  </p></div><p>
  This checklist is focused on the Entry-scale KVM model but you can alter it
  to fit the example configuration you choose for your cloud.
 </p><section class="sect1" id="id-1.4.4.3.4" data-id-title="BIOS and IPMI Settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">BIOS and IPMI Settings</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Ensure that the following BIOS and IPMI settings are applied to each
   bare-metal server:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-bottom: 1px solid ; ">Item</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       
       
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       Choose either UEFI or Legacy BIOS in the BIOS settings
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       <p>
        Verify the Date and Time settings in the BIOS.
       </p>
       <div id="id-1.4.4.3.4.3.1.4.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installs and runs with UTC, not local time.
        </p></div>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">Ensure that Wake-on-LAN is disabled in the BIOS</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       Ensure that the NIC port to be used for PXE installation has PXE
       enabled in the BIOS
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">Ensure that all other NIC ports have PXE disabled in the BIOS</td></tr><tr><td style="border-right: 1px solid ; "> </td><td>
       Ensure all hardware in the server not directly used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is
       disabled
      </td></tr></tbody></table></div></section><section class="sect1" id="id-1.4.4.3.5" data-id-title="Network Setup and Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Network Setup and Configuration</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before installing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the following networks must be provisioned and
   tested. The networks are not installed or managed by the Cloud. You must
   install and manage the networks as documented in
   <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>.
  </p><p>
   Note that if you want a pluggable IPAM driver, it must be specified at
   install time. Only with a clean install of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> can you specify a
   different IPAM driver. If upgrading, you must use the default driver.
   More information can be found in
   <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.3 “Networking Service Overview”, Section 9.3.7 “Using IPAM Drivers in the Networking Service”</span>.
  </p><p>
   Use these checklists to confirm and record your network configuration
   information.
  </p><p>
   <span class="bold"><strong>Router</strong></span>
  </p><p>
   The IP router used with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must support the updated of its ARP table
   through gratuitous ARP packets.
  </p><p>
   <span class="bold"><strong>PXE Installation Network</strong></span>
  </p><p>
   When provisioning the IP range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Instance</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Description</th><th style="border-bottom: 1px solid ; ">IPs</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Deployer O/S</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">1</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Controller server O/S (x3)</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">3</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Compute servers (2nd thru 100th)</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">single IP per server</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; ">block storage host servers</td><td style="border-right: 1px solid ; ">single IP per server</td><td> </td></tr></tbody></table></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Network is untagged</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">No DHCP servers other than <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are on the network</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Switch PVID used to map any "internal" VLANs to untagged</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Routable to the IPMI network</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP CIDR</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP Range (Usable IPs)</td><td style="border-bottom: 1px solid ; ">
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">Default IP Gateway</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>Management Network</strong></span>
  </p><p>
   The management network is the backbone used for the majority of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   management communications. Control messages are exchanged between the
   Controllers, Compute hosts, and Cinder backends through this
   network. In addition to the control flows, the management network is also
   used to transport Swift and iSCSI based Cinder block storage
   traffic between servers.
  </p><p>
   When provisioning the IP Range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Instance</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Description</th><th style="border-bottom: 1px solid ; ">IPs</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Controller server O/S (x3)</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">3</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Controller VIP</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">1</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Compute servers (2nd through 100th)</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">single IP per server</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">VM servers</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">single IP per server</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; ">VIP per cluster</td><td style="border-right: 1px solid ; "> </td><td> </td></tr></tbody></table></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Network is untagged</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">No DHCP servers other than <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are on the network</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Switch PVID used to map any "internal" VLANs to untagged</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP CIDR</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP Range (Usable IPs)</td><td style="border-bottom: 1px solid ; ">
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Default IP Gateway</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">VLAN ID</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>IPMI Network</strong></span>
  </p><p>
   The IPMI network is used to connect the IPMI interfaces on the servers that
   are assigned for use with implementing the cloud.  This network is used by
   Cobbler to control the state of the servers
   during baremetal deployments.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Network is untagged</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Routable to the Management Network</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP Subnet</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">Default IP Gateway</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>External API Network</strong></span>
  </p><p>
   The External network is used to connect <span class="productname">OpenStack</span> endpoints to an external
   public network such as a company’s intranet or the public internet in the
   case of a public cloud provider.
  </p><p>
   When provisioning the IP Range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Instance</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Description</th><th style="border-bottom: 1px solid ; ">IPs</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Controller server O/S (x3)</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">3</td></tr><tr><td style="border-right: 1px solid ; ">Controller VIP</td><td style="border-right: 1px solid ; "> </td><td>1</td></tr></tbody></table></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">VLAN Tag assigned:</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP CIDR</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP Range (Usable IPs)</td><td style="border-bottom: 1px solid ; ">
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Default IP Gateway</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">VLAN ID</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>External VM Network</strong></span>
  </p><p>
   The External VM network is used to connect cloud instances to an external
   public network such as a company’s intranet or the public internet in the
   case of a public cloud provider. The external network has a predefined range
   of Floating IPs which are assigned to individual instances to enable
   communications to and from the instance to the assigned corporate
   intranet/internet. There should be a route between the External VM and
   External API networks so that instances provisioned in the cloud, may access
   the Cloud API endpoints, using the instance floating IPs.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">VLAN Tag assigned:</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP CIDR</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP Range (Usable IPs)</td><td style="border-bottom: 1px solid ; ">
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Default IP Gateway</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">VLAN ID</td><td> </td></tr></tbody></table></div></section><section class="sect1" id="id-1.4.4.3.6" data-id-title="Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.3 </span><span class="title-name">Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This server contains the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer, which is based on Git, Ansible,
   and Cobbler.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       Disk Requirement: Single 8GB disk needed per the
       <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span>
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <a class="xref" href="#sec-depl-adm-inst-add-on" title="3.5.2. Installing the SUSE OpenStack Cloud Extension">Section 3.5.2, “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</a>
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       Ensure your local DNS nameserver is placed into your
       <code class="filename">/etc/resolv.conf</code> file
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Install and configure NTP for your environment</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       Ensure your NTP server(s) is placed into your
       <code class="filename">/etc/ntp.conf</code> file
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">NTP time source:</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td> </td></tr></tbody></table></div></section><section class="sect1" id="id-1.4.4.3.7" data-id-title="Information for the nic_mappings.yml Input File"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.4 </span><span class="title-name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Log on to each type of physical server you have and issue platform-appropriate
   commands to identify the <code class="literal">bus-address</code> and
   <code class="literal">port-num</code> values that may be required. For example, run the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen">sudo lspci -D | grep -i net</pre></div><p>
   and enter this information in the space below. Use this information for the
   <code class="literal">bus-address</code> value in your
   <code class="literal">nic_mappings.yml</code> file.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/></colgroup><thead><tr><th style="border-bottom: 1px solid ; ">NIC Adapter PCI Bus Address Output</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">











</pre></div>
      </td></tr></tbody></table></div><p>
   To find the <code class="literal">port-num</code> use:
  </p><div class="verbatim-wrap"><pre class="screen">cat /sys/class/net/&lt;device name&gt;/dev_port</pre></div><p>
   where the 'device-name' is the name of the device <span class="bold"><strong>currently mapped</strong></span> to this address, not necessarily the
   name of the device <span class="bold"><strong>to be mapped</strong></span>. Enter the
   information for your system in the space below.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/></colgroup><thead><tr><th style="border-bottom: 1px solid ; ">Network Device Port Number Output</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">











</pre></div>
      </td></tr></tbody></table></div></section><section class="sect1" id="id-1.4.4.3.8" data-id-title="Control Plane"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.5 </span><span class="title-name">Control Plane</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Control Plane consists of at least three servers in a highly available
   cluster that host the core <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services including Nova, Keystone,
   Glance, Cinder, Heat, Neutron, Swift, Ceilometer, and
   Horizon. Additional services include mariadb, ip-cluster, apache2,
   rabbitmq, memcached, zookeeper, kafka, storm, monasca, logging, and cmc.
  </p><div id="id-1.4.4.3.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    To mitigate the <span class="quote">“<span class="quote">split-brain</span>”</span> situation described in
    <span class="intraxref">Book “Operations Guide”, Chapter 15 “Troubleshooting Issues”, Section 15.4 “Network Service Troubleshooting”</span> it is recommended
    that you have HA network configuration with Multi-Chassis Link Aggregation
    (MLAG) and NIC bonding configured for all the controllers to deliver
    system-level redundancy as well network-level resiliency. Also reducing
    the ARP timeout on the TOR switches will help.
   </p></div><div class="table" id="cp-1" data-id-title="Control Plane 1"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.1: </span><span class="title-name">Control Plane 1 </span></span><a title="Permalink" class="permalink" href="#cp-1">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Disk Requirement: 3x 512 GB disks (or enough space to create three
logical drives with that amount of space)
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Ensure the disks are wiped</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">MAC address of first NIC</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">A second NIC, or a set of bonded NICs are required</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI IP address</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">IPMI Username/Password</td><td> </td></tr></tbody></table></div></div><div class="table" id="cp-2" data-id-title="Control Plane 2"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.2: </span><span class="title-name">Control Plane 2 </span></span><a title="Permalink" class="permalink" href="#cp-2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       Disk Requirement: 3x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Ensure the disks are wiped</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">MAC address of first NIC</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">A second NIC, or a set of bonded NICs are required</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI IP address</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">IPMI Username/Password</td><td> </td></tr></tbody></table></div></div><div class="table" id="cp-3" data-id-title="Control Plane 3"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.3: </span><span class="title-name">Control Plane 3 </span></span><a title="Permalink" class="permalink" href="#cp-3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Item</th><th style="border-bottom: 1px solid ; ">Value</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       Disk Requirement: 3x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Ensure the disks are wiped</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">MAC address of first NIC</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">A second NIC, or a set of bonded NICs are required</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI IP address</td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; ">IPMI Username/Password</td><td> </td></tr></tbody></table></div></div></section><section class="sect1" id="id-1.4.4.3.9" data-id-title="Compute Hosts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.6 </span><span class="title-name">Compute Hosts</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.9">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   One or more KVM Compute servers will be used as the compute host targets for
   instances.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-bottom: 1px solid ; ">Item</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       Disk Requirement: 2x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       A NIC for PXE boot and a second NIC, or a NIC for PXE and a set of
       bonded NICs are required
      </td></tr><tr><td style="border-right: 1px solid ; "> </td><td>Ensure the disks are wiped</td></tr></tbody></table></div><p>
   Table to record your Compute host details:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/><col class="c4"/><col class="c5"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">ID</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">NIC MAC Address</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI Username/Password</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">IMPI IP Address</th><th style="border-bottom: 1px solid ; ">CPU/Mem/Disk</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td> </td></tr></tbody></table></div></section><section class="sect1" id="id-1.4.4.3.10" data-id-title="Storage Hosts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.7 </span><span class="title-name">Storage Hosts</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.10">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Three or more servers with local disk volumes to provide Cinder
   block storage resources.
  </p><div id="id-1.4.4.3.10.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The cluster created from block storage nodes must allow for quorum. In
    other words, the node count of the cluster must be 3, 5, 7, or another odd
    number.
   </p></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">☐</th><th style="border-bottom: 1px solid ; ">Item</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       <p>
        Disk Requirement: 3x 512 GB disks (or enough space to create three
        logical drives with that amount of space)
       </p>
       <p>
        The block storage appliance deployed on a host is expected to consume
        ~40 GB of disk space from the host root disk for ephemeral storage to
        run the block storage virtual machine.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; ">
       A NIC for PXE boot and a second NIC, or a NIC for PXE and a set of
       bonded NICs are required
      </td></tr><tr><td style="border-right: 1px solid ; "> </td><td>Ensure the disks are wiped</td></tr></tbody></table></div><p>
   Table to record your block storage host details:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/><col class="c4"/><col class="c5"/><col class="c6"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">ID</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">NIC MAC Address</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI Username/Password</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI IP Address</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">CPU/Mem/Disk</th><th style="border-bottom: 1px solid ; ">Data Volume</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td> </td></tr></tbody></table></div></section><section class="sect1" id="id-1.4.4.3.11" data-id-title="Additional Comments"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.8 </span><span class="title-name">Additional Comments</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.11">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-preinstall_checklist.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section is for any additional information that you deem necessary.
  </p><div class="verbatim-wrap"><pre class="screen">













</pre></div></section></section><section class="chapter" id="cha-depl-dep-inst" data-id-title="Installing the Cloud Lifecycle Manager server"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Installing the Cloud Lifecycle Manager server</span></span> <a title="Permalink" class="permalink" href="#cha-depl-dep-inst">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This chapter will show how to install the Cloud Lifecycle Manager from scratch. It will run
    on SUSE Linux Enterprise Server 12 SP3, include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> extension, and, optionally, the
    Subscription Management Tool (SMT) server.
   </p></div></div></div></div><section class="sect1" id="sec-depl-adm-inst-online-update" data-id-title="Registration and Online Updates"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Registration and Online Updates</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-online-update">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Registering SUSE Linux Enterprise Server 12 SP3 during the installation process is required for
   getting product updates and for installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   extension. Refer to <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-conf-manual-cc" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-conf-manual-cc</a>
   for further instructions.
  </p><p>
   After a successful registration you will be asked whether
   to add the update repositories. If you agree, the latest updates will
   automatically be installed, ensuring that your system is on the latest
   patch level after the initial installation. We strongly recommend adding the
   update repositories immediately. If you choose to skip this step you need to
   perform an online update later, before starting the installation.
  </p><div id="id-1.4.4.4.2.4" data-id-title="SUSE Login Required" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: SUSE Login Required</div><p>
    To register a product, you need to have a SUSE login.
    If you do not have such a login, create it at
    <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>.
   </p></div></section><section class="sect1" id="sec-depl-adm-inst-os" data-id-title="Starting the Operating System Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">Starting the Operating System Installation</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-os">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.4.4.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Installing SUSE Linux Enterprise Server for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> requires the following steps, which are
    different from the default SUSE Linux Enterprise Server installation process.
   </p></div><div id="id-1.4.4.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    For an overview of a default SUSE Linux Enterprise Server installation, refer to <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-installquick/#art-sle-installquick" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-installquick/#art-sle-installquick</a>.
   </p></div><p>
   Start the installation by booting into the SUSE Linux Enterprise Server 12 SP3 installation system.
  </p></section><section class="sect1" id="sec-depl-adm-inst-partitioning" data-id-title="Partitioning"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.3 </span><span class="title-name">Partitioning</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-partitioning">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Create a custom partition setup using the <span class="guimenu">Expert
   Partitioner</span>. The following setup is required:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Two partitions are needed: one for boot, EFI or UEFI, and one for
     everything else.
    </p></li><li class="listitem"><p>
     If the system is using a UEFI BIOS, there must be a UEFI boot
     partition.
    </p></li><li class="listitem"><p>
     An LVM setup with no encryption is recommended, Btrfs will work. The file
     system must contain:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       a volume group named <code class="literal">ardana-vg</code> on the first disk
       (<code class="systemitem">/dev/sda</code>)
      </p></li><li class="listitem"><p>
       a volume named <code class="literal">root</code> with a size of 50GB and an ext4 filesystem
      </p></li></ul></div></li><li class="listitem"><p>
     no separate mount point for <code class="filename">/home</code>
    </p></li><li class="listitem"><p>
     no swap partition or file (No swap is a general <span class="productname">OpenStack</span>
     recommendation. Some services such as rabbit and cassandra do not perform
     well with swapping.)
    </p></li></ul></div></section><section class="sect1" id="sec-depl-adm-inst-user" data-id-title="Creating a User"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.4 </span><span class="title-name">Creating a User</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-user">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Setting up Cloud Lifecycle Manager requires a regular user which you can set up during the
   installation. You are free to choose any available user name except for
   <code class="systemitem">ardana</code>, because the <code class="systemitem">ardana</code> user is reserved by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></section><section class="sect1" id="sec-depl-adm-inst-settings" data-id-title="Installation Settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.5 </span><span class="title-name">Installation Settings</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   With <span class="guimenu">Installation Settings</span>, you need to adjust the
   software selection for your Cloud Lifecycle Manager setup. For more information refer to the
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-proposal" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-i-yast2-proposal</a>.
  </p><div id="id-1.4.4.4.6.3" data-id-title="Additional Installation Settings" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Additional Installation Settings</div><p>
    The default firewall must be disabled, as <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> enables its own
    firewall during deployment.
   </p><p>
    SSH must be enabled.
   </p><p>
    Set <code class="literal">text</code> as the <span class="guimenu">Default systemd
    target</span>.
   </p></div><section class="sect2" id="sec-depl-adm-inst-settings-software" data-id-title="Software Selection"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.5.1 </span><span class="title-name">Software Selection</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings-software">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Installing a minimal base system is sufficient to set up the
    Administration Server. The following patterns are the minimum required:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="guimenu">Base System</span>
     </p></li><li class="listitem"><p>
      <span class="guimenu">Minimal System (Appliances)</span>
     </p></li><li class="listitem"><p>
      <span class="guimenu">Meta Package for Pattern cloud-ardana</span> (in case you have
      chosen to install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension)
     </p></li><li class="listitem"><p>
      <span class="guimenu">Subscription Management Tool</span> (optional, also see <a class="xref" href="#tip-depl-adm-inst-settings-smt" title="Tip: Installing a Local SMT Server (Optional)">Tip: Installing a Local SMT Server (Optional)</a>)
     </p></li><li class="listitem"><p>
      <span class="guimenu">YaST2 configuration packages</span>
     </p></li></ul></div><div id="tip-depl-adm-inst-settings-smt" data-id-title="Installing a Local SMT Server (Optional)" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Installing a Local SMT Server (Optional)</div><p>
     If you do not have a SUSE Manager or SMT server in your organization, or
     are planning to manually update the repositories required for deployment
     of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes, you need to set up an SMT server on the
     Administration Server. Choose the pattern <span class="guimenu">Subscription Management
     Tool</span> in addition to the patterns listed above to install the
     SMT server software.
    </p></div></section><section class="sect2" id="sec-depl-adm-inst-add-on" data-id-title="Installing the SUSE OpenStack Cloud Extension"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.5.2 </span><span class="title-name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-add-on">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-prepare-deployer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is an extension to SUSE Linux Enterprise Server. Installing it during the SUSE Linux Enterprise Server
    installation is the easiest and recommended way to set up the Cloud Lifecycle Manager. To get
    access to the extension selection dialog, you need to register SUSE Linux Enterprise Server 12 SP3
    during the installation. After a successful registration, the SUSE Linux Enterprise Server 12 SP3
    installation continues with the <span class="guimenu">Extension &amp; Module
    Selection</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    <span class="phrase"><span class="phrase">8</span></span></span> and provide the registration key you obtained by
    purchasing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The registration and the extension installation
    require an Internet connection.
   </p><p>
    If you do not have Internet access or are not able to register during
    installation, then once Internet access is available for the Cloud Lifecycle Manager do the
    following steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <code class="prompt user">tux &gt; </code>sudo SUSEConnect -r
      <em class="replaceable">SLES_REGISTRATION_CODE</em>
     </p></li><li class="step"><p>
      List repositories to verify:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper lr</pre></div></li><li class="step"><p>
      Refresh the repositories:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper ref</pre></div></li></ol></div></div><p>
    Alternatively, install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> after the
   SUSE Linux Enterprise Server 12 SP3 installation via <span class="guimenu">YaST</span> › <span class="guimenu">Software</span> › <span class="guimenu">Add-On Products</span>.
   For details, refer to <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-add-ons-extensions" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-deployment/#sec-add-ons-extensions</a>.
   </p></section></section></section><section class="chapter" id="app-deploy-smt-lcm" data-id-title="Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-lcm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-smt-setup.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is to install a Subscription Management Tool (SMT) server on the Cloud Lifecycle Manager server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Cloud Lifecycle Manager server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Cloud Lifecycle Manager server, skip this step.
   </p></div></div></div></div><div id="id-1.4.4.5.3" data-id-title="Use of SMT Server and Ports" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Use of SMT Server and Ports</div><p>
   When installing an SMT server on the Cloud Lifecycle Manager server, use it exclusively
   for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. To use the SMT server for other
   products, run it outside of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Make sure it can be accessed
   from the Cloud Lifecycle Manager for mirroring the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   When the SMT server is installed on the Cloud Lifecycle Manager server, Cloud Lifecycle Manager
   provides the mirrored repositories on port <code class="literal">79</code>.
  </p></div><section class="sect1" id="app-deploy-smt-install" data-id-title="SMT Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">SMT Installation</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-smt-setup.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you have not installed the SMT server during the initial Cloud Lifecycle Manager server
   installation as suggested in <a class="xref" href="#sec-depl-adm-inst-settings-software" title="3.5.1. Software Selection">Section 3.5.1, “Software Selection”</a>, run the following command
   to install it:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern smt</pre></div></section><section class="sect1" id="app-deploy-smt-config" data-id-title="SMT Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">SMT Configuration</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-smt-setup.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   No matter whether the SMT server was installed during the initial
   installation or in the running system, it needs to be configured with the
   following steps.
  </p><div id="id-1.4.4.5.5.3" data-id-title="Prerequisites" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Prerequisites</div><p>
    To configure the SMT server, a SUSE account is required. If you do not
    have such an account, register at <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>. All products and
    extensions for which you want to mirror updates with the SMT
    server should be registered at the SUSE Customer Center (<a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>).
   </p><p>
    If you did not register with the SUSE Customer Center during installation, then at this
    point you will need to register in order to proceed. Ensure that the Cloud Lifecycle Manager has external
    network access and then run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -r
     <em class="replaceable">SLES_REGISTRATION_CODE</em></pre></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Configuring the SMT server requires you to have your mirroring
     credentials (user name and password) and your registration e-mail
     address at hand. To access them, proceed as follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Open a Web browser and log in to the SUSE Customer Center at
       <a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>.
      </p></li><li class="step"><p>
       Click your name to see the e-mail address which you have registered.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Organization</span> › <span class="guimenu">Organization Credentials</span> to obtain
       your mirroring credentials (user name and password).
      </p></li></ol></li><li class="step"><p>
     Start <span class="guimenu">YaST</span> › <span class="guimenu">Network
     Services</span> › <span class="guimenu">SMT Configuration
     Wizard</span>.
    </p></li><li class="step"><p>
     Activate <span class="guimenu">Enable Subscription Management Tool Service
     (SMT)</span>.
    </p></li><li class="step"><p>
     Enter the <span class="guimenu">Customer Center Configuration</span> data as
     follows:
    </p><table style="border: 0; " class="simplelist"><tr><td><span class="guimenu">Use Custom Server</span>:
     Do <span class="emphasis"><em>not</em></span> activate this option</td></tr><tr><td><span class="guimenu">User</span>: The user name you retrieved from the
     SUSE Customer Center</td></tr><tr><td><span class="guimenu">Password</span>: The password you retrieved from the
     SUSE Customer Center</td></tr></table><p>
     Check your input with <span class="guimenu">Test</span>. If the test does not
     return <code class="literal">success</code>, check the credentials you entered.
    </p></li><li class="step"><p>
     Enter the e-mail address you retrieved from the SUSE Customer Center at
     <span class="guimenu">SCC E-Mail Used for Registration</span>.
    </p></li><li class="step"><p>
     <span class="guimenu">Your SMT Server URL</span> shows the HTTP address of your
     server. Usually it should not be necessary to change it.
    </p></li><li class="step"><p>
     Select <span class="guimenu">Next</span> to proceed to step two of the <span class="guimenu">SMT Configuration Wizard</span>.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Database Password for SMT User</span> and confirm
     it by entering it once again.
    </p></li><li class="step"><p>
     Enter one or more e-mail addresses to which SMT status reports are
     sent by selecting <span class="guimenu">Add</span>.
    </p></li><li class="step"><p>
     Select <span class="guimenu">Next</span> to save your SMT configuration. When
     setting up the database you will be prompted for the MariaDB root
     password. If you have not already created one then create it in this step. Note that this is
     the global MariaDB root password, not the database password for the SMT
     user you specified before.
    </p><p>
     The SMT server requires a server certificate at
     <code class="filename">/etc/pki/trust/anchors/YaST-CA.pem</code>. Choose
     <span class="guimenu">Run CA Management</span>, provide a password and choose
     <span class="guimenu">Next</span> to create such a certificate. If your
     organization already provides a CA certificate, <span class="guimenu">Skip</span>
     this step and import the certificate via <span class="guimenu">YaST</span> › <span class="guimenu">Security and Users</span> › <span class="guimenu">CA Management</span> after the SMT
     configuration is done. See
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-security/#cha-security-yast-ca" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-security/#cha-security-yast-ca</a>
   for more information.
    </p><p>
     After you complete your configuration a synchronization check with the SUSE Customer Center will run, which may take several minutes.
    </p></li></ol></div></div></section><section class="sect1" id="app-deploy-smt-repos" data-id-title="Setting up Repository Mirroring on the SMT Server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">Setting up Repository Mirroring on the SMT Server</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-smt-setup.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The final step in setting up the SMT server is configuring it to
   mirror the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The SMT server
   mirrors the repositories from the SUSE Customer Center. Make
   sure to have the appropriate subscriptions registered in SUSE Customer Center with the
   same e-mail address you specified when configuring SMT.
  </p><section class="sect2" id="app-deploy-smt-repos-mandatory" data-id-title="Adding Mandatory Repositories"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.1 </span><span class="title-name">Adding Mandatory Repositories</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mandatory">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-smt-setup.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Mirroring the SUSE Linux Enterprise Server 12 SP3 and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
    repositories is mandatory. Run the following commands as user
    <code class="systemitem">root</code> to add them to the list of mirrored repositories:
   </p><div class="verbatim-wrap"><pre class="screen">for REPO in SLES12-SP3-{Pool,Updates} <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></section><section class="sect2" id="app-deploy-smt-repos-mirror" data-id-title="Updating the Repositories"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.2 </span><span class="title-name">Updating the Repositories</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mirror">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-smt-setup.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    New repositories added to SMT must be updated immediately by running the following command as user <code class="systemitem">root</code>:
   </p><div class="verbatim-wrap"><pre class="screen">smt-mirror -L /var/log/smt/smt-mirror.log</pre></div><p>
    This command will download several GB of patches. This process may last
    up to several hours. A log file is written to
    <code class="filename">/var/log/smt/smt-mirror.log</code>. After this first manual update the repositories are updated automatically via cron
    job. A list of all
    repositories and their location in the file system on the Cloud Lifecycle Manager server can be
    found at <a class="xref" href="#tab-smt-repos-local" title="SMT Repositories Hosted on the Administration Server">Table 5.2, “SMT Repositories Hosted on the Administration Server”</a>.
   </p></section></section><section class="sect1" id="app-deploy-smt-info" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-smt-setup.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For detailed information about SMT refer to the Subscription Management Tool manual at
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/</a>.
  </p></section></section><section class="chapter" id="cha-depl-repo-conf-lcm" data-id-title="Software Repository Setup"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></span> <a title="Permalink" class="permalink" href="#cha-depl-repo-conf-lcm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Software repositories containing products, extensions, and the respective
  updates for all software need to be available to all nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  in order to complete the deployment. These can be managed manually, or they
  can be hosted on the Cloud Lifecycle Manager server. In this configuration step, these
  repositories are made available on the Cloud Lifecycle Manager server. There are two types of
  repositories:
 </p><p>
  <span class="bold"><strong>Product Media Repositories</strong></span>: Product media
  repositories are copies of the installation media. They need to be
  directly copied to the Cloud Lifecycle Manager server, <span class="quote">“<span class="quote">loop-mounted</span>”</span> from an iso
  image, or mounted from a remote server via NFS. Affected are SUSE Linux Enterprise Server 12 SP3 and
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. These are static repositories; they do not
  change or receive updates. See <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a> for setup instructions.
 </p><p>
  <span class="bold"><strong>Update and Pool Repositories</strong></span>: Update and
  Pool repositories are provided by the SUSE Customer Center. They contain all updates and
  patches for the products and extensions. To make them available for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> they need to be mirrored from the SUSE Customer Center. Their content is
  regularly updated, so they must be kept in synchronization with SUSE Customer Center. For
  these purposes, SUSE provides the Subscription Management Tool (SMT). See <a class="xref" href="#sec-depl-adm-conf-repos-scc" title="5.2. Update and Pool Repositories">Section 5.2, “Update and Pool Repositories”</a> for setup instructions.
 </p><section class="sect1" id="sec-depl-adm-conf-repos-product" data-id-title="Copying the Product Media Repositories"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Copying the Product Media Repositories</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-product">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The files in the product repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> do not
   change, therefore they do not need to be synchronized with a remote
   source. If you have installed the product media from a downloaded ISO image,
   the product repositories will automatically be made available to the client
   nodes and these steps can be skipped. These steps can also be skipped if you
   prefer to install from the Pool repositories provided by SUSE Customer Center. Otherwise,
   it is sufficient to either copy the data (from a remote host or the
   installation media), to mount the product repository from a remote server
   via <code class="literal">NFS</code>, or to loop mount a copy of the installation
   images.
   </p><p>
    If you choose to install from the product media rather than from the SUSE Customer Center
    repositories, the following product media needs to reside in the specified
    directories:
   </p><div class="table" id="id-1.4.4.6.5.4" data-id-title="Local Product Repositories for SUSE OpenStack Cloud"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.1: </span><span class="title-name">Local Product Repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Repository
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Directory
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
        <p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD #1
        </p>
       </td><td>
        <p>
         <code class="filename">/srv/www/suse-12.3/x86_64/repos/Cloud</code>
        </p>
       </td></tr></tbody></table></div></div><p>
    The data can be copied by a variety of methods:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.6.5.6.1"><span class="term">Copying from the Installation Media</span></dt><dd><p>
      We recommend using <code class="command">rsync</code> for copying. If the
      installation data is located on a removable device, make sure to mount
      it first (for example, after inserting the DVD1 in the Administration Server and
      waiting for the device to become ready):
     </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.6.5.6.1.2.2"><span class="name">
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
     </span><a title="Permalink" class="permalink" href="#id-1.4.4.6.5.6.1.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.3/x86_64/repos/Cloud
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/www/suse-12.3/x86_64/repos/Cloud/
umount /mnt</pre></div></dd><dt id="id-1.4.4.6.5.6.2"><span class="term">Copying from a Remote Host</span></dt><dd><p>
       If the data is provided by a remote machine, log in to that machine and
       push the data to the Administration Server (which has the IP address <code class="systemitem">192.168.245.10</code> in the following
       example):
      </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.6.5.6.2.2.2"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.4.4.6.5.6.2.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.3/x86_64/repos/Cloud
rsync -avPz <em class="replaceable">/data/SUSE-OPENSTACK-CLOUD//DVD1/</em> <em class="replaceable">192.168.245.10</em>:/srv/www/suse-12.3/x86_64/repos/Cloud/</pre></div></dd><dt id="id-1.4.4.6.5.6.3"><span class="term">Mounting from an NFS Server</span></dt><dd><p>
       If the installation data is provided via NFS by a remote machine, mount
       the respective shares as follows. To automatically mount these
       directories either create entries in <code class="filename">/etc/fstab</code> or
       set up the automounter.
      </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.6.5.6.3.2.2"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.4.4.6.5.6.3.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.3/x86_64/repos/Cloud/
mount -t nfs <em class="replaceable">nfs.example.com:/exports/SUSE-OPENSTACK-CLOUD/DVD1/</em> /srv/www/suse-12.3/x86_64/repos/Cloud</pre></div></dd></dl></div></section><section class="sect1" id="sec-depl-adm-conf-repos-scc" data-id-title="Update and Pool Repositories"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Update and Pool Repositories</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Update and Pool Repositories are required on the Cloud Lifecycle Manager server to set up and
    maintain the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes. They are provided by SUSE Customer Center and contain all
    software packages needed to install SUSE Linux Enterprise Server 12 SP3 and the extensions (pool
    repositories). In addition, they contain all updates and patches (update
    repositories).
   </p><p>
    The repositories can be made available on the Cloud Lifecycle Manager server using one or more of the
    following methods:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-local-smt" title="5.2.1.  Repositories Hosted on an SMT Server Installed on the Cloud Lifecycle Manager">Section 5.2.1, “
     Repositories Hosted on an SMT Server Installed on the Cloud Lifecycle Manager
    ”</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-alternatives" title="5.2.2. Alternative Ways to Make the Repositories Available">Section 5.2.2, “Alternative Ways to Make the Repositories Available”</a>
     </p></li></ul></div><section class="sect2" id="sec-depl-adm-conf-repos-scc-local-smt" data-id-title="Repositories Hosted on an SMT Server Installed on the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.1 </span><span class="title-name">
     Repositories Hosted on an SMT Server Installed on the Cloud Lifecycle Manager
    </span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-local-smt">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     When all update and pool repositories are managed by an SMT server
     installed on the Cloud Lifecycle Manager server (see <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a>),
     the Cloud Lifecycle Manager automatically detects all available repositories. No further
     action is required.
    </p></section><section class="sect2" id="sec-depl-adm-conf-repos-scc-alternatives" data-id-title="Alternative Ways to Make the Repositories Available"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.2 </span><span class="title-name">Alternative Ways to Make the Repositories Available</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-alternatives">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     If you want to keep your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> network as isolated from the company
     network as possible, or your infrastructure does not allow accessing a
     SUSE Manager or an SMT server, you can alternatively provide access to the
     required repositories by one of the following methods:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Mount the repositories from a remote server.
      </p></li><li class="listitem"><p>
       Synchronize the repositories from a remote server (for example via
       <code class="command">rsync</code> and cron).
      </p></li><li class="listitem"><p>
        Manually synchronize the update repositories from removable media.
      </p></li></ul></div><p>
     The repositories must be made available at the
     default locations on the Cloud Lifecycle Manager server as listed in <a class="xref" href="#tab-depl-adm-conf-local-repos" title="Repository Locations on the Cloud Lifecycle Manager server">Table 5.3, “Repository Locations on the Cloud Lifecycle Manager server”</a>.
    </p></section></section><section class="sect1" id="sec-deploy-repo-locations" data-id-title="Repository Locations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Repository Locations</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-repo-locations">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div></div></div></div><p>
The following tables show the locations of all repositories that can be used for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="table" id="tab-smt-repos-local" data-id-title="SMT Repositories Hosted on the Administration Server"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.2: </span><span class="title-name">SMT Repositories Hosted on the Administration Server </span></span><a title="Permalink" class="permalink" href="#tab-smt-repos-local">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Repository
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Directory
      </p>
     </th></tr></thead><tbody><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP3-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/<span class="phrase"><span class="phrase">OpenStack-Cloud</span></span>/8/x86_64/product/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/<span class="phrase"><span class="phrase">OpenStack-Cloud</span></span>/8/x86_64/update/</code>
      </p>
     </td></tr></tbody></table></div></div><p>
  The following table shows the required repository locations  to use when manually copying, synchronizing, or mounting the
  repositories.
 </p><div class="table" id="tab-depl-adm-conf-local-repos" data-id-title="Repository Locations on the Cloud Lifecycle Manager server"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.3: </span><span class="title-name">Repository Locations on the Cloud Lifecycle Manager server </span></span><a title="Permalink" class="permalink" href="#tab-depl-adm-conf-local-repos">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/preinstall-configure-deployer-repos.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Channel
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Directory on the Administration Server
      </p>
     </th></tr></thead><tbody><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP3-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/SLES12-SP3-Pool/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/SLES12-SP3-Updates/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Pool/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Updates</code>
      </p>
     </td></tr></tbody></table></div></div></section></section><section class="chapter" id="multipath-boot-from-san" data-id-title="Boot from SAN and Multipath Configuration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Boot from SAN and Multipath Configuration</span></span> <a title="Permalink" class="permalink" href="#multipath-boot-from-san">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-multipath_boot_from_san.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="multipath-overview" data-id-title="Introduction"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Introduction</span></span> <a title="Permalink" class="permalink" href="#multipath-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-multipath_boot_from_san.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For information about supported hardware for multipathing, see
   <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”, Section 2.2 “Supported Hardware Configurations”</span>.
  </p><div id="boot-from-san-LUN0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    When exporting a LUN to a node for boot from SAN, you should ensure that
    <span class="emphasis"><em>LUN 0</em></span> is assigned to the LUN and configure any setup
    dialog that is necessary in the firmware to consume this LUN 0 for OS boot.
   </p></div><div id="boot-from-san-host-persona" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Any hosts that are connected to 3PAR storage must have a <code class="literal">host
    persona</code> of <code class="literal">2-generic-alua</code> set on the 3PAR.
    Refer to the 3PAR documentation for the steps necessary to check this and
    change if necessary.
   </p></div><p>
   iSCSI boot from SAN is not supported. For more information on the use of
   Cinder with multipath, see <a class="xref" href="#sec-3par-multipath" title="22.1.3. Multipath Support">Section 22.1.3, “Multipath Support”</a>.
  </p><p>
   To allow <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> to use volumes from a SAN, you have to specify
   configuration options for both the installation and the OS configuration
   phase. In all cases, the devices that are utilized are devices for which
   multipath is configured.
  </p></section><section class="sect1" id="id-1.4.4.7.3" data-id-title="Install Phase Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Install Phase Configuration</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.7.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-multipath_boot_from_san.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For FC connected nodes and for FCoE nodes where the network processor used
   is from the Emulex family such as for the 650FLB, the following changes need
   to be made.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In each stanza of the <code class="filename">servers.yml</code> insert a line
     stating <code class="literal">boot-from-san: true</code>
    </p><div class="verbatim-wrap"><pre class="screen">- id: controller2
      ip-addr: 192.168.10.4
      role: CONTROLLER-ROLE
      server-group: RACK2
      nic-mapping: HP-DL360-4PORT</pre></div><p>
     This uses the disk <code class="filename">/dev/mapper/mpatha</code> as the default
     device on which to install the OS.
    </p></li><li class="step"><p>
     In the disk input models, specify the devices that will be used via their
     multipath names (which will be of the form
     <code class="filename">/dev/mapper/mpatha</code>,
     <code class="filename">/dev/mapper/mpathb</code>, etc.).
    </p><div class="verbatim-wrap"><pre class="screen">    volume-groups:
      - name: ardana-vg
        physical-volumes:

          # NOTE: 'sda_root' is a templated value. This value is checked in
          # os-config and replaced by the partition actually used on sda
          #for example sda1 or sda5
          - /dev/mapper/mpatha_root

...
      - name: vg-comp
        physical-volumes:
          - /dev/mapper/mpathb</pre></div></li></ol></div></div><p>
   Instead of using Cobbler, you need to provision a baremetal node manually
   using the following procedure.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Assign a static IP to the node.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Use the <code class="command">ip addr</code> command to list active network
       interfaces on your system:
      </p><div class="verbatim-wrap"><pre class="screen">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:70 brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff</pre></div></li><li class="step"><p>
       Identify the network interface that matches the MAC address of your
       server and edit the corresponding configuration file in
       <code class="filename">/etc/sysconfig/network-scripts</code>. For example, for
       the <code class="systemitem">eno1</code> interface, open the
       <code class="systemitem">/etc/sysconfig/network-scripts/ifcfg-eno1</code> file
       and edit <em class="replaceable">IPADDR</em> and
       <em class="replaceable">NETMASK</em> values to match your environment.
       Note that the <em class="replaceable">IPADDR</em> is used in the
       corresponding stanza in <code class="filename">servers.yml</code>. You may also
       need to set <code class="literal">BOOTPROTO</code> to <code class="literal">none</code>:
      </p><div class="verbatim-wrap"><pre class="screen">TYPE=Ethernet
BOOTPROTO=none
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=36060f7a-12da-469b-a1da-ee730a3b1d7c
DEVICE=eno1
ONBOOT=yes
NETMASK=255.255.255.192
IPADDR=10.13.111.14</pre></div></li><li class="step"><p>
       Reboot the SLES node and ensure that it can be accessed from the
       Cloud Lifecycle Manager.
      </p></li></ol></li><li class="step"><p>
     Add the <code class="literal">ardana</code> user and home directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>useradd -m -d /var/lib/ardana -U ardana</pre></div></li><li class="step"><p>
     Allow the user <code class="literal">ardana</code> to run <code class="command">sudo</code>
     without a password by creating the
     <code class="filename">/etc/sudoers.d/ardana</code> file with the following
     configuration:
    </p><div class="verbatim-wrap"><pre class="screen">ardana ALL=(ALL) NOPASSWD:ALL</pre></div></li><li class="step"><p>
     When you start installation using the Cloud Lifecycle Manager, or if you are adding a SLES
     node to an existing cloud, you need to copy the Cloud Lifecycle Manager public key to the
     SLES node to enable passwordless SSH access. One way of doing this is to
     copy the file <code class="filename">~/.ssh/authorized_keys</code> from
     another node in the cloud to the same location on the SLES node. If you
     are installing a new cloud, this file will be available on the nodes after
     running the <code class="filename">bm-reimage.yml</code> playbook. Ensure that
     there is global read access to the file
     <code class="filename">/var/lib/ardana/.ssh/authorized_keys</code>.
    </p><p>
     Use the following command to test passwordless SSH from the deployer and
     check the ability to remotely execute sudo commands:
    </p><div class="verbatim-wrap"><pre class="screen">ssh stack@<em class="replaceable">SLES_NODE_IP</em> "sudo tail -5 /var/log/messages"</pre></div></li></ol></div></div><section class="sect2" id="depl-cloud" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.2.1 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#depl-cloud">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-multipath_boot_from_san.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     For automated installation, you can specify the required parameters. For
     example, the following command disables encryption by the configuration
     processor:
    </p><div class="verbatim-wrap"><pre class="screen">    ansible-playbook -i hosts/localhost config-processor-run.yml \
    -e encrypt="" -e rekey=""</pre></div></li><li class="step"><p>
     Use the following playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     To ensure that all existing non-OS partitions on the nodes are wiped prior to
     installation, you need to run the <code class="filename">wipe_disks.yml</code>
     playbook. The <code class="filename">wipe_disks.yml</code> playbook is only meant
     to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
      This step is not required if you are using clean machines.
    </p><p>
     Before you run the <code class="filename">wipe_disks.yml</code> playbook, you need
     to make the following changes in the deployment directory.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       In the
       <code class="filename">~/scratch/ansible/next/ardana/ansible/roles/diskconfig/tasks/get_disk_info.yml</code>
       file, locate the following line:
      </p><div class="verbatim-wrap"><pre class="screen">shell: ls -1 /dev/mapper/ | grep "mpath" | grep -v {{ wipe_disks_skip_partition }}$ | grep -v {{ wipe_disks_skip_partition }}[0-9]</pre></div><p>
       Replace it with:
      </p><div class="verbatim-wrap"><pre class="screen">shell: ls -1 /dev/mapper/ | grep "mpath"  | grep -v {{ wipe_disks_skip_partition }}$ | grep -v {{ wipe_disks_skip_partition }}[0-9] | grep -v {{ wipe_disks_skip_partition }}_part[0-9]</pre></div></li><li class="listitem"><p>
       In the
       <code class="filename">~/scratch/ansible/next/ardana/ansible/roles/multipath/tasks/install.yml</code>
       file, set the <code class="literal">multipath_user_friendly_names</code> variable
       value to <code class="literal">yes</code> for all occurrences.
      </p></li></ul></div><p>
     Run the <code class="filename">wipe_disks.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor, use the command below, and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step"><p>
     Run the <code class="filename">site.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor, use the command below, and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><p>
     The step above runs <code class="systemitem">osconfig</code> to configure the
     cloud and <code class="systemitem">ardana-deploy</code> to deploy the cloud.
     Depending on the number of nodes, this step may take considerable time to
     complete.
    </p></li></ol></div></div></section></section><section class="sect1" id="restriction2" data-id-title="QLogic FCoE restrictions and additional configurations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">QLogic FCoE restrictions and additional configurations</span></span> <a title="Permalink" class="permalink" href="#restriction2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-multipath_boot_from_san.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you are using network cards such as Qlogic Flex Fabric 536 and 630
   series, there are additional OS configuration steps to support the
   importation of LUNs as well as some restrictions on supported
   configurations.
  </p><p>
   The restrictions are:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Only one network card can be enabled in the system.
    </p></li><li class="listitem"><p>
     The FCoE interfaces on this card are dedicated to FCoE traffic. They
     cannot have IP addresses associated with them.
    </p></li><li class="listitem"><p>
     NIC mapping cannot be used.
    </p></li></ul></div><p>
   In addition to the configuration options above, you also need to specify the
   FCoE interfaces for install and for os configuration. There are 3 places
   where you need to add additional configuration options for fcoe-support:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     In <code class="literal">servers.yml</code>, which is used for configuration of the
     system during OS install, FCoE interfaces need to be specified for each
     server. In particular, the mac addresses of the FCoE interfaces need to be
     given, <span class="emphasis"><em>not</em></span> the symbolic name (for example,
     <code class="literal">eth2</code>).
    </p><div class="verbatim-wrap"><pre class="screen">    - id: compute1
      ip-addr: 10.245.224.201
      role: COMPUTE-ROLE
      server-group: RACK2
      mac-addr: 6c:c2:17:33:4c:a0
      ilo-ip: 10.1.66.26
      ilo-user: linuxbox
      ilo-password: linuxbox123
      boot-from-san: True
      fcoe-interfaces:
         - <span class="bold"><strong>6c:c2:17:33:4c:a1</strong></span>
         - <span class="bold"><strong>6c:c2:17:33:4c:a9</strong></span></pre></div><div id="id-1.4.4.7.4.6.1.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      NIC mapping cannot be used.
     </p></div></li><li class="listitem"><p>
     For the osconfig phase, you will need to specify the
     <code class="literal">fcoe-interfaces</code> as a peer of
     <code class="literal">network-interfaces</code> in the
     <code class="literal">net_interfaces.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">    - name: CONTROLLER-INTERFACES
      fcoe-interfaces:
        - name: fcoe
          devices:
             - <span class="bold"><strong>eth2</strong></span>
             - <span class="bold"><strong>eth3</strong></span>
      network-interfaces:
        - name: eth0
          device:
              name: eth0
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT</pre></div><div id="id-1.4.4.7.4.6.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      The MAC addresses specified in the <code class="literal">fcoe-interfaces</code>
      stanza in <code class="filename">servers.yml</code> must correspond to the
      symbolic names used in the <code class="literal">fcoe-interfaces</code> stanza in
      <code class="filename">net_interfaces.yml</code>.
     </p><p>
      Also, to satisfy the FCoE restriction outlined in
      <a class="xref" href="#restriction2" title="6.3. QLogic FCoE restrictions and additional configurations">Section 6.3, “QLogic FCoE restrictions and additional configurations”</a> above, there can be no overlap between the
      devices in <code class="literal">fcoe-interfaces</code> and those in
      <code class="literal">network-interfaces</code> in the
      <code class="filename">net_interfaces.yml</code> file. In the example,
      <code class="literal">eth2</code> and <code class="literal">eth3</code> are
      <code class="literal">fcoe-interfaces</code> while <code class="literal">eth0</code> is in
      <code class="literal">network-interfaces</code>.
     </p></div></li><li class="listitem"><p>
     As part of the initial install from an iso, additional parameters need to
     be supplied on the kernel command line:
    </p><div class="verbatim-wrap"><pre class="screen">multipath=true partman-fcoe/interfaces=&lt;mac address1&gt;,&lt;mac address2&gt; disk-detect/fcoe/enable=true --- quiet</pre></div></li></ul></div><p>
   Since NIC mapping is not used to guarantee order of the networks across the
   system the installer will remap the network interfaces in a deterministic
   fashion as part of the install. As part of the installer dialogue, if DHCP
   is not configured for the interface, it is necessary to confirm that the
   appropriate interface is assigned the ip address. The network interfaces may
   not be at the names expected when installing via an ISO. When you are asked
   to apply an IP address to an interface, press <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F2</span> and in the console
   window, run the command <code class="command">ip a</code> to examine the interfaces
   and their associated MAC addresses. Make a note of the interface name with
   the expected MAC address and use this in the subsequent dialog. Press
   <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span> to
   return to the installation screen. You should note that the names of the
   interfaces may have changed after the installation completes. These names
   are used consistently in any subsequent operations.
  </p><p>
   Therefore, even if FCoE is not used for boot from SAN (for example for
   cinder), then it is recommended that <code class="literal">fcoe-interfaces</code> be
   specified as part of install (without the multipath or disk detect options).
   Alternatively, you need to run
   <code class="filename">osconfig-fcoe-reorder.yml</code> before
   <code class="filename">site.yml</code> or <code class="filename">osconfig-run.yml</code> is
   invoked to reorder the networks in a similar manner to the installer. In
   this case, the nodes will need to be manually rebooted for the network
   reorder to take effect. Run <code class="filename">osconfig-fcoe-reorder.yml</code>
   in the following scenarios:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If you have used a third-party installer to provision your bare-metal
     nodes
    </p></li><li class="listitem"><p>
     If you are booting from a local disk (that is one that is not presented
     from the SAN) but you want to use FCoE later, for example, for cinder.
    </p></li></ul></div><p>
   To run the command:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-fcoe-reorder.yml</pre></div><p>
   If you do not run <code class="filename">osconfig-fcoe-reorder.yml</code>, you will
   encounter a failure in <code class="filename">osconfig-run.yml</code>.
  </p><p>
   If you are booting from a local disk, the LUNs that will be imported over
   FCoE will not be visible before <code class="filename">site.yml</code> or
   <code class="filename">osconfig-run.yml</code> has been run. However, if you need to
   import the LUNs before this, for instance, in scenarios where you need to
   run <code class="filename">wipe_disks.yml</code> (run this only after first running
   <code class="filename">bm-reimage.yml</code>), then you can run the
   <code class="literal">fcoe-enable</code> playbook across the nodes in question. This
   will configure FCoE and import the LUNs presented to the nodes.
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/verb_hosts fcoe-enable.yml</pre></div></section><section class="sect1" id="install-boot-from-san" data-id-title="Installing the SUSE OpenStack Cloud 8 ISO for Nodes That Support Boot From SAN"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> ISO for Nodes That Support Boot From SAN</span></span> <a title="Permalink" class="permalink" href="#install-boot-from-san">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-multipath_boot_from_san.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     During manual installation of SUSE Linux Enterprise Server 12 SP3, select the desired SAN disk and
     create an LVM partitioning scheme that meets <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> requirements,
     that is it has an <code class="literal">ardana-vg</code> volume group and an
     <code class="literal">ardana-vg-root</code> logical volume. For further information
     on partitioning, see
     <a class="xref" href="#sec-depl-adm-inst-partitioning" title="3.3. Partitioning">Section 3.3, “Partitioning”</a>.
    </p></li><li class="step"><p>
     After the installation is completed and the system is booted up, open the
     file <code class="filename">/etc/multipath.conf</code> and edit the defaults as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">defaults {
    user_friendly_names yes
    bindings_file "/etc/multipath/bindings"
}</pre></div></li><li class="step"><p>
     Open the <code class="filename">/etc/multipath/bindings</code> file and map the
     expected device name to the SAN disk selected during installation. In
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the naming convention is <code class="literal">mpatha</code>,
     <code class="literal">mpathb</code>, and so on. For example:
    </p><div class="verbatim-wrap"><pre class="screen">mpatha-part1    360000000030349030-part1
mpatha-part2    360000000030349030-part2
mpatha-part3    360000000030349030-part3

mpathb-part1    360000000030349000-part1
mpathb-part2    360000000030349000-part2</pre></div></li><li class="step"><p>
     Reboot the machine to enable the changes.
    </p></li></ol></div></div></section></section></div><div class="part" id="cloudinstallation" data-id-title="Cloud Installation"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part II </span><span class="title-name">Cloud Installation </span></span><a title="Permalink" class="permalink" href="#cloudinstallation">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-cloudinstallation_overview.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cloudinstallation-overview"><span class="title-number">7 </span><span class="title-name">Overview</span></a></span></li><dd class="toc-abstract"><p>
   Before starting the installation, review the sample configurations offered
   by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with highly tuned example configurations for each of
   these cloud models:
  </p></dd><li><span class="chapter"><a href="#preparing-standalone"><span class="title-number">8 </span><span class="title-name">Preparing for Stand-Alone Deployment</span></a></span></li><dd class="toc-abstract"><p>
  The Cloud Lifecycle Manager can be installed on a Control Plane or on a stand-alone
  server.
 </p></dd><li><span class="chapter"><a href="#install-gui"><span class="title-number">9 </span><span class="title-name">Installing with the Install UI</span></a></span></li><dd class="toc-abstract"><p>
 </p></dd><li><span class="chapter"><a href="#using-git"><span class="title-number">10 </span><span class="title-name">Using Git for Configuration Management</span></a></span></li><dd class="toc-abstract"><p>In SUSE OpenStack Cloud 8, a local git repository is used to track configuration changes; the Configuration Processor (CP) uses this repository. Use of a git workflow means that your configuration history is maintained, making rollbacks easier and keeping a record of previous configuration settings.…</p></dd><li><span class="chapter"><a href="#install-standalone"><span class="title-number">11 </span><span class="title-name">Installing a Stand-Alone Cloud Lifecycle Manager</span></a></span></li><dd class="toc-abstract"><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 1. Overview">Chapter 1, <em>Overview</em></a>.
    </p></dd><li><span class="chapter"><a href="#install-kvm"><span class="title-number">12 </span><span class="title-name">Installing Mid-scale and Entry-scale KVM</span></a></span></li><dd class="toc-abstract"><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 1. Overview">Chapter 1, <em>Overview</em></a>.
    </p></dd><li><span class="chapter"><a href="#DesignateInstallOverview"><span class="title-number">13 </span><span class="title-name">DNS Service Installation Overview</span></a></span></li><dd class="toc-abstract"><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service supports several different backends for domain name
  service. The choice of backend must be included in the deployment model
  before the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install is completed.
 </p></dd><li><span class="chapter"><a href="#MagnumOverview"><span class="title-number">14 </span><span class="title-name">Magnum Overview</span></a></span></li><dd class="toc-abstract"><p>The SUSE OpenStack Cloud Magnum Service provides container orchestration engines such as Docker Swarm, Kubernetes, and Apache Mesos available as first class resources. SUSE OpenStack Cloud Magnum uses Heat to orchestrate an OS image which contains Docker and Kubernetes and runs that image in either …</p></dd><li><span class="chapter"><a href="#install-esx-ovsvapp"><span class="title-number">15 </span><span class="title-name">Installing ESX Computes and OVSvAPP</span></a></span></li><dd class="toc-abstract"><p>
  This section describes the installation step requirements for ESX
  Computes (nova-proxy) and OVSvAPP.
 </p></dd><li><span class="chapter"><a href="#integrate-nsx-vsphere"><span class="title-number">16 </span><span class="title-name">Integrating NSX for vSphere</span></a></span></li><dd class="toc-abstract"><p>
  This section describes the installation and integration of NSX-v, a Software
  Defined Networking (SDN) network virtualization and security platform for
  VMware's vSphere.
 </p></dd><li><span class="chapter"><a href="#install-ironic-overview"><span class="title-number">17 </span><span class="title-name">Installing Baremetal (Ironic)</span></a></span></li><dd class="toc-abstract"><p>
  Bare Metal as a Service is enabled in this release for deployment of Nova
  instances on bare metal nodes using flat networking.
  
 </p></dd><li><span class="chapter"><a href="#install-swift"><span class="title-number">18 </span><span class="title-name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></span></li><dd class="toc-abstract"><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with Swift Only model.
 </p></dd><li><span class="chapter"><a href="#install-sles-compute"><span class="title-number">19 </span><span class="title-name">Installing SLES Compute</span></a></span></li><dd class="toc-abstract"><p>SUSE OpenStack Cloud 8 supports SLES compute nodes, specifically SUSE Linux Enterprise Server 12 SP3. SUSE does not ship a SLES ISO with SUSE OpenStack Cloud so you will need to download a copy of the SLES ISO (SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso) and the SLES SDK ISO (SLE-12-SP3-SDK-DVD-x86_64…</p></dd><li><span class="chapter"><a href="#install-ardana-manila"><span class="title-number">20 </span><span class="title-name">Installing Manila and Creating Manila Shares</span></a></span></li><dd class="toc-abstract"><p>The OpenStack Shared File Systems service (Manila) provides file storage to a virtual machine. The Shared File Systems service provides a storage provisioning control plane for shared or distributed file systems. The service enables management of share types and share snapshots if you have a driver …</p></dd><li><span class="chapter"><a href="#install-heat-templates"><span class="title-number">21 </span><span class="title-name">Installing SUSE CaaS Platform Heat Templates</span></a></span></li><dd class="toc-abstract"><p>
  This chapter describes how to install SUSE CaaS Platform Heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p></dd><li><span class="chapter"><a href="#integrations"><span class="title-number">22 </span><span class="title-name">Integrations</span></a></span></li><dd class="toc-abstract"><p>
  Once you have completed your cloud installation, these are some of the common
  integrations you may want to perform.
 </p></dd><li><span class="chapter"><a href="#troubleshooting-installation"><span class="title-number">23 </span><span class="title-name">Troubleshooting the Installation</span></a></span></li><dd class="toc-abstract"><p>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </p></dd><li><span class="chapter"><a href="#esx-troubleshooting-installation"><span class="title-number">24 </span><span class="title-name">Troubleshooting the ESX</span></a></span></li><dd class="toc-abstract"><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud</span></span>
  <span class="phrase"><span class="phrase">8</span></span> for ESX.
 </p></dd></ul></div><section class="chapter" id="cloudinstallation-overview" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#cloudinstallation-overview">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-cloudinstallation_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before starting the installation, review the sample configurations offered
   by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with highly tuned example configurations for each of
   these cloud models:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Name</th><th style="border-bottom: 1px solid ; ">Location</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.1 “Entry-Scale Cloud”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.2 “Entry Scale Cloud with Metering and Monitoring Services”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.4 “ESX Examples”, Section 9.4.1 “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</span>
     </td><td style="border-bottom: 1px solid ; "><code class="filename">~/openstack/examples/entry-scale-esx-kvm</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.4 “ESX Examples”, Section 9.4.2 “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-esx-kvm-mml</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.5 “Swift Examples”, Section 9.5.1 “Entry-scale Swift Model”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.6 “Ironic Examples”, Section 9.6.1 “Entry-Scale Cloud with Ironic Flat Network”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.6 “Ironic Examples”, Section 9.6.2 “Entry-Scale Cloud with Ironic Multi-Tenancy”</span>
     </td><td style="border-bottom: 1px solid ; ">
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td style="border-right: 1px solid ; "><span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.3 “Single-Region Mid-Size Model”</span>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div><p>
   There are two methods for installation to choose from:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     You can use a GUI that runs in your Web browser.
    </p></li><li class="listitem"><p>
     You can install via the command line that gives you the flexibility and
     full control of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
    </p></li></ul></div><p>
   <span class="bold"><strong>Using the GUI</strong></span>
  </p><p>
   You should use the GUI if:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     You are not planning to deploy availability zones, L3 segmentation, or
     server groups functionality in your initial deployment.
    </p></li><li class="listitem"><p>
     You are satisfied with the tuned <span class="phrase"><span class="phrase">SUSE</span></span>-default <span class="productname">OpenStack</span> configuration.
    </p></li></ul></div><p>
   Instructions for GUI installation are in <a class="xref" href="#install-gui" title="Chapter 9. Installing with the Install UI">Chapter 9, <em>Installing with the Install UI</em></a>.
  </p><div id="id-1.4.5.2.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Reconfiguring your cloud can only be done via the command line. The GUI
    installer is for initial installation only.
   </p></div><p>
   <span class="bold"><strong>Using the Command Line</strong></span>
  </p><p>
   You should use the command line if:
  </p><div class="itemizedlist" id="idg-installation-installation-cloudinstallation-overview-xml-7"><ul class="itemizedlist"><li class="listitem"><p>
     You are installing a complex or large-scale cloud.
    </p></li><li class="listitem"><p>
     You are planning to deploy availability zones, L3 segmentation, or server
     groups functionality. For more information, see the <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”</span>.
    </p></li><li class="listitem"><p>
     You want to customize the cloud configuration beyond the tuned defaults
     that <span class="phrase"><span class="phrase">SUSE</span></span> provides out of the box.
    </p></li><li class="listitem"><p>
     You need more extensive customizations than are possible using the GUI.
    </p></li></ul></div><p>
   Instructions for installing via the command line are in <a class="xref" href="#install-kvm" title="Chapter 12. Installing Mid-scale and Entry-scale KVM">Chapter 12, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p></section><section class="chapter" id="preparing-standalone" data-id-title="Preparing for Stand-Alone Deployment"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Preparing for Stand-Alone Deployment</span></span> <a title="Permalink" class="permalink" href="#preparing-standalone">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-preparing-standalone.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="id-1.4.5.3.2" data-id-title="Cloud Lifecycle Manager Installation Alternatives"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">Cloud Lifecycle Manager Installation Alternatives</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-preparing-standalone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The Cloud Lifecycle Manager can be installed on a Control Plane or on a stand-alone
  server.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Installing the Cloud Lifecycle Manager on a Control Plane is done during the process of
    deploying your Cloud. Your Cloud and the Cloud Lifecycle Manager are deployed together.
   </p></li><li class="listitem"><p>
    With a standalone Cloud Lifecycle Manager, you install the deployer first and then deploy
    your Cloud in a separate process. Either the Install UI or command line
    can be used to deploy a stand-alone Cloud Lifecycle Manager.
   </p></li></ul></div><p>
  Each method is suited for particular needs. The best choice depends on your
  situation.
 </p><p>
  <span class="bold"><strong>Stand-alone Deployer</strong></span>
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    + Compared to a Control Plane deployer, a stand-alone deployer is easier to
    backup and redeploy in case of disaster
   </p></li><li class="listitem"><p>
    + Separates cloud management from components being managed
   </p></li><li class="listitem"><p>
    + Does not use Control Plane resources
   </p></li><li class="listitem"><p>
    - Another server is required (less of a disadvantage if using a VM)
   </p></li><li class="listitem"><p>
    - Installation may be more complex than a Control Plane Cloud Lifecycle Manager
   </p></li></ul></div><p>
  <span class="bold"><strong>Control Plane Deployer</strong></span>
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    + Installation is usually simpler than installing a stand-alone deployer
   </p></li><li class="listitem"><p>
    + Requires fewer servers or VMs
   </p></li><li class="listitem"><p>
    - Could contend with workloads for resources
   </p></li><li class="listitem"><p>
    - Harder to redeploy in case of failure compared to stand-alone deployer
   </p></li><li class="listitem"><p>
    - There is a risk to the Cloud Lifecycle Manager when updating or modifying controllers
   </p></li><li class="listitem"><p>
    - Runs on one of the servers that is deploying or managing your Cloud
   </p></li></ul></div><p>
  <span class="bold"><strong>Summary</strong></span>
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    A Control Plane Cloud Lifecycle Manager is best for small, simple Cloud deployments.
   </p></li><li class="listitem"><p>
    With a larger, more complex cloud, a stand-alone deployer provides better
    recoverability and the separation of manager from managed components.
   </p></li></ul></div></section><section class="sect1" id="id-1.4.5.3.3" data-id-title="Installing a Stand-Alone Deployer"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.2 </span><span class="title-name">Installing a Stand-Alone Deployer</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-preparing-standalone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you do not intend to install a stand-alone deployer, proceed to
   installing the Cloud Lifecycle Manager on a Control Plane.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Instructions for GUI installation are in <a class="xref" href="#install-gui" title="Chapter 9. Installing with the Install UI">Chapter 9, <em>Installing with the Install UI</em></a>.
    </p></li><li class="listitem"><p>
     Instructions for installing via the command line are in <a class="xref" href="#install-kvm" title="Chapter 12. Installing Mid-scale and Entry-scale KVM">Chapter 12, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
    </p></li></ul></div><section class="sect2" id="id-1.4.5.3.3.4" data-id-title="Before You Start"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.2.1 </span><span class="title-name">Before You Start</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.3.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-preparing-standalone.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step"><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step"><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="3.4. Creating a User">Section 3.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable">CLOUD</em> with your user name
       choice.
      </p></li><li class="step"><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step"><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li><li class="step"><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></section><section class="sect2" id="id-1.4.5.3.3.5" data-id-title="Configuring for a Stand-Alone Deployer"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.2.2 </span><span class="title-name">Configuring for a Stand-Alone Deployer</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.3.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-preparing-standalone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following steps are necessary to set up a stand-alone deployer whether
   you will be using the Install UI or command line.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Copy the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM example input model to a
     stand-alone input model. This new input model will be edited so that it
     can be used as a stand-alone Cloud Lifecycle Manager installation.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cp -r ~/openstack/examples/entry-scale-kvm/* \
~/openstack/examples/entry-scale-kvm-stand-alone-deployer</pre></div></li><li class="step"><p>
     Change to the new directory
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/openstack/examples/entry-scale-kvm-stand-alone-deployer</pre></div></li><li class="step"><p>
     Edit the cloudConfig.yml file to change the name of the input model. This
     will make the model available both to the Install UI and to the command
     line installation process.
    </p><p>
     Change <code class="literal">name: entry-scale-kvm</code> to <code class="literal">name:
     entry-scale-kvm-stand-alone-deployer</code>
    </p></li><li class="step"><p>
     Change to the <code class="filename">data</code> directory.
    </p></li><li class="step"><p>
     Make the following edits to your configuration files.
    </p><div id="id-1.4.5.3.3.5.3.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      The indentation of each of the input files is important and will cause
      errors if not done correctly. Use the existing content in each of these
      files as a reference when adding additional content for your Cloud Lifecycle Manager.
     </p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Update <code class="filename">control_plane.yml</code> to add the Cloud Lifecycle Manager.
      </p></li><li class="listitem"><p>
       Update <code class="filename">server_roles.yml</code> to add the Cloud Lifecycle Manager role.
      </p></li><li class="listitem"><p>
       Update <code class="filename">net_interfaces.yml</code> to add the interface
       definition for the Cloud Lifecycle Manager.
      </p></li><li class="listitem"><p>
       Create a <code class="filename">disks_lifecycle_manager.yml</code> file to define
       the disk layout for the Cloud Lifecycle Manager.
      </p></li><li class="listitem"><p>
       Update <code class="filename">servers.yml</code> to add the dedicated Cloud Lifecycle Manager node.
      </p></li></ul></div><p>
     <code class="filename">Control_plane.yml</code>: The snippet below shows the
     addition of a single node cluster into the control plane to host the Cloud Lifecycle Manager
     service.
    </p><div id="id-1.4.5.3.3.5.3.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      In addition to adding the new cluster, you also have to remove the Cloud Lifecycle Manager
      component from the <code class="literal">cluster1</code> in the examples.
     </p></div><div class="verbatim-wrap"><pre class="screen">  clusters:
<span class="bold"><strong>     - name: cluster0
       cluster-prefix: c0
       server-role: LIFECYCLE-MANAGER-ROLE
       member-count: 1
       allocation-policy: strict
       service-components:
         - lifecycle-manager</strong></span>
         - ntp-client
     - name: cluster1
       cluster-prefix: c1
       server-role: CONTROLLER-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - ntp-server</pre></div><p>
     This specifies a single node of role
     <code class="literal">LIFECYCLE-MANAGER-ROLE</code> hosting the Cloud Lifecycle Manager.
    </p><p>
     <code class="filename">Server_roles.yml</code>: The snippet below shows the
     insertion of the new server roles definition:
    </p><div class="verbatim-wrap"><pre class="screen">   server-roles:

<span class="bold"><strong>      - name: LIFECYCLE-MANAGER-ROLE
        interface-model: LIFECYCLE-MANAGER-INTERFACES
        disk-model: LIFECYCLE-MANAGER-DISKS</strong></span>

      - name: CONTROLLER-ROLE</pre></div><p>
     This defines a new server role which references a new interface-model and
     disk-model to be used when configuring the server.
    </p><p>
     <code class="filename">net-interfaces.yml</code>: The snippet below shows the
     insertion of the network-interface info:
    </p><div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>    - name: LIFECYCLE-MANAGER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                 mode: active-backup
                 miimon: 200
                 primary: hed3
             provider: linux
             devices:
                 - name: hed3
                 - name: hed4
          network-groups:
             - MANAGEMENT</strong></span></pre></div><p>
     This assumes that the server uses the same physical networking layout as
     the other servers in the example.
    </p><p>
     <code class="filename">disks_lifecycle_manager.yml</code>: In the examples,
     disk-models are provided as separate files (this is just a convention, not
     a limitation) so the following should be added as a new file named
     <code class="filename">disks_lifecycle_manager.yml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">---
   product:
      version: 2

   disk-models:
<span class="bold"><strong>   - name: LIFECYCLE-MANAGER-DISKS
     # Disk model to be used for Cloud Lifecycle Managers nodes
     # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5

     volume-groups:
       - name: ardana-vg
         physical-volumes:
           - /dev/sda_root

       logical-volumes:
       # The policy is not to consume 100% of the space of each volume group.
       # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 80%
            fstype: ext4
            mount: /
          - name: crash
            size: 15%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
              name: os</strong></span></pre></div><p>
     <code class="filename">Servers.yml</code>: The snippet below shows the insertion of
     an additional server used for hosting the Cloud Lifecycle Manager. Provide the address
     information here for the server you are running on, that is, the node
     where you have installed the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO.
    </p><div class="verbatim-wrap"><pre class="screen">  servers:
     # NOTE: Addresses of servers need to be changed to match your environment.
     #
     #       Add additional servers as required

<span class="bold"><strong>     #Lifecycle-manager
     - id: lifecycle-manager
       ip-addr: <em class="replaceable">YOUR IP ADDRESS HERE</em>
       role: LIFECYCLE-MANAGER-ROLE
       server-group: RACK1
       nic-mapping: HP-SL230-4PORT
       mac-addr: 8c:dc:d4:b5:c9:e0
       # ipmi information is not needed </strong></span>

     # Controllers
     - id: controller1
       ip-addr: 192.168.10.3
       role: CONTROLLER-ROLE</pre></div></li></ol></div></div><p>
   With the stand-alone input model complete, you are ready to proceed to
   installing the stand-alone deployer with either the Install UI or the
   command line.
  </p></section></section></section><section class="chapter" id="install-gui" data-id-title="Installing with the Install UI"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Installing with the Install UI</span></span> <a title="Permalink" class="permalink" href="#install-gui">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-gui_installer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with a GUI-based installation wizard for first-time cloud
  installations. It will guide you through the configuration process and deploy
  your cloud based on the custom configuration you provide.  The Install UI
  will start with a set of example cloud configurations for you to choose
  from. Based on your cloud choice, you can refine your configuration to match
  your needs using Install UI widgets. You can also directly edit your model
  configuration files.
 </p><div id="id-1.4.5.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The Install UI is only for initial deployments. It will not function
   properly after your cloud has been deployed successfully, whether it was
   from the CLI or with the Install UI.
  </p></div><p>
  When you are satisfied with your configuration and the Install UI has
  validated your configuration successfully, you can then deploy the cloud into
  your environment. Deploying the cloud will version-control your configuration
  into a git repository and provide you with live progress of your deployment.
 </p><p>
  With the Install UI, you have the option of provisioning SLES12-SP3 to
  IPMI-capable machines described in your configuration files. Provisioning
  machines with the Install UI will also properly configure them for Ansible
  access.
 </p><p>
  The Install UI is designed to make the initial installation process
  simpler, more accurate, and faster than manual installation.
 </p><section class="sect1" id="id-1.4.5.4.8" data-id-title="Before You Start"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.1 </span><span class="title-name">Before You Start</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-gui_installer.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step"><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step"><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="3.4. Creating a User">Section 3.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable">CLOUD</em> with your user name
       choice.
      </p></li><li class="step"><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step"><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li><li class="step"><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></section><section class="sect1" id="id-1.4.5.4.9" data-id-title="Preparing to Run the Install UI"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.2 </span><span class="title-name">Preparing to Run the Install UI</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.9">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-gui_installer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you launch the Install UI to install your cloud, do the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Gather the following details from the servers that will make up your
     cloud:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Server names
      </p></li><li class="listitem"><p>
       IP addresses
      </p></li><li class="listitem"><p>
       Server roles
      </p></li><li class="listitem"><p>
       PXE MAC addresses
      </p></li><li class="listitem"><p>
       PXE IP addresses
      </p></li><li class="listitem"><p>
       PXE interfaces
      </p></li><li class="listitem"><p>
       IPMI IP address, username, password
      </p></li></ul></div></li><li class="step"><p>
     Choose an input model from <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>. No
     action other than an understanding of your needs is necessary at this
     point. In the Install UI you will indicate which input model you wish to
     deploy.
    </p></li><li class="step"><p>
     Before you use the Install UI to install your cloud, you may install the
     operating system, SLES, on your nodes (servers) if you choose.
     Otherwise, the Install UI will install it for you.
    </p><p> If you are installing the operating system on all nodes yourself,
    you must do so using the SLES image included in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
    package.
    </p></li></ol></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, a local git repository is used to track configuration
  changes; the Configuration Processor (CP) uses this repository. Use of a git
  workflow means that your configuration history is maintained, making
  rollbacks easier and keeping a record of previous configuration settings. The
  git repository also provides a way for you to merge changes that you pull down as
  <span class="quote">“<span class="quote">upstream</span>”</span> updates (that is, updates from <span class="phrase"><span class="phrase">SUSE</span></span>). It also
  allows you to manage your own configuration changes.
 </p><p>
  The git repository is installed by the Cloud Lifecycle Manager on the Cloud Lifecycle Manager node.
 </p><p>
  Using the Install UI does not require the use of the git repository. After
  the installation, it may be useful to know more about
  <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>.
 </p></section><section class="sect1" id="create-csv-file" data-id-title="Optional: Creating a CSV File to Import Server Data"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.3 </span><span class="title-name">Optional: Creating a CSV File to Import Server Data</span></span> <a title="Permalink" class="permalink" href="#create-csv-file">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-gui_installer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before beginning the installation, you can create a CSV file with your
   server information to import directly into the Install UI to avoid
   entering it manually on the <span class="guimenu">Assign Servers</span> page.
  </p><p>
   The following table shows the fields needed for your CSV file.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col1"/><col style="text-align: center; " class="col2"/><col style="text-align: center; " class="col3"/><col class="col4"/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><span class="bold"><strong>Field</strong></span>
      </td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; "><span class="bold"><strong>Required</strong></span>
      </td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; "><span class="bold"><strong>Required for OS Provisioning</strong></span>
      </td><td style="border-bottom: 1px solid ; "><span class="bold"><strong>Aliases</strong></span>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Server ID</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="border-bottom: 1px solid ; ">server_id, id</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IP Address</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="border-bottom: 1px solid ; ">ip, ip_address, ip_addr</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">MAC Address</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="border-bottom: 1px solid ; ">mac, mac_address, mac_addr</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI IP Address</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="border-bottom: 1px solid ; ">ipmi_ip, ipmi_ip_address</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI User</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="border-bottom: 1px solid ; ">ipmi_user, user</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">IPMI Password</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="border-bottom: 1px solid ; ">ipmi_password, password</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Server Role</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="border-bottom: 1px solid ; ">server_role, role</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Server Group</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="border-bottom: 1px solid ; ">server_group, group</td></tr><tr><td style="border-right: 1px solid ; ">NIC Mapping</td><td style="text-align: center; border-right: 1px solid ; ">No</td><td style="text-align: center; border-right: 1px solid ; ">No</td><td>server_nic_map, nic_map, nic_mapping</td></tr></tbody></table></div><p>
   The aliases are all the valid names that can be used in the CSV file for the
   column header for a given field. Field names are not case sensitive. You can
   use either <code class="literal"> </code> (space) or <code class="literal">-</code> (hyphen) in
   place of underscore for a field name.
  </p><p>
   An example CSV file could be:
  </p><div class="verbatim-wrap"><pre class="screen">id,ip-addr,mac-address,server-group,nic-mapping,server-role,ipmi-ip,ipmi-user
controller1,192.168.110.3,b2:72:8d:ac:7c:6f,RACK1,HP-DL360-4PORT,CONTROLLER-ROLE,192.168.109.3,admin
myserver4,10.2.10.24,00:14:22:01:23:44,AZ1,,,,</pre></div></section><section class="sect1" id="discover-servers" data-id-title="Optional: Importing Certificates for SUSE Manager and HPE OneView"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.4 </span><span class="title-name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span></span> <a title="Permalink" class="permalink" href="#discover-servers">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-gui_installer.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you intend to use SUSE Manager or HPE OneView to add servers, certificates for
   those services must be accessible to the Install UI.
  </p><p>
   Use the following steps to import a SUSE Manager certificate.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Retrieve the <code class="filename">.pem</code> file from the SUSE Manager.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k https://<em class="replaceable">SUSE_MANAGER_IP</em>:<em class="replaceable">PORT</em>/pub/RHN-ORG-TRUSTED-SSL-CERT &gt; <em class="replaceable">PEM_NAME</em>.pem</pre></div></li><li class="step"><p>
     Copy the <code class="filename">.pem</code> file to the proper location on the
     Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/pki/trust/anchors
sudo cp ~/<em class="replaceable">PEM_NAME</em>.pem .</pre></div></li><li class="step"><p>
     Install the certificate.
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step"><p>
     Add <em class="replaceable">SUSE Manager host IP address</em> if
     <em class="replaceable">SUSE Manager.test.domain</em> is not reachable by DNS.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/hosts</pre></div><p>
     Add <em class="replaceable">SUSE Manager host IP address</em>
     <em class="replaceable">SUSE Manager.test.domain</em>. For example:
    </p><div class="verbatim-wrap"><pre class="screen">10.10.10.10 SUSE Manager.test.domain</pre></div></li></ol></div></div><p>
   Use the following steps to import an HPE OneView certificate.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step" id="st-oneview-retrieve-id"><p>
     Retrieve the <code class="literal">sessionID</code>.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k -H "X-Api-Version:500" -H "Content-Type: application/json" \
-d '{"userName":<em class="replaceable">ONEVIEW_USER</em>, "password":<em class="replaceable">ONEVIEW_PASSWORD</em>, \
"loginMsgAck":"true"}' https://<em class="replaceable">ONEVIEW_MANAGER_URL</em>:<em class="replaceable">PORT</em>/rest/login-sessions</pre></div><p>
     The response will be similar to:
    </p><div class="verbatim-wrap"><pre class="screen">{"partnerData":{},"sessionID":"LTYxNjA1O1NjkxMHcI1b2ypaGPscErUOHrl7At3-odHPmR"}</pre></div></li><li class="step"><p>
     Retrieve a Certificate Signing Request (CSR) using the
     <em class="replaceable">sessionID</em> from
     <a class="xref" href="#st-oneview-retrieve-id" title="Step 1">Step 1</a>.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k -i -H "X-Api-Version:500" -H <em class="replaceable">sessionID</em> \
<em class="replaceable">ONEVIEW_MANAGER_URL</em>/rest/certificates/ca \
&gt; <em class="replaceable">CA_NAME</em>.csr</pre></div></li><li class="step"><p>
     Follow instructions in the HPE OneView User Guide to validate the CSR and
     obtain a signed certificate
     (<em class="replaceable">CA_NAME</em><code class="filename">.crt</code>).
    </p></li><li class="step"><p>
     Copy the <code class="filename">.crt</code> file to the proper location on the
     Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/pki/trust/anchors
sudo cp ~/data/<em class="replaceable">CA_NAME</em>.crt .</pre></div></li><li class="step"><p>
     Install the certificate.
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step"><p>
     Follow instructions in your HPE OneView User Guide to import the
     <em class="replaceable">CA_NAME</em>.crt certificate into HPE OneView.
    </p></li><li class="step"><p>
     Add <em class="replaceable">HPE OneView host IP address</em> if
     <em class="replaceable">HPE OneView.test.domain</em> is not reachable by DNS.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/hosts</pre></div><p>
     Add <em class="replaceable">HPE OneView host IP address</em>
     <em class="replaceable">HPE OneView.test.domain</em> For example:
    </p><div class="verbatim-wrap"><pre class="screen">10.84.84.84  HPE OneView.test.domain</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.5.4.12" data-id-title="Running the Install UI"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.5 </span><span class="title-name">Running the Install UI</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.12">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-gui_installer.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.4.12.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    The Install UI must run continuously without stopping for authentication
    at any step. When using the Install UI it is required to launch the Cloud Lifecycle Manager
    with the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</pre></div></div><p>
   Deploying the cloud to your servers will reconfigure networking and firewall
   rules on your cloud servers. To avoid problems with these networking changes
   when using the Install UI, we recommend you run a browser directly on your
   Cloud Lifecycle Manager node and point it to <code class="uri">http://localhost:3000</code>.
  </p><p>
   If you cannot run a browser on the Cloud Lifecycle Manager node to perform the install, you
   can run a browser from a Linux-based computer in your
   <span class="bold"><strong>MANAGEMENT</strong></span> network. However, firewall rules
   applied during cloud deployment will block access to the Install UI. To
   avoid blocking the connection, you can use the Install UI via an SSH
   tunnel to the Cloud Lifecycle Manager server. This will allow SSH connections through the
   <span class="bold"><strong>MANAGEMENT</strong></span> network when you reach the
   "Review Configuration Files" step of the install process.
  </p><p>
   To open an SSH tunnel from your Linux-based computer in your
   <span class="bold"><strong>MANAGEMENT</strong></span> network to the Cloud Lifecycle Manager:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a new terminal and enter the following command:
    </p><div class="verbatim-wrap"><pre class="screen">ssh -N -L 8080:localhost:3000 ardana@<em class="replaceable">MANAGEMENT IP address of Cloud Lifecycle Manager</em></pre></div><p>
     The user name and password should be what was set in
     <a class="xref" href="#sec-depl-adm-inst-add-on" title="3.5.2. Installing the SUSE OpenStack Cloud Extension">Section 3.5.2, “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</a>. There will be no prompt after
     you have logged in.
    </p></li><li class="step"><p>
     Leave this terminal session open to keep the SSH tunnel open and running.
     This SSH tunnel will forward connections from your Linux-based computer
     directly to the Cloud Lifecycle Manager, bypassing firewall restrictions.
    </p></li><li class="step"><p>
     On your local computer (the one you are tunneling from), point your
     browser to <code class="uri">http://localhost:8080</code>.
    </p></li><li class="step"><p>
     If the connection is interrupted, refresh your browser.
    </p></li></ol></div></div><div id="id-1.4.5.4.12.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    If you use an SSH tunnel to connect to the Install UI, there is an
    important note in the "Review Configuration Files" step about modifying
    <code class="filename">firewall_rules.yml</code> to allow SSH connections on the
    <span class="bold"><strong>MANAGEMENT</strong></span> network.
   </p></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.8"><span class="name">Overview</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.8">#</a></h5></div><p>
   The first page of the Install UI shows the general installation process
   and a reminder to gather some information before beginning. Clicking the
   <span class="guimenu">Next</span> button brings up the <span class="guimenu">Model
   Selection</span> page.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_intro.png"><img src="images/installer_ui_intro.png" alt="Image" title="Image"/></a></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.11"><span class="name">Choose an <span class="productname">OpenStack</span> Cloud Model</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.11">#</a></h5></div><p>
   The input model choices are displayed on this page. Details of each model
   can be seen on the right by clicking the model name on the left. If you have
   already decided some aspects of your cloud environment, models can be
   filtered using the dropdown selections. Narrowing a parameter affects the
   range of choices of models and changes other dropdown choices to only those
   that are compatible.
  </p><p>
   Selecting a model will determine the base template from which the cloud will
   be deployed. Models can be adjusted later in the process, though selecting
   the closest match to your requirements reduces the effort required to deploy
   your cloud.
  </p><div id="id-1.4.5.4.12.14" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    If you select any ESX model, extra manual steps are required to avoid
    configuration failures. While installing an ESX model with the
    Install UI, you will be asked for interfaces related to ESX and
    OVSvApp. Those interfaces must be defined before being entered in the
    Install UI.  Instructions are available at <a class="xref" href="#esxi-overview" title="15.3. Overview of ESXi and OVSvApp">Section 15.3, “Overview of ESXi and OVSvApp”</a>.
   </p></div><div id="id-1.4.5.4.12.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    <span class="bold"><strong>Installing a Stand-alone Deployer</strong></span>
   </p><p>
    If you are using the Install UI to install a stand-alone deployer, select
    that model, which was created previously in <a class="xref" href="#preparing-standalone" title="Chapter 8. Preparing for Stand-Alone Deployment">Chapter 8, <em>Preparing for Stand-Alone Deployment</em></a>.
   </p><p>
    Continue with the remaining Install UI steps to finish installing the
    stand-alone deployer.
   </p></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_select_model.png"><img src="images/installer_ui_select_model.png" alt="Image" title="Image"/></a></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.17"><span class="name">Cloud Model to Deploy</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.17">#</a></h5></div><p>
   Based on the cloud example selected on the previous page, more detail is
   shown about that cloud configuration and the components that will be
   deployed. If you go back and select a different model, the deployment
   process restarts from the beginning. Any configuration changes you have made
   will be deleted.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="emphasis"><em>Mandatory components</em></span> have assigned quantities. We
     strongly suggest not changing those quantities to avoid potential problems
     later in the installation process.
    </p></li><li class="listitem"><p>
     <span class="emphasis"><em>Additional components</em></span> can be adjusted within the
     parameters shown.
    </p></li></ul></div><p>
   The number of nodes (servers) dedicated to each server role can be adjusted.
   Most input models are designed to support High Availability and to
   distribute <span class="productname">OpenStack</span> services appropriately.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_model_details.png"><img src="images/installer_ui_model_details.png" alt="Image" title="Image"/></a></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.22"><span class="name">Adding Servers and Assigning Server Roles</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.22">#</a></h5></div><p>
   This page provides more detail about the number and assignment of each type
   of node based on the information from the previous page (any changes must be
   made there).
  </p><p>
   Components that do not meet the required parameters will be shown in red in
   the accordion bar. Missing required fields and duplicate server names will
   also be red, as will the accordion bar. The <span class="guimenu">Next</span> button
   will be disabled.
  </p><p>
   Servers may be discovered using SUSE Manager, HPE OneView, or both. Ensure that
   the certificates are accessible, as described in
   <a class="xref" href="#discover-servers" title="9.4. Optional: Importing Certificates for SUSE Manager and HPE OneView">Section 9.4, “Optional: Importing Certificates for SUSE Manager and HPE OneView”</a>. Clicking the <span class="guimenu">Discover</span>
   button will prompt for access credentials to the system management software
   to be used for discovery. Certificates can be verified by checking
   <span class="guimenu">Verify SSL certificate</span>. After validating credentials,
   Discovery will retrieve a list of known servers from SUSE Manager and/or
   HPE OneView and allow access to server details on those management platforms.
  </p><p>
   You can drag and drop to move servers from the left to the right in order to
   assign server roles, from right to left, or up and down in the accordion
   bar.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_assign_servers.png"><img src="images/installer_ui_assign_servers.png" alt="Image" title="Image"/></a></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_discovery.png"><img src="images/installer_ui_discovery.png" alt="Image" title="Image"/></a></div></div><p>
   Server information may also be entered manually or imported via CSV in the
   <span class="guimenu">Manual Entry</span> tab. The format for CSV entry is described
   in <a class="xref" href="#create-csv-file" title="9.3. Optional: Creating a CSV File to Import Server Data">Section 9.3, “Optional: Creating a CSV File to Import Server Data”</a>. The server assignment list includes
   placeholder server details that can be edited to reflect real hardware, or
   can be removed and replaced with discovered or manually added systems.
  </p><p>
   For more information about server roles, see
   <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”, Section 5.2 “Concepts”, Section 5.2.4 “Server Roles”</span>.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_add_servers_manually.png"><img src="images/installer_ui_add_servers_manually.png" alt="Image" title="Image"/></a></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_add_server_manually.png"><img src="images/installer_ui_add_server_manually.png" alt="Image" title="Image"/></a></div></div><p>
   Subnet and netmask values should be set on this page as they may impact the
   IP addresses being assigned to various servers.
  </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.34"><span class="name">Choose servers on which SLES will be installed</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.34">#</a></h5></div><p>
   If an OS has not previously been installed on the servers that make up the
   cloud configuration, the OS installation page allows for Cobbler to deploy
   SLES on servers in the cloud configuration. Enter password, select
   servers and click <span class="guimenu">Install</span> to deploy SLES to these
   servers. An installation log and progress indicators will be displayed.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_install_os.png"><img src="images/installer_ui_install_os.png" alt="Image" title="Image"/></a></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_install_os_inprogress.png"><img src="images/installer_ui_install_os_inprogress.png" alt="Image" title="Image"/></a></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.38"><span class="name">Server and Role Summary</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.38">#</a></h5></div><p>
   When the OS installation is complete, a Server and Server Role Summary page
   is displayed. It shows which servers have been assigned to each role, and
   provides an opportunity to edit the server configurations. Various cloud
   components can be configured by clicking on the <span class="guimenu">Manage Cloud
   Settings</span> button. Incorrect information will be shown in red.
  </p><p>
   Below is the list of what can be changed within the Install UI, followed
   by a list of customizations that can only be changed by directly editing the
   files on the <span class="guimenu">Review Configuration Files</span> page. Anything
   changed directly in the files themselves during the Install UI process
   will be overwritten by values you have entered with the Install UI.
  </p><p>
   Changes to the following items can be made:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     servers (including SLES installation configuration)
    </p></li><li class="listitem"><p>
     networks
    </p></li><li class="listitem"><p>
     disk models
    </p></li><li class="listitem"><p>
     interface models
    </p></li><li class="listitem"><p>
     NIC mappings
    </p></li><li class="listitem"><p>
     NTP servers
    </p></li><li class="listitem"><p>
     name servers
    </p></li><li class="listitem"><p>
     tags in network groups
    </p></li></ul></div><p>
   Changes to the following items can only be made by manually editing the
   associated <code class="filename">.yml</code> files on the <span class="guimenu">Review
   Configuration</span> page of the Install UI:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     server groups
    </p></li><li class="listitem"><p>
     server roles
    </p></li><li class="listitem"><p>
     network groups
    </p></li><li class="listitem"><p>
     firewall rules
    </p></li><li class="listitem"><p>
     DNS, SMTP, firewall settings (<code class="filename">cloudConfig.yml</code>)
    </p></li><li class="listitem"><p>
     control planes
    </p></li></ul></div><div id="id-1.4.5.4.12.45" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Directly changing files may cause the configuration to fail validation.
    During the process of installing with the Install UI, any changes should
    be made with the tools provided within the Install UI.
   </p></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_server_summary.png"><img src="images/installer_ui_server_summary.png" alt="Image" title="Image"/></a></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_edit_server.png"><img src="images/installer_ui_edit_server.png" alt="Image" title="Image"/></a></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_manage_cloud_settings.png"><img src="images/installer_ui_manage_cloud_settings.png" alt="Image" title="Image"/></a></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.49"><span class="name">Review Configuration Files</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.49">#</a></h5></div><p>
   Advanced editing of the cloud configuration can be done on the
   <code class="literal">Review Configuration Files</code> page. Individual
   <code class="filename">.yml</code> and <code class="filename">.j2</code> files can be edited
   directly with the embedded editor in the <span class="guimenu">Model</span> and
   <span class="guimenu">Templates and Services</span> tabs. The
   <span class="guimenu">Deployment</span> tab contains the items <span class="guimenu">Wipe Data
   Disks</span>, <span class="guimenu">Encryption Key</span> and <span class="guimenu">Verbosity
   Level</span>.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_review_configuration.png"><img src="images/installer_ui_review_configuration.png" alt="Image" title="Image"/></a></div></div><div id="id-1.4.5.4.12.52" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    If you are using an SSH tunnel to connect to the Install UI, you will
    need to make an extra modification here to allow SSH connections through
    the firewall:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      While on the Review Configuration Files page, click on the
      <span class="guimenu">Model</span> tab.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Firewall Rules</span>.
     </p></li><li class="step"><p>
      Uncomment the <code class="literal">SSH</code> section (remove the
      <code class="literal">#</code> at the beginning of the line for the <code class="literal">-
      name: SSH</code> section).
     </p></li><li class="step"><p>
      If you do not have such a <code class="literal">- name: SSH</code> section,
      manually add the following under the <code class="literal">firewall-rules:</code>
      section:
     </p><div class="verbatim-wrap"><pre class="screen">name: SSH
network-groups:
- MANAGEMENT
rules:
- type: allow
  remote-ip-prefix: 0.0.0.0/0
  port-range-min: 22
  port-range-max: 22
  protocol: tcp</pre></div></li></ol></div></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_review_configuration_edit_yml.png"><img src="images/installer_ui_review_configuration_edit_yml.png" alt="Image" title="Image"/></a></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_review_configuration_edit_services.png"><img src="images/installer_ui_review_configuration_edit_services.png" alt="Image" title="Image"/></a></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_review_configuration_deployment.png"><img src="images/installer_ui_review_configuration_deployment.png" alt="Image" title="Image"/></a></div></div><div id="id-1.4.5.4.12.56" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   Manual edits to your configuration files outside of the Install UI may not
   be reflected in the Install UI. If you make changes to any files directly,
   refresh the browser to make sure changes are seen by the Install UI.
  </p></div><p>
   Before performing the deployment, the configuration must be validated by
   clicking the <span class="guimenu">Validate</span> button below the list of
   configuration files on the <span class="guimenu">Model</span> tab. This ensures the
   configuration will be successful <span class="bold"><strong>before</strong></span> the
   actual configuration process runs and possibly fails. The
   <span class="guimenu">Validate</span> button also commits any changes. If there are
   issues with the validation, the configuration processor will provide
   detailed information about the causes. When validation completes
   successfully, a message will be displayed that the model is valid. If either
   validation or commit fail, the <span class="guimenu">Next</span> button is disabled.
  </p><p>
   Clicking the <span class="guimenu">Deploy</span> button starts the actual deployment
   process.
  </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.12.59"><span class="name">Cloud Deployment in Progress</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.59">#</a></h5></div><p>
   General progress steps are shown on the left. Detailed activity is shown on
   the right.
  </p><p>
   To start the deployment process, the Install UI runs scripts and playbooks
   based on the actual final configuration. Completed operations are green,
   black means in process, gray items are not started yet.
  </p><p>
   The log stream on the right shows finished states. If there are any
   failures, the log stream will show the errors and the
   <span class="guimenu">Next</span> button will be disabled. The <span class="guimenu">Back</span>
   and <span class="guimenu">Next</span> buttons are disabled during the deployment
   process.
  </p><p>
   The log files in <code class="filename">~/ardana/.ansible/ansible.log</code> and
   <code class="filename">/var/cache/ardana_installer/</code> have debugging
   information.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/ardana-service.log</code>
     is created and used during the deployment step.
    </p></li><li class="listitem"><p>
     Each of the time-stamped files in
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/logs/*.log</code>
     shows the output of a single Ansible playbook run invoked during the UI
     installation process and the log output for each of those runs.
    </p></li><li class="listitem"><p>
     The <code class="filename">~/ardana/.ansible/ansible.log</code> file is the output
     of all Ansible playbook runs. This includes the logs from
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/logs/*.log</code>.
    </p></li></ul></div><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_deploy_inprogress.png"><img src="images/installer_ui_deploy_inprogress.png" alt="Image" title="Image"/></a></div></div><p>
   When the deployment process is complete, all items on the left will be
   green. Some deployments will not include all steps shown if they do not apply
   to the selected input model. In such a situation, those unneeded steps will
   remain gray.
  </p><p>
   The <span class="guimenu">Next</span> button will be enabled when deployment is
   successful.
  </p><p>
   Clicking <span class="guimenu">Next</span> will display the <code class="literal">Cloud Deployment
   Successful</code> page with information about the deployment, including
   the chosen input model and links to cloud management tools.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/installer_ui_deploy_successful.png"><img src="images/installer_ui_deploy_successful.png" alt="Image" title="Image"/></a></div></div><p>
   After installation is complete, shutdown the Install UI by logging into
   the Cloud Lifecycle Manager and running the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost installui-stop.yml</pre></div><p>
   After deployment, continue to <a class="xref" href="#cloud-verification" title="Chapter 26. Cloud Verification">Chapter 26, <em>Cloud Verification</em></a> and
   <a class="xref" href="#postinstall-checklist" title="Chapter 32. Other Common Post-Installation Tasks">Chapter 32, <em>Other Common Post-Installation Tasks</em></a>.
  </p><p>
   To understand cloud configuration more thoroughly and to learn how to make
   changes later, see:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”</span>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>
    </p></li></ul></div></section></section><section class="chapter" id="using-git" data-id-title="Using Git for Configuration Management"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">Using Git for Configuration Management</span></span> <a title="Permalink" class="permalink" href="#using-git">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, a local git repository is used to track configuration
  changes; the Configuration Processor (CP) uses this repository. Use of a git
  workflow means that your configuration history is maintained, making
  rollbacks easier and keeping a record of previous configuration settings. The
  git repository also provides a way for you to merge changes that you pull down as
  <span class="quote">“<span class="quote">upstream</span>”</span> updates (that is, updates from <span class="phrase"><span class="phrase">SUSE</span></span>). It also
  allows you to manage your own configuration changes.
 </p><p>
  The git repository is installed by the Cloud Lifecycle Manager on the Cloud Lifecycle Manager node.
 </p><section class="sect1" id="id-1.4.5.5.4" data-id-title="Initialization on a new deployment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.1 </span><span class="title-name">Initialization on a new deployment</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   On a system new to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the Cloud Lifecycle Manager will prepare a git repository
   under <code class="literal">~/openstack</code>. The Cloud Lifecycle Manager provisioning runs the
   <code class="literal">ardana-init-deployer</code> script automatically. This calls
   <code class="literal">ansible-playbook -i hosts/localhost git-00-initialise.yml</code>.
  </p><p>
   As a result, the <code class="literal">~/openstack</code> directory is initialized as
   a git repo (if it is empty). It is initialized with four empty branches:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.4.4.1"><span class="term">ardana</span></dt><dd><p>
      This holds the upstream source code corresponding to the contents of the
      <code class="literal">~/openstack</code> directory on a pristine installation.
      Every source code release that is downloaded from <span class="phrase"><span class="phrase">SUSE</span></span> is applied as
      a fresh commit to this branch. This branch contains no customization by
      the end user.
     </p></dd><dt id="id-1.4.5.5.4.4.2"><span class="term">site</span></dt><dd><p>
      This branch begins life as a copy of the first <code class="literal">ardana</code>
      drop. It is onto this branch that you commit your configuration changes.
      It is the branch most visible to the end user.
     </p></dd><dt id="id-1.4.5.5.4.4.3"><span class="term">ansible</span></dt><dd><p>
      This branch contains the variable definitions generated by the CP that
      our main ansible playbooks need. This includes the
      <code class="literal">verb_hosts</code> file that describes to ansible what servers
      are playing what roles. The <code class="literal">ready-deployment</code> playbook
      takes this output and assembles a <code class="literal">~/scratch</code> directory
      containing the ansible playbooks together with the variable definitions
      in this branch. The result is a working ansible directory
      <code class="literal">~/scratch/ansible/next/ardana/ansible</code> from which the
      main deployment playbooks may be successfully run.
     </p></dd><dt id="id-1.4.5.5.4.4.4"><span class="term">cp-persistent</span></dt><dd><p>
      This branch contains the persistent state that the CP needs to maintain.
      That state is mostly the assignment of IP addresses and roles to
      particular servers. Some operational procedures may involve editing the
      contents of this branch: for example, retiring a machine from service or
      repurposing it.
     </p></dd></dl></div><p>
   Two temporary branches are created and populated at run time:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.4.6.1"><span class="term">staging-ansible</span></dt><dd><p>
      This branch hosts the most recent commit that will be appended to the
      Ansible branch.
     </p></dd><dt id="id-1.4.5.5.4.6.2"><span class="term">staging-cp-persistent</span></dt><dd><p>
      This branch hosts the most recent commit that will be appended to the
      cp-persistent branch.
     </p></dd></dl></div><div id="id-1.4.5.5.4.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The information above provides insight into the workings of the
    configuration processor and the git repository. However, in practice you
    can simply follow the steps below to make configuration changes.
   </p></div></section><section class="sect1" id="updating-configuration-including-default-config" data-id-title="Updating any configuration, including the default configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.2 </span><span class="title-name">Updating any configuration, including the default configuration</span></span> <a title="Permalink" class="permalink" href="#updating-configuration-including-default-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When you need to make updates to a configuration you must:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Check out the <code class="literal">site</code> branch. You may already be on that
     branch. If so, git will tell you that and the command will leave you
     there.
    </p><div class="verbatim-wrap"><pre class="screen">git checkout site</pre></div></li><li class="step"><p>
     Edit the YAML file or files that contain the configuration you want to
     change.
    </p></li><li class="step"><p>
     Commit the changes to the <code class="literal">site</code> branch.
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "your commit message goes here in quotes"</pre></div><p>
     If you want to add a single file to your git repository, you can use the
     command below, as opposed to using <code class="command">git add -A</code>.
    </p><div class="verbatim-wrap"><pre class="screen">git add PATH_TO_FILE</pre></div><p>
     For example, if you made a change to your <code class="command">servers.yml</code>
     file and wanted to only commit that change, you would use this command:
    </p><div class="verbatim-wrap"><pre class="screen">git add ~/openstack/my_cloud/definition/data/servers.yml</pre></div></li><li class="step"><p>
     To produce the required configuration processor output from those changes.
     Review the output files manually if required, run the configuration
     processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     Ready the deployment area
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Run the deployment playbooks from the resulting scratch directory.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></section><section class="sect1" id="git-merge" data-id-title="Resolving Git merge conflicts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.3 </span><span class="title-name">Resolving Git merge conflicts</span></span> <a title="Permalink" class="permalink" href="#git-merge">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When you make changes, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> attempts to incorporate new or updated
   configuration information on top of your existing environment. However, with
   some changes to your environment, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cannot automatically
   determine whether to keep your changes or drop them in favor of the new or
   updated configurations. This will result in <code class="literal">merge
   conflicts</code>, and it will be up to you to manually resolve the
   conflicts before you can proceed. This is common, but it can be
   confusing. Git provides a way to resolve these situations.
  </p><p>
   This section gives an overview of how to approach the issue of merge
   conflicts, showing general procedures for determining where the conflict is
   occurring, alternative methods for resolution, and a fallback procedure for
   the case where the resolution goes wrong and you need to start changes from
   the beginning.
  </p><p>
   For a general overview of how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses Git, see the introductory
   article <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>. In particular, note how the
   <code class="literal">site</code> branch is the one most used by the end-user, while
   the <code class="literal">ardana</code> branch contains the "upstream" source code
   corresponding to the contents of the <code class="literal">~/openstack</code>
   directory on a pristine fresh installation.
  </p><section class="sect2" id="id-1.4.5.5.6.5" data-id-title="Identifying the occurrence of a merge conflict"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.1 </span><span class="title-name">Identifying the occurrence of a <code class="literal">merge conflict</code></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you get a <code class="literal">merge conflict</code>, you will observe output
   similar to the following on the console:
  </p><div class="verbatim-wrap"><pre class="screen">Auto-merging ardana/ansible/roles/nova-compute-esx/defaults/main.yml
Auto-merging ardana/ansible/roles/nova-common/templates/nova.conf.j2
<span class="bold"><strong>CONFLICT (content): Merge conflict in ardana/ansible/roles/nova-common/templates/nova.conf.j2</strong></span>
Auto-merging ardana/ansible/roles/nova-cli/tasks/availability_zones.yml
Auto-merging ardana/ansible/roles/nova-api/templates/api-paste.ini.j2</pre></div></section><section class="sect2" id="id-1.4.5.5.6.6" data-id-title="Examining Conflicts"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.2 </span><span class="title-name">Examining Conflicts</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Use <code class="literal">git status</code> to discover the source of the problem:
  </p><div class="verbatim-wrap"><pre class="screen">...
        new file:   tech-preview/entry-scale-kvm-mml/data/swift/rings.yml
        modified:   tech-preview/mid-scale/README.md
        modified:   tech-preview/mid-scale/data/control_plane.yml

Unmerged paths:
  (use "git add/rm &lt;file&gt;..." as appropriate to mark resolution)

        <span class="bold"><strong>both modified:   ardana/ansible/roles/nova-common/templates/nova.conf.j2</strong></span></pre></div><p>
   Edit the file
   <code class="literal">ardana/ansible/roles/nova-common/templates/nova.conf.j2</code>
   to see the conflict markers:
  </p><div class="verbatim-wrap"><pre class="screen">[neutron]
admin_auth_url = {{ neutron_admin_auth_url }}
admin_password = {{ neutron_admin_password }}
admin_tenant_name = {{ neutron_admin_tenant_name }}
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>&lt;&lt;&lt;&lt;&lt;&lt;&lt;HEAD
metadata_proxy_shared_secret = top_secret_password111
=======
metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ardana</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
service_metadata_proxy = True</pre></div><p>
   This indicates that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>is trying to set the value of
   <code class="literal">metadata_proxy_shared_secret</code> to be <code class="literal">{{
   neutron_metadata_proxy_shared_secret }}</code> whereas previously you
   have set its value to <code class="literal">top_secret_password111</code>.
  </p></section><section class="sect2" id="id-1.4.5.5.6.7" data-id-title="Examining differences between your current version and the previous upstream version"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.3 </span><span class="title-name">Examining differences between your current version and the previous upstream version</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Typically, the previous upstream version will be the last-but-one commit on
   the <code class="literal">ardana</code> branch. This version will have been created
   during the initial installation of your cloud (or perhaps during a previous
   upgrade or configuration change). So in total, there are three significant
   versions of the file:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The previous "upstream" version on the <code class="literal">ardana</code> branch.
    </p></li><li class="listitem"><p>
     Your current version on the <code class="literal">site</code> branch.
    </p></li><li class="listitem"><p>
     The new "upstream" version on the <code class="literal">ardana</code> branch.
    </p></li></ul></div><p>
   You can identify the commit number of the previous "upstream" version using
   <code class="literal">git merge-base</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git merge-base ardana site
<span class="bold"><strong>2eda1df48e2822533c50f80f5bfd7a9d788bdf73</strong></span></pre></div><p>
   And then use <code class="literal">git log</code> to see where this commit occurs in
   the history of the <code class="literal">ardana</code> branch.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git log ardana --
commit 22cfa83f3526baf30b3697e971a712930f86f611
Author: Openstack git user &lt;openstack@example.com&gt;
Date:   Mon Jan 18 00:30:33 2018 +0000

    New drop

commit <span class="bold"><strong>2eda1df48e2822533c50f80f5bfd7a9d788bdf73</strong></span>
Author: Openstack git user &lt;openstack@example.com&gt;
Date:   Sun Jan 17 19:14:01 2018 +0000

    New drop</pre></div><p>
   In this instance, we can see that the relevant commit is in fact the
   last-but-one commit. We can use the simplified name
   <code class="literal">ardana^1</code> to identify that commit.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff <span class="bold"><strong>ardana^1</strong></span> HEAD -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   In the diff output, you should be able to see how you changed the value for
   <code class="literal">metadata_proxy_shared_secret</code> from
   <code class="literal">unset</code> to <code class="literal">top_secret_password111</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>diff --git a/ardana/ansible/roles/nova-common/templates/nova.conf.j2 b/ardana/ansible/roles/nova-common/templates/nova.conf.j2
index 597a982..05cb07c 100644
--- a/ardana/ansible/roles/nova-common/templates/nova.conf.j2
+++ b/ardana/ansible/roles/nova-common/templates/nova.conf.j2
@@ -132,7 +132,7 @@ admin_auth_url = {{ neutron_admin_auth_url }}
 admin_password = {{ neutron_admin_password }}
 admin_tenant_name = {{ neutron_admin_tenant_name }}
 admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>-metadata_proxy_shared_secret = unset
+metadata_proxy_shared_secret = top_secret_password111</strong></span>
 neutron_auth_strategy = keystone
 neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
 service_metadata_proxy = True</pre></div></section><section class="sect2" id="id-1.4.5.5.6.8" data-id-title="Examining differences between your current version and the new upstream version"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.4 </span><span class="title-name">Examining differences between your current version and the new upstream version</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.8">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To compare your change with the new upstream version from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> on
   the <code class="literal">ardana</code> branch you can use <code class="literal">git diff HEAD
   ardana -- &lt;&lt;path/to/file&gt;&gt;</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff HEAD ardana -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   In the extract of output below, you can see your value
   <code class="literal">top_secret_password111</code> and the new value <code class="literal">{{
   neutron_metadata_proxy_shared_secret }}</code> that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> wants
   to set.
  </p><div class="verbatim-wrap"><pre class="screen">..
 admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>-metadata_proxy_shared_secret = top_secret_password111
+metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}</strong></span>
 neutron_auth_strategy = keystone
 neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt</pre></div></section><section class="sect2" id="id-1.4.5.5.6.9" data-id-title="Examining differences between the new upstream version and the previous upstream version"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.5 </span><span class="title-name">Examining differences between the new upstream version and the previous upstream version</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.9">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To compare the new upstream version from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> with the previous
   upstream version from your initial install (or previous change):
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff $(git merge-base ardana HEAD) ardana -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   In the extract of output below, you can see the new upstream value
   <code class="literal">{{ neutron_metadata_proxy_shared_secret }}</code> that
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> wants to set and the previous upstream value
   <code class="literal">unset</code>.
  </p><div class="verbatim-wrap"><pre class="screen"> admin_password = {{ neutron_admin_password }}
 admin_tenant_name = {{ neutron_admin_tenant_name }}
 admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>-metadata_proxy_shared_secret = unset
+metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}</strong></span>
 neutron_auth_strategy = keystone
 neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt</pre></div></section><section class="sect2" id="id-1.4.5.5.6.10" data-id-title="Using stage markers to view clean versions of files (without conflict markers)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.6 </span><span class="title-name">Using <code class="literal">stage markers</code> to view clean versions of files (without conflict markers)</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.10">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can use the <code class="literal">git show</code> command with stage markers to
   view files without having conflict markers embedded in them. Stage 1 is the
   previous upstream version (<span class="bold"><strong>:1</strong></span>), stage 2 is
   your current version (<span class="bold"><strong>:2</strong></span>) while stage 3 is
   the new upstream version (<span class="bold"><strong>:3</strong></span>).
  </p><p>
   <span class="bold"><strong>Stage 1</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:1</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><div class="verbatim-wrap"><pre class="screen">...
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>metadata_proxy_shared_secret = unset</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
...</pre></div><p>
   <span class="bold"><strong>Stage 2</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:2</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><div class="verbatim-wrap"><pre class="screen">...
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>metadata_proxy_shared_secret = top_secret_password111</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
...</pre></div><p>
   <span class="bold"><strong>Stage 3</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:3</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><div class="verbatim-wrap"><pre class="screen">...
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
...</pre></div></section><section class="sect2" id="id-1.4.5.5.6.11" data-id-title="Resolving the conflict"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.7 </span><span class="title-name">Resolving the conflict</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.11">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   There are two approaches to resolving the conflict:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Edit the merged file containing the conflict markers, keeping the change
     you want to preserve and removing the conflict markers and any changes you
     want to discard.
    </p></li><li class="listitem"><p>
     Take the new upstream version of the file and re-apply any changes you
     would like to keep from your current version.
    </p></li></ol></div></section><section class="sect2" id="id-1.4.5.5.6.12" data-id-title="Resolving the conflict - editing the file containing the conflict markers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.8 </span><span class="title-name">Resolving the conflict - editing the file containing the conflict markers</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.12">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Edit the file
   <code class="literal">ardana/ansible/roles/nova-common/templates/nova.conf.j2</code>
   and if you want to maintain your change, then delete the lines in bold
   below:
  </p><div class="verbatim-wrap"><pre class="screen">[neutron]
admin_auth_url = {{ neutron_admin_auth_url }}
admin_password = {{ neutron_admin_password }}
admin_tenant_name = {{ neutron_admin_tenant_name }}
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>&lt;&lt;&lt;&lt;&lt;&lt;&lt;HEAD</strong></span>
metadata_proxy_shared_secret = top_secret_password111
<span class="bold"><strong>=======
metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ardana</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
service_metadata_proxy = True</pre></div><p>
   Your file should now look like this:
  </p><div class="verbatim-wrap"><pre class="screen">[neutron]
admin_auth_url = {{ neutron_admin_auth_url }}
admin_password = {{ neutron_admin_password }}
admin_tenant_name = {{ neutron_admin_tenant_name }}
admin_username = {{ neutron_admin_username }}
metadata_proxy_shared_secret = top_secret_password111
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
service_metadata_proxy = True</pre></div></section><section class="sect2" id="id-1.4.5.5.6.13" data-id-title="Resolving the conflict - re-applying your changes to new upstream version"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.9 </span><span class="title-name">Resolving the conflict - re-applying your changes to new upstream version</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.13">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Create a copy of the new upstream version (see Stage 3 above) in your
   working directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:3</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2 &gt; \
ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   Now edit the file
   <code class="literal">ardana/ansible/roles/nova-common/templates/nova.conf.j2</code>
   and manually re-apply the changes you want.
  </p></section><section class="sect2" id="id-1.4.5.5.6.14" data-id-title="Completing the merge procedure"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.10 </span><span class="title-name">Completing the merge procedure</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.14">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You may want to check that the changes you have applied are correct. Compare
   the new upstream version with the version in your working directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff ardana -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   If you are happy with the resolution, you can stage your changes using:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git add ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   Apply the above steps to all the merge conflicts you encounter, and when you
   have them resolved to your satisfaction, complete the merge:
  </p><div class="verbatim-wrap"><pre class="screen">git commit -m "complete merge"</pre></div></section><section class="sect2" id="id-1.4.5.5.6.15" data-id-title="Recovering from Errors"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.11 </span><span class="title-name">Recovering from Errors</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.6.15">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-using_git.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you make a mistake during the resolution process, you can return your
   working directory to a clean copy of the <code class="literal">site</code> branch
   using:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git reset --hard</pre></div><p>
   If the new upstream version contains files that did not exist in the previous
   version, these files will be left behind - you can see them using
   <code class="literal">git status</code>. To clean up these files, remove them and then
   reset:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git rm -rf ardana
<code class="prompt user">tux &gt; </code>git reset --hard</pre></div><p>
   Alternatively, you can use <code class="literal">git stash</code> to save these files
   to a transient stash queue.
  </p></section></section></section><section class="chapter" id="install-standalone" data-id-title="Installing a Stand-Alone Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">11 </span><span class="title-name">Installing a Stand-Alone Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#install-standalone">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="id-1.4.5.6.2" data-id-title="Important Notes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.1 </span><span class="title-name">Important Notes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 1. Overview">Chapter 1, <em>Overview</em></a>.
    </p></li><li class="listitem"><p>
     Review the <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span> that we have listed.
    </p></li><li class="listitem"><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem"><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem"><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting-installation" title="Chapter 23. Troubleshooting the Installation">Chapter 23, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem"><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For Swift, refer to <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.)
    </p></li><li class="listitem"><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>.
    </p></li><li class="listitem"><p>
     The terms deployer and Cloud Lifecycle Manager are used interchangeably. They refer to the
     same nodes in your cloud environment.
    </p></li><li class="listitem"><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem"><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem"><p>
     When you attach a Cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li><li class="listitem"><p>
     The installation process will create several <span class="productname">OpenStack</span> roles. Not all roles
     will be relevant for a cloud with Swift only, but they will not cause
     problems.
    </p></li></ul></div></section><section class="sect1" id="id-1.4.5.6.3" data-id-title="Before You Start"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.2 </span><span class="title-name">Before You Start</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step"><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step"><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="3.4. Creating a User">Section 3.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable">CLOUD</em> with your user name
       choice.
      </p></li><li class="step"><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step"><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li><li class="step"><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></section><section class="sect1" id="id-1.4.5.6.4" data-id-title="Configuring Your Environment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.3 </span><span class="title-name">Configuring Your Environment</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      You have already configured an input model for a stand-alone deployer in
      a previous step (<a class="xref" href="#preparing-standalone" title="Chapter 8. Preparing for Stand-Alone Deployment">Chapter 8, <em>Preparing for Stand-Alone Deployment</em></a>). Now that input
      model needs to be moved into the setup directory.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/entry-scale-kvm-stand-alone-deployer/* \
~/openstack/my_cloud/definition/</pre></div></li><li class="step"><p><span class="step-optional">(Optional)</span> 
      You can use the <code class="literal">ardanaencrypt.py</code> script to encrypt
      your IPMI passwords. This script uses OpenSSL.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Change to the Ansible directory:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="step"><p>
        Enter the encryption key into the following environment variable:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step"><p>
        Run the python script below and follow the instructions. Enter a
        password that you want to encrypt.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./ardanaencrypt.py</pre></div></li><li class="step"><p>
        Take the string generated and place it in the
        <code class="literal">ilo-password</code> field in your
        <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
        file, remembering to enclose it in quotes.
       </p></li><li class="step"><p>
        Repeat the above for each server.
       </p><div id="id-1.4.5.6.4.2.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
         Before you run any playbooks, remember that you need to export the
         encryption key in the following environment variable: <code class="literal">export
         ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
        </p></div></li></ol></li><li class="step"><p>
      Commit your configuration to the local git repo (<a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>), as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div><div id="id-1.4.5.6.4.2.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       This step needs to be repeated any time you make changes to your
       configuration files before you move on to the following steps. See <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a> for more information.
      </p></div></li></ol></div></div></section><section class="sect1" id="id-1.4.5.6.5" data-id-title="Running the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.4 </span><span class="title-name">Running the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="23.2. Issues while Updating Configuration Files">Section 23.2, “Issues while Updating Configuration Files”</a>.
  </p></section><section class="sect1" id="id-1.4.5.6.6" data-id-title="Configuring TLS"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.5 </span><span class="title-name">Configuring TLS</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.6.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    This section is optional, but recommended, for a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation.
   </p></div><p>
   After you run the configuration processor the first time, the IP addresses
   for your environment will be generated and populated in the
   <code class="filename">~/openstack/my_cloud/info/address_info.yml</code> file. At
   this point, consider whether to configure TLS and set up an SSL certificate
   for your environment. Please read <a class="xref" href="#tls30" title="Chapter 29. Configuring Transport Layer Security (TLS)">Chapter 29, <em>Configuring Transport Layer Security (TLS)</em></a> before proceeding
   for how to achieve this.
  </p></section><section class="sect1" id="id-1.4.5.6.7" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.6 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step"><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.4.5.6.7.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step"><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="23.3. Issues while Deploying the Cloud">Section 23.3, “Issues while Deploying the Cloud”</a>.
  </p></section><section class="sect1" id="id-1.4.5.6.8" data-id-title="Installing OpenStack Assets on the Stand-alone Deployer"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.7 </span><span class="title-name">Installing <span class="productname">OpenStack</span> Assets on the Stand-alone Deployer</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <span class="productname">OpenStack</span> CLI and <span class="productname">OpenStack</span> clients will not be installed
   automatically. If you require access to these clients, you will need to
   follow the procedure below to add the appropriate software.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     [OPTIONAL] To confirm that <span class="productname">OpenStack</span> clients have not been installed,
     connect to your stand-alone deployer and try to use the OpenStack CLI:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc
<span class="bold"><strong><code class="prompt user">ardana &gt; </code>openstack project list</strong></span>

-bash: openstack: command not found</pre></div></li><li class="step"><p>
     Edit the configuration file containing details of your Control Plane,
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    </p></li><li class="step"><p>
     Locate the stanza for the cluster where you want to install the
     client(s). This will look like the following extract:
    </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager</pre></div></li><li class="step"><p>
     Choose the client(s) you wish to install from the following list of
     available clients:
    </p><div class="verbatim-wrap"><pre class="screen"> - barbican-client
 - ceilometer-client
 - cinder-client
 - designate-client
 - glance-client
 - heat-client
 - ironic-client
 - keystone-client
 - magnum-client
 - manila-client
 - monasca-client
 - neutron-client
 - nova-client
 - ntp-client
 - octavia-client
 - openstack-client
 - swift-client</pre></div></li><li class="step"><p>
     Add the client(s) to the list of <code class="literal">service-components</code> -
     in the following example, several <span class="productname">OpenStack</span> clients are added to the
     stand-alone deployer:
      </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager
            <span class="bold"><strong>- openstack-client
            - ceilometer-client
            - cinder-client
            - designate-client
            - glance-client
            - heat-client
            - ironic-client
            - keystone-client
            - neutron-client
            - nova-client
            - swift-client
            - monasca-client
            - barbican-client
</strong></span></pre></div></li><li class="step"><p>
     Commit the configuration changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Add explicit client service deployment"</pre></div></li><li class="step"><p>
     Run the configuration processor, followed by the
     <code class="literal">ready-deployment</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" \
  -e rekey=""
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Add the software for the clients using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts clients-upgrade.yml</pre></div></li><li class="step"><p>
     Check that the software has been installed correctly. Using the same test
     that was unsuccessful before, connect to your stand-alone deployer and try
     to use the OpenStack CLI:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
     You should now see a list of projects returned:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code><span class="bold"><strong>openstack project list</strong></span>

+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 076b6e879f324183bbd28b46a7ee7826 | kronos           |
| 0b81c3a9e59c47cab0e208ea1bb7f827 | backup           |
| 143891c2a6094e2988358afc99043643 | octavia          |
| 1d3972a674434f3c95a1d5ed19e0008f | glance-swift     |
| 2e372dc57cac4915bf06bbee059fc547 | glance-check     |
| 383abda56aa2482b95fb9da0b9dd91f4 | monitor          |
| 606dd3b1fa6146668d468713413fb9a6 | swift-monitor    |
| 87db9d1b30044ea199f0293f63d84652 | admin            |
| 9fbb7494956a483ca731748126f50919 | demo             |
| a59d0c682474434a9ddc240ddfe71871 | services         |
| a69398f0f66a41b2872bcf45d55311a7 | swift-dispersion |
| f5ec48d0328d400992c1c5fb44ec238f | cinderinternal   |
+----------------------------------+------------------+</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.5.6.9" data-id-title="Post-Installation Verification and Administration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.8 </span><span class="title-name">Post-Installation Verification and Administration</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.9">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_standalone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud-verification" title="Chapter 26. Cloud Verification">Chapter 26, <em>Cloud Verification</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall-checklist" title="Chapter 32. Other Common Post-Installation Tasks">Chapter 32, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></section></section><section class="chapter" id="install-kvm" data-id-title="Installing Mid-scale and Entry-scale KVM"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">12 </span><span class="title-name">Installing Mid-scale and Entry-scale KVM</span></span> <a title="Permalink" class="permalink" href="#install-kvm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="sec-kvm-important-notes" data-id-title="Important Notes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.1 </span><span class="title-name">Important Notes</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-important-notes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 1. Overview">Chapter 1, <em>Overview</em></a>.
    </p></li><li class="listitem"><p>
     Review the <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span> that we have listed.
    </p></li><li class="listitem"><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem"><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem"><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting-installation" title="Chapter 23. Troubleshooting the Installation">Chapter 23, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem"><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For Swift, refer to <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.)
    </p></li><li class="listitem"><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>.
    </p></li><li class="listitem"><p>
     The terms deployer and Cloud Lifecycle Manager are used interchangeably. They refer to the
     same nodes in your cloud environment.
    </p></li><li class="listitem"><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem"><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem"><p>
     When you attach a Cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li><li class="listitem"><p>
     The installation process will create several <span class="productname">OpenStack</span> roles. Not all roles
     will be relevant for a cloud with Swift only, but they will not cause
     problems.
    </p></li></ul></div></section><section class="sect1" id="sec-kvm-prereqs" data-id-title="Before You Start"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.2 </span><span class="title-name">Before You Start</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-prereqs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step"><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step"><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="3.4. Creating a User">Section 3.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable">CLOUD</em> with your user name
       choice.
      </p></li><li class="step"><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step"><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li><li class="step"><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></section><section class="sect1" id="sec-kvm-configuration" data-id-title="Configuring Your Environment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.3 </span><span class="title-name">Configuring Your Environment</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   During the configuration phase of the installation you will be making
   modifications to the example configuration input files to match your cloud
   environment. You should use the <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>
   documentation for detailed information on how to do this. There is also a
   <code class="filename">README.md</code> file included in each of the example
   directories on the Cloud Lifecycle Manager that has useful information about the models.
  </p><p>
   In the steps below we show how to set up the directory structure with the
   example input files as well as use the optional encryption methods for your
   sensitive data.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set up your configuration files, as follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment.
      </p><p>
       For example, if you want to use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Mid-scale KVM model,
       you can use this command to copy the files to your cloud definition
       directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/mid-scale-kvm/* \
~/openstack/my_cloud/definition/</pre></div><p>
       If you want to use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM model, you can use
       this command to copy the files to your cloud definition directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/entry-scale-kvm/* \
~/openstack/my_cloud/definition/</pre></div></li><li class="step"><p>
       Begin inputting your environment information into the configuration
       files in the <code class="filename">~/openstack/my_cloud/definition</code>
       directory.
      </p></li></ol></li><li class="step"><p><span class="step-optional">(Optional)</span> 
     You can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="step"><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step"><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./ardanaencrypt.py</pre></div></li><li class="step"><p>
       Take the string generated and place it in the
       <code class="literal">ilo-password</code> field in your
       <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
       file, remembering to enclose it in quotes.
      </p></li><li class="step"><p>
       Repeat the above for each server.
      </p><div id="id-1.4.5.7.4.4.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Before you run any playbooks, remember that you need to export the
        encryption key in the following environment variable: <code class="literal">export
        ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
       </p></div></li></ol></li><li class="step"><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div><div id="id-1.4.5.7.4.4.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      This step needs to be repeated any time you make changes to your
      configuration files before you move on to the following steps. See
      <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a> for more information.
     </p></div></li></ol></div></div></section><section class="sect1" id="sec-kvm-provision" data-id-title="Provisioning Your Baremetal Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.4 </span><span class="title-name">Provisioning Your Baremetal Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-provision">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><section class="sect2" id="id-1.4.5.7.5.3" data-id-title="Using Third Party Baremetal Installers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.4.1 </span><span class="title-name">Using Third Party Baremetal Installers</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.7.5.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem"><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem"><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem"><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem"><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem"><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle">Running the Configuration Processor</em>.
   </p></section><section class="sect2" id="id-1.4.5.7.5.4" data-id-title="Using the Automated Operating System Installation Provided by SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.4.2 </span><span class="title-name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.7.5.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><section class="sect3" id="id-1.4.5.7.5.4.3" data-id-title="Deploying Cobbler"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.4.2.1 </span><span class="title-name">Deploying Cobbler</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.7.5.4.3">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you cannot reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div id="id-1.4.5.7.5.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step"><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></section><section class="sect3" id="id-1.4.5.7.5.4.4" data-id-title="Imaging the Nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.4.2.2 </span><span class="title-name">Imaging the Nodes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.7.5.4.4">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Powers down the nodes to be installed
      </p></li><li class="step"><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step"><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step"><p>
       Waits for the nodes to power themselves down (this indicates a
       successful install). This can take some time.
      </p></li><li class="step"><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step"><p>
       Waits for the nodes to be reachable by SSH and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     Deploying nodes has been automated in the Cloud Lifecycle Manager and requires the
     following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       All of your nodes using SLES must already be installed, either
       manually or via Cobbler.
      </p></li><li class="listitem"><p>
       Your input model should be configured for your SLES nodes, according
       to the instructions at <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
      </p></li><li class="listitem"><p>
       You should have run the configuration processor and the
       <code class="filename">ready-deployment.yml</code> playbook.
      </p></li></ul></div><p>
     Execute the following steps to re-image one or more nodes after you have
     run the <code class="filename">ready-deployment.yml</code> playbook.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Run the following playbook, specifying your SLES nodes using the
       nodelist. This playbook will reconfigure Cobbler for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e \
      nodelist=node1[,node2,node3]</pre></div></li><li class="step"><p>
       Re-image the node(s) with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
      -e nodelist=node1[,node2,node3]</pre></div></li></ol></div></div><p>
     If a nodelist is not specified then the set of nodes in Cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></section></section></section><section class="sect1" id="sec-kvm-config-processor" data-id-title="Running the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.5 </span><span class="title-name">Running the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-config-processor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="23.2. Issues while Updating Configuration Files">Section 23.2, “Issues while Updating Configuration Files”</a>.
  </p></section><section class="sect1" id="sec-kvm-security" data-id-title="Configuring TLS"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.6 </span><span class="title-name">Configuring TLS</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-security">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.7.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    This section is optional, but recommended, for a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation.
   </p></div><p>
   After you run the configuration processor the first time, the IP addresses
   for your environment will be generated and populated in the
   <code class="filename">~/openstack/my_cloud/info/address_info.yml</code> file. At
   this point, consider whether to configure TLS and set up an SSL certificate
   for your environment. Please read <a class="xref" href="#tls30" title="Chapter 29. Configuring Transport Layer Security (TLS)">Chapter 29, <em>Configuring Transport Layer Security (TLS)</em></a> before proceeding
   for how to achieve this.
  </p></section><section class="sect1" id="sec-kvm-deploy" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.7 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-deploy">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step"><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.4.5.7.8.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step"><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="23.3. Issues while Deploying the Cloud">Section 23.3, “Issues while Deploying the Cloud”</a>.
  </p></section><section class="sect1" id="sec-kvm-configure-backend" data-id-title="Configuring a Block Storage Backend (Optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.8 </span><span class="title-name">Configuring a Block Storage Backend (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-configure-backend">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports multiple block storage backend options. You can use one or
   more of these for setting up multiple block storage backends. Multiple
   volume types are also supported.
  </p><p>
   Whether you have a single or multiple block storage backends defined in your
   <code class="filename">cinder.conf.j2</code> file, you can create one or more volume
   types using the specific attributes associated with the backend. For more
   information, see <a class="xref" href="#config-3par" title="22.1. Configuring for 3PAR Block Storage Backend">Section 22.1, “Configuring for 3PAR Block Storage Backend”</a>.
  </p></section><section class="sect1" id="sec-kvm-post-installation" data-id-title="Post-Installation Verification and Administration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.9 </span><span class="title-name">Post-Installation Verification and Administration</span></span> <a title="Permalink" class="permalink" href="#sec-kvm-post-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_kvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud-verification" title="Chapter 26. Cloud Verification">Chapter 26, <em>Cloud Verification</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall-checklist" title="Chapter 32. Other Common Post-Installation Tasks">Chapter 32, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></section></section><section class="chapter" id="DesignateInstallOverview" data-id-title="DNS Service Installation Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">13 </span><span class="title-name">DNS Service Installation Overview</span></span> <a title="Permalink" class="permalink" href="#DesignateInstallOverview">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-designate_install_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service supports several different backends for domain name
  service. The choice of backend must be included in the deployment model
  before the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install is completed.
 </p><div id="id-1.4.5.8.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
   By default any user in the project is allowed to manage a DNS domain. This
   can be changed by updating the Policy.json file for Designate.
  </p></div><p>
  The backends that are available within the DNS Service are separated into two
  categories, self-contained and external.
 </p><div class="table" id="DNSBackendTable" data-id-title="DNS Backends"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 13.1: </span><span class="title-name">DNS Backends </span></span><a title="Permalink" class="permalink" href="#DNSBackendTable">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-designate_install_overview.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/><col class="c4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Category</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Backend</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Description</th><th style="border-bottom: 1px solid ; ">Recommended For</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Self-contained</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">PowerDNS 3.4.1, BIND 9.9.5</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      All components necessary will be installed and configured as part of
      the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install.
     </td><td style="border-bottom: 1px solid ; ">
      POCs and customers who wish to keep cloud and traditional DNS separated.
     </td></tr><tr><td style="border-right: 1px solid ; ">External</td><td style="border-right: 1px solid ; ">InfoBlox</td><td style="border-right: 1px solid ; ">
      The authoritative DNS server itself is external to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Management
      and configuration is out of scope for the Cloud Lifecycle Manager but remains
      the responsibility of the customer.
     </td><td>
      Customers who wish to integrate with their existing DNS infrastructure.
     </td></tr></tbody></table></div></div><section class="sect1" id="DesignateBIND" data-id-title="Installing the DNS Service with BIND"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.1 </span><span class="title-name">Installing the DNS Service with BIND</span></span> <a title="Permalink" class="permalink" href="#DesignateBIND">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-install_designate_BIND.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service defaults to the BIND back-end if another back-end is
   not configured for domain name service. BIND will be deployed to one or
   more control planes clusters. The following configuration example shows how
   the BIND service is installed.
 </p><section class="sect2" id="sec-bind-configure-back-end" data-id-title="Configuring the Back-end"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.1.1 </span><span class="title-name">Configuring the Back-end</span></span> <a title="Permalink" class="permalink" href="#sec-bind-configure-back-end">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-install_designate_BIND.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Ensure the DNS Service components and the BIND component have been placed
   on a cluster. BIND can be placed on a cluster separate from the other DNS
   service components.
  </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
          region-name: region1

          clusters:
          - name: cluster1
          service-components:
          - lifecycle-manager
          - mariadb
          - ip-cluster
          - apache2
          - ...
          - designate-api
          - designate-central
          - designate-pool-manager
          - designate-zone-manager
          - designate-mdns
          - designate-client
          - bind</pre></div><p>
   <span class="bold"><strong>Updating the Input Model</strong></span>
  </p><p>
   When the back-end is configured, add <code class="literal">bind-ext</code> to the file
   <code class="filename">network_groups.yml</code>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit
     <code class="filename">~/openstack/my_cloud/definition/data/network_groups.yml</code>
     to add <code class="literal">bind-ext</code> to component-endpoints.
    </p><div class="verbatim-wrap"><pre class="screen">name: EXTERNAL-API
hostname-suffix: extapi
component-endpoints:
- bind-ext</pre></div></li><li class="step"><p>
     Save the file.
    </p></li></ol></div></div></section></section><section class="sect1" id="DesignatePowerDNS" data-id-title="Install the DNS Service with PowerDNS"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.2 </span><span class="title-name">Install the DNS Service with PowerDNS</span></span> <a title="Permalink" class="permalink" href="#DesignatePowerDNS">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-install_designate_PowerDNS.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="id-1.4.5.8.7.2" data-id-title="Installing DNS Service with PowerDNS"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.1 </span><span class="title-name">Installing DNS Service with PowerDNS</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.8.7.2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-install_designate_PowerDNS.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service and <span class="bold"><strong>PowerDNS</strong></span> can be
   installed together instead of the default
   <span class="bold"><strong>BIND</strong></span> backend. PowerDNS will be deployed to
   one or more control planes clusters. The following configuration example
   shows how the PowerDNS service is installed.
  </p></section><section class="sect2" id="id-1.4.5.8.7.3" data-id-title="Configure the Backend"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.2 </span><span class="title-name">Configure the Backend</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.8.7.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-install_designate_PowerDNS.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To configure the backend for PowerDNS, follow these steps.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Ensure the DNS Service components and the PowerDNS component have been
     placed on a cluster. PowerDNS may be placed on a separate cluster to the
     other DNS Service components. Ensure the default
     <span class="bold"><strong>bind</strong></span> component has been removed.
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
          region-name: region1

          clusters:
          - name: cluster1
          service-components:
          - lifecycle-manager
          - mariadb
          - ip-cluster
          - apache2
          - ...
          - designate-api
          - designate-central
          - designate-pool-manager
          - designate-zone-manager
          - designate-mdns
          - designate-client
          - powerdns</pre></div></li><li class="step"><p>
     Edit the
     <code class="filename">~/openstack/my_cloud/definitions/data/network_groups.yml</code>
     file to include the powerdns-ext.
    </p><div class="verbatim-wrap"><pre class="screen">- name: EXTERNAL-API
hostname-suffix: extapi
component-endpoints:
 - powerdns-ext
load-balancers:
 - provider: ip-cluster</pre></div></li><li class="step"><p>
     Edit the
     <code class="filename">~/openstack/my_cloud/definitions/data/firewall_rules.yml</code>
     to allow UDP/TCP access.
    </p><div class="verbatim-wrap"><pre class="screen">	    - name: DNSudp
      # network-groups lists the network group names that the rules apply to
      network-groups:
      - EXTERNAL-API
      rules:
      - type: allow
        # range of remote addresses in CIDR format that this rule applies to
        remote-ip-prefix:  0.0.0.0/0
        port-range-min: 53
        port-range-max: 53
        protocol: udp

    - name: DNStcp
      # network-groups lists the network group names that the rules apply to
      network-groups:
      - EXTERNAL-API
      rules:
      - type: allow
        # range of remote addresses in CIDR format that this rule applies to
        remote-ip-prefix:  0.0.0.0/0
        port-range-min: 53
        port-range-max: 53
        protocol: tcp</pre></div></li></ol></div></div><p>
   Please see <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.2 “DNS Service Overview”, Section 9.2.2 “Designate Initial Configuration”</span> for post-installation
   DNS Service configuration.
  </p></section></section><section class="sect1" id="DNS-NS" data-id-title="Configure DNS Domain and NS Records"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.3 </span><span class="title-name">Configure DNS Domain and NS Records</span></span> <a title="Permalink" class="permalink" href="#DNS-NS">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-designate-designate_cfg_dns_ns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To configure the default DNS domain and Name Server records for the default
  pool, follow these steps.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Ensure that <code class="literal">designate_config.yml</code> file is present in the
    <code class="literal">~/openstack/my_cloud/definition/data/designate</code> folder. If
    the file or folder is not present, create the folder and copy
    <code class="literal">designate_config.yml</code> file from one of the example input
    models (for example,
    <code class="filename">~/openstack/examples/entry-scale-kvm/data/designate/designate_config.yml</code>).
   </p></li><li class="step"><p>
    Modify the <span class="bold"><strong>dns_domain</strong></span> and/or
    <span class="bold"><strong>ns_records</strong></span> entries in the
    <code class="filename">designate_config.yml</code> file.
   </p><div class="verbatim-wrap"><pre class="screen">data:
dns_domain: example.org.
ns_records:
    hostname: ns1.example.org.
    priority: 1
    hostname: ns2.example.org.
    priority: 2</pre></div></li><li class="step"><p>
    Edit your input model's <code class="filename">control_plane.yml</code> file to
    include <span class="bold"><strong>DESIGNATE-CONFIG-CP1</strong></span> in
    <span class="bold"><strong>configuration-data</strong></span> section.
   </p><div class="verbatim-wrap"><pre class="screen">control-planes:
   - name: control-plane-1
     region-name: region1
     lifecycle-manager-target
     configuration-data:
        - DESIGNATE-CONFIG-CP1
        - NEUTRON-CONFIG-CP1</pre></div></li><li class="step"><p>
    Continue your cloud deployment by reviewing and committing your changes.
   </p><div class="verbatim-wrap"><pre class="screen">$ git add ~/openstack/my_cloud/definition/data/designate/designate_config.yml
$ git commit -m "Adding DNS Domain and NS Records"</pre></div></li></ol></div></div><div id="id-1.4.5.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   In an entry-scale model (<span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.1 “Entry-Scale Cloud”</span>),
   you will have 3 ns_records since the DNS service runs on all three
   control planes.
  </p><p>
   In a mid-scale model (<span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.3 “Single-Region Mid-Size Model”</span>) or
   dedicated metering, monitoring and logging model
   (<span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.3 “KVM Examples”, Section 9.3.2 “Entry Scale Cloud with Metering and Monitoring Services”</span>), the above example would be
   correct since there are only two controller nodes.
  </p></div></section></section><section class="chapter" id="MagnumOverview" data-id-title="Magnum Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">14 </span><span class="title-name">Magnum Overview</span></span> <a title="Permalink" class="permalink" href="#MagnumOverview">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum Service provides container orchestration engines such as
  Docker Swarm, Kubernetes, and Apache Mesos available as first class
  resources. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum uses Heat to orchestrate an OS image which
  contains Docker and Kubernetes and runs that image in either virtual machines
  or bare metal in a cluster configuration.
 </p><section class="sect1" id="MagnumArchitecture" data-id-title="Magnum Architecture"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.1 </span><span class="title-name">Magnum Architecture</span></span> <a title="Permalink" class="permalink" href="#MagnumArchitecture">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_architecture.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  As an OpenStack API service, Magnum provides Container as a Service (CaaS)
  functionality. Magnum is capable of working with container orchestration
  engines (COE) such as Kubernetes, Docker Swarm, and Apache Mesos. Some
  operations work with a User CRUD (Create, Read, Update, Delete) filter.
 </p><p>
  <span class="bold"><strong>Components</strong></span>
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Magnum API</strong></span>: RESTful API for cluster and
    cluster template operations.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Magnum Conductor</strong></span>: Performs operations on
    clusters requested by Magnum API in an asynchronous manner.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Magnum CLI</strong></span>: Command-line interface to the
    Magnum API.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Etcd (planned, currently using public
    service)</strong></span>: Remote key/value storage for distributed cluster
    bootstrap and discovery.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Kubemaster (in case of Kubernetes COE)</strong></span>:
    One or more VM(s) or baremetal server(s), representing a control plane for
    Kubernetes cluster.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Kubeminion (in case of Kubernetes COE)</strong></span>:
    One or more VM(s) or baremetal server(s), representing a workload node for
    Kubernetes cluster.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Octavia VM aka Amphora (in case of Kubernetes COE
    with enabled load balancer functionality)</strong></span>: One or more VM(s),
    created by LBaaS v2, performing request load balancing for Kubemasters.
   </p></li></ul></div><div class="table" id="table-ebc-x5v-jz" data-id-title="Data"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 14.1: </span><span class="title-name">Data </span></span><a title="Permalink" class="permalink" href="#table-ebc-x5v-jz">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_architecture.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/><col class="c4"/><col class="c5"/><col class="c6"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Data Name</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Confidentiality</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Integrity</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Availability</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Backup?</th><th style="border-bottom: 1px solid ; ">Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Session Tokens</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Confidential</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">High</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Medium</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="border-bottom: 1px solid ; ">Session tokens not stored.</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">System Request</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Confidential</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">High</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Medium</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">No</td><td style="border-bottom: 1px solid ; ">Data in motion or in MQ not stored.</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">MariaDB Database "Magnum"</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Confidential</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">High</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">High</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Yes</td><td style="border-bottom: 1px solid ; ">Contains user preferences. Backed up to Swift daily.</td></tr><tr><td style="border-right: 1px solid ; ">etcd data</td><td style="border-right: 1px solid ; ">Confidential</td><td style="border-right: 1px solid ; ">High</td><td style="border-right: 1px solid ; ">Low</td><td style="border-right: 1px solid ; ">No</td><td>Kubemaster IPs and cluster info. Only used during cluster bootstrap.</td></tr></tbody></table></div></div><div class="figure" id="magnum-service-arch-diagram"><div class="figure-contents"><div class="mediaobject"><a href="images/media-magnum-magnum_service_arch_diagram.png"><img src="images/media-magnum-magnum_service_arch_diagram.png" alt="Service Architecture Diagram for Kubernetes" title="Service Architecture Diagram for Kubernetes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 14.1: </span><span class="title-name">Service Architecture Diagram for Kubernetes </span></span><a title="Permalink" class="permalink" href="#magnum-service-arch-diagram">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_architecture.xml" title="Edit source document"> </a></div></div></div><div class="table" id="table-fst-gxv-jz" data-id-title="Interfaces"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 14.2: </span><span class="title-name">Interfaces </span></span><a title="Permalink" class="permalink" href="#table-fst-gxv-jz">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_architecture.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/><col class="c4"/><col class="c5"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Interface</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Network</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Request</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Response</th><th style="border-bottom: 1px solid ; ">Operation Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">1</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Manage clusters
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> User
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Manage objects that
       belong to current project
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Magnum API
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       CRUD operations on cluster templates and clusters
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">2a</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> AMQP over HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Enqueue messages
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum API
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> RabbitMQ username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> RabbitMQ queue
       read/write operations
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> RabbitMQ
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Notifications issued when cluster CRUD operations requested
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">2b</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> AMQP over HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Read queued messages
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> RabbitMQ username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> RabbitMQ queue
       read/write operations
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> RabbitMQ
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Notifications issued when cluster CRUD operations requested
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">3</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> MariaDB over HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Persist data in MariaDB
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> MariaDB username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Magnum database
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> MariaDB
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Persist cluster/cluster template data, read persisted data
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">4</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Create per-cluster user in
       dedicated domain, no role assignments initially
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Trustee domain admin
       username, password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Manage users in
       dedicated Magnum domain
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Keystone
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Magnum generates user record in a dedicated Keystone domain for each
       cluster
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">5</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Create per-cluster user stack
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Heat
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Magnum creates Heat stack for each cluster
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">6</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> External Network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Bootstrap a cluster in public
       discovery <a class="link" href="https://discovery.etcd.io/" target="_blank">https://discovery.etcd.io/</a>
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Unguessable URL over
       HTTPS. URL is only available to software processes needing it.
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Read and update
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Public discovery service
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Cluster discovery URL
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Create key/value registry of specified size in public storage. This is
       used to stand up a cluster of kubernetes master nodes (refer to
       interface call #12).
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">7</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Create Cinder volumes
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cinder API
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Heat creates Cinder volumes as part of stack.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">8</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Create networks, routers, load
       balancers
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Neutron API
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Heat creates networks, routers, load balancers as part of the stack.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">9</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Create Nova VMs, attach
       volumes
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Nova API
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Heat creates Nova VMs as part of the stack.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">10</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Read pre-configured Glance
       image
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Nova
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Glance API
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Nova uses pre-configured image in Glance to bootstrap VMs.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">11a</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Heat notification
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Heat API
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Heat uses OS::Heat::WaitCondition resource. VM is expected to call Heat
       notification URL upon completion of certain bootstrap operation.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">11b</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Heat notification
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Heat API
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Heat uses OS::Heat::WaitCondition resource. VM is expected to call Heat
       notification URL upon completion of certain bootstrap operation.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">12</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Update cluster member state in
       a public registry at https://discovery.etcd.io
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Unguessable URL over HTTPS
       only available to software processes needing it.
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Read and update
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Public discovery service
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Update key/value pair in a registry created by interface call #6.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">13a</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> VxLAN encapsulated private
       network on the Guest network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Various communications inside
       Kubernetes cluster
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Various calls performed to build Kubernetes clusters, deploy
       applications and put workload
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">13b</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> VxLAN encapsulated private
       network on the Guest network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Various communications inside
       Kubernetes cluster
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Various calls performed to build Kubernetes clusters, deploy
       applications and put workload
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">14</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Guest/External
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Download container images
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> None
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> None
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> External
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Container image data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Kubernetes makes calls to external repositories to download pre-packed
       container images
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">15a</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> External/EXT_VM (Floating IP)
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Octavia load balancer
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       External workload handled by container applications
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">15b</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> Guest
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       External workload handled by container applications
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">15c</td><td style="border-right: 1px solid ; ">
      <p>
       <span class="bold"><strong>Name:</strong></span> External/EXT_VM (Floating IP)
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td>
      <p>
       External workload handled by container applications
      </p>
     </td></tr></tbody></table></div></div><p>
  <span class="bold"><strong>Dependencies</strong></span>
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Keystone
   </p></li><li class="listitem"><p>
    RabbitMQ
   </p></li><li class="listitem"><p>
    MariaDB
   </p></li><li class="listitem"><p>
    Heat
   </p></li><li class="listitem"><p>
    Glance
   </p></li><li class="listitem"><p>
    Nova
   </p></li><li class="listitem"><p>
    Cinder
   </p></li><li class="listitem"><p>
    Neutron
   </p></li><li class="listitem"><p>
    Barbican
   </p></li><li class="listitem"><p>
    Swift
   </p></li></ul></div><p>
  <span class="bold"><strong>Implementation</strong></span>
 </p><p>
  Magnum API and Magnum Conductor are run on the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> controllers (or core
  nodes in case of mid-scale deployments).
 </p><div class="informalfigure"><div class="mediaobject"><a href="images/media-networkImages-Mid-Scale-AllNetworks.png"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" alt="Image" title="Image"/></a></div></div><div class="table" id="security-groups-table" data-id-title="Security Groups"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 14.3: </span><span class="title-name">Security Groups </span></span><a title="Permalink" class="permalink" href="#security-groups-table">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_architecture.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/><col class="c4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Source CIDR/Security Group</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Port/Range</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Protocol</th><th style="border-bottom: 1px solid ; ">Notes</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Any IP</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">22</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">SSH</td><td style="border-bottom: 1px solid ; ">Tenant Admin access</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Any IP/Kubernetes Security Group</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">2379-2380</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HTTPS</td><td style="border-bottom: 1px solid ; ">Etcd Traffic</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Any IP/Kubernetes Security Group</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">6443</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HTTPS</td><td style="border-bottom: 1px solid ; ">kube-apiserver</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Any IP/Kubernetes Security Group</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">7080</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HTTPS</td><td style="border-bottom: 1px solid ; ">kube-apiserver</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Any IP/Kubernetes Security Group</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">8080</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HTTPS</td><td style="border-bottom: 1px solid ; ">kube-apiserver</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Any IP/Kubernetes Security Group</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">30000-32767</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HTTPS</td><td style="border-bottom: 1px solid ; ">kube-apiserver</td></tr><tr><td style="border-right: 1px solid ; ">Any IP/Kubernetes Security Group</td><td style="border-right: 1px solid ; ">any</td><td style="border-right: 1px solid ; ">tenant app specific</td><td>tenant app specific</td></tr></tbody></table></div></div><div class="table" id="network-ports-table" data-id-title="Network Ports"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 14.4: </span><span class="title-name">Network Ports </span></span><a title="Permalink" class="permalink" href="#network-ports-table">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_architecture.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Port/Range</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Protocol</th><th style="border-bottom: 1px solid ; ">Notes</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">22</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">SSH</td><td style="border-bottom: 1px solid ; ">Admin Access</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">9511</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HTTPS</td><td style="border-bottom: 1px solid ; ">Magnum API Access</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">2379-2380</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HTTPS</td><td style="border-bottom: 1px solid ; ">Etcd (planned)</td></tr><tr><td style="border-right: 1px solid ; "> </td><td style="border-right: 1px solid ; "> </td><td> </td></tr></tbody></table></div></div><p>
  Summary of controls spanning multiple components and interfaces:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Audit</strong></span>: Magnum performs logging. Logs are
    collected by the centralized logging service.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Authentication</strong></span>: Authentication via
    Keystone tokens at APIs. Password authentication to MQ and DB using
    specific users with randomly-generated passwords.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Authorization</strong></span>: OpenStack provides admin
    and non-admin roles that are indicated in session tokens. Processes run at
    minimum privilege. Processes run as unique user/group definitions
    (magnum/magnum). Appropriate filesystem controls prevent other processes
    from accessing service’s files. Magnum config file is mode 600. Logs
    written using group adm, user magnum, mode 640. IPtables ensure that no
    unneeded ports are open. Security Groups provide authorization controls
    between in-cloud components.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Availability</strong></span>: Redundant hosts, clustered
    DB, and fail-over provide high availability.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Confidentiality</strong></span>: Network connections over
    TLS. Network separation via VLANs. Data and config files protected via
    filesystem controls. Unencrypted local traffic is bound to localhost.
    Separation of customer traffic on the TUL network via Open Flow (VxLANs).
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Integrity</strong></span>: Network connections over TLS.
    Network separation via VLANs. DB API integrity protected by SQL Alchemy.
    Data and config files are protected by filesystem controls. Unencrypted
    traffic is bound to localhost.
   </p></li></ul></div></section><section class="sect1" id="MagnumInstall" data-id-title="Install the Magnum Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.2 </span><span class="title-name">Install the Magnum Service</span></span> <a title="Permalink" class="permalink" href="#MagnumInstall">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_install.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Installing the Magnum Service can be performed as part of a new
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> environment or can be added to an existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
  environment. Both installations require container management services,
  running in Magnum cluster VMs with access to specific Openstack API
  endpoints. The following TCP ports need to be open in your firewall to allow
  access from VMs to external (public) <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> endpoints.
 </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">TCP Port</th><th style="border-bottom: 1px solid ; ">Service</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">5000</td><td style="border-bottom: 1px solid ; ">Identity</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">8004</td><td style="border-bottom: 1px solid ; ">Heat</td></tr><tr><td style="border-right: 1px solid ; ">9511</td><td>Magnum</td></tr></tbody></table></div><p>
  Magnum is dependent on the following OpenStack services.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Keystone
   </p></li><li class="listitem"><p>
    Heat
   </p></li><li class="listitem"><p>
    Nova KVM
   </p></li><li class="listitem"><p>
    Neutron
   </p></li><li class="listitem"><p>
    Glance
   </p></li><li class="listitem"><p>
    Cinder
   </p></li><li class="listitem"><p>
    Swift
   </p></li><li class="listitem"><p>
    Barbican
   </p></li><li class="listitem"><p>
    LBaaS v2 (Octavia) - <span class="emphasis"><em>optional</em></span>
   </p></li></ul></div><div id="id-1.4.5.9.4.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
   Magnum relies on the public discovery service
   <span class="emphasis"><em>https://discovery.etcd.io</em></span> during cluster bootstrapping
   and update. This service does not perform authentication checks. Although
   running a cluster cannot be harmed by unauthorized changes in the public
   discovery registry, it can be compromised during a cluster update operation.
   To avoid this, it is recommended that you keep your cluster discovery URL
   (that is,
   <code class="literal">https://discovery.etc.io/<em class="replaceable">SOME_RANDOM_ID</em></code>)
   secret.
  </p></div><section class="sect2" id="id-1.4.5.9.4.7" data-id-title="Installing Magnum as part of new SUSE OpenStack Cloud 8 environment"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">14.2.1 </span><span class="title-name">Installing Magnum as part of new <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> environment</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.9.4.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_install.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Magnum components are already included in example <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> models based on
   Nova KVM, such as <span class="bold"><strong>entry-scale-kvm</strong></span>,
   <span class="bold"><strong>entry-scale-kvm-mml</strong></span> and
   <span class="bold"><strong>mid-scale</strong></span>. These models contain the Magnum
   dependencies (see above). You can follow generic installation instruction
   for Mid-Scale and Entry-Scale KM model by using this guide:
   <a class="xref" href="#install-kvm" title="Chapter 12. Installing Mid-scale and Entry-scale KVM">Chapter 12, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p><div id="id-1.4.5.9.4.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      If you modify the cloud model to utilize a dedicated Cloud Lifecycle Manager, add
      <code class="literal">magnum-client</code> item to the list of service components
      for the Cloud Lifecycle Manager cluster.
     </p></li><li class="listitem"><p>
      Magnum needs a properly configured external endpoint. While preparing the
      cloud model, ensure that <code class="literal">external-name</code> setting in
      <code class="literal">data/network_groups.yml</code> is set to valid hostname,
      which can be resolved on DNS server, and a valid TLS certificate is
      installed for your external endpoint. For non-production test
      installations, you can omit <code class="literal">external-name</code>. In test
      installations, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer will use an IP address as a public
      endpoint hostname, and automatically generate a new certificate, signed
      by the internal CA. Please refer to <a class="xref" href="#tls30" title="Chapter 29. Configuring Transport Layer Security (TLS)">Chapter 29, <em>Configuring Transport Layer Security (TLS)</em></a> for more
      details.
     </p></li><li class="listitem"><p>
      To use LBaaS v2 (Octavia) for container management and container
      applications, follow the additional steps to configure LBaaS v2 in the
      guide.
     </p></li></ol></div></div></section><section class="sect2" id="sec-magnum-exist" data-id-title="Adding Magnum to an Existing SUSE OpenStack Cloud Environment"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">14.2.2 </span><span class="title-name">Adding Magnum to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span></span> <a title="Permalink" class="permalink" href="#sec-magnum-exist">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_install.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Adding Magnum to an already deployed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> installation or during
   an upgrade can be achieved by performing the following steps.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Add items listed below to the list of service components in
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
     Add them to clusters which have <code class="literal">server-role</code> set to
     <code class="literal">CONTROLLER-ROLE</code> (entry-scale models) or
     <code class="literal">CORE_ROLE</code> (mid-scale model).
    </p><div class="verbatim-wrap"><pre class="screen">- magnum-api
- magnum-conductor</pre></div></li><li class="step"><p>
     If your environment utilizes a dedicated Cloud Lifecycle Manager, add
     <code class="literal">magnum-client</code> to the list of service components for the
     Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Commit your changes to the local git repository. Run the following
     playbooks as described in <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a> for your
     installation.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">config-processor-run.yml</code>
      </p></li><li class="listitem"><p>
       <code class="literal">ready-deployment.yml</code>
      </p></li><li class="listitem"><p>
       <code class="literal">site.yml</code>
      </p></li></ul></div></li><li class="step"><p>
     Ensure that your external endpoint is configured correctly. The current
     public endpoint configuration can be verified by running the following
     commands on the Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">$ source service.osrc
$ openstack endpoint list --interface=public --service=identity
+-----------+---------+--------------+----------+---------+-----------+------------------------+
| ID        | Region  | Service Name | Service  | Enabled | Interface | URL                    |
|           |         |              | Type     |         |           |                        |
+-----------+---------+--------------+----------+---------+-----------+------------------------+
| d83...aa3 | region0 | keystone     | identity | True    | public    | https://10.245.41.168: |
|           |         |              |          |         |           |             5000/v2.0  |
+-----------+---------+--------------+----------+---------+-----------+------------------------+</pre></div><p>
     Ensure that the endpoint URL is using either an IP address, or a valid
     hostname, which can be resolved on the DNS server. If the URL is using an
     invalid hostname (for example, <code class="literal">myardana.test</code>), follow
     the steps in <a class="xref" href="#tls30" title="Chapter 29. Configuring Transport Layer Security (TLS)">Chapter 29, <em>Configuring Transport Layer Security (TLS)</em></a> to configure a valid external
     endpoint. You will need to update the <code class="literal">external-name</code>
     setting in the <code class="literal">data/network_groups.yml</code> to a valid
     hostname, which can be resolved on DNS server, and provide a valid TLS
     certificate for the external endpoint. For non-production test
     installations, you can omit the <code class="literal">external-name</code>. The
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer will use an IP address as public endpoint hostname, and
     automatically generate a new certificate, signed by the internal CA.
     For more information, see <a class="xref" href="#tls30" title="Chapter 29. Configuring Transport Layer Security (TLS)">Chapter 29, <em>Configuring Transport Layer Security (TLS)</em></a>.
    </p></li><li class="step"><p>
     Ensure that LBaaS v2 (Octavia) is correctly configured. For more
     information, see <a class="xref" href="#OctaviaInstall" title="Chapter 31. Configuring Load Balancer as a Service">Chapter 31, <em>Configuring Load Balancer as a Service</em></a>.
    </p></li></ol></div></div><div id="id-1.4.5.9.4.8.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    By default <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> stores the private key used by Magnum and its
    passphrase in Barbican which provides a secure place to store such
    information. You can change this such that this sensitive information is
    stored on the file system or in the database without encryption. Making
    such a change exposes you to the risk of this information being exposed
    to others. If stored in the database then any database backups, or a
    database breach, could lead to the disclosure of the sensitive
    information. Similarly, if stored unencrypted on the file system this
    information is exposed more broadly than if stored in Barbican.
   </p></div></section></section><section class="sect1" id="MagnumIntegrateDNS" data-id-title="Integrate Magnum with the DNS Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.3 </span><span class="title-name">Integrate Magnum with the DNS Service</span></span> <a title="Permalink" class="permalink" href="#MagnumIntegrateDNS">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-magnum-magnum_integrate_dns.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Integration with DNSaaS may be needed if:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    The external endpoint is configured to use <code class="literal">myardana.test</code>
    as host name and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> front-end certificate is issued for this host name.
   </p></li><li class="listitem"><p>
    Minions are registered using Nova VM names as hostnames Kubernetes API
    server. Most kubectl commands will not work if the VM name (for example,
    <code class="literal">cl-mu3eevqizh-1-b3vifun6qtuh-kube-minion-ff4cqjgsuzhy</code>)
    is not getting resolved at the provided DNS server.
   </p></li></ol></div><p>
  Follow these steps to integrate the Magnum Service with the DNS Service.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Allow connections from VMs to EXT-API
   </p><div class="verbatim-wrap"><pre class="screen">sudo modprobe 8021q
sudo ip link add link virbr5 name vlan108 type vlan id 108
sudo ip link set dev vlan108 up
sudo ip addr add 192.168.14.200/24 dev vlan108
sudo iptables -t nat -A POSTROUTING -o vlan108 -j MASQUERADE</pre></div></li><li class="step"><p>
    Run the designate reconfigure playbook.
   </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/scratch/ansible/next/ardana/ansible/
$ ansible-playbook -i hosts/verb_hosts designate-reconfigure.yml</pre></div></li><li class="step"><p>
    Set up Designate to resolve myardana.test correctly.
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack zone create --email hostmaster@myardana.test myardana.test.
# wait for status to become active
$ EXTERNAL_VIP=$(grep HZN-WEB-extapi /etc/hosts | awk '{ print $1 }')
$ openstack recordset create --records $EXTERNAL_VIP --type A myardana.test. myardana.test.
# wait for status to become active
$ LOCAL_MGMT_IP=$(grep `hostname` /etc/hosts | awk '{ print $1 }')
$ nslookup myardana.test $LOCAL_MGMT_IP
Server:        192.168.14.2
Address:       192.168.14.2#53
Name:          myardana.test
Address:       192.168.14.5</pre></div></li><li class="step"><p>
    If you need to add/override a top level domain record, the following
    example should be used, substituting proxy.example.org with your own real
    address:
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack tld create --name net
$ openstack zone create --email hostmaster@proxy.example.org proxy.example.org.
$ openstack recordset create --records 16.85.88.10 --type A proxy.example.org. proxy.example.org.
$ nslookup proxy.example.org. 192.168.14.2
Server:        192.168.14.2
Address:       192.168.14.2#53
Name:          proxy.example.org
Address:       16.85.88.10</pre></div></li><li class="step"><p>
    Enable propagation of dns_assignment and dns_name attributes to neutron
    ports, as per
    <a class="link" href="https://docs.openstack.org/neutron/pike/admin/config-dns-int.html" target="_blank">https://docs.openstack.org/neutron/pike/admin/config-dns-int.html</a>
   </p><div class="verbatim-wrap"><pre class="screen"># optionally add 'dns_domain = &lt;some domain name&gt;.' to [DEFAULT] section
# of ardana/ansible/roles/neutron-common/templates/neutron.conf.j2
stack@ksperf2-cp1-c1-m1-mgmt:~/openstack$ cat &lt;&lt;-EOF &gt;&gt;ardana/services/designate/api.yml

   provides-data:
   -   to:
       -   name: neutron-ml2-plugin
       data:
       -   option: extension_drivers
           values:
           -   dns
EOF
$ git commit -a -m "Enable DNS support for neutron ports"
$ cd ardana/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml
$ ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
    Enable DNSaaS registration of created VMs by editing the
    <code class="filename">~/openstack/ardana/ansible/roles/neutron-common/templates/neutron.conf.j2</code>
    file. You will need to add <code class="literal">external_dns_driver =
    designate</code> to the <span class="bold"><strong>[DEFAULT]</strong></span>
    section and create a new <span class="bold"><strong>[designate]</strong></span>
    section for the Designate specific configurations.
   </p><div class="verbatim-wrap"><pre class="screen">...
advertise_mtu = False
dns_domain = ksperf.
external_dns_driver = designate
{{ neutron_api_extensions_path|trim }}
{{ neutron_vlan_transparent|trim }}

# Add additional options here

[designate]
url = https://10.240.48.45:9001
admin_auth_url = https://10.240.48.45:35357/v3
admin_username = designate
admin_password = P8lZ9FdHuoW
admin_tenant_name = services
allow_reverse_dns_lookup = True
ipv4_ptr_zone_prefix_size = 24
ipv6_ptr_zone_prefix_size = 116
ca_cert = /etc/ssl/certs/ca-certificates.crt</pre></div></li><li class="step"><p>
    Commit your changes.
   </p><div class="verbatim-wrap"><pre class="screen">$ git commit -a -m "Enable DNSaaS registration of Nova VMs"
[site f4755c0] Enable DNSaaS registration of Nova VMs
1 file changed, 11 insertions(+)</pre></div></li></ol></div></div></section></section><section class="chapter" id="install-esx-ovsvapp" data-id-title="Installing ESX Computes and OVSvAPP"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">15 </span><span class="title-name">Installing ESX Computes and OVSvAPP</span></span> <a title="Permalink" class="permalink" href="#install-esx-ovsvapp">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes the installation step requirements for ESX
  Computes (nova-proxy) and OVSvAPP.
 </p><section class="sect1" id="sec-ironic-prereqs" data-id-title="Before You Start"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.1 </span><span class="title-name">Before You Start</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-prereqs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step"><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step"><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="3.4. Creating a User">Section 3.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable">CLOUD</em> with your user name
       choice.
      </p></li><li class="step"><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step"><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li><li class="step"><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></section><section class="sect1" id="sec-ironic-setup-deployer" data-id-title="Setting Up the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.2 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-setup-deployer">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="id-1.4.5.10.4.2" data-id-title="Installing the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.1 </span><span class="title-name">Installing the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.4.2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the product from:
     </p><ol type="a" class="substeps"><li class="step"><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step"><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step"><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step"><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step"><p>
      Select the location.
     </p></li><li class="step"><p>
      Select the keyboard layout.
     </p></li><li class="step"><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step"><p>
      Create new account:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Enter a username.
       </p></li><li class="step"><p>
        Enter a password.
       </p></li><li class="step"><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step"><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.4.5.10.4.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></section></section><section class="sect1" id="esxi-overview" data-id-title="Overview of ESXi and OVSvApp"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.3 </span><span class="title-name">Overview of ESXi and OVSvApp</span></span> <a title="Permalink" class="permalink" href="#esxi-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   ESXi is a hypervisor developed by VMware for deploying and serving virtual
   computers. OVSvApp is a service VM that allows for leveraging advanced
   networking capabilities that OpenStack Neutron provides. As a result,
   OpenStack features can be added quickly with minimum effort where ESXi is
   used. OVSvApp allows for hosting VMs on ESXi hypervisors together with the
   flexibility of creating port groups dynamically on Distributed Virtual
   Switches (DVS). Network traffic can then be steered through the OVSvApp VM
   which provides VLAN and VXLAN underlying infrastructure for VM communication
   and security features based on OpenStack. More information is available at
   the <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">OpenStack
   wiki</a>.
  </p><p>
   The diagram below illustrates the OVSvApp architecture.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/OVSvApp-Architecture.png"><img src="images/OVSvApp-Architecture.png" alt="Image" title="Image"/></a></div></div></section><section class="sect1" id="id-1.4.5.10.6" data-id-title="VM Appliances Used in OVSvApp Implementation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.4 </span><span class="title-name">VM Appliances Used in OVSvApp Implementation</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The default configuration deployed with the Cloud Lifecycle Manager for VMware ESX hosts uses
   service appliances that run as VMs on the VMware hypervisor. There is one
   OVSvApp VM per VMware ESX host and one nova Compute Proxy per VMware cluster
   or VMware vCenter Server. Instructions for how to create a template for the
   Nova Compute Proxy or ovsvapp can be found at 
   <a class="xref" href="#create-vapp-template" title="15.9. Create a SUSE-based Virtual Appliance Template in vCenter">Section 15.9, “Create a SUSE-based Virtual Appliance Template in vCenter”</a>.
  </p><section class="sect2" id="id-1.4.5.10.6.3" data-id-title="OVSvApp VM"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.1 </span><span class="title-name">OVSvApp VM</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.6.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   OVSvApp implementation is comprised of:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     a service VM called OVSvApp VM hosted on each ESXi hypervisor within a
     cluster, and
    </p></li><li class="listitem"><p>
     two vSphere Distributed vSwitches (DVS).
    </p></li></ul></div><p>
   OVSvApp VMs run SUSE Linux Enterprise and have Open vSwitch installed with an agent called
   <code class="literal">OVSvApp agent</code>. The OVSvApp VM routes network traffic to
   the various VMware tenants and cooperates with the <span class="productname">OpenStack</span> deployment to
   configure the appropriate port and network settings for VMware tenants.
  </p></section><section class="sect2" id="id-1.4.5.10.6.4" data-id-title="Nova Compute Proxy VM"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.2 </span><span class="title-name">Nova Compute Proxy VM</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.6.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Nova compute proxy is the <code class="literal">nova-compute</code> service
   for VMware ESX. Only one instance of this service is required for each ESX
   cluster that is deployed and is communicating with a single VMware vCenter
   server. (This is not like KVM where the <code class="literal">nova-compute</code>
   service must run on every KVM Host.) The single instance of
   <code class="literal">nova-compute</code> service can run in the <span class="productname">OpenStack</span> controller
   node or any other service node in your cloud. The main component of the
   <code class="literal">nova-compute</code> VM is the OVSvApp nova VCDriver that talks
   to the VMware vCenter server to perform VM operations such as VM creation
   and deletion.
  </p></section></section><section class="sect1" id="id-1.4.5.10.7" data-id-title="Prerequisites for Installing ESXi and Managing with vCenter"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.5 </span><span class="title-name">Prerequisites for Installing ESXi and Managing with vCenter</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   ESX/vCenter integration is not fully automatic. vCenter administrators are
   responsible for taking steps to ensure secure operation.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The VMware administrator is responsible for administration of the vCenter
     servers and the ESX nodes using the VMware administration tools. These
     responsibilities include:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Installing and configuring vCenter server
      </p></li><li class="listitem"><p>
       Installing and configuring ESX server and ESX cluster
      </p></li><li class="listitem"><p>
       Installing and configuring shared datastores
      </p></li><li class="listitem"><p>
       Establishing network connectivity between the ESX network and the Cloud Lifecycle Manager
       <span class="productname">OpenStack</span> management network
      </p></li></ul></div></li><li class="listitem"><p>
     The VMware administration staff is responsible for the review of vCenter
     logs. These logs are not automatically included in Cloud Lifecycle Manager <span class="productname">OpenStack</span> centralized logging.
    </p></li><li class="listitem"><p>
     The VMware administrator is responsible for administration of the vCenter
     servers and the ESX nodes using the VMware administration tools.
    </p></li><li class="listitem"><p>
     Logging levels for vCenter should be set appropriately to prevent logging
     of the password for the Cloud Lifecycle Manager <span class="productname">OpenStack</span> message queue.
    </p></li><li class="listitem"><p>
     The vCenter cluster and ESX Compute nodes must be appropriately backed up.
    </p></li><li class="listitem"><p>
     Backup procedures for vCenter should ensure that the file containing the
     Cloud Lifecycle Manager <span class="productname">OpenStack</span> configuration as part of Nova and Cinder volume
     services is backed up and the backups are protected appropriately.
    </p></li><li class="listitem"><p>
     Since the file containing the Cloud Lifecycle Manager <span class="productname">OpenStack</span> message queue password could
     appear in the swap area of a vCenter server, appropriate controls should
     be applied to the vCenter cluster to prevent discovery of the password via
     snooping of the swap area or memory dumps.
    </p></li><li class="listitem"><p>
     It is recommended to have a common shared storage for all the ESXi hosts
     in a particular cluster.
    </p></li><li class="listitem"><p>
     Ensure that you have enabled HA (High Availability) and DRS (Distributed
     Resource Scheduler) settings in a cluster configuration before running the
     installation. DRS and HA are disabled only for OVSvApp. This is done so that it
     does not move to a different host. If you do not enable DRS and HA prior to
     installation then you will not be able to disable it only for OVSvApp. As
     a result DRS or HA could migrate OVSvApp to a different host, which would create a
     network loop.
    </p></li></ul></div><div id="id-1.4.5.10.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   No two clusters should have the same name across datacenters in a given
   vCenter.
  </p></div></section><section class="sect1" id="id-1.4.5.10.8" data-id-title="ESXi/vCenter System Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.6 </span><span class="title-name">ESXi/vCenter System Requirements</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For information about recommended hardware minimums, consult
   <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 3 “Recommended Hardware Minimums for the Example Configurations”, Section 3.2 “Recommended Hardware Minimums for an Entry-scale ESX KVM Model”</span>.
  </p></section><section class="sect1" id="create-esx-cluster" data-id-title="Creating an ESX Cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.7 </span><span class="title-name">Creating an ESX Cluster</span></span> <a title="Permalink" class="permalink" href="#create-esx-cluster">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/create-esx-cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Steps to create an ESX Cluster:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Download the ESXi Hypervisor and vCenter Appliance from the VMware
     website.
    </p></li><li class="step"><p>
     Install the ESXi Hypervisor.
    </p></li><li class="step"><p>
     Configure the Management Interface.
    </p></li><li class="step"><p>
     Enable the CLI and Shell access.
    </p></li><li class="step"><p>
     Set the password and login credentials.
    </p></li><li class="step"><p>
     Extract the vCenter Appliance files.
    </p></li><li class="step"><p>
     The vCenter Appliance offers two ways to install the vCenter. The
     directory <code class="filename">vcsa-ui-installer</code> contains the graphical
     installer. The <code class="filename">vcsa-cli-installer</code> directory contains
     the command line installer. The remaining steps demonstrate using the
     <code class="filename">vcsa-ui-installer</code> installer.
    </p></li><li class="step"><p>
     In the <code class="filename">vcsa-ui-installer</code>, click the
     <span class="guimenu">installer</span> to start installing the vCenter Appliance in
     the ESXi Hypervisor.
    </p></li><li class="step"><p>
     Note the <em class="replaceable">MANAGEMENT IP</em>, <em class="replaceable">USER
     ID</em>, and <em class="replaceable">PASSWORD</em> of the ESXi
     Hypervisor.
    </p></li><li class="step"><p>
     Assign an <em class="replaceable">IP ADDRESS</em>, <em class="replaceable">USER
     ID</em>, and <em class="replaceable">PASSWORD</em> to the vCenter
     server.
    </p></li><li class="step"><p>
     Complete the installation.
    </p></li><li class="step"><p>
     When the installation is finished, point your Web browser to the
     <em class="replaceable">IP ADDRESS</em> of the vCenter. Connect to the
     vCenter by clicking on link in the browser.
    </p></li><li class="step"><p>
     Enter the information for the vCenter you just created: <em class="replaceable">IP
     ADDRESS</em>, <em class="replaceable">USER ID</em>, and
     <em class="replaceable">PASSWORD</em>.
    </p></li><li class="step"><p>
     When connected, configure the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">Datacenter</code>
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Go to <code class="literal">Home</code> &gt; <code class="literal">Inventory</code> &gt;
         <code class="literal">Hosts and Clusters</code>
        </p></li><li class="step"><p>
         Select File &gt; New &gt; Datacenter
        </p></li><li class="step"><p>
         Rename the datacenter
        </p></li></ol></div></div></li><li class="listitem"><p>
       Cluster
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Right-click a datacenter or directory in the vSphere Client and select
         <span class="guimenu">New Cluster</span>.
        </p></li><li class="step"><p>
         Enter a name for the cluster.
        </p></li><li class="step"><p>
         Choose cluster features.
        </p></li></ol></div></div></li><li class="listitem"><p>
       Add a Host to Cluster
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         In the vSphere Web Client, navigate to a datacenter, cluster, or
         directory within a datacenter.
        </p></li><li class="step"><p>
         Right-click the datacenter, cluster, or directory and select
         <span class="guimenu">Add Host</span>.
        </p></li><li class="step"><p>
          Type the IP address or the name of the host and click
          <span class="guimenu">Next</span>.
         </p></li><li class="step"><p>
          Enter the administrator credentials and click <span class="guimenu">Next</span>.
         </p></li><li class="step"><p>
          Review the host summary and click <span class="guimenu">Next</span>.
         </p></li><li class="step"><p>
          Assign a license key to the host.
         </p></li></ol></div></div></li></ul></div></li></ol></div></div></section><section class="sect1" id="config-dvs-pg" data-id-title="Configuring the Required Distributed vSwitches and Port Groups"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.8 </span><span class="title-name">Configuring the Required Distributed vSwitches and Port Groups</span></span> <a title="Permalink" class="permalink" href="#config-dvs-pg">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/config-dvs-pg.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#create-esxi-trunk-dvs" title="15.8.1. Creating ESXi TRUNK DVS and Required Portgroup">Section 15.8.1, “Creating ESXi TRUNK DVS and Required Portgroup”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#create-esxi-mgmt-dvs" title="15.8.2. Creating ESXi MGMT DVS and Required Portgroup">Section 15.8.2, “Creating ESXi MGMT DVS and Required Portgroup”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#config-ansible-playbook" title="15.8.3. Configuring OVSvApp Network Resources Using Ansible-Playbook">Section 15.8.3, “Configuring OVSvApp Network Resources Using Ansible-Playbook”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#config-ovsvapp-python-vsphere" title="15.8.4. Configuring OVSVAPP Using Python-Networking-vSphere">Section 15.8.4, “Configuring OVSVAPP Using Python-Networking-vSphere”</a>
   </p></li></ul></div><p>
  The required Distributed vSwitches (DVS) and port groups can be created by
  using the vCenter graphical user interface (GUI) or by using the command line
  tool provided by <code class="literal">python-networking-vsphere</code>. The vCenter
  GUI is recommended.
 </p><p>
  OVSvApp virtual machines (VMs) give ESX installations the ability to leverage
  some of the advanced networking capabilities and other benefits <span class="productname">OpenStack</span>
  provides. In particular, OVSvApp allows for hosting VMs on ESX/ESXi
  hypervisors together with the flexibility of creating port groups dynamically
  on Distributed Virtual Switch.
 </p><p>
  A port group is a management object for aggregation of multiple ports (on a
  virtual switch) under a common configuration. A VMware port group is used to
  group together a list of ports in a virtual switch (DVS in this section) so
  that they can be configured all at once. The member ports of a port group
  inherit their configuration from the port group, allowing for configuration
  of a port by simply dropping it into a predefined port group.
 </p><p>
  The following sections cover configuring OVSvApp switches on ESX. More
  information about OVSvApp is available at
     <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">https://wiki.openstack.org/wiki/Neutron/Networking-vSphere</a>
 </p><p>
  The diagram below illustrates a typical configuration that uses OVSvApp and
  Distributed vSwitches.
 </p><div class="informalfigure"><div class="mediaobject"><a href="images/ovsapp-dvs-esx.png"><img src="images/ovsapp-dvs-esx.png" alt="Image" title="Image"/></a></div></div><p>
  Detailed instructions are shown in the following sections for four example
  installations and two command line procedures.
 </p><section class="sect2" id="create-esxi-trunk-dvs" data-id-title="Creating ESXi TRUNK DVS and Required Portgroup"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.8.1 </span><span class="title-name">Creating ESXi TRUNK DVS and Required Portgroup</span></span> <a title="Permalink" class="permalink" href="#create-esxi-trunk-dvs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/create-esxi-trunk-dvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The process of creating an ESXi Trunk Distributed vSwitch (DVS) consists of three
    steps: create a switch, add host and physical adapters, and add a port
    group. Use the following detailed instructions to create a trunk DVS and a
    required portgroup. These instructions use a graphical user interface
    (GUI). The GUI menu options may vary slightly depending on the specific version
    of vSphere installed. Command line interface (CLI) instructions are below the GUI
    instructions.
   </p><section class="sect3" id="id-1.4.5.10.10.10.3" data-id-title="Creating ESXi Trunk DVS with vSphere Web Client"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.8.1.1 </span><span class="title-name">Creating ESXi Trunk DVS with vSphere Web Client</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.10.10.3">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/create-esxi-trunk-dvs.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the switch.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Using vSphere webclient, connect to the vCenter server.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Hosts and cluster</span>, right-click on the
        appropriate datacenter. Select <span class="guimenu">Distributed Switch</span>
        &gt; <span class="guimenu">New Distributed Switch</span>.
       </p></li><li class="step"><p>
        Name the switch <code class="literal">TRUNK</code>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Select version 6.0.0 or larger. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Edit settings</span>, lower the number of uplink
        ports to the lowest possible number (0 or 1).  Uncheck <span class="guimenu">Create a
        default port group</span>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Ready to complete</span>, verify the settings are
        correct and click <span class="guimenu">Finish</span>.
       </p></li></ol></li><li class="step"><p>
      Add host and physical adapters.
     </p><ol type="a" class="substeps"><li class="step"><p>
       Under <span class="guimenu">Networking</span> find the DVS named
       <code class="literal">TRUNK</code> you just created. Right-click on it and select
       <span class="guimenu">Manage hosts</span>.
      </p></li><li class="step"><p>
        Under <span class="guimenu">Select task</span>, select <span class="guimenu">Add
        hosts</span>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Click <span class="guimenu">New hosts</span>.
       </p></li><li class="step"><p>
        Select the <code class="literal">CURRENT ESXI HOST</code> and select
        <span class="guimenu">OK</span>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Select network adapter tasks</span>, select
        <span class="guimenu">Manage advanced host settings</span> and
        <span class="bold"><strong>UNCHECK</strong></span> all other boxes. Click
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Advanced host settings</span>, check that the <code class="literal">Maximum
        Number of Ports</code> reads <code class="literal">(auto)</code>. There
        is nothing else to do. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Ready to complete</span>, verify that one and only
        one host is being added and click <span class="guimenu">Finish</span>.
       </p></li></ol></li><li class="step"><p>
      Add port group.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Right-click on the TRUNK DVS that was just created (or modified) and
        select <span class="guimenu">Distributed Port Group</span> &gt; <span class="guimenu">New
        Distributed Port Group</span>.
       </p></li><li class="step"><p>
        Name the port group <code class="literal">TRUNK-PG</code>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Configure settings</span> select:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem"><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem"><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">VLAN trunking</code> with
          range of 1–4094.
         </p></li></ul></div></li><li class="step"><p>
        Check <code class="literal">Customized default policies
        configuration</code>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Security</span> use the following values:
       </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Setting</p></th><th style="border-bottom: 1px solid ; "><p>Value</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>promiscuous mode</p></td><td style="border-bottom: 1px solid ; "><p>accept</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>MAC address changes</p></td><td style="border-bottom: 1px solid ; "><p>reject</p></td></tr><tr><td style="border-right: 1px solid ; "><p>Forged transmits</p></td><td><p>accept</p></td></tr></tbody></table></div></li><li class="step"><p>
        Set <span class="guimenu">Autoexpand</span> to <code class="literal">true</code> (port
        count growing).
       </p></li><li class="step"><p>
        Skip <span class="guimenu">Traffic shaping</span> and click
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Skip <span class="guimenu">Teaming and fail over</span> and click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Skip <span class="guimenu">Monitoring</span> and click
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Miscellaneous</span> there is nothing to be
        done. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Edit additional settings</span> add a description if
        desired. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Ready to complete</span> verify everything is as
        expected and click <span class="guimenu">Finish</span>.
       </p></li></ol></li></ol></div></div></section></section><section class="sect2" id="create-esxi-mgmt-dvs" data-id-title="Creating ESXi MGMT DVS and Required Portgroup"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.8.2 </span><span class="title-name">Creating ESXi MGMT DVS and Required Portgroup</span></span> <a title="Permalink" class="permalink" href="#create-esxi-mgmt-dvs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/create-esxi-mgmt-dvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The process of creating an ESXi Mgmt Distributed vSwitch (DVS) consists of three
    steps: create a switch, add host and physical adapters, and add a port
    group. Use the following detailed instructions to create a mgmt DVS and a
    required portgroup.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the switch.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Using the vSphere webclient, connect to the vCenter server.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Hosts and Cluster</span>, right-click on the
        appropriate datacenter, and select <code class="literal">Distributed
        Switch</code> &gt; <code class="literal">New Distributed Switch</code>
       </p></li><li class="step"><p>
        Name the switch <code class="literal">MGMT</code>. Click 
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Select version 6.0.0 or higher. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Edit settings</span>, select the appropriate number
        of uplinks. The <code class="literal">MGMT</code> DVS is what connects the ESXi
        host to the <span class="productname">OpenStack</span> management network. Uncheck <code class="literal">Create a default
        port group</code>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Ready to complete</span>, verify the settings are
        correct. Click <span class="guimenu">Finish</span>.
       </p></li></ol></li><li class="step"><p>
      Add host and physical adapters to Distributed Virtual Switch.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Under <code class="literal">Networking</code>, find the <code class="literal">MGMT</code>
        DVS you just created. Right-click on it and select <span class="guimenu">Manage
        hosts</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Select task</span>, select <span class="guimenu">Add
        hosts</span>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Click <span class="guimenu">New hosts</span>.
       </p></li><li class="step"><p>
        Select the current ESXi host and select <span class="guimenu">OK</span>. Click 
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Select network adapter tasks</span>, select
        <span class="guimenu">Manage physical adapters</span> and <span class="bold"><strong>UNCHECK</strong></span> all other boxes. Click 
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Manage physical network adapters</span>, click on the
        interface you are using to connect the ESXi to the <span class="productname">OpenStack</span> management
        network. The name is of the form <code class="literal">vmnic#</code> (for
        example, <code class="literal">vmnic0</code>, <code class="literal">vmnic1</code>, etc.). When the
        interface is highlighted, select <span class="guimenu">Assign uplink</span> then
        select the uplink name to assign or auto assign. Repeat the process for
        each uplink physical NIC you will be using to connect to the <span class="productname">OpenStack</span>
        data network.  Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Verify that you understand and accept the impact shown by <span class="guimenu">Analyze
        impact</span>. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Verify that everything is correct and click on
        <span class="guimenu">Finish</span>.
       </p></li></ol></li><li class="step"><p>
      Add MGMT port group to switch.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Right-click on the <code class="literal">MGMT</code> DVS and select
        <span class="guimenu">Distributed Port Group</span> &gt; <span class="guimenu">New Distributed
        Port Group</span>.
       </p></li><li class="step"><p>
        Name the port group <code class="literal">MGMT-PG</code>. Click 
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Configure settings</span>, select:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem"><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem"><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">None</code>
         </p></li></ul></div><p>
        Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Ready to complete</span>, verify that everything is
        as expected and click <span class="guimenu">Finish</span>.
       </p></li></ol></li><li class="step"><p>
      Add GUEST port group to the switch.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Right-click on the DVS (MGMT) that was just created (or modified).
        Select <span class="guimenu">Distributed Port Group</span> &gt; <span class="guimenu">New
        Distributed Port Group</span>.
       </p></li><li class="step"><p>
        Name the port group <code class="literal">GUEST-PG</code>. Click
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Configure settings</span>, select:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static binding</code>
         </p></li><li class="listitem"><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem"><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">VLAN trunking</code> The
          VLAN range corresponds to the VLAN ids being used by the <span class="productname">OpenStack</span>
          underlay. This is the same VLAN range as configured in the
          <code class="filename">neutron.conf</code> configuration file for the Neutron
          server.
         </p></li></ul></div></li><li class="step"><p>
        Select <span class="guimenu">Customize default policies configuration</span>.
        Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Security</span>, use the following settings:
       </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
            <p>
             setting
            </p>
           </th><th style="border-bottom: 1px solid ; ">
            <p>
             value
            </p>
           </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
            <p>
             promiscuous mode
            </p>
           </td><td style="border-bottom: 1px solid ; ">
            <p>
             accept
            </p>
           </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
            <p>
             MAC address changes
            </p>
           </td><td style="border-bottom: 1px solid ; ">
            <p>
             reject
            </p>
           </td></tr><tr><td style="border-right: 1px solid ; ">
            <p>
             Forged transmits
            </p>
           </td><td>
            <p>
             accept
            </p>
           </td></tr></tbody></table></div></li><li class="step"><p>
        Skip <span class="guimenu">Traffic shaping</span> and click
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Teaming and fail over</span>, make changes appropriate
        for your network and deployment.
       </p></li><li class="step"><p>
        Skip <span class="guimenu">Monitoring</span> and click
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Skip <span class="guimenu">Miscellaneous</span> and click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Edit addition settings</span>, add a description if
        desired. Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Ready to complete</span>, verify everything is as
       expected. Click <span class="guimenu">Finish</span>.
       </p></li></ol></li><li class="step"><p>
      Add ESX-CONF port group.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Right-click on the DVS (MGMT) that was just created (or
        modified). Select <span class="guimenu">Distributed Port Group</span> &gt;
        <span class="guimenu">New Distributed Port Group</span>.
       </p></li><li class="step"><p>
        Name the port group <code class="literal">ESX-CONF-PG</code>. Click
        <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Configure settings</span>, select:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem"><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem"><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">None</code>
         </p></li></ul></div><p>
        Click <span class="guimenu">Next</span>.
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem"><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem"><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">None</code>
         </p></li></ul></div><p>
        Click <span class="guimenu">Next</span>.
       </p></li><li class="step"><p>
        Under <span class="guimenu">Ready to complete</span>, verify that everything is
        as expected and click <span class="guimenu">Finish</span>.
       </p></li></ol></li></ol></div></div></section><section class="sect2" id="config-ansible-playbook" data-id-title="Configuring OVSvApp Network Resources Using Ansible-Playbook"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.8.3 </span><span class="title-name">Configuring OVSvApp Network Resources Using Ansible-Playbook</span></span> <a title="Permalink" class="permalink" href="#config-ansible-playbook">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/config-ansible_playbook.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The Ardana ansible playbook
  <code class="filename">neutron-create-ovsvapp-resources.yml</code> can be used to
  create Distributed Virtual Switches and Port Groups on a vCenter cluster.
 </p><p>
  The playbook requires the following inputs:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="literal">vcenter_username</code>
   </p></li><li class="listitem"><p>
    <code class="literal">vcenter_encrypted_password</code>
   </p></li><li class="listitem"><p>
    <code class="literal">vcenter_ip</code>
   </p></li><li class="listitem"><p>
    <code class="literal">vcenter_port</code> (default 443)
   </p></li><li class="listitem"><p>
    <code class="literal">vc_net_resources_location</code> This is the path to a file which
    contains the definition of the resources to be created. The definition is
    in JSON format.
   </p></li></ul></div><p>
  In order to execute the playbook from the Cloud Lifecycle Manager, the
  <code class="literal">python-networking-vsphere</code> package must be installed.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install python-networking-vsphere</pre></div><p>
  Running the playbook:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook neutron-create-ovsvapp-resources.yml \
-i hosts/verb_hosts -vvv -e 'variable_host=localhost
vcenter_username=<em class="replaceable">USERNAME</em>
vcenter_encrypted_password=<em class="replaceable">ENCRYPTED_PASSWORD</em>
vcenter_ip=<em class="replaceable">IP_ADDRESS</em>
vcenter_port=443
vc_net_resources_location=<em class="replaceable">LOCATION_TO_RESOURCE_DEFINITION_FILE</em>
'</pre></div><p>
  The <code class="literal">RESOURCE_DEFINITION_FILE</code> is in JSON format and
  contains the resources to be created.
 </p><p>
  Sample file contents:
 </p><div class="verbatim-wrap"><pre class="screen">{
  "datacenter_name": "DC1",
  "host_names": [
    "192.168.100.21",
    "192.168.100.222"
  ],
  "network_properties": {
    "switches": [
      {
        "type": "dvSwitch",
        "name": "TRUNK",
        "pnic_devices": [],
        "max_mtu": "1500",
        "description": "TRUNK DVS for ovsvapp.",
        "max_ports": 30000
      },
      {
        "type": "dvSwitch",
        "name": "MGMT",
        "pnic_devices": [
          "vmnic1"
        ],
        "max_mtu": "1500",
        "description": "MGMT DVS for ovsvapp. Uses 'vmnic0' to connect to OpenStack Management network",
        "max_ports": 30000
      }
    ],
    "portGroups": [
      {
        "name": "TRUNK-PG",
        "vlan_type": "trunk",
        "vlan_range_start": "1",
        "vlan_range_end": "4094",
        "dvs_name": "TRUNK",
        "nic_teaming": null,
        "allow_promiscuous": true,
        "forged_transmits": true,
        "auto_expand": true,
        "description": "TRUNK port group. Configure as trunk for vlans 1-4094. Default nic_teaming selected."
      },
      {
        "name": "MGMT-PG",
        "dvs_name": "MGMT",
        "nic_teaming": null,
        "description": "MGMT port group. Configured as type 'access' (vlan with vlan_id = 0, default). Default nic_teaming. Promiscuous false, forged_transmits default"
      },
      {
        "name": "GUEST-PG",
        "dvs_name": "GUEST",
        "vlan_type": "MGMT",
        "vlan_range_start": 100,
        "vlan_range_end": 200,
        "nic_teaming": null,
        "allow_promiscuous": true,
        "forged_transmits": true,
        "auto_expand": true,
        "description": "GUEST port group. Configure for vlans 100 through 200."
      },
      {
        "name": "ESX-CONF-PG",
        "dvs_name": "MGMT",
        "nic_teaming": null,
        "description": "ESX-CONF port group. Configured as type 'access' (vlan with vlan_id = 0, default)."
      }
    ]
  }
}</pre></div></section><section class="sect2" id="config-ovsvapp-python-vsphere" data-id-title="Configuring OVSVAPP Using Python-Networking-vSphere"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.8.4 </span><span class="title-name">Configuring OVSVAPP Using Python-Networking-vSphere</span></span> <a title="Permalink" class="permalink" href="#config-ovsvapp-python-vsphere">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/config-ovsvapp-python-vsphere.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Scripts can be used with the <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">Networking-vSphere
  Project</a>. The scripts automate some of the process of configuring OVSvAPP
  from the command line. The following are help entries for two
  of the scripts:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd /opt/repos/networking-vsphere
 <code class="prompt user">tux &gt; </code>ovsvapp-manage-dvs -h
usage: ovsvapp-manage-dvs [-h] [--tcp tcp_port]
                          [--pnic_devices pnic_devices [pnic_devices ...]]
                          [--max_mtu max_mtu]
                          [--host_names host_names [host_names ...]]
                          [--description description] [--max_ports max_ports]
                          [--cluster_name cluster_name] [--create]
                          [--display_spec] [-v]
                          dvs_name vcenter_user vcenter_password vcenter_ip
                          datacenter_name
positional arguments:
  dvs_name              Name to use for creating the DVS
  vcenter_user          Username to be used for connecting to vCenter
  vcenter_password      Password to be used for connecting to vCenter
  vcenter_ip            IP address to be used for connecting to vCenter
  datacenter_name       Name of data center where the DVS will be created
optional arguments:
  -h, --help            show this help message and exit
  --tcp tcp_port        TCP port to be used for connecting to vCenter
  --pnic_devices pnic_devices [pnic_devices ...]
                        Space separated list of PNIC devices for DVS
  --max_mtu max_mtu     MTU to be used by the DVS
  --host_names host_names [host_names ...]
                        Space separated list of ESX hosts to add to DVS
  --description description
                        DVS description
  --max_ports max_ports
                        Maximum number of ports allowed on DVS
  --cluster_name cluster_name
                        Cluster name to use for DVS
  --create              Create DVS on vCenter
  --display_spec        Print create spec of DVS
 -v                    Verbose output</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd /opt/repos/networking-vsphere
 <code class="prompt user">tux &gt; </code>ovsvapp-manage-dvpg -h
usage: ovsvapp-manage-dvpg [-h] [--tcp tcp_port] [--vlan_type vlan_type]
                           [--vlan_id vlan_id]
                           [--vlan_range_start vlan_range_start]
                           [--vlan_range_stop vlan_range_stop]
                           [--description description] [--allow_promiscuous]
                           [--allow_forged_transmits] [--notify_switches]
                           [--network_failover_detection]
                           [--load_balancing {loadbalance_srcid,loadbalance_ip,loadbalance_srcmac,loadbalance_loadbased,failover_explicit}]
                           [--create] [--display_spec]
                           [--active_nics ACTIVE_NICS [ACTIVE_NICS ...]] [-v]
                           dvpg_name vcenter_user vcenter_password vcenter_ip
                           dvs_name
positional arguments:
  dvpg_name             Name to use for creating theDistributed Virtual Port
                        Group (DVPG)
  vcenter_user          Username to be used for connecting to vCenter
  vcenter_password      Password to be used for connecting to vCenter
  vcenter_ip            IP address to be used for connecting to vCenter
  dvs_name              Name of the Distributed Virtual Switch (DVS) to create
                        the DVPG in
optional arguments:
  -h, --help            show this help message and exit
  --tcp tcp_port        TCP port to be used for connecting to vCenter
  --vlan_type vlan_type
                        Vlan type to use for the DVPG
  --vlan_id vlan_id     Vlan id to use for vlan_type='vlan'
  --vlan_range_start vlan_range_start
                        Start of vlan id range for vlan_type='trunk'
  --vlan_range_stop vlan_range_stop
                        End of vlan id range for vlan_type='trunk'
  --description description
                        DVPG description
  --allow_promiscuous   Sets promiscuous mode of DVPG
  --allow_forged_transmits
                        Sets forge transmit mode of DVPG
  --notify_switches     Set nic teaming 'notify switches' to True.
  --network_failover_detection
                        Set nic teaming 'network failover detection' to True
  --load_balancing {loadbalance_srcid,loadbalance_ip,loadbalance_srcmac,loadbalance_loadbased,failover_explicit}
                        Set nic teaming load balancing algorithm.
                        Default=loadbalance_srcid
  --create              Create DVPG on vCenter
  --display_spec        Send DVPG's create spec to OUTPUT
  --active_nics ACTIVE_NICS [ACTIVE_NICS ...]
                        Space separated list of active nics to use in DVPG nic
                        teaming
 -v                    Verbose output</pre></div></section></section><section class="sect1" id="create-vapp-template" data-id-title="Create a SUSE-based Virtual Appliance Template in vCenter"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.9 </span><span class="title-name">Create a SUSE-based Virtual Appliance Template in vCenter</span></span> <a title="Permalink" class="permalink" href="#create-vapp-template">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/create-vapp_template-vcenter.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Download the SLES12-SP3 ISO image
    (<code class="filename">SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso</code>) from <a class="link" href="https://www.suse.com/products/server/download/" target="_blank">https://www.suse.com/products/server/download/</a>. You need to
    sign in or create a SUSE customer service account before downloading.
   </p></li><li class="step"><p>
    Create a new Virtual Machine in vCenter Resource Pool.
   </p></li><li class="step"><p>
    Configure the Storage selection.
   </p></li><li class="step"><p>
    Configure the Guest Operating System.
   </p></li><li class="step"><p>
    Create a Disk.
   </p></li><li class="step"><p>
    Ready to Complete.
   </p></li><li class="step"><p>
    Edit Settings before booting the VM with additional Memory, typically
    16GB or 32GB, though large scale environments may require larger memory
    allocations.
   </p></li><li class="step"><p>
    Edit Settings before booting the VM with additional Network Settings.
    Ensure there are four network adapters, one each for TRUNK, MGMT, 
    ESX-CONF, and GUEST.
   </p></li><li class="step"><p>
    Attach the ISO image to the DataStore.
   </p></li><li class="step"><p>
    Configure the 'disk.enableUUID=TRUE' flag in the General - Advanced
    Settings.
   </p></li><li class="step"><p>
    After attaching the CD/DVD drive with the ISO image and completing the
    initial VM configuration, power on the VM by clicking the Play button on
    the VM's summary page.
   </p></li><li class="step"><p>
    Click <span class="guimenu">Installation</span> when the VM boots from the console
    window.
   </p></li><li class="step"><p>
    Accept the License agreement, language and Keyboard selection.
   </p></li><li class="step"><p>
    Select the System Role to Xen Virtualization Host.
   </p></li><li class="step"><p>
    Select the 'Proposed Partitions' in the Suggested Partition screen.
   </p></li><li class="step"><p>
    Edit the Partitions to select the 'LVM' Mode and then select the 'ext4'
    filesystem type.
   </p></li><li class="step"><p>
    Increase the size of the root partition from 10GB to 60GB.
   </p></li><li class="step"><p>
    Create an additional logical volume to accommodate the LV_CRASH volume
    (15GB). Do not mount the volume at this time, it will be used later.
   </p></li><li class="step"><p>
    Configure the Admin User/Password and User name.
   </p></li><li class="step"><p>
    Installation Settings (Disable Firewall and enable SSH).
   </p></li><li class="step"><p>
    The operating system will be successfully installed and the VM will reboot.
   </p></li><li class="step"><p>
    Check that the contents of the ISO files are copied to the locations shown
    below on your Cloud Lifecycle Manager. This may already be completed on the Cloud Lifecycle Manager.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The contents of the SLES SDK ISO
      (<code class="filename">SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso</code>) must be
      mounted or copied to
      <code class="filename">/opt/ardana_packager/ardana/sles12/zypper/SDK/</code>
      (create the directory if it is missing). If you
      choose to mount the ISO, we recommend creating an
      <code class="filename">/etc/fstab</code> entry to ensure the ISO is mounted after
      a reboot.
     </p></li><li class="listitem"><p>
      Mount or copy the contents of
      <code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code> to
      <code class="filename">/opt/ardana_packager/ardana/sles12/zypper/OS/</code>
      (create the directory if it is missing).
     </p></li><li class="listitem"><p>
      Mount or copy the contents of
      <code class="filename">SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso</code> to
      <code class="filename">/opt/ardana_packager/ardana/sles12/zypper/SDK/</code>.
     </p></li></ul></div></li><li class="step"><p>
    Log in to the VM with the configured user credentials.
   </p></li><li class="step"><p>
    The VM must be set up before a template can be created with it. The
    IP addresses configured here are temporary and will need to be
    reconfigured as VMs are created using this template. The temporary
    IP address should not overlap with the network range for
    the MGMT network.
   </p><ol type="a" class="substeps"><li class="step"><p>
      The VM should now have four network interfaces. Configure them as
      follows:
     </p><ol type="i" class="substeps"><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /etc/sysconfig/network
       <code class="prompt user">tux &gt; </code>sudo ls</pre></div><p>
        The directory will contain the files: <code class="filename">ifcfg-br0</code>,
        <code class="filename">ifcfg-br1</code>, <code class="filename">ifcfg-br2</code>,
        <code class="filename">ifcfg-br3</code>, <code class="filename">ifcfg-eth0</code>,
        <code class="filename">ifcfg-eth1</code>, <code class="filename">ifcfg-eth2</code>, and
        <code class="filename">ifcfg-eth3</code>.
       </p></li><li class="step"><p>
        If you have configured a default route while installing the VM, then
        there will be a <code class="filename">routes</code> file.
       </p></li><li class="step"><p>
        Note the IP addresses configured for MGMT.
       </p></li><li class="step"><p>
	Configure the temporary IP for the MGMT network.
	Edit the <code class="filename">ifcfg-eth1</code> file.
       </p><ol type="A" class="substeps"><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth1
BOOTPROTO='static'
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR='192.168.24.132/24' (Configure the IP address of the MGMT Interface)
MTU=''
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li></ol></li><li class="step"><p>
        Edit the <code class="filename">ifcfg-eth0</code>,
        <code class="filename">ifcfg-eth2</code>, and
        <code class="filename">ifcfg-eth3</code> files.
       </p><ol type="A" class="substeps"><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth0
BOOTPROTO='static'
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR=''
MTU=''
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth2
BOOTPROTO=''
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR=''
MTU=''
NAME='VMXNET3 Ethernet Controller'
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth3
BOOTPROTO=''
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR=''
MTU=''
NAME='VMXNET3 Ethernet Controller'
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li><li class="step"><p>
          If the default route is not configured, add a default
          route file manually.
         </p><ol type="I" class="substeps"><li class="step"><p>
            Create a file <code class="filename">routes</code> in
            <code class="filename">/etc/sysconfig/network</code>.
           </p></li><li class="step"><p>
            Edit the file to add your default route.
           </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo sudo vi routes
           default 192.168.24.140 - -</pre></div></li></ol></li></ol></li><li class="step"><p>
        Delete all the bridge configuration files, which are not required:
        <code class="filename">ifcfg-br0</code>, <code class="filename">ifcfg-br1</code>,
        <code class="filename">ifcfg-br2</code>, and <code class="filename">ifcfg-br3</code>.
       </p></li></ol></li><li class="step"><p>
      Add <code class="literal">ardana</code> user and home directory if that is not your
      default <code class="literal">user</code>. The username and password should be
      <code class="literal">ardana/ardana</code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo useradd -m ardana
      <code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
      Create a <code class="literal">ardana</code> usergroup in the VM if it does not
      exist.
     </p><ol type="i" class="substeps"><li class="step"><p>
        Check for an existing <code class="literal">ardana</code> group.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo groups ardana</pre></div></li><li class="step"><p>
        Add <code class="literal">ardana</code> group if necessary.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo groupadd ardana</pre></div></li><li class="step"><p>
        Add <code class="literal">ardana</code> user to the <code class="literal">ardana</code>
        group.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo gpasswd -a ardana ardana</pre></div></li></ol></li><li class="step"><p>
      Allow the <code class="literal">ardana</code> user to <code class="literal">sudo</code>
      without password. Setting up <code class="literal">sudo</code> on SLES is covered
      in the SUSE documentation at <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-sudo-conf" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-sudo-conf</a>.
      We recommend creating user specific sudo config files in the
      <code class="filename">/etc/sudoers.d</code> directory. Create an
      <code class="filename">/etc/sudoers.d/ardana</code> config file with the following
      content to allow sudo commands without the requirement of a password.
     </p><div class="verbatim-wrap"><pre class="screen">ardana ALL=(ALL) NOPASSWD:ALL</pre></div></li><li class="step"><p>
      Add the Zypper repositories using the ISO-based repositories created
      previously. Change the value of <code class="literal">deployer_ip</code> if necessary.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo <em class="replaceable">DEPLOYER_IP</em>=192.168.24.140
     <code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh \
     http://$deployer_ip:79/ardana/sles12/zypper/OS SLES-OS
     <code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh \
     http://$deployer_ip:79/ardana/sles12/zypper/SDK SLES-SDK</pre></div><p>
      Verify that the repositories have been added.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>zypper repos --detail</pre></div></li><li class="step"><p>
      Set up SSH access that does not require a password to the temporary
      IP address that was configured for <code class="literal">eth1</code> .
     </p><p>
      When you have started the installation using the Cloud Lifecycle Manager or if you are
      adding a SLES node to an existing cloud, the Cloud Lifecycle Manager public key needs to
      be copied to the SLES node. You can do this by copying
      <code class="filename">~/.ssh/authorized_keys</code> from another node
      in the cloud to the same location on the SLES node. If you are
      installing a new cloud, this file will be available on the nodes after
      running the <code class="filename">bm-reimage.yml</code> playbook.
     </p><div id="id-1.4.5.10.11.2.24.2.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       Ensure that there is global read access to the
       file <code class="filename">~/.ssh/authorized_keys</code>.
      </p></div><p>
      Test passwordless ssh from the Cloud Lifecycle Manager and check your ability to remotely
      execute sudo commands.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ssh ardana@<em class="replaceable">IP_OF_SLES_NODE_eth1</em>
     "sudo tail -5 /var/log/messages"</pre></div></li></ol></li><li class="step"><p>
    Shutdown the VM and create a template out of the VM appliance for future
    use.
   </p></li><li class="step"><p>
    The VM Template will be saved in your vCenter Datacenter and you can view
    it from <span class="guimenu">VMS and Templates</span> menu. Note that menu options
    will vary slightly depending on the version of vSphere that is deployed.
   </p></li></ol></div></div></section><section class="sect1" id="id-1.4.5.10.12" data-id-title="ESX Network Model Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.10 </span><span class="title-name">ESX Network Model Requirements</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.10.12">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install-esx_computes-ovsvapp.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     For this model the following networks are needed:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">MANAGEMENT-NET</code> : This is an untagged network this is used for the control plane as well as the esx-compute proxy and ovsvapp VMware instance.  It is tied to the MGMT DVS/PG in vSphere. 
     </p></li><li class="listitem"><p><code class="literal">EXTERNAL-API_NET</code> : This is a tagged network for the external/public API. There is no difference in this model from those without ESX and there is no additional setup needed in vSphere for this network.
      </p></li><li class="listitem"><p><code class="literal">EXTERNAL-VM-NET</code> : This is a tagged network used for Floating IP (FIP) assignment to running instances. There is no difference in this model from those without ESX and there is no additional setup needed in vSphere for this network.
      </p></li><li class="listitem"><p><code class="literal">GUEST-NET</code> : This is a tagged network used internally for neutron.  It is tied to the GUEST PG in vSphere.
      </p></li><li class="listitem"><p><code class="literal">ESX-CONF-NET</code> : This is a separate configuration network for ESX that must be reachable via the MANAGEMENT-NET.  It is tied to the ESX-CONF PG in vSphere.
      </p></li><li class="listitem"><p><code class="literal">TRUNK-NET</code> : This is an untagged network used internally for ESX. It is tied to the TRUNC DVS/PG in vSphere.
      </p></li></ul></div></section><section class="sect1" id="create-vms-vapp-template" data-id-title="Creating and Configuring Virtual Machines Based on Virtual Appliance Template"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.11 </span><span class="title-name">Creating and Configuring Virtual Machines Based on Virtual Appliance
 Template</span></span> <a title="Permalink" class="permalink" href="#create-vms-vapp-template">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/create-vms-vapp_template.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following process for creating and configuring VMs from the vApp template
  should be repeated for every cluster in the DataCenter. Each cluster should
  host a Nova Proxy VM, and each host in a cluster should have an OVSvApp
  VM running. The following method uses the <code class="literal">vSphere Client
  Management Tool</code> to deploy saved templates from the vCenter.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Identify the cluster that you want Nova Proxy to manage.
   </p></li><li class="step"><p>
    Create a VM from the template on a chosen cluster.
   </p></li><li class="step"><p>
    The first VM that was deployed will be the <code class="literal">Nova Compute Proxy
    VM</code>. This VM can reside on any <code class="literal">HOST</code> inside a
    cluster. There should be only one instance of this VM in a cluster.
   </p></li><li class="step"><p>
    The <code class="literal">Nova Compute Proxy</code> will use only two of the
    four interfaces configured previously (<code class="literal">ESX_CONF</code> and
    <code class="literal">MANAGEMENT</code>).
   </p><div id="id-1.4.5.10.13.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     Do not swap the interfaces. They must be in the specified order
     (<code class="literal">ESX_CONF</code> is <code class="literal">eth0</code>,
     <code class="literal">MGMT</code> is <code class="literal">eth1</code>).
    </p></div></li><li class="step"><p>
    After the VM has been deployed, log in to it with
    <code class="literal">ardana/ardana</code> credentials. Log in to the VM with SSH using
    the <code class="literal">MGMT</code> IP address. Make sure that all root level
    commands work with <code class="literal">sudo</code>. This is required for the Cloud Lifecycle Manager
    to configure the appliance for services and networking.
   </p></li><li class="step"><p>
    Install another VM from the template and name it
    <code class="literal">OVSvApp-VM1-HOST1</code>. (You can add a suffix with the
    host name to identify the host it is associated with).
   </p><div id="id-1.4.5.10.13.3.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The VM must have four interfaces configured in the right order. The VM must
    be accessible from the Management Network through SSH from the Cloud Lifecycle Manager.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth0</code> is
       <code class="literal">ESX_CONF</code>.
      </p></li><li class="listitem"><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth1</code> is
       <code class="literal">MGMT</code>.
      </p></li><li class="listitem"><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth2</code> is
       <code class="literal">TRUNK</code>.
      </p></li><li class="listitem"><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth3</code> is
       <code class="literal">GUEST</code>.
      </p></li></ul></div></div></li><li class="step"><p>
    If there is more than one <code class="literal">HOST</code> in the cluster, deploy
    another VM from the Template and name it
    <code class="literal">OVSvApp-VM2-HOST2</code>.
   </p></li><li class="step"><p>
    If the OVSvApp VMs end up on the same <code class="literal">HOST</code>, then
    manually separate the VMs and follow the instructions below to add rules
    for High Availability (HA) and Distributed Resource Scheduler (DRS).
   </p><div id="id-1.4.5.10.13.3.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     HA seeks to minimize system downtime and data loss. See also <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 4 “High Availability”</span>. DRS is a utility that balances computing workloads
     with available resources in a virtualized environment.
    </p></div></li><li class="step"><p>
    When installed from a template to a cluster, the VM will not be bound to a
    particular host if you have more than one Hypervisor. The requirement for
    the OVSvApp is that there be only one OVSvApp Appliance per host and that
    it should be constantly bound to the same host. DRS or VMotion should
    not be allowed to migrate the VMs from the existing HOST. This would cause
    major network interruption. In order to achieve this we need to configure
    rules in the cluster HA and DRS settings.
   </p><div id="id-1.4.5.10.13.3.9.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     VMotion enables the live migration of running virtual machines from one
     physical server to another with zero downtime, continuous service
     availability, and complete transaction integrity.
    </p></div></li><li class="step"><p>
    Configure rules for OVSvApp VMs.
   </p><ol type="a" class="substeps"><li class="step"><p>
      Configure <span class="guimenu">vSphere HA - Virtual Machine Options</span>.
     </p></li><li class="step"><p>
      <span class="guimenu">Use Cluster Setting</span> must be disabled.
     </p></li><li class="step"><p>
      VM should be <code class="literal">Power-On</code>.
     </p></li></ol></li><li class="step"><p>
    Configure <span class="guimenu">Cluster DRS Groups/Rules</span>.
   </p><ol type="a" class="substeps"><li class="step"><p>
      Configure <span class="guimenu">vSphere DRS - DRS Group Manager</span>.
     </p></li><li class="step"><p>
      Create a DRS Group for the OVSvApp VMs.
     </p></li><li class="step"><p>
      Add VMs to the DRS Group.
     </p></li><li class="step"><p>
      Add appropriate <span class="guimenu">Rules</span> to the DRS Groups.
     </p></li></ol></li><li class="step"><p>
    All three VMs are up and running. Following the preceding process, there is
    one Nova Compute Proxy VM per cluster, and
    <code class="literal">OVSvAppVM1</code> and <code class="literal">OVSvAppVM2</code> on each
    HOST in the cluster.
   </p></li><li class="step"><p>
    Record the configuration attributes of the VMs.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Nova Compute Proxy VM:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">Cluster Name</code> where this VM is located
       </p></li><li class="listitem"><p>
        <code class="literal">Management IP Address</code>
       </p></li><li class="listitem"><p>
        <code class="literal">VM Name</code> The actual name given to the VM to identify
        it.
       </p></li></ul></div></li><li class="listitem"><p>
      OVSvAppVM1
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">Cluster Name</code> where this VM is located
       </p></li><li class="listitem"><p>
        <code class="literal">Management IP Address</code>
       </p></li><li class="listitem"><p>
        <code class="literal">esx_hostname</code> that this OVSvApp is bound to
       </p></li><li class="listitem"><p>
        <code class="literal">cluster_dvs_mapping</code> The Distributed vSwitch name
        created in the datacenter for this particular cluster.
       </p><p>
        Example format:
       </p><p>
       <em class="replaceable">DATA_CENTER</em>/host/<em class="replaceable">CLUSTERNAME</em>:
       <em class="replaceable">DVS-NAME</em> Do not substitute for
       <code class="literal">host</code>'. It is a constant.
       </p></li></ul></div></li><li class="listitem"><p>
      OVSvAppVM2:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">Cluster Name</code> where this VM is located
       </p></li><li class="listitem"><p>
        <code class="literal">Management IP Address</code>
       </p></li><li class="listitem"><p>
        <code class="literal">esx_hostname</code> that this OVSvApp is bound to
       </p></li><li class="listitem"><p>
        <code class="literal">cluster_dvs_mapping</code> The Distributed vSwitch name
        created in the datacenter for this particular cluster.
       </p><p>
        Example format:
       </p><p>
       <em class="replaceable">DATA_CENTER</em>/host/<em class="replaceable">CLUSTERNAME</em>:
       <em class="replaceable">DVS-NAME</em> Do not substitute for
       <code class="literal">host</code>'. It is a constant.
       </p></li></ul></div></li></ul></div></li></ol></div></div></section><section class="sect1" id="collect-vcenter-credentials" data-id-title="Collect vCenter Credentials and UUID"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.12 </span><span class="title-name">Collect vCenter Credentials and UUID</span></span> <a title="Permalink" class="permalink" href="#collect-vcenter-credentials">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/collect-vcenter-credentials.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Obtain the vCenter UUID from vSphere with the URL shown below:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">VCENTER-IP</em>/mob/?moid=ServiceInstance&amp;doPath=content.about</pre></div><p>
    Select the field <code class="literal">instanceUUID</code>. Copy and paste the
    <span class="bold"><strong>value</strong></span> of <code class="literal"># field
    instanceUUID</code>.
   </p></li><li class="listitem"><p>
    Record the <code class="literal">UUID</code>
   </p></li><li class="listitem"><p>
    Record the <code class="literal">vCenter Password</code>
   </p></li><li class="listitem"><p>
    Record the <code class="literal">vCenter Management IP</code>
   </p></li><li class="listitem"><p>
    Record the <code class="literal">DataCenter Name</code>
   </p></li><li class="listitem"><p>
    Record the <code class="literal">Cluster Name</code>
   </p></li><li class="listitem"><p>
    Record the <code class="literal">DVS (Distributed vSwitch) Name</code>
   </p></li></ul></div></section><section class="sect1" id="edit-input-models" data-id-title="Edit Input Models to Add and Configure Virtual Appliances"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.13 </span><span class="title-name">Edit Input Models to Add and Configure Virtual Appliances</span></span> <a title="Permalink" class="permalink" href="#edit-input-models">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/edit-input_models.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following steps should be used to edit the Ardana input model data to add
  and configure the Virtual Appliances that were just created. The process
  assumes that the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is deployed and a valid Cloud Lifecycle Manager is in place.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Edit the following files in
    <code class="filename">~/openstack/my_cloud/definition/data/</code>:
    <code class="filename">servers.yml</code>, <code class="filename">disks_app_vm.yml</code>,
    and <code class="filename">pass_through.yml</code>. Fill in attribute values
    recorded in the previous step.
   </p></li><li class="step"><p>
    Follow the instructions in <code class="filename">pass_through.yml</code> to encrypt
    your vCenter password using an encryption key.
   </p></li><li class="step"><p>
    Export an environment variable for the encryption key.
   </p><div class="verbatim-wrap"><pre class="screen">ARDANA_USER_PASSWORD_ENCRYPT_KEY=ENCRYPTION_KEY</pre></div></li><li class="step"><p>
    Run <code class="filename">~ardana/openstack/ardana/ansible/ardanaencrypt.py</code> script. 
    It will prompt for <code class="literal">unencrypted value?</code>. Enter the unencrypted vCenter
    password and it will return an encrypted string.
   </p></li><li class="step"><p>
    Copy and paste the encrypted password string in the
    <code class="filename">pass_through.yml</code> file as a value for the
    <code class="literal">password</code> field <span class="bold"><strong>enclosed in double
    quotes</strong></span>.
   </p></li><li class="step"><p>
    Enter the <code class="literal">username</code>, <code class="literal">ip</code>, and
    <code class="literal">id</code> of the vCenter server in the Global section of the
    <code class="filename">pass_through.yml</code> file. Use the values recorded in the
    previous step.
   </p></li><li class="step"><p>
     In the <code class="literal">servers</code> section of the
     <code class="filename">pass_through.yml</code> file, add the details about the
     Nova Compute Proxy and OVSvApp VMs that was recorded in the previous
     step.
    </p><div class="verbatim-wrap"><pre class="screen"># Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml.
      # NOTE: There should be one esx-compute-proxy node per ESX
      # resource pool or cluster.
      # cluster_dvs_mapping in the format
      # 'Datacenter-name/host/Cluster-Name:Trunk-DVS-Name'
      # Here 'host' is a string and should not be changed or
      # substituted.
      # vcenter_id is same as the 'vcenter-uuid' obtained in the global
      # section.
      # 'id': is the name of the appliance manually installed
      # 'vcenter_cluster': Name of the vcenter target cluster
      # esx_hostname: Name of the esx host hosting the ovsvapp
      # NOTE: For every esx host in a cluster there should be an ovsvapp
      # instance running.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: &lt;vmware cluster1 name&gt;
          vcenter_id: &lt;vcenter-uuid&gt;
    -
      id: ovsvapp1
      data:
        vmware:
          vcenter_cluster: &lt;vmware cluster1 name&gt;
          cluster_dvs_mapping: &lt;cluster dvs mapping&gt;
          esx_hostname: &lt;esx hostname hosting the ovsvapp&gt;
          vcenter_id: &lt;vcenter-uuid&gt;
    -
      id: ovsvapp2
      data:
        vmware:
          vcenter_cluster: &lt;vmware cluster1 name&gt;
          cluster_dvs_mapping: &lt;cluster dvs mapping&gt;
          esx_hostname: &lt;esx hostname hosting the ovsvapp&gt;
          vcenter_id: &lt;vcenter-uuid&gt;</pre></div><p>
     The VM <code class="literal">id</code> string should match exactly with the data
     written in the <code class="filename">servers.yml</code> file.
    </p></li><li class="step"><p>
     Edit the <code class="filename">servers.yml</code> file, adding the Nova Proxy
     VM and OVSvApp information recorded in the previous step.
    </p><div class="verbatim-wrap"><pre class="screen"># Below entries shall be added by the user
    # for entry-scale-kvm-esx after following
    # the doc instructions in creating the
    # esx-compute-proxy VM Appliance and the
    # esx-ovsvapp VM Appliance.
    # Added just for the reference
    # NOTE: There should be one esx-compute per
    # Cluster and one ovsvapp per Hypervisor in
    # the Cluster.
    # id - is the name of the virtual appliance
    # ip-addr - is the Mgmt ip address of the appliance
    # The values shown below are examples and has to be
    # substituted based on your setup.
    # Nova Compute proxy node
    - id: esx-compute1
      server-group: RACK1
      ip-addr: 192.168.24.129
      role: ESX-COMPUTE-ROLE
    # OVSVAPP node
    - id: ovsvapp1
      server-group: RACK1
      ip-addr: 192.168.24.130
      role: OVSVAPP-ROLE
    - id: ovsvapp2
      server-group: RACK1
      ip-addr: 192.168.24.131
      role: OVSVAPP-ROLE</pre></div><p>
     Examples of <code class="filename">pass_through.yml</code> and
     <code class="filename">servers.yml</code> files:
    </p><div class="verbatim-wrap"><pre class="screen">pass_through.yml
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: administrator@vsphere.local
        ip: 10.84.79.3
        port: '443'
        cert_check: false
        password: @hos@U2FsdGVkX19aqGOUYGgcAIMQSN2lZ1X+gyNoytAGCTI=
        id: a0742a39-860f-4177-9f38-e8db82ad59c6
  servers:
    - data:
        vmware:
          vcenter_cluster: QE
          vcenter_id: a0742a39-860f-4177-9f38-e8db82ad59c6
      id: lvm-nova-compute1-esx01-qe
    - data:
        vmware:
          vcenter_cluster: QE
          cluster_dvs_mapping: 'PROVO/host/QE:TRUNK-DVS-QE'
          esx_hostname: esx01.qe.provo
          vcenter_id: a0742a39-860f-4177-9f38-e8db82ad59c6
      id: lvm-ovsvapp1-esx01-qe
    - data:
        vmware:
          vcenter_cluster: QE
          cluster_dvs_mapping: 'PROVO/host/QE:TRUNK-DVS-QE'
          esx_hostname: esx02.qe.provo
          vcenter_id: a0742a39-860f-4177-9f38-e8db82ad59c6
          id: lvm-ovsvapp2-esx02-qe</pre></div><div class="verbatim-wrap"><pre class="screen">servers.yml
product:
  version: 2
servers:
  - id: deployer
    ilo-ip: 192.168.10.129
    ilo-password: 8hAcPMne
    ilo-user: CLM004
    ip-addr: 192.168.24.125
    is-deployer: true
    mac-addr: '8c:dc:d4:b4:c5:4c'
    nic-mapping: MY-2PORT-SERVER
    role: DEPLOYER-ROLE
    server-group: RACK1
  - id: controller3
    ilo-ip: 192.168.11.52
    ilo-password: 8hAcPMne
    ilo-user: HLM004
    ip-addr: 192.168.24.128
    mac-addr: '8c:dc:d4:b5:ed:b8'
    nic-mapping: MY-2PORT-SERVER
    role: CONTROLLER-ROLE
    server-group: RACK1
  - id: controller2
    ilo-ip: 192.168.10.204
    ilo-password: 8hAcPMne
    ilo-user: HLM004
    ip-addr: 192.168.24.127
    mac-addr: '8c:dc:d4:b5:ca:c8'
    nic-mapping: MY-2PORT-SERVER
    role: CONTROLLER-ROLE
    server-group: RACK2
  - id: controller1
    ilo-ip: 192.168.11.57
    ilo-password: 8hAcPMne
    ilo-user: CLM004
    ip-addr: 192.168.24.126
    mac-addr: '5c:b9:01:89:c6:d8'
    nic-mapping: MY-2PORT-SERVER
    role: CONTROLLER-ROLE
    server-group: RACK3
  # Nova compute proxy for QE cluster added manually
  - id: lvm-nova-compute1-esx01-qe
    server-group: RACK1
    ip-addr: 192.168.24.129
    role: ESX-COMPUTE-ROLE
  # OVSvApp VM for QE cluster added manually
  # First ovsvapp vm in esx01 node
  - id: lvm-ovsvapp1-esx01-qe
    server-group: RACK1
    ip-addr: 192.168.24.132
    role: OVSVAPP-ROLE
  # Second ovsvapp vm in esx02 node
  - id: lvm-ovsvapp2-esx02-qe
    server-group: RACK1
    ip-addr: 192.168.24.131
    role: OVSVAPP-ROLE
baremetal:
  subnet: 192.168.24.0
  netmask: 255.255.255.0</pre></div></li><li class="step"><p>
     Edit the <code class="filename">disks_app_vm.yml</code> file based on your
     <code class="literal">lvm</code> configuration. The attributes of <code class="literal">Volume
     Group</code>, <code class="literal">Physical Volume</code>, and <code class="literal">Logical
     Volumes</code> must be edited based on the <code class="literal">LVM</code>
     configuration of the VM.
    </p><p>
     When you partitioned <code class="literal">LVM</code> during installation, you
     received <code class="literal">Volume Group</code> name, <code class="literal">Physical
     Volume</code> name and <code class="literal">Logical Volumes</code> with their
     partition sizes.
    </p><p>
     This information can be retrieved from any of the VMs (Nova Proxy VM
     or the OVSvApp VM):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo pvdisplay</pre></div><div class="verbatim-wrap"><pre class="screen"># — Physical volume —
    # PV Name /dev/sda1
    # VG Name system
    # PV Size 80.00 GiB / not usable 3.00 MiB
    # Allocatable yes
    # PE Size 4.00 MiB
    # Total PE 20479
    # Free PE 511
    # Allocated PE 19968
    # PV UUID 7Xn7sm-FdB4-REev-63Z3-uNdM-TF3H-S3ZrIZ</pre></div><p>
     The Physical Volume Name is <code class="literal">/dev/sda1</code>. And the Volume
     Group Name is <code class="literal">system</code>.
    </p><p>
     To find <code class="literal">Logical Volumes</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo fdisk -l</pre></div><div class="verbatim-wrap"><pre class="screen"># Disk /dev/sda: 80 GiB, 85899345920 bytes, 167772160 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disklabel type: dos
    # Disk identifier: 0x0002dc70
    # Device Boot Start End Sectors Size Id Type
    # /dev/sda1 * 2048 167772159 167770112 80G 8e Linux LVM
    # Disk /dev/mapper/system-root: 60 GiB, 64424509440 bytes,
    # 125829120 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-swap: 2 GiB, 2147483648 bytes, 4194304 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-LV_CRASH: 16 GiB, 17179869184 bytes,
    # 33554432 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # NOTE: Even though we have configured the SWAP partition, it is
    # not required to be configured in here. Just configure the root
    # and the LV_CRASH partition</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       The line with <code class="literal">/dev/mapper/system-root: 60 GiB, 64424509440
       bytes</code> indicates that the first logical partition is
       <code class="literal">root</code>.
      </p></li><li class="listitem"><p>
       The line with <code class="literal">/dev/mapper/system-LV_CRASH: 16 GiB, 17179869184
       bytes</code> indicates that the second logical partition is
       <code class="literal">LV_CRASH</code>.
      </p></li><li class="listitem"><p>
       The line with <code class="literal">/dev/mapper/system-swap: 2 GiB, 2147483648 bytes,
       4194304 sectors</code> indicates that the third logical partition is
       <code class="literal">swap</code>.
      </p></li></ul></div></li><li class="step"><p>
     Edit the <code class="filename">disks_app_vm.yml</code> file. It is not necessary
     to configure the <code class="literal">swap</code> partition.
    </p><div class="verbatim-wrap"><pre class="screen">volume-groups:
    - name: system (Volume Group Name)
      physical-volumes:
       - /dev/sda1 (Physical Volume Name)
      logical-volumes:
        - name: root   ( Logical Volume 1)
          size: 75%    (Size in percentage)
          fstype: ext4 ( filesystem type)
          mount: /     ( Mount point)
        - name: LV_CRASH   (Logical Volume 2)
          size: 20%        (Size in percentage)
          mount: /var/crash (Mount point)
          fstype: ext4      (filesystem type)
          mkfs-opts: -O large_file</pre></div><p>
     An example <code class="filename">disks_app_vm.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">disks_app_vm.yml
---
  product:
    version: 2
  disk-models:
  - name: APP-VM-DISKS
    # Disk model to be used for application vms such as nova-proxy and ovsvapp
    # /dev/sda1 is used as a volume group for /, /var/log and /var/crash
    # Additional disks can be added to either volume group
    #
    # NOTE: This is just an example file and has to filled in by the user
    # based on the lvm partition map for their virtual appliance
    # While installing the operating system opt for the LVM partition and
    # create three partitions as shown below
    # Here is an example partition map
    # In this example we have three logical partitions
    # root partition (75%)
    # swap (5%) and
    # LV_CRASH (20%)
    # Run this command 'sudo pvdisplay' on the virtual appliance to see the
    # output as shown below
    #
    # — Physical volume —
    # PV Name /dev/sda1
    # VG Name system
    # PV Size 80.00 GiB / not usable 3.00 MiB
    # Allocatable yes
    # PE Size 4.00 MiB
    # Total PE 20479
    # Free PE 511
    # Allocated PE 19968
    # PV UUID 7Xn7sm-FdB4-REev-63Z3-uNdM-TF3H-S3ZrIZ
    #
    # Next run the following command on the virtual appliance
    #
    # sudo fdisk -l
    # The output will be as shown below
    #
    # Disk /dev/sda: 80 GiB, 85899345920 bytes, 167772160 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disklabel type: dos
    # Disk identifier: 0x0002dc70
    # Device Boot Start End Sectors Size Id Type
    # /dev/sda1 * 2048 167772159 167770112 80G 8e Linux LVM
    # Disk /dev/mapper/system-root: 60 GiB, 64424509440 bytes,
    # 125829120 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-swap: 2 GiB, 2147483648 bytes, 4194304 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-LV_CRASH: 16 GiB, 17179869184 bytes,
    # 33554432 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # NOTE: Even though we have configured the SWAP partition, it is
    # not required to be configured in here. Just configure the root
    # and the LV_CRASH partition
    volume-groups:
      - name: system
        physical-volumes:
         - /dev/sda1
        logical-volumes:
          - name: root
            size: 75%
            fstype: ext4
            mount: /
          - name: LV_CRASH
            size: 20%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></li></ol></div></div></section><section class="sect1" id="run-config-processor" data-id-title="Running the Configuration Processor With Applied Changes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.14 </span><span class="title-name">Running the Configuration Processor With Applied Changes</span></span> <a title="Permalink" class="permalink" href="#run-config-processor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/run-config-processor.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If the changes are being applied to a previously deployed cloud, then after
  the previous section is completed, the Configuration Processor should be
  run with the changes that were applied.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Run the Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
-e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
    Run the <code class="filename">site.yml</code> playbook against only the VMs that
    were added.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --extra-vars \
"hux_svc_ignore_stop":true --limit hlm004-cp1-esx-comp0001-mgmt, \
hlm004-cp1-esx-ovsvapp0001-mgmt,hlm004-cp1-esx-ovsvapp0002-mgmt</pre></div></li></ol></div></div><p>
  If the changes are being applied ahead of deploying a new (greenfield) cloud, then after
  the previous section is completed, the following steps should be run.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Run the Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana/ansible
	<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
    Run the <code class="filename">site.yml</code> playbook against only the VMs that
    were added.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
	<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></section><section class="sect1" id="test-esx-environment" data-id-title="Test the ESX-OVSvApp Environment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.15 </span><span class="title-name">Test the ESX-OVSvApp Environment</span></span> <a title="Permalink" class="permalink" href="#test-esx-environment">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/test-esx-environment.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  When all of the preceding installation steps have been completed, test the
  ESX-OVSvApp environment with the following steps:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    SSH to the Controller
   </p></li><li class="step"><p>
    Source the <code class="filename">service.osrc</code> file
   </p></li><li class="step"><p>
    Create a Network
   </p></li><li class="step"><p>
    Create a Subnet
   </p></li><li class="step"><p>
    Create a VMware-based Glance image if there is not one available in the
    Glance repo. The following instructions can be used to create such an
    image that can be used by Nova to to create a VM in vCenter.
   </p><ol type="a" class="substeps"><li class="step"><p>
      Download a <code class="literal">vmdk</code> image file for the corresponding distro that
      you want for a VM.
     </p></li><li class="step"><p>
      Create a Nova image for VMware Hypervisor
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>glance image-create --name
     <em class="replaceable">DISTRO</em> --container-format bare --disk-format
     vmdk --property vmware_disktype="sparse" --property
     vmware_adaptertype="ide" --property hypervisor_type=vmware &lt;
     <em class="replaceable">SERVER_CLOUDIMG.VMDK</em></pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| checksum           | 45a4a06997e64f7120795c68beeb0e3c     |
| container_format   | bare                                 |
| created_at         | 2018-02-17T10:42:14Z                 |
| disk_format        | vmdk                                 |
| hypervisor_type    | vmware                               |
| id                 | 17e4915a-ada0-4b95-bacf-ba67133f39a7 |
| min_disk           | 0                                    |
| min_ram            | 0                                    |
| name               | leap                                 |
| owner              | 821b7bb8148f439191d108764301af64     |
| protected          | False                                |
| size               | 372047872                            |
| status             | active                               |
| tags               | []                                   |
| updated_at         | 2018-02-17T10:42:23Z                 |
| virtual_size       | None                                 |
| visibility         | shared                               |
| vmware_adaptertype | ide                                  |
| vmware_disktype    | sparse                               |
+--------------------+--------------------------------------+</pre></div><p>
      The image you created needs to be uploaded or saved. Otherwise the size
      will still be <code class="literal">0</code>.
     </p></li><li class="step"><p>
      Upload/save the image
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image save --file \
     ./<em class="replaceable">SERVER_CLOUDIMG.VMDK</em>
     17e4915a-ada0-4b95-bacf-ba67133f39a7</pre></div></li><li class="step"><p>
      After saving the image, check that it is active and has a valid size.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image list</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------+--------+
| ID                                   | Name                   | Status |
+--------------------------------------+------------------------+--------+
| c48a9349-8e5c-4ca7-81ac-9ed8e2cab3aa | cirros-0.3.2-i386-disk | active |
| 17e4915a-ada0-4b95-bacf-ba67133f39a7 | leap                   | active |
+--------------------------------------+------------------------+--------+</pre></div></li><li class="step"><p>
      Check the details of the image
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image show 17e4915a-ada0-4b95-bacf-ba67133f39a7</pre></div><div class="verbatim-wrap"><pre class="screen">+------------------+------------------------------------------------------------------------------+
| Field            | Value                                                                       |
+------------------+------------------------------------------------------------------------------+
| checksum         | 45a4a06997e64f7120795c68beeb0e3c                                            |
| container_format | bare                                                                        |
| created_at       | 2018-02-17T10:42:14Z                                                        |
| disk_format      | vmdk                                                                        |
| file             | /v2/images/40aa877c-2b7a-44d6-9b6d-f635dcbafc77/file                        |
| id               | 17e4915a-ada0-4b95-bacf-ba67133f39a7                                        |
| min_disk         | 0                                                                           |
| min_ram          | 0                                                                           |
| name             | leap                                                                        |
| owner            | 821b7bb8148f439191d108764301af64                                            |
| properties       | hypervisor_type='vmware', vmware_adaptertype='ide', vmware_disktype='sparse' |
| protected        | False                                                                       |
| schema           | /v2/schemas/image                                                           |
| size             | 372047872                                                                   |
| status           | active                                                                      |
| tags             |                                                                             |
| updated_at       | 2018-02-17T10:42:23Z                                                        |
| virtual_size     | None                                                                        |
| visibility       | shared                                                                      |
+------------------+------------------------------------------------------------------------------+</pre></div></li><li class="step"><p>
      Create a Nova instance with the VMware VMDK-based image and target it
      to the new cluster in the vCenter.
     </p></li><li class="step"><p>
      The new VM will appear in the vCenter.
     </p></li><li class="step"><p>
      The respective PortGroups for the OVSvApp on the Trunk-DVS will be
      created and connected.
     </p></li><li class="step"><p>
      Test the VM for connectivity and service.
     </p></li></ol></li></ol></div></div></section></section><section class="chapter" id="integrate-nsx-vsphere" data-id-title="Integrating NSX for vSphere"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">16 </span><span class="title-name">Integrating NSX for vSphere</span></span> <a title="Permalink" class="permalink" href="#integrate-nsx-vsphere">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/integrate-nsx-vsphere.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes the installation and integration of NSX-v, a Software
  Defined Networking (SDN) network virtualization and security platform for
  VMware's vSphere.
 </p><p>
  VMware's NSX embeds networking and security functionality, normally handled
  by hardware, directly into the hypervisor. NSX can reproduce, in software, an
  entire networking environment, and provides a complete set of logical
  networking elements and services including logical switching, routing,
  firewalling, load balancing, VPN, QoS, and monitoring. Virtual networks are
  programmatically provisioned and managed independent of the underlying
  hardware.
 </p><p>
  VMware's Neutron plugin called NSX for vSphere (NSX-v) has been tested under
  the following scenarios:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Virtual <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment
   </p></li><li class="listitem"><p>
    Baremetal <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment
   </p></li></ul></div><p>
  Installation instructions are provided for both scenarios. This documentation
  is meant as an example of how to integrate VMware's NSX-v Neutron plugin
  with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The examples in this documentation are not suitable for
  all environments. To configure this for your specific environment, use the
  design guide <a class="link" href="https://communities.vmware.com/servlet/JiveServlet/downloadBody/27683-102-8-41631/NSX" target="_blank">Reference
  Design: VMware® NSX for vSphere (NSX) Network Virtualization Design
  Guide</a>.
 </p><p>
  This section includes instructions for:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Integrating with NSX for vSphere on Baremetal
   </p></li><li class="listitem"><p>
    Integrating with NSX for vSphere on virtual machines with changes necessary
    for Baremetal integration
   </p></li><li class="listitem"><p>
    Verifying NSX-v functionality
   </p></li></ul></div><section class="sect1" id="nsx-vsphere-vm" data-id-title="Integrating with NSX for vSphere"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.1 </span><span class="title-name">Integrating with NSX for vSphere</span></span> <a title="Permalink" class="permalink" href="#nsx-vsphere-vm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes the installation steps and requirements for
  integrating with NSX for vSphere on virtual machines and baremetal hardware.
 </p><section class="sect2" id="nsx-pre-integration" data-id-title="Pre-Integration Checklist"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.1.1 </span><span class="title-name">Pre-Integration Checklist</span></span> <a title="Permalink" class="permalink" href="#nsx-pre-integration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-pre_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following installation and integration instructions assumes an
    understanding of VMware's ESXI and vSphere products for setting up virtual
    environments.
   </p><p>
    Please review the following requirements for the VMware vSphere
    environment.
   </p><p>
    <span class="bold"><strong>Software Requirements</strong></span>
   </p><p>
    Before you install or upgrade NSX, verify your software versions. The
    following are the required versions.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Software</p></th><th style="border-bottom: 1px solid ; "><p>Version</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></p></td><td style="border-bottom: 1px solid ; "><p>8</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>VMware NSX-v Manager</p></td><td style="border-bottom: 1px solid ; "><p>6.3.4 or higher</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>VMWare NSX-v Neutron Plugin</p></td><td style="border-bottom: 1px solid ; "><p>Pike Release (TAG=11.0.0)</p></td></tr><tr><td style="border-right: 1px solid ; "><p>VMWare ESXi and vSphere Appliance (vSphere web Client)</p></td><td><p>6.0 or higher</p></td></tr></tbody></table></div><p>
    A vCenter server (appliance) is required to manage the vSphere
    environment. It is recommended that you install a vCenter appliance as an
    ESX virtual machine.
   </p><div id="id-1.4.5.11.9.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     Each ESXi compute cluster is required to have shared storage between the
     hosts in the cluster, otherwise attempts to create instances through
     nova-compute will fail.
    </p></div></section><section class="sect2" id="id-1.4.5.11.9.4" data-id-title="Installing OpenStack"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.1.2 </span><span class="title-name">Installing <span class="productname">OpenStack</span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.9.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="productname">OpenStack</span> can be deployed in two ways: on baremetal (physical hardware) or in
   an ESXi virtual environment on virtual machines. The following instructions
   describe how to install <span class="productname">OpenStack</span>.
 </p><div id="id-1.4.5.11.9.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   <span class="bold"><strong>Changes for installation on baremetal hardware are noted in each
   section.</strong></span>  
  </p></div><p>
   This deployment example will consist of two ESXi clusters at minimum: a
   <code class="literal">control-plane</code> cluster and a <code class="literal">compute</code>
   cluster. The control-plane cluster must have 3 ESXi hosts minimum (due to
   VMware's recommendation that each NSX controller virtual machine is on a
   separate host). The compute cluster must have 2 ESXi hosts minimum.  There
   can be multiple compute clusters. The following table outlines the virtual
   machine specifications to be built in the control-plane cluster:
  </p><div class="table" id="nsx-hw-reqs-vm" data-id-title="NSX Hardware Requirements for Virtual Machine Integration"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.1: </span><span class="title-name">NSX Hardware Requirements for Virtual Machine Integration </span></span><a title="Permalink" class="permalink" href="#nsx-hw-reqs-vm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/><col class="5"/><col class="6"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Virtual Machine Role</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Required Number</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Disk</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Memory</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Network</p></th><th style="border-bottom: 1px solid ; "><p>CPU</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Dedicated lifecycle manager
       </p>
       <p>
        <span class="bold"><strong>Baremetal - not needed</strong></span>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>100GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>8GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 VMXNET Virtual Network Adapters</p></td><td style="border-bottom: 1px solid ; "><p>4 vCPU</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Controller virtual machines
       </p>
       <p>
        <span class="bold"><strong>Baremetal - not needed</strong></span>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 x 300GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>32GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 VMXNET Virtual Network Adapters</p></td><td style="border-bottom: 1px solid ; "><p>8 vCPU</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Compute virtual machines</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>1 per compute cluster</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>80GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>4GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 VMXNET Virtual Network Adapters</p></td><td style="border-bottom: 1px solid ; "><p>2 vCPU</p></td></tr><tr><td style="border-right: 1px solid ; "><p>NSX Edge Gateway/DLR/Metadata-proxy appliances</p></td><td style="border-right: 1px solid ; "/><td style="border-right: 1px solid ; "><p>Autogenerated by NSXv</p></td><td style="border-right: 1px solid ; "><p>Autogenerated by NSXv</p></td><td style="border-right: 1px solid ; "><p>Autogenerated by NSXv</p></td><td><p>Autogenerated by NSXv</p></td></tr></tbody></table></div></div><p>
   <span class="bold"><strong>Baremetal: In addition to the ESXi hosts, it is
   recommended to have one physical host for the Cloud Lifecycle Manager node and three physical
   hosts for the controller nodes.</strong></span>
  </p><section class="sect3" id="nsx-ntwk-requirements" data-id-title="Network Requirements"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.1 </span><span class="title-name">Network Requirements</span></span> <a title="Permalink" class="permalink" href="#nsx-ntwk-requirements">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-ntwk-requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  NSX-v requires the following for networking:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The ESXi hosts, vCenter, and the NSX Manager appliance must resolve DNS lookup.
   </p></li><li class="listitem"><p>
    The ESXi host must have the NTP service configured and enabled.
   </p></li><li class="listitem"><p>
    Jumbo frames must be enabled on the switch ports that the ESXi hosts are connected to.
   </p></li><li class="listitem"><p>
    The ESXi hosts must have at least 2 physical network cards each.
   </p></li></ul></div></section><section class="sect3" id="nsx-ntwk-model" data-id-title="Network Model"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.2 </span><span class="title-name">Network Model</span></span> <a title="Permalink" class="permalink" href="#nsx-ntwk-model">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-ntwk-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The model in these instructions requires the following networks:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.11.9.4.8.3.1"><span class="term">ESXi Hosts and vCenter</span></dt><dd><p>
     This is the network that the ESXi hosts and vCenter use to route traffic with.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.2"><span class="term">NSX Management</span></dt><dd><p>
      The network which the NSX controllers and NSX Manager will use.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.3"><span class="term">NSX VTEP Pool</span></dt><dd><p>
     The network that NSX uses to create endpoints for VxLAN tunnels.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.4"><span class="term">Management</span></dt><dd><p>
     The network that <span class="productname">OpenStack</span> uses for deployment and maintenance of the cloud.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.5"><span class="term">Internal API (optional)</span></dt><dd><p>
     The network group that will be used for management (private API) traffic within the cloud.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.6"><span class="term">External API</span></dt><dd><p>
     This is the network that users will use to make requests to the cloud.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.7"><span class="term">External VM</span></dt><dd><p>
     VLAN-backed provider network for external access to guest VMs (floating IPs).
    </p></dd></dl></div></section><section class="sect3" id="id-1.4.5.11.9.4.9" data-id-title="vSphere port security settings"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.3 </span><span class="title-name">vSphere port security settings</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.9.4.9">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Baremetal: Even though the <span class="productname">OpenStack</span>
   deployment is on baremetal, it is still necessary to define each VLAN within
   a vSphere Distributed Switch for the Nova compute proxy virtual
   machine.</strong></span>
  </p><p>
   The vSphere port security settings for both VMs and baremetal are shown in the
   table below.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Network Group</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>VLAN Type</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Interface</p></th><th style="border-bottom: 1px solid ; "><p>vSphere Port Group Security Settings</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>ESXi Hosts and vCenter</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; "><p>Defaults</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>NSX Manager</p>
      <p>Must be able to reach ESXi Hosts and vCenter</p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; "><p>Defaults</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>NSX VTEP Pool</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; "><p>Defaults</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Management</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged or Untagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>eth0</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>:Reject
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Internal API (Optional, may be combined with the Management Network. If
        network segregation is required for security reasons, you can keep this
        as a separate network.)
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>eth2</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>External API (Public)</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>eth1</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>External VM</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; "><p><span class="bold"><strong>Baremetal Only: IPMI</strong></span></p></td><td style="border-right: 1px solid ; "><p>Untagged</p></td><td style="border-right: 1px solid ; "><p>N/A</p></td><td><p>N/A</p></td></tr></tbody></table></div></section><section class="sect3" id="nsx-configure-vsphere-env" data-id-title="Configuring the vSphere Environment"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4 </span><span class="title-name">Configuring the vSphere Environment</span></span> <a title="Permalink" class="permalink" href="#nsx-configure-vsphere-env">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-vsphere_env.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before deploying <span class="productname">OpenStack</span> with NSX-v, the VMware vSphere environment must be
   properly configured, including setting up vSphere distributed switches and
   port groups. For detailed instructions, see <a class="xref" href="#install-esx-ovsvapp" title="Chapter 15. Installing ESX Computes and OVSvAPP">Chapter 15, <em>Installing ESX Computes and OVSvAPP</em></a>.
  </p><p>
   Installing and configuring the VMware NSX Manager and creating the NSX
   network within the vSphere environment is covered below.
  </p><p>
   Before proceeding with the installation, ensure that the following are
   configured in the vSphere environment.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The vSphere datacenter is configured with at least two clusters, one
     <span class="bold"><strong>control-plane</strong></span> cluster and one <span class="bold"><strong>compute</strong></span> cluster.
    </p></li><li class="listitem"><p>
     Verify that all software, hardware, and networking requirements have been
     met.
    </p></li><li class="listitem"><p>
     Ensure the vSphere distributed virtual switches (DVS) are configured for each
     cluster.
    </p></li></ul></div><div id="id-1.4.5.11.9.4.10.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The MTU setting for each DVS should be set to 1600. NSX should
    automatically apply this setting to each DVS during the setup
    process. Alternatively, the setting can be manually applied to each DVS
    before setup if desired.
   </p></div><p>
   Make sure there is a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
   <code class="literal">ardana</code> home directory,
   <code class="filename">var/lib/ardana</code>, and that it is called
   <code class="filename">sles12sp3.iso</code>.
  </p><p>
   Install the <code class="literal">open-vm-tools</code> package.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install open-vm-tools</pre></div><section class="sect4" id="nsx-install-manager" data-id-title="Install NSX Manager"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.1 </span><span class="title-name">Install NSX Manager</span></span> <a title="Permalink" class="permalink" href="#nsx-install-manager">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-install-manager.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The NSX Manager is the centralized network management component of NSX. It
  provides a single point of configuration and REST API entry-points.
 </p><p>
   The NSX Manager is installed as a virtual appliance on one of the ESXi hosts
   within the vSphere environment. This guide will cover installing the
   appliance on one of the ESXi hosts within the control-plane cluster. For
   more detailed information, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html" target="_blank">VMware's
   NSX Installation Guide.</a>
 </p><p>
  To install the NSX Manager, download the virtual appliance from <a class="link" href="https://www.vmware.com/go/download-nsx-vsphere" target="_blank">VMware</a> and
  deploy the appliance within vCenter onto one of the ESXi hosts. For
  information on deploying appliances within vCenter, refer to VMware's
  documentation for ESXi <a class="link" href="https://pubs.vmware.com/vsphere-55/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">5.5</a>
  or <a class="link" href="https://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">6.0</a>.
 </p><p>
  During the deployment of the NSX Manager appliance, be aware of the
  following:
 </p><p>
  When prompted, select <span class="guimenu">Accept extra configuration options</span>.
  This will present options for configuring IPv4 and IPv6 addresses, the
  default gateway, DNS, NTP, and SSH properties during the installation, rather
  than configuring these settings manually after the installation.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Choose an ESXi host that resides within the control-plane cluster.
   </p></li><li class="listitem"><p>
    Ensure that the network mapped port group is the DVS port group that
    represents the VLAN the NSX Manager will use for its networking (in this
    example it is labeled as the <code class="literal">NSX Management</code> network).
   </p></li></ul></div><div id="id-1.4.5.11.9.4.10.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The IP address assigned to the NSX Manager must be able to resolve
   reverse DNS.
  </p></div><p>
  Power on the NSX Manager virtual machine after it finishes deploying and wait
  for the operating system to fully load. When ready, carry out the following
  steps to have the NSX Manager use single sign-on (SSO) and to
  register the NSX Manager with vCenter:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Open a web browser and enter the hostname or IP address that was assigned
    to the NSX Manager during setup.
   </p></li><li class="step"><p>
    Log in with the username <code class="literal">admin</code> and the
    password set during the deployment.
   </p></li><li class="step"><p>
    After logging in, click on <span class="guimenu">Manage vCenter Registration</span>.
   </p></li><li class="step"><p>
    Configure the NSX Manager to connect to the vCenter server.
   </p></li><li class="step"><p>
    Configure NSX manager for single sign on (SSO) under the <span class="guimenu">Lookup
    Server URL</span> section.
   </p></li></ol></div></div><div id="id-1.4.5.11.9.4.10.10.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   When configuring SSO, use <code class="literal">Lookup Service Port 443</code> for
   vCenter version 6.0. Use <code class="literal">Lookup Service Port 7444</code> for
   vCenter version 5.5.
  </p><p>
   SSO makes vSphere and NSX more secure by allowing the various components to
   communicate with each other through a secure token exchange mechanism,
   instead of requiring each component to authenticate a user separately. For
   more details, refer to VMware's documentation on <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html" target="_blank">Configure
   Single Sign-On</a>.
  </p></div><p>
  Both the <code class="literal">Lookup Service URL</code> and the <code class="literal">vCenter
  Server</code> sections should have a status of
  <code class="literal">connected</code> when configured properly.
 </p><p>
  Log into the vSphere Web Client (log out and and back in if already logged
  in). The NSX Manager will appear under the <span class="guimenu">Networking &amp;
  Security</span> section of the client.
 </p><div id="id-1.4.5.11.9.4.10.10.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The <span class="guimenu">Networking &amp; Security</span> section will not appear
   under the vSphere desktop client. Use of the web client is required for the
   rest of this process.
  </p></div></section><section class="sect4" id="nsx-add-controllers" data-id-title="Add NSX Controllers"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.2 </span><span class="title-name">Add NSX Controllers</span></span> <a title="Permalink" class="permalink" href="#nsx-add-controllers">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-add-controllers.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The NSX controllers serve as the central control point for all logical
  switches within the vSphere environment's network, and they maintain
  information about all hosts, logical switches (VXLANs), and distributed
  logical routers.
 </p><p>
  NSX controllers will each be deployed as a virtual appliance on the ESXi
  hosts within the control-plane cluster to form the NSX Controller
  cluster. For details about NSX controllers and the NSX control plane in
  general, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-4E0FEE83-CF2C-45E0-B0E6-177161C3D67C.html" target="_blank">VMware's
  NSX documentation</a>.
 </p><div id="id-1.4.5.11.9.4.10.11.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   Whatever the size of the NSX deployment, the following conditions must be
   met:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Each NSX Controller cluster must contain three controller nodes. Having a
     different number of controller nodes is not supported.
    </p></li><li class="listitem"><p>
     Before deploying NSX Controllers, you must deploy an NSX Manager appliance
     and register vCenter with NSX Manager.
    </p></li><li class="listitem"><p>
     Determine the IP pool settings for your controller cluster, including the
     gateway and IP address range. DNS settings are optional.
    </p></li><li class="listitem"><p>
     The NSX Controller IP network must have connectivity to the NSX Manager
     and to the management interfaces on the ESXi hosts.
    </p></li></ul></div></div><p>
   Log in to the vSphere web client and do the following steps to add the NSX
   controllers:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In vCenter, navigate to <span class="guimenu">Home</span>, select
     <span class="guimenu">Networking &amp;
     Security</span> › <span class="guimenu">Installation</span>, and then
     select the <span class="guimenu">Management</span> tab.
    </p></li><li class="step"><p>
     In the <span class="guimenu">NSX Controller nodes</span> section, click the
     <span class="guimenu">Add Node</span> icon represented by a green plus sign.
    </p></li><li class="step"><p>
     Enter the NSX Controller settings appropriate to your
     environment. If you are following this example, use the control-plane
     clustered ESXi hosts and control-plane DVS port group for the controller
     settings.
    </p></li><li class="step"><p>
     If it has not already been done, create an IP pool for the NSX Controller
     cluster with at least three IP addressess by clicking <span class="guimenu">New IP
     Pool</span>. Individual controllers can be in separate IP subnets, if
     necessary.
    </p></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to deploy the controller. After the first controller is
     completely deployed, deploy two additional controllers.
    </p></li></ol></div></div><div id="id-1.4.5.11.9.4.10.11.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Three NSX controllers is mandatory. VMware recommends configuring a DRS
    anti-affinity rule to prevent the controllers from residing on the same
    ESXi host. See more information about <a class="link" href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.resmgmt.doc%2FGUID-FF28F29C-8B67-4EFF-A2EF-63B3537E6934.html" target="_blank">DRS
    Affinity Rules</a>.
   </p></div></section><section class="sect4" id="nsx-prepare-clusters" data-id-title="Prepare Clusters for NSX Management"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.3 </span><span class="title-name">Prepare Clusters for NSX Management</span></span> <a title="Permalink" class="permalink" href="#nsx-prepare-clusters">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-prepare-clusters.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  During <span class="guimenu">Host Preparation</span>, the NSX Manager:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Installs the NSX kernel modules on ESXi hosts that are members of vSphere
    clusters
   </p></li><li class="listitem"><p>
    Builds the NSX control-plane and management-plane infrastructure
   </p></li></ul></div><p>
  The NSX kernel modules are packaged in <code class="filename">VIB</code>
  (vSphere Installation Bundle) files. They run within the hypervisor kernel and
  provide services such as distributed routing, distributed firewall, and VXLAN
  bridging capabilities. These files are installed on a per-cluster level, and
  the setup process deploys the required software on all ESXi hosts in the
  target cluster. When a new ESXi host is added to the cluster, the required
  software is automatically installed on the newly added host.
 </p><p>
  Before beginning the NSX host preparation process, make sure of the following
  in your environment:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Register vCenter with NSX Manager and deploy the NSX controllers.
   </p></li><li class="listitem"><p>
    Verify that DNS reverse lookup returns a fully qualified domain name when
    queried with the IP address of NSX Manager.
   </p></li><li class="listitem"><p>
    Verify that the ESXi hosts can resolve the DNS name of vCenter server.
   </p></li><li class="listitem"><p>
    Verify that the ESXi hosts can connect to vCenter Server on port 80.
   </p></li><li class="listitem"><p>
    Verify that the network time on vCenter Server and the ESXi hosts is
    synchronized.
   </p></li><li class="listitem"><p>
    For each vSphere cluster that will participate in NSX, verify that the ESXi
    hosts within each respective cluster are attached to a common VDS.
   </p><p>
    For example, given a deployment with two clusters named Host1 and
    Host2. Host1 is attached to VDS1 and VDS2. Host2 is attached to VDS1 and
    VDS3. When you prepare a cluster for NSX, you can only associate NSX with
    VDS1 on the cluster. If you add another host (Host3) to the cluster and
    Host3 is not attached to VDS1, it is an invalid configuration, and Host3
    will not be ready for NSX functionality.
   </p></li><li class="listitem"><p>
    If you have vSphere Update Manager (VUM) in your environment, you must
    disable it before preparing clusters for network virtualization. For
    information on how to check if VUM is enabled and how to disable it if
    necessary, see the <a class="link" href="http://kb.vmware.com/kb/2053782" target="_blank">VMware knowledge base</a>.
   </p></li><li class="listitem"><p>
    In the vSphere web client, ensure that the cluster is in the resolved state
    (listed under the <span class="guimenu">Host Preparation</span> tab). If the Resolve option does not
    appear in the cluster's Actions list, then it is in a resolved state.
   </p></li></ul></div><p>
  To prepare the vSphere clusters for NSX:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    In vCenter, select <span class="guimenu">Home</span> › <span class="guimenu">Networking
    &amp; Security</span> › <span class="guimenu">Installation</span>, and
    then select the <span class="guimenu">Host Preparation</span> tab.
   </p></li><li class="step"><p>
    Continuing with the example in these instructions, click on the
    <span class="guimenu">Actions</span> button (gear icon) and select
    <span class="guimenu">Install</span> for both the control-plane cluster and compute
    cluster (if you are using something other than this example, then only
    install on the clusters that require NSX logical switching, routing, and
    firewalls).
   </p></li><li class="step"><p>
    Monitor the installation until the <code class="literal">Installation Status</code>
    column displays a green check mark.
   </p><div id="id-1.4.5.11.9.4.10.12.8.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     While installation is in
     progress, do not deploy, upgrade, or uninstall any service or component.
    </p></div><div id="id-1.4.5.11.9.4.10.12.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     If the <code class="literal">Installation Status</code> column displays a red
     warning icon and says <code class="literal">Not Ready</code>, click
     <span class="guimenu">Resolve</span>. Clicking <span class="guimenu">Resolve</span> might
     result in a reboot of the host. If the installation is still not
     successful, click the warning icon. All errors will be displayed. Take the
     required action and click <span class="guimenu">Resolve</span> again.
    </p></div></li><li class="step"><p>
    To verify the VIBs (<code class="filename">esx-vsip</code> and
    <code class="filename">esx-vxlan</code>) are installed and registered, SSH into an
    ESXi host within the prepared cluster. List the names and versions of the
    VIBs installed by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>esxcli software vib list | grep esx</pre></div><div class="verbatim-wrap"><pre class="screen">...
esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
...</pre></div></li></ol></div></div><div id="id-1.4.5.11.9.4.10.12.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   After host preparation:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A host reboot is not required
    </p></li><li class="listitem"><p>
     If you add a host to a prepared cluster, the NSX VIBs are automatically
     installed on the host.
    </p></li><li class="listitem"><p>
     If you move a host to an unprepared cluster, the NSX VIBs are
     automatically uninstalled from the host. In this case, a host reboot
     is required to complete the uninstall process.
    </p></li></ul></div></div></section><section class="sect4" id="nsx-configure-vxlan-transport" data-id-title="Configure VXLAN Transport Parameters"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.4 </span><span class="title-name">Configure VXLAN Transport Parameters</span></span> <a title="Permalink" class="permalink" href="#nsx-configure-vxlan-transport">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-vxlan-transport.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  VXLAN is configured on a per-cluster basis, where each vSphere cluster that
  is to participate in NSX is mapped to a vSphere Distributed Virtual Switch
  (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in that
  cluster is enabled for logical switches. The settings chosen in this section
  will be used in creating the VMkernel interface.
 </p><p>
  Configuring transport parameters involves selecting a DVS, a VLAN ID, an MTU
  size, an IP addressing mechanism, and a NIC teaming policy. The MTU for each
  switch must be set to 1550 or higher. By default, it is set to 1600 by
  NSX. This is also the recommended setting for integration with <span class="productname">OpenStack</span>.
 </p><p>
  To configure the VXLAN transport parameters:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    In the vSphere web client, navigate to
    <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
    Security</span> › <span class="guimenu">Installation</span>.
   </p></li><li class="step"><p>
    Select the <span class="guimenu">Host Preparation</span> tab.
   </p></li><li class="step"><p>
    Click the <span class="guimenu">Configure</span> link in the VXLAN column.
   </p></li><li class="step"><p>
    Enter the required information.
   </p></li><li class="step"><p>
    If you have not already done so, create an IP pool for the VXLAN tunnel end
    points (VTEP) by clicking <span class="guimenu">New IP Pool</span>:
   </p></li><li class="step"><p>
    Click <span class="guimenu">OK</span> to create the VXLAN network.
   </p></li></ol></div></div><p>
  When configuring the VXLAN transport network, consider the following:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Use a NIC teaming policy that best suits the environment being
    built. <code class="literal">Load Balance - SRCID</code> as the VMKNic teaming policy
    is usually the most flexible out of all the available options. This allows
    each host to have a VTEP vmkernel interface for each dvuplink on the
    selected distributed switch (two dvuplinks gives two VTEP interfaces per
    ESXi host).
   </p></li><li class="listitem"><p>
    Do not mix different teaming policies for different portgroups on a VDS
    where some use Etherchannel or Link Aggregation Control Protocol (LACPv1 or
    LACPv2) and others use a different teaming policy. If uplinks are shared in
    these different teaming policies, traffic will be interrupted. If logical
    routers are present, there will be routing problems. Such a configuration
    is not supported and should be avoided.
   </p></li><li class="listitem"><p>
    For larger environments it may be better to use DHCP for the VMKNic IP
    Addressing.
   </p></li><li class="listitem"><p>
    For more information and further guidance, see the <a class="link" href="https://communities.vmware.com/docs/DOC-27683" target="_blank">VMware NSX for
    vSphere Network Virtualization Design Guide</a>.
   </p></li></ul></div></section><section class="sect4" id="nsx-assign-segment-id-pool" data-id-title="Assign Segment ID Pool"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.5 </span><span class="title-name">Assign Segment ID Pool</span></span> <a title="Permalink" class="permalink" href="#nsx-assign-segment-id-pool">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-assign-segment_id-pool.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Each VXLAN tunnel will need a segment ID to isolate its network
  traffic. Therefore, it is necessary to configure a segment ID pool for the
  NSX VXLAN network to use. If an NSX controller is not deployed within the
  vSphere environment, a multicast address range must be added to spread
  traffic across the network and avoid overloading a single multicast address.
 </p><p>
  For the purposes of the example in these instructions, do the following steps
  to assign a segment ID pool. Otherwise, follow best practices as outlined in
  <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
  documentation</a>.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    In the vSphere web client, navigate to
    <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
    Security</span> › <span class="guimenu">Installation</span>.
   </p></li><li class="step"><p>
    Select the <span class="guimenu">Logical Network Preparation</span> tab.
   </p></li><li class="step"><p>
    Click <span class="guimenu">Segment ID</span>, and then <span class="guimenu">Edit</span>.
   </p></li><li class="step"><p>
    Click <span class="guimenu">OK</span> to save your changes.
   </p></li></ol></div></div></section><section class="sect4" id="nsx-create-transport-zone" data-id-title="Create a Transport Zone"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.6 </span><span class="title-name">Create a Transport Zone</span></span> <a title="Permalink" class="permalink" href="#nsx-create-transport-zone">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-create-transport-zone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  A transport zone controls which hosts a logical switch can reach and has the
  following characteristics.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    It can span one or more vSphere clusters.
   </p></li><li class="listitem"><p>
    Transport zones dictate which clusters can participate in the use of a
    particular network. Therefore they dictate which VMs can participate in the
    use of a particular network.
   </p></li><li class="listitem"><p>
    A vSphere NSX environment can contain one or more transport zones based on the
    environment's requirements.
   </p></li><li class="listitem"><p>
    A host cluster can belong to multiple transport
    zones.
   </p></li><li class="listitem"><p>
    A logical switch can belong to only one transport zone.
   </p></li></ul></div><div id="id-1.4.5.11.9.4.10.15.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   <span class="productname">OpenStack</span> has only been verified to work with a single transport zone within
   a vSphere NSX-v environment. Other configurations are currently not
   supported.
  </p></div><p>
   For more information on transport zones, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html" target="_blank">VMware's
   Add A Transport Zone</a>.
  </p><p>
   To create a transport zone:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In the vSphere web client, navigate to
     <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
     Security</span> › <span class="guimenu">Installation</span>.
    </p></li><li class="step"><p>
     Select the <span class="guimenu">Logical Network Preparation</span> tab.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Transport Zones</span>, and then click the <span class="guimenu">New
     Transport Zone</span> (New Logical Switch) icon.
    </p></li><li class="step"><p>
     In the <span class="guimenu">New Transport Zone</span> dialog box, type a name and
     an optional description for the transport zone.
    </p></li><li class="step"><p>
     For these example instructions, select the control plane mode as
     <code class="literal">Unicast</code>.
    </p><div id="id-1.4.5.11.9.4.10.15.7.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Whether there is a controller in the environment or if the environment is
      going to use multicast addresses will determine the control plane mode to
      select:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">Unicast</code> (what this set of instructions uses): The
        control plane is handled by an NSX controller. All unicast traffic
        leverages optimized headend replication. No multicast IP addresses or
        special network configuration is required.
       </p></li><li class="listitem"><p>
        <code class="literal">Multicast</code>: Multicast IP addresses in the physical
        network are used for the control plane. This mode is recommended only
        when upgrading from older VXLAN deployments. Requires PIM/IGMP in the
        physical network.
       </p></li><li class="listitem"><p>
        <code class="literal">Hybrid</code>: Offloads local traffic replication to the
        physical network (L2 multicast). This requires IGMP snooping on the
        first-hop switch and access to an IGMP querier in each VTEP subnet, but
        does not require PIM. The first-hop switch handles traffic replication
        for the subnet.
       </p></li></ul></div></div></li><li class="step"><p>
     Select the clusters to be added to the transport zone.
    </p></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to save your changes.
    </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.9.4.10.16" data-id-title="Deploying SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.7 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.9.4.10.16">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-vsphere_env.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    With vSphere environment setup completed, the <span class="productname">OpenStack</span> can be deployed. The
    following sections will cover creating virtual machines within the vSphere
    environment, configuring the cloud model and integrating NSX-v Neutron
    core plugin into the <span class="productname">OpenStack</span>:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the virtual machines
     </p></li><li class="step"><p>
      Deploy the Cloud Lifecycle Manager
     </p></li><li class="step"><p>
      Configure the Neutron environment with NSX-v
     </p></li><li class="step"><p>
      Modify the cloud input model
     </p></li><li class="step"><p>
      Set up the parameters
     </p></li><li class="step"><p>
      Deploy the Operating System with Cobbler
     </p></li><li class="step"><p>
      Deploy the cloud
     </p></li></ol></div></div></section></section><section class="sect3" id="id-1.4.5.11.9.4.11" data-id-title="Deploying SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.9.4.11">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Within the vSphere environment, create the <span class="productname">OpenStack</span> virtual machines. At
    minimum, there must be the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      One Cloud Lifecycle Manager deployer
     </p></li><li class="listitem"><p>
      Three <span class="productname">OpenStack</span> controllers
     </p></li><li class="listitem"><p>
      One <span class="productname">OpenStack</span> Neutron compute proxy
     </p></li></ul></div><p>
    For the minimum NSX hardware requirements, refer to <a class="xref" href="#nsx-hw-reqs-vm" title="NSX Hardware Requirements for Virtual Machine Integration">Table 16.1, “NSX Hardware Requirements for Virtual Machine Integration”</a>.
   </p><p>
    If ESX VMs are to be used as Nova compute proxy nodes, set up three LAN
    interfaces in each virtual machine as shown in the networking model table below.  There must
    be at least one Nova compute proxy node per cluster.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Network Group</p></th><th style="border-bottom: 1px solid ; "><p>Interface</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Management</p></td><td style="border-bottom: 1px solid ; "><p><code class="literal">eth0</code></p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>External API</p></td><td style="border-bottom: 1px solid ; "><p><code class="literal">eth1</code></p></td></tr><tr><td style="border-right: 1px solid ; "><p>Internal API</p></td><td><p><code class="literal">eth2</code></p></td></tr></tbody></table></div><section class="sect4" id="nsx-advanced-config" data-id-title="Advanced Configuration Option"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.1 </span><span class="title-name">Advanced Configuration Option</span></span> <a title="Permalink" class="permalink" href="#nsx-advanced-config">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-advanced-config.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.11.9.4.11.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   Within vSphere for each in the virtual machine:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     In the <span class="guimenu">Options</span> section, under <span class="guimenu">Advanced
     configuration parameters</span>, ensure that
     <code class="literal">disk.EnableUUIDoption</code> is set to
     <code class="literal">true</code>.
    </p></li><li class="listitem"><p>
     If the option does not exist, it must be added. This
     option is required for the <span class="productname">OpenStack</span> deployment.
    </p></li><li class="listitem"><p>
     If the option is not specified, then the deployment will fail when
     attempting to configure the disks of each virtual machine.
    </p></li></ul></div></div></section><section class="sect4" id="sec-nsx-setup-deployer" data-id-title="Setting Up the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.2 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#sec-nsx-setup-deployer">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect5" id="id-1.4.5.11.9.4.11.8.2" data-id-title="Installing the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.2.1 </span><span class="title-name">Installing the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.9.4.11.8.2">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the product from:
     </p><ol type="a" class="substeps"><li class="step"><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step"><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step"><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step"><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step"><p>
      Select the location.
     </p></li><li class="step"><p>
      Select the keyboard layout.
     </p></li><li class="step"><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step"><p>
      Create new account:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Enter a username.
       </p></li><li class="step"><p>
        Enter a password.
       </p></li><li class="step"><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step"><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.4.5.11.9.4.11.8.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></section></section><section class="sect4" id="nsx-configure-neutron-env-nsx" data-id-title="Configure the Neutron Environment with NSX-v"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3 </span><span class="title-name">Configure the Neutron Environment with NSX-v</span></span> <a title="Permalink" class="permalink" href="#nsx-configure-neutron-env-nsx">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-neutron-env-nsx.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In summary, integrating NSX with vSphere has four major steps:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Modify the input model to define the server roles, servers, network roles
    and networks. <a class="xref" href="#nsx-modify-input-model" title="16.1.2.5.3.2. Modify the Input Model">Section 16.1.2.5.3.2, “Modify the Input Model”</a>
   </p></li><li class="step"><p>
    Set up the parameters needed for Neutron and Nova to communicate with the
    ESX and NSX Manager. <a class="xref" href="#nsx-deploy-os-cobbler" title="16.1.2.5.3.3. Deploying the Operating System with Cobbler">Section 16.1.2.5.3.3, “Deploying the Operating System with Cobbler”</a>
   </p></li><li class="step"><p>
    Do the steps to deploy the cloud. <a class="xref" href="#nsx-deploy-cloud" title="16.1.2.5.3.4. Deploying the Cloud">Section 16.1.2.5.3.4, “Deploying the Cloud”</a>
   </p></li></ol></div></div><section class="sect5" id="nsx-import-third-party" data-id-title="Third-Party Import of VMware NSX-v Into Neutron and Neutronclient"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.1 </span><span class="title-name">Third-Party Import of VMware NSX-v Into Neutron and
 Neutronclient</span></span> <a title="Permalink" class="permalink" href="#nsx-import-third-party">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-import-third_party.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To import the NSX-v Neutron core-plugin into Cloud Lifecycle Manager, run the third-party
  import playbook.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost third-party-import.yml</pre></div></section><section class="sect5" id="nsx-modify-input-model" data-id-title="Modify the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2 </span><span class="title-name">Modify the Input Model</span></span> <a title="Permalink" class="permalink" href="#nsx-modify-input-model">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-modify-input-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  After the third-party import has completed successfully, modify the input
  model:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Prepare for input model changes
   </p></li><li class="step"><p>
    Define the servers and server roles needed for a NSX-v cloud.
   </p></li><li class="step"><p>
    Define the necessary networks and network groups
   </p></li><li class="step"><p>
    Specify the services needed to be deployed on the Cloud Lifecycle Manager controllers and the
    Nova ESX compute proxy nodes.
   </p></li><li class="step"><p>
    Commit the changes and run the configuration processor.
   </p></li></ol></div></div><section class="sect6" id="nsx-prepare-input-model-changes" data-id-title="Prepare for Input Model Changes"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.1 </span><span class="title-name">Prepare for Input Model Changes</span></span> <a title="Permalink" class="permalink" href="#nsx-prepare-input-model-changes">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-prepare-input-model-changes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The previous steps created a modified <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tarball with the NSX-v
  core plugin in the Neutron and <code class="literal">neutronclient</code> venvs. The
  <code class="filename">tar</code> file can now be extracted and the
  <code class="filename">ardana-init.bash</code> script can be run to set up the
  deployment files and directories. If a modified <code class="filename">tar</code> file
  was not created, then extract the tar from the /media/cdrom/ardana location.
 </p><p>
  To run the <code class="filename">ardana-init.bash</code> script which is included in
  the build, use this commands:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/ardana/hos-init.bash</pre></div></section><section class="sect6" id="nsx-create-input-model" data-id-title="Create the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.2 </span><span class="title-name">Create the Input Model</span></span> <a title="Permalink" class="permalink" href="#nsx-create-input-model">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-create-input-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copy the example input model to
  <code class="filename">~/openstack/my_cloud/definition/</code> directory:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx
<code class="prompt user">ardana &gt; </code>cp -R entry-scale-nsx ~/openstack/my_cloud/definition</pre></div><p>
  Refer to the reference input model in
  <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
  for details about how these definitions should be made.  The main differences
  between this model and the standard Cloud Lifecycle Manager input models are:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Only the neutron-server is deployed.  No other neutron agents are deployed.
   </p></li><li class="listitem"><p>
    Additional parameters need to be set in
    <code class="filename">pass_through.yml</code> and
    <code class="filename">nsx/nsx_config.yml</code>.
   </p></li><li class="listitem"><p>
    Nova ESX compute proxy nodes may be ESX virtual machines.
   </p></li></ul></div><section class="sect6" id="id-1.4.5.11.9.4.11.9.5.5.6" data-id-title="Set up the Parameters"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.2.1 </span><span class="title-name">Set up the Parameters</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.9.4.11.9.5.5.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-create-input-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The special parameters needed for the NSX-v integrations are set in the
   files <code class="filename">pass_through.yml</code> and
   <code class="filename">nsx/nsx_config.yml</code>. They are in the
   <code class="filename">~/openstack/my_cloud/definition/data</code> directory.
  </p><p>
   Parameters in <code class="filename">pass_through.yml</code> are in the sample input
   model in the
   <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
   directory.  The comments in the sample input model file describe how to
   locate the values of the required parameters.
  </p><div class="verbatim-wrap"><pre class="screen">#
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <em class="replaceable">VCENTER_ADMIN_USERNAME</em>
        ip: <em class="replaceable">VCENTER_IP</em>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for the vCenter password. The string
        # generated is the encrypted password. Enter the string
        # enclosed by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_PASSWD_FROM_ARDANAENCRYPT</em>"

        # The id is is obtained by the URL
        # https://<em class="replaceable">VCENTER_IP</em>/mob/?moid=ServiceInstance&amp;doPath=content%2eabout,
        # field instanceUUID.
        id: <em class="replaceable">VCENTER_UUID</em>
  servers:
    -
      # Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml. There should be one esx-compute-proxy node per ESX
      # resource pool.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER1_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em>
    -
      id: esx-compute2
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER2_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em></pre></div><p>
   There are parameters in <code class="filename">nsx/nsx_config.yml</code>.  The
   comments describes how to retrieve the values.
  </p><div class="verbatim-wrap"><pre class="screen"># (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name: NSX-CONFIG-CP1
      services:
        - nsx
      data:
        # (Required) URL for NSXv manager (e.g - https://management_ip).
        manager_uri: 'https://<em class="replaceable">NSX_MGR_IP</em>

        # (Required) NSXv username.
        user: 'admin'

        # (Required) Encrypted NSX Manager password.
        # Password encryption is done by the script
        # ~/openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # NOTE: Make sure that the NSX Manager password is encrypted with the same key
        # used to encrypt the VCenter password.
        #
        # The script will prompt for the NSX Manager password. The string
        # generated is the encrypted password. Enter the string enclosed
        # by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_NSX_MGR_PASSWD_FROM_ARDANAENCRYPT</em>"
        # (Required) datacenter id for edge deployment.
        # Retrieved using
        #    http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob/?moid=ServiceInstance&amp;doPath=content
        # click on the value from the rootFolder property. The datacenter_moid is
        # the value of the childEntity property.
        # The vCenter-ip-address comes from the file pass_through.yml in the
        # input model under "pass-through.global.vmware.ip".
        datacenter_moid: 'datacenter-21'
        # (Required) id of logic switch for physical network connectivity.
        # How to retrieve
        # 1. Get to the same page where the datacenter_moid is found.
        # 2. Click on the value of the rootFolder property.
        # 3. Click on the value of the childEntity property
        # 4. Look at the network property. The external network is
        #    network associated with EXTERNAL VM in VCenter.
        external_network: 'dvportgroup-74'
        # (Required) clusters ids containing OpenStack hosts.
        # Retrieved using http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob, click on the value
        # from the rootFolder property. Then click on the value of the
        # hostFolder property. Cluster_moids are the values under childEntity
        # property of the compute clusters.
        cluster_moid: 'domain-c33,domain-c35'
        # (Required) resource-pool id for edge deployment.
        resource_pool_id: 'resgroup-67'
        # (Optional) datastore id for edge deployment. If not needed,
        # do not declare it.
        # datastore_id: 'datastore-117'

        # (Required) network scope id of the transport zone.
        # To get the vdn_scope_id, in the vSphere web client from the Home
        # menu:
        #   1. click on Networking &amp; Security
        #   2. click on installation
        #   3. click on the Logical Netowrk Preparation tab.
        #   4. click on the Transport Zones button.
        #   5. Double click on the transport zone being configure.
        #   6. Select Manage tab.
        #   7. The vdn_scope_id will appear at the end of the URL.
        vdn_scope_id: 'vdnscope-1'

        # (Optional) Dvs id for VLAN based networks. If not needed,
        # do not declare it.
        # dvs_id: 'dvs-68'

        # (Required) backup_edge_pool: backup edge pools management range,
        # - edge_type&gt;[edge_size]:<em class="replaceable">MINIMUM_POOLED_EDGES</em>:<em class="replaceable">MAXIMUM_POOLED_EDGES</em>
        # - edge_type: service (service edge) or  vdr (distributed edge)
        # - edge_size:  compact ,  large (by default),  xlarge  or  quadlarge
        backup_edge_pool: 'service:compact:4:10,vdr:compact:4:10'

        # (Optional) mgt_net_proxy_ips: management network IP address for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_ips: '10.142.14.251,10.142.14.252'

        # (Optional) mgt_net_proxy_netmask: management network netmask for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_netmask: '255.255.255.0'

        # (Optional) mgt_net_moid: Network ID for management network connectivity
        # Do not declare if not used.
        # mgt_net_moid: 'dvportgroup-73'

        # ca_file: Name of the certificate file. If insecure is set to True,
        # then this parameter is ignored. If insecure is set to False and this
        # parameter is not defined, then the system root CAs will be used
        # to verify the server certificate.
        ca_file: a/nsx/certificate/file

        # insecure:
        # If true (default), the NSXv server certificate is not verified.
        # If false, then the default CA truststore is used for verification.
        # This option is ignored if "ca_file" is set
        insecure: True
        # (Optional) edge_ha: if true, will duplicate any edge pool resources
        # Default to False if undeclared.
        # edge_ha: False
        # (Optional) spoofguard_enabled:
        # If True (default), indicates NSXV spoofguard component is used to
        # implement port-security feature.
        # spoofguard_enabled: True
        # (Optional) exclusive_router_appliance_size:
        # Edge appliance size to be used for creating exclusive router.
        # Valid values: 'compact', 'large', 'xlarge', 'quadlarge'
        # Defaults to 'compact' if not declared.  # exclusive_router_appliance_size:
        'compact'</pre></div></section></section><section class="sect6" id="commit-config-processor" data-id-title="Commit Changes and Run the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.3 </span><span class="title-name">Commit Changes and Run the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="#commit-config-processor">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/commit-config_processor.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Commit your changes with the input model and the required configuration
  values added to the <code class="filename">pass_through.yml</code> and
  <code class="filename">nsx/nsx_config.yml</code> files.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git commit -A -m "Configuration changes for NSX deployment"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
 -e \encrypt="" -e rekey=""</pre></div><p>
  If the playbook <code class="filename">config-processor-run.yml</code> fails, there is
  an error in the input model. Fix the error and repeat the above steps.
 </p></section></section><section class="sect5" id="nsx-deploy-os-cobbler" data-id-title="Deploying the Operating System with Cobbler"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.3 </span><span class="title-name">Deploying the Operating System with Cobbler</span></span> <a title="Permalink" class="permalink" href="#nsx-deploy-os-cobbler">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-deploy-os-cobbler.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    From the Cloud Lifecycle Manager, run Cobbler to install the operating system on the nodes
    after it has to be deployed:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
    Verify the nodes that will have an operating system installed by Cobbler by
    running this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system find --netboot-enabled=1</pre></div></li><li class="step"><p>
    Reimage the nodes using Cobbler.  Do not use Cobbler to reimage the nodes
    running as ESX virtual machines. The command below is run on a setup where
    the Nova ESX compute proxies are VMs. Controllers 1, 2, and 3 are
    running on physical servers.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e \
   nodelist=controller1,controller2,controller3</pre></div></li><li class="step"><p>
    When the playbook has completed, each controller node should have an
    operating system installed with an IP address configured on
    <code class="literal">eth0</code>.
   </p></li><li class="step"><p>
    After your controller nodes have been completed, you should install the
    operating system on your Nova compute proxy virtual machines. Each
    configured virtual machine should be able to PXE boot into the operating
    system installer.
   </p></li><li class="step"><p>
    From within the vSphere environment, power on each Nova compute proxy
    virtual machine and watch for it to PXE boot into the OS installer via its
    console.
   </p><ol type="a" class="substeps"><li class="step"><p>
    If successful, the virtual machine will have the operating system
    automatically installed and will then automatically power off.
     </p></li><li class="step"><p>
      When the virtual machine has powered off, power it on and let it boot
      into the operating system.
     </p></li></ol></li><li class="step"><p>
    Verify network settings after deploying the operating system to each node.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Verify that the NIC bus mapping specified in the cloud model input file
      (<code class="filename">~/ardana/my_cloud/definition/data/nic_mappings.yml</code>)
      matches the NIC bus mapping on each <span class="productname">OpenStack</span> node.
     </p><p>
      Check the NIC bus mapping with this command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem"><p>
      After the playbook has completed, each controller node should have an
      operating system installed with an IP address configured on eth0.
     </p></li></ul></div></li><li class="step"><p>
    When the ESX compute proxy nodes are VMs, install the operating system if
    you have not already done so.
   </p></li></ol></div></div></section><section class="sect5" id="nsx-deploy-cloud" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.4 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#nsx-deploy-cloud">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-deploy-cloud.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  When the configuration processor has completed successfully, the cloud can be
  deployed. Set the ARDANA_USER_PASSWORD_ENCRYPT_KEY environment
  variable before running <code class="filename">site.yml</code>.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">PASSWORD_KEY</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><p>
<em class="replaceable">PASSWORD_KEY</em> in the <code class="literal">export</code>
command is the key used to encrypt the passwords for vCenter and NSX Manager.
</p></section></section></section></section></section><section class="sect1" id="nsx-vsphere-baremetal" data-id-title="Integrating with NSX for vSphere on Baremetal"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.2 </span><span class="title-name">Integrating with NSX for vSphere on Baremetal</span></span> <a title="Permalink" class="permalink" href="#nsx-vsphere-baremetal">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes the installation steps and requirements for
  integrating with NSX for vSphere on baremetal physical hardware.
 </p><section class="sect2" id="id-1.4.5.11.10.3" data-id-title="Pre-Integration Checklist"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.2.1 </span><span class="title-name">Pre-Integration Checklist</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following installation and integration instructions assumes an
   understanding of VMware's ESXI and vSphere products for setting up virtual
   environments.
  </p><p>
   Please review the following requirements for the VMware vSphere environment.
  </p><p>
   <span class="bold"><strong>Software Requirements</strong></span>
  </p><p>
   Before you install or upgrade NSX, verify your software versions. The
   following are the required versions.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Software
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Version
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        8
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        VMware NSX-v Manager
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        6.3.4 or higher
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        VMWare NSX-v Neutron Plugin
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Pike Release (TAG=11.0.0)
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        VMWare ESXi and vSphere Appliance (vSphere web Client)
       </p>
      </td><td>
       <p>
        6.0 or higher
       </p>
      </td></tr></tbody></table></div><p>
   A vCenter server (appliance) is required to manage the vSphere environment.
   It is recommended that you install a vCenter appliance as an ESX virtual
   machine.
  </p><div id="id-1.4.5.11.10.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Each ESXi compute cluster is required to have shared storage between the
    hosts in the cluster, otherwise attempts to create instances through
    nova-compute will fail.
   </p></div></section><section class="sect2" id="id-1.4.5.11.10.4" data-id-title="Installing on Baremetal"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.2.2 </span><span class="title-name">Installing on Baremetal</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="productname">OpenStack</span> can be deployed in two ways: on baremetal (physical hardware) or in
   an ESXi virtual environment on virtual machines. The following instructions
   describe how to install <span class="productname">OpenStack</span> on baremetal nodes with vCenter and NSX
   Manager running as virtual machines. For instructions on virtual machine
   installation, see <a class="xref" href="#nsx-vsphere-vm" title="16.1. Integrating with NSX for vSphere">Section 16.1, “Integrating with NSX for vSphere”</a>.
  </p><p>
   This deployment example will consist of two ESXi clusters at minimum: a
   <code class="literal">control-plane</code> cluster and a <code class="literal">compute</code>
   cluster. The control-plane cluster must have 3 ESXi hosts minimum (due to
   VMware's recommendation that each NSX controller virtual machine is on a
   separate host). The compute cluster must have 2 ESXi hosts minimum. There
   can be multiple compute clusters. The following table outlines the virtual
   machine specifications to be built in the control-plane cluster:
  </p><div class="table" id="nsx-hw-reqs-bm" data-id-title="NSX Hardware Requirements for Baremetal Integration"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.2: </span><span class="title-name">NSX Hardware Requirements for Baremetal Integration </span></span><a title="Permalink" class="permalink" href="#nsx-hw-reqs-bm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/><col class="5"/><col class="6"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Virtual Machine Role
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Required Number
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Disk
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Memory
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Network
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        CPU
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Compute virtual machines
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        1 per compute cluster
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        80GB
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        4GB
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        3 VMXNET Virtual Network Adapters
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        2 vCPU
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        NSX Edge Gateway/DLR/Metadata-proxy appliances
       </p>
      </td><td style="border-right: 1px solid ; ">
       
      </td><td style="border-right: 1px solid ; ">
       <p>
        Autogenerated by NSXv
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        Autogenerated by NSXv
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        Autogenerated by NSXv
       </p>
      </td><td>
       <p>
        Autogenerated by NSXv
       </p>
      </td></tr></tbody></table></div></div><p>
   In addition to the ESXi hosts, it is recommended that there is one physical
   host for the Cloud Lifecycle Manager node and three physical hosts for the controller nodes.
  </p><section class="sect3" id="id-1.4.5.11.10.4.6" data-id-title="Network Requirements"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.1 </span><span class="title-name">Network Requirements</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.6">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    NSX-v requires the following for networking:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The ESXi hosts, vCenter, and the NSX Manager appliance must resolve DNS
      lookup.
     </p></li><li class="listitem"><p>
      The ESXi host must have the NTP service configured and enabled.
     </p></li><li class="listitem"><p>
      Jumbo frames must be enabled on the switch ports that the ESXi hosts are
      connected to.
     </p></li><li class="listitem"><p>
      The ESXi hosts must have at least 2 physical network cards each.
     </p></li></ul></div></section><section class="sect3" id="id-1.4.5.11.10.4.7" data-id-title="Network Model"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.2 </span><span class="title-name">Network Model</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.7">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The model in these instructions requires the following networks:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.11.10.4.7.3.1"><span class="term">ESXi Hosts and vCenter</span></dt><dd><p>
       This is the network that the ESXi hosts and vCenter use to route traffic
       with.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.2"><span class="term">NSX Management</span></dt><dd><p>
       The network which the NSX controllers and NSX Manager will use.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.3"><span class="term">NSX VTEP Pool</span></dt><dd><p>
       The network that NSX uses to create endpoints for VxLAN tunnels.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.4"><span class="term">Management</span></dt><dd><p>
       The network that <span class="productname">OpenStack</span> uses for deployment and maintenance of the
       cloud.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.5"><span class="term">Internal API (optional)</span></dt><dd><p>
       The network group that will be used for management (private API) traffic
       within the cloud.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.6"><span class="term">External API</span></dt><dd><p>
       This is the network that users will use to make requests to the cloud.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.7"><span class="term">External VM</span></dt><dd><p>
       VLAN-backed provider network for external access to guest VMs (floating
       IPs).
      </p></dd></dl></div></section><section class="sect3" id="id-1.4.5.11.10.4.8" data-id-title="vSphere port security settings"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.3 </span><span class="title-name">vSphere port security settings</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.8">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Even though the <span class="productname">OpenStack</span> deployment is on baremetal, it is still necessary
    to define each VLAN within a vSphere Distributed Switch for the Nova
    compute proxy virtual machine. Therefore, the vSphere port security
    settings are shown in the table below.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Network Group
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         VLAN Type
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Interface
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         vSphere Port Group Security Settings
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         IPMI
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Untagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         ESXi Hosts and vCenter
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Defaults
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         NSX Manager
        </p>
        <p>
         Must be able to reach ESXi Hosts and vCenter
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Defaults
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         NSX VTEP Pool
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Defaults
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Management
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged or Untagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         bond0
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>:Reject
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Internal API (Optional, may be combined with the Management Network.
         If network segregation is required for security reasons, you can keep
         this as a separate network.)
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         bond0
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         External API (Public)
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         External VM
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr></tbody></table></div></section><section class="sect3" id="id-1.4.5.11.10.4.9" data-id-title="Configuring the vSphere Environment"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4 </span><span class="title-name">Configuring the vSphere Environment</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before deploying <span class="productname">OpenStack</span> with NSX-v, the VMware vSphere environment must
    be properly configured, including setting up vSphere distributed switches
    and port groups. For detailed instructions, see
    <a class="xref" href="#install-esx-ovsvapp" title="Chapter 15. Installing ESX Computes and OVSvAPP">Chapter 15, <em>Installing ESX Computes and OVSvAPP</em></a>.
   </p><p>
    Installing and configuring the VMware NSX Manager and creating the NSX
    network within the vSphere environment is covered below.
   </p><p>
    Before proceeding with the installation, ensure that the following are
    configured in the vSphere environment.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The vSphere datacenter is configured with at least two clusters, one
      <span class="bold"><strong>control-plane</strong></span> cluster and one
      <span class="bold"><strong>compute</strong></span> cluster.
     </p></li><li class="listitem"><p>
      Verify that all software, hardware, and networking requirements have been
      met.
     </p></li><li class="listitem"><p>
      Ensure the vSphere distributed virtual switches (DVS) are configured for
      each cluster.
     </p></li></ul></div><div id="id-1.4.5.11.10.4.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     The MTU setting for each DVS should be set to 1600. NSX should
     automatically apply this setting to each DVS during the setup process.
     Alternatively, the setting can be manually applied to each DVS before
     setup if desired.
    </p></div><p>
    Make sure there is a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
    <code class="literal">ardana</code> home directory,
    <code class="filename">var/lib/ardana</code>, and that it is called
    <code class="filename">sles12sp3.iso</code>.
   </p><p>
    Install the <code class="literal">open-vm-tools</code> package.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install open-vm-tools</pre></div><section class="sect4" id="id-1.4.5.11.10.4.9.10" data-id-title="Install NSX Manager"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.1 </span><span class="title-name">Install NSX Manager</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.10">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The NSX Manager is the centralized network management component of NSX. It
     provides a single point of configuration and REST API entry-points.
    </p><p>
     The NSX Manager is installed as a virtual appliance on one of the ESXi
     hosts within the vSphere environment. This guide will cover installing the
     appliance on one of the ESXi hosts within the control-plane cluster. For
     more detailed information, refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html" target="_blank">VMware's
     NSX Installation Guide.</a>
    </p><p>
     To install the NSX Manager, download the virtual appliance from
     <a class="link" href="https://www.vmware.com/go/download-nsx-vsphere" target="_blank">VMware</a>
     and deploy the appliance within vCenter onto one of the ESXi hosts. For
     information on deploying appliances within vCenter, refer to VMware's
     documentation for ESXi
     <a class="link" href="https://pubs.vmware.com/vsphere-55/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">5.5</a>
     or
     <a class="link" href="https://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">6.0</a>.
    </p><p>
     During the deployment of the NSX Manager appliance, be aware of the
     following:
    </p><p>
     When prompted, select <span class="guimenu">Accept extra configuration
     options</span>. This will present options for configuring IPv4 and IPv6
     addresses, the default gateway, DNS, NTP, and SSH properties during the
     installation, rather than configuring these settings manually after the
     installation.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Choose an ESXi host that resides within the control-plane cluster.
      </p></li><li class="listitem"><p>
       Ensure that the network mapped port group is the DVS port group that
       represents the VLAN the NSX Manager will use for its networking (in this
       example it is labeled as the <code class="literal">NSX Management</code> network).
      </p></li></ul></div><div id="id-1.4.5.11.10.4.9.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The IP address assigned to the NSX Manager must be able to resolve
      reverse DNS.
     </p></div><p>
     Power on the NSX Manager virtual machine after it finishes deploying and
     wait for the operating system to fully load. When ready, carry out the
     following steps to have the NSX Manager use single sign-on (SSO) and to
     register the NSX Manager with vCenter:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Open a web browser and enter the hostname or IP address that was
       assigned to the NSX Manager during setup.
      </p></li><li class="step"><p>
       Log in with the username <code class="literal">admin</code> and the password set
       during the deployment.
      </p></li><li class="step"><p>
       After logging in, click on <span class="guimenu">Manage vCenter
       Registration</span>.
      </p></li><li class="step"><p>
       Configure the NSX Manager to connect to the vCenter server.
      </p></li><li class="step"><p>
       Configure NSX manager for single sign on (SSO) under the <span class="guimenu">Lookup
       Server URL</span> section.
      </p></li></ol></div></div><div id="id-1.4.5.11.10.4.9.10.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      When configuring SSO, use <code class="literal">Lookup Service Port 443</code> for
      vCenter version 6.0. Use <code class="literal">Lookup Service Port 7444</code> for
      vCenter version 5.5.
     </p><p>
      SSO makes vSphere and NSX more secure by allowing the various components
      to communicate with each other through a secure token exchange mechanism,
      instead of requiring each component to authenticate a user separately.
      For more details, refer to VMware's documentation on
      <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html" target="_blank">Configure
      Single Sign-On</a>.
     </p></div><p>
     Both the <code class="literal">Lookup Service URL</code> and the <code class="literal">vCenter
     Server</code> sections should have a status of
     <code class="literal">connected</code> when configured properly.
    </p><p>
     Log into the vSphere Web Client (log out and and back in if already logged
     in). The NSX Manager will appear under the <span class="guimenu">Networking &amp;
     Security</span> section of the client.
    </p><div id="id-1.4.5.11.10.4.9.10.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The <span class="guimenu">Networking &amp; Security</span> section will not appear
      under the vSphere desktop client. Use of the web client is required for
      the rest of this process.
     </p></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.11" data-id-title="Add NSX Controllers"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.2 </span><span class="title-name">Add NSX Controllers</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.11">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The NSX controllers serve as the central control point for all logical
     switches within the vSphere environment's network, and they maintain
     information about all hosts, logical switches (VXLANs), and distributed
     logical routers.
    </p><p>
     NSX controllers will each be deployed as a virtual appliance on the ESXi
     hosts within the control-plane cluster to form the NSX Controller cluster.
     For details about NSX controllers and the NSX control plane in general,
     refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-4E0FEE83-CF2C-45E0-B0E6-177161C3D67C.html" target="_blank">VMware's
     NSX documentation</a>.
    </p><div id="id-1.4.5.11.10.4.9.11.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      Whatever the size of the NSX deployment, the following conditions must be
      met:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Each NSX Controller cluster must contain three controller nodes. Having
        a different number of controller nodes is not supported.
       </p></li><li class="listitem"><p>
        Before deploying NSX Controllers, you must deploy an NSX Manager
        appliance and register vCenter with NSX Manager.
       </p></li><li class="listitem"><p>
        Determine the IP pool settings for your controller cluster, including
        the gateway and IP address range. DNS settings are optional.
       </p></li><li class="listitem"><p>
        The NSX Controller IP network must have connectivity to the NSX Manager
        and to the management interfaces on the ESXi hosts.
       </p></li></ul></div></div><p>
     Log in to the vSphere web client and do the following steps to add the NSX
     controllers:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In vCenter, navigate to <span class="guimenu">Home</span>, select
       <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>, and then
       select the <span class="guimenu">Management</span> tab.
      </p></li><li class="step"><p>
       In the <span class="guimenu">NSX Controller nodes</span> section, click the
       <span class="guimenu">Add Node</span> icon represented by a green plus sign.
      </p></li><li class="step"><p>
       Enter the NSX Controller settings appropriate to your environment. If
       you are following this example, use the control-plane clustered ESXi
       hosts and control-plane DVS port group for the controller settings.
      </p></li><li class="step"><p>
       If it has not already been done, create an IP pool for the NSX
       Controller cluster with at least three IP addressess by clicking
       <span class="guimenu">New IP Pool</span>. Individual controllers can be in
       separate IP subnets, if necessary.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to deploy the controller. After the first
       controller is completely deployed, deploy two additional controllers.
      </p></li></ol></div></div><div id="id-1.4.5.11.10.4.9.11.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      Three NSX controllers is mandatory. VMware recommends configuring a DRS
      anti-affinity rule to prevent the controllers from residing on the same
      ESXi host. See more information about
      <a class="link" href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.resmgmt.doc%2FGUID-FF28F29C-8B67-4EFF-A2EF-63B3537E6934.html" target="_blank">DRS
      Affinity Rules</a>.
     </p></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.12" data-id-title="Prepare Clusters for NSX Management"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.3 </span><span class="title-name">Prepare Clusters for NSX Management</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.12">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     During <span class="guimenu">Host Preparation</span>, the NSX Manager:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Installs the NSX kernel modules on ESXi hosts that are members of
       vSphere clusters
      </p></li><li class="listitem"><p>
       Builds the NSX control-plane and management-plane infrastructure
      </p></li></ul></div><p>
     The NSX kernel modules are packaged in <code class="filename">VIB</code> (vSphere
     Installation Bundle) files. They run within the hypervisor kernel and
     provide services such as distributed routing, distributed firewall, and
     VXLAN bridging capabilities. These files are installed on a per-cluster
     level, and the setup process deploys the required software on all ESXi
     hosts in the target cluster. When a new ESXi host is added to the cluster,
     the required software is automatically installed on the newly added host.
    </p><p>
     Before beginning the NSX host preparation process, make sure of the
     following in your environment:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Register vCenter with NSX Manager and deploy the NSX controllers.
      </p></li><li class="listitem"><p>
       Verify that DNS reverse lookup returns a fully qualified domain name
       when queried with the IP address of NSX Manager.
      </p></li><li class="listitem"><p>
       Verify that the ESXi hosts can resolve the DNS name of vCenter server.
      </p></li><li class="listitem"><p>
       Verify that the ESXi hosts can connect to vCenter Server on port 80.
      </p></li><li class="listitem"><p>
       Verify that the network time on vCenter Server and the ESXi hosts is
       synchronized.
      </p></li><li class="listitem"><p>
       For each vSphere cluster that will participate in NSX, verify that the
       ESXi hosts within each respective cluster are attached to a common VDS.
      </p><p>
       For example, given a deployment with two clusters named Host1 and Host2.
       Host1 is attached to VDS1 and VDS2. Host2 is attached to VDS1 and VDS3.
       When you prepare a cluster for NSX, you can only associate NSX with VDS1
       on the cluster. If you add another host (Host3) to the cluster and Host3
       is not attached to VDS1, it is an invalid configuration, and Host3 will
       not be ready for NSX functionality.
      </p></li><li class="listitem"><p>
       If you have vSphere Update Manager (VUM) in your environment, you must
       disable it before preparing clusters for network virtualization. For
       information on how to check if VUM is enabled and how to disable it if
       necessary, see the
       <a class="link" href="http://kb.vmware.com/kb/2053782" target="_blank">VMware knowledge
       base</a>.
      </p></li><li class="listitem"><p>
       In the vSphere web client, ensure that the cluster is in the resolved
       state (listed under the <span class="guimenu">Host Preparation</span> tab). If the
       Resolve option does not appear in the cluster's Actions list, then it is
       in a resolved state.
      </p></li></ul></div><p>
     To prepare the vSphere clusters for NSX:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In vCenter, select
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>, and then
       select the <span class="guimenu">Host Preparation</span> tab.
      </p></li><li class="step"><p>
       Continuing with the example in these instructions, click on the
       <span class="guimenu">Actions</span> button (gear icon) and select
       <span class="guimenu">Install</span> for both the control-plane cluster and
       compute cluster (if you are using something other than this example,
       then only install on the clusters that require NSX logical switching,
       routing, and firewalls).
      </p></li><li class="step"><p>
       Monitor the installation until the <code class="literal">Installation
       Status</code> column displays a green check mark.
      </p><div id="id-1.4.5.11.10.4.9.12.8.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        While installation is in progress, do not deploy, upgrade, or uninstall
        any service or component.
       </p></div><div id="id-1.4.5.11.10.4.9.12.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        If the <code class="literal">Installation Status</code> column displays a red
        warning icon and says <code class="literal">Not Ready</code>, click
        <span class="guimenu">Resolve</span>. Clicking <span class="guimenu">Resolve</span> might
        result in a reboot of the host. If the installation is still not
        successful, click the warning icon. All errors will be displayed. Take
        the required action and click <span class="guimenu">Resolve</span> again.
       </p></div></li><li class="step"><p>
       To verify the VIBs (<code class="filename">esx-vsip</code> and
       <code class="filename">esx-vxlan</code>) are installed and registered, SSH into
       an ESXi host within the prepared cluster. List the names and versions of
       the VIBs installed by running the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>esxcli software vib list | grep esx</pre></div><div class="verbatim-wrap"><pre class="screen">...
esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
...</pre></div></li></ol></div></div><div id="id-1.4.5.11.10.4.9.12.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      After host preparation:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        A host reboot is not required
       </p></li><li class="listitem"><p>
        If you add a host to a prepared cluster, the NSX VIBs are automatically
        installed on the host.
       </p></li><li class="listitem"><p>
        If you move a host to an unprepared cluster, the NSX VIBs are
        automatically uninstalled from the host. In this case, a host reboot is
        required to complete the uninstall process.
       </p></li></ul></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.13" data-id-title="Configure VXLAN Transport Parameters"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.4 </span><span class="title-name">Configure VXLAN Transport Parameters</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.13">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     VXLAN is configured on a per-cluster basis, where each vSphere cluster
     that is to participate in NSX is mapped to a vSphere Distributed Virtual
     Switch (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in
     that cluster is enabled for logical switches. The settings chosen in this
     section will be used in creating the VMkernel interface.
    </p><p>
     Configuring transport parameters involves selecting a DVS, a VLAN ID, an
     MTU size, an IP addressing mechanism, and a NIC teaming policy. The MTU
     for each switch must be set to 1550 or higher. By default, it is set to
     1600 by NSX. This is also the recommended setting for integration with
     <span class="productname">OpenStack</span>.
    </p><p>
     To configure the VXLAN transport parameters:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Host Preparation</span> tab.
      </p></li><li class="step"><p>
       Click the <span class="guimenu">Configure</span> link in the VXLAN column.
      </p></li><li class="step"><p>
       Enter the required information.
      </p></li><li class="step"><p>
       If you have not already done so, create an IP pool for the VXLAN tunnel
       end points (VTEP) by clicking <span class="guimenu">New IP Pool</span>:
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to create the VXLAN network.
      </p></li></ol></div></div><p>
     When configuring the VXLAN transport network, consider the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Use a NIC teaming policy that best suits the environment being built.
       <code class="literal">Load Balance - SRCID</code> as the VMKNic teaming policy is
       usually the most flexible out of all the available options. This allows
       each host to have a VTEP vmkernel interface for each dvuplink on the
       selected distributed switch (two dvuplinks gives two VTEP interfaces per
       ESXi host).
      </p></li><li class="listitem"><p>
       Do not mix different teaming policies for different portgroups on a VDS
       where some use Etherchannel or Link Aggregation Control Protocol (LACPv1
       or LACPv2) and others use a different teaming policy. If uplinks are
       shared in these different teaming policies, traffic will be interrupted.
       If logical routers are present, there will be routing problems. Such a
       configuration is not supported and should be avoided.
      </p></li><li class="listitem"><p>
       For larger environments it may be better to use DHCP for the VMKNic IP
       Addressing.
      </p></li><li class="listitem"><p>
       For more information and further guidance, see the
       <a class="link" href="https://communities.vmware.com/docs/DOC-27683" target="_blank">VMware
       NSX for vSphere Network Virtualization Design Guide</a>.
      </p></li></ul></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.14" data-id-title="Assign Segment ID Pool"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.5 </span><span class="title-name">Assign Segment ID Pool</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.14">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </p><p>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
     documentation</a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Logical Network Preparation</span> tab.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Segment ID</span>, and then <span class="guimenu">Edit</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save your changes.
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.15" data-id-title="Assign Segment ID Pool"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.6 </span><span class="title-name">Assign Segment ID Pool</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.15">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </p><p>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
     documentation</a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Logical Network Preparation</span> tab.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Segment ID</span>, and then <span class="guimenu">Edit</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save your changes.
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.16" data-id-title="Create a Transport Zone"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.7 </span><span class="title-name">Create a Transport Zone</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.16">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     A transport zone controls which hosts a logical switch can reach and has
     the following characteristics.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       It can span one or more vSphere clusters.
      </p></li><li class="listitem"><p>
       Transport zones dictate which clusters can participate in the use of a
       particular network. Therefore they dictate which VMs can participate in
       the use of a particular network.
      </p></li><li class="listitem"><p>
       A vSphere NSX environment can contain one or more transport zones based
       on the environment's requirements.
      </p></li><li class="listitem"><p>
       A host cluster can belong to multiple transport zones.
      </p></li><li class="listitem"><p>
       A logical switch can belong to only one transport zone.
      </p></li></ul></div><div id="id-1.4.5.11.10.4.9.16.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      <span class="productname">OpenStack</span> has only been verified to work with a single transport zone
      within a vSphere NSX-v environment. Other configurations are currently
      not supported.
     </p></div><p>
     For more information on transport zones, refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html" target="_blank">VMware's
     Add A Transport Zone</a>.
    </p><p>
     To create a transport zone:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Logical Network Preparation</span> tab.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Transport Zones</span>, and then click the
       <span class="guimenu">New Transport Zone</span> (New Logical Switch) icon.
      </p></li><li class="step"><p>
       In the <span class="guimenu">New Transport Zone</span> dialog box, type a name and
       an optional description for the transport zone.
      </p></li><li class="step"><p>
       For these example instructions, select the control plane mode as
       <code class="literal">Unicast</code>.
      </p><div id="id-1.4.5.11.10.4.9.16.7.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Whether there is a controller in the environment or if the environment
        is going to use multicast addresses will determine the control plane
        mode to select:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">Unicast</code> (what this set of instructions uses): The
          control plane is handled by an NSX controller. All unicast traffic
          leverages optimized headend replication. No multicast IP addresses or
          special network configuration is required.
         </p></li><li class="listitem"><p>
          <code class="literal">Multicast</code>: Multicast IP addresses in the physical
          network are used for the control plane. This mode is recommended only
          when upgrading from older VXLAN deployments. Requires PIM/IGMP in the
          physical network.
         </p></li><li class="listitem"><p>
          <code class="literal">Hybrid</code>: Offloads local traffic replication to the
          physical network (L2 multicast). This requires IGMP snooping on the
          first-hop switch and access to an IGMP querier in each VTEP subnet,
          but does not require PIM. The first-hop switch handles traffic
          replication for the subnet.
         </p></li></ul></div></div></li><li class="step"><p>
       Select the clusters to be added to the transport zone.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save your changes.
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.17" data-id-title="Deploying SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.8 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.17">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     With vSphere environment setup completed, the <span class="productname">OpenStack</span> can be deployed.
     The following sections will cover creating virtual machines within the
     vSphere environment, configuring the cloud model and integrating NSX-v
     Neutron core plugin into the <span class="productname">OpenStack</span>:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create the virtual machines
      </p></li><li class="step"><p>
       Deploy the Cloud Lifecycle Manager
      </p></li><li class="step"><p>
       Configure the Neutron environment with NSX-v
      </p></li><li class="step"><p>
       Modify the cloud input model
      </p></li><li class="step"><p>
       Set up the parameters
      </p></li><li class="step"><p>
       Deploy the Operating System with Cobbler
      </p></li><li class="step"><p>
       Deploy the cloud
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.18" data-id-title="Deploying SUSE OpenStack Cloud on Baremetal"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> on Baremetal</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Within the vSphere environment, create the <span class="productname">OpenStack</span> compute proxy virtual
     machines. There needs to be one Neutron compute proxy virtual machine per
     ESXi compute cluster.
    </p><p>
     For the minimum NSX hardware requirements, refer to
     <a class="xref" href="#nsx-hw-reqs-bm" title="NSX Hardware Requirements for Baremetal Integration">Table 16.2, “NSX Hardware Requirements for Baremetal Integration”</a>. Also be aware of the networking
     model to use for the VM network interfaces, see
     <a class="xref" href="#nsx-interface-reqs" title="NSX Interface Requirements">Table 16.3, “NSX Interface Requirements”</a>:
    </p><p>
     If ESX VMs are to be used as Nova compute proxy nodes, set up three
     LAN interfaces in each virtual machine as shown in the table below. There
     is at least one Nova compute proxy node per cluster.
    </p><div class="table" id="nsx-interface-reqs" data-id-title="NSX Interface Requirements"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.3: </span><span class="title-name">NSX Interface Requirements </span></span><a title="Permalink" class="permalink" href="#nsx-interface-reqs">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Network Group
         </p>
        </th><th style="border-bottom: 1px solid ; ">
         <p>
          Interface
         </p>
        </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Management
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          <code class="literal">eth0</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          External API
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          <code class="literal">eth1</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Internal API
         </p>
        </td><td>
         <p>
          <code class="literal">eth2</code>
         </p>
        </td></tr></tbody></table></div></div><section class="sect5" id="id-1.4.5.11.10.4.9.18.6" data-id-title="Advanced Configuration Option"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.1 </span><span class="title-name">Advanced Configuration Option</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.11.10.4.9.18.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       Within vSphere for each in the virtual machine:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         In the <span class="guimenu">Options</span> section, under <span class="guimenu">Advanced
         configuration parameters</span>, ensure that
         <code class="literal">disk.EnableUUIDoption</code> is set to
         <code class="literal">true</code>.
        </p></li><li class="listitem"><p>
         If the option does not exist, it must be added. This option is
         required for the <span class="productname">OpenStack</span> deployment.
        </p></li><li class="listitem"><p>
         If the option is not specified, then the deployment will fail when
         attempting to configure the disks of each virtual machine.
        </p></li></ul></div></div></section><section class="sect5" id="id-1.4.5.11.10.4.9.18.7" data-id-title="Setting Up the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.2 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.7">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.7.2" data-id-title="Installing the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.2.1 </span><span class="title-name">Installing the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.7.2">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the product from:
     </p><ol type="a" class="substeps"><li class="step"><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step"><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step"><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step"><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step"><p>
      Select the location.
     </p></li><li class="step"><p>
      Select the keyboard layout.
     </p></li><li class="step"><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step"><p>
      Create new account:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Enter a username.
       </p></li><li class="step"><p>
        Enter a password.
       </p></li><li class="step"><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step"><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.4.5.11.10.4.9.18.7.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></section></section><section class="sect5" id="id-1.4.5.11.10.4.9.18.8" data-id-title="Configure the Neutron Environment with NSX-v"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3 </span><span class="title-name">Configure the Neutron Environment with NSX-v</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      In summary, integrating NSX with vSphere has four major steps:
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Modify the input model to define the server roles, servers, network
        roles and networks. <a class="xref" href="#nsx-modify-input-model" title="16.1.2.5.3.2. Modify the Input Model">Section 16.1.2.5.3.2, “Modify the Input Model”</a>
       </p></li><li class="step"><p>
        Set up the parameters needed for Neutron and Nova to communicate
        with the ESX and NSX Manager. <a class="xref" href="#nsx-deploy-os-cobbler" title="16.1.2.5.3.3. Deploying the Operating System with Cobbler">Section 16.1.2.5.3.3, “Deploying the Operating System with Cobbler”</a>
       </p></li><li class="step"><p>
        Do the steps to deploy the cloud. <a class="xref" href="#nsx-deploy-cloud" title="16.1.2.5.3.4. Deploying the Cloud">Section 16.1.2.5.3.4, “Deploying the Cloud”</a>
       </p></li></ol></div></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.4" data-id-title="Third-Party Import of VMware NSX-v Into Neutron and Neutronclient"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.1 </span><span class="title-name">Third-Party Import of VMware NSX-v Into Neutron and Neutronclient</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
       To import the NSX-v Neutron core-plugin into Cloud Lifecycle Manager, run the third-party
       import playbook.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost third-party-import.yml</pre></div></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5" data-id-title="Modify the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2 </span><span class="title-name">Modify the Input Model</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
       After the third-party import has completed successfully, modify the
       input model:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Prepare for input model changes
        </p></li><li class="step"><p>
         Define the servers and server roles needed for a NSX-v cloud.
        </p></li><li class="step"><p>
         Define the necessary networks and network groups
        </p></li><li class="step"><p>
         Specify the services needed to be deployed on the Cloud Lifecycle Manager controllers
         and the Nova ESX compute proxy nodes.
        </p></li><li class="step"><p>
         Commit the changes and run the configuration processor.
        </p></li></ol></div></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.4" data-id-title="Prepare for Input Model Changes"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.1 </span><span class="title-name">Prepare for Input Model Changes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.5.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The previous steps created a modified <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tarball with the
        NSX-v core plugin in the Neutron and <code class="literal">neutronclient</code>
        venvs. The <code class="filename">tar</code> file can now be extracted and the
        <code class="filename">ardana-init.bash</code> script can be run to set up the
        deployment files and directories. If a modified
        <code class="filename">tar</code> file was not created, then extract the tar
        from the /media/cdrom/ardana location.
       </p><p>
        To run the <code class="filename">ardana-init.bash</code> script which is
        included in the build, use this commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/ardana/hos-init.bash</pre></div></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.5" data-id-title="Create the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.2 </span><span class="title-name">Create the Input Model</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.5.5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Copy the example input model to
        <code class="filename">~/openstack/my_cloud/definition/</code> directory:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx
<code class="prompt user">ardana &gt; </code>cp -R entry-scale-nsx ~/openstack/my_cloud/definition</pre></div><p>
        Refer to the reference input model in
        <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
        for details about how these definitions should be made. The main
        differences between this model and the standard Cloud Lifecycle Manager input models are:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Only the neutron-server is deployed. No other neutron agents are
          deployed.
         </p></li><li class="listitem"><p>
          Additional parameters need to be set in
          <code class="filename">pass_through.yml</code> and
          <code class="filename">nsx/nsx_config.yml</code>.
         </p></li><li class="listitem"><p>
          Nova ESX compute proxy nodes may be ESX virtual machines.
         </p></li></ul></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.5.6" data-id-title="Set up the Parameters"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.2.1 </span><span class="title-name">Set up the Parameters</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.5.5.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         The special parameters needed for the NSX-v integrations are set in
         the files <code class="filename">pass_through.yml</code> and
         <code class="filename">nsx/nsx_config.yml</code>. They are in the
         <code class="filename">~/openstack/my_cloud/definition/data</code> directory.
        </p><p>
         Parameters in <code class="filename">pass_through.yml</code> are in the sample
         input model in the
         <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
         directory. The comments in the sample input model file describe how to
         locate the values of the required parameters.
        </p><div class="verbatim-wrap"><pre class="screen">#
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <em class="replaceable">VCENTER_ADMIN_USERNAME</em>
        ip: <em class="replaceable">VCENTER_IP</em>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for the vCenter password. The string
        # generated is the encrypted password. Enter the string
        # enclosed by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_PASSWD_FROM_ARDANAENCRYPT</em>"

        # The id is is obtained by the URL
        # https://<em class="replaceable">VCENTER_IP</em>/mob/?moid=ServiceInstance&amp;doPath=content%2eabout,
        # field instanceUUID.
        id: <em class="replaceable">VCENTER_UUID</em>
  servers:
    -
      # Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml. There should be one esx-compute-proxy node per ESX
      # resource pool.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER1_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em>
    -
      id: esx-compute2
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER2_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em></pre></div><p>
         There are parameters in <code class="filename">nsx/nsx_config.yml</code>. The
         comments describes how to retrieve the values.
        </p><div class="verbatim-wrap"><pre class="screen"># (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name: NSX-CONFIG-CP1
      services:
        - nsx
      data:
        # (Required) URL for NSXv manager (e.g - https://management_ip).
        manager_uri: 'https://<em class="replaceable">NSX_MGR_IP</em>

        # (Required) NSXv username.
        user: 'admin'

        # (Required) Encrypted NSX Manager password.
        # Password encryption is done by the script
        # ~/openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # NOTE: Make sure that the NSX Manager password is encrypted with the same key
        # used to encrypt the VCenter password.
        #
        # The script will prompt for the NSX Manager password. The string
        # generated is the encrypted password. Enter the string enclosed
        # by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_NSX_MGR_PASSWD_FROM_ARDANAENCRYPT</em>"
        # (Required) datacenter id for edge deployment.
        # Retrieved using
        #    http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob/?moid=ServiceInstance&amp;doPath=content
        # click on the value from the rootFolder property. The datacenter_moid is
        # the value of the childEntity property.
        # The vCenter-ip-address comes from the file pass_through.yml in the
        # input model under "pass-through.global.vmware.ip".
        datacenter_moid: 'datacenter-21'
        # (Required) id of logic switch for physical network connectivity.
        # How to retrieve
        # 1. Get to the same page where the datacenter_moid is found.
        # 2. Click on the value of the rootFolder property.
        # 3. Click on the value of the childEntity property
        # 4. Look at the network property. The external network is
        #    network associated with EXTERNAL VM in VCenter.
        external_network: 'dvportgroup-74'
        # (Required) clusters ids containing OpenStack hosts.
        # Retrieved using http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob, click on the value
        # from the rootFolder property. Then click on the value of the
        # hostFolder property. Cluster_moids are the values under childEntity
        # property of the compute clusters.
        cluster_moid: 'domain-c33,domain-c35'
        # (Required) resource-pool id for edge deployment.
        resource_pool_id: 'resgroup-67'
        # (Optional) datastore id for edge deployment. If not needed,
        # do not declare it.
        # datastore_id: 'datastore-117'

        # (Required) network scope id of the transport zone.
        # To get the vdn_scope_id, in the vSphere web client from the Home
        # menu:
        #   1. click on Networking &amp; Security
        #   2. click on installation
        #   3. click on the Logical Netowrk Preparation tab.
        #   4. click on the Transport Zones button.
        #   5. Double click on the transport zone being configure.
        #   6. Select Manage tab.
        #   7. The vdn_scope_id will appear at the end of the URL.
        vdn_scope_id: 'vdnscope-1'

        # (Optional) Dvs id for VLAN based networks. If not needed,
        # do not declare it.
        # dvs_id: 'dvs-68'

        # (Required) backup_edge_pool: backup edge pools management range,
        # - edge_type&gt;[edge_size]:<em class="replaceable">MINIMUM_POOLED_EDGES</em>:<em class="replaceable">MAXIMUM_POOLED_EDGES</em>
        # - edge_type: service (service edge) or  vdr (distributed edge)
        # - edge_size:  compact ,  large (by default),  xlarge  or  quadlarge
        backup_edge_pool: 'service:compact:4:10,vdr:compact:4:10'

        # (Optional) mgt_net_proxy_ips: management network IP address for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_ips: '10.142.14.251,10.142.14.252'

        # (Optional) mgt_net_proxy_netmask: management network netmask for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_netmask: '255.255.255.0'

        # (Optional) mgt_net_moid: Network ID for management network connectivity
        # Do not declare if not used.
        # mgt_net_moid: 'dvportgroup-73'

        # ca_file: Name of the certificate file. If insecure is set to True,
        # then this parameter is ignored. If insecure is set to False and this
        # parameter is not defined, then the system root CAs will be used
        # to verify the server certificate.
        ca_file: a/nsx/certificate/file

        # insecure:
        # If true (default), the NSXv server certificate is not verified.
        # If false, then the default CA truststore is used for verification.
        # This option is ignored if "ca_file" is set
        insecure: True
        # (Optional) edge_ha: if true, will duplicate any edge pool resources
        # Default to False if undeclared.
        # edge_ha: False
        # (Optional) spoofguard_enabled:
        # If True (default), indicates NSXV spoofguard component is used to
        # implement port-security feature.
        # spoofguard_enabled: True
        # (Optional) exclusive_router_appliance_size:
        # Edge appliance size to be used for creating exclusive router.
        # Valid values: 'compact', 'large', 'xlarge', 'quadlarge'
        # Defaults to 'compact' if not declared.  # exclusive_router_appliance_size:
        'compact'</pre></div></section></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.6" data-id-title="Commit Changes and Run the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.3 </span><span class="title-name">Commit Changes and Run the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.5.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Commit your changes with the input model and the required configuration
        values added to the <code class="filename">pass_through.yml</code> and
        <code class="filename">nsx/nsx_config.yml</code> files.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git commit -A -m "Configuration changes for NSX deployment"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
 -e \encrypt="" -e rekey=""</pre></div><p>
        If the playbook <code class="filename">config-processor-run.yml</code> fails,
        there is an error in the input model. Fix the error and repeat the
        above steps.
       </p></section></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.6" data-id-title="Deploying the Operating System with Cobbler"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.3 </span><span class="title-name">Deploying the Operating System with Cobbler</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         From the Cloud Lifecycle Manager, run Cobbler to install the operating system on the
         nodes after it has to be deployed:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
         Verify the nodes that will have an operating system installed by
         Cobbler by running this command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system find --netboot-enabled=1</pre></div></li><li class="step"><p>
         Reimage the nodes using Cobbler. Do not use Cobbler to reimage the
         nodes running as ESX virtual machines. The command below is run on a
         setup where the Nova ESX compute proxies are VMs. Controllers 1,
         2, and 3 are running on physical servers.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e \
   nodelist=controller1,controller2,controller3</pre></div></li><li class="step"><p>
         When the playbook has completed, each controller node should have an
         operating system installed with an IP address configured on
         <code class="literal">eth0</code>.
        </p></li><li class="step"><p>
         After your controller nodes have been completed, you should install
         the operating system on your Nova compute proxy virtual machines.
         Each configured virtual machine should be able to PXE boot into the
         operating system installer.
        </p></li><li class="step"><p>
         From within the vSphere environment, power on each Nova compute
         proxy virtual machine and watch for it to PXE boot into the OS
         installer via its console.
        </p><ol type="a" class="substeps"><li class="step"><p>
           If successful, the virtual machine will have the operating system
           automatically installed and will then automatically power off.
          </p></li><li class="step"><p>
           When the virtual machine has powered off, power it on and let it
           boot into the operating system.
          </p></li></ol></li><li class="step"><p>
         Verify network settings after deploying the operating system to each
         node.
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Verify that the NIC bus mapping specified in the cloud model input
           file
           (<code class="filename">~/ardana/my_cloud/definition/data/nic_mappings.yml</code>)
           matches the NIC bus mapping on each <span class="productname">OpenStack</span> node.
          </p><p>
           Check the NIC bus mapping with this command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem"><p>
           After the playbook has completed, each controller node should have
           an operating system installed with an IP address configured on eth0.
          </p></li></ul></div></li><li class="step"><p>
         When the ESX compute proxy nodes are VMs, install the operating system
         if you have not already done so.
        </p></li></ol></div></div></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.7" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.4 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.11.10.4.9.18.8.7">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
       When the configuration processor has completed successfully, the cloud
       can be deployed. Set the ARDANA_USER_PASSWORD_ENCRYPT_KEY environment
       variable before running <code class="filename">site.yml</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">PASSWORD_KEY</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><p>
       <em class="replaceable">PASSWORD_KEY</em> in the <code class="literal">export</code>
       command is the key used to encrypt the passwords for vCenter and NSX
       Manager.
      </p></section></section></section></section></section></section><section class="sect1" id="nsx-verification" data-id-title="Verifying the NSX-v Functionality After Integration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.3 </span><span class="title-name">Verifying the NSX-v Functionality After Integration</span></span> <a title="Permalink" class="permalink" href="#nsx-verification">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  After you have completed your <span class="productname">OpenStack</span> deployment and integrated the NSX-v
  Neutron plugin, you can use these steps to verify that NSX-v is enabled and
  working in the environment.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Validating Neutron from the Cloud Lifecycle Manager. All of these commands require that you
    authenticate by <code class="filename">service.osrc</code> file.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="step"><p>
    List your Neutron networks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron network list
+--------------------------------------+----------------+-------------------------------------------------------+
| id                                   | name           | subnets                                               |
+--------------------------------------+----------------+-------------------------------------------------------+
| 574d5f6c-871e-47f8-86d2-4b7c33d91002 | inter-edge-net | c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512 169.254.128.0/17 |
+--------------------------------------+----------------+-------------------------------------------------------+</pre></div></li><li class="step"><p>
    List your Neutron subnets:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron subnet list
+--------------------------------------+-------------------+------------------+------------------------------------------------------+
| id                                   | name              | cidr             | allocation_pools                                     |
+--------------------------------------+-------------------+------------------+------------------------------------------------------+
| c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512 | inter-edge-subnet | 169.254.128.0/17 | {"start": "169.254.128.2", "end": "169.254.255.254"} |
+--------------------------------------+-------------------+------------------+------------------------------------------------------+</pre></div></li><li class="step"><p>
    List your Neutron routers:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron router list
+--------------------------------------+-----------------------+-----------------------+-------------+
| id                                   | name                  | external_gateway_info | distributed |
+--------------------------------------+-----------------------+-----------------------+-------------+
| 1c5bf781-5120-4b7e-938b-856e23e9f156 | metadata_proxy_router | null                  | False       |
| 8b5d03bf-6f77-4ea9-bb27-87dd2097eb5c | metadata_proxy_router | null                  | False       |
+--------------------------------------+-----------------------+-----------------------+-------------+</pre></div></li><li class="step"><p>
    List your Neutron ports:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port list
+--------------------------------------+------+-------------------+------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                            |
+--------------------------------------+------+-------------------+------------------------------------------------------+
| 7f5f0461-0db4-4b9a-a0c6-faa0010b9be2 |      | fa:16:3e:e5:50:d4 | {"subnet_id":                                        |
|                                      |      |                   | "c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512",              |
|                                      |      |                   | "ip_address": "169.254.128.2"}                       |
| 89f27dff-f38d-4084-b9b0-ded495255dcb |      | fa:16:3e:96:a0:28 | {"subnet_id":                                        |
|                                      |      |                   | "c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512",              |
|                                      |      |                   | "ip_address": "169.254.128.3"}                       |
+--------------------------------------+------+-------------------+------------------------------------------------------+</pre></div></li><li class="step"><p>
    List your Neutron security group rules:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron security group rule list
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+
| id                                   | security_group | direction | ethertype | protocol/port | remote          |
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+
| 0385bd3a-1050-4bc2-a212-22ddab00c488 | default        | egress    | IPv6      | any           | any             |
| 19f6f841-1a9a-4b4b-bc45-7e8501953d8f | default        | ingress   | IPv6      | any           | default (group) |
| 1b3b5925-7aa6-4b74-9df0-f417ee6218f1 | default        | egress    | IPv4      | any           | any             |
| 256953cc-23d7-404d-b140-2600d55e44a2 | default        | ingress   | IPv4      | any           | default (group) |
| 314c4e25-5822-44b4-9d82-4658ae87d93f | default        | egress    | IPv6      | any           | any             |
| 59d4a71e-9f99-4b3b-b75b-7c9ad34081e0 | default        | ingress   | IPv6      | any           | default (group) |
| 887e25ef-64b7-4b69-b301-e053f88efa6c | default        | ingress   | IPv4      | any           | default (group) |
| 949e9744-75cd-4ae2-8cc6-6c0f578162d7 | default        | ingress   | IPv4      | any           | default (group) |
| 9a83027e-d6d6-4b6b-94fa-7c0ced2eba37 | default        | egress    | IPv4      | any           | any             |
| abf63b79-35ad-428a-8829-8e8d796a9917 | default        | egress    | IPv4      | any           | any             |
| be34b72b-66b6-4019-b782-7d91674ca01d | default        | ingress   | IPv6      | any           | default (group) |
| bf3d87ce-05c8-400d-88d9-a940e43760ca | default        | egress    | IPv6      | any           | any             |
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+</pre></div></li></ol></div></div><p>
  Verify metadata proxy functionality
 </p><p>
  To test that the metadata proxy virtual machines are working as intended,
  verify that there are at least two metadata proxy virtual machines from
  within vSphere (there will be four if edge high availability was set to
  true).
 </p><p>
  When that is verified, create a new compute instance either with the API,
  CLI, or within the cloud console GUI and log into the instance. From within
  the instance, using curl, grab the metadata instance-id from the
  metadata proxy address.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl http://169.254.169.254/latest/meta-data/instance-id
i-00000004</pre></div></section></section><section class="chapter" id="install-ironic-overview" data-id-title="Installing Baremetal (Ironic)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">17 </span><span class="title-name">Installing Baremetal (Ironic)</span></span> <a title="Permalink" class="permalink" href="#install-ironic-overview">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_ironic_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Bare Metal as a Service is enabled in this release for deployment of Nova
  instances on bare metal nodes using flat networking.
  
 </p><section class="sect1" id="install-ironic" data-id-title="Installation for SUSE OpenStack Cloud Entry-scale Cloud with Ironic Flat Network"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.1 </span><span class="title-name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span></span> <a title="Permalink" class="permalink" href="#install-ironic">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with Ironic Flat Network.
 </p><section class="sect2" id="id-1.4.5.12.3.3" data-id-title="Configure Your Environment"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.1.1 </span><span class="title-name">Configure Your Environment</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Prior to deploying an operational environment with Ironic, operators need to
   be aware of the nature of TLS certificate authentication. As pre-built
   deployment agent ramdisks images are supplied, these ramdisk images will
   only authenticate known third-party TLS Certificate Authorities in the
   interest of end-to-end security. As such, uses of self-signed certificates
   and private certificate authorities will be unable to leverage ironic
   without modifying the supplied ramdisk images.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set up your configuration files, as follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       See the sample sets of configuration files in the
       <code class="literal">~/openstack/examples/</code> directory. Each set will have an
       accompanying README.md file that explains the contents of each of the
       configuration files.
      </p></li><li class="step"><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/entry-scale-ironic-flat-network/* \
  ~/openstack/my_cloud/definition/</pre></div></li></ol></li><li class="step"><p><span class="step-optional">(Optional)</span> 
     You can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="step"><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step"><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./ardanaencrypt.py</pre></div></li><li class="step"><p>
       Take the string generated and place it in the
       <code class="literal">ilo-password</code> field in your
       <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
       file, remembering to enclose it in quotes.
      </p></li><li class="step"><p>
       Repeat the above for each server.
      </p><div id="id-1.4.5.12.3.3.3.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Before you run any playbooks, remember that you need to export the
        encryption key in the following environment variable: <code class="literal">export
        ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
       </p></div></li></ol></li><li class="step"><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div><div id="id-1.4.5.12.3.3.3.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      This step needs to be repeated any time you make changes to your
      configuration files before you move on to the following steps. See
      <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a> for more information.
     </p></div></li></ol></div></div></section><section class="sect2" id="sec-ironic-provision" data-id-title="Provisioning Your Baremetal Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.1.2 </span><span class="title-name">Provisioning Your Baremetal Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><section class="sect3" id="id-1.4.5.12.3.4.3" data-id-title="Using Third Party Baremetal Installers"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">17.1.2.1 </span><span class="title-name">Using Third Party Baremetal Installers</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.4.3">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem"><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem"><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem"><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem"><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem"><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle">Running the Configuration Processor</em>.
   </p></section><section class="sect3" id="id-1.4.5.12.3.4.4" data-id-title="Using the Automated Operating System Installation Provided by SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">17.1.2.2 </span><span class="title-name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.4.4">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><section class="sect4" id="id-1.4.5.12.3.4.4.3" data-id-title="Deploying Cobbler"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">17.1.2.2.1 </span><span class="title-name">Deploying Cobbler</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.4.4.3">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you cannot reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div id="id-1.4.5.12.3.4.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step"><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></section><section class="sect4" id="id-1.4.5.12.3.4.4.4" data-id-title="Imaging the Nodes"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">17.1.2.2.2 </span><span class="title-name">Imaging the Nodes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.4.4.4">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Powers down the nodes to be installed
      </p></li><li class="step"><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step"><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step"><p>
       Waits for the nodes to power themselves down (this indicates a
       successful install). This can take some time.
      </p></li><li class="step"><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step"><p>
       Waits for the nodes to be reachable by SSH and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     Deploying nodes has been automated in the Cloud Lifecycle Manager and requires the
     following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       All of your nodes using SLES must already be installed, either
       manually or via Cobbler.
      </p></li><li class="listitem"><p>
       Your input model should be configured for your SLES nodes, according
       to the instructions at <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
      </p></li><li class="listitem"><p>
       You should have run the configuration processor and the
       <code class="filename">ready-deployment.yml</code> playbook.
      </p></li></ul></div><p>
     Execute the following steps to re-image one or more nodes after you have
     run the <code class="filename">ready-deployment.yml</code> playbook.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Run the following playbook, specifying your SLES nodes using the
       nodelist. This playbook will reconfigure Cobbler for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e \
      nodelist=node1[,node2,node3]</pre></div></li><li class="step"><p>
       Re-image the node(s) with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
      -e nodelist=node1[,node2,node3]</pre></div></li></ol></div></div><p>
     If a nodelist is not specified then the set of nodes in Cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></section></section></section><section class="sect2" id="sec-ironic-config-processor" data-id-title="Running the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.1.3 </span><span class="title-name">Running the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-config-processor">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="23.2. Issues while Updating Configuration Files">Section 23.2, “Issues while Updating Configuration Files”</a>.
  </p></section><section class="sect2" id="sec-ironic-deploy" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.1.4 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-deploy">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step"><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.4.5.12.3.6.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step"><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="23.3. Issues while Deploying the Cloud">Section 23.3, “Issues while Deploying the Cloud”</a>.
  </p></section><section class="sect2" id="id-1.4.5.12.3.7" data-id-title="Ironic configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.1.5 </span><span class="title-name">Ironic configuration</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Run the <code class="literal">ironic-cloud-configure.yml</code> playbook below:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-cloud-configure.yml</pre></div><p>
   This step configures ironic flat network, uploads glance images and sets the
   ironic configuration.
  </p><p>
   To see the images uploaded to glance, run:
  </p><div class="verbatim-wrap"><pre class="screen">$ source ~/service.osrc
$ glance image list</pre></div><p>
   This will produce output like the following example, showing three images
   that have been added by Ironic:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| d4e2a0ff-9575-4bed-ac5e-5130a1553d93 | ir-deploy-iso-HOS3.0     |
| b759a1f0-3b33-4173-a6cb-be5706032124 | ir-deploy-kernel-HOS3.0  |
| ce5f4037-e368-46f2-941f-c01e9072676c | ir-deploy-ramdisk-HOS3.0 |
+--------------------------------------+--------------------------+</pre></div><p>
   To see the network created by Ironic, run:
  </p><div class="verbatim-wrap"><pre class="screen">$ neutron net-list</pre></div><p>
   This returns details of the "flat-net" generated by the Ironic install:
  </p><div class="verbatim-wrap"><pre class="screen"> +---------------+----------+-------------------------------------------------------+
 | id            | name     | subnets                                               |
 +---------------+----------+-------------------------------------------------------+
 | f9474...11010 | flat-net | ca8f8df8-12c8-4e58-b1eb-76844c4de7e8 192.168.245.0/24 |
 +---------------+----------+-------------------------------------------------------+</pre></div></section><section class="sect2" id="ironic-node-config" data-id-title="Node Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.1.6 </span><span class="title-name">Node Configuration</span></span> <a title="Permalink" class="permalink" href="#ironic-node-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_config.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect3" id="id-1.4.5.12.3.8.2" data-id-title="DHCP"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">17.1.6.1 </span><span class="title-name">DHCP</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.8.2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_config.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once booted, nodes obtain network configuration via DHCP. If multiple
   interfaces are to be utilized, you may want to pre-build images with
   settings to execute DHCP on all interfaces. An easy way to build custom
   images is with KIWI, the command line utility to build Linux system
   appliances.
  </p><p>
   For information about building custom KIWI images, see
   <a class="xref" href="#sec-ironic-provision-kiwi" title="17.3.11. Building Glance Images Using KIWI">Section 17.3.11, “Building Glance Images Using KIWI”</a>.
   For more information, see the KIWI documentation at
   <a class="link" href="https://osinside.github.io/kiwi/" target="_blank">https://osinside.github.io/kiwi/</a>.
  </p></section><section class="sect3" id="id-1.4.5.12.3.8.3" data-id-title="Configuration Drives"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">17.1.6.2 </span><span class="title-name">Configuration Drives</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.3.8.3">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_config.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.12.3.8.3.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    Configuration Drives are stored unencrypted and should not include any
    sensitive data.
   </p></div><p>
   You can use Configuration Drives to store metadata for initial boot
   setting customization. Configuration Drives are extremely useful for
   initial machine configuration. However, as a general security practice,
   they should not include any
   sensitive data. Configuration Drives should only be trusted upon the initial
   boot of an instance. <code class="literal">cloud-init</code> utilizes a lock file for
   this purpose. Custom instance images should not rely upon the integrity of a
   Configuration Drive beyond the initial boot of a host as an administrative
   user within a deployed instance can potentially modify a configuration drive
   once written to disk and released for use.
  </p><p>
   For more information about Configuration Drives, see
   <a class="link" href="http://docs.openstack.org/user-guide/cli_config_drive.html" target="_blank">http://docs.openstack.org/user-guide/cli_config_drive.html</a>.
  </p></section></section><section class="sect2" id="ironic-tls" data-id-title="TLS Certificates with Ironic Python Agent (IPA) Images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.1.7 </span><span class="title-name">TLS Certificates with Ironic Python Agent (IPA) Images</span></span> <a title="Permalink" class="permalink" href="#ironic-tls">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  As part of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, Ironic Python Agent, better known as IPA in the
  OpenStack community, images are supplied and loaded into Glance. Two types of
  image exist. One is a traditional boot ramdisk which is used by the
  <code class="literal">agent_ipmitool</code>, <code class="literal">pxe_ipmitool</code>, and
  <code class="literal">pxe_ilo</code> drivers. The other is an ISO image that is
  supplied as virtual media to the host when using the
  <code class="literal">agent_ilo</code> driver.
 </p><p>
  As these images are built in advance, they are unaware of any private
  certificate authorities. Users attempting to utilize self-signed certificates
  or a private certificate authority will need to inject their signing
  certificate(s) into the image in order for IPA to be able to boot on a remote
  node, and ensure that the TLS endpoints being connected to in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can be
  trusted. This is not an issue with publicly signed certificates.
 </p><p>
  As two different types of images exist, below are instructions for
  disassembling the image ramdisk file or the ISO image. Once this has been
  done, you will need to re-upload the files to glance, and update any impacted
  node's <code class="literal">driver_info</code>, for example, the
  <code class="literal">deploy_ramdisk</code> and <code class="literal">ilo_deploy_iso</code>
  settings that were set when the node was first defined. Respectively, this
  can be done with the
 </p><div class="verbatim-wrap"><pre class="screen">ironic node-update &lt;node&gt; replace driver_info/deploy_ramdisk=&lt;glance_id&gt;</pre></div><p>
  or
 </p><div class="verbatim-wrap"><pre class="screen">ironic node-update &lt;node&gt; replace driver_info/ilo_deploy_iso=&lt;glance_id&gt;</pre></div><section class="sect3" id="cert-ramdisk" data-id-title="Add New Trusted CA Certificate Into Deploy Images"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">17.1.7.1 </span><span class="title-name">Add New Trusted CA Certificate Into Deploy Images</span></span> <a title="Permalink" class="permalink" href="#cert-ramdisk">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-cert_ramdisk.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Perform the following steps.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    To upload your trusted CA certificate to the Cloud Lifecycle Manager, follow the directions
    in <a class="xref" href="#sec-upload-toclm" title="29.7. Upload to the Cloud Lifecycle Manager">Section 29.7, “Upload to the Cloud Lifecycle Manager”</a>.
   </p></li><li class="step"><p>
    Delete the deploy images.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image delete ir-deploy-iso-ARDANA5.0
<code class="prompt user">ardana &gt; </code>openstack image delete ir-deploy-ramdisk-ARDANA5.0</pre></div></li><li class="step"><p>
    On the deployer, run <code class="filename">ironic-reconfigure.yml</code> playbook
    to re-upload the images that include the new trusted CA bundle.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li><li class="step"><p>
    Update the existing Ironic nodes with the new image IDs accordingly. For
    example,
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack baremetal node set --driver-info \
deploy_ramdisk=<em class="replaceable">NEW_RAMDISK_ID</em> <em class="replaceable">NODE_ID</em></pre></div></li></ol></div></div></section></section></section><section class="sect1" id="ironic-multi-control-plane" data-id-title="Ironic in Multiple Control Plane"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.2 </span><span class="title-name">Ironic in Multiple Control Plane</span></span> <a title="Permalink" class="permalink" href="#ironic-multi-control-plane">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> introduces the concept of multiple control planes - see the
  Input Model documentation for the relevant <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”, Section 5.2 “Concepts”, Section 5.2.2 “Control Planes”, Section 5.2.2.1 “Control Planes and Regions”</span> and <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”, Section 6.2.3 “Multiple Control Planes”</span>. This document covers the use
  of an Ironic region in a multiple control plane cloud model in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><section class="sect2" id="id-1.4.5.12.4.3" data-id-title="Networking for Baremetal in Multiple Control Plane"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.1 </span><span class="title-name">Networking for Baremetal in Multiple Control Plane</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.4.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>IRONIC-FLAT-NET</strong></span> is the network
   configuration for baremetal control plane.
  </p><p>
   You need to set the environment variable <span class="bold"><strong>OS_REGION_NAME</strong></span> to the Ironic region in baremetal
   control plane. This will set up the Ironic flat networking in
   Neutron.
  </p><div class="verbatim-wrap"><pre class="screen">export OS_REGION_NAME=&lt;ironic_region&gt;</pre></div><p>
   To see details of the <code class="literal">IRONIC-FLAT-NETWORK</code> created during
   configuration, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen">neutron net-list</pre></div><p>
   Referring to the diagram below, the Cloud Lifecycle Manager is a shared service that runs in a
   Core API Controller in a Core API Cluster. Ironic Python Agent (IPA) must
   be able to make REST API calls to the Ironic API (the connection is
   represented by the green line to Internal routing). The IPA connect to
   Swift to get user images (the gray line connecting to Swift routing).
  </p><div class="figure" id="id-1.4.5.12.4.3.8"><div class="figure-contents"><div class="mediaobject"><a href="images/media-ironic-ironic_multi_control_plane.png"><img src="images/media-ironic-ironic_multi_control_plane.png" alt="Architecture of Multiple Control Plane with Ironic" title="Architecture of Multiple Control Plane with Ironic"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 17.1: </span><span class="title-name">Architecture of Multiple Control Plane with Ironic </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.12.4.3.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="id-1.4.5.12.4.4" data-id-title="Handling Optional Swift Service"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.2 </span><span class="title-name">Handling Optional Swift Service</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.4.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Swift is very resource-intensive and as a result, it is now optional in the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> control plane. A number of services depend on Swift, and if it is
   not present, they must provide a fallback strategy. For example, Glance can
   use the filesystem in place of Swift for its backend store.
  </p><p>
   In Ironic, agent-based drivers require Swift. If it is not present, it is
   necessary to disable access to this Ironic feature in the control plane. The
   <code class="literal">enable_agent_driver</code> flag has been added to the Ironic
   configuration data and can have values of <code class="literal">true</code> or
   <code class="literal">false</code>. Setting this flag to <code class="literal">false</code> will
   disable Swift configurations and the agent based drivers in the Ironic
   control plane.
  </p></section><section class="sect2" id="id-1.4.5.12.4.5" data-id-title="Instance Provisioning"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.3 </span><span class="title-name">Instance Provisioning</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.4.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In a multiple control plane cloud setup, changes for Glance container name
   in the Swift namespace of <code class="literal">ironic-conductor.conf</code>
   introduces a conflict with the one in <code class="literal">glance-api.conf</code>.
   Provisioning with agent-based drivers requires the container name to be the
   same in Ironic and Glance. Hence, on instance provisioning with agent-based
   drivers (Swift-enabled), the agent is not able to fetch the images from
   Glance store and fails at that point.
  </p><p>
   You can resolve this issue using the following steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Copy the value of <code class="literal">swift_store_container</code> from the file
     <code class="literal">/opt/stack/service/glance-api/etc/glance-api.conf</code>
    </p></li><li class="step"><p>
     Log in to the Cloud Lifecycle Manager and use the value for
     <code class="literal">swift_container</code> in glance namespace of
     <code class="literal">~/scratch/ansible/next/ardana/ansible/roles/ironic-common/templates/ironic-conductor.conf.j2</code>
    </p></li><li class="step"><p>
     Run the following playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></section></section><section class="sect1" id="ironic-provisioning" data-id-title="Provisioning Bare-Metal Nodes with Flat Network Model"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.3 </span><span class="title-name">Provisioning Bare-Metal Nodes with Flat Network Model</span></span> <a title="Permalink" class="permalink" href="#ironic-provisioning">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.12.5.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
   Providing bare-metal resources to an untrusted third party is not advised
   as a malicious user can potentially modify hardware firmware.
  </p></div><div id="id-1.4.5.12.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   The steps outlined in <a class="xref" href="#ironic-tls" title="17.1.7. TLS Certificates with Ironic Python Agent (IPA) Images">Section 17.1.7, “TLS Certificates with Ironic Python Agent (IPA) Images”</a>
   <span class="emphasis"><em>must</em></span> be performed.
  </p></div><p>
  A number of drivers are available to provision and manage bare-metal
  machines. The drivers are named based on the deployment mode and the power
  management interface. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has been tested with the following drivers:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    agent_ilo
   </p></li><li class="listitem"><p>
    agent_ipmi
   </p></li><li class="listitem"><p>
    pxe_ilo
   </p></li><li class="listitem"><p>
    pxe_ipmi
   </p></li></ul></div><p>
  Before you start, you should be aware that:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    Node Cleaning is enabled for all the drivers in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
   </p></li><li class="listitem"><p>
    Node parameter settings must have matching flavors in terms of
    <code class="literal">cpus</code>, <code class="literal">local_gb</code>, and
    <code class="literal">memory_mb</code>, <code class="literal">boot_mode</code> and
    <code class="literal">cpu_arch</code>.
   </p></li><li class="listitem"><p>
    It is advisable that nodes enrolled for ipmitool drivers are pre-validated
    in terms of BIOS settings, in terms of boot mode, prior to setting
    capabilities.
   </p></li><li class="listitem"><p>
    Network cabling and interface layout should also be pre-validated in any
    given particular boot mode or configuration that is registered.
   </p></li><li class="listitem"><p>
    The use of <code class="literal">agent_</code> drivers is predicated upon Glance
    images being backed by a Swift image store, specifically the need for the
    temporary file access features. Using the file system as a Glance back-end
    image store means that the <code class="literal">agent_</code> drivers cannot be
    used.
   </p></li><li class="listitem"><p>
    Manual Cleaning (RAID) and Node inspection is supported by ilo drivers
    (<code class="literal">agent_ilo</code> and <code class="literal">pxe_ilo)</code>
   </p></li></ol></div><section class="sect2" id="sec-ironic-provision-image" data-id-title="Supplied Images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.1 </span><span class="title-name">Supplied Images</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-image">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Ironic Cloud installation,
   Ironic Python Agent (IPA) images are supplied and loaded into Glance.
   To see the images that have been loaded, execute the following commands on
   the deployer node:
  </p><div class="verbatim-wrap"><pre class="screen">$ source ~/service.osrc
glance image list</pre></div><p>
   This will display three images that have been added by Ironic:
  </p><div class="verbatim-wrap"><pre class="screen">Deploy_iso : openstack-ironic-image.x86_64-8.0.0.kernel.4.4.120-94.17-default
Deploy_kernel : openstack-ironic-image.x86_64-8.0.0.xz
Deploy_ramdisk : openstack-ironic-image.x86_64-8.0.0.iso</pre></div><p>
   The <code class="literal">ir-deploy-ramdisk</code> image is a traditional boot ramdisk
   used by the <code class="literal">agent_ipmitool</code>,
   <code class="literal">pxe_ipmitool</code>, and <code class="literal">pxe_ilo</code> drivers
   while <code class="literal">ir-deploy-iso</code> is an ISO image that is supplied as
   virtual media to the host when using the <code class="literal">agent_ilo</code>
   driver.
  </p></section><section class="sect2" id="sec-ironic-provision-provision" data-id-title="Provisioning a Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.2 </span><span class="title-name">Provisioning a Node</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-provision">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The information required to provision a node varies slightly depending on
   the driver used. In general the following details are required.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Network access information and credentials to connect to the management
     interface of the node.
    </p></li><li class="listitem"><p>
     Sufficient properties to allow for Nova flavor matching.
    </p></li><li class="listitem"><p>
     A deployment image to perform the actual deployment of the guest operating
     system to the bare-metal node.
    </p></li></ul></div><p>
   A combination of the <code class="literal">ironic node-create</code> and
   <code class="literal">ironic node-update</code> commands are used for registering a
   node's characteristics with the Ironic service. In particular,
   <code class="literal">ironic node-update &lt;nodeid&gt;
   <em class="replaceable">add</em></code> and <code class="literal">ironic node-update
   &lt;nodeid&gt; <em class="replaceable">replace</em></code> can be used to
   modify the properties of a node after it has been created while
   <code class="literal">ironic node-update &lt;nodeid&gt;
   <em class="replaceable">remove</em></code> will remove a property.
  </p></section><section class="sect2" id="sec-ironic-provision-create-ilo" data-id-title="Creating a Node Using agent_ilo"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.3 </span><span class="title-name">Creating a Node Using <code class="command">agent_ilo</code></span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-create-ilo">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you want to use a boot mode of BIOS as opposed to UEFI, then you need to
   ensure that the boot mode has been set correctly on the IPMI:
  </p><p>
   While the iLO driver can automatically set a node to boot in UEFI mode via
   the <code class="literal">boot_mode</code> defined capability, it cannot set BIOS boot
   mode once UEFI mode has been set.
  </p><p>
   Use the <code class="literal">ironic node-create</code> command to specify the
   <code class="literal">agent_ilo</code> driver, network access and credential
   information for the IPMI, properties of the node and the Glance ID of the
   supplied ISO IPA image. Note that memory size is specified in megabytes while
   disk size is specified in gigabytes.
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d agent_ilo -i ilo_address=<em class="replaceable">IP_ADDRESS</em> -i \
  ilo_username=Administrator -i ilo_password=<em class="replaceable">PASSWORD</em> \
  -p cpus=2 -p cpu_arch=x86_64 -p memory_mb=64000 -p local_gb=99 \
  -i ilo_deploy_iso=<em class="replaceable">DEPLOY_UUID</em></pre></div><p>
   This will generate output similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------+---------------------------------------------------------------+
| Property     | Value                                                         |
+--------------+---------------------------------------------------------------+
| uuid         | <em class="replaceable">NODE_UUID</em>                                                     |
| driver_info  | {u'ilo_address': u'<em class="replaceable">IP_ADDRESS</em>', u'ilo_password': u'******',   |
|              | u'ilo_deploy_iso': u'<em class="replaceable">DEPLOY_UUID</em>',                            |
|              | u'ilo_username': u'Administrator'}                            |
| extra        | {}                                                            |
| driver       | agent_ilo                                                     |
| chassis_uuid |                                                               |
| properties   | {u'memory_mb': 64000, u'local_gb': 99, u'cpus': 2,            |
|              | u'cpu_arch': u'x86_64'}                                       |
| name         | None                                                          |
+--------------+---------------------------------------------------------------+</pre></div><p>
   Now update the node with <code class="literal">boot_mode</code> and
   <code class="literal">boot_option</code> properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update <em class="replaceable">NODE_UUID</em> add \
  properties/capabilities="boot_mode:bios,boot_option:local"</pre></div><p>
   The <code class="literal">ironic node-update</code> command returns details for all of
   the node's characteristics.
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+------------------------------------------------------------------+
| Property               | Value                                                            |
+------------------------+------------------------------------------------------------------+
| target_power_state     | None                                                             |
| extra                  | {}                                                               |
| last_error             | None                                                             |
| updated_at             | None                                                             |
| maintenance_reason     | None                                                             |
| provision_state        | available                                                        |
| clean_step             | {}                                                               |
| uuid                   | <em class="replaceable">NODE_UUID</em>                                                        |
| console_enabled        | False                                                            |
| target_provision_state | None                                                             |
| provision_updated_at   | None                                                             |
| maintenance            | False                                                            |
| inspection_started_at  | None                                                             |
| inspection_finished_at | None                                                             |
| power_state            | None                                                             |
| driver                 | agent_ilo                                                        |
| reservation            | None                                                             |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,   |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local'}|
| instance_uuid          | None                                                             |
| name                   | None                                                             |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',    |
|                        | u'ilo_deploy_iso': u'<em class="replaceable">DEPLOY_UUID</em>',                               |
|                        | u'ilo_username': u'Administrator'}                               |
| created_at             | 2016-03-11T10:17:10+00:00                                        |
| driver_internal_info   | {}                                                               |
| chassis_uuid           |                                                                  |
| instance_info          | {}                                                               |
+------------------------+------------------------------------------------------------------+</pre></div></section><section class="sect2" id="sec-ironic-provision-create-ipmi" data-id-title="Creating a Node Using agent_ipmi"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.4 </span><span class="title-name">Creating a Node Using <code class="command">agent_ipmi</code></span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-create-ipmi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Use the <code class="literal">ironic node-create</code> command to specify the
   <code class="literal">agent_ipmi</code> driver, network access and credential
   information for the IPMI, properties of the node and the Glance IDs of the
   supplied kernel and ramdisk images. Note that memory size is specified in
   megabytes while disk size is specified in gigabytes.
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d <span class="bold"><strong>agent_ipmitool</strong></span> \
  -i ipmi_address=<em class="replaceable">IP_ADDRESS</em> \
  -i ipmi_username=Administrator -i ipmi_password=<em class="replaceable">PASSWORD</em> \
  -p cpus=2 -p memory_mb=64000 -p local_gb=99 -p cpu_arch=x86_64 \
  -i deploy_kernel=<em class="replaceable">KERNEL_UUID</em> \
  -i deploy_ramdisk=<em class="replaceable">RAMDISK_UUID</em></pre></div><p>
   This will generate output similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------+-----------------------------------------------------------------------+
| Property     | Value                                                                 |
+--------------+-----------------------------------------------------------------------+
| uuid         | <em class="replaceable">NODE2_UUID</em>                                                            |
| driver_info  | {u'deploy_kernel': u'<em class="replaceable">KERNEL_UUID</em>',                                    |
|              | u'ipmi_address': u'<em class="replaceable">IP_ADDRESS</em>', u'ipmi_username': u'Administrator',   |
|              | u'ipmi_password': u'******',                                          |
|              | u'deploy_ramdisk': u'<em class="replaceable">RAMDISK_UUID</em>'}                                   |
| extra        | {}                                                                    |
| driver       | agent_ipmitool                                                        |
| chassis_uuid |                                                                       |
| properties   | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,        |
|              | u'cpus': 2}                                                           |
| name         | None                                                                  |
+--------------+-----------------------------------------------------------------------+</pre></div><p>
   Now update the node with <code class="literal">boot_mode</code> and
   <code class="literal">boot_option</code> properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update <em class="replaceable">NODE_UUID</em> add \
  properties/capabilities="boot_mode:bios,boot_option:local"</pre></div><p>
   The <code class="literal">ironic node-update</code> command returns details for all of
   the node's characteristics.
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+-----------------------------------------------------------------+
| Property               | Value                                                           |
+------------------------+-----------------------------------------------------------------+
| target_power_state     | None                                                            |
| extra                  | {}                                                              |
| last_error             | None                                                            |
| updated_at             | None                                                            |
| maintenance_reason     | None                                                            |
| provision_state        | available                                                       |
| clean_step             | {}                                                              |
| uuid                   | <em class="replaceable">NODE2_UUID</em>                                                      |
| console_enabled        | False                                                           |
| target_provision_state | None                                                            |
| provision_updated_at   | None                                                            |
| maintenance            | False                                                           |
| inspection_started_at  | None                                                            |
| inspection_finished_at | None                                                            |
| power_state            | None                                                            |
| driver                 | agent_ipmitool                                                  |
| reservation            | None                                                            |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64',                   |
|                        | u'local_gb': 99, u'cpus': 2,                                    |
|                        | u'capabilities': u'boot_mode:bios,boot_option:local'}           |
| instance_uuid          | None                                                            |
| name                   | None                                                            |
| driver_info            | {u'ipmi_password': u'******', u'ipmi_address': u'<em class="replaceable">IP_ADDRESS</em>',   |
|                        | u'ipmi_username': u'Administrator', u'deploy_kernel':           |
|                        | u'<em class="replaceable">KERNEL_UUID</em>',                                                 |
|                        | u'deploy_ramdisk': u'<em class="replaceable">RAMDISK_UUID</em>'}                             |
| created_at             | 2016-03-11T14:19:18+00:00                                       |
| driver_internal_info   | {}                                                              |
| chassis_uuid           |                                                                 |
| instance_info          | {}                                                              |
+------------------------+-----------------------------------------------------------------+</pre></div><p>
   For more information on node enrollment, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/install-guide.html#enrollment" target="_blank">http://docs.openstack.org/developer/ironic/deploy/install-guide.html#enrollment</a>.
  </p></section><section class="sect2" id="sec-ironic-provision-flavor" data-id-title="Creating a Flavor"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.5 </span><span class="title-name">Creating a Flavor</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-flavor">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Nova uses flavors when fulfilling requests for bare-metal nodes. The Nova
   scheduler attempts to match the requested flavor against the properties of
   the created Ironic nodes. So an administrator needs to set up flavors that
   correspond to the available bare-metal nodes using the command
   <code class="command">nova flavor-create</code>:
  </p><div class="verbatim-wrap"><pre class="screen">nova flavor-create bmtest auto 64000  99 2

+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+
| ID             | Name   | Mem_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+
| 645de0...b1348 | bmtest | 64000  | 99   | 0         |      | 2     | 1.0         | True      |
+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+</pre></div><p>
   To see a list of all the available flavors, run <code class="command">nova
   flavor-list</code>:
  </p><div class="verbatim-wrap"><pre class="screen">nova flavor-list

+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+
| ID          | Name         | Mem_MB | Disk | Ephemeral | Swap | VCPUs |  RXTX  | Is_Public |
|             |              |        |      |           |      |       | Factor |           |
+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+
| 1           | m1.tiny      | 512    | 1    | 0         |      | 1     | 1.0    | True      |
| 2           | m1.small     | 2048   | 20   | 0         |      | 1     | 1.0    | True      |
| 3           | m1.medium    | 4096   | 40   | 0         |      | 2     | 1.0    | True      |
| 4           | m1.large     | 8192   | 80   | 0         |      | 4     | 1.0    | True      |
| 5           | m1.xlarge    | 16384  | 160  | 0         |      | 8     | 1.0    | True      |
| 6           | m1.baremetal | 4096   | 80   | 0         |      | 2     | 1.0    | True      |
| 645d...1348 | bmtest       | 64000  | 99   | 0         |      | 2     | 1.0    | True      |
+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+</pre></div><p>
   Now set the CPU architecture and boot mode and boot option capabilities:
  </p><div class="verbatim-wrap"><pre class="screen">nova flavor-key 645de08d-2bc6-43f1-8a5f-2315a75b1348 set cpu_arch=x86_64
nova flavor-key 645de08d-2bc6-43f1-8a5f-2315a75b1348 set capabilities:boot_option="local"
nova flavor-key 645de08d-2bc6-43f1-8a5f-2315a75b1348 set capabilities:boot_mode="bios"</pre></div><p>
   For more information on flavor creation, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/install-guide.html#flavor-creation" target="_blank">http://docs.openstack.org/developer/ironic/deploy/install-guide.html#flavor-creation</a>.
  </p></section><section class="sect2" id="sec-ironic-provision-net" data-id-title="Creating a Network Port"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.6 </span><span class="title-name">Creating a Network Port</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-net">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Register the MAC addresses of all connected physical network interfaces
   intended for use with the bare-metal node.
  </p><div class="verbatim-wrap"><pre class="screen">ironic port-create -a 5c:b9:01:88:f0:a4 -n ea7246fd-e1d6-4637-9699-0b7c59c22e67</pre></div></section><section class="sect2" id="sec-ironic-provision-glance-image" data-id-title="Creating a Glance Image"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.7 </span><span class="title-name">Creating a Glance Image</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-glance-image">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can create a complete disk image using the instructions at
   <a class="xref" href="#sec-ironic-provision-kiwi" title="17.3.11. Building Glance Images Using KIWI">Section 17.3.11, “Building Glance Images Using KIWI”</a>.
  </p><p>
   The image you create can then be loaded into Glance:
  </p><div class="verbatim-wrap"><pre class="screen">glance image-create --name='leap' --disk-format=raw \
  --container-format=bare \
  --file /tmp/myimage/LimeJeOS-Leap-42.3.x86_64-1.42.3.raw

+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 45a4a06997e64f7120795c68beeb0e3c     |
| container_format | bare                                 |
| created_at       | 2018-02-17T10:42:14Z                 |
| disk_format      | raw                                  |
| id               | <span class="bold"><strong>17e4915a-ada0-4b95-bacf-ba67133f39a7</strong></span> |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | leap                                 |
| owner            | 821b7bb8148f439191d108764301af64     |
| protected        | False                                |
| size             | 372047872                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2018-02-17T10:42:23Z                 |
| virtual_size     | None                                 |
| visibility       | private                              |
+------------------+--------------------------------------+</pre></div><p>
   This image will subsequently be used to boot the bare-metal node.
  </p></section><section class="sect2" id="sec-ironic-provision-key" data-id-title="Generating a Key Pair"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.8 </span><span class="title-name">Generating a Key Pair</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-key">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Create a key pair that you will use when you login to the newly booted node:
  </p><div class="verbatim-wrap"><pre class="screen">nova keypair-add <span class="bold"><strong>ironic_kp</strong></span> &gt; ironic_kp.pem</pre></div></section><section class="sect2" id="sec-ironic-provision-neutron-id" data-id-title="Determining the Neutron Network ID"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.9 </span><span class="title-name">Determining the Neutron Network ID</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-neutron-id">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">neutron net-list

+---------------+----------+----------------------------------------------------+
| id            | name     | subnets                                            |
+---------------+----------+----------------------------------------------------+
| <span class="bold"><strong>c0102...1ca8c </strong></span>| flat-net | 709ee2a1-4110-4b26-ba4d-deb74553adb9 192.3.15.0/24 |
+---------------+----------+----------------------------------------------------+</pre></div></section><section class="sect2" id="sec-ironic-provision-boot" data-id-title="Booting the Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.10 </span><span class="title-name">Booting the Node</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-boot">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before booting, it is advisable to power down the node:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-power-state ea7246fd-e1d6-4637-9699-0b7c59c22e67 off</pre></div><p>
   You can now boot the bare-metal node with the information compiled in the
   preceding steps, using the Neutron network ID, the whole disk image ID, the
   matching flavor and the key name:
  </p><div class="verbatim-wrap"><pre class="screen">nova boot --nic net-id=c010267c-9424-45be-8c05-99d68531ca8c \
  --image 17e4915a-ada0-4b95-bacf-ba67133f39a7 \
  --flavor 645de08d-2bc6-43f1-8a5f-2315a75b1348 \
  --key-name ironic_kp leap</pre></div><p>
   This command returns information about the state of the node that is booting:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------+
| Property                             | Value                  |
+--------------------------------------+------------------------+
| OS-EXT-AZ:availability_zone          |                        |
| OS-EXT-SRV-ATTR:host                 | -                      |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                      |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000001      |
| OS-EXT-STS:power_state               | 0                      |
| OS-EXT-STS:task_state                | scheduling             |
| OS-EXT-STS:vm_state                  | building               |
| OS-SRV-USG:launched_at               | -                      |
| OS-SRV-USG:terminated_at             | -                      |
| accessIPv4                           |                        |
| accessIPv6                           |                        |
| adminPass                            | adpHw3KKTjHk           |
| config_drive                         |                        |
| created                              | 2018-03-11T11:00:28Z   |
| flavor                               | bmtest (645de...b1348) |
| hostId                               |                        |
| id                                   | a9012...3007e          |
| image                                | leap (17e49...f39a7)   |
| key_name                             | ironic_kp              |
| metadata                             | {}                     |
| name                                 | leap                   |
| os-extended-volumes:volumes_attached | []                     |
| progress                             | 0                      |
| security_groups                      | default                |
| status                               | BUILD                  |
| tenant_id                            | d53bcaf...baa60dd      |
| updated                              | 2016-03-11T11:00:28Z   |
| user_id                              | e580c64...4aaf990      |
+--------------------------------------+------------------------+</pre></div><p>
   The boot process can take up to 10 minutes. Monitor the progress with the
   IPMI console or with <code class="literal">nova list</code>, <code class="literal">nova show
   &lt;nova_node_id&gt;</code>, and <code class="literal">ironic node-show
   &lt;ironic_node_id&gt;</code> commands.
  </p><div class="verbatim-wrap"><pre class="screen">nova list

+---------------+--------+--------+------------+-------------+----------------------+
| ID            | Name   | Status | Task State | Power State | Networks             |
+---------------+--------+--------+------------+-------------+----------------------+
| a9012...3007e | leap   | BUILD  | spawning   | NOSTATE     | flat-net=192.3.15.12 |
+---------------+--------+--------+------------+-------------+----------------------+</pre></div><p>
   During the boot procedure, a login prompt will appear for SLES:
  </p><p>
   Ignore this login screen and wait for the login screen of your target
   operating system to appear:
  </p><p>
   If you now run the command <code class="command">nova list</code>, it should show the
   node in the running state:
  </p><div class="verbatim-wrap"><pre class="screen">nova list
+---------------+--------+--------+------------+-------------+----------------------+
| ID            | Name   | Status | Task State | Power State | Networks             |
+---------------+--------+--------+------------+-------------+----------------------+
| a9012...3007e | leap   | ACTIVE | -          | Running     | flat-net=<span class="bold"><strong>192.3.15.14</strong></span> |
+---------------+--------+--------+------------+-------------+----------------------+</pre></div><p>
   You can now log in to the booted node using the key you generated earlier.
   (You may be prompted to change the permissions of your private key files, so
   that they are not accessible by others).
  </p><div class="verbatim-wrap"><pre class="screen">ssh leap@192.3.15.14 -i ironic_kp.pem</pre></div></section><section class="sect2" id="sec-ironic-provision-kiwi" data-id-title="Building Glance Images Using KIWI"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.11 </span><span class="title-name">Building Glance Images Using KIWI</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-kiwi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following sections show you how to create your own images using KIWI,
   the command line utility to build Linux system appliances. For information
   on installing KIWI, see <a class="link" href="https://osinside.github.io/kiwi/installation.html" target="_blank">https://osinside.github.io/kiwi/installation.html</a>.
  </p><p>
   KIWI creates images in a two-step process:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     The <code class="literal">prepare</code> operation generates an unpacked image tree
     using the information provided in the image description.
    </p></li><li class="listitem"><p>
     The <code class="literal">create</code> operation creates the packed image based on
     the unpacked image and the information provided in the configuration file
     (<code class="filename">config.xml</code>).
    </p></li></ol></div><p>
   Instructions for installing KIWI are available at
   <a class="link" href="https://osinside.github.io/kiwi/installation.html" target="_blank">https://osinside.github.io/kiwi/installation.html</a>.
  </p><p>
   Image creation with KIWI is automated and does not require any user
   interaction. The information required for the image creation process is
   provided by the image description.
  </p><p>
   To use and run KIWI requires:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A recent Linux distribution such as:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       openSUSE Leap 42.3
      </p></li><li class="listitem"><p>
       SUSE Linux Enterprise 12 SP3
      </p></li><li class="listitem"><p>
       openSUSE Tumbleweed
      </p></li></ul></div></li><li class="listitem"><p>
     Enough free disk space to build and store the image (a minimum of 10 GB is
     recommended).
    </p></li><li class="listitem"><p>
     Python version 2.7, 3.4 or higher. KIWI supports both Python 2 and 3
     versions
    </p></li><li class="listitem"><p>
     Git (package <span class="package">git-core</span>) to clone a repository.
    </p></li><li class="listitem"><p>
     Virtualization technology to start the image (QEMU is recommended).
    </p></li></ul></div></section><section class="sect2" id="sec-ironic-provision-opensuse" data-id-title="Creating an openSUSE Image with KIWI"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.3.12 </span><span class="title-name">Creating an openSUSE Image with KIWI</span></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-opensuse">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following example shows how to build an openSUSE Leap image that is
   ready to run in QEMU.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Retrieve the example image descriptions.
    </p><div class="verbatim-wrap"><pre class="screen">git clone https://github.com/SUSE/kiwi-descriptions</pre></div></li><li class="step"><p>
     Build the image with KIWI:
    </p><div class="verbatim-wrap"><pre class="screen">sudo kiwi-ng --type vmx system build \
  --description kiwi-descriptions/suse/x86_64/suse-leap-42.3-JeOS \
  --target-dir /tmp/myimage</pre></div><p>
     A <code class="filename">.raw</code> image will be built in the
     <code class="filename">/tmp/myimage</code> directory.
    </p></li><li class="step"><p>
     Test the live image with QEMU:
    </p><div class="verbatim-wrap"><pre class="screen">qemu \
  -drive file=LimeJeOS-Leap-42.3.x86_64-1.42.3.raw,format=raw,if=virtio \
  -m 4096</pre></div></li><li class="step"><p>
     With a successful test, the image is complete.
    </p></li></ol></div></div><p>
   By default, KIWI generates a file in the <code class="filename">.raw</code> format.
   The <code class="filename">.raw</code> file is a disk image with a structure
   equivalent to a physical hard disk. <code class="filename">.raw</code> images are
   supported by any hypervisor, but are not compressed and do not offer the
   best performance.
  </p><p>
   Virtualization systems support their own formats (such as
   <code class="literal">qcow2</code> or <code class="literal">vmdk</code>) with compression and
   improved I/O performance. To build an image in a format other than
   <code class="filename">.raw</code>, add the format attribute to the type definition
   in the preferences section of <code class="filename">config.xml</code>. Using
   <code class="literal">qcow2</code> for example:
  </p><div class="verbatim-wrap"><pre class="screen">&lt;image ...&gt;
  &lt;preferences&gt;
    &lt;type format="qcow2" .../&gt;
    ...
  &lt;/preferences&gt;
  ...
&lt;/image</pre></div><p>
   More information about KIWI is at
   <a class="link" href="https://osinside.github.io/kiwi/" target="_blank">https://osinside.github.io/kiwi/</a>.
  </p></section></section><section class="sect1" id="ironic-provisioning-multi-tenancy" data-id-title="Provisioning Baremetal Nodes with Multi-Tenancy"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.4 </span><span class="title-name">Provisioning Baremetal Nodes with Multi-Tenancy</span></span> <a title="Permalink" class="permalink" href="#ironic-provisioning-multi-tenancy">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_provisioning_multi_tenancy.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To enable Ironic multi-tenancy, you must first manually install the
  <code class="literal">python-networking-generic-switch</code> package along with all
  its dependents on all Neutron nodes.
 </p><p>
  To manually enable the <code class="literal">genericswitch</code> mechanism driver in
  Neutron, the <code class="literal">networking-generic-switch</code> package must be
  installed first. Do the following steps in each of the controllers where
  Neutron is running.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Comment out the <code class="literal">multi_tenancy_switch_config</code> section in
    <code class="filename">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>.
   </p></li><li class="step"><p>
    SSH into the controller node
   </p></li><li class="step"><p>
    Change to root
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo -i</pre></div></li><li class="step"><p>
    Activate the neutron venv
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo . /opt/stack/venv/neutron-20180528T093206Z/bin/activate</pre></div></li><li class="step"><p>
    Install netmiko package
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo pip install netmiko</pre></div></li><li class="step"><p>
    Clone the <code class="literal">networking-generic-switch</code> source code into
    <code class="filename">/tmp</code>
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cd /tmp
<code class="prompt user">tux &gt; </code>sudo git clone
   https://github.com/openstack/networking-generic-switch.git</pre></div></li><li class="step"><p>
    Install <code class="literal">networking_generic_switch</code> package
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo python setup.py install</pre></div></li></ol></div></div><p>
  After the <code class="literal">networking_generic_switch</code> package is installed,
  the <code class="literal">genericswitch</code> settings must be enabled in the input
  model. The following process must be run again any time a maintenance update
  is installed that updates the Neutron venv.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    SSH into the deployer node as the user <code class="literal">ardana</code>.
   </p></li><li class="step"><p>
    Edit the Ironic configuration data in the input model
    <code class="filename">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>. Make
    sure the <code class="literal">multi_tenancy_switch_config:</code> section is
    uncommented and has the appropriate settings. <code class="literal">driver_type</code> should be <code class="literal">genericswitch</code> and
    <code class="literal">device_type</code> should be
    <code class="literal">netmiko_hp_comware</code>.
   </p><div class="verbatim-wrap"><pre class="screen">multi_tenancy_switch_config:
  -
    id: switch1
    driver_type: genericswitch
    device_type: netmiko_hp_comware
    ip_address: 192.168.75.201
    username: IRONICSHARE
    password: 'k27MwbEDGzTm'</pre></div></li><li class="step"><p>
    Run the configure process to generate the model
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
    Run <code class="filename">neutron-reconfigure.yml</code>
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost neutron-reconfigure.yml</pre></div></li><li class="step"><p>
    Run <code class="filename">neutron-status.yml</code> to make sure everything is OK
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li></ol></div></div><p>
  With the <code class="literal">networking-generic-switch</code> package installed and
  enabled, you can proceed with provisioning baremetal nodes with multi-tenancy.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Create a network and a subnet:
   </p><div class="verbatim-wrap"><pre class="screen">$ neutron net-create guest-net-1
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2017-06-10T02:49:56Z                 |
| description               |                                      |
| id                        | 256d55a6-9430-4f49-8a4c-cc5192f5321e |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| mtu                       | 1500                                 |
| name                      | guest-net-1                          |
| project_id                | 57b792cdcdd74d16a08fc7a396ee05b6     |
| provider:network_type     | vlan                                 |
| provider:physical_network | physnet1                             |
| provider:segmentation_id  | 1152                                 |
| revision_number           | 2                                    |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | 57b792cdcdd74d16a08fc7a396ee05b6     |
| updated_at                | 2017-06-10T02:49:57Z                 |
+---------------------------+--------------------------------------+

$ neutron  subnet-create guest-net-1 200.0.0.0/24
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {"start": "200.0.0.2", "end": "200.0.0.254"} |
| cidr              | 200.0.0.0/24                                 |
| created_at        | 2017-06-10T02:53:08Z                         |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | True                                         |
| gateway_ip        | 200.0.0.1                                    |
| host_routes       |                                              |
| id                | 53accf35-ae02-43ae-95d8-7b5efed18ae9         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              |                                              |
| network_id        | 256d55a6-9430-4f49-8a4c-cc5192f5321e         |
| project_id        | 57b792cdcdd74d16a08fc7a396ee05b6             |
| revision_number   | 2                                            |
| service_types     |                                              |
| subnetpool_id     |                                              |
| tenant_id         | 57b792cdcdd74d16a08fc7a396ee05b6             |
| updated_at        | 2017-06-10T02:53:08Z                         |
+-------------------+----------------------------------------------+</pre></div></li><li class="step"><p>
    Review glance image list
   </p><div class="verbatim-wrap"><pre class="screen">$ glance image list
+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| 0526d2d7-c196-4c62-bfe5-a13bce5c7f39 | cirros-0.4.0-x86_64      |
+--------------------------------------+--------------------------+</pre></div></li><li class="step"><p>
    Create Ironic node
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 node-create -d agent_ipmitool \
  -n test-node-1 -i ipmi_address=192.168.9.69 -i ipmi_username=ipmi_user \
  -i ipmi_password=XXXXXXXX --network-interface neutron -p  memory_mb=4096 \
  -p cpu_arch=x86_64 -p local_gb=80 -p cpus=2 \
  -p capabilities=boot_mode:bios,boot_option:local \
  -p root_device='{"name":"/dev/sda"}' \
  -i deploy_kernel=db3d131f-2fb0-4189-bb8d-424ee0886e4c \
  -i deploy_ramdisk=304cae15-3fe5-4f1c-8478-c65da5092a2c

+-------------------+-------------------------------------------------------------------+
| Property          | Value                                                             |
+-------------------+-------------------------------------------------------------------+
| chassis_uuid      |                                                                   |
| driver            | agent_ipmitool                                                    |
| driver_info       | {u'deploy_kernel': u'db3d131f-2fb0-4189-bb8d-424ee0886e4c',       |
|                   | u'ipmi_address': u'192.168.9.69',                                 |
|                   | u'ipmi_username': u'gozer', u'ipmi_password': u'******',          |
|                   | u'deploy_ramdisk': u'304cae15-3fe5-4f1c-8478-c65da5092a2c'}       |
| extra             | {}                                                                |
| name              | test-node-1                                                       |
| network_interface | neutron                                                           |
| properties        | {u'cpu_arch': u'x86_64', u'root_device': {u'name': u'/dev/sda'},  |
|                   | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local', |
|                   | u'memory_mb': 4096, u'local_gb': 80}                              |
| resource_class    | None                                                              |
| uuid              | cb4dda0d-f3b0-48b9-ac90-ba77b8c66162                              |
+-------------------+-------------------------------------------------------------------+</pre></div><p>
    ipmi_address, ipmi_username and ipmi_password are IPMI access parameters for
    baremetal Ironic node. Adjust memory_mb, cpus, local_gb to your node size
    requirements. They also need to be reflected in flavor setting (see below).
    Use capabilities boot_mode:bios for baremetal nodes operating in Legacy
    BIOS mode. For UEFI baremetal nodes, use boot_mode:uefi lookup
    deploy_kernel and deploy_ramdisk in glance image list output above.
   </p><div id="id-1.4.5.12.6.8.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     Since we are using Ironic API version 1.22, node is created initial state
     <span class="bold"><strong>enroll</strong></span>. It needs to be explicitly moved
     to <span class="bold"><strong>available</strong></span> state. This behavior changed
     in API version 1.11
    </p></div></li><li class="step"><p>
    Create port
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 port-create --address f0:92:1c:05:6c:40 \
  --node cb4dda0d-f3b0-48b9-ac90-ba77b8c66162 -l switch_id=e8:f7:24:bf:07:2e -l \
  switch_info=hp59srv1-a-11b -l port_id="Ten-GigabitEthernet 1/0/34" \
  --pxe-enabled true
+-----------------------+--------------------------------------------+
| Property              | Value                                      |
+-----------------------+--------------------------------------------+
| address               | f0:92:1c:05:6c:40                          |
| extra                 | {}                                         |
| local_link_connection | {u'switch_info': u'hp59srv1-a-11b',        |
|                       | u'port_id': u'Ten-GigabitEthernet 1/0/34', |
|                       | u'switch_id': u'e8:f7:24:bf:07:2e'}        |
| node_uuid             | cb4dda0d-f3b0-48b9-ac90-ba77b8c66162       |
| pxe_enabled           | True                                       |
| uuid                  | a49491f3-5595-413b-b4a7-bb6f9abec212       |
+-----------------------+--------------------------------------------+</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      for <code class="option">--address</code>, use MAC of 1st NIC of ironic baremetal
      node, which will be used for PXE boot
     </p></li><li class="listitem"><p>
      for <code class="option">--node</code>, use ironic node uuid (see above)
     </p></li><li class="listitem"><p>
      for <code class="option">-l switch_id</code>, use switch management interface MAC
      address. It can be
      retrieved by pinging switch management IP and looking up MAC address in
      'arp -l -n' command output.
     </p></li><li class="listitem"><p>
      for <code class="option">-l switch_info</code>, use switch_id from
      <code class="filename">data/ironic/ironic_config.yml</code>
      file. If you have several switch config definitions, use the right switch
      your baremetal node is connected to.
     </p></li><li class="listitem"><p>
      for -l port_id, use port ID on the switch
     </p></li></ul></div></li><li class="step"><p>
    Move ironic node to manage and then available state
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 manage
$ ironic node-set-provision-state test-node-1 provide</pre></div></li><li class="step"><p>
    Once node is successfully moved to available state, its resources should
    be included into Nova hypervisor statistics
   </p><div class="verbatim-wrap"><pre class="screen">$ nova hypervisor-stats
+----------------------+-------+
| Property             | Value |
+----------------------+-------+
| count                | 1     |
| current_workload     | 0     |
| disk_available_least | 80    |
| free_disk_gb         | 80    |
| free_ram_mb          | 4096  |
| local_gb             | 80    |
| local_gb_used        | 0     |
| memory_mb            | 4096  |
| memory_mb_used       | 0     |
| running_vms          | 0     |
| vcpus                | 2     |
| vcpus_used           | 0     |
+----------------------+-------+</pre></div></li><li class="step"><p>
    Prepare a keypair, which will be used for logging into the node
   </p><div class="verbatim-wrap"><pre class="screen">$ nova keypair-add ironic_kp &gt; ironic_kp.pem</pre></div></li><li class="step"><p>
    Obtain user image and upload it to glance. Please refer to OpenStack
    documentation on user image creation:
    <a class="link" href="https://docs.openstack.org/project-install-guide/baremetal/draft/configure-glance-images.html" target="_blank">https://docs.openstack.org/project-install-guide/baremetal/draft/configure-glance-images.html</a>.
   </p><div id="id-1.4.5.12.6.8.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     Deployed images are already populated by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer.
    </p></div><div class="verbatim-wrap"><pre class="screen">$ glance image-create --name='Ubuntu Trusty 14.04' --disk-format=qcow2 \
  --container-format=bare --file ~/ubuntu-trusty.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | d586d8d2107f328665760fee4c81caf0     |
| container_format | bare                                 |
| created_at       | 2017-06-13T22:38:45Z                 |
| disk_format      | qcow2                                |
| id               | 9fdd54a3-ccf5-459c-a084-e50071d0aa39 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | Ubuntu Trusty 14.04                  |
| owner            | 57b792cdcdd74d16a08fc7a396ee05b6     |
| protected        | False                                |
| size             | 371508736                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2017-06-13T22:38:55Z                 |
| virtual_size     | None                                 |
| visibility       | private                              |
+------------------+--------------------------------------+

$ glance image list
+--------------------------------------+---------------------------+
| ID                                   | Name                      |
+--------------------------------------+---------------------------+
| 0526d2d7-c196-4c62-bfe5-a13bce5c7f39 | cirros-0.4.0-x86_64       |
| 83eecf9c-d675-4bf9-a5d5-9cf1fe9ee9c2 | ir-deploy-iso-<em class="replaceable">EXAMPLE</em>     |
| db3d131f-2fb0-4189-bb8d-424ee0886e4c | ir-deploy-kernel-<em class="replaceable">EXAMPLE</em>  |
| 304cae15-3fe5-4f1c-8478-c65da5092a2c | ir-deploy-ramdisk-<em class="replaceable"> EXAMPLE</em> |
| 9fdd54a3-ccf5-459c-a084-e50071d0aa39 | Ubuntu Trusty 14.04       |
+--------------------------------------+---------------------------+</pre></div></li><li class="step"><p>
    Create a baremetal flavor and set flavor keys specifying requested node
    size, architecture and boot mode. A flavor can be re-used for several nodes
    having the same size, architecture and boot mode
   </p><div class="verbatim-wrap"><pre class="screen">$ nova flavor-create m1.ironic auto 4096 80 2
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+
| ID          | Name      | Mem_MB | Disk | Ephemrl | Swap | VCPUs | RXTX_Factor | Is_Public |
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+
| ab69...87bf | m1.ironic | 4096   | 80   | 0       |      | 2     | 1.0         | True      |
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+

$ nova flavor-key ab6988...e28694c87bf set cpu_arch=x86_64
$ nova flavor-key ab6988...e28694c87bf set capabilities:boot_option="local"
$ nova flavor-key ab6988...e28694c87bf set capabilities:boot_mode="bios"</pre></div><p>
    Parameters must match parameters of ironic node above. Use
    <code class="literal">capabilities:boot_mode="bios"</code> for Legacy BIOS nodes. For
    UEFI nodes, use <code class="literal">capabilities:boot_mode="uefi"</code>
   </p></li><li class="step"><p>
    Boot the node
   </p><div class="verbatim-wrap"><pre class="screen">$ nova boot --flavor m1.ironic --image 9fdd54a3-ccf5-459c-a084-e50071d0aa39 \
  --key-name ironic_kp --nic net-id=256d55a6-9430-4f49-8a4c-cc5192f5321e \
  test-node-1
+--------------------------------------+-------------------------------------------------+
| Property                             | Value                                           |
+--------------------------------------+-------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                          |
| OS-EXT-AZ:availability_zone          |                                                 |
| OS-EXT-SRV-ATTR:host                 | -                                               |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                               |
| OS-EXT-SRV-ATTR:instance_name        |                                                 |
| OS-EXT-STS:power_state               | 0                                               |
| OS-EXT-STS:task_state                | scheduling                                      |
| OS-EXT-STS:vm_state                  | building                                        |
| OS-SRV-USG:launched_at               | -                                               |
| OS-SRV-USG:terminated_at             | -                                               |
| accessIPv4                           |                                                 |
| accessIPv6                           |                                                 |
| adminPass                            | XXXXXXXXXXXX                                    |
| config_drive                         |                                                 |
| created                              | 2017-06-14T21:25:18Z                            |
| flavor                               | m1.ironic (ab69881...5a-497d-93ae-6e28694c87bf) |
| hostId                               |                                                 |
| id                                   | f1a8c63e-da7b-4d9a-8648-b1baa6929682            |
| image                                | Ubuntu Trusty 14.04 (9fdd54a3-ccf5-4a0...0aa39) |
| key_name                             | ironic_kp                                       |
| metadata                             | {}                                              |
| name                                 | test-node-1                                     |
| os-extended-volumes:volumes_attached | []                                              |
| progress                             | 0                                               |
| security_groups                      | default                                         |
| status                               | BUILD                                           |
| tenant_id                            | 57b792cdcdd74d16a08fc7a396ee05b6                |
| updated                              | 2017-06-14T21:25:17Z                            |
| user_id                              | cc76d7469658401fbd4cf772278483d9                |
+--------------------------------------+-------------------------------------------------+</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      for <code class="option">--image</code>, use the ID of user image created at
      previous step
     </p></li><li class="listitem"><p>
      for <code class="option">--nic net-id</code>, use ID of
      the tenant network created at the beginning
     </p></li></ul></div><div id="id-1.4.5.12.6.8.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     During the node provisioning, the following is happening in the
     background:
    </p><p>
     Neutron connects to switch management interfaces and assigns provisioning
     VLAN to baremetal node port on the switch. Ironic powers up the node using
     IPMI interface. Node is booting IPA image via PXE. IPA image is writing
     provided user image onto specified root device
     (<code class="filename">/dev/sda</code>) and powers node
     down. Neutron connects to switch management interfaces and assigns tenant
     VLAN to baremetal node port on the switch. A VLAN ID is selected from
     provided range. Ironic powers up the node using IPMI interface. Node is
     booting user image from disk.
    </p></div></li><li class="step"><p>
    Once provisioned, node will join the private tenant network. Access to
    private network from other networks is defined by switch configuration.
   </p></li></ol></div></div></section><section class="sect1" id="ironic-system-details" data-id-title="View Ironic System Details"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.5 </span><span class="title-name">View Ironic System Details</span></span> <a title="Permalink" class="permalink" href="#ironic-system-details">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-system_details.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="id-1.4.5.12.7.2" data-id-title="View details about the server using nova show nova-node-id"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.5.1 </span><span class="title-name">View details about the server using <code class="command">nova show &lt;nova-node-id&gt;</code></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.7.2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-system_details.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">nova show a90122ce-bba8-496f-92a0-8a7cb143007e

+--------------------------------------+-----------------------------------------------+
| Property                             | Value                                         |
+--------------------------------------+-----------------------------------------------+
| OS-EXT-AZ:availability_zone          | nova                                          |
| OS-EXT-SRV-ATTR:host                 | ardana-cp1-ir-compute0001-mgmt                |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | ea7246fd-e1d6-4637-9699-0b7c59c22e67          |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000a                             |
| OS-EXT-STS:power_state               | 1                                             |
| OS-EXT-STS:task_state                | -                                             |
| OS-EXT-STS:vm_state                  | active                                        |
| OS-SRV-USG:launched_at               | 2016-03-11T12:26:25.000000                    |
| OS-SRV-USG:terminated_at             | -                                             |
| accessIPv4                           |                                               |
| accessIPv6                           |                                               |
| config_drive                         |                                               |
| created                              | 2016-03-11T12:17:54Z                          |
| flat-net network                     | 192.3.15.14                                   |
| flavor                               | bmtest (645de08d-2bc6-43f1-8a5f-2315a75b1348) |
| hostId                               | ecafa4f40eb5f72f7298...3bad47cbc01aa0a076114f |
| id                                   | a90122ce-bba8-496f-92a0-8a7cb143007e          |
| image                                | ubuntu (17e4915a-ada0-4b95-bacf-ba67133f39a7) |
| key_name                             | ironic_kp                                     |
| metadata                             | {}                                            |
| name                                 | ubuntu                                        |
| os-extended-volumes:volumes_attached | []                                            |
| progress                             | 0                                             |
| security_groups                      | default                                       |
| status                               | ACTIVE                                        |
| tenant_id                            | d53bcaf15afb4cb5aea3adaedbaa60dd              |
| updated                              | 2016-03-11T12:26:26Z                          |
| user_id                              | e580c645bfec4faeadef7dbd24aaf990              |
+--------------------------------------+-----------------------------------------------+</pre></div></section><section class="sect2" id="id-1.4.5.12.7.3" data-id-title="View detailed information about a node using ironic node-show ironic-node-id"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.5.2 </span><span class="title-name">View detailed information about a node using <code class="command">ironic node-show &lt;ironic-node-id&gt;</code></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.7.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-system_details.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-show  ea7246fd-e1d6-4637-9699-0b7c59c22e67

+------------------------+--------------------------------------------------------------------------+
| Property               | Value                                                                    |
+------------------------+--------------------------------------------------------------------------+
| target_power_state     | None                                                                     |
| extra                  | {}                                                                       |
| last_error             | None                                                                     |
| updated_at             | 2016-03-11T12:26:25+00:00                                                |
| maintenance_reason     | None                                                                     |
| provision_state        | active                                                                   |
| clean_step             | {}                                                                       |
| uuid                   | ea7246fd-e1d6-4637-9699-0b7c59c22e67                                     |
| console_enabled        | False                                                                    |
| target_provision_state | None                                                                     |
| provision_updated_at   | 2016-03-11T12:26:25+00:00                                                |
| maintenance            | False                                                                    |
| inspection_started_at  | None                                                                     |
| inspection_finished_at | None                                                                     |
| power_state            | power on                                                                 |
| driver                 | agent_ilo                                                                |
| reservation            | None                                                                     |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,           |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local'}        |
| instance_uuid          | a90122ce-bba8-496f-92a0-8a7cb143007e                                     |
| name                   | None                                                                     |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',            |
|                        | u'ilo_deploy_iso': u'b9499494-7db3-4448-b67f-233b86489c1f',              |
|                        | u'ilo_username': u'Administrator'}                                       |
| created_at             | 2016-03-11T10:17:10+00:00                                                |
| driver_internal_info   | {u'agent_url': u'http://192.3.15.14:9999',                               |
|                        | u'is_whole_disk_image': True, u'agent_last_heartbeat': 1457699159}       |
| chassis_uuid           |                                                                          |
| instance_info          | {u'root_gb': u'99', u'display_name': u'ubuntu', u'image_source': u       |
|                        | '17e4915a-ada0-4b95-bacf-ba67133f39a7', u'capabilities': u'{"boot_mode": |
|                        | "bios", "boot_option": "local"}', u'memory_mb': u'64000', u'vcpus':      |
|                        | u'2', u'image_url': u'http://192.168.12.2:8080/v1/AUTH_ba121db7732f4ac3a |
|                        | 50cc4999a10d58d/glance/17e4915a-ada0-4b95-bacf-ba67133f39a7?temp_url_sig |
|                        | =ada691726337805981ac002c0fbfc905eb9783ea&amp;temp_url_expires=1457699878',  |
|                        | u'image_container_format': u'bare', u'local_gb': u'99',                  |
|                        | u'image_disk_format': u'qcow2', u'image_checksum':                       |
|                        | u'2d7bb1e78b26f32c50bd9da99102150b', u'swap_mb': u'0'}                   |
+------------------------+--------------------------------------------------------------------------+</pre></div></section><section class="sect2" id="id-1.4.5.12.7.4" data-id-title="View detailed information about a port using ironic port-show ironic-port-id"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.5.3 </span><span class="title-name">View detailed information about a port using <code class="command">ironic port-show &lt;ironic-port-id&gt;</code></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.7.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-system_details.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic port-show a17a4ef8-a711-40e2-aa27-2189c43f0b67

+------------+-----------------------------------------------------------+
| Property   | Value                                                     |
+------------+-----------------------------------------------------------+
| node_uuid  | ea7246fd-e1d6-4637-9699-0b7c59c22e67                      |
| uuid       | a17a4ef8-a711-40e2-aa27-2189c43f0b67                      |
| extra      | {u'vif_port_id': u'82a5ab28-76a8-4c9d-bfb4-624aeb9721ea'} |
| created_at | 2016-03-11T10:40:53+00:00                                 |
| updated_at | 2016-03-11T12:17:56+00:00                                 |
| address    | 5c:b9:01:88:f0:a4                                         |
+------------+-----------------------------------------------------------+</pre></div></section><section class="sect2" id="id-1.4.5.12.7.5" data-id-title="View detailed information about a hypervisor using nova hypervisor-list and nova hypervisor-show"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.5.4 </span><span class="title-name">View detailed information about a hypervisor using <code class="command">nova hypervisor-list</code> and <code class="command">nova hypervisor-show</code></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.7.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-system_details.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">nova hypervisor-list

+-----+--------------------------------------+-------+---------+
| ID  | Hypervisor hostname                  | State | Status  |
+-----+--------------------------------------+-------+---------+
| 541 | ea7246fd-e1d6-4637-9699-0b7c59c22e67 | up    | enabled |
+-----+--------------------------------------+-------+---------+</pre></div><div class="verbatim-wrap"><pre class="screen">nova hypervisor-show ea7246fd-e1d6-4637-9699-0b7c59c22e67

+-------------------------+--------------------------------------+
| Property                | Value                                |
+-------------------------+--------------------------------------+
| cpu_info                |                                      |
| current_workload        | 0                                    |
| disk_available_least    | 0                                    |
| free_disk_gb            | 0                                    |
| free_ram_mb             | 0                                    |
| host_ip                 | 192.168.12.6                         |
| hypervisor_hostname     | ea7246fd-e1d6-4637-9699-0b7c59c22e67 |
| hypervisor_type         | ironic                               |
| hypervisor_version      | 1                                    |
| id                      | 541                                  |
| local_gb                | 99                                   |
| local_gb_used           | 99                                   |
| memory_mb               | 64000                                |
| memory_mb_used          | 64000                                |
| running_vms             | 1                                    |
| service_disabled_reason | None                                 |
| service_host            | ardana-cp1-ir-compute0001-mgmt       |
| service_id              | 25                                   |
| state                   | up                                   |
| status                  | enabled                              |
| vcpus                   | 2                                    |
| vcpus_used              | 2                                    |
+-------------------------+--------------------------------------+</pre></div></section><section class="sect2" id="id-1.4.5.12.7.6" data-id-title="View a list of all running services using nova service-list"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.5.5 </span><span class="title-name">View a list of all running services using <code class="command">nova service-list</code></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.7.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-system_details.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">nova service-list

+----+------------------+-----------------------+----------+---------+-------+------------+----------+
| Id | Binary           | Host                  | Zone     | Status  | State | Updated_at | Disabled |
|    |                  |                       |          |         |       |            | Reason   |
+----+------------------+-----------------------+----------+---------+-------+------------+----------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt | internal | enabled | up    | date:time  | -        |
| 7  | nova-conductor   |  " -cp1-c1-m2-mgmt    | internal | enabled | up    | date:time  | -        |
| 10 | nova-conductor   |  " -cp1-c1-m3-mgmt    | internal | enabled | up    | date:time  | -        |
| 13 | nova-scheduler   |  " -cp1-c1-m1-mgmt    | internal | enabled | up    | date:time  | -        |
| 16 | nova-scheduler   |  " -cp1-c1-m3-mgmt    | internal | enabled | up    | date:time  | -        |
| 19 | nova-scheduler   |  " -cp1-c1-m2-mgmt    | internal | enabled | up    | date:time  | -        |
| 22 | nova-consoleauth |  " -cp1-c1-m1-mgmt    | internal | enabled | up    | date:time  | -        |
| 25 | nova-compute     |  " -cp1-ir- | nova    |          | enabled | up    | date:time  | -        |
|    |                  |      compute0001-mgmt |          |         |       |            |          |
+----+------------------+-----------------------+----------+---------+-------+------------+----------+</pre></div></section></section><section class="sect1" id="ironic-toubleshooting" data-id-title="Troubleshooting Ironic Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.6 </span><span class="title-name">Troubleshooting Ironic Installation</span></span> <a title="Permalink" class="permalink" href="#ironic-toubleshooting">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Sometimes the <code class="literal">nova boot</code> command does not succeed and when
  you do a <code class="literal">nova list</code>, you will see output like the
  following:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova list

+------------------+--------------+--------+------------+-------------+----------+
| ID               | Name         | Status | Task State | Power State | Networks |
+------------------+--------------+--------+------------+-------------+----------+
| ee08f82...624e5f | OpenSUSE42.3 | ERROR  | -          | NOSTATE     |          |
+------------------+--------------+--------+------------+-------------+----------+</pre></div><p>
  You should execute the <code class="literal">nova show &lt;nova-node-id&gt;</code> and
  <code class="literal">ironic node-show &lt;ironic-node-id&gt;</code> commands to get
  more information about the error.
 </p><section class="sect2" id="id-1.4.5.12.8.5" data-id-title="Error: No valid host was found."><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.1 </span><span class="title-name">Error: No valid host was found.</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The error <code class="literal">No valid host was found. There are not enough
   hosts.</code> is typically seen when performing the <code class="literal">nova
   boot</code> where there is a mismatch between the properties set on the
   node and the flavor used. For example, the output from a <code class="literal">nova
   show</code> command may look like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova show ee08f82e-8920-4360-be51-a3f995624e5f

+------------------------+------------------------------------------------------------------------------+
| Property               | Value                                                                        |
+------------------------+------------------------------------------------------------------------------+
| OS-EXT-AZ:             |                                                                              |
|   availability_zone    |                                                                              |
| OS-EXT-SRV-ATTR:host   | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   hypervisor_hostname  | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   instance_name        | instance-00000001                                                            |
| OS-EXT-STS:power_state | 0                                                                            |
| OS-EXT-STS:task_state  | -                                                                            |
| OS-EXT-STS:vm_state    | error                                                                        |
| OS-SRV-USG:launched_at | -                                                                            |
| OS-SRV-USG:            |                                                                              |
|    terminated_at       | -                                                                            |
| accessIPv4             |                                                                              |
| accessIPv6             |                                                                              |
| config_drive           |                                                                              |
| created                | 2016-03-11T11:00:28Z                                                         |
| fault                  | {"message": "<span class="bold"><strong>No valid host was found. There are not enough hosts             |
|                        |  available.</strong></span>", "code": 500, "details": "  File \<span class="bold"><strong>"/opt/stack/                  |
|                        |  venv/nova-20160308T002421Z/lib/python2.7/site-packages/nova/                |
|                        |  conductor/manager.py\"</strong></span>, line 739, in build_instances                        |
|                        |     request_spec, filter_properties)                                         |
|                        |   File \<span class="bold"><strong>"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/utils.py\"</strong></span>, line 343, in wrapped              |
|                        |     return func(*args, **kwargs)                                             |
|                        |   File \<span class="bold"><strong>"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\"</strong></span>, line 52,                |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\",line 37,in __run_method  |
|                        |     return getattr(self.instance, __name)(*args, **kwargs)                   |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/query.py\", line 34,                   |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/rpcapi.py\", line 120, in select_destinations |
|                        |     request_spec=request_spec, filter_properties=filter_properties)          |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/rpc/client.py\", line 158, in call            |
|                        |     retry=self.retry)                                                        |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/transport.py\", line 90, in _send             |
|                        |     timeout=timeout, retry=retry)                                            |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 462, in send   |
|                        |     retry=retry)                                                             |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 453, in _send  |
|                        |     raise result                                                             |
|                        | ", "created": "2016-03-11T11:00:29Z"}                                        |
| flavor                 | bmtest (645de08d-2bc6-43f1-8a5f-2315a75b1348)                                |
| hostId                 |                                                                              |
| id                     | ee08f82e-8920-4360-be51-a3f995624e5f                                         |
| image                  | opensuse (17e4915a-ada0-4b95-bacf-ba67133f39a7)                              |
| key_name               | ironic_kp                                                                    |
| metadata               | {}                                                                           |
| name                   | opensuse                                                                     |
| os-extended-volumes:   |                                                                              |
|    volumes_attached    | []                                                                           |
| status                 | ERROR                                                                        |
| tenant_id              | d53bcaf15afb4cb5aea3adaedbaa60dd                                             |
| updated                | 2016-03-11T11:00:28Z                                                         |
| user_id                | e580c645bfec4faeadef7dbd24aaf990                                             |
+------------------------+------------------------------------------------------------------------------+</pre></div><p>
   You can find more information about the error by inspecting the log file at
   <code class="literal">/var/log/nova/nova-scheduler.log</code> or alternatively by
   viewing the error location in the source files listed in the stack-trace (in
   bold above).
  </p><p>
   To find the mismatch, compare the properties of the Ironic node:
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+---------------------------------------------------------------------+
| Property               | Value                                                               |
+------------------------+---------------------------------------------------------------------+
| target_power_state     | None                                                                |
| extra                  | {}                                                                  |
| last_error             | None                                                                |
| updated_at             | None                                                                |
| maintenance_reason     | None                                                                |
| provision_state        | available                                                           |
| clean_step             | {}                                                                  |
| uuid                   | ea7246fd-e1d6-4637-9699-0b7c59c22e67                                |
| console_enabled        | False                                                               |
| target_provision_state | None                                                                |
| provision_updated_at   | None                                                                |
| maintenance            | False                                                               |
| inspection_started_at  | None                                                                |
| inspection_finished_at | None                                                                |
| power_state            | None                                                                |
| driver                 | agent_ilo                                                           |
| reservation            | None                                                                |
| properties             | <span class="bold"><strong>{u'memory_mb': 64000, u'local_gb': 99, u'cpus': 2, u'capabilities':</strong></span> |
|                        | <span class="bold"><strong>u'boot_mode:bios,boot_option:local'} </strong></span>                               |
| instance_uuid          | None                                                                |
| name                   | None                                                                |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',       |
|                        | u'ilo_deploy_iso': u'b9499494-7db3-4448-b67f-233b86489c1f',         |
|                        | u'ilo_username': u'Administrator'}                                  |
| created_at             | 2016-03-11T10:17:10+00:00                                           |
| driver_internal_info   | {}                                                                  |
| chassis_uuid           |                                                                     |
| instance_info          | {}                                                                  |
+------------------------+---------------------------------------------------------------------+</pre></div><p>
   with the flavor characteristics:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova flavor-show

+----------------------------+-------------------------------------------------------------------+
| Property                   | Value                                                             |
+----------------------------+-------------------------------------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                                             |
| OS-FLV-EXT-DATA:ephemeral  | 0                                                                 |
| disk                       | <span class="bold"><strong>99 </strong></span>                                                               |
| extra_specs                | <span class="bold"><strong>{"capabilities:boot_option": "local", "cpu_arch": "x86_64",       |
|                            | "capabilities:boot_mode": "bios"}</strong></span>                                 |
| id                         | 645de08d-2bc6-43f1-8a5f-2315a75b1348                              |
| name                       | bmtest                                                            |
| os-flavor-access:is_public | True                                                              |
| ram                        | <span class="bold"><strong>64000</strong></span>                                                             |
| rxtx_factor                | 1.0                                                               |
| swap                       |                                                                   |
| vcpus                      | <span class="bold"><strong>2</strong></span>                                                                 |
+----------------------------+-------------------------------------------------------------------+</pre></div><p>
   In this instance, the problem is caused by the absence of the
   <span class="bold"><strong>"cpu_arch": "x86_64"</strong></span> property on the Ironic
   node. This can be resolved by updating the Ironic node, adding the missing
   property:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ironic node-update ea7246fd-e1d6-4637-9699-0b7c59c22e67 \
  <span class="bold"><strong>add properties/cpu_arch=x86_64</strong></span></pre></div><p>
   and then re-running the <code class="literal">nova boot</code> command.
  </p></section><section class="sect2" id="id-1.4.5.12.8.6" data-id-title="Node fails to deploy because it has timed out"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.2 </span><span class="title-name">Node fails to deploy because it has timed out</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> The Neutron API session
   timed out before port creation was completed.
  </p><p>
   <span class="bold"><strong>Resolution: </strong></span> Switch response time varies
   by vendor; the value of <code class="literal">url_timeout</code> must be increased to
   allow for switch response.
  </p><p>
   Check Ironic Conductor logs
   (<code class="filename">/var/log/ironic/ironic-conductor.log</code>) for
   <code class="literal">ConnectTimeout</code> errors while connecting to Neutron for
   port creation. For example:
  </p><div class="verbatim-wrap"><pre class="screen">19-03-20 19:09:14.557 11556 ERROR ironic.conductor.utils
[req-77f3a7b...1b10c5b - default default] Unexpected error while preparing
to deploy to node 557316...84dbdfbe8b0: ConnectTimeout: Request to
https://192.168.75.1:9696/v2.0/ports timed out</pre></div><p>
   Use the following steps to increase the value of
   <code class="literal">url_timeout</code>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the deployer node.
    </p></li><li class="step"><p>
     Edit <code class="filename">./roles/ironic-common/defaults/main.yml</code>,
     increasing the value of <code class="literal">url_timeout</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi ./roles/ironic-common/defaults/main.yml</pre></div><p>
     Increase the value of the <code class="literal">url_timeout</code> parameter in the
     <code class="literal">ironic_neutron:</code> section. Increase the parameter from
     the default (60 seconds) to 120 and then in increments of 60 seconds until
     the node deploys successfully.
    </p></li><li class="step"><p>
     Reconfigure Ironic.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.4.5.12.8.7" data-id-title="Deployment to a node fails and in ironic node-list command, the power_state column for the node is shown as None"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.3 </span><span class="title-name">Deployment to a node fails and in "ironic node-list" command, the power_state column for the node is shown as "None"</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> The IPMI commands to the
   node take longer to change the power state of the server.
  </p><p>
   <span class="bold"><strong>Resolution: </strong></span> Check if the node power state
   can be changed using the following command
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ironic node-set-power-state $NODEUUID on</pre></div><p>
   If the above command succeeds and the power_state column is updated
   correctly, then the following steps are required to increase the power sync
   interval time.
  </p><p>
   On the first controller, reconfigure Ironic to increase the power sync
   interval time. In the example below, it is set to 120 seconds. This value
   may have to be tuned based on the setup.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Go to the <code class="literal">~/openstack/my_cloud/config/ironic/</code> directory
     and edit <code class="literal">ironic-conductor.conf.j2</code> to set the
     <code class="literal">sync_power_state_interval</code> value:
    </p><div class="verbatim-wrap"><pre class="screen">[conductor]
sync_power_state_interval = 120</pre></div></li><li class="step"><p>
     Save the file and then run the following playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.4.5.12.8.8" data-id-title="Error Downloading Image"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.4 </span><span class="title-name">Error Downloading Image</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.8">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you encounter the error below during the deployment:
  </p><div class="verbatim-wrap"><pre class="screen">"u'message': u'Error downloading image: Download of image id 77700...96551 failed:
Image download failed for all URLs.',
u'code': 500,
u'type': u'ImageDownloadError',
u'details': u'Download of image id 77700b53-9e15-406c-b2d5-13e7d9b96551 failed:
Image download failed for all URLs.'"</pre></div><p>
   you should visit the Single Sign-On Settings in the Security page of IPMI and
   change the Single Sign-On Trust Mode setting from the default of "Trust None
   (SSO disabled)" to "Trust by Certificate".
  </p></section><section class="sect2" id="id-1.4.5.12.8.9" data-id-title="Using node-inspection can cause temporary claim of IP addresses"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.5 </span><span class="title-name">Using <code class="literal">node-inspection</code> can cause temporary claim of IP addresses</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.9">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> Running
   <code class="literal">node-inspection</code> on a node discovers all the NIC ports
   including the NICs that do not have any connectivity. This causes a
   temporary consumption of the network IPs and increased usage of the
   allocated quota. As a result, other nodes are deprived of IP addresses and
   deployments can fail.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span>You can add node properties
   manually added instead of using the inspection tool.
  </p><p>
   Note: Upgrade <code class="literal">ipmitool</code> to a version &gt;= 1.8.15 or it
   may not return detailed information about the NIC interface for
   <code class="literal">node-inspection</code>.
  </p></section><section class="sect2" id="id-1.4.5.12.8.10" data-id-title="Node permanently stuck in deploying state"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.6 </span><span class="title-name">Node permanently stuck in deploying state</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.10">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span>
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ironic conductor service associated with the node could go down.
    </p></li><li class="listitem"><p>
     There might be a properties mismatch. MAC address registered for the node
     could be incorrect.
    </p></li></ul></div><p>
   <span class="bold"><strong>Resolution:</strong></span> To recover from this
   condition, set the provision state of the node to <code class="literal">Error</code>
   and maintenance to <code class="literal">True</code>. Delete the node and re-register
   again.
  </p></section><section class="sect2" id="id-1.4.5.12.8.11" data-id-title="The NICs in the baremetal node should come first in boot order"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.7 </span><span class="title-name">The NICs in the baremetal node should come first in boot order</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.11">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span> By default, the boot
   order of baremetal node is set as NIC1, HDD and NIC2. If NIC1 fails, the
   nodes starts booting from HDD and the provisioning fails.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> Set boot order so that all the
   NICs appear before the hard disk of the baremetal as NIC1, NIC2…, HDD.
  </p></section><section class="sect2" id="id-1.4.5.12.8.12" data-id-title="Increase in the number of nodes can cause power commands to fail"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.8 </span><span class="title-name">Increase in the number of nodes can cause power commands to fail</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.12">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span>Ironic periodically
   performs a power state sync with all the baremetal nodes. When the number of
   nodes increase, Ironic does not get sufficient time to perform power
   operations.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> The following procedure gives a
   way to increase <code class="literal">sync_power_state_interval</code>:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit the file
     <code class="literal">~/openstack/my_cloud/config/ironic/ironic-conductor.conf.j2</code>
     and navigate to the section for <code class="literal">[conductor]</code>
    </p></li><li class="step"><p>
     Increase the <code class="literal">sync_power_state_interval</code>. For example,
     for 100 nodes, set <code class="literal">sync_power_state_interval = 90</code> and
     save the file.
    </p></li><li class="step"><p>
     Execute the following set of commands to reconfigure Ironic:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.4.5.12.8.13" data-id-title="DHCP succeeds with PXE but times out with iPXE"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.6.9 </span><span class="title-name">DHCP succeeds with PXE but times out with iPXE</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.13">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you see DHCP error "No configuration methods succeeded" in iPXE right
   after successful DHCP performed by embedded NIC firmware, there may be an
   issue with Spanning Tree Protocol on the switch.
  </p><p>
   To avoid this error, Rapid Spanning Tree Protocol needs to be enabled on the
   switch. If this is not an option due to conservative loop detection
   strategies, use the steps outlined below to install the iPXE binary with
   increased DHCP timeouts.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Clone iPXE source code
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git clone git://git.ipxe.org/ipxe.git
<code class="prompt user">tux &gt; </code>cd ipxe/src</pre></div></li><li class="step"><p>
     Modify lines 22-25 in file <code class="literal">config/dhcp.h</code>, which declare
     reduced DHCP timeouts (1-10 secs). Comment out lines with reduced timeouts
     and uncomment normal PXE timeouts (4-32)
    </p><div class="verbatim-wrap"><pre class="screen">//#define DHCP_DISC_START_TIMEOUT_SEC     1
//#define DHCP_DISC_END_TIMEOUT_SEC       10
#define DHCP_DISC_START_TIMEOUT_SEC   4       /* as per PXE spec */
#define DHCP_DISC_END_TIMEOUT_SEC     32      /* as per PXE spec */</pre></div></li><li class="step"><p>
     Make <code class="literal">undionly.kpxe</code> (BIOS) and
     <code class="literal">ipxe.efi</code> (UEFI) images
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>make bin/undionly.kpxe
<code class="prompt user">tux &gt; </code>make bin-x86_64-efi/ipxe.efi</pre></div></li><li class="step"><p>
     Copy iPXE images to Cloud Lifecycle Manager
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>scp bin/undionly.kpxe bin-x86_64-efi/ipxe.efi stack@10.0.0.4:
stack@10.0.0.4's password:
undionly.kpxe                                    100%   66KB  65.6KB/s   00:00
ipxe.efi                                         100%  918KB 918.2KB/s   00:00</pre></div></li><li class="step"><p>
     From deployer, distribute image files onto all 3 controllers
    </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/

stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=~/ipxe.efi dest=/tftpboot'
...
stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=~/undionly.kpxe dest=/tftpboot'
...</pre></div></li></ol></div></div><p>
   There is no need to restart services. With next PXE boot attempt, iPXE
   binary with the increased timeout will be downloaded to the target node via
   TFTP.
  </p><section class="sect3" id="id-1.4.5.12.8.13.6" data-id-title="Ironic Support and Limitations"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">17.6.9.1 </span><span class="title-name">Ironic Support and Limitations</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.8.13.6">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following drivers are supported and tested:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="systemitem">pxe_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem"><p>
      <code class="systemitem">pxe_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem"><p>
      <code class="systemitem">pxe_ilo</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem"><p>
      <code class="systemitem">agent_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem"><p>
      <code class="systemitem">agent_ilo</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li></ul></div><p>
    <span class="bold"><strong>ISO Image Exceeds Free Space</strong></span>
   </p><p>
   When using the <code class="systemitem">agent_ilo</code> driver, provisioning will
   fail if the size of the user ISO image exceeds the free space available on
   the ramdisk partition. This will produce an error in the Ironic Conductor
   logs that may look like as follows:
  </p><div class="verbatim-wrap"><pre class="screen">"ERROR root [-] Command failed: prepare_image, error: Error downloading
image: Download of image id 0c4d74e4-58f1-4f8d-8c1d-8a49129a2163 failed: Unable
to write image to /tmp/0c4d74e4-58f1-4f8d-8c1d-8a49129a2163. Error: [Errno 28]
No space left on device: ImageDownloadError: Error downloading image: Download
of image id 0c4d74e4-58f1-4f8d-8c1d-8a49129a2163 failed: Unable to write image
to /tmp/0c4d74e4-58f1-4f8d-8c1d-8a49129a2163. Error: [Errno 28] No space left
on device"</pre></div><p>
   By default, the total amount of space allocated to ramdisk is 4GB. To
   increase the space allocated for the ramdisk, you can update the deploy
   ISO image using the following workaround.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Save the deploy ISO to a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image save --file deploy.iso<em class="replaceable">IMAGE_ID</em></pre></div><p>
    Replace <em class="replaceable">IMAGE_ID</em> with the ID of the deploy ISO
    stored in Glance. The ID can be obtained using the <code class="command">openstack image list</code>.
   </p></li><li class="step"><p>
     Mount the saved ISO:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /tmp/mnt
<code class="prompt user">tux &gt; </code>sudo mount -t iso9660 -o loop deploy.iso /tmp/mnt</pre></div><p>
     Since the mount directory is read-only, it is necessary to copy its
     content to be able to make modifications.
    </p></li><li class="step"><p>
     Copy the content of the mount directory to a custom directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /tmp/custom
<code class="prompt user">tux &gt; </code>cp -aRvf /tmp/mnt/* /tmp/custom/</pre></div></li><li class="step"><p>
     Modify the bootloader files to increase the size of the ramdisk:
    </p><div class="verbatim-wrap"><pre class="screen">/tmp/custom/boot/x86_64/loader/isolinux.cfg
/tmp/custom/EFI/BOOT/grub.cfg
/tmp/custom/boot/grub2/grub.cfg</pre></div><p>
     Find the <code class="literal">openstack-ironic-image</code> label and modify the
     <code class="literal">ramdisk_size</code> parameter in the <code class="literal">append</code>
     property. The <code class="literal">ramdisk_size</code> value must be specified in Kilobytes.
    </p><div class="verbatim-wrap"><pre class="screen">label openstack-ironic-image
  kernel linux
  append initrd=initrd ramdisk_size=10485760 ramdisk_blocksize=4096 \
boot_method=vmedia showopts</pre></div><p>
      Make sure that your baremetal node has the
      amount of RAM that equals or exceeds  the <code class="literal">ramdisk_size</code> value.
     </p></li><li class="step"><p>
     Repackage the ISO using the genisoimage tool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd /tmp/custom
<code class="prompt user">tux &gt; </code>genisoimage -b boot/x86_64/loader/isolinux.bin -R -J -pad -joliet-long \
-iso-level 4 -A '0xaa2dab53' -no-emul-boot -boot-info-table \
-boot-load-size 4 -c boot/x86_64/boot.catalog -hide boot/x86_64/boot.catalog \
-hide-joliet boot/x86_64/boot.catalog -eltorito-alt-boot -b boot/x86_64/efi \
-no-emul-boot -joliet-long -hide glump -hide-joliet glump -o /tmp/custom_deploy.iso ./</pre></div><div id="id-1.4.5.12.8.13.6.8.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       When repackaging the ISO, make sure that you use the same label. You can
       find the label file in the <code class="filename">/tmp/custom/boot/</code>
       directory. The label begins with <code class="literal">0x</code>. For example, <code class="literal">0x51e568cb</code>.
      </p></div></li><li class="step"><p>
     Delete the existing deploy ISO in Glance:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image delete <em class="replaceable">IMAGE_ID</em></pre></div></li><li class="step"><p>
     Create a new image with <code class="literal">custom_deploy.iso</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image create --container-format bare \
--disk-format iso --public --file custom_deploy.iso ir-deploy-iso-ARDANA5.0</pre></div></li><li class="step"><p>
     Re-deploy the Ironic node.
    </p></li></ol></div></div><p>
    <span class="bold"><strong>Partition Image Exceeds Free Space</strong></span>
   </p><p>
   The previous procedure applies to ISO images. It does not apply to
   <code class="literal">partition images</code>, although there will be a similar error
   in the Ironic logs. However the resolution is different. An option must be
   added to the <code class="literal">PXE</code> line in the
   <code class="filename">main.yml</code> file to increase the <code class="filename">/tmp</code>
   disk size with the following workaround:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit
     <code class="filename">/openstack/ardana/ansible/roles/ironic-common/defaults/main.yml</code>.
    </p></li><li class="step"><p>
     Add <code class="literal">suse.tmpsize=4G</code> to
     <code class="literal">pxe_append_params</code>. Adjust the size of
     <code class="literal">suse.tmpsize</code> as needed for the partition image.
    </p><div class="verbatim-wrap"><pre class="screen">pxe_append_params : "nofb nomodeset vga=normal elevator=deadline
                     security=apparmor crashkernel=256M console=tty0
                     console=ttyS0 suse.tmpsize=4G"</pre></div></li><li class="step"><p>
     Update Git and run playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Add suse.tmpsize variable"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li><li class="step"><p>
     Re-deploy the Ironic node.
    </p></li></ol></div></div></section></section></section><section class="sect1" id="ironic-node-cleaning" data-id-title="Node Cleaning"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.7 </span><span class="title-name">Node Cleaning</span></span> <a title="Permalink" class="permalink" href="#ironic-node-cleaning">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_cleaning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Cleaning is the process by which data is removed after a previous tenant has
  used the node. Cleaning requires use of ironic's agent_ drivers. It is
  extremely important to note that if the pxe_ drivers are utilized, no node
  cleaning operations will occur, and a previous tenant's data could be found
  on the node. The same risk of a previous tenant's data possibly can occur if
  cleaning is explicitly disabled as part of the installation.
 </p><p>
  By default, cleaning attempts to utilize ATA secure erase to wipe the
  contents of the disk. If secure erase is unavailable, the cleaning
  functionality built into the Ironic Python Agent falls back to an operation
  referred to as "shred" where random data is written over the contents of the
  disk, and then followed up by writing "0"s across the disk. This can be a
  time-consuming process.
 </p><p>
  An additional feature of cleaning is the ability to update firmware or
  potentially assert new hardware configuration, however, this is an advanced
  feature that must be built into the Ironic Python Agent image. Due to the
  complex nature of such operations, and the fact that no one size fits all,
  this requires a custom Ironic Python Agent image to be constructed with an
  appropriate hardware manager. For more information on hardware managers, see
  <a class="link" href="http://docs.openstack.org/developer/ironic-python-agent/#hardware-managers" target="_blank">http://docs.openstack.org/developer/ironic-python-agent/#hardware-managers</a>
 </p><p>
  Ironic's upstream documentation for cleaning may be found here:
  <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/cleaning.html" target="_blank">http://docs.openstack.org/developer/ironic/deploy/cleaning.html</a>
 </p><section class="sect2" id="id-1.4.5.12.9.6" data-id-title="Setup"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.7.1 </span><span class="title-name">Setup</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.9.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_cleaning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Cleaning is enabled by default in ironic when installed via the Cloud Lifecycle Manager.
   You can verify this by examining the ironic-conductor.conf file.
   Look for:
  </p><div class="verbatim-wrap"><pre class="screen">[conductor]
clean_nodes=true</pre></div></section><section class="sect2" id="id-1.4.5.12.9.7" data-id-title="In use"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.7.2 </span><span class="title-name">In use</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.9.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_cleaning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When enabled, cleaning will be run automatically when nodes go from active
   to available state or from manageable to available. To monitor what step of
   cleaning the node is in, run <code class="literal">ironic node-show</code>:
  </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c1-m1-mgmt:~$ ironic node-show 4e6d4273-2535-4830-a826-7f67e71783ed
+------------------------+-----------------------------------------------------------------------+
| Property               | Value                                                                 |
+------------------------+-----------------------------------------------------------------------+
| target_power_state     | None                                                                  |
| extra                  | {}                                                                    |
| last_error             | None                                                                  |
| updated_at             | 2016-04-15T09:33:16+00:00                                             |
| maintenance_reason     | None                                                                  |
| provision_state        | cleaning                                                              |
| clean_step             | {}                                                                    |
| uuid                   | 4e6d4273-2535-4830-a826-7f67e71783ed                                  |
| console_enabled        | False                                                                 |
| target_provision_state | available                                                             |
| provision_updated_at   | 2016-04-15T09:33:16+00:00                                             |
| maintenance            | False                                                                 |
| inspection_started_at  | None                                                                  |
| inspection_finished_at | None                                                                  |
| power_state            | power off                                                             |
| driver                 | agent_ilo                                                             |
| reservation            | ardana-cp1-c1-m1-mgmt                                                 |
| properties             | {u'memory_mb': 4096, u'cpu_arch': u'amd64', u'local_gb': 80,          |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:uefi,boot_option:local'}     |
| instance_uuid          | None                                                                  |
| name                   | None                                                                  |
| driver_info            | {u'ilo_deploy_iso': u'249bf095-e741-441d-bc28-0f44a9b8cd80',          |
|                        | u'ipmi_username': u'Administrator', u'deploy_kernel':                 |
|                        | u'3a78c0a9-3d8d-4764-9300-3e9c00e167a1', u'ilo_address':              |
|                        | u'10.1.196.113', u'ipmi_address': u'10.1.196.113', u'deploy_ramdisk': |
|                        | u'd02c811c-e521-4926-9f26-0c88bbd2ee6d', u'ipmi_password': u'******', |
|                        | u'ilo_password': u'******', u'ilo_username': u'Administrator'}        |
| created_at             | 2016-04-14T08:30:08+00:00                                             |
| driver_internal_info   | {<span class="bold"><strong>u'clean_steps': None</strong></span>,                      |
|                        | u'hardware_manager_version': {u'generic_hardware_manager': u'1.0'},   |
|                        | u'is_whole_disk_image': True, u'agent_erase_devices_iterations': 1,   |
|                        | u'agent_url': u'http://192.168.246.245:9999',                         |
|                        | u'agent_last_heartbeat': 1460633166}                                  |
| chassis_uuid           |                                                                       |
| instance_info          | {}                                                                    |
+------------------------+-----------------------------------------------------------------------+</pre></div><p>
   The status will be in the <code class="literal">driver_internal_info</code> field. You
   will also be able to see the <code class="literal">clean_steps</code> list there.
  </p></section><section class="sect2" id="id-1.4.5.12.9.8" data-id-title="Troubleshooting"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.7.3 </span><span class="title-name">Troubleshooting</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.9.8">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_cleaning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If an error occurs during the cleaning process, the node will enter the
   clean failed state so that it is not deployed. The node remains powered on
   for debugging purposes. The node can be moved to the manageable state to
   attempt a fix using the following command:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-provision-state &lt;node id&gt; manage</pre></div><p>
   Once you have identified and fixed the issue, you can return the node to
   available state by executing the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-maintenance &lt;node id&gt; false
ironic node-set-provision-state &lt;node id&gt; provide</pre></div><p>
   This will retry the cleaning steps and set the node to available state upon
   their successful completion.
  </p></section><section class="sect2" id="id-1.4.5.12.9.9" data-id-title="Disabling Node Cleaning"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.7.4 </span><span class="title-name">Disabling Node Cleaning</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.9.9">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-node_cleaning.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To disable node cleaning, edit
   <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
   and set <code class="literal">enable_node_cleaning</code> to <code class="literal">false</code>.
  </p><p>
   Commit your changes:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "disable node cleaning"</pre></div><p>
   Deploy these changes by re-running the configuration processor and
   reconfigure the ironic installation:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></section></section><section class="sect1" id="ironic-oneview" data-id-title="Ironic and HPE OneView"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.8 </span><span class="title-name">Ironic and HPE OneView</span></span> <a title="Permalink" class="permalink" href="#ironic-oneview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="id-1.4.5.12.10.2" data-id-title="Enabling Ironic HPE OneView driver in SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.1 </span><span class="title-name">Enabling Ironic HPE OneView driver in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Edit the file
   <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironicconfig.yml</code>
   and set the value
  </p><div class="verbatim-wrap"><pre class="screen">enable_oneview: true</pre></div><p>
   This will enable the HPE OneView driver for Ironic in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></section><section class="sect2" id="id-1.4.5.12.10.3" data-id-title="Adding HPE OneView Appliance Credentials"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.2 </span><span class="title-name">Adding HPE OneView Appliance Credentials</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">manage_url: https://&lt;Onview appliance URL&gt;

oneview_username: "&lt;Appliance username&gt;"

oneview_encrypted_password: "&lt;Encrypted password&gt;"

oneview_allow_insecure_connections: &lt;true/false&gt;

tls_cacert_file: &lt;CA certificate for connection&gt;</pre></div></section><section class="sect2" id="id-1.4.5.12.10.4" data-id-title="Encrypting the HPE OneView Password"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.3 </span><span class="title-name">Encrypting the HPE OneView Password</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Encryption can be applied using <code class="literal">ardanaencrypt.py</code> or using
   <code class="literal">openssl</code>. On the Cloud Lifecycle Manager node, export the key
   used for encryption as environment variable:
  </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY="<em class="replaceable">ENCRYPTION_KEY</em>"</pre></div><p>
   And then execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
python ardanaencrypt.py</pre></div><p>
   Enter password to be encrypted when prompted. The script uses the key that
   was exported in the <code class="literal">ARDANA_USER_PASSWORD_ENCRYPT_KEY</code> to do
   the encryption.
  </p><p>
   For more information, see <span class="intraxref">Book “Security Guide”, Chapter 9 “Encryption of Passwords and Sensitive Data”</span>.
  </p></section><section class="sect2" id="id-1.4.5.12.10.5" data-id-title="Decrypting the HPE OneView Password"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.4 </span><span class="title-name">Decrypting the HPE OneView Password</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before running the <code class="literal">site.yml</code> playbook, export the key used
   for encryption as environment variable:
  </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY="<em class="replaceable">ENCRYPTION_KEY</em>"</pre></div><p>
   The decryption of the password is then automatically handled in
   ironic-ansible playbooks.
  </p></section><section class="sect2" id="id-1.4.5.12.10.6" data-id-title="Registering Baremetal Node for HPE OneView Driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.5 </span><span class="title-name">Registering Baremetal Node for HPE OneView Driver</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-create -d agent_pxe_oneview</pre></div><p>
   Update node driver-info:
  </p><div class="verbatim-wrap"><pre class="screen"> ironic node-update $NODE_UUID add driver_info/server_hardware_uri=$SH_URI</pre></div></section><section class="sect2" id="id-1.4.5.12.10.7" data-id-title="Updating Node Properties"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.6 </span><span class="title-name">Updating Node Properties</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-update $NODE_UUID add \
  properties/capabilities=server_hardware_type_uri:$SHT_URI,\
        enclosure_group_uri:$EG_URI,server_profile_template_uri=$SPT_URI</pre></div></section><section class="sect2" id="id-1.4.5.12.10.8" data-id-title="Creating Port for Driver"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.7 </span><span class="title-name">Creating Port for Driver</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.8">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic port-create -n $NODE_UUID -a $MAC_ADDRESS</pre></div></section><section class="sect2" id="id-1.4.5.12.10.9" data-id-title="Creating a Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.8 </span><span class="title-name">Creating a Node</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.9">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Create Node:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -n ovbay7 -d agent_pxe_oneview</pre></div><p>
   Update driver info:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update $ID add driver_info/server_hardware_uri="/rest/server-hardware/3037...464B" \
driver_info/deploy_kernel="$KERNELDISK" driver_info/deploy_ramdisk="$RAMDISK"</pre></div><p>
   Update node properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update $ID add properties/local_gb=10
ironic node-update $ID add properties/cpus=24 properties/memory_mb=262144 \
properties/cpu_arch=x86_64</pre></div><div class="verbatim-wrap"><pre class="screen">ironic node-update \
$ID add properties/capabilities=server_hardware_type_uri:'/rest/server-hardware-types/B31...F69E',\
enclosure_group_uri:'/rest/enclosure-groups/80efe...b79fa',\
server_profile_template_uri:'/rest/server-profile-templates/faafc3c0-6c81-47ca-a407-f67d11262da5'</pre></div></section><section class="sect2" id="id-1.4.5.12.10.10" data-id-title="Getting Data using REST API"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.9 </span><span class="title-name">Getting Data using REST API</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.10">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   GET login session auth id:
  </p><div class="verbatim-wrap"><pre class="screen">curl -k https://<em class="replaceable">ONEVIEW_MANAGER_URL</em>/rest/login-sessions \
  -H "content-type:application/json" \
  -X POST \
  -d '{"userName":"<em class="replaceable">USER_NAME</em>", "password":"<em class="replaceable">PASSWORD</em>"}'</pre></div><p>
   Get the complete node details in JSON format:
  </p><div class="verbatim-wrap"><pre class="screen">curl -k "https://<em class="replaceable">ONEVIEW_MANAGER_URL</em>;/rest/server-hardware/30373237-3132-4753-4835-32325652464B" -H "content-type:application/json" -H "Auth:&lt;auth_session_id&gt;"| python -m json.tool</pre></div></section><section class="sect2" id="id-1.4.5.12.10.11" data-id-title="Ironic HPE OneView CLI"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.8.10 </span><span class="title-name">Ironic HPE OneView CLI</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.10.11">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <code class="literal">ironic-oneview-cli</code> is already installed in
   <code class="literal">ironicclient</code> venv with a symbolic link to it. To generate
   an <code class="literal">rc</code> file for the HPE OneView CLI, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
glance image list</pre></div></li><li class="step"><p>
     Note the <code class="literal">deploy-kernel</code> and
     <code class="literal">deploy-ramdisk</code> UUID and then run the following command
     to generate the <code class="literal">rc</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">ironic-oneview genrc</pre></div><p>
     You will be prompted to enter:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       HPE OneView Manager URL
      </p></li><li class="listitem"><p>
       Username
      </p></li><li class="listitem"><p>
       deploy-kernel
      </p></li><li class="listitem"><p>
       deploy-ramdisk
      </p></li><li class="listitem"><p>
       allow_insecure_connection
      </p></li><li class="listitem"><p>
       cacert file
      </p></li></ul></div><p>
     The <code class="literal">ironic-oneview.rc</code> file will be generated in the
     current directory, by default. It is possible to specify a different
     location.
    </p></li><li class="step"><p>
     Source the generated file:
    </p><div class="verbatim-wrap"><pre class="screen">source ironic-oneview.rc</pre></div><p>
     Now enter the password of the HPE OneView appliance.
    </p></li><li class="step"><p>
     You can now use the CLI for node and flavor creation as follows:
    </p><div class="verbatim-wrap"><pre class="screen">ironic-oneview node-create
ironic-oneview flavor-create</pre></div></li></ol></div></div></section></section><section class="sect1" id="ironic-raid-config" data-id-title="RAID Configuration for Ironic"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.9 </span><span class="title-name">RAID Configuration for Ironic</span></span> <a title="Permalink" class="permalink" href="#ironic-raid-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_raid_config.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Node Creation:
   </p><p>
    Check the raid capabilities of the driver:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version 1.15 driver-raid-logical-disk-properties pxe_ilo</pre></div><p>
    This will generate output similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">+----------------------+-------------------------------------------------------------------------+
| Property             | Description                                                             |
+----------------------+-------------------------------------------------------------------------+
| controller           | Controller to use for this logical disk. If not specified, the          |
|                      | driver will choose a suitable RAID controller on the bare metal node.   |
|                      | Optional.                                                               |
| disk_type            | The type of disk preferred. Valid values are 'hdd' and 'ssd'. If this   |
|                      | is not specified, disk type will not be a selection criterion for       |
|                      | choosing backing physical disks. Optional.                              |
| interface_type       | The interface type of disk. Valid values are 'sata', 'scsi' and 'sas'.  |
|                      | If this is not specified, interface type will not be a selection        |
|                      | criterion for choosing backing physical disks. Optional.                |
| is_root_volume       | Specifies whether this disk is a root volume. By default, this is False.|
|                      | Optional.                                                               |
| #_of_physical_disks  | Number of physical disks to use for this logical disk. By default, the  |
|                      | driver uses the minimum number of disks required for that RAID level.   |
|                      | Optional.                                                               |
| physical_disks       | The physical disks to use for this logical disk. If not specified, the  |
|                      | driver will choose suitable physical disks to use. Optional.            |
| <span class="bold"><strong>raid_level           | RAID level for the logical disk. Valid values are '0', '1', '2', '5', </strong></span>  |
|                      | <span class="bold"><strong>'6', '1+0', '5+0' and '6+0'. Required.</strong></span>                                  |
| share_physical_disks | Specifies whether other logical disks can share physical disks with this|
|                      | logical disk. By default, this is False. Optional.                      |
| <span class="bold"><strong>size_gb              | Size in GiB (Integer) for the logical disk. Use 'MAX' as size_gb if </strong></span>    |
|                      | <span class="bold"><strong>this logical disk is supposed to use the rest of</strong></span>                        |
|                      | <span class="bold"><strong>the space available. Required.</strong></span>                                          |
| volume_name          | Name of the volume to be created. If this is not specified, it will be  |
|                      | auto-generated. Optional.                                               |
+----------------------+-------------------------------------------------------------------------+</pre></div><p>
    Node State will be <span class="bold"><strong>Available</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d pxe_ilo -i ilo_address=&lt;ip_address&gt; \
  -i ilo_username=&lt;username&gt; -i ilo_password=&lt;password&gt; \
  -i ilo_deploy_iso=&lt;iso_id&gt; -i deploy_kernel=&lt;kernel_id&gt; \
  -i deploy_ramdisk=&lt;ramdisk_id&gt; -p cpus=2 -p memory_mb=4096 \
  -p local_gb=80  -p cpu_arch=amd64 \
  -p capabilities="boot_option:local,boot_mode:bios"</pre></div><div class="verbatim-wrap"><pre class="screen">ironic port-create -a &lt;port&gt; -n &lt;node-uuid&gt;</pre></div></li><li class="step"><p>
    Apply the target raid configuration on the node:
   </p><p>
    See the OpenStack documentation for RAID configuration at
    <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/raid.html" target="_blank">http://docs.openstack.org/developer/ironic/deploy/raid.html</a>.
   </p><p>
    Set the target RAID configuration by editing the file
    <code class="literal">raid_conf.json</code> and setting the appropriate values, for
    example:
   </p><div class="verbatim-wrap"><pre class="screen">{ "logical_disks": [ {"size_gb": 5, "raid_level": "0", "is_root_volume": true} ] }</pre></div><p>
    and then run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version 1.15 node-set-target-raid-config &lt;node-uuid&gt; raid_conf.json</pre></div><p>
    The output produced should be similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-------------------------------------------------------------------------+
| Property              | Value                                                                   |
+-----------------------+-------------------------------------------------------------------------+
| chassis_uuid          |                                                                         |
| clean_step            | {}                                                                      |
| console_enabled       | False                                                                   |
| created_at            | 2016-06-14T14:58:07+00:00                                               |
| driver                | pxe_ilo                                                                 |
| driver_info           | {u'ilo_deploy_iso': u'd43e589a-07db-4fce-a06e-98e2f38340b4',            |
|                       | u'deploy_kernel': u'915c5c74-1ceb-4f78-bdb4-8547a90ac9c0',              |
|                       | u'ilo_address': u'10.1.196.116', u'deploy_ramdisk':                     |
|                       | u'154e7024-bf18-4ad2-95b0-726c09ce417a', u'ilo_password': u'******',    |
|                       | u'ilo_username': u'Administrator'}                                      |
| driver_internal_info  | {u'agent_cached_clean_steps_refreshed': u'2016-06-15 07:16:08.264091',  |
|                       | u'agent_cached_clean_steps': {u'raid': [{u'interface': u'raid',         |
|                       | u'priority': 0, u'step': u'delete_configuration'}, {u'interface':       |
|                       | u'raid', u'priority': 0, u'step': u'create_configuration'}], u'deploy': |
|                       | [{u'priority': 10, u'interface': u'deploy', u'reboot_requested': False, |
|                       | u'abortable': True, u'step': u'erase_devices'}]}, u'clean_steps': None, |
|                       | u'hardware_manager_version': {u'generic_hardware_manager': u'3'},       |
|                       | u'agent_erase_devices_iterations': 1, u'agent_url':                     |
|                       | u'http://192.168.245.143:9999', u'agent_last_heartbeat': 1465974974}    |
| extra                 | {}                                                                      |
| inspection_finished_at| None                                                                    |
| inspection_started_at | None                                                                    |
| instance_info         | {u'deploy_key': u'XXN2ON0V9ER429MECETJMUG5YHTKOQOZ'}                    |
| instance_uuid         | None                                                                    |
| last_error            | None                                                                    |
| maintenance           | False                                                                   |
| maintenance_reason    | None                                                                    |
| name                  | None                                                                    |
| power_state           | power off                                                               |
| properties            | {u'cpu_arch': u'amd64', u'root_device': {u'wwn': u'0x600508b1001ce286'},|
|                       | u'cpus': 2, u'capabilities':                                            |
|                       | u'boot_mode:bios,raid_level:6,boot_option:local', u'memory_mb': 4096,   |
|                       | u'local_gb': 4}                                                         |
| provision_state       | available                                                               |
| provision_updated_at  | 2016-06-15T07:16:27+00:00                                               |
| reservation           | padawan-ironic-cp1-c1-m2-mgmt                                           |
| target_power_state    | power off                                                               |
| target_provision_state| None                                                                    |
| <span class="bold"><strong>target_raid_config</strong></span>    | {u'logical_disks': [{u'size_gb': 5, <span class="bold"><strong>u'raid_level': u'6',</strong></span>                |
|                       | u'is_root_volume': True}]}                                              |
| updated_at            | 2016-06-15T07:44:22+00:00                                               |
| uuid                  | 22ab9f85-71a1-4748-8d6b-f6411558127e                                    |
+-----------------------+-------------------------------------------------------------------------+</pre></div><p>
    Now set the state of the node to
    <span class="bold"><strong>manageable</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state &lt;node-uuid&gt; manage</pre></div></li><li class="step"><p>
    Manual cleaning steps:
   </p><p>
    Manual cleaning is enabled by default in production - the following are the
    steps to enable cleaning if the manual cleaning has been disabled.
   </p><ol type="a" class="substeps"><li class="step"><p>
      Provide <code class="literal">cleaning_network_uuid</code> in
      <code class="literal">ironic-conductor.conf</code>
     </p></li><li class="step"><p>
      Edit the file
      <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
      and set <code class="literal">enable_node_cleaning</code> to
      <code class="literal">true</code>.
     </p></li><li class="step"><p>
      Then run the following set of commands:
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "enabling node cleaning"
cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div><p>
      After performing these steps, the state of the node will become
      <span class="bold"><strong>Cleaning</strong></span>.
     </p></li></ol><p>
    Run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state 2123254e-8b31...aa6fd \
  clean --clean-steps '[{ "interface": "raid","step": "delete_configuration"}, \
  { "interface": "raid" ,"step": "create_configuration"}]'</pre></div><p>
    Node-information after a Manual cleaning:
   </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-------------------------------------------------------------------------+
| Property              | Value                                                                   |
+-----------------------+-------------------------------------------------------------------------+
| chassis_uuid          |                                                                         |
| clean_step            | {}                                                                      |
| console_enabled       | False                                                                   |
| created_at            | 2016-06-14T14:58:07+00:00                                               |
| driver                | pxe_ilo                                                                 |
| driver_info           | {u'ilo_deploy_iso': u'd43e589a-07db-4fce-a06e-98e2f38340b4',            |
|                       | u'deploy_kernel': u'915c5c74-1ceb-4f78-bdb4-8547a90ac9c0',              |
|                       | u'ilo_address': u'10.1.196.116', u'deploy_ramdisk':                     |
|                       | u'154e7024-bf18-4ad2-95b0-726c09ce417a', u'ilo_password': u'******',    |
|                       | u'ilo_username': u'Administrator'}                                      |
| driver_internal_info  | {u'agent_cached_clean_steps_refreshed': u'2016-06-15 07:16:08.264091',  |
|                       | u'agent_cached_clean_steps': {u'raid': [{u'interface': u'raid',         |
|                       | u'priority': 0, u'step': u'delete_configuration'}, {u'interface':       |
|                       | u'raid', u'priority': 0, u'step': u'create_configuration'}], u'deploy': |
|                       | [{u'priority': 10, u'interface': u'deploy', u'reboot_requested': False, |
|                       | u'abortable': True, u'step': u'erase_devices'}]}, u'clean_steps': None, |
|                       | u'hardware_manager_version': {u'generic_hardware_manager': u'3'},       |
|                       | u'agent_erase_devices_iterations': 1, u'agent_url':                     |
|                       | u'http://192.168.245.143:9999', u'agent_last_heartbeat': 1465974974}    |
| extra                 | {}                                                                      |
| inspection_finished_at| None                                                                    |
| inspection_started_at | None                                                                    |
| instance_info         | {u'deploy_key': u'XXN2ON0V9ER429MECETJMUG5YHTKOQOZ'}                    |
| instance_uuid         | None                                                                    |
| last_error            | None                                                                    |
| maintenance           | False                                                                   |
| maintenance_reason    | None                                                                    |
| name                  | None                                                                    |
| power_state           | power off                                                               |
| properties            | {u'cpu_arch': u'amd64', u'root_device': {u'wwn': u'0x600508b1001ce286'},|
|                       | u'cpus': 2, u'capabilities':                                            |
|                       | u'boot_mode:bios,raid_level:6,boot_option:local', u'memory_mb': 4096,   |
|                       | u'local_gb': 4}                                                         |
| provision_state       | manageable                                                              |
| provision_updated_at  | 2016-06-15T07:16:27+00:00                                               |
| raid_config           | {u'last_updated': u'2016-06-15 07:16:14.584014', u'physical_disks':     |
|                       | [{u'status': u'ready', u'size_gb': 1024, u'interface_type': u'sata',    |
|                       | u'firmware': u'HPGC', u'controller': u'Smart Array P440ar in Slot 0     |
|                       | (Embedded)', u'model': u'ATA     MM1000GBKAL', u'disk_type': u'hdd',    |
|                       | u'id': u'1I:3:3'}, {u'status': u'ready', u'size_gb': 1024,              |
|                       | u'interface_type': u'sata', u'firmware': u'HPGC', u'controller': u'Smart|
|                       | Array P440ar in Slot 0 (Embedded)', u'model': u'ATA     MM1000GBKAL',   |
|                       | u'disk_type': u'hdd', u'id': u'1I:3:1'}, {u'status': u'active',         |
|                       | u'size_gb': 1024, u'interface_type': u'sata', u'firmware': u'HPGC',     |
|                       | u'controller': u'Smart Array P440ar in Slot 0 (Embedded)', u'model':    |
|                       | u'ATA     MM1000GBKAL', u'disk_type': u'hdd', u'id': u'1I:3:2'},        |
|                       | {u'status': u'active', u'size_gb': 1024, u'interface_type': u'sata',    |
|                       | u'firmware': u'HPGC', u'controller': u'Smart Array P440ar in Slot 0     |
|                       | (Embedded)', u'model': u'ATA     MM1000GBKAL', u'disk_type': u'hdd',    |
|                       | u'id': u'2I:3:6'}, {u'status': u'active', u'size_gb': 1024,             |
|                       | u'interface_type': u'sata', u'firmware': u'HPGC', u'controller': u'Smart|
|                       | Array P440ar in Slot 0 (Embedded)', u'model': u'ATA     MM1000GBKAL',   |
|                       | u'disk_type': u'hdd', u'id': u'2I:3:5'}, {u'status': u'active',         |
|                       | u'size_gb': 1024, u'interface_type': u'sata', u'firmware': u'HPGC',     |
|                       | u'controller': u'Smart Array P440ar in Slot 0 (Embedded)', u'model':    |
|                       | u'ATA     MM1000GBKAL', u'disk_type': u'hdd', u'id': u'1I:3:4'}],       |
|                       | u'logical_disks': [{u'size_gb': 4, u'physical_disks': [u'1I:3:2',       |
|                       | u'2I:3:6', u'2I:3:5', u'1I:3:4'], u'raid_level': u'6',                  |
|                       | u'is_root_volume': True, u'root_device_hint': {u'wwn':                  |
|                       | u'0x600508b1001ce286'}, u'controller': u'Smart Array P440ar in Slot 0   |
|                       | (Embedded)', u'volume_name': u'015E795CPDNLH0BRH8N406AAB7'}]}           |
| reservation           | padawan-ironic-cp1-c1-m2-mgmt                                           |
| target_power_state    | power off                                                               |
| target_provision_state| None                                                                    |
| target_raid_config    | {u'logical_disks': [{u'size_gb': 5, u'raid_level': u'6',                |
|                       | u'is_root_volume': True}]}                                              |
| updated_at            | 2016-06-15T07:44:22+00:00                                               |
| uuid                  | 22ab9f85-71a1-4748-8d6b-f6411558127e                                    |
+-----------------------+-------------------------------------------------------------------------+</pre></div><p>
    After the manual cleaning, run the following command to change the state of
    a node to <span class="bold"><strong>available</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state &lt;node-uuid&gt; \
  provide</pre></div></li></ol></div></div></section><section class="sect1" id="ironic-audit-support" data-id-title="Audit Support for Ironic"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.10 </span><span class="title-name">Audit Support for Ironic</span></span> <a title="Permalink" class="permalink" href="#ironic-audit-support">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="id-1.4.5.12.12.2" data-id-title="API Audit Logging"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.10.1 </span><span class="title-name">API Audit Logging</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.12.2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Audit middleware supports delivery of CADF audit events via Oslo messaging
   notifier capability. Based on <code class="literal">notification_driver</code>
   configuration, audit events can be routed to messaging infrastructure
   (<code class="literal">notification_driver = messagingv2</code>) or can be routed to a
   log file (<code class="literal">notification_driver = log</code>).
  </p><p>
   Audit middleware creates two events per REST API interaction. The first
   event has information extracted from request data and the second one
   contains information on the request outcome (response).
  </p></section><section class="sect2" id="id-1.4.5.12.12.3" data-id-title="Enabling API Audit Logging"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.10.2 </span><span class="title-name">Enabling API Audit Logging</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.12.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can enable audit logging for Ironic by changing the configuration in the
   input model. Edit the file
   <code class="literal">~/openstack/my_cloud/definition/cloudConfig.yml</code> and in the
   <code class="literal">audit-settings</code> section, change the
   <code class="literal">default</code> value to <code class="literal">enabled</code>. The
   ironic-ansible playbooks will now enable audit support for Ironic.
  </p><p>
   API audit events will be logged in the corresponding audit directory, for
   example, <code class="literal">/var/audit/ironic/ironic-api-audit.log</code>. An audit
   event will be logged in the log file for every request and response for an
   API call.
  </p></section><section class="sect2" id="id-1.4.5.12.12.4" data-id-title="Sample Audit Event"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.10.3 </span><span class="title-name">Sample Audit Event</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.12.12.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following output is an example of an audit event for an <code class="literal">ironic
   node-list</code> command:
  </p><div class="verbatim-wrap"><pre class="screen">{
   "event_type":"audit.http.request",
   "timestamp":"2016-06-15 06:04:30.904397",
   "payload":{
      "typeURI":"http://schemas.dmtf.org/cloud/audit/1.0/event",
      "eventTime":"2016-06-15T06:04:30.903071+0000",
      "target":{
         "id":"ironic",
         "typeURI":"unknown",
         "addresses":[
            {
               "url":"http://{ironic_admin_host}:6385",
               "name":"admin"
            },
           {
               "url":"http://{ironic_internal_host}:6385",
               "name":"private"
           },
           {
               "url":"http://{ironic_public_host}:6385",
               "name":"public"
           }
         ],
         "name":"ironic"
      },
      "observer":{
         "id":"target"
      },
      "tags":[
         "correlation_id?value=685f1abb-620e-5d5d-b74a-b4135fb32373"
      ],
      "eventType":"activity",
      "initiator":{
         "typeURI":"service/security/account/user",
         "name":"admin",
         "credential":{
            "token":"***",
            "identity_status":"Confirmed"
         },
         "host":{
            "agent":"python-ironicclient",
            "address":"10.1.200.129"
         },
         "project_id":"d8f52dd7d9e1475dbbf3ba47a4a83313",
         "id":"8c1a948bad3948929aa5d5b50627a174"
      },
      "action":"read",
      "outcome":"pending",
      "id":"061b7aa7-5879-5225-a331-c002cf23cb6c",
      "requestPath":"/v1/nodes/?associated=True"
   },
   "priority":"INFO",
   "publisher_id":"ironic-api",
   "message_id":"2f61ebaa-2d3e-4023-afba-f9fca6f21fc2"
}</pre></div></section></section></section><section class="chapter" id="install-swift" data-id-title="Installation for SUSE OpenStack Cloud Entry-scale Cloud with Swift Only"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">18 </span><span class="title-name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></span> <a title="Permalink" class="permalink" href="#install-swift">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with Swift Only model.
 </p><section class="sect1" id="sec-swift-important-notes" data-id-title="Important Notes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.1 </span><span class="title-name">Important Notes</span></span> <a title="Permalink" class="permalink" href="#sec-swift-important-notes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 1. Overview">Chapter 1, <em>Overview</em></a>.
    </p></li><li class="listitem"><p>
     Review the <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span> that we have listed.
    </p></li><li class="listitem"><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem"><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem"><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting-installation" title="Chapter 23. Troubleshooting the Installation">Chapter 23, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem"><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For Swift, refer to <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.)
    </p></li><li class="listitem"><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”</span>.
    </p></li><li class="listitem"><p>
     The terms deployer and Cloud Lifecycle Manager are used interchangeably. They refer to the
     same nodes in your cloud environment.
    </p></li><li class="listitem"><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem"><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem"><p>
     When you attach a Cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li><li class="listitem"><p>
     The installation process will create several <span class="productname">OpenStack</span> roles. Not all roles
     will be relevant for a cloud with Swift only, but they will not cause
     problems.
    </p></li></ul></div></section><section class="sect1" id="sec-swift-prereqs" data-id-title="Before You Start"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.2 </span><span class="title-name">Before You Start</span></span> <a title="Permalink" class="permalink" href="#sec-swift-prereqs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step"><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step"><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="3.4. Creating a User">Section 3.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable">CLOUD</em> with your user name
       choice.
      </p></li><li class="step"><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step"><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li><li class="step"><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></section><section class="sect1" id="sec-swift-setup-deployer" data-id-title="Setting Up the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.3 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#sec-swift-setup-deployer">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="id-1.4.5.13.5.2" data-id-title="Installing the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.1 </span><span class="title-name">Installing the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.13.5.2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the product from:
     </p><ol type="a" class="substeps"><li class="step"><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step"><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step"><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step"><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step"><p>
      Select the location.
     </p></li><li class="step"><p>
      Select the keyboard layout.
     </p></li><li class="step"><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step"><p>
      Create new account:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Enter a username.
       </p></li><li class="step"><p>
        Enter a password.
       </p></li><li class="step"><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step"><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.4.5.13.5.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></section></section><section class="sect1" id="id-1.4.5.13.6" data-id-title="Configure Your Environment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.4 </span><span class="title-name">Configure Your Environment</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.13.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This part of the install is going to depend on the specific cloud
   configuration you are going to use.
  </p><p>
     Setup your configuration files, as follows:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       See the sample sets of configuration files in the
       <code class="literal">~/openstack/examples/</code> directory. Each set will have an
       accompanying README.md file that explains the contents of each of the
       configuration files.
      </p></li><li class="step"><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/entry-scale-swift/* \
  ~/openstack/my_cloud/definition/</pre></div></li><li class="step"><p>
       Begin inputting your environment information into the configuration
       files in the <code class="literal">~/openstack/my_cloud/definition</code>
       directory.
      </p><p>
       Full details of how to do this can be found here:
       <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.10 “Understanding Swift Ring Specifications”, Section 11.10.1 “Ring Specifications in the Input Model”</span>.
      </p><p>
       In many cases, the example models provide most of the data you need to
       create a valid input model. However, there are two important aspects you
       must plan and configure before starting a deploy as follows:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Check the disk model used by your nodes. Specifically, check that all
         disk drives are correctly named and used as described in
         <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.
        </p></li><li class="listitem"><p>
         Select an appropriate partition power for your rings. Detailed
         information about this is provided at
         <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.10 “Understanding Swift Ring Specifications”</span>.
        </p></li></ul></div></li></ol></div></div><p>
     Optionally, you can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible</pre></div></li><li class="step"><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step"><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen">ardanaencrypt.py</pre></div></li><li class="step"><p>
       Take the string generated and place it in the
       <code class="literal">"ilo_password"</code> field in your
       <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file,
       remembering to enclose it in quotes.
      </p></li><li class="step"><p>
       Repeat the above for each server.
      </p></li></ol></div></div><div id="id-1.4.5.13.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable: <code class="literal">export
      ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
     </p></div><p>
   Commit your configuration to the local git repo
   (<a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>), as follows:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div><div id="id-1.4.5.13.6.10" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    This step needs to be repeated any time you make changes to your
    configuration files before you move onto the following steps. See
    <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a> for more information.
   </p></div></section><section class="sect1" id="sec-swift-provision" data-id-title="Provisioning Your Baremetal Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.5 </span><span class="title-name">Provisioning Your Baremetal Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-swift-provision">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><section class="sect2" id="id-1.4.5.13.7.3" data-id-title="Using Third Party Baremetal Installers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.5.1 </span><span class="title-name">Using Third Party Baremetal Installers</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.13.7.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem"><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem"><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem"><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem"><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem"><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle">Running the Configuration Processor</em>.
   </p></section><section class="sect2" id="id-1.4.5.13.7.4" data-id-title="Using the Automated Operating System Installation Provided by SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.5.2 </span><span class="title-name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.13.7.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><section class="sect3" id="id-1.4.5.13.7.4.3" data-id-title="Deploying Cobbler"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">18.5.2.1 </span><span class="title-name">Deploying Cobbler</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.13.7.4.3">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you cannot reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div id="id-1.4.5.13.7.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step"><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></section><section class="sect3" id="id-1.4.5.13.7.4.4" data-id-title="Imaging the Nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">18.5.2.2 </span><span class="title-name">Imaging the Nodes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.13.7.4.4">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Powers down the nodes to be installed
      </p></li><li class="step"><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step"><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step"><p>
       Waits for the nodes to power themselves down (this indicates a
       successful install). This can take some time.
      </p></li><li class="step"><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step"><p>
       Waits for the nodes to be reachable by SSH and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     Deploying nodes has been automated in the Cloud Lifecycle Manager and requires the
     following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       All of your nodes using SLES must already be installed, either
       manually or via Cobbler.
      </p></li><li class="listitem"><p>
       Your input model should be configured for your SLES nodes, according
       to the instructions at <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
      </p></li><li class="listitem"><p>
       You should have run the configuration processor and the
       <code class="filename">ready-deployment.yml</code> playbook.
      </p></li></ul></div><p>
     Execute the following steps to re-image one or more nodes after you have
     run the <code class="filename">ready-deployment.yml</code> playbook.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Run the following playbook, specifying your SLES nodes using the
       nodelist. This playbook will reconfigure Cobbler for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e \
      nodelist=node1[,node2,node3]</pre></div></li><li class="step"><p>
       Re-image the node(s) with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
      -e nodelist=node1[,node2,node3]</pre></div></li></ol></div></div><p>
     If a nodelist is not specified then the set of nodes in Cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></section></section></section><section class="sect1" id="sec-swift-config-processor" data-id-title="Running the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.6 </span><span class="title-name">Running the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="#sec-swift-config-processor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="23.2. Issues while Updating Configuration Files">Section 23.2, “Issues while Updating Configuration Files”</a>.
  </p></section><section class="sect1" id="sec-swift-deploy" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.7 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#sec-swift-deploy">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step"><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.4.5.13.9.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step"><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="23.3. Issues while Deploying the Cloud">Section 23.3, “Issues while Deploying the Cloud”</a>.
  </p><div id="id-1.4.5.13.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      HPE Smart Storage Administrator (HPE SSA) CLI component will have to be
      installed on all control nodes that are Swift nodes, in order to generate
      the following Swift metrics:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        swiftlm.hp_hardware.hpssacli.smart_array
       </p></li><li class="listitem"><p>
        swiftlm.hp_hardware.hpssacli.logical_drive
       </p></li><li class="listitem"><p>
        swiftlm.hp_hardware.hpssacli.smart_array.firmware
       </p></li><li class="listitem"><p>
        swiftlm.hp_hardware.hpssacli.physical_drive
       </p></li></ul></div></li><li class="listitem"><p>
      HPE-specific binaries that are not based on open source are distributed
      directly from and supported by HPE. To download and install the SSACLI
      utility to enable management of disk controllers, please refer to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
     </p></li><li class="listitem"><p>
      After the HPE SSA CLI component is installed on the Swift nodes, the
      metrics will be generated automatically during the next agent polling
      cycle. Manual reboot of the node is not required.
     </p></li></ul></div></div></section><section class="sect1" id="sec-swift-post-installation" data-id-title="Post-Installation Verification and Administration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.8 </span><span class="title-name">Post-Installation Verification and Administration</span></span> <a title="Permalink" class="permalink" href="#sec-swift-post-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installing_swift_object_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud-verification" title="Chapter 26. Cloud Verification">Chapter 26, <em>Cloud Verification</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall-checklist" title="Chapter 32. Other Common Post-Installation Tasks">Chapter 32, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></section></section><section class="chapter" id="install-sles-compute" data-id-title="Installing SLES Compute"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">19 </span><span class="title-name">Installing SLES Compute</span></span> <a title="Permalink" class="permalink" href="#install-sles-compute">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-install_sles_compute.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="sles-overview" data-id-title="SLES Compute Node Installation Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.1 </span><span class="title-name">SLES Compute Node Installation Overview</span></span> <a title="Permalink" class="permalink" href="#sles-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-sles_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports SLES compute nodes, specifically SUSE Linux Enterprise Server 12 SP3. <span class="phrase"><span class="phrase">SUSE</span></span>
  does not ship a SLES ISO with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> so you will need to download a copy of
  the SLES ISO (<code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code>)
  and the SLES SDK ISO
  (<code class="filename">SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso</code>) from SUSE. You
  can use the following
  link to download the ISO. To do so, either log in or create a SUSE
  account before downloading:
  <a class="link" href="https://www.suse.com/products/server/download/" target="_blank">https://www.suse.com/products/server/download/</a>.
 </p><p>
  There are two approaches for deploying SLES compute nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Using the Cloud Lifecycle Manager to automatically deploy SLES Compute Nodes.
   </p></li><li class="listitem"><p>
    Provisioning SLES nodes yourself, either manually or using a third-party
    tool, and then providing the relevant information to the Cloud Lifecycle Manager.
   </p></li></ul></div><p>
  These two approaches can be used whether you are installing a cloud for the
  first time or adding a compute node to an existing cloud. Regardless of your
  approach, you should be certain to register your SLES compute nodes in order
  to get product updates as they come available. For more information, see
  <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 1 “Registering SLES”</span>.
 </p></section><section class="sect1" id="sles-support" data-id-title="SLES Support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.2 </span><span class="title-name">SLES Support</span></span> <a title="Permalink" class="permalink" href="#sles-support">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-sles_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  SUSE Linux Enterprise Server (SLES) Host OS KVM and/or supported SLES guests
  have been tested and qualified by <span class="phrase"><span class="phrase">SUSE</span></span> to run on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  
 </p></section><section class="sect1" id="install-sles" data-id-title="Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.3 </span><span class="title-name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span></span> <a title="Permalink" class="permalink" href="#install-sles">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-install_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The method used for deploying SLES compute nodes using Cobbler on the
  Cloud Lifecycle Manager uses legacy BIOS.
 </p><div id="id-1.4.5.14.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   UEFI and Secure boot are not supported on SLES Compute.
  </p></div><section class="sect2" id="id-1.4.5.14.4.4" data-id-title="Deploying legacy BIOS SLES Compute nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.3.1 </span><span class="title-name">Deploying legacy BIOS SLES Compute nodes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.4.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-install_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The installation process for legacy BIOS SLES Compute nodes is similar to
   that described in <a class="xref" href="#install-kvm" title="Chapter 12. Installing Mid-scale and Entry-scale KVM">Chapter 12, <em>Installing Mid-scale and Entry-scale KVM</em></a> with some additional requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The standard SLES ISO (SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso) must be
     accessible as <code class="literal">~/sles12sp3.iso</code>. Rename the ISO or
     create a symbolic link:
    </p><div class="verbatim-wrap"><pre class="screen">mv SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso ~/sles12sp3.iso</pre></div></li><li class="listitem"><p>
     You must identify the node(s) on which you want to install SLES, by
     adding the key/value pair <code class="literal">distro-id: sles12sp3-x86_64</code>
     to server details in <code class="literal">servers.yml</code>. If there are any
     network interface or disk layout differences in the new server compared to
     the servers already in the model, you may also need to update
     <code class="literal">net_interfaces.yml</code>,
     <code class="literal">server_roles.yml</code>, <code class="literal">disk_compute.yml</code>
     and <code class="literal">control_plane.yml</code>. For more information on
     configuration of the Input Model for SLES, see <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
    </p></li><li class="listitem"><p>
     Run the playbook <code class="filename">config-processor-run.yml</code> to check
     for errors in the updated model.
    </p></li><li class="listitem"><p>
     Run the <code class="filename">ready-deployment.yml</code> playbook to build the
     new <code class="filename">scratch</code> directory.
    </p></li><li class="listitem"><p>
     Record the management network IP address that is used for the new
     server. It will be used in the installation process.
    </p></li></ul></div></section><section class="sect2" id="sles-uefi-overview" data-id-title="Deploying UEFI SLES compute nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.3.2 </span><span class="title-name">Deploying UEFI SLES compute nodes</span></span> <a title="Permalink" class="permalink" href="#sles-uefi-overview">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-install_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Deploying UEFI nodes has been automated in the Cloud Lifecycle Manager and requires the
   following to be met:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     All of your nodes using SLES must already be installed, either manually
     or via Cobbler.
    </p></li><li class="listitem"><p>
     Your input model should be configured for your SLES nodes, per the
     instructions at <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
    </p></li><li class="listitem"><p>
     You should have run the configuration processor and the
     <code class="filename">ready-deployment.yml</code> playbook.
    </p></li></ul></div><p>
   Execute the following steps to re-image one or more nodes after you have run
   the <code class="filename">ready-deployment.yml</code> playbook.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Run the following playbook, ensuring that you specify only your UEFI
     SLES nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step"><p>
     Re-image the node(s), ensuring that you only specify your UEFI SLES
     nodes using the nodelist.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
-e nodelist=node1[,node2,node3]</pre></div></li><li class="step"><p>
     Back up the <code class="filename">grub.cfg-*</code> files in
     <code class="filename">/srv/tftpboot/</code> as they will be overwritten when
     running the cobbler-deploy playbook on the next step. You will need these
     files if you need to reimage the nodes in the future.
    </p></li><li class="step"><p>
     Run the <code class="filename">cobbler-deploy.yml</code> playbook, which will reset
     Cobbler back to the default values:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div><section class="sect3" id="id-1.4.5.14.4.5.6" data-id-title="UEFI Secure Boot"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">19.3.2.1 </span><span class="title-name">UEFI Secure Boot</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.4.5.6">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-install_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Secure Boot is a method used to restrict binary execution for booting the
    system. With this option enabled, system BIOS will only allow boot loaders
    with trusted cryptographic signatures to be executed, thus preventing
    malware from hiding embedded code in the boot chain. Each boot loader
    launched during the boot process is digitally signed and that signature is
    validated against a set of trusted certificates embedded in the UEFI BIOS.
    Secure Boot is completely implemented in the BIOS and does not require
    special hardware.
   </p><p>Thus Secure Boot is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Intended to prevent boot-sector malware or kernel code injection.
     </p></li><li class="listitem"><p>Hardware-based code signing.</p></li><li class="listitem"><p>Extension of the UEFI BIOS architecture.</p></li><li class="listitem"><p>
      Optional with the ability to enable or disable it through the BIOS.
     </p></li></ul></div><p>
    In Boot Options of RBSU, <span class="guimenu">Boot Mode</span> needs to be set to
    <code class="literal">UEFI Mode</code> and <span class="guimenu">UEFI Optimized Boot</span>
    should be <code class="literal">Enabled</code>&gt;.
   </p><p>
    Secure Boot is enabled at <span class="guimenu">System
    Configuration</span> › <span class="guimenu">BIOS/Platform Configuration (RBSU)</span> › <span class="guimenu">Server
         Security</span> › <span class="guimenu">Secure Boot Configuration</span> › <span class="guimenu">Secure Boot Enforcement</span>.
   </p></section></section></section><section class="sect1" id="provisioning-sles" data-id-title="Provisioning SLES Yourself"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.4 </span><span class="title-name">Provisioning SLES Yourself</span></span> <a title="Permalink" class="permalink" href="#provisioning-sles">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section outlines the steps needed to manually provision a SLES node so
  that it can be added to a new or existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> cloud.
 </p><section class="sect2" id="id-1.4.5.14.5.3" data-id-title="Configure Cloud Lifecycle Manager to Enable SLES"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.1 </span><span class="title-name">Configure Cloud Lifecycle Manager to Enable SLES</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.5.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Take note of the IP address of the Cloud Lifecycle Manager node. It will be used below
     during <a class="xref" href="#sec-provisioning-sles-add-zypper" title="19.4.6. Add zypper repository">Section 19.4.6, “Add zypper repository”</a>.
    </p></li><li class="step"><p>
     Mount or copy the contents of
     <code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code> to
     <code class="literal">/srv/www/suse-12.3/x86_64/repos/ardana/sles12/zypper/OS/</code>
    </p></li></ol></div></div><div id="id-1.4.5.14.5.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    If you choose to mount an ISO, we recommend creating an <code class="filename">/etc/fstab</code> entry to
    ensure the ISO is mounted after a reboot.
   </p></div></section><section class="sect2" id="id-1.4.5.14.5.4" data-id-title="Install SUSE Linux Enterprise Server 12 SP3"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.2 </span><span class="title-name">Install SUSE Linux Enterprise Server 12 SP3</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.5.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Install SUSE Linux Enterprise Server 12 SP3 using the standard iso
   (<code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code>)
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Boot the SUSE Linux Enterprise Server 12 SP3 ISO.
    </p></li><li class="step"><p>
     Agree to the license
    </p></li><li class="step"><p>
     Edit the network settings, enter the the management network IP address
     recorded earlier. It is not necessary to enter a
     <span class="guimenu">Hostname</span>/. For product registration to work correctly,
     you must provide a DNS server. Enter the <span class="guimenu">Name Server</span> IP
     address and the <span class="guimenu">Default IPv4 Gateway</span>.
    </p></li><li class="step"><p>
     Additional <code class="literal">System Probing</code> will occur.
    </p></li><li class="step"><p>
     On the <code class="literal">Registration</code> page, you can skip registration if
     the database server does not have a external interface or if there is no
     SMT server on the MGMT LAN.
    </p></li><li class="step"><p>
     No <code class="literal">Add On Products</code> are needed.
    </p></li><li class="step"><p>
     For <code class="literal">System Role</code>, select <span class="guimenu">Default
     System</span>. Do not select <span class="guimenu">KVM Virtualization
     Host</span>.
    </p></li><li class="step"><p>
     Partitioning
    </p><ol type="a" class="substeps"><li class="step"><p>
       Select <span class="guimenu">Expert Partitioner</span> and <span class="guimenu">Rescan
       Devices</span> to clear <span class="guimenu">Proposed Settings</span>.
      </p></li><li class="step"><p>
       Delete all <code class="literal">Volume Groups</code>.
      </p></li><li class="step"><p>
       Under the root of the directory tree, delete
       <code class="literal">/dev/sda</code>.
      </p></li><li class="step"><p>
       Delete any other partitions on any other drives.
      </p></li><li class="step"><p>
       <span class="guimenu">Add Partition</span> under <code class="literal">sda</code>, called
       <code class="literal">ardana</code>, with a <span class="guimenu">Custom Size</span>
       of 250MB.
      </p></li><li class="step"><p>
       Add an <span class="guimenu">EFI Boot Partition</span>. Partition should be
       formatted as <code class="literal">FAT</code> and mounted at
       <span class="guimenu">/boot/efi</span>.
      </p></li><li class="step"><p>
       <span class="guimenu">Add Partition</span> with all the remaining space
       (<span class="guimenu">Maximum Size</span>). The role for this partition is
       <span class="guimenu">Raw Volume (unformatted)</span>. It should not be
       mounted. It should not be formatted.
      </p></li><li class="step"><p>
       Select <span class="guimenu">Volume Management</span> and add a volume group to <code class="literal">/dev/sda2</code>
       called <code class="literal">ardana-vg</code>.
      </p></li><li class="step"><p>
       Add an LV to <code class="literal">ardana-vg</code> called <code class="literal">root</code>,
       <code class="literal">Type</code> of <span class="guimenu">Normal Volume</span>,
       <span class="guimenu">Custom Size</span> of 50GB, <span class="guimenu">Raw Volume
       (unformatted)</span>. Format as <span class="guimenu">Ext4 File System</span>
       and mount at <code class="literal">/</code>.
      </p></li><li class="step"><p>
       Acknowledge the warning about having no swap partition.
      </p></li><li class="step"><p>
       Press <span class="guimenu">Next</span> on the <code class="literal">Suggested
       Partitioning</code> page.
      </p></li></ol></li><li class="step"><p>
     Pick your <span class="guimenu">Time Zone</span> and check <code class="literal">Hardware Clock
     Set to UTC</code>.
    </p></li><li class="step"><p>
     Create a user named <code class="literal">ardana</code> and a password for
     <code class="literal">system administrator</code>. Do not check <span class="guimenu">Automatic
     Login</span>.
    </p></li><li class="step"><p>
     On the <code class="literal">Installation Settings</code> page:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Disable firewall
      </p></li><li class="listitem"><p>
       Enable SSH service
      </p></li><li class="listitem"><p>
       Set <code class="literal">text</code> as the <span class="guimenu">Default systemd
       target</span>.
      </p></li></ul></div></li><li class="step"><p>
     Press <span class="guimenu">Install</span> and <code class="literal">Confirm
     Installation</code> with the <span class="guimenu">Install</span> button.
    </p></li><li class="step"><p>
     Installation will begin and the system will reboot automatically when
     installation is complete.
    </p></li><li class="step"><p>
     When the system is booted, log in as <code class="literal">root</code>, using the
     system administrator set during installation.
    </p></li><li class="step"><p>
     Set up the <code class="literal">ardana</code> user and add
     <code class="literal">ardana</code> to the <code class="literal">sudoers</code> group.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>useradd -s /bin/bash -d /var/lib/ardana -m
    ardana
<code class="prompt user">root # </code>passwd ardana</pre></div><p>
     Enter and retype the password for user <code class="literal">ardana</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>echo "ardana ALL=(ALL) NOPASSWD:ALL" | sudo tee -a \
    /etc/sudoers.d/ardana</pre></div></li><li class="step"><p>
     Add an ardana group (id 1000) and change group owner to
     <code class="literal">ardana</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>groupadd --gid 1000 ardana
<code class="prompt user">root # </code>chown -R ardana:ardana /var/lib/ardana</pre></div></li><li class="step"><p>
     Disconnect the installation ISO. List repositories and remove the repository that was used
     for the installation.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper lr</pre></div><p>
     Identify the <code class="literal">Name</code> of the repository to remove.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper rr <em class="replaceable">REPOSITORY_NAME</em></pre></div></li><li class="step"><p>
     Copy the SSH key from the Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ssh-copy-id ardana@<em class="replaceable">DEPLOYER_IP_ADDRESS</em></pre></div></li><li class="step"><p>
     Log in to the SLES via SSH.
    </p></li><li class="step"><p>
     Continue with the <code class="filename">site.yml</code> playbook to scale out
     the node.
    </p></li></ol></div></div></section><section class="sect2" id="id-1.4.5.14.5.5" data-id-title="Assign a static IP"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.3 </span><span class="title-name">Assign a static IP</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.5.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Use the <code class="literal">ip addr</code> command to find out what network
     devices are on your system:
    </p><div class="verbatim-wrap"><pre class="screen">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: <span class="bold"><strong>eno1</strong></span>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether <span class="bold"><strong>f0:92:1c:05:89:70</strong></span> brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff</pre></div></li><li class="step"><p>
     Identify the one that matches the MAC address of your server and edit the
     corresponding config file in
     <code class="literal">/etc/sysconfig/network-scripts</code>.
    </p><div class="verbatim-wrap"><pre class="screen">vi /etc/sysconfig/network-scripts/<span class="bold"><strong>ifcfg-eno1</strong></span></pre></div></li><li class="step"><p>
     Edit the <code class="literal">IPADDR</code> and <code class="literal">NETMASK</code> values
     to match your environment. Note that the <code class="literal">IPADDR</code> is used
     in the corresponding stanza in <code class="literal">servers.yml</code>. You may
     also need to set <code class="literal">BOOTPROTO</code> to <code class="literal">none</code>.
    </p><div class="verbatim-wrap"><pre class="screen">TYPE=Ethernet
<span class="bold"><strong>BOOTPROTO=none</strong></span>
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=36060f7a-12da-469b-a1da-ee730a3b1d7c
DEVICE=eno1
ONBOOT=yes
<span class="bold"><strong>NETMASK=255.255.255.192</strong></span>
<span class="bold"><strong>IPADDR=10.13.111.14</strong></span></pre></div></li><li class="step"><p>
     [OPTIONAL] Reboot your SLES node and ensure that it can be accessed from
     the Cloud Lifecycle Manager.
    </p></li></ol></div></div></section><section class="sect2" id="id-1.4.5.14.5.6" data-id-title="Add ardana user and home directory"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.4 </span><span class="title-name">Add <code class="literal">ardana</code> user and home directory</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.5.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><div class="verbatim-wrap"><pre class="screen">useradd -m ardana
passwd ardana</pre></div></section><section class="sect2" id="id-1.4.5.14.5.7" data-id-title="Allow user ardana to sudo without password"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.5 </span><span class="title-name">Allow user <code class="literal">ardana</code> to <code class="literal">sudo</code> without password</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.5.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Setting up sudo on SLES is covered in the <em class="citetitle">SLES Administration Guide</em> at
    <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-sudo-conf" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-sudo-conf</a>.
   </p><p>
    The recommendation is to create user specific <code class="command">sudo</code> config files under
    <code class="filename">/etc/sudoers.d</code>, therefore creating an <code class="filename">/etc/sudoers.d/ardana</code> config file with
    the following content will allow sudo commands without the requirement of a
    password.
   </p><div class="verbatim-wrap"><pre class="screen">ardana ALL=(ALL) NOPASSWD:ALL</pre></div></section><section class="sect2" id="sec-provisioning-sles-add-zypper" data-id-title="Add zypper repository"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.6 </span><span class="title-name">Add zypper repository</span></span> <a title="Permalink" class="permalink" href="#sec-provisioning-sles-add-zypper">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Using the ISO-based repositories created above, add the zypper repositories.
  </p><p>
   Follow these steps. Update the value of deployer_ip as necessary.
  </p><div class="verbatim-wrap"><pre class="screen">deployer_ip=192.168.10.254
<code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh http://$deployer_ip:79/ardana/sles12/zypper/OS SLES-OS
<code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh http://$deployer_ip:79/ardana/sles12/zypper/SDK SLES-SDK</pre></div><p>
   To verify that the repositories have been added, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper repos --detail</pre></div><p>
   For more information about Zypper, see the
   <em class="citetitle">SLES Administration Guide</em> at
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-zypper" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-zypper</a>.
  </p><div id="id-1.4.5.14.5.8.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    If you intend on attaching encrypted volumes to any of your SLES
    Compute nodes, install the cryptographic libraries through cryptsetup on
    each node. Run the following command to install the necessary
    cryptographic libraries:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper in cryptsetup</pre></div></div></section><section class="sect2" id="id-1.4.5.14.5.9" data-id-title="Add Required Packages"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.7 </span><span class="title-name">Add Required Packages</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.5.9">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As documented in <a class="xref" href="#sec-kvm-provision" title="12.4. Provisioning Your Baremetal Nodes">Section 12.4, “Provisioning Your Baremetal Nodes”</a>,
   you need to add extra packages.
   <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
   <code class="literal">python</code>,
   and <code class="literal">rsync</code> are installed.</span>
  </p></section><section class="sect2" id="id-1.4.5.14.5.10" data-id-title="Set up passwordless SSH access"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.4.8 </span><span class="title-name">Set up passwordless SSH access</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.14.5.10">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-sles-provisioning_sles.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have started your installation using the Cloud Lifecycle Manager, or if
   you are adding a SLES node to an existing cloud, you need to copy the
   Cloud Lifecycle Manager public key to the SLES node. One way of doing this is to
   copy the <code class="literal">/home/ardana/.ssh/authorized_keys</code> from another
   node in the cloud to the same location on the SLES node. If you are
   installing a new cloud, this file will be available on the nodes after
   running the <code class="literal">bm-reimage.yml</code> playbook.
  </p><div id="id-1.4.5.14.5.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Ensure that there is global read access to the file
    <code class="filename">/home/ardana/.ssh/authorized_keys</code>.
   </p></div><p>
   Now test passwordless SSH from the deployer and check your ability to
   remotely execute sudo commands:
  </p><div class="verbatim-wrap"><pre class="screen">ssh ardana@<em class="replaceable">IP_OF_SLES_NODE</em> "sudo tail -5 /var/log/messages"</pre></div></section></section></section><section class="chapter" id="install-ardana-manila" data-id-title="Installing Manila and Creating Manila Shares"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">20 </span><span class="title-name">Installing Manila and Creating Manila Shares</span></span> <a title="Permalink" class="permalink" href="#install-ardana-manila">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="id-1.4.5.15.2" data-id-title="Installing Manila"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.1 </span><span class="title-name">Installing Manila</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.15.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <span class="productname">OpenStack</span> Shared File Systems service (Manila) provides file storage
   to a virtual machine. The Shared File Systems service provides a storage
   provisioning control plane for shared or distributed file systems. The
   service enables management of share types and share snapshots if you have a
   driver that supports them.
  </p><p>
   The Manila service consists of the following components:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     manila-api
    </p></li><li class="listitem"><p>
     manila-data
    </p></li><li class="listitem"><p>
     manila-scheduler
    </p></li><li class="listitem"><p>
     manila-share
    </p></li><li class="listitem"><p>
     messaging queue
    </p></li></ul></div><p>
   These Manila components are included in example <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> models
   based on Nova KVM, such as <code class="literal">entry-scale-kvm</code>,
   <code class="literal">entry-scale-kvm-mml</code>, and
   <code class="literal">mid-scale-kvm</code>. General installation instructions are
   available at <a class="xref" href="#install-kvm" title="Chapter 12. Installing Mid-scale and Entry-scale KVM">Chapter 12, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p><p>
   If you modify one of these cloud models to set up a dedicated Cloud Lifecycle Manager, add
   <code class="literal">manila-client</code> item to the list of service components for
   the Cloud Lifecycle Manager cluster.
  </p><p>
   The following steps install Manila if it is not already present in your
   cloud data model:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Apply Manila changes in <code class="filename">control_plane.yml</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/openstack/my_cloud/definition/data/</pre></div><p>
     Add <code class="literal">manila-client</code> to the list of service components for
     Cloud Lifecycle Manager, and both <code class="literal">manila-api</code> and <code class="literal">manila-share</code>
     to the Control Node.
    </p></li><li class="step"><p>
     Run the Configuration Processor.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "manila config"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Deploy Manila
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-deploy.yml</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-gen-hosts-file.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts clients-deploy.yml</pre></div><p>
     If Manila has already been installed and is being reconfigured, run
     the following for the changes to take effect:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-start.yml</pre></div></li><li class="step"><p>
     Verify the Manila installation
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd
<code class="prompt user">ardana &gt; </code>. manila.osrc
<code class="prompt user">ardana &gt; </code>. service.osrc
<code class="prompt user">ardana &gt; </code>manila api-version
<code class="prompt user">ardana &gt; </code>manila service-list</pre></div><p>
     The Manila CLI can be run from Cloud Lifecycle Manager or controller nodes.
    </p></li></ol></div></div><div id="id-1.4.5.15.2.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The <code class="literal">manila-share</code> service component is not started by the
    <code class="filename">manila-deploy.yml</code> playbook when run under default
    conditions. This component requires that a valid backend be configured,
    which is described in <a class="xref" href="#configure-manila-backend" title="20.3. Configure Manila Backend">Section 20.3, “Configure Manila Backend”</a>.
   </p></div></section><section class="sect1" id="id-1.4.5.15.3" data-id-title="Adding Manila to an Existing SUSE OpenStack Cloud Environment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.2 </span><span class="title-name">Adding Manila to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.15.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Add Manila to an existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation or as part of an
   upgrade with the following steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Add the items listed below to the list of service components in
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
     Add them to clusters that have <code class="literal">server-role</code> set to
     <code class="literal">CONTROLLER-ROLE</code> (applies to entry-scale models).
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       manila-client
      </p></li><li class="listitem"><p>
       manila-api
      </p></li></ul></div></li><li class="step"><p>
     If your environment uses a dedicated Cloud Lifecycle Manager, add
     <code class="literal">magnum-client</code> to the list of service components for the
     Cloud Lifecycle Manager in
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
    </p></li><li class="step"><p>
     Commit your configuration to the local git repo.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step"><p>
     Run the configuration processor.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     Update deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Deploy Manila.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-gen-hosts-file.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts clients-deploy.yml</pre></div></li><li class="step"><p>
     Verify the Manila installation.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd
<code class="prompt user">ardana &gt; </code>. manila.osrc
<code class="prompt user">ardana &gt; </code>. service.osrc
<code class="prompt user">ardana &gt; </code>manila api-version
<code class="prompt user">ardana &gt; </code>manila service-list</pre></div></li></ol></div></div><p>
   The Manila CLI can be run from the Cloud Lifecycle Manager or controller nodes.
  </p><p>
   Proceed to <a class="xref" href="#configure-manila-backend" title="20.3. Configure Manila Backend">Section 20.3, “Configure Manila Backend”</a>.
  </p></section><section class="sect1" id="configure-manila-backend" data-id-title="Configure Manila Backend"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.3 </span><span class="title-name">Configure Manila Backend</span></span> <a title="Permalink" class="permalink" href="#configure-manila-backend">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="id-1.4.5.15.4.2" data-id-title="Configure NetaApp Manila Back-end"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.3.1 </span><span class="title-name">Configure NetaApp Manila Back-end</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.15.4.2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.15.4.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     An account with cluster administrator privileges must be used with the
     <code class="literal">netapp_login</code> option when using Share Server management.
     Share Server management creates Storage Virtual Machines (SVM), thus SVM
     administrator privileges are insufficient.
    </p><p>
     There are two modes for the NetApp Manila back-end:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">driver_handles_share_servers = True</code>
      </p></li><li class="listitem"><p>
       <code class="literal">driver_handles_share_servers = False</code> This value must
       be set to <code class="literal">False</code> if you want the driver to operate
       without managing share servers.
      </p></li></ul></div><p>
     More information is available from
     <a class="link" href="https://netapp-openstack-dev.github.io/openstack-docs/rocky/manila/configuration/manila_config_files/section_unified-driver-without-share-server.html" target="_blank">NetApp
     OpenStack</a>
    </p></div><p>
    The steps to configure a NetApp Manila back-end are:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Configure a back-end in the Manila configuration file, following the
      directions and comments in the file.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud
<code class="prompt user">ardana &gt; </code>vi config/manila/manila.conf.j2</pre></div></li><li class="step"><p>
      Commit your configuration to the local Git repo.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step"><p>
      Run the configuration processor.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
      Update deployment directory.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
      Run reconfiguration playbook.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-reconfigure.yml</pre></div></li><li class="step"><p>
      Restart Manila services.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-start.yml</pre></div></li></ol></div></div><div id="id-1.4.5.15.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     After the <code class="literal">manila-share</code> service has been initialized
     with a backend, it can be controlled independently of
     <code class="literal">manila-api</code> by using the playbooks
     <code class="filename">manila-share-start.yml</code>,
     <code class="filename">manila-share-stop.yml</code>, and
     <code class="filename">manila-share-status.yml</code>.
    </p></div></section><section class="sect2" id="id-1.4.5.15.4.3" data-id-title="Configure CephFS Manila Backend"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">20.3.2 </span><span class="title-name">Configure CephFS Manila Backend</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.15.4.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Configure a back-end in the Manila configuration file,
    <code class="filename">~/openstack/my_cloud vi config/manila/manila.conf.j2</code>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      To define a CephFS native back-end, create a section like the following:
     </p><div class="verbatim-wrap"><pre class="screen">[cephfsnative1]
driver_handles_share_servers = False
share_backend_name = CEPHFSNATIVE1
share_driver = manila.share.drivers.cephfs.driver.CephFSDriver
cephfs_conf_path = /etc/ceph/ceph.conf
cephfs_protocol_helper_type = CEPHFS
cephfs_auth_id = manila
cephfs_cluster_name = ceph
cephfs_enable_snapshots = False</pre></div></li><li class="step"><p>
      Add CephFS to <code class="literal">enabled_share_protocols</code>:
     </p><div class="verbatim-wrap"><pre class="screen">enabled_share_protocols = NFS,CIFS,CEPHFS</pre></div></li><li class="step"><p>
      Edit the <code class="literal">enabled_share_backends</code> option in the
      <code class="literal">DEFAULT</code> section to point to the driver’s back-end
      section name.
     </p></li><li class="step"><p>
      According to the environment, modify back-end specific lines in
      <code class="filename">~/openstack/my_cloud vi
      config/manila/manila.conf.j2</code>.
     </p></li><li class="step"><p>
      Commit your configuration to the local Git repo.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step"><p>
      Run the configuration processor.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
      Update deployment directory.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
      Run reconfiguration playbook.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-reconfigure.yml</pre></div></li><li class="step"><p>
      Restart Manila services.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-start.yml</pre></div></li></ol></div></div><div id="id-1.4.5.15.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     After the <code class="literal">manila-share</code> service has been initialized
     with a back-end, it can be controlled independently of
     <code class="literal">manila-api</code> by using the playbooks
     <code class="filename">manila-share-start.yml</code>,
     <code class="filename">manila-share-stop.yml</code>, and
     <code class="filename">manila-share-status.yml</code>.
    </p></div><p>
    For more details of the CephFS Manila back-end, see
    <a class="link" href="https://docs.openstack.org/manila/rocky/admin/cephfs_driver.html" target="_blank">OpenStack
    CephFS driver</a>.
   </p></section></section><section class="sect1" id="id-1.4.5.15.5" data-id-title="Creating Manila Shares"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.4 </span><span class="title-name">Creating Manila Shares</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.15.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Manila can support two modes, with and without the handling of share
   servers. The mode depends on driver support.
  </p><p>
   Mode 1: The back-end is a generic driver,
   <code class="literal">driver_handles_share_servers = False</code> (DHSS is disabled).
   The following example creates a VM using Manila share image.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://tarballs.openstack.org/manila-image-elements/images/manila-service-image-master.qcow2
<code class="prompt user">ardana &gt; </code>. service.osrc;openstack image create --name
"manila-service-image-new" \
--file manila-service-image-master.qcow2 --disk-format qcow2 \
--container-format bare --visibility public --progress
<code class="prompt user">ardana &gt; </code>openstack image list (verify manila image)
<code class="prompt user">ardana &gt; </code>openstack security group create manila-security-group \
--description "Allows web and NFS traffic to manila server."
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol tcp --dst-port 2049
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol udp --dst-port 2049
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol tcp --dst-port 22
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol icmp
<code class="prompt user">ardana &gt; </code>openstack security group rule list manila-security-group (verify manila security group)
<code class="prompt user">ardana &gt; </code>openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey
<code class="prompt user">ardana &gt; </code>openstack network create n1
<code class="prompt user">ardana &gt; </code>openstack subnet create s1 --network n1 <em class="replaceable">--subnet-range 11.11.11.0/24</em>
<code class="prompt user">ardana &gt; </code>openstack router create r1
<code class="prompt user">ardana &gt; </code>openstack router add subnet r1 s1
<code class="prompt user">ardana &gt; </code>openstack router set r1 ext-net
<code class="prompt user">ardana &gt; </code>openstack network list
<code class="prompt user">ardana &gt; </code>openstack server create manila-vm --flavor m1.small \
--image IMAGE_ID --nic net-id=N1_ID --security-group manila-security-group \
--key-name myKey
<code class="prompt user">ardana &gt; </code>oenstack floating ip create <em class="replaceable">EXT-NET_ID</em>
<code class="prompt user">ardana &gt; </code>openstack server add floating ip manila-vm <em class="replaceable">EXT-NET_ID</em></pre></div></li><li class="step"><p>
     Validate your ability to ping or connect by SSH to
     <code class="literal">manila-vm</code> with credentials
     <code class="literal">manila/manila</code>.
    </p></li><li class="step"><p>
     Modify the configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi
    /etc/manila/manila.conf.d/100-manila.conf</pre></div><p>
     Make changes in [generic1] section
    </p><div class="verbatim-wrap"><pre class="screen">service_instance_name_or_id = <em class="replaceable">MANILA_VM_ID</em>
service_net_name_or_ip = <em class="replaceable">MANILA_VM_FLOATING_IP</em>
tenant_net_name_or_ip = <em class="replaceable">MANILA_VM_FLOATING_IP</em></pre></div></li><li class="step"><p>
     Create a share type. OpenStack docs has
     <a class="link" href="https://docs.openstack.org/manila/rocky/install/post-install.html" target="_blank">detailed
     instructions</a>. Use the instructions for <code class="literal">manila type-create
     default_share_type False</code>
    </p></li><li class="step"><p>
     Restart Manila services and verify they are up.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>systemctl restart openstack-manila-api \
openstack-manila-share openstack-manila-scheduler
<code class="prompt user">ardana &gt; </code>manila service-list</pre></div></li><li class="step"><p>
     Continue creating a share
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>manila create NFS 1 --name <em class="replaceable">SHARE</em>
<code class="prompt user">ardana &gt; </code>manila list (status will change from  creating to available)
<code class="prompt user">ardana &gt; </code>manila show share1
<code class="prompt user">ardana &gt; </code>manila access-allow <em class="replaceable">SHARE</em> ip <em class="replaceable">INSTANCE_IP</em></pre></div></li><li class="step"><p>
     Mount the share on a Compute instance
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir ~/test_directory
<code class="prompt user">tux &gt; </code>sudo mount -vt nfs <em class="replaceable">EXT-NET_ID</em>:/shares/<em class="replaceable">SHARE-SHARE-ID</em> ~/test_folder</pre></div></li></ol></div></div><p>
   Mode 2: The back-end is <code class="literal">NetApp</code>,
   <code class="literal">driver_handles_share_servers = True</code> (DHSS is enabled).
   Procedure for driver_handles_share_servers = False is similar to Mode 1.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Modify the configuration
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi /etc/manila/manila.conf.d/100-manila.conf</pre></div><p>
     Add a <code class="literal">backendNetApp</code> section
    </p><div class="verbatim-wrap"><pre class="screen">share_driver = manila.share.drivers.netapp.common.NetAppDriver
driver_handles_share_servers = True
share_backend_name=backendNetApp
netapp_login=<em class="replaceable">NetApp_USERNAME</em>
netapp_password=<em class="replaceable">NetApp_PASSWORD</em>
netapp_server_hostname=<em class="replaceable">NETAPP_HOSTNAME</em>
netapp_root_volume_aggregate=<em class="replaceable">AGGREGATE_NAME</em></pre></div><p>
     Add to [DEFAULT] section
    </p><div class="verbatim-wrap"><pre class="screen">enabled_share_backends = backendNetApp
default_share_type = default1</pre></div></li><li class="step"><p>
     Create a Manila share image and verify it
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://tarballs.openstack.org/manila-image-elements/images/manila-service-image-master.qcow2
<code class="prompt user">ardana &gt; </code>. service.osrc;openstack image create <code class="literal">manila-service-image-new</code> \
--file manila-service-image-master.qcow2 --disk-format qcow2 \
--container-format bare --visibility public --progress
<code class="prompt user">ardana &gt; </code>openstack image list (verify a Manila image)</pre></div></li><li class="step"><p>
     Create a share type. OpenStack docs has
     <a class="link" href="https://docs.openstack.org/manila/rocky/install/post-install.html" target="_blank">detailed
     instructions</a>. Use the instructions for <code class="literal">manila type-create
     default_share_type True</code> .
    </p></li><li class="step"><p>
     Restart services
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>systemctl restart openstack-manila-api openstack-manila-share \openstack-manila-scheduler
<code class="prompt user">ardana &gt; </code>manila service-list (verify services are up)</pre></div></li><li class="step"><p>
     Continue creating a share. <code class="literal">OCTAVIA-MGMT-NET</code> can be used
     as <em class="replaceable">PRIVATE_NETWORK</em> in this example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>manila share-network-create --name demo-share-network1 \
--neutron-net-id PRIVATE_NETWORK_ID --neutron-subnet-id PRIVATE_NETWORK_SUBNET_ID
<code class="prompt user">ardana &gt; </code>manila create NFS 1 --name share2 --share-network demo-share-network1</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.5.15.6" data-id-title="Troubleshooting"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.5 </span><span class="title-name">Troubleshooting</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.15.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-ardana-manila.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If manila-list shows share status in error, use <code class="literal">storage aggregate
   show</code> to list available aggregates. Errors may be found in
   <code class="filename">/var/log/manila/manila-share.log</code>
  </p><p>
   if the compute nodes do not have access to Manila back-end server, use
   the <code class="literal">manila-share</code> service on controller nodes instead. You
   can do so by either running <code class="literal">sudo systemctl stop
   openstack-manila-share</code> on compute to turn off its share service or
   skipping adding "manila-share" to compute hosts in the input-model
   (<code class="filename">control_plane.yml</code> in
   <code class="filename">/var/lib/ardana/openstack/my_cloud/definition/data</code>).
  </p></section></section><section class="chapter" id="install-heat-templates" data-id-title="Installing SUSE CaaS Platform Heat Templates"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">21 </span><span class="title-name">Installing SUSE CaaS Platform Heat Templates</span></span> <a title="Permalink" class="permalink" href="#install-heat-templates">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This chapter describes how to install SUSE CaaS Platform Heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><section class="sect1" id="sec-heat-templates-install" data-id-title="SUSE CaaS Platform Heat Installation Procedure"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.1 </span><span class="title-name">SUSE CaaS Platform Heat Installation Procedure</span></span> <a title="Permalink" class="permalink" href="#sec-heat-templates-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.4.5.16.3.2" data-id-title="Preparation"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.1: </span><span class="title-name">Preparation </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.16.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Download the latest SUSE CaaS Platform for <span class="productname">OpenStack</span> image (for example,
     <code class="filename">SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</code>)
     from <a class="link" href="https://download.suse.com" target="_blank">https://download.suse.com</a>.
    </p></li><li class="step"><p>
     Upload the image to Glance:
    </p><div class="verbatim-wrap"><pre class="screen">openstack image create --public --disk-format qcow2 --container-format \
bare --file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
CaaSP-3</pre></div></li><li class="step"><p>
     Install the <span class="package">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p><p>
     Alternatively, you can get official Heat templates by cloning the
     appropriate Git repository:
    </p><div class="verbatim-wrap"><pre class="screen">git clone https://github.com/SUSE/caasp-openstack-heat-templates</pre></div></li></ol></div></div><div class="procedure" id="id-1.4.5.16.3.3" data-id-title="Installing Templates via Horizon"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.2: </span><span class="title-name">Installing Templates via Horizon </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.16.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In Horizon, go to
     <span class="guimenu">Project</span> › <span class="guimenu">Stacks</span> › <span class="guimenu">Launch
     Stack</span>.
    </p></li><li class="step"><p>
     Select <span class="guimenu">File</span> from the <span class="guimenu">Template Source</span>
     drop-down box and upload the <code class="filename">caasp-stack.yaml</code> file.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Launch Stack</span> dialog, provide the required
     information (stack name, password, flavor size, external network of your
     environment, etc.).
    </p></li><li class="step"><p>
     Click <span class="guimenu">Launch</span> to launch the stack. This creates all
     required resources for running SUSE CaaS Platform in an <span class="productname">OpenStack</span> environment. The
     stack creates one Admin Node, one Master Node, and server worker nodes as
     specified.
    </p></li></ol></div></div><div class="procedure" id="id-1.4.5.16.3.4" data-id-title="Install Templates from the Command Line"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.3: </span><span class="title-name">Install Templates from the Command Line </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.16.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-environment.yaml</code> file.
    </p></li><li class="step"><p>
     Create a stack in Heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen">openstack stack create -t caasp-stack.yaml -e caasp-environment.yaml \
--parameter image=CaaSP-3 caasp-stack</pre></div></li></ol></div></div><div class="procedure" id="id-1.4.5.16.3.5" data-id-title="Accessing Velum SUSE CaaS Platform dashboard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.4: </span><span class="title-name">Accessing Velum SUSE CaaS Platform dashboard </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.16.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the Admin Node.
     You can access it using the Admin Node's floating IP address.
    </p></li><li class="step"><p>
     Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to complete the
     SUSE CaaS Platform installation.
    </p></li></ol></div></div><p>
   When you have successfully accessed the admin node web interface via the
   floating IP, follow the instructions at <a class="link" href="https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/" target="_blank">https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/</a> to
   continue the setup of SUSE CaaS Platform.
  </p></section><section class="sect1" id="id-1.4.5.16.4" data-id-title="Installing SUSE CaaS Platform with Multiple Masters"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.2 </span><span class="title-name">Installing SUSE CaaS Platform with Multiple Masters</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.16.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.16.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    A Heat stack with load balancing and multiple master nodes can only be
    created from the command line, because Horizon does not have support for nested
    Heat templates.
   </p></div><p>
   Install the <span class="package">caasp-openstack-heat-templates</span> package on a
   machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
  </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
   The installed templates are located in
   <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
  </p><p>
   A working load balancer is needed in your SUSE <span class="productname">OpenStack</span> Cloud deployment. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   uses HAProxy.
  </p><p>
   Verify that load balancing with HAProxy is working correctly
   in your <span class="productname">OpenStack</span> installation by creating a load balancer manually and
   checking that the <code class="literal">provisioning_status</code> changes to
   <code class="literal">Active</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer show
&lt;<em class="replaceable">LOAD_BALANCER_ID</em>&gt;</pre></div><p>
   HAProxy is the default load balancer provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   The steps below can be used to set up a network, subnet, router, security
   and IPs for a test <code class="literal">lb_net1</code> network with
   <code class="literal">lb_subnet1</code> subnet.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create lb_net1
  <code class="prompt user">ardana &gt; </code>openstack subnet create --name lb_subnet1 lb_net1 \
--subnet-range 172.29.0.0/24 --gateway 172.29.0.2
<code class="prompt user">ardana &gt; </code>openstack router create lb_router1
<code class="prompt user">ardana &gt; </code>openstack router add subnet lb_router1 lb_subnet1
<code class="prompt user">ardana &gt; </code>openstack router set lb_router1 --external-gateway ext-net
<code class="prompt user">ardana &gt; </code>openstack network list</pre></div><div class="procedure" id="id-1.4.5.16.4.12" data-id-title="Steps to Install SUSE CaaS Platform with Multiple Masters"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.5: </span><span class="title-name">Steps to Install SUSE CaaS Platform with Multiple Masters </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.16.4.12">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file.
    </p></li><li class="step"><p>
     Set <code class="literal">master_count</code> to the desired number in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file. The master
     count must be set to an odd number of nodes.
    </p><div class="verbatim-wrap"><pre class="screen">master_count: 3</pre></div></li><li class="step"><p>
     Create a stack in Heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack create -t caasp-multi-master-stack.yaml \
-e caasp-multi-master-environment.yaml --parameter image=CaaSP-3 caasp-multi-master-stack</pre></div></li><li class="step"><p>
     Find the floating IP address of the load balancer. This is necessary for
     accessing the Velum SUSE CaaS Platform dashboard.
    </p><ol type="a" class="substeps"><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer list --provider</pre></div></li><li class="step"><p>
       From the output, copy the <code class="literal">id</code> and enter it in the
       following command as shown in the following example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer show id</pre></div><div class="verbatim-wrap"><pre class="screen">+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| id                  | 0d973d80-1c79-40a4-881b-42d111ee9625           |
| listeners           | {"id": "c9a34b63-a1c8-4a57-be22-75264769132d"} |
|                     | {"id": "4fa2dae0-126b-4eb0-899f-b2b6f5aab461"} |
| name                | caasp-stack-master_lb-bhr66gtrx3ue             |
| operating_status    | ONLINE                                         |
| pools               | {"id": "8c011309-150c-4252-bb04-6550920e0059"} |
|                     | {"id": "c5f55af7-0a25-4dfa-a088-79e548041929"} |
| provider            | haproxy                                        |
| provisioning_status | ACTIVE                                         |
| tenant_id           | fd7ffc07400642b1b05dbef647deb4c1               |
| vip_address         | 172.28.0.6                                     |
| vip_port_id         | 53ad27ba-1ae0-4cd7-b798-c96b53373e8b           |
| vip_subnet_id       | 87d18a53-ad0c-4d71-b82a-050c229b710a           |
+---------------------+------------------------------------------------+</pre></div></li><li class="step"><p>
       Search the floating IP list for <code class="literal">vip_address</code>
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack floating ip list | grep 172.28.0.6
| d636f3...481b0c | fd7ff...deb4c1 | 172.28.0.6  | 10.84.65.37  | 53ad2...373e8b |</pre></div></li><li class="step"><p>
       The load balancer floating ip address is 10.84.65.37
      </p></li></ol></li></ol></div></div><p>
   <span class="bold"><strong>Accessing the Velum SUSE CaaS Platform Dashboard</strong></span>
  </p><p>
   After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the
   admin node. You can access it using the floating IP address of the admin
   node.
  </p><p>
   Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to
   complete the SUSE CaaS Platform installation.
  </p><p>
   SUSE CaaS Platform Admin Node Install: Screen 1
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_1.png"><img src="images/caasp_1.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 2
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_2.png"><img src="images/caasp_2.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 3
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_3.png"><img src="images/caasp_3.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 4
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_4.png"><img src="images/caasp_4.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 5
  </p><p>
   Set External Kubernetes API to
   <em class="replaceable">LOADBALANCER_FLOATING_IP</em>, External Dashboard FQDN
   to <em class="replaceable">ADMIN_NODE_FLOATING_IP</em>
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_5.png"><img src="images/caasp_5.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 6
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_6.png"><img src="images/caasp_6.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 7
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_7.png"><img src="images/caasp_7.png" width="90%" alt="Image" title="Image"/></a></div></div></section><section class="sect1" id="id-1.4.5.16.5" data-id-title="Deploy SUSE CaaS Platform Stack Using Heat SUSE CaaS Platform Playbook"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.3 </span><span class="title-name">Deploy SUSE CaaS Platform Stack Using Heat SUSE CaaS Platform Playbook</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.16.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install the <span class="package">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p></li><li class="step"><p>
     Run <code class="filename">heat-caasp-deploy.yml</code> on the Cloud Lifecycle Manager to create a
     SUSE CaaS Platform cluster with Heat templates from the
     <code class="literal">caasp-openstack-heat-templates</code> package.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml</pre></div></li><li class="step"><p>
     In a browser, navigate to the Horizon UI to determine the floating IP
     address assigned to the admin node.
    </p></li><li class="step"><p>
     Go to http://<em class="replaceable">ADMIN-NODE-FLOATING-IP</em>/ to bring
     up the Velum dashboard.
    </p></li><li class="step"><p>
     Follow the steps for <code class="literal">bootstrap</code> and <code class="literal">add nodes
     to the cluster</code> to bring up the SUSE CaaS Platform Kubernetes cluster.
    </p></li></ol></div></div></section><section class="sect1" id="id-1.4.5.16.6" data-id-title="Deploy SUSE CaaS Platform Cluster with Multiple Masters Using Heat CaasP Playbook"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.4 </span><span class="title-name">Deploy SUSE CaaS Platform Cluster with Multiple Masters Using Heat CaasP
  Playbook</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.16.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install the <span class="package">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p></li><li class="step"><p>
     On the Cloud Lifecycle Manager, run the <code class="filename">heat-caasp-deploy.yml</code> playbook
     and pass parameters for <code class="literal">caasp_stack_name</code>,
     <code class="literal">caasp_stack_yaml_file</code> and
     <code class="literal">caasp_stack_env_yaml_file</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml -e "caasp_stack_name=caasp_multi-master caasp_stack_yaml_file=caasp-multi-master-stack.yaml caasp_stack_env_yaml_file=caasp-multi-master-environment.yaml"</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.5.16.7" data-id-title="SUSE CaaS Platform OpenStack Image for Heat SUSE CaaS Platform Playbook"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.5 </span><span class="title-name">SUSE CaaS Platform <span class="productname">OpenStack</span> Image for Heat SUSE CaaS Platform Playbook</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.16.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   By default the Heat SUSE CaaS Platform playbook downloads the SUSE CaaS Platform image from <a class="link" href="http://download.suse.de/install/SUSE-CaaSP-3-GM/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2" target="_blank">http://download.suse.de/install/SUSE-CaaSP-3-GM/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2</a>. If
   this URL is not accessible, the SUSE CaaS Platform image can be downloaded from <a class="link" href="https://download.suse.com/Download?buildid=z7ezhywXXRc" target="_blank">https://download.suse.com/Download?buildid=z7ezhywXXRc</a> and
   copied to the deployer.
  </p><p>
   To create a SUSE CaaS Platform cluster and pass the path to the downloaded image, run the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml -e "caasp_image_tmp_path=~/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2"</pre></div><p>
   To create a SUSE CaaS Platform cluster with multiple masters and pass the path to the
   downloaded image, run the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml -e "caasp_image_tmp_path=caasp_image_tmp_path=~/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2
 caasp_stack_name=caasp_multi-master caasp_stack_yaml_file=caasp-multi-master-stack.yaml caasp_stack_env_yaml_file=caasp-multi-master-environment.yaml"</pre></div></section><section class="sect1" id="id-1.4.5.16.8" data-id-title="Enabling the Cloud Provider Integration (CPI) Feature"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.6 </span><span class="title-name">Enabling the Cloud Provider Integration (CPI) Feature</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.16.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When deploying a CaaaSP cluster using SUSE CaaS Platform <span class="productname">OpenStack</span> Heat
    templates, the following CPI parameters can be set in
    <code class="filename">caasp-environment.yaml</code> or
    <code class="filename">caasp-multi-master-environment.yaml</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.16.8.3.1"><span class="term">cpi_auth_url</span></dt><dd><p>
       The URL of the Keystone API used to authenticate the user. This value
       can be found on <span class="productname">OpenStack</span> Dashboard under
       <span class="guimenu">Access and Security</span> › <span class="guimenu">API
       Access</span> › <span class="guimenu">Credentials</span> (for
       example, https://api.keystone.example.net:5000/v3/)
      </p></dd><dt id="id-1.4.5.16.8.3.2"><span class="term">cpi_domain_name</span></dt><dd><p>
       Name of the domain the user belongs to
      </p></dd><dt id="id-1.4.5.16.8.3.3"><span class="term">cpi_tenant_name</span></dt><dd><p>
       Name of the project the user belongs to. This is the project where the
       resources will be created.
      </p></dd><dt id="id-1.4.5.16.8.3.4"><span class="term">cpi_region</span></dt><dd><p>
       Name of the region to use when running a multi-region <span class="productname">OpenStack</span>
       cloud. The region is a general division of an <span class="productname">OpenStack</span> deployment.
      </p></dd><dt id="id-1.4.5.16.8.3.5"><span class="term">cpi_username</span></dt><dd><p>
       Username of a valid user that has been set in Keystone. Default: admin
      </p></dd><dt id="id-1.4.5.16.8.3.6"><span class="term">cpi_password</span></dt><dd><p>
       Password of a valid user that has been set in Keystone.
      </p></dd><dt id="id-1.4.5.16.8.3.7"><span class="term">cpi_monitor_max_retries</span></dt><dd><p>
       Neutron load balancer monitoring max retries. Default: 3
      </p></dd><dt id="id-1.4.5.16.8.3.8"><span class="term">cpi_bs_version</span></dt><dd><p>
       Cinder Block Storage API version. Possible values are v1, v2 , v3 or
       auto. Default: auto
      </p></dd><dt id="id-1.4.5.16.8.3.9"><span class="term">cpi_ignore_volume_az</span></dt><dd><p>
       Ignore Cinder and Nova availability zones. Default: true
      </p></dd></dl></div><p>
    When the SUSE CaaS Platform cluster has been deployed by the Heat templates, the
    Velum dashboard on the admin node
    (https://<em class="replaceable">ADMIN-NODE-IP</em>/) will have entries for
    <span class="guimenu">Cloud Provider Integration</span> (CPI). The <span class="productname">OpenStack</span>
    settings form will show the values that were set in the
    <code class="filename">caasp-environment.yaml</code> or
    <code class="filename">caasp-multi-master-environment.yaml</code> files.
   </p><div class="informalfigure"><div class="mediaobject"><a href="images/cpi_1.png"><img src="images/cpi_1.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
    After the SUSE CaaS Platform cluster comes up, install the latest SUSE CaaS Platform 3.0 Maintenance
    Update using the following steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Spin up a SUSE CaaS Platform cluster using Heat templates following the
      instructions in <a class="xref" href="#sec-heat-templates-install" title="21.1. SUSE CaaS Platform Heat Installation Procedure">Section 21.1, “SUSE CaaS Platform Heat Installation Procedure”</a>. Do not go
      through the bootstrapping steps, because the SUSE CaaS Platform Maintenance Update
      (MU) must be applied first. After the maintenance update process below
      succeeds, return to the SUSE CaaS Platform installation instructions and follow the
      admin node bootstrapping steps.
     </p><p>
      Apply the SUSE CaaS Platform 3.0 Maintenance Update with the following steps:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Log in to each node and add the update repository.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper ar http://nu.novell.com/SUSE/Updates/SUSE-CAASP/3.0/x86_64/update/ caasp_update</pre></div></li><li class="step"><p>
        With root privileges, run <code class="literal">transactional-update</code> on each node.
       </p></li><li class="step"><p>
        Reboot each node
       </p></li><li class="step"><p>
        Verify that the Velum image packages were updated
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper se --detail velum-image
i | sles12-velum-image | package    | 3.1.7-3.27.3  | x86_64 | update_caasp</pre></div></li></ol></li><li class="step"><p>
      Upload a valid trust certificate that can validate a certificate
      presented by Keystone at the specified
      <code class="literal">keystone_auth_url</code> in the <code class="literal">system-wide
      certificate</code> section of Velum. The certificate must be uploaded
      to avoid bootstrapping failure with a <code class="literal">x509: certificate signed
      by unknown authority</code> error message.
     </p><p>
      Get SUSE Root CA certificate with the following steps:
     </p><ol type="a" class="substeps"><li class="step"><p>
        From the RPM provided in the repos:
       </p><ol type="i" class="substeps"><li class="step"><p>
          Add a repository for your distribution at
          <code class="literal">http://download.suse.de/ibs/SUSE:/CA/</code>. For
          example, with <code class="literal">openSUSE Leap</code>, use
          <code class="literal">http://download.suse.de/ibs/SUSE:/CA/openSUSE_Leap_15.0/SUSE:CA.repo</code>
         </p></li><li class="step"><p>
          Install certificates and find the <code class="filename">.pem</code> file.
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper in ca-certificates-suse
<code class="prompt user">ardana &gt; </code>rpm -ql ca-certificates-suse</pre></div><p>
          The path to the cert is <code class="filename">/usr/share/pki/trust/anchors/SUSE_Trust_Root.crt.pem</code>
         </p></li></ol></li><li class="step"><p>
        In <code class="literal">system wide certificate</code> during setup in Velum,
        upload this certificate so you can bootstrap with CPI.
     </p></li></ol></li><li class="step"><p>
      After the nodes come up, continue with the installation instructions in
      <a class="xref" href="#sec-heat-templates-install" title="21.1. SUSE CaaS Platform Heat Installation Procedure">Section 21.1, “SUSE CaaS Platform Heat Installation Procedure”</a> following the admin node
      cluster bootstrapping steps.
     </p></li></ol></div></div></section><section class="sect1" id="id-1.4.5.16.9" data-id-title="More Information about SUSE CaaS Platform"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.7 </span><span class="title-name">More Information about SUSE CaaS Platform</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.16.9">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/install_caasp_heat_templates.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  More information about the SUSE CaaS Platform is available at <a class="link" href="https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/" target="_blank">https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/</a>
 </p></section></section><section class="chapter" id="integrations" data-id-title="Integrations"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">22 </span><span class="title-name">Integrations</span></span> <a title="Permalink" class="permalink" href="#integrations">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-integrations.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Once you have completed your cloud installation, these are some of the common
  integrations you may want to perform.
 </p><section class="sect1" id="config-3par" data-id-title="Configuring for 3PAR Block Storage Backend"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.1 </span><span class="title-name">Configuring for 3PAR Block Storage Backend</span></span> <a title="Permalink" class="permalink" href="#config-3par">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_3par.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This page describes how to configure your 3PAR backend for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale with KVM cloud model.
 </p><section class="sect2" id="idg-installation-installation-configure-3par-xml-7" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure-3par-xml-7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_3par.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     You must have the license for the following software before you start your
     3PAR backend configuration for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale with KVM cloud
     model:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Thin Provisioning
      </p></li><li class="listitem"><p>
       Virtual Copy
      </p></li><li class="listitem"><p>
       System Reporter
      </p></li><li class="listitem"><p>
       Dynamic Optimization
      </p></li><li class="listitem"><p>
       Priority Optimization
      </p></li></ul></div></li><li class="listitem"><p>
     Your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM Cloud should be up and running.
     Installation steps can be found in
     <a class="xref" href="#install-kvm" title="Chapter 12. Installing Mid-scale and Entry-scale KVM">Chapter 12, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
    </p></li><li class="listitem"><p>
     Your 3PAR Storage Array should be available in the cloud management
     network or routed to the cloud management network and the 3PAR FC and
     iSCSI ports configured.
    </p></li><li class="listitem"><p>
     The 3PAR management IP and iSCSI port IPs must have connectivity from the
     controller and compute nodes.
    </p></li><li class="listitem"><p>
     Please refer to the system requirements for 3PAR in the OpenStack
     configuration guide, which can be found here:
     <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/hp-3par-sys-reqs.html" target="_blank">3PAR
     System Requirements</a>.
    </p></li></ul></div></section><section class="sect2" id="idg-installation-installation-configure-3par-xml-9" data-id-title="Notes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.2 </span><span class="title-name">Notes</span></span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure-3par-xml-9">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_3par.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="literal">cinder_admin</code> role must be added in order to configure
   3Par ICSI as a volume type in Horizon.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack role add --user admin --project admin cinder_admin</pre></div><p>
   <span class="bold"><strong>Encrypted 3Par Volume</strong></span>: Attaching an
   encrypted 3Par volume is possible after installation by setting
   <code class="literal">volume_use_multipath = true</code> under the libvirt stanza in
   the <code class="literal">nova/kvm-hypervisor.conf.j2</code> file and reconfigure
   nova.
  </p><p>
   <span class="bold"><strong>Concerning using multiple backends:</strong></span> If you
   are using multiple backend options, ensure that you specify each of the
   backends you are using when configuring your
   <code class="literal">cinder.conf.j2</code> file using a comma-delimited list.
   Also create multiple volume types so you can specify a backend to utilize
   when creating volumes. Instructions are included below.
   You can also read the OpenStack documentation about <a class="link" href="https://wiki.openstack.org/wiki/Cinder-multi-backend" target="_blank">Cinder
   multiple storage backends</a>.
  </p><p>
   <span class="bold"><strong>Concerning iSCSI and Fiber Channel:</strong></span> You
   should not configure cinder backends so that multipath volumes are exported
   over both iSCSI and Fiber Channel from a 3PAR backend to the same Nova
   compute server.
  </p><p>
   <span class="bold"><strong>3PAR driver correct name:</strong></span> In a previous
   release, the 3PAR driver used for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> integration had its name
   updated from <code class="literal">HP3PARFCDriver</code> and
   <code class="literal">HP3PARISCSIDriver</code> to <code class="literal">HPE3PARFCDriver</code>
   and <code class="literal">HPE3PARISCSIDriver</code> respectively
   (<code class="literal">HP</code> changed to <code class="literal">HPE</code>). You may get a
   warning or an error if the deprecated filenames are used. The correct values
   are those in
   <code class="filename">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code>.
  </p></section><section class="sect2" id="sec-3par-multipath" data-id-title="Multipath Support"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.3 </span><span class="title-name">Multipath Support</span></span> <a title="Permalink" class="permalink" href="#sec-3par-multipath">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_3par.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Setting up multipath support is highly recommended for 3PAR FC/iSCSI
   backends, and should be considered a default best practice.
   For instructions on this process, refer to the
   <code class="filename">~/openstack/ardana/ansible/roles/multipath/README.md</code>
   file on the Cloud Lifecycle Manager. The <code class="filename">README.md</code> file contains
   detailed procedures for configuring multipath for 3PAR FC/iSCSI Cinder
   volumes.
  </p><div id="id-1.4.5.17.3.5.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    If multipath functionality is enabled, ensure that all 3PAR fibre channel ports are active and zoned correctly in the 3PAR storage for proper operation.
   </p></div><p>
   In addition, take the following steps to enable 3PAR FC/iSCSI multipath
   support in <span class="productname">OpenStack</span> configuration files:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/config/nova/kvm-hypervisor.conf.j2</code>
     file and add this line under the <code class="literal">[libvirt]</code> section:
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">[libvirt]
...
iscsi_use_multipath=true</pre></div><p>
     Additionally, if you are planning on attaching an encrypted 3PAR volume
     after installation, set <code class="literal">volume_use_multipath=true</code>
     in the same section.
    </p></li><li class="step"><p>
     Edit the file
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code>
     and add the following lines in the <code class="literal">[3par]</code> section:
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">[3par]
...
enforce_multipath_for_image_xfer=True
use_multipath_for_image_xfer=True</pre></div></li><li class="step"><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Run the Nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect2" id="config-fc" data-id-title="Configure 3PAR FC as a Cinder Backend"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.4 </span><span class="title-name">Configure 3PAR FC as a Cinder Backend</span></span> <a title="Permalink" class="permalink" href="#config-fc">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_3par.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You must modify the <code class="literal">cinder.conf.j2</code> to configure the FC
   details.
  </p><p>
   Perform the following steps to configure 3PAR FC as Cinder backend:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Make the following changes to the
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code> file:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Add your 3PAR backend to the <code class="literal">enabled_backends</code>
       section:
      </p><div class="verbatim-wrap"><pre class="screen"># Configure the enabled backends
enabled_backends=3par_FC</pre></div><div id="id-1.4.5.17.3.6.4.2.2.1.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        If you are using multiple backend types, you can use a comma-delimited
        list here.
       </p></div></li><li class="step"><p>
       <code class="literal">[OPTIONAL]</code> If you want your volumes to use a default
       volume type, then enter the name of the volume type in the
       <code class="literal">[DEFAULT]</code> section with the syntax below.
       <span class="bold"><strong>Remember this value for when you
       create your volume type in the next section.</strong></span>
      </p><div id="id-1.4.5.17.3.6.4.2.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        If you do not specify a default type then your volumes will default
        unpredictably. We recommended that you create a volume type that meets
        the needs of your environment and specify it here.
       </p></div><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type&gt;</pre></div></li><li class="step"><p>
       Uncomment the <code class="literal">StoreServ (3par) iscsi cluster</code> section
       and fill the values per your cluster information. Storage performance
       can be improved by enabling the <code class="literal">Image-Volume</code>
       cache. Here is an example:
      </p><div class="verbatim-wrap"><pre class="screen">[3par_FC]
san_ip: &lt;3par-san-ipaddr&gt;
san_login: &lt;3par-san-username&gt;
san_password: &lt;3par-san-password&gt;
hpe3par_username: &lt;3par-username&gt;
hpe3par_password: &lt;hpe3par_password&gt;
hpe3par_api_url: https://&lt;3par-san-ipaddr&gt;:8080/api/v1
hpe3par_cpg: &lt;3par-cpg-name-1&gt;[,&lt;3par-cpg-name-2&gt;, ...]
volume_backend_name: &lt;3par-backend-name&gt;
volume_driver = cinder.volume.drivers.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver
image_volume_cache_enabled = True</pre></div></li></ol><div id="id-1.4.5.17.3.6.4.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      Do not use <code class="literal">backend_host</code> variable in
      <code class="literal">cinder.conf</code> file. If <code class="literal">backend_host</code>
      is set, it will override the [DEFAULT]/host value which <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
      is dependent on.
     </p></div></li><li class="step"><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Run the following playbook to complete the configuration:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect2" id="config-iscsi" data-id-title="Configure 3PAR iSCSI as Cinder backend"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.5 </span><span class="title-name">Configure 3PAR iSCSI as Cinder backend</span></span> <a title="Permalink" class="permalink" href="#config-iscsi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_3par.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You must modify the <code class="literal">cinder.conf.j2</code> to configure the iSCSI
   details.
  </p><p>
   Perform the following steps to configure 3PAR iSCSI as Cinder backend:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Make the following changes to the
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code> file:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Add your 3PAR backend to the <code class="literal">enabled_backends</code>
       section:
      </p><div class="verbatim-wrap"><pre class="screen"># Configure the enabled backends
enabled_backends=3par_iSCSI</pre></div></li><li class="step"><p>
       Uncomment the <code class="literal">StoreServ (3par) iscsi cluster</code> section
       and fill the values per your cluster information. Here is an example:
      </p><div class="verbatim-wrap"><pre class="screen">[3par_iSCSI]
san_ip: &lt;3par-san-ipaddr&gt;
san_login: &lt;3par-san-username&gt;
san_password: &lt;3par-san-password&gt;
hpe3par_username: &lt;3par-username&gt;
hpe3par_password: &lt;hpe3par_password&gt;
hpe3par_api_url: https://&lt;3par-san-ipaddr&gt;:8080/api/v1
hpe3par_cpg: &lt;3par-cpg-name-1&gt;[,&lt;3par-cpg-name-2&gt;, ...]
volume_backend_name: &lt;3par-backend-name&gt;
volume_driver: cinder.volume.drivers.san.hp.hp_3par_iscsi.hpe3parISCSIDriver
hpe3par_iscsi_ips: &lt;3par-ip-address-1&gt;[,&lt;3par-ip-address-2&gt;,&lt;3par-ip-address-3&gt;, ...]
hpe3par_iscsi_chap_enabled=true</pre></div><div id="id-1.4.5.17.3.7.4.2.2.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        Do not use <code class="literal">backend_host</code> variable in
        <code class="literal">cinder.conf</code> file. If <code class="literal">backend_host</code>
        is set, it will override the [DEFAULT]/host value which <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
        is dependent on.
       </p></div></li></ol></li><li class="step"><p>
     Commit your configuration your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "&lt;commit message&gt;"</pre></div></li><li class="step"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     When you run the configuration processor you will be prompted for two
     passwords. Enter the first password to make the configuration processor
     encrypt its sensitive data, which consists of the random inter-service
     passwords that it generates and the Ansible group_vars and host_vars that
     it produces for subsequent deploy runs. You will need this key for
     subsequent Ansible deploy runs and subsequent configuration processor
     runs. If you wish to change an encryption password that you have already
     used when running the configuration processor then enter the new password
     at the second prompt, otherwise press <span class="keycap">Enter</span>.
    </p><p>
     For CI purposes you can specify the required passwords on the ansible
     command line. For example, the command below will disable encryption by
     the configuration processor
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
     If you receive an error during either of these steps then there is an
     issue with one or more of your configuration files. We recommend that you
     verify that all of the information in each of your configuration files is
     correct for your environment and then commit those changes to git using
     the instructions above.
    </p></li><li class="step"><p>
     Run the following command to create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Run the following command to complete the configuration:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect2" id="idg-installation-installation-configure-3par-xml-16" data-id-title="Post-Installation Tasks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.6 </span><span class="title-name">Post-Installation Tasks</span></span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure-3par-xml-16">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_3par.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After configuring 3PAR as your Block Storage backend, perform the
   following tasks:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 7 “Managing Block Storage”, Section 7.1 “Managing Block Storage using Cinder”, Section 7.1.2 “Creating a Volume Type for your Volumes”</span>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-verify-block-storage-volume" title="27.1. Verifying Your Block Storage Backend">Section 27.1, “Verifying Your Block Storage Backend”</a>
    </p></li></ul></div></section></section><section class="sect1" id="ironic-oneview-integration" data-id-title="Ironic HPE OneView Integration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.2 </span><span class="title-name">Ironic HPE OneView Integration</span></span> <a title="Permalink" class="permalink" href="#ironic-oneview-integration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic_oneview_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports integration of Ironic (Baremetal) service with
  HPE OneView using <span class="emphasis"><em>agent_pxe_oneview</em></span> driver. Please refer to
  <a class="link" href="https://docs.openstack.org/developer/ironic/drivers/oneview.html" target="_blank">OpenStack
  Documentation</a> for more information.
 </p><section class="sect2" id="id-1.4.5.17.4.3" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.2.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.17.4.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic_oneview_integration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="orderedlist" id="prereq-list"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> with entry-scale-ironic-flat-network or
     entry-scale-ironic-multi-tenancy model.
    </p></li><li class="listitem"><p>
     HPE OneView 3.0 instance is running and connected to management network.
    </p></li><li class="listitem"><p>
     HPE OneView configuration is set into
     <code class="literal">definition/data/ironic/ironic_config.yml</code> (and
     <code class="literal">ironic-reconfigure.yml</code> playbook ran if needed). This
     should enable <span class="emphasis"><em>agent_pxe_oneview</em></span> driver in ironic
     conductor.
    </p></li><li class="listitem"><p>
     Managed node(s) should support PXE booting in legacy BIOS mode.
    </p></li><li class="listitem"><p>
     Managed node(s) should have PXE boot NIC listed first. That is, embedded
     1Gb NIC must be disabled (otherwise it always goes first).
    </p></li></ol></div></section><section class="sect2" id="id-1.4.5.17.4.4" data-id-title="Integrating with HPE OneView"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.2.2 </span><span class="title-name">Integrating with HPE OneView</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.17.4.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic_oneview_integration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On the Cloud Lifecycle Manager, open the file
     <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen">~$ cd ~/openstack
vi my_cloud/definition/data/ironic/ironic_config.yml</pre></div></li><li class="step"><p>
     Modify the settings listed below:
    </p><ol type="a" class="substeps"><li class="step"><p>
       <code class="literal">enable_oneview</code>: should be set to "true" for HPE OneView
       integration
      </p></li><li class="step"><p>
       <code class="literal">oneview_manager_url</code>: HTTPS endpoint of HPE OneView
       management interface, for example:
       <span class="bold"><strong>https://10.0.0.10/</strong></span>
      </p></li><li class="step"><p>
       <code class="literal">oneview_username</code>: HPE OneView username, for example:
       <span class="bold"><strong>Administrator</strong></span>
      </p></li><li class="step"><p>
       <code class="literal">oneview_encrypted_password</code>: HPE OneView password in
       encrypted or clear text form. The encrypted form is distinguished by
       presence of <code class="literal">@ardana@</code> at the beginning of the
       string. The encrypted form can be created by running the
       <code class="command">ardanaencrypt.py</code>
       program. This program is shipped as part of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and can be found in
       <code class="filename">~/openstack/ardana/ansible</code> directory on Cloud Lifecycle Manager.
      </p></li><li class="step"><p>
       <code class="literal">oneview_allow_insecure_connections</code>: should be set to
       "true" if HPE OneView is using self-generated certificate.
      </p></li></ol></li><li class="step"><p>
     Once you have saved your changes and exited the editor, add files, commit
     changes to local git repository, and run
     <code class="literal">config-processor-run.yml</code> and
     <code class="literal">ready-deployment.yml</code> playbooks, as described in
     <a class="xref" href="#using-git" title="Chapter 10. Using Git for Configuration Management">Chapter 10, <em>Using Git for Configuration Management</em></a>.
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack$ git add my_cloud/definition/data/ironic/ironic_config.yml
~/openstack$ cd ardana/ansible
~/openstack/ardana/ansible$ ansible-playbook -i hosts/localhost \
  config-processor-run.yml
...
~/openstack/ardana/ansible$ ansible-playbook -i hosts/localhost \
  ready-deployment.yml</pre></div></li><li class="step"><p>
     Run ironic-reconfigure.yml playbook.
    </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/scratch/ansible/next/ardana/ansible/

# This is needed if password was encrypted in ironic_config.yml file
~/scratch/ansible/next/ardana/ansible$ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=your_password_encrypt_key
~/scratch/ansible/next/ardana/ansible$ ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml
...</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.4.5.17.4.5" data-id-title="Registering Node in HPE OneView"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.2.3 </span><span class="title-name">Registering Node in HPE OneView</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.17.4.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic_oneview_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the HPE OneView web interface:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Navigate to
     <span class="guimenu">Menu</span> › <span class="guimenu">Server Hardware</span>.
     Add new <span class="guimenu">Server Hardware</span> item, using
     managed node IPMI IP and credentials. If this is the first node of this
     type being added, corresponding
     <span class="guimenu">Server Hardware Type</span> will be created automatically.
    </p></li><li class="step"><p>
     Navigate to
     <span class="guimenu">Menu</span> › <span class="guimenu">Server Profile Template</span>.
     Add <span class="guimenu">Server Profile Template</span>. Use
     <span class="guimenu">Server Hardware Type</span> corresponding to node being
     registered. In <span class="guimenu">BIOS Settings</span> section, set
     <span class="guimenu">Manage Boot Mode</span> and <span class="guimenu">Manage Boot
     Order</span> options must be turned on:
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/media-ironic-OneViewWebRegister.png"><img src="images/media-ironic-OneViewWebRegister.png" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Verify that node is powered off. Power the node off if needed.
    </p></li></ol></div></div></section><section class="sect2" id="id-1.4.5.17.4.6" data-id-title="Provisioning Ironic Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.2.4 </span><span class="title-name">Provisioning Ironic Node</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.17.4.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ironic_oneview_integration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Login to the Cloud Lifecycle Manager and source respective credentials file
     (for example <code class="filename">service.osrc</code> for admin account).
    </p></li><li class="step"><p>
     Review glance images with <code class="literal">glance image list</code>
    </p><div class="verbatim-wrap"><pre class="screen">$ glance image list
+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| c61da588-622c-4285-878f-7b86d87772da | cirros-0.3.4-x86_64      |
+--------------------------------------+--------------------------+</pre></div><p>
     Ironic deploy images (boot image,
     <code class="literal">ir-deploy-kernel</code>, <code class="literal">ir-deploy-ramdisk</code>,
     <code class="literal">ir-deploy-iso</code>) are created automatically. The
     <code class="systemitem">agent_pxe_oneview</code> Ironic driver requires
     <code class="systemitem">ir-deploy-kernel</code> and
     <code class="systemitem">ir-deploy-ramdisk</code> images.
    </p></li><li class="step"><p>
     Create node using <code class="literal">agent_pxe_oneview</code> driver.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 node-create -d agent_pxe_oneview --name test-node-1 \
  --network-interface neutron -p memory_mb=131072 -p cpu_arch=x86_64 -p local_gb=80 -p cpus=2 \
  -p 'capabilities=boot_mode:bios,boot_option:local,server_hardware_type_uri:\
     /rest/server-hardware-types/E5366BF8-7CBF-48DF-A752-8670CF780BB2,server_profile_template_uri:\
     /rest/server-profile-templates/00614918-77f8-4146-a8b8-9fc276cd6ab2' \
  -i 'server_hardware_uri=/rest/server-hardware/32353537-3835-584D-5135-313930373046' \
  -i dynamic_allocation=True \
  -i deploy_kernel=633d379d-e076-47e6-b56d-582b5b977683 \
  -i deploy_ramdisk=d5828785-edf2-49fa-8de2-3ddb7f3270d5

+-------------------+--------------------------------------------------------------------------+
| Property          | Value                                                                    |
+-------------------+--------------------------------------------------------------------------+
| chassis_uuid      |                                                                          |
| driver            | agent_pxe_oneview                                                        |
| driver_info       | {u'server_hardware_uri': u'/rest/server-                                 |
|                   | hardware/32353537-3835-584D-5135-313930373046', u'dynamic_allocation':   |
|                   | u'True', u'deploy_ramdisk': u'd5828785-edf2-49fa-8de2-3ddb7f3270d5',     |
|                   | u'deploy_kernel': u'633d379d-e076-47e6-b56d-582b5b977683'}               |
| extra             | {}                                                                       |
| name              | test-node-1                                                              |
| network_interface | neutron                                                                  |
| properties        | {u'memory_mb': 131072, u'cpu_arch': u'x86_64', u'local_gb': 80, u'cpus': |
|                   | 2, u'capabilities':                                                      |
|                   | u'boot_mode:bios,boot_option:local,server_hardware_type_uri:/rest        |
|                   | /server-hardware-types/E5366BF8-7CBF-                                    |
|                   | 48DF-A752-8670CF780BB2,server_profile_template_uri:/rest/server-profile- |
|                   | templates/00614918-77f8-4146-a8b8-9fc276cd6ab2'}                         |
| resource_class    | None                                                                     |
| uuid              | c202309c-97e2-4c90-8ae3-d4c95afdaf06                                     |
+-------------------+--------------------------------------------------------------------------+</pre></div><div id="id-1.4.5.17.4.6.2.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        For deployments created via Ironic/HPE OneView integration,
        <code class="literal">memory_mb</code> property must reflect physical amount of
        RAM installed in the managed node. That is, for a server with 128 Gb of RAM
        it works out to 132*1024=13072.
       </p></li><li class="listitem"><p>
        Boot mode in capabilities property must reflect boot mode used by the
        server, that is 'bios' for Legacy BIOS and 'uefi' for UEFI.
       </p></li><li class="listitem"><p>
        Values for <code class="literal">server_hardware_type_uri</code>,
        <code class="literal">server_profile_template_uri</code> and
        <code class="literal">server_hardware_uri</code> can be grabbed from browser URL
        field while navigating to respective objects in HPE OneView UI. URI
        corresponds to the part of URL which starts form the token
        <code class="literal">/rest</code>.
        That is, the URL
        <code class="literal">https://oneview.mycorp.net/#/profile-templates/show/overview/r/rest/server-profile-templates/12345678-90ab-cdef-0123-012345678901</code>
        corresponds to the URI
        <code class="literal">/rest/server-profile-templates/12345678-90ab-cdef-0123-012345678901</code>.
       </p></li><li class="listitem"><p>
        Grab IDs of <code class="literal">deploy_kernel</code> and
        <code class="literal">deploy_ramdisk</code> from <span class="bold"><strong>glance
        image list</strong></span> output above.
       </p></li></ul></div></div></li><li class="step"><p>
     Create port.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 port-create \
  --address aa:bb:cc:dd:ee:ff \
  --node c202309c-97e2-4c90-8ae3-d4c95afdaf06 \
  -l switch_id=ff:ee:dd:cc:bb:aa \
  -l switch_info=MY_SWITCH \
  -l port_id="Ten-GigabitEthernet 1/0/1" \
  --pxe-enabled true
+-----------------------+----------------------------------------------------------------+
| Property              | Value                                                          |
+-----------------------+----------------------------------------------------------------+
| address               | 8c:dc:d4:b5:7d:1c                                              |
| extra                 | {}                                                             |
| local_link_connection | {u'switch_info': u'C20DATA', u'port_id': u'Ten-GigabitEthernet |
|                       | 1/0/1',    u'switch_id': u'ff:ee:dd:cc:bb:aa'}                 |
| node_uuid             | c202309c-97e2-4c90-8ae3-d4c95afdaf06                           |
| pxe_enabled           | True                                                           |
| uuid                  | 75b150ef-8220-4e97-ac62-d15548dc8ebe                           |
+-----------------------+----------------------------------------------------------------+</pre></div><div id="id-1.4.5.17.4.6.2.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
      Ironic Multi-Tenancy networking model is used in this example.
      Therefore, ironic port-create command contains information about the
      physical switch. HPE OneView integration can also be performed using the
      Ironic Flat Networking model. For more information, see
      <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 9 “Example Configurations”, Section 9.6 “Ironic Examples”</span>.
     </p></div></li><li class="step"><p>
     Move node to manageable provisioning state. The connectivity between
     Ironic and HPE OneView will be verified, Server Hardware Template settings
     validated, and Server Hardware power status retrieved from HPE OneView and set
     into the Ironic node.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 manage</pre></div></li><li class="step"><p>
     Verify that node power status is populated.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-show test-node-1
+-----------------------+-----------------------------------------------------------------------+
| Property              | Value                                                                 |
+-----------------------+-----------------------------------------------------------------------+
| chassis_uuid          |                                                                       |
| clean_step            | {}                                                                    |
| console_enabled       | False                                                                 |
| created_at            | 2017-06-30T21:00:26+00:00                                             |
| driver                | agent_pxe_oneview                                                     |
| driver_info           | {u'server_hardware_uri': u'/rest/server-                              |
|                       | hardware/32353537-3835-584D-5135-313930373046', u'dynamic_allocation':|
|                       | u'True', u'deploy_ramdisk': u'd5828785-edf2-49fa-8de2-3ddb7f3270d5',  |
|                       | u'deploy_kernel': u'633d379d-e076-47e6-b56d-582b5b977683'}            |
| driver_internal_info  | {}                                                                    |
| extra                 | {}                                                                    |
| inspection_finished_at| None                                                                  |
| inspection_started_at | None                                                                  |
| instance_info         | {}                                                                    |
| instance_uuid         | None                                                                  |
| last_error            | None                                                                  |
| maintenance           | False                                                                 |
| maintenance_reason    | None                                                                  |
| name                  | test-node-1                                                           |
| network_interface     |                                                                       |
| power_state           | power off                                                             |
| properties            | {u'memory_mb': 131072, u'cpu_arch': u'x86_64', u'local_gb': 80,       |
|                       | u'cpus': 2, u'capabilities':                                          |
|                       | u'boot_mode:bios,boot_option:local,server_hardware_type_uri:/rest     |
|                       | /server-hardware-types/E5366BF8-7CBF-                                 |
|                       | 48DF-A752-86...BB2,server_profile_template_uri:/rest/server-profile-  |
|                       | templates/00614918-77f8-4146-a8b8-9fc276cd6ab2'}                      |
| provision_state       | manageable                                                            |
| provision_updated_at  | 2017-06-30T21:04:43+00:00                                             |
| raid_config           |                                                                       |
| reservation           | None                                                                  |
| resource_class        |                                                                       |
| target_power_state    | None                                                                  |
| target_provision_state| None                                                                  |
| target_raid_config    |                                                                       |
| updated_at            | 2017-06-30T21:04:43+00:00                                             |
| uuid                  | c202309c-97e2-4c90-8ae3-d4c95afdaf06                                  |
+-----------------------+-----------------------------------------------------------------------+</pre></div></li><li class="step"><p>
     Move node to available provisioning state. The Ironic node will be
     reported to Nova as available.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 provide</pre></div></li><li class="step"><p>
     Verify that node resources were added to Nova hypervisor stats.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova hypervisor-stats
+----------------------+--------+
| Property             | Value  |
+----------------------+--------+
| count                | 1      |
| current_workload     | 0      |
| disk_available_least | 80     |
| free_disk_gb         | 80     |
| free_ram_mb          | 131072 |
| local_gb             | 80     |
| local_gb_used        | 0      |
| memory_mb            | 131072 |
| memory_mb_used       | 0      |
| running_vms          | 0      |
| vcpus                | 2      |
| vcpus_used           | 0      |
+----------------------+--------+</pre></div></li><li class="step"><p>
     Create Nova flavor.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova flavor-create m1.ironic auto 131072 80 2
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
| ID          | Name      | Mem_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
| 33c8...f8d8 | m1.ironic | 131072 | 80   | 0         |      | 2     | 1.0         | True      |
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
$ nova flavor-key m1.ironic set capabilities:boot_mode="bios"
$ nova flavor-key m1.ironic set capabilities:boot_option="local"
$ nova flavor-key m1.ironic set cpu_arch=x86_64</pre></div><div id="id-1.4.5.17.4.6.2.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      All parameters (specifically, amount of RAM and boot mode) must
      correspond to ironic node parameters.
     </p></div></li><li class="step"><p>
     Create Nova keypair if needed.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova keypair-add ironic_kp --pub-key ~/.ssh/id_rsa.pub</pre></div></li><li class="step"><p>
     Boot Nova instance.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova boot --flavor m1.ironic --image d6b5...e942 --key-name ironic_kp \
  --nic net-id=5f36...dcf3 test-node-1
+-------------------------------+-----------------------------------------------------+
| Property                      | Value                                               |
+-------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig             | MANUAL                                              |
| OS-EXT-AZ:availability_zone   |                                                     |
| OS-EXT-SRV-ATTR:host          | -                                                   |
| OS-EXT-SRV-ATTR:              |                                                     |
|       hypervisor_hostname     | -                                                   |
| OS-EXT-SRV-ATTR:instance_name |                                                     |
| OS-EXT-STS:power_state        | 0                                                   |
| OS-EXT-STS:task_state         | scheduling                                          |
| OS-EXT-STS:vm_state           | building                                            |
| OS-SRV-USG:launched_at        | -                                                   |
| OS-SRV-USG:terminated_at      | -                                                   |
| accessIPv4                    |                                                     |
| accessIPv6                    |                                                     |
| adminPass                     | pE3m7wRACvYy                                        |
| config_drive                  |                                                     |
| created                       | 2017-06-30T21:08:42Z                                |
| flavor                        | m1.ironic (33c81884-b8aa-46...3b72f8d8)             |
| hostId                        |                                                     |
| id                            | b47c9f2a-e88e-411a-abcd-6172aea45397                |
| image                         | Ubuntu Trusty 14.04 BIOS (d6b5d971-42...5f2d88e942) |
| key_name                      | ironic_kp                                           |
| metadata                      | {}                                                  |
| name                          | test-node-1                                         |
| os-extended-volumes:          |                                                     |
|       volumes_attached        | []                                                  |
| progress                      | 0                                                   |
| security_groups               | default                                             |
| status                        | BUILD                                               |
| tenant_id                     | c8573f7026d24093b40c769ca238fddc                    |
| updated                       | 2017-06-30T21:08:42Z                                |
| user_id                       | 2eae99221545466d8f175eeb566cc1b4                    |
+-------------------------------+-----------------------------------------------------+</pre></div><p>
     During nova instance boot, the following operations will be performed by
     Ironic via HPE OneView REST API.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       In HPE OneView, new Server Profile is generated for specified Server
       Hardware, using specified Server Profile Template. Boot order in Server
       Profile is set to list PXE as the first boot source.
      </p></li><li class="listitem"><p>
       The managed node is powered on and boots IPA image from PXE.
      </p></li><li class="listitem"><p>
       IPA image writes user image onto disk and reports success back to
       Ironic.
      </p></li><li class="listitem"><p>
       Ironic modifies Server Profile in HPE OneView to list 'Disk' as default boot
       option.
      </p></li><li class="listitem"><p>
       Ironic reboots the node (via HPE OneView REST API call).
      </p></li></ul></div></li></ol></div></div></section></section><section class="sect1" id="ses-integration" data-id-title="SUSE Enterprise Storage Integration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.3 </span><span class="title-name">SUSE Enterprise Storage Integration</span></span> <a title="Permalink" class="permalink" href="#ses-integration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ses_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The current version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports integration with SUSE Enterprise Storage (SES).
  Integrating SUSE Enterprise Storage enables Ceph block storage as well as object and image
  storage services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><section class="sect2" id="ses-installation" data-id-title="Enabling SUSE Enterprise Storage Integration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.1 </span><span class="title-name">Enabling SUSE Enterprise Storage Integration</span></span> <a title="Permalink" class="permalink" href="#ses-installation">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ses_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The SUSE Enterprise Storage integration is provided through the <span class="package">ardana-ses</span>
   RPM package. This package is included in the
   <code class="systemitem">patterns-cloud-ardana</code> pattern, its installation is
   covered in <a class="xref" href="#cha-depl-dep-inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>. The update repositories and
   the installation covered there are required to support SUSE Enterprise Storage integration.
   The latest updates should be applied before proceeding.
  </p></section><section class="sect2" id="ses-config" data-id-title="Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.2 </span><span class="title-name">Configuration</span></span> <a title="Permalink" class="permalink" href="#ses-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ses_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After the SUSE Enterprise Storage integration package has been installed, it must be
   configured. Files that contain relevant SUSE Enterprise Storage/Ceph deployment information
   must be placed into a directory on the deployer node. This includes the
   configuration file that describes various aspects of the Ceph environment
   as well as keyrings for each user and pool created in the Ceph
   environment. In addition to that, you need to edit the
   <code class="filename">settings.yml</code> file to enable the SUSE Enterprise Storage integration to
   run and update all of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> service configuration files.
  </p><p>
   The <code class="filename">settings.yml</code> file resides in the
   <code class="filename">~/openstack/my_cloud/config/ses/</code> directory. Open the
   file for editing, uncomment the <code class="literal">ses_config_path:</code>
   parameter to specify the location on the deployer host containing the
   <code class="filename">ses_config.yml</code> file and all Ceph keyring files. For example:
  </p><div class="verbatim-wrap"><pre class="screen"># the directory where the ses config files are.
ses_config_path: /var/lib/ardana/openstack/my_cloud/config/ses/
ses_config_file: ses_config.yml

# Allow nova libvirt images_type to be set to rbd?
# Set this to false, if you only want rbd_user and rbd_secret to be set
# in the [libvirt] section of hypervisor.conf
ses_nova_set_images_type: True

# The unique uuid for use with virsh for cinder and nova
ses_secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</pre></div><div id="id-1.4.5.17.5.4.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    If you are integrating with SUSE Enterprise Storage and do not want to store Nova images
    in Ceph, change the line in <code class="filename">settings.yml</code> from
    <code class="literal">ses_nova_set_images_type: True</code> to
    <code class="literal">ses_nova_set_images_type: False</code>
   </p><p>
    For security reasons, you should use a unique UUID in the
    <code class="filename">settings.yml</code> file for
    <code class="literal">ses_secret_id</code>, replacing the fixed, hard-coded UUID in
    that file. You can generate a UUID that will be unique to your deployment
    using the command <code class="command">uuidgen</code>.
   
   </p></div><p>
   For SES deployments that have version 5.5 and higher, there is a Salt runner
   that can create all the users, keyrings, and pools. It will also generate a yaml
   configuration that is needed to integrate with SUSE <span class="productname">OpenStack</span> Cloud. The integration
   runner will create the <code class="literal">cinder</code>, <code class="literal">cinder-backup</code>, and <code class="literal">glance</code>
   Ceph users. Both the Cinder and Nova services will have the
   same user, as Cinder needs access to create objects that Nova
   uses.
  </p><p>
   Login in as root to run the SES 5.5 Salt runner on the salt admin host.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run --out=yaml openstack.integrate prefix=<em class="replaceable">mycloud</em></pre></div><p>
   The prefix parameter allows pools to be created with the specified prefix.
   In this way, multiple cloud deployments can use different users and pools on
   the same SES deployment.
  </p><p>
   The sample yaml output:
  </p><div class="verbatim-wrap"><pre class="screen">ceph_conf:
 cluster_network: 10.84.56.0/21
 fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
 mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
 mon_initial_members: ses-osd1, ses-osd2, ses-osd3
 public_network: 10.84.56.0/21
 cinder:
 key: AQCdfIRaxefEMxAAW4zp2My/5HjoST2Y8mJg8A==
 rbd_store_pool: mycloud-cinder
 rbd_store_user: cinder
 cinder-backup:
 key: AQBb8hdbrY2bNRAAqJC2ZzR5Q4yrionh7V5PkQ==
 rbd_store_pool: mycloud-backups
 rbd_store_user: cinder-backup
 glance:
 key: AQD9eYRachg1NxAAiT6Hw/xYDA1vwSWLItLpgA==
 rbd_store_pool: mycloud-glance
 rbd_store_user: glance
 nova:
 rbd_store_pool: mycloud-nova
 radosgw_urls:
     - http://10.84.56.7:80/swift/v1
     - http://10.84.56.8:80/swift/v1</pre></div><p>
      After you have run the <code class="literal">openstack.integrate</code> runner,
      copy the yaml output into the new <code class="filename">ses_config.yml</code>
      file, and save this file in the path specified in the
      <code class="filename">settings.yml</code> file on the deployer node. In this
      case, the file <code class="filename">ses_config.yml</code> must be saved in the
      <code class="filename">/var/lib/ardana/openstack/my_cloud/config/ses/</code>
      directory on the deployer node.
     </p><p>
      For SUSE Enterprise Storage/Ceph deployments that have version older than 5.5, the
      following applies. For Ceph, it is necessary to create pools and users
      to allow the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services to use the SUSE Enterprise Storage/Ceph
      cluster. Pools and users must be created for the Cinder,
      Cinder backup, and Glance services. Both the Cinder and
      Nova services must have the same user, as Cinder needs access
      to create objects that Nova uses.  Instructions for creating and
      managing pools, users and keyrings is covered in the SUSE Enterprise Storage documentation
      under <a class="link" href="https://documentation.suse.com/en-us/ses/5.5/single-html/ses-admin/#storage-cephx-keymgmt" target="_blank">Key
      Management</a>.
     </p><p>
      Example of <code class="filename">ses_config.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ses_cluster_configuration:
    ses_cluster_name: ceph
    ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"

    conf_options:
        ses_fsid: d5d7c7cb-5858-3218-a36f-d028df7b1111
        ses_mon_initial_members: ses-osd2, ses-osd3, ses-osd1
        ses_mon_host: 192.168.56.8, 192.168.56.9, 192.168.56.7
        ses_public_network: 192.168.56.0/21
        ses_cluster_network: 192.168.56.0/21

    cinder:
        rbd_store_pool: cinder
        rbd_store_pool_user: cinder
        keyring_file_name: ceph.client.cinder.keyring

    cinder-backup:
        rbd_store_pool: backups
        rbd_store_pool_user: cinder_backup
        keyring_file_name: ceph.client.cinder-backup.keyring

    # Nova uses the cinder user to access the nova pool, cinder pool
    # So all we need here is the nova pool name.
    nova:
        rbd_store_pool: nova

    glance:														
        rbd_store_pool: glance												
        rbd_store_pool_user: glance
        keyring_file_name: ceph.client.glance.keyring</pre></div><p>
	 Example contents of the directory specified in <code class="filename">settings.yml</code> file:
	</p><div class="verbatim-wrap"><pre class="screen">ardana &gt; ~/openstack/my_cloud/config/ses&gt; ls -al
ceph.client.cinder-backup.keyring											
ceph.client.cinder.keyring												
ceph.client.glance.keyring					
ses_config.yml</pre></div><p>
 Modify the <code class="literal">glance_default_store</code> option in <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>:
</p><div class="verbatim-wrap"><pre class="screen">.
.
            - rabbitmq
#            - glance-api
            - glance-api:
                glance_default_store: 'rbd'
            - glance-registry
            - glance-client</pre></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Commit your configuration to your local git repo:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "add SES integration"</pre></div></li><li class="step"><p>
     Run the configuration processor.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     Create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Run a series of reconfiguration playbooks.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ses-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Configuring SUSE Enterprise Storage for Integration with RADOS
   Gateway</strong></span>
  </p><p>
   RADOS gateway integration can be enabled (disabled) by adding (removing)
   the following line in the <code class="filename">ses_config.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"</pre></div><p>
   If RADOS gateway integration is enabled, additional SUSE Enterprise Storage configuration is
   needed. RADOS gateway must be configured to use Keystone for
   authentication. This is done by adding the configuration statements below to
   the rados section of <code class="filename">ceph.conf</code> on the RADOS node.
  </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.<em class="replaceable">HOSTNAME</em>]
rgw frontends = "civetweb port=80+443s"
rgw enable usage log = true
rgw keystone url = <em class="replaceable">KEYSTONE_ENDPOINT</em> (for example:
https://192.168.24.204:5000)
rgw keystone admin user = <em class="replaceable">KEYSTONE_ADMIN_USER</em>
rgw keystone admin password = <em class="replaceable">KEYSTONE_ADMIN_PASSWORD</em>
rgw keystone admin project = <em class="replaceable">KEYSTONE_ADMIN_PROJECT</em>
rgw keystone admin domain = <em class="replaceable">KEYSTONE_ADMIN_DOMAIN</em>
rgw keystone api version = 3
rgw keystone accepted roles = admin,Member,_member_
rgw keystone accepted admin roles = admin
rgw keystone revocation interval = 0
rgw keystone verify ssl = false # If keystone is using self-signed
   certificate</pre></div><p>
   After making these changes to <code class="filename">ceph.conf</code>, the RADOS
   gateway service needs to be restarted.
  </p><p>
   Enabling RADOS gateway replaces the existing Object Storage endpoint with the
   RADOS gateway endpoint.
  </p><p>
   <span class="bold"><strong>Enabling HTTPS, Creating and Importing a
   Certificate</strong></span>
  </p><p>
   SUSE Enterprise Storage integration uses the HTTPS protocol to connect to the RADOS gateway.
   However, with SUSE Enterprise Storage 5, HTTPS is not enabled by default. To enable the
   gateway role to communicate securely using SSL, you need to either have a
   CA-issued certificate or create a self-signed one. Instructions for both are
   available in the
   <a class="link" href="https://documentation.suse.com/ses/5.5/single-html/ses-admin/#ceph-rgw-access" target="_blank">SUSE Enterprise Storage
   documentation</a>.
  </p><p>
   The certificate needs to be installed on your Cloud Lifecycle Manager. On the Cloud Lifecycle Manager, copy the
   cert to <code class="filename">/tmp/ardana_tls_cacerts</code>. Then deploy it.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tls-trust-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tls-reconfigure.yml</pre></div><p>
   When creating the certificate, the <code class="literal">subjectAltName</code> must
   match the <code class="literal">ses_radosgw_url</code> entry in
   <code class="filename">ses_config.yml</code>. Either an IP address or FQDN can be
   used, but these values must be the same in both places.
  </p></section><section class="sect2" id="id-1.4.5.17.5.5" data-id-title="Deploying SUSE Enterprise Storage Configuration for RADOS Integration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.3 </span><span class="title-name">Deploying SUSE Enterprise Storage Configuration for RADOS Integration</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.17.5.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ses_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following steps will deploy your configuration.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Commit your configuration to your local git repo.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "add SES integration"</pre></div></li><li class="step"><p>
     Run the configuration processor.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     Create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Run a series of reconfiguration playbooks.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ses-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li><li class="step"><p>
     Reconfigure the Cloud Lifecycle Manager to complete the deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.4.5.17.5.6" data-id-title="Enable Copy-On-Write Cloning of Images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.4 </span><span class="title-name">Enable Copy-On-Write Cloning of Images</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.17.5.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ses_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Due to a security issue described in <a class="link" href="http://docs.ceph.com/docs/master/rbd/rbd-openstack/?highlight=uuid#enable-copy-on-write-cloning-of-images" target="_blank">http://docs.ceph.com/docs/master/rbd/rbd-openstack/?highlight=uuid#enable-copy-on-write-cloning-of-images</a>, we do not recommend the copy-on-write cloning of images when
    Glance and Cinder are both using a Ceph back-end.
    However, if you want to use this feature for faster operation,
    you can enable it as follows.</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Open the
       <code class="literal">~/openstack/my_cloud/config/glance/glance-api.conf.j2</code>
       file for editing and add <code class="literal">show_image_direct_url = True</code>
       under the <code class="literal">[DEFAULT]</code> section.
      </p></li><li class="step"><p>
       Commit changes:</p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Enable Copy-on-Write Cloning"</pre></div></li><li class="step"><p>
       Run the required playbooks:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div><div id="id-1.4.5.17.5.6.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
     Note that this exposes the back-end location via Glance's API, so the
     end-point should not be publicly accessible when Copy-On-Write image
     cloning is enabled.
    </p></div></section><section class="sect2" id="id-1.4.5.17.5.7" data-id-title="Improve SUSE Enterprise Storage Storage Performance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.5 </span><span class="title-name">Improve SUSE Enterprise Storage Storage Performance</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.17.5.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ses_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Enterprise Storage performance can be improved with Image-Volume cache. Be aware that
    Image-Volume cache and Copy-on-Write cloning cannot be used for the same
    storage back-end. For more information, see the <a class="link" href="https://docs.openstack.org/cinder/pike/admin/blockstorage-image-volume-cache.html" target="_blank">OpenStack
    documentation</a>.
   </p><p>
    Enable Image-Volume cache with the following steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the
      <code class="filename">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code>
      file for editing.
     </p></li><li class="step"><p>
      Add <code class="literal">image_volume_cache_enabled = True</code> option under the
      <code class="literal">[ses_ceph]</code> section.
     </p></li><li class="step"><p>
      Commit changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enable Image-Volume cache"</pre></div></li><li class="step"><p>
      Run the required playbooks:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></section></section></section><section class="chapter" id="troubleshooting-installation" data-id-title="Troubleshooting the Installation"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">23 </span><span class="title-name">Troubleshooting the Installation</span></span> <a title="Permalink" class="permalink" href="#troubleshooting-installation">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installation_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#sec-trouble-deployer-setup" title="23.1. Issues during Cloud Lifecycle Manager Setup">Section 23.1, “Issues during Cloud Lifecycle Manager Setup”</a>
   </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-trouble-config-processor" title="23.2. Issues while Updating Configuration Files">Section 23.2, “Issues while Updating Configuration Files”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-trouble-deploy-cloud" title="23.3. Issues while Deploying the Cloud">Section 23.3, “Issues while Deploying the Cloud”</a>
   </p></li></ul></div><section class="sect1" id="sec-trouble-deployer-setup" data-id-title="Issues during Cloud Lifecycle Manager Setup"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.1 </span><span class="title-name">Issues during Cloud Lifecycle Manager Setup</span></span> <a title="Permalink" class="permalink" href="#sec-trouble-deployer-setup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installation_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.4.2"><span class="name">Issue: Running the ardana-init.bash script when configuring your Cloud Lifecycle Manager does not complete</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.4.2">#</a></h5></div><p>
   Part of what the <code class="literal">ardana-init.bash</code> script does is
   install Git. So if your DNS server(s) is/are not specified in your
   <code class="filename">/etc/resolv.conf</code> file, is not valid, or is not
   functioning properly on your Cloud Lifecycle Manager, it will not be able to
   complete.
  </p><p>
   To resolve this issue, double check your nameserver in your
   <code class="filename">/etc/resolv.conf</code> file and then re-run the script.
  </p></section><section class="sect1" id="sec-trouble-config-processor" data-id-title="Issues while Updating Configuration Files"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.2 </span><span class="title-name">Issues while Updating Configuration Files</span></span> <a title="Permalink" class="permalink" href="#sec-trouble-config-processor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installation_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.5.2"><span class="name">Configuration Processor Fails Due to Wrong yml Format</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.5.2">#</a></h5></div><p>
   If you receive the error below when running the configuration processor then
   you may have a formatting error:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [fail msg="Configuration processor run failed, see log output above for
details"]</pre></div><p>
   First you should check the Ansible log in the location below for more
   details on which yml file in your input model has the error:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ansible/ansible.log</pre></div><p>
   Check the configuration file to locate and fix the error, keeping in mind
   the following tips below.
  </p><p>
   Check your files to ensure that they do not contain the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Non-ascii characters
    </p></li><li class="listitem"><p>
     Unneeded spaces
    </p></li></ul></div><p>
   Once you have fixed the formatting error in your files, commit the changes
   with these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Commit your changes to Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step"><p>
     Re-run the configuration processor playbook and confirm the error is not
     received again.
    </p></li></ol></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.5.12"><span class="name">Configuration processor fails with provider network OCTAVIA-MGMT-NET error</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.5.12">#</a></h5></div><p>
   If you receive the error below when running the configuration processor then
   you have not correctly configured your VLAN settings for Octavia.
  </p><div class="verbatim-wrap"><pre class="screen">################################################################################,
# The configuration processor failed.
#   config-data-2.0           ERR: Provider network OCTAVIA-MGMT-NET host_routes:
# destination '192.168.10.0/24' is not defined as a Network in the input model.
# Add 'external: True' to this host_route if this is for an external network.
################################################################################</pre></div><p>
   To resolve the issue, ensure that your settings in
   <code class="literal">~/openstack/my_cloud/definition/data/neutron/neutron_config.yml</code>
   are correct for the VLAN setup for Octavia.
  </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.5.16"><span class="name">Changes Made to your Configuration Files</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.5.16">#</a></h5></div><p>
   If you have made corrections to your configuration files and need to re-run
   the Configuration Processor, the only thing you need to do is commit your
   changes to your local Git repository:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "commit message"</pre></div><p>
   You can then re-run the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.5.21"><span class="name">Configuration Processor Fails Because Encryption Key Does Not Meet Requirements</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.5.21">#</a></h5></div><p>
   If you choose to set an encryption password when running the configuration
   processor, you may receive the following error if the chosen password does
   not meet the complexity requirements:
  </p><div class="verbatim-wrap"><pre class="screen">################################################################################
# The configuration processor failed.
#   encryption-key ERR: The Encryption Key does not meet the following requirement(s):
#       The Encryption Key must be at least 12 characters
#       The Encryption Key must contain at least 3 of following classes of characters:
#                           Uppercase Letters, Lowercase Letters, Digits, Punctuation
################################################################################</pre></div><p>
   If you receive the above error, run the configuration processor again and
   select a password that meets the complexity requirements detailed in the
   error message:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></section><section class="sect1" id="sec-trouble-deploy-cloud" data-id-title="Issues while Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.3 </span><span class="title-name">Issues while Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="#sec-trouble-deploy-cloud">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-installation_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.6.2"><span class="name">Issue: If the site.yml playbook fails, you can query the log for the reason</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.6.2">#</a></h5></div><p>
   Ansible is good about outputting the errors into the command line output,
   however if you would like to view the full log for any reason the location is:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ansible/ansible.log</pre></div><p>
   This log is updated real time as you run Ansible playbooks.
  </p><div id="id-1.4.5.18.6.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
    Use grep to parse through the log. Usage: <code class="literal">grep &lt;text&gt;
    ~/.ansible/ansible.log</code>
   </p></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.6.7"><span class="name">Issue: How to Wipe the Disks of your Machines</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.6.7">#</a></h5></div><p>
   If you have re-run the <code class="literal">site.yml</code> playbook, you may need to
   wipe the disks of your nodes
  </p><p>
   You should run the <code class="filename">wipe_disks.yml</code> playbook only after
   re-running the <code class="literal">bm-reimage.yml</code> playbook but before you
   re-run the <code class="literal">site.yml</code> playbook.
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
   The playbook will show you the disks to be wiped in the output and allow you
   to confirm that you want to complete this action or abort it if you do not
   want to proceed. You can optionally use the <code class="literal">--limit
   &lt;NODE_NAME&gt;</code> switch on this playbook to restrict it to
   specific nodes. This action will not affect the OS partitions on the servers.
  </p><p>
   If you receive an error stating that <code class="literal">osconfig</code> has already
   run on your nodes then you will need to remove the
   <code class="literal">/etc/ardana/osconfig-ran</code> file on each of the nodes you want
   to wipe with this command:
  </p><div class="verbatim-wrap"><pre class="screen">sudo rm /etc/ardana/osconfig-ran</pre></div><p>
   That will clear this flag and allow the disk to be wiped.
  </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.6.15"><span class="name">Issue: Freezer installation fails if an independent network is used for the External_API</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.6.15">#</a></h5></div><p>
   The Freezer installation fails if an independent network is used
   for the External_API. If you intend to deploy the External API
   on an independent network, the following changes need to be made:
  </p><p>
   In <code class="literal">roles/freezer-agent/defaults/main.yml</code> add the
   following line:
  </p><div class="verbatim-wrap"><pre class="screen">backup_freezer_api_url: "{{ FRE_API | item('advertises.vips.private[0].url', default=' ') }}"</pre></div><p>
   In <code class="literal">roles/freezer-agent/templates/backup.osrc.j2</code> add the
   following line:
  </p><div class="verbatim-wrap"><pre class="screen">export OS_FREEZER_URL={{ backup_freezer_api_url }}</pre></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.6.21"><span class="name">Error Received if Root Logical Volume is Too Small</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.6.21">#</a></h5></div><p>
   When running the <code class="literal">site.yml</code> playbook, you may receive a
   message that includes the error below if your root logical volume is too
   small. This error needs to be parsed out and resolved.
  </p><div class="verbatim-wrap"><pre class="screen">2015-09-29 15:54:03,022 p=26345 u=stack | stderr: New size given (7128 extents)
not larger than existing size (7629 extents)</pre></div><p>
   The error message may also reference the root volume:
  </p><div class="verbatim-wrap"><pre class="screen">"name": "root", "size": "10%"</pre></div><p>
   The problem here is that the root logical volume, as specified in the
   <code class="literal">disks_controller.yml</code> file, is set to
   <code class="literal">10%</code> of the overall physical volume and this value is too
   small.
  </p><p>
   To resolve this issue you need to ensure that the percentage is set properly
   for the size of your logical-volume. The default values in the configuration
   files is based on a 500 GB disk, so if your logical volumes are smaller you
   may need to increase the percentage so there is enough room.
  </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.18.6.28"><span class="name">Multiple Keystone Failures Received during site.yml</span><a title="Permalink" class="permalink" href="#id-1.4.5.18.6.28">#</a></h5></div><p>
   If you receive the Keystone error below during your
   <code class="literal">site.yml</code> run then follow these steps:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [OPS-MON | _keystone_conf | Create Ops Console service in Keystone] *****
failed:
[...]
msg: An unexpected error prevented the server from fulfilling your request.
(HTTP 500) (Request-ID: req-23a09c72-5991-4685-b09f-df242028d742), failed

FATAL: all hosts have already failed -- aborting</pre></div><p>
   The most likely cause of this error is that the virtual IP address is having
   issues and the Keystone API communication through the virtual IP address is
   not working properly. You will want to check the Keystone log on the
   controller where you will likely see authorization failure errors.
  </p><p>
   Verify that your virtual IP address is active and listening on the proper
   port on all of your controllers using this command:
  </p><div class="verbatim-wrap"><pre class="screen">netstat -tplan | grep 35357</pre></div><p>
   Ensure that your Cloud Lifecycle Manager did not pick the wrong (unusable) IP
   address from the list of IP addresses assigned to your Management network.
  </p><p>
   The Cloud Lifecycle Manager will take the first available IP address after the
   <code class="literal">gateway-ip</code> defined in your
   <code class="filename">~/openstack/my_cloud/definition/data/networks.yml</code> file.
   This IP will be used as the virtual IP address for that particular network.
   If this IP address is used and reserved for another purpose outside of your
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment then you will receive the error above.
  </p><p>
   To resolve this issue we recommend that you utilize the
   <code class="literal">start-address</code> and possibly the
   <code class="literal">end-address</code> (if needed) options in your
   <code class="filename">networks.yml</code> file to further define which IP addresses
   you want your cloud deployment to use. For more information, see
   <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.14 “Networks”</span>.
  </p><p>
   After you have made changes to your <code class="filename">networks.yml</code> file,
   follow these steps to commit the changes:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Ensuring that you stay within the <code class="filename">~/openstack</code> directory,
     commit the changes you just made:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "commit message"</pre></div></li><li class="step"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Re-run the site.yml playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></section></section><section class="chapter" id="esx-troubleshooting-installation" data-id-title="Troubleshooting the ESX"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">24 </span><span class="title-name">Troubleshooting the ESX</span></span> <a title="Permalink" class="permalink" href="#esx-troubleshooting-installation">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud</span></span>
  <span class="phrase"><span class="phrase">8</span></span> for ESX.
 </p><section class="sect1" id="id-1.4.5.19.3" data-id-title="Issue: ardana-service.service is not running"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.1 </span><span class="title-name">Issue: ardana-service.service is not running</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.19.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you perform any maintenance work or reboot the Cloud Lifecycle Manager/deployer
   node, make sure to restart the Cloud Lifecycle Manager API service for standalone deployer node
   and shared Cloud Lifecycle Manager/controller node based on your environment.
  </p><p>
   For standalone deployer node, execute <code class="literal">ardana-start.yml</code>
   playbook to restart the Cloud Lifecycle Manager API service on the deployer node after a reboot.
  </p><p>
   For shared deployer/controller node, execute
   <code class="literal">ardana-start.yml</code> playbook on all the controllers to
   restart Cloud Lifecycle Manager API service.
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit <em class="replaceable">HOST_NAME</em></pre></div><p>
   Replace <em class="replaceable">HOST_NAME</em> with the host name of the Cloud Lifecycle Manager
   node or the Cloud Lifecycle Manager Node/Shared Controller.
  </p></section><section class="sect1" id="id-1.4.5.19.4" data-id-title="Issue: ESX Cluster shows UNKNOWN in Operations Console"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.2 </span><span class="title-name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.19.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the Operations Console Alarms dashboard, if all the alarms for ESX cluster are
   showing UNKNOWN then restart the <code class="literal">openstack-monasca-agent</code> running in
   ESX compute proxy.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     SSH to the respective compute proxy. You can find the hostname of the
     proxy from the dimensions list shown against the respective alarm.
    </p></li><li class="step"><p>
     Restart the <code class="literal">openstack-monasca-agent</code> service.
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart openstack-monasca-agent</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.5.19.5" data-id-title="Issue: Unable to view the VM console in Horizon UI"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.3 </span><span class="title-name">Issue: Unable to view the VM console in Horizon UI</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.19.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   By default the gdbserver firewall is disabled in ESXi host which results in
   a Handshake error when accessing the VM instance console in the Horizon UI.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/media-esx-gdbserver.png"><img src="images/media-esx-gdbserver.png" alt="Image" title="Image"/></a></div></div><p>
   <span class="bold"><strong>Procedure to enable gdbserver</strong></span>
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Login to vSphere Client.
    </p></li><li class="step"><p>
     Select the ESXi Host and click
     <span class="guimenu">Configuration</span> tab in the menu bar. You
     must perform the following actions on all the ESXi hosts in the compute
     clusters.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/media-esx-1.png"><img src="images/media-esx-1.png" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     On the left hand side select <span class="bold"><strong>Security
     Profile</strong></span> from the list of
     <span class="bold"><strong>Software</strong></span>. Click
     <span class="bold"><strong>Properties</strong></span> on the right hand side.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/media-esx-2.png"><img src="images/media-esx-2.png" alt="Image" title="Image"/></a></div></div><p>
     Firewall Properties box displays.
    </p></li><li class="step"><p>
     Select <span class="bold"><strong>gdbserver</strong></span> checkbox and click
     <span class="bold"><strong>OK</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/media-esx-3.png"><img src="images/media-esx-3.png" alt="Image" title="Image"/></a></div></div></li></ol></div></div></section></section></div><div class="part" id="post-install" data-id-title="Post-Installation"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part III </span><span class="title-name">Post-Installation </span></span><a title="Permalink" class="permalink" href="#post-install">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-post_install_overview.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#post-install-overview"><span class="title-number">25 </span><span class="title-name">Overview</span></a></span></li><dd class="toc-abstract"><p>
   Once you have completed your cloud deployment, these are some of the common
   post-installation tasks you may need to perform. Take a look at the
   descriptions below to determine which of these you need to do.
  </p></dd><li><span class="chapter"><a href="#cloud-verification"><span class="title-number">26 </span><span class="title-name">Cloud Verification</span></a></span></li><dd class="toc-abstract"><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p></dd><li><span class="chapter"><a href="#ui-verification"><span class="title-number">27 </span><span class="title-name">UI Verification</span></a></span></li><dd class="toc-abstract"><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p></dd><li><span class="chapter"><a href="#install-openstack-clients"><span class="title-number">28 </span><span class="title-name">Installing OpenStack Clients</span></a></span></li><dd class="toc-abstract"><p>
  If you have a standalone deployer, the OpenStack CLI and other clients will
  not be installed automatically on that node. If you require access to these
  clients, you will need to follow the procedure below to add the appropriate
  software.
 </p></dd><li><span class="chapter"><a href="#tls30"><span class="title-number">29 </span><span class="title-name">Configuring Transport Layer Security (TLS)</span></a></span></li><dd class="toc-abstract"><p>
    TLS is enabled by default during the installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> and
    additional configuration options are available to secure your environment,
    as described below.
   </p></dd><li><span class="chapter"><a href="#config-availability-zones"><span class="title-number">30 </span><span class="title-name">Configuring Availability Zones</span></a></span></li><dd class="toc-abstract"><p>
  The Cloud Lifecycle Manager only creates a default availability zone during
  installation. If your system has multiple failure/availability zones defined
  in your input model, these zones will not get created automatically.
 </p></dd><li><span class="chapter"><a href="#OctaviaInstall"><span class="title-number">31 </span><span class="title-name">Configuring Load Balancer as a Service</span></a></span></li><dd class="toc-abstract"><p>
    The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Neutron LBaaS service supports several load balancing
    providers. By default, both Octavia and the namespace HAProxy driver are
    configured to be used. We describe this in more detail here.
   </p></dd><li><span class="chapter"><a href="#postinstall-checklist"><span class="title-number">32 </span><span class="title-name">Other Common Post-Installation Tasks</span></a></span></li><dd class="toc-abstract"><p>On your Cloud Lifecycle Manager, in the ~/scratch/ansible/next/ardana/ansible/group_vars/ directory you will find several files. In the one labeled as first control plane node you can locate the user credentials for both the Administrator user (admin) and your Demo user (demo) which you will use to …</p></dd></ul></div><section class="chapter" id="post-install-overview" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">25 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#post-install-overview">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-post_install_overview.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have completed your cloud deployment, these are some of the common
   post-installation tasks you may need to perform. Take a look at the
   descriptions below to determine which of these you need to do.
  </p></section><section class="chapter" id="cloud-verification" data-id-title="Cloud Verification"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">26 </span><span class="title-name">Cloud Verification</span></span> <a title="Permalink" class="permalink" href="#cloud-verification">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-cloud_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p><section class="sect1" id="api-verification" data-id-title="API Verification"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.1 </span><span class="title-name">API Verification</span></span> <a title="Permalink" class="permalink" href="#api-verification">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> provides a tool, Tempest, that you can use to verify that
  your cloud deployment completed successfully:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-api-verification-prereq" title="26.1.1. Prerequisites">Section 26.1.1, “Prerequisites”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-api-verification-tempest" title="26.1.2. Tempest Integration Tests">Section 26.1.2, “Tempest Integration Tests”</a>
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#sec-api-verification-running" title="26.1.3. Running the Tests">Section 26.1.3, “Running the Tests”</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#sec-api-verification-result" title="26.1.4. Viewing Test Results">Section 26.1.4, “Viewing Test Results”</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#sec-api-verification-custom" title="26.1.5. Customizing the Test Run">Section 26.1.5, “Customizing the Test Run”</a>
     </p></li></ul></div></li><li class="listitem"><p>
    <a class="xref" href="#sec-verify-block-storage-volume" title="27.1. Verifying Your Block Storage Backend">Section 27.1, “Verifying Your Block Storage Backend”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-verify-block-storage-swift" title="27.2. Verify the Object Storage (Swift) Operations">Section 27.2, “Verify the Object Storage (Swift) Operations”</a>
   </p></li></ul></div><section class="sect2" id="sec-api-verification-prereq" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="#sec-api-verification-prereq">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The verification tests rely on you having an external network setup and a
   cloud image in your image (Glance) repository. Run the following playbook to
   configure your cloud:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><div id="id-1.4.6.3.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the EXT_NET_CIDR setting for the external network is
    now specified in the input model - see
    <span class="intraxref">Book “<em class="citetitle">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.16 “Configuration Data”, Section 6.16.2 “Neutron Configuration Data”, Section 6.16.2.2 “neutron-external-networks”</span>.
   </p></div></section><section class="sect2" id="sec-api-verification-tempest" data-id-title="Tempest Integration Tests"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.2 </span><span class="title-name">Tempest Integration Tests</span></span> <a title="Permalink" class="permalink" href="#sec-api-verification-tempest">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Tempest is a set of integration tests for OpenStack API validation,
   scenarios, and other specific tests to be run against a live OpenStack
   cluster. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, Tempest has been modeled as a service and this
   gives you the ability to locate Tempest anywhere in the cloud. It is
   recommended that you install Tempest on your Cloud Lifecycle Manager node - that
   is where it resides by default in a new installation.
  </p><p>
   A version of the upstream
   <a class="link" href="http://docs.openstack.org/developer/tempest/" target="_blank">Tempest</a>
   integration tests is pre-deployed on the Cloud Lifecycle Manager node.
   For details on what Tempest is testing, you can check the contents of this
   file on your Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/run_filters/ci.txt</pre></div><p>
   You can use these embedded tests to verify if the deployed cloud is
   functional.
  </p><p>
   For more information on running Tempest tests, see
   <a class="link" href="https://git.openstack.org/cgit/openstack/tempest/tree/README.rst" target="_blank">Tempest
   - The OpenStack Integration Test Suite</a>.
  </p><div id="id-1.4.6.3.3.5.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Running these tests requires access to the deployed cloud's identity admin
    credentials
   </p></div><p>
   Tempest creates and deletes test accounts and test resources for test
   purposes.
  </p><p>
   In certain cases Tempest might fail to clean-up some of test resources after
   a test is complete, for example in case of failed tests.
  </p></section><section class="sect2" id="sec-api-verification-running" data-id-title="Running the Tests"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.3 </span><span class="title-name">Running the Tests</span></span> <a title="Permalink" class="permalink" href="#sec-api-verification-running">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To run the default set of Tempest tests:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Ensure you can access your cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
source /etc/environment</pre></div></li><li class="step"><p>
     Run the tests:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts tempest-run.yml</pre></div></li></ol></div></div><p>
   Optionally, you can <a class="xref" href="#sec-api-verification-custom" title="26.1.5. Customizing the Test Run">Section 26.1.5, “Customizing the Test Run”</a>.
  </p></section><section class="sect2" id="sec-api-verification-result" data-id-title="Viewing Test Results"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.4 </span><span class="title-name">Viewing Test Results</span></span> <a title="Permalink" class="permalink" href="#sec-api-verification-result">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Tempest is deployed under <code class="literal">/opt/stack/tempest</code>. Test
   results are written in a log file in the following directory:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/logs</pre></div><p>
   A detailed log file is written to:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/tempest.log</pre></div><p>
   The results are also stored in the <code class="literal">testrepository</code>
   database.
  </p><p>
   To access the results after the run:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Change to the <code class="literal">tempest</code> directory and list the test
     results:
    </p><div class="verbatim-wrap"><pre class="screen">cd /opt/stack/tempest
./venv/bin/testr last</pre></div></li></ol></div></div><div id="id-1.4.6.3.3.7.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    If you encounter an error saying "local variable 'run_subunit_content'
    referenced before assignment", you may need to log in as the
    <code class="literal">tempest</code> user to run this command. This is due to a known
    issue reported at
    <a class="link" href="https://bugs.launchpad.net/testrepository/+bug/1348970" target="_blank">https://bugs.launchpad.net/testrepository/+bug/1348970</a>.
   </p></div><p>
   See
   <a class="link" href="https://testrepository.readthedocs.org/en/latest/" target="_blank">Test
   Repository Users Manual</a> for more details on how to manage the test
   result repository.
  </p></section><section class="sect2" id="sec-api-verification-custom" data-id-title="Customizing the Test Run"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.5 </span><span class="title-name">Customizing the Test Run</span></span> <a title="Permalink" class="permalink" href="#sec-api-verification-custom">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   There are several ways available to customize which tests will be executed.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#sec-api-verification-service" title="26.1.6. Run Tests for Specific Services and Exclude Specific Features">Section 26.1.6, “Run Tests for Specific Services and Exclude Specific Features”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-api-verification-list" title="26.1.7. Run Tests Matching a Series of White and Blacklists">Section 26.1.7, “Run Tests Matching a Series of White and Blacklists”</a>
    </p></li></ul></div></section><section class="sect2" id="sec-api-verification-service" data-id-title="Run Tests for Specific Services and Exclude Specific Features"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.6 </span><span class="title-name">Run Tests for Specific Services and Exclude Specific Features</span></span> <a title="Permalink" class="permalink" href="#sec-api-verification-service">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Tempest allows you to test specific services and features using the
   <code class="literal">tempest.conf</code> configuration file.
  </p><p>
   A working configuration file with inline documentation is deployed under
   <code class="literal">/opt/stack/tempest/etc/</code>.
  </p><p>
   To use this, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Edit the
     <code class="literal">/opt/stack/tempest/configs/tempest_region0.conf</code> file.
    </p></li><li class="step"><p>
     To test specific service, edit the <code class="literal">[service_available]</code>
     section and clear the comment character <code class="literal">#</code> and set a
     line to <code class="literal">true</code> to test that service or
     <code class="literal">false</code> to not test that service.
    </p><div class="verbatim-wrap"><pre class="screen">cinder = true
neutron = false</pre></div></li><li class="step"><p>
     To test specific features, edit any of the
     <code class="literal">*_feature_enabled</code> sections to enable or disable tests
     on specific features of a service.
    </p><div class="verbatim-wrap"><pre class="screen">[volume-feature-enabled]
[compute-feature-enabled]
[identity-feature-enabled]
[image-feature-enabled]
[network-feature-enabled]
[object-storage-feature-enabled]</pre></div><div class="verbatim-wrap"><pre class="screen">#Is the v2 identity API enabled (boolean value)
api_v2 = true
#Is the v3 identity API enabled (boolean value)
api_v3 = false</pre></div></li><li class="step"><p>
     Then run tests normally
    </p></li></ol></div></div></section><section class="sect2" id="sec-api-verification-list" data-id-title="Run Tests Matching a Series of White and Blacklists"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.7 </span><span class="title-name">Run Tests Matching a Series of White and Blacklists</span></span> <a title="Permalink" class="permalink" href="#sec-api-verification-list">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-api_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can run tests against specific scenarios by editing or creating a run
   filter file.
  </p><p>
   Run filter files are deployed under
   <code class="literal">/opt/stack/tempest/run_filters</code>.
  </p><p>
   Use run filters to whitelist or blacklist specific tests or groups of tests:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     lines starting with # or empty are ignored
    </p></li><li class="listitem"><p>
     lines starting with <code class="literal">+</code> are whitelisted
    </p></li><li class="listitem"><p>
     lines starting with <code class="literal">-</code> are blacklisted
    </p></li><li class="listitem"><p>
     lines not matching any of the above conditions are blacklisted
    </p></li></ul></div><p>
   If whitelist is empty, all available tests are fed to blacklist. If
   blacklist is empty, all tests from whitelist are returned.
  </p><p>
   Whitelist is applied first. The blacklist is executed against the set of
   tests returned by the whitelist.
  </p><p>
   To run whitelist and blacklist tests:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Make sure you can access the cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
source /etc/environment</pre></div></li><li class="step"><p>
     Run the tests:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts tempest-run.yml  -e run_filter &lt;run_filter_name&gt;</pre></div></li></ol></div></div><p>
   Note that the run_filter_name is the name of the run_filter file except for
   the extension. For instance, to run using the filter from the file
   /opt/stack/tempest/run_filters/ci.txt, use the following:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts tempest-run.yml -e run_filter=ci</pre></div><p>
   Documentation on the format of white and black-lists is available at:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/tests2skip.py</pre></div><p>
   Example:
  </p><p>
   The following entries run API tests, exclude tests that are less relevant
   for deployment validation, such as negative, admin, cli and third-party (EC2)
   tests:
  </p><div class="verbatim-wrap"><pre class="screen">+tempest\.api\.*
*[Aa]dmin.*
*[Nn]egative.*
- tempest\.cli.*
- tempest\.thirdparty\.*</pre></div></section></section></section><section class="chapter" id="ui-verification" data-id-title="UI Verification"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">27 </span><span class="title-name">UI Verification</span></span> <a title="Permalink" class="permalink" href="#ui-verification">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-ui_verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p><section class="sect1" id="sec-verify-block-storage-volume" data-id-title="Verifying Your Block Storage Backend"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.1 </span><span class="title-name">Verifying Your Block Storage Backend</span></span> <a title="Permalink" class="permalink" href="#sec-verify-block-storage-volume">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-verify_block_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The sections below will show you the steps to verify that your Block Storage
  backend was setup properly.
 </p><section class="sect2" id="id-1.4.6.4.3.3" data-id-title="Create a Volume"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.1.1 </span><span class="title-name">Create a Volume</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.3.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-verify_block_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Perform the following steps to create a volume using Horizon dashboard.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step"><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Volumes</span>.
    </p></li><li class="step"><p>
     On the <span class="guimenu">Volumes</span> tabs, click the
     <span class="guimenu">Create Volume</span> button to create a volume.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Create Volume</span> options, enter the
     required details into the fields and then click the
     <span class="guimenu">Create Volume</span> button:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Volume Name - This is the name you specify for your volume.
      </p></li><li class="step"><p>
       Description (optional) - This is an optional description for the volume.
      </p></li><li class="step"><p>
       Type - Select the volume type you have created for your volumes from the
       drop down.
      </p></li><li class="step"><p>
       Size (GB) - Enter the size, in GB, you would like the volume to be.
      </p></li><li class="step"><p>
       Availability Zone - You can either leave this at the default option of
       <span class="guimenu">Any Availability Zone</span> or select a
       specific zone from the drop-down box.
      </p></li></ol></li></ol></div></div><p>
   The dashboard will then show the volume you have just created.
  </p></section><section class="sect2" id="id-1.4.6.4.3.4" data-id-title="Attach Volume to an Instance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.1.2 </span><span class="title-name">Attach Volume to an Instance</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.3.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-verify_block_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Perform the following steps to attach a volume to an instance:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step"><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Instances</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Action</span> column, choose the
     <span class="guimenu">Edit Attachments</span> in the drop-down
     box next to the instance you want to attach the volume to.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Attach To Instance</span> drop-down,
     select the volume that you want to attach.
    </p></li><li class="step"><p>
     Edit the <span class="guimenu">Device Name</span> if necessary.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Attach Volume</span> to complete the
     action.
    </p></li><li class="step"><p>
     On the <span class="guimenu">Volumes</span> screen,
     verify that the volume you attached is displayed in the
     <span class="guimenu">Attached To</span> columns.
    </p></li></ol></div></div></section><section class="sect2" id="id-1.4.6.4.3.5" data-id-title="Detach Volume from Instance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.1.3 </span><span class="title-name">Detach Volume from Instance</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.3.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-verify_block_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Perform the following steps to detach the volume from instance:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step"><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Instances</span>.
    </p></li><li class="step"><p>
     Click the check box next to the name of the volume you want to detach.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Action</span> column, choose the
     <span class="guimenu">Edit Attachments</span> in the drop-down
     box next to the instance you want to attach the volume to.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Detach Attachment</span>. A confirmation
     dialog box appears.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Detach Attachment</span> to confirm the
     detachment of the volume from the associated instance.
    </p></li></ol></div></div></section><section class="sect2" id="id-1.4.6.4.3.6" data-id-title="Delete Volume"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.1.4 </span><span class="title-name">Delete Volume</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.3.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-verify_block_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Perform the following steps to delete a volume using Horizon dashboard:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step"><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Volumes</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Actions</span> column, click
     <span class="guimenu">Delete Volume</span> next to the volume you
     would like to delete.
    </p></li><li class="step"><p>
     To confirm and delete the volume, click <span class="guimenu">Delete Volume</span>
     again.
    </p></li><li class="step"><p>
     Verify that the volume was removed from the
     <span class="guimenu">Volumes</span> screen.
    </p></li></ol></div></div></section><section class="sect2" id="id-1.4.6.4.3.7" data-id-title="Verifying Your Object Storage (Swift)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.1.5 </span><span class="title-name">Verifying Your Object Storage (Swift)</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.3.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-verify_block_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following procedure shows how to validate that all servers have been
   added to the Swift rings:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Run the swift-compare-model-rings.yml playbook as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div></li><li class="step"><p>
     Search for output similar to the following. Specifically, look at the
     number of drives that are proposed to be added.
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | validate-input-model | Print report] *********
ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
    "var": {
        "report.stdout_lines": [
            "Rings:",
            "  ACCOUNT:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced",
            "  CONTAINER:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced",
            "  OBJECT-0:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced"
        ]
    }
}</pre></div></li><li class="step"><p>
     If the text contains "no device changes" then the deploy was successful
     and no further action is needed.
    </p></li><li class="step"><p>
     If more drives need to be added, it indicates that the deploy
     failed on some nodes and that you restarted the deploy to include those
     nodes. However, the nodes are not in the Swift rings because enough time
     has not elapsed to allow the rings to be rebuilt. You have two options to
     continue:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Repeat the deploy. There are two steps:
      </p><ol type="i" class="substeps"><li class="step"><p>
         Delete the ring builder files as described in
         <span class="intraxref">Book “Operations Guide”, Chapter 15 “Troubleshooting Issues”, Section 15.6 “Storage Troubleshooting”, Section 15.6.2 “Swift Storage Troubleshooting”, Section 15.6.2.8 “Restarting the Object Storage Deployment”</span>.
        </p></li><li class="step"><p>
         Repeat the installation process starting by running the
         <code class="filename">site.yml</code> playbook as described in
         <a class="xref" href="#sec-kvm-deploy" title="12.7. Deploying the Cloud">Section 12.7, “Deploying the Cloud”</a>.
        </p></li></ol></li><li class="step"><p>
       Rebalance the rings several times until all drives are incorporated in
       the rings. This process may take several hours to complete (because you
       need to wait one hour between each rebalance). The steps are as follows:
      </p><ol type="i" class="substeps"><li class="step"><p>
         Change the min-part-hours to 1 hour. See
         <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.5 “Managing Swift Rings”, Section 8.5.7 “Changing min-part-hours in Swift”</span>.
        </p></li><li class="step"><p>
         Use the "First phase of ring rebalance" and "Final rebalance phase" as
         described in <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.5 “Managing Swift Rings”, Section 8.5.5 “Applying Input Model Changes to Existing Rings”</span>.
         The <span class="quote">“<span class="quote">Weight change phase of ring rebalance</span>”</span> does not
         apply because you have not set the weight-step attribute at this
         stage.
        </p></li><li class="step"><p>
         Set the min-part-hours to the recommended 16 hours as described in
         <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.5 “Managing Swift Rings”, Section 8.5.7 “Changing min-part-hours in Swift”</span>.
        </p></li></ol></li></ol></li></ol></div></div><p>
   If you receive errors during the validation, see
   <span class="intraxref">Book “Operations Guide”, Chapter 15 “Troubleshooting Issues”, Section 15.6 “Storage Troubleshooting”, Section 15.6.2 “Swift Storage Troubleshooting”, Section 15.6.2.3 “Interpreting Swift Input Model Validation Errors”</span>.
  </p></section></section><section class="sect1" id="sec-verify-block-storage-swift" data-id-title="Verify the Object Storage (Swift) Operations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.2 </span><span class="title-name">Verify the Object Storage (Swift) Operations</span></span> <a title="Permalink" class="permalink" href="#sec-verify-block-storage-swift">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-verify_swift.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For information about verifying the operations, see
   <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.1 “Running the Swift Dispersion Report”</span>.
  </p></section><section class="sect1" id="upload-image" data-id-title="Uploading an Image for Use"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.3 </span><span class="title-name">Uploading an Image for Use</span></span> <a title="Permalink" class="permalink" href="#upload-image">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-upload_image.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To create a Compute instance, you need to obtain an image that you can use.
  The Cloud Lifecycle Manager provides an Ansible playbook that will
  download a CirrOS Linux image, and then upload it as a public image to your
  image repository for use across your projects.
 </p><section class="sect2" id="id-1.4.6.4.5.3" data-id-title="Running the Playbook"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.3.1 </span><span class="title-name">Running the Playbook</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.5.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-upload_image.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Use the following command to run this playbook:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts glance-cloud-configure.yml -e proxy=&lt;PROXY&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Switch</th><th style="border-bottom: 1px solid ; ">Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
       <p>
        <code class="literal">-e proxy="&lt;proxy_address:port&gt;"</code>
       </p>
      </td><td>
       <p>
        Optional. If your environment requires a proxy for the internet, use
        this switch to specify the proxy information.
       </p>
      </td></tr></tbody></table></div></section><section class="sect2" id="id-1.4.6.4.5.4" data-id-title="How to Curate Your Own Images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.3.2 </span><span class="title-name">How to Curate Your Own Images</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.5.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-upload_image.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   OpenStack has created a guide to show you how to obtain, create, and modify
   images that will be compatible with your cloud:
  </p><p>
   <a class="link" href="http://docs.openstack.org/image-guide/content/" target="_blank">OpenStack
   Virtual Machine Image Guide</a>
  </p></section><section class="sect2" id="id-1.4.6.4.5.5" data-id-title="Using the GlanceClient CLI to Create Images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.3.3 </span><span class="title-name">Using the GlanceClient CLI to Create Images</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.5.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-upload_image.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can use the GlanceClient on a machine accessible to your cloud or on
   your Cloud Lifecycle Manager where it is automatically installed.
  </p><p>
   The GlanceClient allows you to create, update, list, and delete images as
   well as manage your image member lists, which allows you to share access to
   images across multiple tenants. As with most of the OpenStack CLI tools, you
   can use the <code class="literal">glance help</code> command to get a full list of
   commands as well as their syntax.
  </p><p>
   If you would like to use the <code class="literal">--copy-from</code> option when
   creating an image, you will need to have your Administrator enable the http
   store in your environment using the instructions outlined at
   <span class="intraxref">Book “Operations Guide”, Chapter 5 “Managing Compute”, Section 5.6 “Configuring the Image Service”, Section 5.6.2 “Allowing the Glance copy-from option in your environment”</span>.
  </p></section></section><section class="sect1" id="create-extnet" data-id-title="Creating an External Network"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.4 </span><span class="title-name">Creating an External Network</span></span> <a title="Permalink" class="permalink" href="#create-extnet">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-create_extnet.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  You must have an external network set up to allow your Compute instances to
  reach the internet. There are multiple methods you can use to create this
  external network and we provide two of them here. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer
  provides an Ansible playbook that will create this network for use across
  your projects. We also show you how to create this network via the command
  line tool from your Cloud Lifecycle Manager.
 </p><section class="sect2" id="sec-create-extnet-playbook" data-id-title="Using the Ansible Playbook"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.4.1 </span><span class="title-name">Using the Ansible Playbook</span></span> <a title="Permalink" class="permalink" href="#sec-create-extnet-playbook">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-create_extnet.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This playbook will query the Networking service for an existing external
   network, and then create a new one if you do not already have one. The
   resulting external network will have the name <code class="literal">ext-net</code>
   with a subnet matching the CIDR you specify in the command below.
  </p><p>
   If you need to specify more granularity, for example specifying an
   allocation pool for the subnet then you should utilize the
   <a class="xref" href="#sec-create-extnet-cli" title="27.4.2. Using the NeutronClient CLI">Section 27.4.2, “Using the NeutronClient CLI”</a>.
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&lt;CIDR&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Switch</th><th style="border-bottom: 1px solid ; ">Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
       <p>
        <code class="literal">-e EXT_NET_CIDR=&lt;CIDR&gt;</code>
       </p>
      </td><td>
       <p>
        Optional. You can use this switch to specify the external network CIDR.
        If you choose not to use this switch, or use a wrong value, the VMs
        will not be accessible over the network.
       </p>
       <p>
        This CIDR will be from the <code class="literal">EXTERNAL VM</code> network.
       </p>
       <div id="id-1.4.6.4.6.3.6.1.4.1.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
         If this option is not defined the default value is "172.31.0.0/16"
        </p></div>
      </td></tr></tbody></table></div></section><section class="sect2" id="sec-create-extnet-cli" data-id-title="Using the NeutronClient CLI"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.4.2 </span><span class="title-name">Using the NeutronClient CLI</span></span> <a title="Permalink" class="permalink" href="#sec-create-extnet-cli">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-create_extnet.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For more granularity you can utilize the Neutron command line tool to create
   your external network.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Source the Admin credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step"><p>
     Create the external network and then the subnet using these commands
     below.
    </p><p>
     Creating the network:
    </p><div class="verbatim-wrap"><pre class="screen">neutron net-create --router:external &lt;external-network-name&gt;</pre></div><p>
     Creating the subnet:
    </p><div class="verbatim-wrap"><pre class="screen">neutron subnet-create &lt;external-network-name&gt; &lt;CIDR&gt; --gateway &lt;gateway&gt; \
--allocation-pool start=&lt;IP_start&gt;,end=&lt;IP_end&gt; [--disable-dhcp]</pre></div><p>
     Where:
    </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Value</th><th style="border-bottom: 1px solid ; ">Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">external-network-name</td><td style="border-bottom: 1px solid ; ">
         <p>
          This is the name given to your external network. This is a unique
          value that you will choose. The value <code class="literal">ext-net</code> is
          usually used.
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">CIDR</td><td style="border-bottom: 1px solid ; ">
         <p>
          You can use this switch to specify the external network CIDR. If you
          choose not to use this switch, or use a wrong value, the VMs will not
          be accessible over the network.
         </p>
         <p>
          This CIDR will be from the EXTERNAL VM network.
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">--gateway</td><td style="border-bottom: 1px solid ; ">
         <p>
          Optional switch to specify the gateway IP for your subnet. If this
          is not included then it will choose the first available IP.
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         --allocation-pool start end
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Optional switch to specify a start and end IP address to use as the
          allocation pool for this subnet.
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">--disable-dhcp</td><td>
         <p>
          Optional switch if you want to disable DHCP on this subnet. If this
          is not specified then DHCP will be enabled.
         </p>
        </td></tr></tbody></table></div></li></ol></div></div></section><section class="sect2" id="id-1.4.6.4.6.5" data-id-title="Next Steps"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">27.4.3 </span><span class="title-name">Next Steps</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.4.6.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-create_extnet.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once the external network is created, users can create a Private Network to
   complete their networking setup. For instructions, see
   <span class="intraxref">Book “User Guide”, Chapter 8 “Creating a Private Network”</span>.
  </p></section></section></section><section class="chapter" id="install-openstack-clients" data-id-title="Installing OpenStack Clients"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">28 </span><span class="title-name">Installing OpenStack Clients</span></span> <a title="Permalink" class="permalink" href="#install-openstack-clients">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-install_openstack_clients.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If you have a standalone deployer, the OpenStack CLI and other clients will
  not be installed automatically on that node. If you require access to these
  clients, you will need to follow the procedure below to add the appropriate
  software.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    [OPTIONAL] Connect to your standalone deployer and try to use the OpenStack
    CLI:
   </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc
<span class="bold"><strong>openstack project list</strong></span>

-bash: openstack: command not found</pre></div></li><li class="step"><p>
    Edit the configuration file containing details of your Control Plane,
    typically
    <code class="literal">~/openstack/my_cloud/definition/data/control_plane</code>.
   </p></li><li class="step"><p>
    Locate the stanza for the cluster where you want to install the client(s).
    For a standalone deployer, this will look like the following extract:
   </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager</pre></div></li><li class="step"><p>
    Choose the client(s) you wish to install from the following list of
    available clients:
   </p><div class="verbatim-wrap"><pre class="screen"> - barbican-client
 - ceilometer-client
 - cinder-client
 - designate-client
 - glance-client
 - heat-client
 - ironic-client
 - keystone-client
 - magnum-client
 - manila-client
 - monasca-client
 - neutron-client
 - nova-client
 - ntp-client
 - octavia-client
 - openstack-client
 - swift-client</pre></div></li><li class="step"><p>
    Add the client(s) to the list of <code class="literal">service-components</code> - in
    this example, we add the <code class="literal">openstack-client</code> to the
    standalone deployer:
   </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager
            <span class="bold"><strong>- openstack-client
            - ceilometer-client
            - cinder-client
            - designate-client
            - glance-client
            - heat-client
            - ironic-client
            - keystone-client
            - neutron-client
            - nova-client
            - swift-client
            - monasca-client
            - barbican-client
</strong></span></pre></div></li><li class="step"><p>
    Commit the configuration changes:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "Add explicit client service deployment"</pre></div></li><li class="step"><p>
    Run the configuration processor, followed by the
    <code class="literal">ready-deployment</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" \
  -e rekey=""
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
    Add the software for the clients using the following command:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts clients-upgrade.yml</pre></div></li><li class="step"><p>
    Check that the software has been installed correctly. In this instance,
    connect to your standalone deployer and try to use the OpenStack CLI:
   </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc
openstack project list</pre></div><p>
    You should now see a list of projects returned:
   </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c0-m1-mgmt:~$ <span class="bold"><strong>openstack project list</strong></span>

+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 076b6e879f324183bbd28b46a7ee7826 | kronos           |
| 0b81c3a9e59c47cab0e208ea1bb7f827 | backup           |
| 143891c2a6094e2988358afc99043643 | octavia          |
| 1d3972a674434f3c95a1d5ed19e0008f | glance-swift     |
| 2e372dc57cac4915bf06bbee059fc547 | glance-check     |
| 383abda56aa2482b95fb9da0b9dd91f4 | monitor          |
| 606dd3b1fa6146668d468713413fb9a6 | swift-monitor    |
| 87db9d1b30044ea199f0293f63d84652 | admin            |
| 9fbb7494956a483ca731748126f50919 | demo             |
| a59d0c682474434a9ddc240ddfe71871 | services         |
| a69398f0f66a41b2872bcf45d55311a7 | swift-dispersion |
| f5ec48d0328d400992c1c5fb44ec238f | cinderinternal   |
+----------------------------------+------------------+</pre></div></li></ol></div></div></section><section class="chapter" id="tls30" data-id-title="Configuring Transport Layer Security (TLS)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">29 </span><span class="title-name">Configuring Transport Layer Security (TLS)</span></span> <a title="Permalink" class="permalink" href="#tls30">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    TLS is enabled by default during the installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> and
    additional configuration options are available to secure your environment,
    as described below.
   </p></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you can provide your own certificate authority and
  certificates for internal and public virtual IP addresses (VIPs), and you
  should do so for any production cloud. The certificates automatically
  generated by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are useful for testing and setup, but you should always
  install your own for production use. Certificate installation is discussed
  below.
 </p><p>
  Read the following if you are using the default <code class="literal">cert-name:
  my-public-cert</code> in your model.
 </p><p>
  The bundled test certificate for public endpoints, located at
  <code class="filename">~/openstack/my_cloud/config/tls/certs/my-public-cert</code>, is
  now expired but was left in the product in case you changed the content with
  your valid certificate. Please verify if the certificate is expired and
  generate your own, as described in
  <a class="xref" href="#sec-generate-certificate" title="29.4. Generate a self-signed CA">Section 29.4, “Generate a self-signed CA”</a>.
 </p><p>
  You can verify the expiry date by running this command:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl x509 -in ~/openstack/my_cloud/config/tls/certs/my-public-cert \
-noout -enddate
notAfter=Oct  8 09:01:58 2016 GMT</pre></div><p>
  Before you begin, the following list of terms will be helpful when generating
  and installing certificates.
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.6.9.1"><span class="term"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-generated public CA</span></dt><dd><p>
     A <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-generated public CA
     (<code class="filename">openstack_frontend_cacert.crt</code>) is available for you
     to use in <code class="filename">/etc/pki/trust/anchors/ca-certificates</code>.
    </p></dd><dt id="id-1.4.6.6.9.2"><span class="term">Fully qualified domain name (FQDN) of the public VIP</span></dt><dd><p>
     The registered domain name. A FQDN is not mandatory. It is perfectly valid
     to have no FQDN and use IP addresses instead. Note that you can use FQDNs
     on public endpoints, and you may change them whenever the need arises.
    </p></dd><dt id="id-1.4.6.6.9.3"><span class="term">Certificate authority (CA) certificate</span></dt><dd><p>
     Your certificates must be signed by a CA, such as your internal IT
     department or a public certificate authority. For this example we will use
     a self-signed certificate.
    </p></dd><dt id="id-1.4.6.6.9.4"><span class="term">Server certificate</span></dt><dd><p>
     It is easy to confuse server certificates and CA certificates. Server
     certificates reside on the server and CA certificates reside on the
     client. A server certificate affirms that the server that sent it serves a
     set of IP addresses, domain names, and set of services. A CA certificate
     is used by the client to authenticate this claim.
    </p></dd><dt id="id-1.4.6.6.9.5"><span class="term">SAN (subject-alt-name)</span></dt><dd><p>
     The set of IP addresses and domain names in a server certificate request:
     A template for a server certificate.
    </p></dd><dt id="id-1.4.6.6.9.6"><span class="term">Certificate signing request (CSR)</span></dt><dd><p>
     A blob of data generated from a certificate request and sent to a CA,
     which would then sign it, produce a server certificate, and send it back.
    </p></dd><dt id="id-1.4.6.6.9.7"><span class="term">External VIP</span></dt><dd><p>
     External virtual IP address
    </p></dd><dt id="id-1.4.6.6.9.8"><span class="term">Internal VIP</span></dt><dd><p>
     Internal virtual IP address
    </p></dd></dl></div><p>
  The major difference between an external VIP certificate and an internal VIP
  certificate is that the internal VIP has approximately 40 domain names in the
  SAN. This is because each service has a different domain name in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. So it is unlikely that you can create an internal server
  certificate before running the configuration processor. But after a
  configuration processor run, a certificate request would be created for each
  of your cert-names.
 </p><section class="sect1" id="id-1.4.6.6.11" data-id-title="Configuring TLS in the input model"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.1 </span><span class="title-name">Configuring TLS in the input model</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.11">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For this example certificate configuration, let us assume there is no FQDN for
   the external VIP and that you are going to use the default IP address
   provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. Let's also assume that for the internal VIP you
   will use the defaults as well. If you were to call your certificate
   authority "example-CA," the CA certificate would then be called
   "example-CA.crt" and the key would be called "example-CA.key." In the
   following examples, the external VIP certificate will be named
   "example-public-cert" and the internal VIP certificate will be named
   "example-internal-cert."
  </p><div id="id-1.4.6.6.11.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Cautions:
   </p></div><p>
   Any time you make a cert change when using your own CA:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     You should use a distinct name from those already existing in
     <code class="filename">config/tls/cacerts</code>. This also means that you should
     not <span class="emphasis"><em>reuse</em></span> your CA names (and use unique and
     distinguishable names such as MyCompanyXYZ_PrivateRootCA.crt). A new name
     is what indicates that a file is new or changed, so reusing a name means
     that the file is not considered changed even its contents have changed.
    </p></li><li class="listitem"><p>
     You should not remove any existing CA files from
     <code class="filename">config/tls/cacerts</code>.
    </p></li><li class="listitem"><p>
     If you want to remove an existing CA you must
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       First remove the file.
      </p></li><li class="step"><p>
       Then run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts FND-STN -a 'sudo keytool -delete -alias \
debian:&lt;filename to remove&gt; \
-keystore /usr/lib/jvm/java-7-openjdk-amd64/jre/lib/security/cacerts \
-storepass changeit'</pre></div></li></ol></div></div></li></ul></div><div id="id-1.4.6.6.11.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Be sure to install your own certificate for all production clouds after
    installing and testing your cloud. If you ever want to test or troubleshoot
    later, you will be able to revert to the sample certificate to get back to
    a stable state for testing.
   </p></div><div id="id-1.4.6.6.11.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Unless this is a new deployment, do not update both the certificate and the
    CA together. Add the CA first and then run a site deploy. Then update the
    certificate and run tls-reconfigure, FND-CLU-stop, FND-CLU-start and then
    ardana-reconfigure. If a playbook has failed, rerun it with -vv to get
    detailed error information. The configure, HAproxy restart, and reconfigure
    steps are included below. If this is a new deployment and you are adding
    your own certs/CA before running site.yml this caveat does not apply.
   </p></div><p>
   You can add your own certificate by following the instructions below. All
   changes must go into the file
   <code class="filename">~/openstack/my_cloud/definition/data/network_groups.yml</code>.
  </p><p>
   Below are the entries for TLS for the internal and admin load balancers:
  </p><div class="verbatim-wrap"><pre class="screen">- provider: ip-cluster
        name: lb
        tls-components:
        - default
        components:
        # These services do not currently support TLS so they are not listed
        # under tls-components
        - nova-metadata
        roles:
        - internal
        - admin
        cert-file: openstack-internal-cert
        # The openstack-internal-cert is a reserved name and
        # this certificate will be autogenerated. You
        # can bring in your own certificate with a different name

        # cert-file: customer-provided-internal-cert
        # replace this with name of file in "config/tls/certs/"</pre></div><p>
   The configuration processor will also create a request template for each
   named certificate under <code class="literal">info/cert_reqs/</code> This will be of
   the form:
  </p><div class="verbatim-wrap"><pre class="screen">info/cert_reqs/customer-provided-internal-cert</pre></div><p>
   These request templates contain the subject <code class="literal">Alt-names</code>
   that the certificates need. You can add to this template before generating
   your certificate signing request .
  </p><p>
   You would then send the CSR to your CA to be signed, and once you receive
   the certificate, place it in <code class="filename">config/tls/certs</code>.
  </p><p>
   When you bring in your own certificate, you may want to bring in the trust
   chains (or CA certificate) for this certificate. This is usually not
   required if the CA is a public signer that is typically bundled with the
   operating system. However, we suggest you include it anyway by copying the
   file into the directory <code class="filename">config/cacerts/</code>.
  </p></section><section class="sect1" id="id-1.4.6.6.12" data-id-title="User-provided certificates and trust chains"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.2 </span><span class="title-name">User-provided certificates and trust chains</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.12">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> generates its own internal certificates but is designed to allow
   you to bring in your own certificates for the VIPs. Here is the general
   process.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     You must have a server certificate and a CA certificate to go with it
     (unless the signer is a public CA and it is already bundled with most
     distributions).
    </p></li><li class="step"><p>
     You must decide the names of the server certificates and configure the
     <code class="literal">network_groups.yml</code> file in the input model such that
     each load balancer provider has at least one cert-name associated with it.
    </p></li><li class="step"><p>
     Run the configuration processor. Note that you may or may not have the
     certificate file at this point. The configuration processor would create
     certificate request file artifacts under
     <code class="literal">info/cert_reqs/</code> for each of the cert-name(s) in the
     <code class="literal">network_groups.yml</code> file. While there is no special
     reason to use the request file created for an external endpoint VIP
     certificate, it is important to use the request files created for internal
     certificates since the canonical names for the internal VIP can be many
     and service specific and each of these need to be in the Subject Alt Names
     attribute of the certificate.
    </p></li><li class="step"><p>
     Create a certificate signing request for this request file and send it to
     your internal CA or a public CA to get it certified and issued with a
     certificate. You will now have a server certificate and possibly a trust
     chain or CA certificate.
    </p></li><li class="step"><p>
     Next, upload it to the Cloud Lifecycle Manager. Server certificates should be added to
     <code class="filename">config/tls/certs</code> and CA certificates should be added
     to <code class="filename">config/tls/cacerts</code>. The file extension should be
     CRT file for the CA certificate to be processed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Detailed
     steps are next.
    </p></li></ol></div></div></section><section class="sect1" id="id-1.4.6.6.13" data-id-title="Edit the input model to include your certificate files"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.3 </span><span class="title-name">Edit the input model to include your certificate files</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.13">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Edit the load balancer configuration in
   <code class="literal">~/openstack/my_cloud/definition/data/network_groups.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">load-balancers:
 - provider: ip-cluster
 name: lb
 tls-components:
 - default
 components:
 - nova-metadata
 roles:
 - internal
 - admin
 cert-file: example-internal-cert #&lt;&lt;&lt;---- Certificate name for the internal VIP

- provider: ip-cluster
 name: extlb
 external-name: myardana.test #&lt;&lt;&lt;--- Use just IP for the external VIP in this example
 tls-components:
 - default
 roles:
 - public
 cert-file: example-public-cert #&lt;&lt;&lt;---- Certificate name for the external VIP</pre></div><p>
   Commit your changes to the local git repository and run the configuration
   processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "changed VIP certificates"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   Verify that certificate requests have been generated by the configuration
   processor for every certificate file configured in the
   <code class="literal">networks_groups.yml</code> file. In this example, there are two
   files, as shown from the list command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls ~/openstack/my_cloud/info/cert_reqs
example-internal-cert
example-public-cert</pre></div></section><section class="sect1" id="sec-generate-certificate" data-id-title="Generate a self-signed CA"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.4 </span><span class="title-name">Generate a self-signed CA</span></span> <a title="Permalink" class="permalink" href="#sec-generate-certificate">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.6.6.14.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    In a production setting you will not perform this step. You will use your
    company's CA or a valid public CA.
   </p></div><p>
   This section demonstrates to how you can create your own self-signed CA and
   then use this CA to sign server certificates. This CA can be your
   organization's IT internal CA that is self-signed and whose CA certificates
   are deployed on your organization's machines. This way the server
   certificate becomes legitimate.
  </p><div id="id-1.4.6.6.14.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Please use a unique CN for your example Certificate Authority and do not
    install multiple CA certificates with the same CN into your cloud.
   </p></div><p>
   Copy the commands below to the command line and execute. This will cause the
   two files, <code class="literal">example-CA.key</code> and
   <code class="literal">example-CA.crt</code> to be created:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_KEY_FILE='example-CA.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_CERT_FILE='example-CA.crt'
<code class="prompt user">ardana &gt; </code>openssl req -x509 -batch -newkey rsa:2048 -nodes -out "${EXAMPLE_CA_CERT_FILE}" \
-keyout "${EXAMPLE_CA_KEY_FILE}" \
-subj "/C=UK/O=hp/CN=YourOwnUniqueCertAuthorityName" \
-days 365</pre></div><p>
   You can tweak the subj and days settings above to meet your needs, or to
   test. For instance, if you want to test what happens when a CA expires, you
   can set 'days' to a very low value. Grab the configuration
   processor-generated request file from <code class="literal">info/cert_reqs/</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/info/cert_reqs/example-internal-cert</pre></div><p>
   Now, copy this file to your working directory and append a
   <code class="literal">.req</code> extension to it.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp ~/openstack/my_cloud/info/cert_reqs/example-internal-cert \
example-internal-cert.req</pre></div><div class="example" id="sec-tls-private-metadata" data-id-title="Certificate request file"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 29.1: </span><span class="title-name">Certificate request file </span></span><a title="Permalink" class="permalink" href="#sec-tls-private-metadata">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[ req_distinguished_name ]
CN = "openstack-vip"

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = "deployerincloud-ccp-c0-m1-mgmt"
DNS.2 = "deployerincloud-ccp-vip-CEI-API-mgmt"
DNS.3 = "deployerincloud-ccp-vip-CND-API-mgmt"
DNS.4 = "deployerincloud-ccp-vip-DES-API-mgmt"
DNS.5 = "deployerincloud-ccp-vip-FND-MDB-mgmt"
DNS.6 = "deployerincloud-ccp-vip-FND-RMQ-mgmt"
DNS.7 = "deployerincloud-ccp-vip-FND-VDB-mgmt"
DNS.8 = "deployerincloud-ccp-vip-FRE-API-mgmt"
DNS.9 = "deployerincloud-ccp-vip-GLA-API-mgmt"
DNS.10 = "deployerincloud-ccp-vip-GLA-REG-mgmt"
DNS.11 = "deployerincloud-ccp-vip-HEA-ACF-mgmt"
DNS.12 = "deployerincloud-ccp-vip-HEA-ACW-mgmt"
DNS.13 = "deployerincloud-ccp-vip-HEA-API-mgmt"
DNS.14 = "deployerincloud-ccp-vip-HUX-SVC-mgmt"
DNS.15 = "deployerincloud-ccp-vip-HZN-WEB-mgmt"
DNS.16 = "deployerincloud-ccp-vip-KEY-API-mgmt"
DNS.17 = "deployerincloud-ccp-vip-KEYMGR-API-mgmt"
DNS.18 = "deployerincloud-ccp-vip-LOG-API-mgmt"
DNS.19 = "deployerincloud-ccp-vip-LOG-SVR-mgmt"
DNS.20 = "deployerincloud-ccp-vip-MON-API-mgmt"
DNS.21 = "deployerincloud-ccp-vip-NEU-SVR-mgmt"
DNS.22 = "deployerincloud-ccp-vip-NOV-API-mgmt"
DNS.23 = "deployerincloud-ccp-vip-NOV-MTD-mgmt"
DNS.24 = "deployerincloud-ccp-vip-OCT-API-mgmt"
DNS.25 = "deployerincloud-ccp-vip-OPS-WEB-mgmt"
DNS.26 = "deployerincloud-ccp-vip-SHP-API-mgmt"
DNS.27 = "deployerincloud-ccp-vip-SWF-PRX-mgmt"
DNS.28 = "deployerincloud-ccp-vip-admin-CEI-API-mgmt"
DNS.29 = "deployerincloud-ccp-vip-admin-CND-API-mgmt"
DNS.30 = "deployerincloud-ccp-vip-admin-DES-API-mgmt"
DNS.31 = "deployerincloud-ccp-vip-admin-FND-MDB-mgmt"
DNS.32 = "deployerincloud-ccp-vip-admin-FRE-API-mgmt"
DNS.33 = "deployerincloud-ccp-vip-admin-GLA-API-mgmt"
DNS.34 = "deployerincloud-ccp-vip-admin-HEA-ACF-mgmt"
DNS.35 = "deployerincloud-ccp-vip-admin-HEA-ACW-mgmt"
DNS.36 = "deployerincloud-ccp-vip-admin-HEA-API-mgmt"
DNS.37 = "deployerincloud-ccp-vip-admin-HUX-SVC-mgmt"
DNS.38 = "deployerincloud-ccp-vip-admin-HZN-WEB-mgmt"
DNS.39 = "deployerincloud-ccp-vip-admin-KEY-API-mgmt"
DNS.40 = "deployerincloud-ccp-vip-admin-KEYMGR-API-mgmt"
DNS.41 = "deployerincloud-ccp-vip-admin-MON-API-mgmt"
DNS.42 = "deployerincloud-ccp-vip-admin-NEU-SVR-mgmt"
DNS.43 = "deployerincloud-ccp-vip-admin-NOV-API-mgmt"
DNS.44 = "deployerincloud-ccp-vip-admin-OPS-WEB-mgmt"
DNS.45 = "deployerincloud-ccp-vip-admin-SHP-API-mgmt"
DNS.46 = "deployerincloud-ccp-vip-admin-SWF-PRX-mgmt"
DNS.47 = "192.168.245.5"
IP.1 = "192.168.245.5"

=============end of certificate request file.</pre></div></div></div><div id="id-1.4.6.6.14.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    In the case of a public VIP certificate, please add all the FQDNs you want
    it to support Currently <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not add the hostname for the
    external-name specified in <code class="literal">network_groups.yml</code> to the
    certificate request file . However, you can add it to the certificate
    request file manually. Here we assume that <code class="literal">myardana.test</code>
    is your external-name. In that case you would add this line (to the
    certificate request file that is shown above in
    <a class="xref" href="#sec-tls-private-metadata" title="Certificate request file">Example 29.1, “Certificate request file”</a>):
   </p><div class="verbatim-wrap"><pre class="screen">DNS.48 = "myardana.test"</pre></div></div><div id="id-1.4.6.6.14.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Any attempt to use IP addresses rather than FQDNs in certificates must use
    subject alternate name entries that list both the IP address (needed for
    Google) and DNS with an IP (needed for a Python bug workaround). Failure to
    create the certificates in this manner will cause future installations of
    Go-based tools (such as Cloud Foundry, Stackato and other PaaS components)
    to fail.
   </p></div></section><section class="sect1" id="id-1.4.6.6.15" data-id-title="Generate a certificate signing request"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.5 </span><span class="title-name">Generate a certificate signing request</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.15">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Now that you have a CA and a certificate request file, it is time to generate
   a CSR.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_KEY_FILE='example-internal-cert.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CSR_FILE='example-internal-cert.csr'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_REQ_FILE=example-internal-cert.req
<code class="prompt user">ardana &gt; </code>openssl req -newkey rsa:2048 -nodes -keyout "$EXAMPLE_SERVER_KEY_FILE" \
-out "$EXAMPLE_SERVER_CSR_FILE" -extensions v3_req -config "$EXAMPLE_SERVER_REQ_FILE"</pre></div><p>
   Note that in production you would usually send the generated
   <code class="literal">example-internal-cert.csr</code> file to your IT department. But
   in this example you are your own CA, so sign and generate a server
   certificate.
  </p></section><section class="sect1" id="id-1.4.6.6.16" data-id-title="Generate a server certificate"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.6 </span><span class="title-name">Generate a server certificate</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.16">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.6.6.16.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    In a production setting you will not perform this step. You will send the
    CSR created in the previous section to your company CA or a to a valid
    public CA and have them sign and send you back the certificate.
   </p></div><p>
   This section demonstrates how you would use your own self-signed CA that
   your created earlier to sign and generate a server certificate. A server
   certificate is essentially a signed public key, the signer being a CA and
   trusted by a client. When you install this the signing CA's certificate
   (called CA certificate or trust chain) on the client machine, you are
   telling the client to trust this CA, and thereby implicitly trusting any
   server certificates that are signed by this CA, thus creating a trust
   anchor.
  </p><p>
   <span class="bold"><strong>CA configuration file</strong></span>
  </p><p>
   When the CA signs the certificate, it uses a configuration file that tells
   it to verify the CSR. Note that in a production scenario the CA takes care
   of this for you.
  </p><p>
   Create a file called <code class="literal">openssl.cnf</code> and add the following
   contents to it.
  </p><div class="verbatim-wrap"><pre class="screen"># Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#...

# OpenSSL configuration file.
#

# Establish working directory.

dir = .

[ ca ]
default_ca = CA_default

[ CA_default ]
serial = $dir/serial
database = $dir/index.txt
new_certs_dir = $dir/
certificate = $dir/cacert.pem
private_key = $dir/cakey.pem
unique_subject = no
default_crl_days = 365
default_days = 365
default_md = md5
preserve = no
email_in_dn = no
nameopt = default_ca
certopt = default_ca
policy = policy_match
copy_extensions = copy


[ policy_match ]
countryName = optional
stateOrProvinceName = optional
organizationName = optional
organizationalUnitName = optional
commonName = supplied
emailAddress = optional

[ req ]
default_bits = 1024 # Size of keys
default_keyfile = key.pem # name of generated keys
default_md = md5 # message digest algorithm
string_mask = nombstr # permitted characters
distinguished_name = req_distinguished_name
req_extensions = v3_req
x509_extensions = v3_ca

[ req_distinguished_name ]
# Variable name Prompt string
#---------------------- ----------------------------------
0.organizationName = Organization Name (company)
organizationalUnitName = Organizational Unit Name (department, division)
emailAddress = Email Address
emailAddress_max = 40
localityName = Locality Name (city, district)
stateOrProvinceName = State or Province Name (full name)
countryName = Country Name (2 letter code)
countryName_min = 2
countryName_max = 2
commonName = Common Name (hostname, IP, or your name)
commonName_max = 64

# Default values for the above, for consistency and less typing.
# Variable name Value
#------------------------------ ------------------------------
0.organizationName_default = Exampleco PLC
localityName_default = Anytown
stateOrProvinceName_default = Anycounty
countryName_default = UK
commonName_default = my-CA

[ v3_ca ]
basicConstraints = CA:TRUE
subjectKeyIdentifier = hash
authorityKeyIdentifier = keyid:always,issuer:always
subjectAltName = @alt_names

[ v3_req ]
basicConstraints = CA:FALSE
subjectKeyIdentifier = hash

[ alt_names ]

######### end of openssl.cnf #########</pre></div><p>
   <span class="bold"><strong>Sign and create a server certificate</strong></span>
  </p><p>
   Now you can sign the server certificate with your CA. Copy the commands
   below to the command line and execute. This will cause the one file,
   example-internal-cert.crt, to be created:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CERT_FILE='example-internal-cert.crt'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CSR_FILE='example-internal-cert.csr'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_KEY_FILE='example-CA.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_CERT_FILE='example-CA.crt'
<code class="prompt user">ardana &gt; </code>touch index.txt
<code class="prompt user">ardana &gt; </code>openssl rand -hex -out serial 6
<code class="prompt user">ardana &gt; </code>openssl ca -batch -notext -md sha256 -in "$EXAMPLE_SERVER_CSR_FILE" \
-cert "$EXAMPLE_CA_CERT_FILE" \
-keyfile "$EXAMPLE_CA_KEY_FILE" \
-out "$EXAMPLE_SERVER_CERT_FILE" \
-config openssl.cnf -extensions v3_req</pre></div><p>
   Finally, concatenate both the server key and certificate in preparation for
   uploading to the Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat example-internal-cert.key example-internal-cert.crt &gt; example-internal-cert</pre></div><p>
   Note that you have only created the internal-cert in this example. Repeat
   the above sequence for example-public-cert. Make sure you use the
   appropriate certificate request generated by the configuration processor.
  </p></section><section class="sect1" id="sec-upload-toclm" data-id-title="Upload to the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.7 </span><span class="title-name">Upload to the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="#sec-upload-toclm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following two files created from the example run above will need to be
   uploaded to the Cloud Lifecycle Manager and copied into <code class="filename">config/tls</code>.
  </p><div class="itemizedlist" id="ul-zcc-v1c-5v"><ul class="itemizedlist"><li class="listitem"><p>
     example-internal-cert
    </p></li><li class="listitem"><p>
     example-CA.crt
    </p></li></ul></div><p>
   Once on the Cloud Lifecycle Manager, execute the following two copy commands to copy to their
   respective directories. Note if you had created an external cert, you can
   copy that in a similar manner, specifying its name using the copy command as
   well.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp example-internal-cert ~/openstack/my_cloud/config/tls/certs/
<code class="prompt user">ardana &gt; </code>cp example-CA.crt ~/openstack/my_cloud/config/tls/cacerts/</pre></div><p>
   <span class="bold"><strong>Continue with the deployment</strong></span>
  </p><p>
   Next, log into the Cloud Lifecycle Manager node, and save and commit the changes to the local
   git repository:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "updated certificate and CA"</pre></div><p>
   Next, rerun the <code class="literal">config-processor-run</code> playbook, and run
   <code class="literal">ready-deployment.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
   If you receive any prompts, enter the required information.
  </p><div id="id-1.4.6.6.17.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    For automated installation (for example CI) you can specify the required
    passwords on the Ansible command line. For example, the command below will
    disable encryption by the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</pre></div></div><p>
   Run this series of runbooks to complete the deployment:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tls-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></section><section class="sect1" id="id-1.4.6.6.18" data-id-title="Configuring the cipher suite"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.8 </span><span class="title-name">Configuring the cipher suite</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.18">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   By default, the cipher suite is set to:
   <code class="literal">HIGH:!aNULL:!eNULL:!DES:!3DES</code>. This setting is
   recommended in the
   <a class="link" href="http://docs.openstack.org/security-guide/secure-communication/introduction-to-ssl-and-tls.html" target="_blank">OpenStack
   documentation site</a>. You may override this. To do so, open
   <code class="filename">config/haproxy/defaults.yml</code> and edit it. The parameters
   can be found under the <code class="literal">haproxy_globals</code> list.
  </p><div class="verbatim-wrap"><pre class="screen">- "ssl-default-bind-ciphers HIGH:!aNULL:!eNULL:!DES:!3DES"
- "ssl-default-server-ciphers HIGH:!aNULL:!eNULL:!DES:!3DES"</pre></div><p>
   Make the changes as needed. It is best to keep the two options identical.
  </p></section><section class="sect1" id="id-1.4.6.6.19" data-id-title="Testing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.9 </span><span class="title-name">Testing</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.19">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can easily determine if an endpoint is behind TLS. To do so, run the
   following command, which probes a Keystone identity service endpoint that is
   behind TLS:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.5:5000 | openssl x509 -fingerprint -noout
depth=0 CN = openstack-vip
verify error:num=20:unable to get local issuer certificate
verify return:1
depth=0 CN = openstack-vip
verify error:num=27:certificate not trusted
verify return:1
depth=0 CN = openstack-vip
verify error:num=21:unable to verify the first certificate
verify return:1
DONE
SHA1 Fingerprint=C6:46:1E:59:C6:11:BF:72:5E:DD:FC:FF:B0:66:A7:A2:CC:32:1C:B8</pre></div><p>
   The next command probes a MariaDB endpoint that is not behind TLS:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.5:3306 | openssl x509 -fingerprint -noout
140448358213264:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:s23_clnt.c:795:
unable to load certificate
140454148159120:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:703
:Expecting: TRUSTED CERTIFICATE</pre></div></section><section class="sect1" id="id-1.4.6.6.20" data-id-title="Verifying that the trust chain is correctly deployed"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.10 </span><span class="title-name">Verifying that the trust chain is correctly deployed</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.20">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can determine if the trust chain is correctly deployed by running the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.9:5000 2&gt;/dev/null | grep code
Verify return code: 21 (unable to verify the first certificate)
echo | openssl s_client -connect 192.168.245.9:5000 \
-CAfile /etc/pki/trust/anchors/ca-certificates/openstack_frontend_cacert.crt 2&gt;/dev/null | grep code
Verify return code: 0 (ok)</pre></div><p>
   Here, the first command produces error 21, which is then fixed by providing
   the CA certificate file. This verifies that the CA certificate matches the
   server certificate.
  </p></section><section class="sect1" id="id-1.4.6.6.21" data-id-title="Turning TLS on or off"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">29.11 </span><span class="title-name">Turning TLS on or off</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.6.21">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-operations-configuring_tls.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You should leave TLS enabled in production. However, if you need to disable
   it for any reason, you must change "tls-components" to "components" in
   <code class="literal">network_groups.yml</code> (as shown earlier) and comment out the
   cert-file. Additionally, if you have a <code class="literal">network_groups.yml</code>
   file from a previous installation, you will not have TLS enabled unless you
   change "components" to "tls-components" in that file. By default, Horizon is
   configured with TLS in the input model. Note that you should not disable TLS
   in the input model for Horizon as that is a public endpoint and is required.
   Additionally, you should keep all services behind TLS, but using the input
   model file <code class="literal">network_groups.yml</code> you may turn TLS off for a
   service for troubleshooting or debugging. TLS should always be enabled for
   production environments.
  </p><p>
   If you are using an example input model on a clean install, all supported
   TLS services will be enabled before deployment of your cloud. If you want to
   change this setting later, for example, when upgrading, you can change the
   input model and reconfigure the system. The process is as follows:
  </p><p>
   Edit the input model <code class="literal">network_groups.yml</code> file
   appropriately, as described above. Then, commit the changes to the git
   repository:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "TLS change"</pre></div><p>
   Change directories again and run the configuration processor and ready
   deployment playbooks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
   Change directories again and run the reconfigure playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></section></section><section class="chapter" id="config-availability-zones" data-id-title="Configuring Availability Zones"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">30 </span><span class="title-name">Configuring Availability Zones</span></span> <a title="Permalink" class="permalink" href="#config-availability-zones">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-config_availability_zones.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The Cloud Lifecycle Manager only creates a default availability zone during
  installation. If your system has multiple failure/availability zones defined
  in your input model, these zones will not get created automatically.
 </p><p>
  Once the installation has finished, you can run the
  <code class="literal">nova-cloud-configure.yml</code> playbook to configure
  availability zones and assign compute nodes to those zones based on the
  configuration specified in the model.
 </p><p>
  You can run the playbook <code class="literal">nova-cloud-configure.yml</code> any time
  you make changes to the configuration of availability zones in your input
  model. Alternatively, you can use Horizon or the command line to perform the
  configuration.
 </p><p>
  
  For more details, see the OpenStack Availability Zone documentation at
 <a class="link" href="https://docs.openstack.org/nova/pike/user/aggregates.html" target="_blank">https://docs.openstack.org/nova/pike/user/aggregates.html</a>.
 </p></section><section class="chapter" id="OctaviaInstall" data-id-title="Configuring Load Balancer as a Service"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">31 </span><span class="title-name">Configuring Load Balancer as a Service</span></span> <a title="Permalink" class="permalink" href="#OctaviaInstall">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_lbaas.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Neutron LBaaS service supports several load balancing
    providers. By default, both Octavia and the namespace HAProxy driver are
    configured to be used. We describe this in more detail here.
   </p></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Neutron LBaaS service supports several load balancing
  providers. By default, both Octavia and the namespace HAProxy driver are
  configured to be used. A user can specify which provider to use with the
  <code class="option">--provider</code> flag upon load balancer creation.
 </p><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-create --name <em class="replaceable">NAME</em> --provider \
  [octavia|haproxy] <em class="replaceable">SUBNET</em></pre></div><p>
  If you do not specify the <code class="literal">--provider</code> option it will
  default to Octavia. The Octavia driver provides more functionality than the
  HAProxy namespace driver which is deprecated. The HAProxy namespace driver
  will be retired in a future version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  There are additional drivers for third-party hardware load balancers. Please
  refer to the vendor directly. The <code class="literal">neutron
  service-provider-list</code> command displays not only the currently
  installed load balancer drivers but also other installed services such as
  VPN. You can see a list of available services as follows:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron service-provider-list
+----------------+----------+---------+
| service_type   | name     | default |
+----------------+----------+---------+
| LOADBALANCERV2 | octavia  | True    |
| VPN            | openswan | True    |
| LOADBALANCERV2 | haproxy  | False   |
| LOADBALANCERV2 | octavia  | True    |
| VPN            | openswan | True    |
| LOADBALANCERV2 | haproxy  | False   |
+----------------+----------+---------+</pre></div><div id="id-1.4.6.8.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The Octavia load balancer provider is listed as the default.
  </p></div><section class="sect1" id="id-1.4.6.8.10" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">31.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.8.10">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_lbaas.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You will need to create an external network and create an image to test
   LBaaS functionality. If you have already created an external network and
   registered an image, this step can be skipped.
  </p><p>
   Creating an external network: <a class="xref" href="#create-extnet" title="27.4. Creating an External Network">Section 27.4, “Creating an External Network”</a>.
  </p><p>
   Creating and uploading a Glance image: <span class="intraxref">Book “User Guide”, Chapter 10 “Creating and Uploading a Glance Image”</span>.
  </p></section><section class="sect1" id="id-1.4.6.8.11" data-id-title="Octavia Load Balancing Provider"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">31.2 </span><span class="title-name">Octavia Load Balancing Provider</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.8.11">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_lbaas.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Octavia Load balancing provider bundled with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> is an
   operator grade load balancer for <span class="productname">OpenStack</span>. It is based on the
   <span class="productname">OpenStack</span> Pike version of Octavia. It differs from the namespace driver by
   starting a new Nova virtual machine to house the HAProxy load balancer
   software, called an <span class="emphasis"><em>amphora</em></span>, that provides the load
   balancer function. A virtual machine for each load balancer requested
   provides a better separation of load balancers between tenants and makes it
   easier to grow load balancing capacity alongside compute node growth.
   Additionally, if the virtual machine fails for any reason Octavia will
   replace it with a replacement VM from a pool of spare VMs, assuming that the
   feature is configured.
  </p><div id="id-1.4.6.8.11.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The Health Monitor will not create or replace failed amphorae. If the pool
    of spare VMs is exhausted there will be no additional virtual machines to
    handle load balancing requests.
   </p></div><p>
   Octavia uses two-way SSL encryption to communicate with the amphora. There
   are demo Certificate Authority (CA) certificates included with
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> in
   <code class="filename">~/scratch/ansible/next/ardana/ansible/roles/octavia-common/files</code>
   on the Cloud Lifecycle Manager. For additional security in production deployments, all
   certificate authorities should be replaced with ones you generated yourself
   by running the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl genrsa -passout pass:foobar -des3 -out cakey.pem 2048
<code class="prompt user">ardana &gt; </code>openssl req -x509 -passin pass:foobar -new -nodes -key cakey.pem -out ca_01.pem
<code class="prompt user">ardana &gt; </code>openssl genrsa -passout pass:foobar -des3 -out servercakey.pem 2048
<code class="prompt user">ardana &gt; </code>openssl req -x509 -passin pass:foobar -new -nodes -key cakey.pem -out serverca_01.pem</pre></div><p>
   For more details refer to the
   <a class="link" href="https://www.openssl.org/docs/manmaster/" target="_blank">openssl
   man page</a>.
  </p><div id="id-1.4.6.8.11.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    If you change the certificate authority and have amphora running with an
    old CA you will not be able to control the amphora. The amphoras will need
    to be failed over so they can utilize the new certificate. If you change
    the CA password for the server certificate you need to change that in the
    Octavia configuration files as well. For more information, see
    <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.3 “Networking Service Overview”, Section 9.3.9 “Load Balancer: Octavia Driver Administration”, Section 9.3.9.2 “Tuning Octavia Installation”</span>.
   </p></div></section><section class="sect1" id="id-1.4.6.8.12" data-id-title="Setup of prerequisites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">31.3 </span><span class="title-name">Setup of prerequisites</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.8.12">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_lbaas.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Octavia Network and Management Network
   Ports</strong></span>
  </p><p>
   The Octavia management network and Management network must have access to
   each other. If you have a configured firewall between the Octavia management
   network and Management network, you must open up the following ports to
   allow network traffic between the networks.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     From Management network to Octavia network
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       TCP 9443 (amphora API)
      </p></li></ul></div></li><li class="listitem"><p>
     From Octavia network to Management network
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       TCP 9876 (Octavia API)
      </p></li><li class="listitem"><p>
       UDP 5555 (Octavia Health Manager)
      </p></li></ul></div></li></ul></div><p>
   <span class="bold"><strong>Installing the Amphora Image</strong></span>
  </p><p>
   Octavia uses Nova VMs for its load balancing function and <span class="phrase"><span class="phrase">SUSE</span></span>
   provides images used to boot those VMs called
   <code class="literal">octavia-amphora</code>.
  </p><div id="id-1.4.6.8.12.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    Without these images the Octavia load balancer will not work.
   </p></div><p>
   <span class="bold"><strong>Register the image.</strong></span> The <span class="productname">OpenStack</span> load
   balancing service (Octavia) does not automatically register the Amphora
   guest image.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     The full path and name for the Amphora image is
     <code class="filename">/srv/www/suse-12.3/x86_64/repos/Cloud/suse/noarch/openstack-octavia-amphora-image-x86_64-0.1.0-1.21.noarch.rpm</code>
    </p><p>
     Switch to the ansible directory and register the image by giving the full
     path and name for the Amphora image as an argument to service_package:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts service-guest-image.yml \
-e service_package=\
/srv/www/suse-12.3/x86_64/repos/Cloud/suse/noarch/openstack-octavia-amphora-image-x86_64-0.1.0-1.21.noarch.rpm</pre></div></li><li class="step"><p>
     Source the service user (this can be done on a different computer)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>source service.osrc</pre></div></li><li class="step"><p>
     Verify that the image was registered (this can be done on a computer with
     access to the Glance CLI client)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image list
+--------------------------------------+------------------------+---------
| ID                                   | Name                   | Status |
+--------------------------------------+------------------------+--------+
...
| 1d4dd309-8670-46b6-801d-3d6af849b6a9 | octavia-amphora-x86_64 | active |
...</pre></div><div id="id-1.4.6.8.12.9.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      In the example above, the status of the
      <code class="literal">octavia-amphora-x86_64</code> image is
      <span class="emphasis"><em>active</em></span> which means the image was successfully
      registered. If a status of the images is <span class="emphasis"><em>queued</em></span>, you
      need to run the image registration again.
     </p><p>
      If you run the registration by accident, the system will only upload a
      new image if the underlying image has been changed.
     </p></div><p>
     Please be aware that if you have already created load balancers they will
     not receive the new image. Only load balancers created after the image has
     been successfully installed will use the new image. If existing load
     balancers need to be switched to the new image please follow the
     instructions in <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.3 “Networking Service Overview”, Section 9.3.9 “Load Balancer: Octavia Driver Administration”, Section 9.3.9.2 “Tuning Octavia Installation”</span>.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Setup network, subnet, router, security and
   IP's</strong></span>
  </p><p>
   If you have already created a network, subnet, router, security settings and
   IPs you can skip the following steps and go directly to creating the load
   balancers.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron network create lb_net1
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 71a1ac88-30a3-48a3-a18b-d98509fbef5c |
| mtu                       | 0                                    |
| name                      | lb_net1                              |
| provider:network_type     | vxlan                                |
| provider:physical_network |                                      |
| provider:segmentation_id  | 1061                                 |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tenant_id                 | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Create a subnet.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron subnet create --name lb_subnet1 lb_net1 10.247.94.128/26 \
  --gateway 10.247.94.129
+-------------------+----------------------------------------------------+
| Field             | Value                                              |
+-------------------+----------------------------------------------------+
| allocation_pools  | {"start": "10.247.94.130", "end": "10.247.94.190"} |
| cidr              | 10.247.94.128/26                                   |
| dns_nameservers   |                                                    |
| enable_dhcp       | True                                               |
| gateway_ip        | 10.247.94.129                                      |
| host_routes       |                                                    |
| id                | 6fc2572c-53b3-41d0-ab63-342d9515f514               |
| ip_version        | 4                                                  |
| ipv6_address_mode |                                                    |
| ipv6_ra_mode      |                                                    |
| name              | lb_subnet1                                         |
| network_id        | 71a1ac88-30a3-48a3-a18b-d98509fbef5c               |
| subnetpool_id     |                                                    |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f                   |
+-------------------+----------------------------------------------------+</pre></div></li><li class="step"><p>
     Create a router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron router create --distributed False lb_router1
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| distributed           | False                                |
| external_gateway_info |                                      |
| ha                    | False                                |
| id                    | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| name                  | lb_router1                           |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 4b31d0508f83437e83d8f4d520cda22f     |
+-----------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Add interface to router. In the following example, the interface
     <code class="literal">426c5898-f851-4f49-b01f-7a6fe490410c</code> will be added to
     the router <code class="literal">lb_router1</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron router add subnet lb_router1 lb_subnet1</pre></div></li><li class="step"><p>
     Set gateway for router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron router set lb_router1 ext-net</pre></div></li><li class="step"><p>
     Check networks.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron network list
+-----------------------------+------------------+-----------------------------+
| id                          | name             | subnets                     |
+-----------------------------+------------------+-----------------------------+
| d3cb12a6-a000-4e3e-         | ext-net          | f4152001-2500-4ebe-ba9d-    |
| 82c4-ee04aa169291           |                  | a8d6149a50df 10.247.96.0/23 |
| 8306282a-3627-445a-a588-c18 | OCTAVIA-MGMT-NET | f00299f8-3403-45ae-ac4b-    |
| 8b6a13163                   |                  | 58af41d57bdc                |
|                             |                  | 10.247.94.128/26            |
| 71a1ac88-30a3-48a3-a18b-    | lb_net1          | 6fc2572c-                   |
| d98509fbef5c                |                  | 53b3-41d0-ab63-342d9515f514 |
|                             |                  | 10.247.94.128/26            |
+-----------------------------+------------------+-----------------------------+</pre></div></li><li class="step"><p>
     Create security group.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group create lb_secgroup1
+----------------+---------------------------------------------------------------------------+
| Field          | Value                                                                     |
+----------------+---------------------------------------------------------------------------+
| description    |                                                                           |
| id             | 75343a54-83c3-464c-8773-802598afaee9                                      |
| name           | lb_secgroup1                                                              |
| security group | {"remote_group_id": null, "direction": "egress", "remote_ip_prefix": null,|
|    rules       | "protocol": null, "tenant_id": "4b31d...da22f", "port_range_max": null,   |
|                | "security_group_id": "75343a54-83c3-464c-8773-802598afaee9",              |
|                | "port_range_min": null, "ethertype": "IPv4", "id": "20ae3...97a7a"}       |
|                | {"remote_group_id": null, "direction": "egress",                          |
|                | "remote_ip_prefix": null, "protocol": null, "tenant_id": "4b31...a22f",   |
|                | "port_range_max": null, "security_group_id": "7534...98afaee9",           |
|                | "port_range_min": null, "ethertype": "IPv6", "id": "563c5c...aaef9"}      |
| tenant_id      | 4b31d0508f83437e83d8f4d520cda22f                                          |
+----------------+---------------------------------------------------------------------------+</pre></div></li><li class="step"><p>
     Create icmp security group rule.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group rule create lb_secgroup1 --protocol icmp
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 16d74150-a5b2-4cf6-82eb-a6c49a972d93 |
| port_range_max    |                                      |
| port_range_min    |                                      |
| protocol          | icmp                                 |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 75343a54-83c3-464c-8773-802598afaee9 |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f     |
+-------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Create TCP port 22 rule.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group rule create lb_secgroup1 --protocol tcp \
  --port-range-min 22 --port-range-max 22
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 472d3c8f-c50f-4ad2-97a1-148778e73af5 |
| port_range_max    | 22                                   |
| port_range_min    | 22                                   |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 75343a54-83c3-464c-8773-802598afaee9 |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f     |
+-------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Create TCP port 80 rule.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group rule create lb_secgroup1 --protocol tcp \
  --port-range-min 80 --port-range-max 80
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 10a76cad-8b1c-46f6-90e8-5dddd279e5f7 |
| port_range_max    | 80                                   |
| port_range_min    | 80                                   |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 75343a54-83c3-464c-8773-802598afaee9 |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f     |
+-------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     If you have not already created a keypair, create one now with
     <code class="literal">nova keypair create</code>. You will use the keypair to boot
     images.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova keypair create lb_kp1 &gt; lb_kp1.pem

chmod 400 lb_kp1.pem

cat lb_kp1.pem
-----BEGIN RSA PRIVATE KEY-----
MIIEqAIBAAKCAQEAkbW5W/XWGRGC0LAJI7lttR7EdDfiTDeFJ7A9b9Cff+OMXjhx
WL26eKIr+jp8DR64YjV2mNnQLsDyCxekFpkyjnGRId3KVAeV5sRQqXgtaCXI+Rvd
IyUtd8p1cp3DRgTd1dxO0oL6bBmwrZatNrrRn4HgKc2c7ErekeXrwLHyE0Pia/pz
C6qs0coRdfIeXxsmS3kXExP0YfsswRS/OyDl8QhRAF0ZW/zV+DQIi8+HpLZT+RW1
8sTTYZ6b0kXoH9wLER4IUBj1I1IyrYdxlAhe2VIn+tF0Ec4nDBn1py9iwEfGmn0+
N2jHCJAkrK/QhWdXO4O8zeXfL4mCZ9FybW4nzQIDAQABAoIBACe0PvgB+v8FuIGp
FjR32J8b7ShF+hIOpufzrCoFzRCKLruV4bzuphstBZK/0QG6Nz/7lX99Cq9SwCGp
pXrK7+3EoGl8CB/xmTUylVA4gRb6BNNsdkuXW9ZigrJirs0rkk8uIwRV0GsYbP5A
Kp7ZNTmjqDN75aC1ngRfhGgTlQUOdxBH+4xSb7GukekD13V8V5MF1Qft089asdWp
l/TpvhYeW9O92xEnZ3qXQYpXYQgEFQoM2PKa3VW7FGLgfw9gdS/MSqpHuHGyKmjl
uT6upUX+Lofbe7V+9kfxuV32sLL/S5YFvkBy2q8VpuEV1sXI7O7Sc411WX4cqmlb
YoFwhrkCggCBALkYE7OMTtdCAGcMotJhTiiS5l8d4U/fn1x0zus43XV5Y7wCnMuU
r5vCoK+a+TR9Ekzc/GjccAx7Wz/YYKp6G8FXW114dLcADXZjqjIlX7ifUud4sLCS
y+x3KAJa7LqyzH53I6FOts9RaB5xx4gZ2WjcJquCTbATZWj7j1yGeNgvAoIAgQDJ
h0r0Te5IliYbCRg+ES9YRZzH/PSLuIn00bbLvpOPNEoKe2Pxs+KI8Fqp6ZIDAB3c
4EPOK5QrJvAny9Z58ZArrNZi15t84KEVAkWUATl+c4SmHc8sW/atgmUoqIzgDQXe
AlwadHLY7JCdg7EYDuUxuTKLLOdqfpf6fKkhNxtEwwKCAIAMxi+d5aIPUxvKAOI/
2L1XKYRCrkI9i/ZooBsjusH1+JG8iQWfOzy/aDhExlJKoBMiQOIerpABHIZYqqtJ
OLIvrsK8ebK8aoGDWS+G1HN9v2kuVnMDTK5MPJEDUJkj7XEVjU1lNZSCTGD+MOYP
a5FInmEA1zZbX4tRKoNjZFh0uwKCAIEAiLs7drAdOLBu4C72fL4KIljwu5t7jATD
zRAwduIxmZq/lYcMU2RaEdEJonivsUt193NNbeeRWwnLLSUWupvT1l4pAt0ISNzb
TbbB4F5IVOwpls9ozc8DecubuM9K7YTIc02kkepqNZWjtMsx74HDrU3a5iSsSkvj
73Z/BeMupCMCggCAS48BsrcsDsHSHE3tO4D8pAIr1r+6WPaQn49pT3GIrdQNc7aO
d9PfXmPoe/PxUlqaXoNAvT99+nNEadp+GTId21VM0Y28pn3EkIGE1Cqoeyl3BEO8
f9SUiRNruDnH4F4OclsDBlmqWXImuXRfeiDHxM8X03UDZoqyHmGD3RqA53I=
-----END RSA PRIVATE KEY-----</pre></div></li><li class="step"><p>
     Check and boot images.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova image list
+--------------+------------------------+--------+--------+
| ID           | Name                   | Status | Server |
+--------------+------------------------+--------+--------+
| 0526d...7f39 | cirros-0.4.0-x86_64    | ACTIVE |        |
| 8aa51...8f2f | octavia-amphora-x86_64 | ACTIVE |        |
+--------------+------------------------+--------+--------+</pre></div><p>
     Boot first VM.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova server create --flavor 1 --image 04b1528b-b1e2-45d4-96d1-fbe04c6b2efd --key-name lb_kp1 \
  --security-groups lb_secgroup1 --nic net-id=71a1ac88-30a3-48a3-a18b-d98509fbef5c \
  lb_vm1 --poll
+--------------------------------------+--------------------------------------+
| Property                             | Value                                |
+--------------------------------------+--------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                               |
| OS-EXT-AZ:availability_zone          |                                      |
| OS-EXT-SRV-ATTR:host                 | -                                    |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                    |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000031                    |
| OS-EXT-STS:power_state               | 0                                    |
| OS-EXT-STS:task_state                | scheduling                           |
| OS-EXT-STS:vm_state                  | building                             |
| OS-SRV-USG:launched_at               | -                                    |
| OS-SRV-USG:terminated_at             | -                                    |
| accessIPv4                           |                                      |
| accessIPv6                           |                                      |
| adminPass                            | NeVvhP5E8iCy                         |
| config_drive                         |                                      |
| created                              | 2016-06-15T16:53:00Z                 |
| flavor                               | m1.tiny (1)                          |
| hostId                               |                                      |
| id                                   | dfdfe15b-ce8d-469c-a9d8-2cea0e7ca287 |
| image                                | cirros-0.4.0-x86_64 (0526d...7f39)   |
| key_name                             | lb_kp1                               |
| metadata                             | {}                                   |
| name                                 | lb_vm1                               |
| os-extended-volumes:volumes_attached | []                                   |
| progress                             | 0                                    |
| security_groups                      | lb_secgroup1                         |
| status                               | BUILD                                |
| tenant_id                            | 4b31d0508f83437e83d8f4d520cda22f     |
| updated                              | 2016-06-15T16:53:00Z                 |
| user_id                              | fd471475faa84680b97f18e55847ec0a     |
+--------------------------------------+--------------------------------------+

            Server building... 100% complete
            Finished</pre></div><p>
     Boot second VM.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova server create --flavor 1 --image 04b1528b-b1e2-45d4-96d1-fbe04c6b2efd --key-name lb_kp1 \
  --security-groups lb_secgroup1 --nic net-id=71a1ac88-30a3-48a3-a18b-d98509fbef5c \
  lb_vm2 --poll
+--------------------------------------+---------------------------------------+
| Property                             | Value                                 |
+--------------------------------------+---------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                |
| OS-EXT-AZ:availability_zone          |                                       |
| OS-EXT-SRV-ATTR:host                 | -                                     |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                     |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000034                     |
| OS-EXT-STS:power_state               | 0                                     |
| OS-EXT-STS:task_state                | scheduling                            |
| OS-EXT-STS:vm_state                  | building                              |
| OS-SRV-USG:launched_at               | -                                     |
| OS-SRV-USG:terminated_at             | -                                     |
| accessIPv4                           |                                       |
| accessIPv6                           |                                       |
| adminPass                            | 3nFXjNrTrmNm                          |
| config_drive                         |                                       |
| created                              | 2016-06-15T16:55:10Z                  |
| flavor                               | m1.tiny (1)                           |
| hostId                               |                                       |
| id                                   | 3844bb10-2c61-4327-a0d4-0c043c674344  |
| image                                | cirros-0.4.0-x86_64 (0526d...7f39)    |
| key_name                             | lb_kp1                                |
| metadata                             | {}                                    |
| name                                 | lb_vm2                                |
| os-extended-volumes:volumes_attached | []                                    |
| progress                             | 0                                     |
| security_groups                      | lb_secgroup1                          |
| status                               | BUILD                                 |
| tenant_id                            | 4b31d0508f83437e83d8f4d520cda22f      |
| updated                              | 2016-06-15T16:55:09Z                  |
| user_id                              | fd471475faa84680b97f18e55847ec0a      |
+--------------------------------------+---------------------------------------+

            Server building... 100% complete
            Finished</pre></div></li><li class="step"><p>
     List the running VM with <code class="literal">nova list</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova server list
+----------------+--------+--------+------------+-------------+-----------------------+
| ID             | Name   | Status | Task State | Power State | Networks              |
+----------------+--------+--------+------------+-------------+-----------------------+
| dfdfe...7ca287 | lb_vm1 | ACTIVE | -          | Running     | lb_net1=10.247.94.132 |
| 3844b...674344 | lb_vm2 | ACTIVE | -          | Running     | lb_net1=10.247.94.133 |
+----------------+--------+--------+------------+-------------+-----------------------+</pre></div></li><li class="step"><p>
     Check ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron port list
+----------------+------+-------------------+--------------------------------+
| id             | name | mac_address       | fixed_ips                      |
+----------------+------+-------------------+--------------------------------+
...
| 7e5e0...36450e |      | fa:16:3e:66:fd:2e | {"subnet_id": "6fc25...5f514", |
|                |      |                   | "ip_address": "10.247.94.132"} |
| ca95c...b36854 |      | fa:16:3e:e0:37:c4 | {"subnet_id": "6fc25...5f514", |
|                |      |                   | "ip_address": "10.247.94.133"} |
+----------------+------+-------------------+--------------------------------+</pre></div></li><li class="step"><p>
     Create the first floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip create ext-net --port-id 7e5e0038-88cf-4f97-a366-b58cd836450e
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.132                        |
| floating_ip_address | 10.247.96.26                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 3ce608bf-8835-4638-871d-0efe8ebf55ef |
| port_id             | 7e5e0038-88cf-4f97-a366-b58cd836450e |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | DOWN                                 |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Create the second floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip create ext-net --port-id ca95cc24-4e8f-4415-9156-7b519eb36854
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.133                        |
| floating_ip_address | 10.247.96.27                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 680c0375-a179-47cb-a8c5-02b836247444 |
| port_id             | ca95cc24-4e8f-4415-9156-7b519eb36854 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | DOWN                                 |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     List the floating IP's.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip list
+----------------+------------------+---------------------+---------------+
| id             | fixed_ip_address | floating_ip_address | port_id       |
+----------------+------------------+---------------------+---------------+
| 3ce60...bf55ef | 10.247.94.132    | 10.247.96.26        | 7e5e0...6450e |
| 680c0...247444 | 10.247.94.133    | 10.247.96.27        | ca95c...36854 |
+----------------+------------------+---------------------+---------------+</pre></div></li><li class="step"><p>
     Show first Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip show 3ce608bf-8835-4638-871d-0efe8ebf55ef
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.132                        |
| floating_ip_address | 10.247.96.26                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 3ce608bf-8835-4638-871d-0efe8ebf55ef |
| port_id             | 7e5e0038-88cf-4f97-a366-b58cd836450e |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | ACTIVE                               |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Show second Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip show 680c0375-a179-47cb-a8c5-02b836247444
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.133                        |
| floating_ip_address | 10.247.96.27                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 680c0375-a179-47cb-a8c5-02b836247444 |
| port_id             | ca95cc24-4e8f-4415-9156-7b519eb36854 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | ACTIVE                               |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Ping first Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ping -c 1 10.247.96.26
PING 10.247.96.26 (10.247.96.26) 56(84) bytes of data.
64 bytes from 10.247.96.26: icmp_seq=1 ttl=62 time=3.50 ms

--- 10.247.96.26 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 3.505/3.505/3.505/0.000 ms</pre></div></li><li class="step"><p>
     Ping second Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ping -c 1 10.247.96.27
PING 10.247.96.27 (10.247.96.27) 56(84) bytes of data.
64 bytes from 10.247.96.27: icmp_seq=1 ttl=62 time=3.47 ms

--- 10.247.96.27 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 3.473/3.473/3.473/0.000 ms</pre></div></li><li class="step"><p>
     Listing the VMs will give you both the fixed and floating IP's for each
     virtual machine.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova list
+---------------+--------+--------+-------+---------+-------------------------------------+
| ID            | Name   | Status | Task  | Power   | Networks                            |
|               |        |        | State | State   |                                     |
+---------------+--------+--------+-------+---------+-------------------------------------+
| dfdfe...ca287 | lb_vm1 | ACTIVE | -     | Running | lb_net1=10.247.94.132, 10.247.96.26 |
| 3844b...74344 | lb_vm2 | ACTIVE | -     | Running | lb_net1=10.247.94.133, 10.247.96.27 |
+---------------+--------+--------+-------+---------+-------------------------------------+</pre></div></li><li class="step"><p>
     List Floating IP's.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip list
+---------------+------------------+---------------------+-----------------+
| id            | fixed_ip_address | floating_ip_address | port_id         |
+---------------+------------------+---------------------+-----------------+
| 3ce60...f55ef | 10.247.94.132    | 10.247.96.26        | 7e5e00...36450e |
| 680c0...47444 | 10.247.94.133    | 10.247.96.27        | ca95cc...b36854 |
+---------------+------------------+---------------------+-----------------+</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.6.8.13" data-id-title="Create Load Balancers"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">31.4 </span><span class="title-name">Create Load Balancers</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.8.13">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_lbaas.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following steps will setup new Octavia Load Balancers.
  </p><div id="id-1.4.6.8.13.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The following examples assume names and values from the previous section.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create load balancer for subnet
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-create --provider octavia \
  --name lb1 6fc2572c-53b3-41d0-ab63-342d9515f514
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| description         |                                      |
| id                  | 3d9170a1-8605-43e6-9255-e14a8b4aae53 |
| listeners           |                                      |
| name                | lb1                                  |
| operating_status    | OFFLINE                              |
| provider            | octavia                              |
| provisioning_status | PENDING_CREATE                       |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
| vip_address         | 10.247.94.134                        |
| vip_port_id         | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51 |
| vip_subnet_id       | 6fc2572c-53b3-41d0-ab63-342d9515f514 |
+---------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     List load balancers. You will need to wait until the load balancer
     <code class="literal">provisioning_status</code>is <code class="literal">ACTIVE</code> before
     proceeding to the next step.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-list
+---------------+------+---------------+---------------------+----------+
| id            | name | vip_address   | provisioning_status | provider |
+---------------+------+---------------+---------------------+----------+
| 3d917...aae53 | lb1  | 10.247.94.134 | ACTIVE              | octavia  |
+---------------+------+---------------+---------------------+----------+</pre></div></li><li class="step"><p>
     Once the load balancer is created, create the listener. This may take some
     time.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-listener-create --loadbalancer lb1 \
  --protocol HTTP --protocol-port=80 --name lb1_listener
+---------------------------+------------------------------------------------+
| Field                     | Value                                          |
+---------------------------+------------------------------------------------+
| admin_state_up            | True                                           |
| connection_limit          | -1                                             |
| default_pool_id           |                                                |
| default_tls_container_ref |                                                |
| description               |                                                |
| id                        | c723b5c8-e2df-48d5-a54c-fc240ac7b539           |
| loadbalancers             | {"id": "3d9170a1-8605-43e6-9255-e14a8b4aae53"} |
| name                      | lb1_listener                                   |
| protocol                  | HTTP                                           |
| protocol_port             | 80                                             |
| sni_container_refs        |                                                |
| tenant_id                 | 4b31d0508f83437e83d8f4d520cda22f               |
+---------------------------+------------------------------------------------+</pre></div></li><li class="step"><p>
     Create the load balancing pool. During the creation of the load balancing
     pool, the status for the load balancer goes to
     <code class="literal">PENDING_UPDATE</code>. Use <code class="literal">neutron
     lbaas-loadbalancer-list</code> to watch for the change to
     <code class="literal">ACTIVE</code>. Once the load balancer returns to
     <code class="literal">ACTIVE</code>, proceed with the next step.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-pool-create --lb-algorithm ROUND_ROBIN \
  --listener lb1_listener --protocol HTTP --name lb1_pool
+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| healthmonitor_id    |                                                |
| id                  | 0f5951ee-c2a0-4e62-ae44-e1491a8988e1           |
| lb_algorithm        | ROUND_ROBIN                                    |
| listeners           | {"id": "c723b5c8-e2df-48d5-a54c-fc240ac7b539"} |
| members             |                                                |
| name                | lb1_pool                                       |
| protocol            | HTTP                                           |
| session_persistence |                                                |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f               |
+---------------------+------------------------------------------------+</pre></div></li><li class="step"><p>
     Create first member of the load balancing pool.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-member-create --subnet 6fc2572c-53b3-41d0-ab63-342d9515f514 \
  --address 10.247.94.132 --protocol-port 80 lb1_pool
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| address        | 10.247.94.132                        |
| admin_state_up | True                                 |
| id             | 61da1e21-e0ae-4158-935a-c909a81470e1 |
| protocol_port  | 80                                   |
| subnet_id      | 6fc2572c-53b3-41d0-ab63-342d9515f514 |
| tenant_id      | 4b31d0508f83437e83d8f4d520cda22f     |
| weight         | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Create the second member.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-member-create --subnet 6fc2572c-53b3-41d0-ab63-342d9515f514 \
  --address 10.247.94.133 --protocol-port 80 lb1_pool
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| address        | 10.247.94.133                        |
| admin_state_up | True                                 |
| id             | 459c7f21-46f7-49e8-9d10-dc7da09f8d5a |
| protocol_port  | 80                                   |
| subnet_id      | 6fc2572c-53b3-41d0-ab63-342d9515f514 |
| tenant_id      | 4b31d0508f83437e83d8f4d520cda22f     |
| weight         | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step"><p>
     You should check to make sure the load balancer is active and check the
     pool members.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-list
+----------------+------+---------------+---------------------+----------+
| id             | name | vip_address   | provisioning_status | provider |
+----------------+------+---------------+---------------------+----------+
| 3d9170...aae53 | lb1  | 10.247.94.134 | ACTIVE              | octavia  |
+----------------+------+---------------+---------------------+----------+

neutron lbaas-member-list lb1_pool
+---------------+---------------+---------------+--------+---------------+----------------+
| id            | address       | protocol_port | weight | subnet_id     | admin_state_up |
+---------------+---------------+---------------+--------+---------------+----------------+
| 61da1...470e1 | 10.247.94.132 |            80 |      1 | 6fc25...5f514 | True           |
| 459c7...f8d5a | 10.247.94.133 |            80 |      1 | 6fc25...5f514 | True           |
+---------------+---------------+---------------+--------+---------------+----------------+</pre></div></li><li class="step"><p>
     You can view the details of the load balancer, listener and pool.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-show 3d9170a1-8605-43e6-9255-e14a8b4aae53
+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| id                  | 3d9170a1-8605-43e6-9255-e14a8b4aae53           |
| listeners           | {"id": "c723b5c8-e2df-48d5-a54c-fc240ac7b539"} |
| name                | lb1                                            |
| operating_status    | ONLINE                                         |
| provider            | octavia                                        |
| provisioning_status | ACTIVE                                         |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f               |
| vip_address         | 10.247.94.134                                  |
| vip_port_id         | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51           |
| vip_subnet_id       | 6fc2572c-53b3-41d0-ab63-342d9515f514           |
+---------------------+------------------------------------------------+

neutron lbaas-listener-list
+-------------+-----------------+--------------+----------+---------------+----------------+
| id          | default_pool_id | name         | protocol | protocol_port | admin_state_up |
+-------------+-----------------+--------------+----------+---------------+----------------+
| c723...b539 | 0f595...8988e1  | lb1_listener | HTTP     |            80 | True           |
+-------------+-----------------+--------------+----------+---------------+----------------+

neutron lbaas-pool-list
+--------------------------------------+----------+----------+----------------+
| id                                   | name     | protocol | admin_state_up |
+--------------------------------------+----------+----------+----------------+
| 0f5951ee-c2a0-4e62-ae44-e1491a8988e1 | lb1_pool | HTTP     | True           |
+--------------------------------------+----------+----------+----------------+</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.6.8.14" data-id-title="Create Floating IPs for Load Balancer"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">31.5 </span><span class="title-name">Create Floating IPs for Load Balancer</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.8.14">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_lbaas.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To create the floating IP's for the load balancer, you will need to list the
   current ports to get the load balancer id. Once you have the id, you can
   then create the floating IP.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     List the current ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron port list
+--------------+---------------+-------------------+-----------------------------------------+
| id           | name          | mac_address       | fixed_ips                               |
+--------------+---------------+-------------------+-----------------------------------------+
...
| 7e5e...6450e |               | fa:16:3e:66:fd:2e | {"subnet_id": "6fc2572c-                |
|              |               |                   | 53b3-41d0-ab63-342d9515f514",           |
|              |               |                   | "ip_address": "10.247.94.132"}          |
| a3d0...55efe |               | fa:16:3e:91:a2:5b | {"subnet_id": "f00299f8-3403-45ae-ac4b- |
|              |               |                   | 58af41d57bdc", "ip_address":            |
|              |               |                   | "10.247.94.142"}                        |
| ca95...36854 |               | fa:16:3e:e0:37:c4 | {"subnet_id": "6fc2572c-                |
|              |               |                   | 53b3-41d0-ab63-342d9515f514",           |
|              |               |                   | "ip_address": "10.247.94.133"}          |
| da28...c3c51 | loadbalancer- | fa:16:3e:1d:a2:1c | {"subnet_id": "6fc2572c-                |
|              | 3d917...aae53 |                   | 53b3-41d0-ab63-342d9515f514",           |
|              |               |                   | "ip_address": "10.247.94.134"}          |
+--------------+---------------+-------------------+-----------------------------------------+</pre></div></li><li class="step"><p>
     Create the floating IP for the load balancer.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip create ext-net --port-id da28aed3-0eb4-4139-afcf-2d8fd3fc3c51
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.134                        |
| floating_ip_address | 10.247.96.28                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 9a3629bd-b0a6-474c-abe9-89c6ecb2b22c |
| port_id             | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | DOWN                                 |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.6.8.15" data-id-title="Testing the Octavia Load Balancer"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">31.6 </span><span class="title-name">Testing the Octavia Load Balancer</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.8.15">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-configure_lbaas.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To test the load balancers, create the following web server script so you
   can run it on each virtual machine. You will use <code class="literal">curl &lt;ip
   address&gt;</code> to test if the load balance services are responding
   properly.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start running web servers on both of the virtual machines. Create the
     webserver.sh script with below contents. In this example, the port is 80.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>vi webserver.sh

#!/bin/bash

MYIP=$(/sbin/ifconfig eth0|grep 'inet addr'|awk -F: '{print $2}'| awk '{print $1}');
while true; do
    echo -e "HTTP/1.0 200 OK

Welcome to $MYIP" | sudo nc -l -p 80
done</pre></div></li><li class="step"><p>
     Deploy the web server and run it on the first virtual machine.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh-keygen -R 10.247.96.26
/home/ardana/.ssh/known_hosts updated.
Original contents retained as /home/ardana/.ssh/known_hosts.old

scp -o StrictHostKeyChecking=no -i lb_kp1.pem webserver.sh cirros@10.247.96.26:
webserver.sh                                      100%  263     0.3KB/s   00:00

ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@10.247.96.26 'chmod +x ./webserver.sh'
ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@10.247.96.26 ./webserver.sh</pre></div></li><li class="step"><p>
     Test the first web server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>curl 10.247.96.26
 Welcome to 10.247.94.132</pre></div></li><li class="step"><p>
     Deploy and start the web server on the second virtual machine like you did
     in the previous steps. Once the second web server is running, list the
     floating IPs.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip list
+----------------+------------------+---------------------+---------------+
| id             | fixed_ip_address | floating_ip_address | port_id       |
+----------------+------------------+---------------------+---------------+
| 3ce60...bf55ef | 10.247.94.132    | 10.247.96.26        | 7e5e0...6450e |
| 680c0...247444 | 10.247.94.133    | 10.247.96.27        | ca95c...36854 |
| 9a362...b2b22c | 10.247.94.134    | 10.247.96.28        | da28a...c3c51 |
+----------------+------------------+---------------------+---------------+</pre></div></li><li class="step"><p>
     Display the floating IP for the load balancer.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip show 9a3629bd-b0a6-474c-abe9-89c6ecb2b22c
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.134                        |
| floating_ip_address | 10.247.96.28                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 9a3629bd-b0a6-474c-abe9-89c6ecb2b22c |
| port_id             | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | ACTIVE                               |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step"><p>
     Finally, test the load balancing.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.132

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.133

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.132

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.133

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.132

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.133</pre></div></li></ol></div></div></section></section><section class="chapter" id="postinstall-checklist" data-id-title="Other Common Post-Installation Tasks"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">32 </span><span class="title-name">Other Common Post-Installation Tasks</span></span> <a title="Permalink" class="permalink" href="#postinstall-checklist">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-postinstall_tasks.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="id-1.4.6.9.2" data-id-title="Determining Your User Credentials"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">32.1 </span><span class="title-name">Determining Your User Credentials</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.9.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-postinstall_tasks.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   On your Cloud Lifecycle Manager, in the
   <code class="literal">~/scratch/ansible/next/ardana/ansible/group_vars/</code> directory
   you will find several files. In the one labeled as first control plane node
   you can locate the user credentials for both the Administrator user
   (<code class="literal">admin</code>) and your Demo user (<code class="literal">demo</code>)
   which you will use to perform many other actions on your cloud.
  </p><p>
   For example, if you are using the Entry-scale KVM model and used
   the default naming scheme given in the example configuration files, you can
   use these commands on your Cloud Lifecycle Manager to <code class="command">grep</code> for your user
   credentials:
  </p><p>
   <span class="bold"><strong>Administrator</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep keystone_admin_pwd entry-scale-kvm-control-plane-1</pre></div><p>
   <span class="bold"><strong>Demo</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep keystone_demo_pwd entry-scale-kvm-control-plane-1</pre></div></section><section class="sect1" id="id-1.4.6.9.3" data-id-title="Configure your Cloud Lifecycle Manager to use the command-line tools"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">32.2 </span><span class="title-name">Configure your Cloud Lifecycle Manager to use the command-line tools</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.9.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-postinstall_tasks.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This playbook will do a series of steps to update your environment variables
   for your cloud so you can use command-line clients.
  </p><p>
   Run the following command, which will replace <code class="literal">/etc/hosts</code>
   on the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml</pre></div><p>
   As the <code class="filename">/etc/hosts</code> file no longer has entries for Cloud Lifecycle Manager,
   sudo commands may become a bit slower. To fix this issue, once
   this step is complete, add "ardana" after "127.0.0.1 localhost". The result
   will look like this:
  </p><div class="verbatim-wrap"><pre class="screen">...
# Localhost Information
127.0.0.1 localhost ardana</pre></div></section><section class="sect1" id="id-1.4.6.9.4" data-id-title="Protect home directory"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">32.3 </span><span class="title-name">Protect home directory</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.9.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-postinstall_tasks.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The home directory of the user that owns the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> scripts should
   not be world readable. Change the permissions so that they are only readable
   by the owner:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>chmod 0700 ~</pre></div></section><section class="sect1" id="id-1.4.6.9.5" data-id-title="Back up Your SSH Keys"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">32.4 </span><span class="title-name">Back up Your SSH Keys</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.9.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-postinstall_tasks.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As part of the cloud deployment setup process, SSH keys to access the
   systems are generated and stored in <code class="literal">~/.ssh</code> on your
   Cloud Lifecycle Manager.
  </p><p>
   These SSH keys allow access to the subsequently deployed systems and should
   be included in the list of content to be archived in any backup strategy.
  </p></section><section class="sect1" id="id-1.4.6.9.6" data-id-title="Retrieving Service Endpoints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">32.5 </span><span class="title-name">Retrieving Service Endpoints</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.9.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-postinstall_tasks.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Source the keystone admin credentials:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>unset OS_TENANT_NAME
<code class="prompt user">ardana &gt; </code>source ~/keystone.osrc</pre></div></li><li class="step"><p>
     Using the <span class="productname">OpenStack</span> command-line tool you can then query the Keystone
     service for your endpoints:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack endpoint list</pre></div><div id="id-1.4.6.9.6.2.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
      You can use <code class="literal">openstack -h</code> to access the client help
      file and a full list of commands.
     </p></div></li></ol></div></div><p>
   To learn more about Keystone, see
   <span class="intraxref">Book “Operations Guide”, Chapter 4 “Managing Identity”, Section 4.1 “The Identity Service”</span>.
  </p></section><section class="sect1" id="id-1.4.6.9.7" data-id-title="Other Common Post-Installation Tasks"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">32.6 </span><span class="title-name">Other Common Post-Installation Tasks</span></span> <a title="Permalink" class="permalink" href="#id-1.4.6.9.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-installation-postinstall_tasks.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Here are the links to other common post-installation tasks that either the
   Administrator or Demo users can perform:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 5 “Managing Compute”, Section 5.4 “Enabling the Nova Resize and Migrate Features”</span>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#create-extnet" title="27.4. Creating an External Network">Section 27.4, “Creating an External Network”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#upload-image" title="27.3. Uploading an Image for Use">Section 27.3, “Uploading an Image for Use”</a>
    </p></li><li class="listitem"><p>
     <span class="intraxref">Book “User Guide”, Chapter 8 “Creating a Private Network”</span>
    </p></li><li class="listitem"><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.1 “Running the Swift Dispersion Report”</span>
    </p></li><li class="listitem"><p>
     <span class="intraxref">Book “Security Guide”, Chapter 4 “<span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud</span></span>: Service Admin Role Segregation in the Identity Service”</span>
    </p></li></ul></div></section></section></div><section class="chapter" id="cha-inst-trouble" data-id-title="Support"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">33 </span><span class="title-name">Support</span></span> <a title="Permalink" class="permalink" href="#cha-inst-trouble">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Find solutions for the most common pitfalls and technical details on how
  to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> here.
 </p><section class="sect1" id="sec-depl-trouble-faq" data-id-title="FAQ"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">33.1 </span><span class="title-name">FAQ</span></span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-faq">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-support.xml" title="Edit source document"> </a></div></div></div></div></div><div class="qandaset" id="id-1.4.7.4.3"><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="id-1.4.7.4.3.1">1. Node Deployment</h4></div><div class="qandadiv"><div class="free-id" id="id-1.4.7.4.3.1.2"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.4.3.1.2.1"><strong>Q:</strong>
       How to Disable the YaST Installer Self-Update when deploying nodes?
      </dt><dd class="answer" id="id-1.4.7.4.3.1.2.2"><p>
       Prior to starting an installation, the YaST installer can update
       itself if respective updates are available. By default this feature is
       enabled. In case of problems with this feature, disable it as follows:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Open
         <code class="filename">~/openstack/ardana/ansible/roles/cobbler/templates/sles.grub.j2</code>
         with an editor and add <code class="literal">self_update=0</code> to the line
         starting with <code class="literal">linuxefi</code>. The results needs to look
         like the following:
        </p><div class="verbatim-wrap"><pre class="screen">linuxefi images/{{ sles_profile_name }}-x86_64/linux ifcfg={{ item[0] }}=dhcp install=http://{{ cobbler_server_ip_addr }}:79/cblr/ks_mirror/{{ sles_profile_name }} self_update=0 AutoYaST2=http://{{ cobbler_server_ip_addr }}:79/cblr/svc/op/ks/system/{{ item[1] }}</pre></div></li><li class="step"><p>
         Commit your changes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Disable Yast Self Update feature" \
~/openstack/ardana/ansible/roles/cobbler/templates/sles.grub.j2</pre></div></li><li class="step"><p>
         If you need to reenable the installer self-update, remove
         <code class="literal">self_update=0</code> and commit the changes.
        </p></li></ol></div></div></dd></dl></div></div></section><section class="sect1" id="sec-installation-trouble-support" data-id-title="Support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">33.2 </span><span class="title-name">Support</span></span> <a title="Permalink" class="permalink" href="#sec-installation-trouble-support">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   
   Before contacting support to help you with a problem on your cloud, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   ships with a tool called <code class="command">supportconfig</code>. It gathers
   system information such as the current kernel version being used, the
   hardware, RPM database, partitions, and other items.
   <code class="command">supportconfig</code> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </p><p>
   It is recommended to always run <code class="command">supportconfig</code> on the
   CLM Server and on the Control Node(s). If a Compute Node or a
   Storage Node is part of the problem, run
   <code class="command">supportconfig</code> on the affected node as well. For
   details on how to run <code class="command">supportconfig</code>, see
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support</a>.
  </p><section class="sect2" id="inst-support-ptf" data-id-title="Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">33.2.1 </span><span class="title-name">
    Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support
   </span></span> <a title="Permalink" class="permalink" href="#inst-support-ptf">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Under certain circumstances, the SUSE support may provide temporary
    fixes, the so-called PTFs, to customers with an L3 support contract.
    These PTFs are provided as RPM packages. To make them available on all
    nodes in SUSE <span class="productname">OpenStack</span> Cloud, proceed as follows. If you prefer to test them first on a
    single node, see <a class="xref" href="#inst-support-ptf-test" title="33.2.2.  Testing PTFs (Program Temporary Fixes) on a Single Node">Section 33.2.2, “
    Testing PTFs (Program Temporary Fixes) on a Single Node
   ”</a>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the packages from the location provided by the SUSE L3
      Support to a temporary location on the CLM Server.
     </p></li><li class="step"><p>
      Move the packages from the temporary download location to the
      following directories on the CLM Server:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.5.4.3.2.2.1"><span class="term">
        <span class="quote">“<span class="quote">noarch</span>”</span> packages (<code class="filename">*.noarch.rpm</code>):
       </span></dt><dd><p>
         <code class="filename">/srv/www/suse-12.2/x86_64/repos/PTF/rpm/noarch/</code>
        </p></dd><dt id="id-1.4.7.5.4.3.2.2.2"><span class="term">
        <span class="quote">“<span class="quote">x86_64</span>”</span> packages (<code class="filename">*.x86_64.rpm</code>)
       </span></dt><dd><p>
         <code class="filename">/srv/www/suse-12.2/x86_64/repos/PTF/rpm/x86_64/</code>
        </p></dd></dl></div></li><li class="step"><p>
      Create or update the repository metadata:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>createrepo-cloud-ptf</pre></div></li><li class="step"><p>
      To deploy the updates, proceed as described in <span class="intraxref">Book “Operations Guide”, Chapter 13 “System Maintenance”, Section 13.3 “Cloud Lifecycle Manager Maintenance Update Procedure”</span> and refresh the PTF repository before
      installing package updates on a node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper refresh -fr PTF</pre></div></li></ol></div></div></section><section class="sect2" id="inst-support-ptf-test" data-id-title="Testing PTFs (Program Temporary Fixes) on a Single Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">33.2.2 </span><span class="title-name">
    Testing PTFs (Program Temporary Fixes) on a Single Node
   </span></span> <a title="Permalink" class="permalink" href="#inst-support-ptf-test">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you want to test a PTF (Program Temporary Fix) before deploying
    it on all nodes, if you want to verify that it fixes a certain issue, you
    can manually install the PTF on a single node.
   </p><p>
     In the following procedure, a PTF named
     <code class="filename">venv-openstack-nova-x86_64-ptf.rpm</code>, containing a fix
     for Nova, is installed on the Compute Node 01.
    </p><div class="procedure" id="id-1.4.7.5.5.4" data-id-title="Testing a Fix for Nova"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 33.1: </span><span class="title-name">Testing a Fix for Nova </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.5.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/installation-support.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Check the version number of the package(s) that will be upgraded with
       the PTF. Run the following command on the deployer node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>rpm -q venv-openstack-nova-x86_64</pre></div></li><li class="step"><p>
       Install the PTF on the deployer node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper up ./venv-openstack-nova-x86_64-ptf.rpm</pre></div><p>
       This will install a new TAR archive in
       <code class="filename">/opt/ardana_packager/ardana-8/sles_venv/x86_64/</code>.
      </p></li><li class="step"><p>
       Register the TAR archive with the indexer:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo  create_index --dir \
      /opt/ardana_packager/ardana-8/sles_venv/x86_64</pre></div><p>
       This will update the indexer
       <code class="filename">/opt/ardana_packager/ardana-8/sles_venv/x86_64/packages</code>.
      </p></li><li class="step"><p>
       Deploy the fix on Compute Node 01:
      </p><ol type="a" class="substeps"><li class="step"><p>
         Check whether the fix can be deployed on a single Compute Node without
         updating the Control Nodes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt --list-hosts</pre></div></li><li class="step"><p>
         If the previous test passes, install the fix:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt</pre></div></li></ol></li><li class="step"><p>
       Validate the fix, for example by logging in to the Compute Node to check
       the log files:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh ardana@inputmodel-ccp-compute0001-mgmt</pre></div></li><li class="step"><p>
       In case your tests are positive, install the PTF on all nodes as
       described in <a class="xref" href="#inst-support-ptf" title="33.2.1.  Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support">Section 33.2.1, “
    Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support
   ”</a>.
      </p><p>
       In case the test are negative uninstall the fix and restore the previous
       state of the Compute Node by running the following commands on the
       deployer node;
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install --force venv-openstack-nova-x86_64-<em class="replaceable">OLD-VERSION</em>
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt</pre></div><p>
       Make sure to replace <em class="replaceable">OLD-VERSION</em> with the
       version number you checked in the first step.
      </p></li></ol></div></div></section></section></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>