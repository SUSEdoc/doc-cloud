<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Compute | OpenStack Administrator Guide | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" />
<meta name="title" content="Compute | SUSE OpenStack Cloud 8" />
<meta name="description" content="The OpenStack Compute service allows you to control an Infrastructure-as-a-Service (IaaS) cloud computing platform. It gives you control over instances and net…" />
<meta name="product-name" content="SUSE OpenStack Cloud" />
<meta name="product-number" content="8" />
<meta name="book-title" content="OpenStack Administrator Guide" />
<meta name="chapter-title" content="Chapter 5. Compute" />
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" />
<meta name="tracker-type" content="bsc" />
<meta name="tracker-bsc-component" content="Documentation" />
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" />
<meta property="og:title" content="Compute | SUSE OpenStack Cloud 8" />
<meta property="og:description" content="The OpenStack Compute service allows you to control an Infrastructure-as-a-Service (IaaS) cloud computing platform. It gives you control over instances and net…" />
<meta property="og:type" content="article" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Compute | SUSE OpenStack Cloud 8" />
<meta name="twitter:description" content="The OpenStack Compute service allows you to control an Infrastructure-as-a-Service (IaaS) cloud computing platform. It gives you control over instances and net…" />
<link rel="home" href="index.html" title="SUSE OpenStack Cloud Crowbar 8 Documentation" /><link rel="up" href="book-upstream-admin.html" title="OpenStack Administrator Guide" /><link rel="prev" href="bk02ch04.html" title="Chapter 4. Dashboard" /><link rel="next" href="bk02ch06.html" title="Chapter 6. Object Storage" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="OpenStack Administrator Guide"><span class="book-icon">OpenStack Administrator Guide</span></a><span> › </span><a class="crumb" href="bk02ch05.html">Compute</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>OpenStack Administrator Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="bk02ch01.html"><span class="number">1 </span><span class="name">Documentation Conventions</span></a></li><li class="inactive"><a href="bk02ch02.html"><span class="number">2 </span><span class="name">Get started with OpenStack</span></a></li><li class="inactive"><a href="cha-identity.html"><span class="number">3 </span><span class="name">Identity management</span></a></li><li class="inactive"><a href="bk02ch04.html"><span class="number">4 </span><span class="name">Dashboard</span></a></li><li class="inactive"><a href="bk02ch05.html"><span class="number">5 </span><span class="name">Compute</span></a></li><li class="inactive"><a href="bk02ch06.html"><span class="number">6 </span><span class="name">Object Storage</span></a></li><li class="inactive"><a href="bk02ch07.html"><span class="number">7 </span><span class="name">Block Storage</span></a></li><li class="inactive"><a href="bk02ch08.html"><span class="number">8 </span><span class="name">Shared File Systems</span></a></li><li class="inactive"><a href="networking.html"><span class="number">9 </span><span class="name">Networking</span></a></li><li class="inactive"><a href="bk02ch10.html"><span class="number">10 </span><span class="name">Telemetry</span></a></li><li class="inactive"><a href="bk02ch11.html"><span class="number">11 </span><span class="name">Database</span></a></li><li class="inactive"><a href="bk02ch12.html"><span class="number">12 </span><span class="name">Bare Metal</span></a></li><li class="inactive"><a href="bk02ch13.html"><span class="number">13 </span><span class="name">Orchestration</span></a></li><li class="inactive"><a href="osadm-os-cli.html"><span class="number">14 </span><span class="name">OpenStack command-line clients</span></a></li><li class="inactive"><a href="bk02ch15.html"><span class="number">15 </span><span class="name">Cross-project features</span></a></li><li class="inactive"><a href="bk02ch16.html"><span class="number">16 </span><span class="name">Appendix</span></a></li><li class="inactive"><a href="bk02go01.html"><span class="number"> </span><span class="name">Glossary</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 4. Dashboard" href="bk02ch04.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 6. Object Storage" href="bk02ch06.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="OpenStack Administrator Guide"><span class="book-icon">OpenStack Administrator Guide</span></a><span> › </span><a class="crumb" href="bk02ch05.html">Compute</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 4. Dashboard" href="bk02ch04.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 6. Object Storage" href="bk02ch06.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="id-1.4.7"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE OpenStack Cloud</span> <span class="productnumber ">8</span></div><div><h1 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute</span> <a title="Permalink" class="permalink" href="bk02ch05.html#">#</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="bk02ch05.html#id-1.4.7.5"><span class="number">5.1 </span><span class="name">System architecture</span></a></span></dt><dt><span class="sect1"><a href="bk02ch05.html#id-1.4.7.6"><span class="number">5.2 </span><span class="name">Images and instances</span></a></span></dt><dt><span class="sect1"><a href="bk02ch05.html#id-1.4.7.7"><span class="number">5.3 </span><span class="name">Networking with nova-network</span></a></span></dt><dt><span class="sect1"><a href="bk02ch05.html#id-1.4.7.8"><span class="number">5.4 </span><span class="name">System administration</span></a></span></dt><dt><span class="sect1"><a href="bk02ch05.html#id-1.4.7.9"><span class="number">5.5 </span><span class="name">Troubleshoot Compute</span></a></span></dt></dl></div></div><p>The OpenStack Compute service allows you to control an
<a class="xref" href="bk02go01.html#term-infrastructure-as-a-service-iaas" title="Infrastructure-as-a-Service (IaaS)">Infrastructure-as-a-Service (IaaS)</a> cloud computing platform.
It gives you control over instances and networks, and allows you to manage
access to the cloud through users and projects.</p><p>Compute does not include virtualization software. Instead, it defines
drivers that interact with underlying virtualization mechanisms that run
on your host operating system, and exposes functionality over a
web-based API.</p><div class="sect1 " id="id-1.4.7.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System architecture</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.5">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack Compute contains several main components.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The <a class="xref" href="bk02go01.html#term-cloud-controller" title="cloud controller">cloud controller</a> represents the global state and interacts with
the other components. The <code class="literal">API server</code> acts as the web services
front end for the cloud controller. The <code class="literal">compute controller</code>
provides compute server resources and usually also contains the
Compute service.</p></li><li class="listitem "><p>The <code class="literal">object store</code> is an optional component that provides storage
services; you can also use OpenStack Object Storage instead.</p></li><li class="listitem "><p>An <code class="literal">auth manager</code> provides authentication and authorization
services when used with the Compute system; you can also use
OpenStack Identity as a separate authentication service instead.</p></li><li class="listitem "><p>A <code class="literal">volume controller</code> provides fast and permanent block-level
storage for the compute servers.</p></li><li class="listitem "><p>The <code class="literal">network controller</code> provides virtual networks to enable
compute servers to interact with each other and with the public
network. You can also use OpenStack Networking instead.</p></li><li class="listitem "><p>The <code class="literal">scheduler</code> is used to select the most suitable compute
controller to host an instance.</p></li></ul></div><p>Compute uses a messaging-based, <code class="literal">shared nothing</code> architecture. All
major components exist on multiple servers, including the compute,
volume, and network controllers, and the Object Storage or Image service.
The state of the entire system is stored in a database. The cloud
controller communicates with the internal object store using HTTP, but
it communicates with the scheduler, network controller, and volume
controller using Advanced Message Queuing Protocol (AMQP). To avoid
blocking a component while waiting for a response, Compute uses
asynchronous calls, with a callback that is triggered when a response is
received.</p><div class="sect2 " id="id-1.4.7.5.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hypervisors</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.5.5">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Compute controls hypervisors through an API server. Selecting the best
hypervisor to use can be difficult, and you must take budget, resource
constraints, supported features, and required technical specifications
into account. However, the majority of OpenStack development is done on
systems using KVM and Xen-based hypervisors. For a detailed list of
features and support across different hypervisors, see the
<a class="link" href="http://docs.openstack.org/developer/nova/support-matrix.html" target="_blank">Feature Support Matrix</a>.</p><p>You can also orchestrate clouds using multiple hypervisors in different
availability zones. Compute supports the following hypervisors:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <a class="link" href="https://wiki.openstack.org/wiki/Ironic" target="_blank">Baremetal</a>
            </p></li><li class="listitem "><p>
              <a class="link" href="https://linuxcontainers.org/" target="_blank">Linux Containers (LXC)</a>
            </p></li><li class="listitem "><p>
              <a class="link" href="http://wiki.qemu.org/Manual" target="_blank">Quick Emulator (QEMU)</a>
            </p></li><li class="listitem "><p>
              <a class="link" href="http://user-mode-linux.sourceforge.net/" target="_blank">User Mode Linux (UML)</a>
            </p></li><li class="listitem "><p>
              <a class="link" href="http://www.vmware.com/products/vsphere-hypervisor/support.html" target="_blank">VMware
vSphere</a>
            </p></li><li class="listitem "><p>
              <a class="link" href="http://www.xen.org/support/documentation.html" target="_blank">Xen</a>
            </p></li></ul></div><p>For more information about hypervisors, see the
<a class="link" href="http://docs.openstack.org/newton/config-reference/compute/hypervisors.html" target="_blank">Hypervisors</a>
section in the OpenStack Configuration Reference.</p></div><div class="sect2 " id="id-1.4.7.5.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Projects, users, and roles</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.5.6">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The Compute system is designed to be used by different consumers in the
form of projects on a shared system, and role-based access assignments.
Roles control the actions that a user is allowed to perform.</p><p>Projects are isolated resource containers that form the principal
organizational structure within the Compute service. They consist of an
individual VLAN, and volumes, instances, images, keys, and users. A user
can specify the project by appending <code class="literal">project_id</code> to their access key.
If no project is specified in the API request, Compute attempts to use a
project with the same ID as the user.</p><p>For projects, you can use quota controls to limit the:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Number of volumes that can be launched.</p></li><li class="listitem "><p>Number of processor cores and the amount of RAM that can be
allocated.</p></li><li class="listitem "><p>Floating IP addresses assigned to any instance when it launches. This
allows instances to have the same publicly accessible IP addresses.</p></li><li class="listitem "><p>Fixed IP addresses assigned to the same instance when it launches.
This allows instances to have the same publicly or privately
accessible IP addresses.</p></li></ul></div><p>Roles control the actions a user is allowed to perform. By default, most
actions do not require a particular role, but you can configure them by
editing the <code class="literal">policy.json</code> file for user roles. For example, a rule can
be defined so that a user must have the <code class="literal">admin</code> role in order to be
able to allocate a public IP address.</p><p>A project limits users' access to particular images. Each user is
assigned a user name and password. Keypairs granting access to an
instance are enabled for each user, but quotas are set, so that each
project can control resource consumption across available hardware
resources.</p><div id="id-1.4.7.5.6.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Earlier versions of OpenStack used the term <code class="literal">tenant</code> instead of
<code class="literal">project</code>. Because of this legacy terminology, some command-line tools
use <code class="literal">--tenant_id</code> where you would normally expect to enter a
project ID.</p></div></div><div class="sect2 " id="id-1.4.7.5.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block storage</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.5.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack provides two classes of block storage: ephemeral storage
and persistent volume.</p><p>
          <span class="bold"><strong>Ephemeral storage</strong></span>
        </p><p>Ephemeral storage includes a root ephemeral volume and an additional
ephemeral volume.</p><p>The root disk is associated with an instance, and exists only for the
life of this very instance. Generally, it is used to store an
instance's root file system, persists across the guest operating system
reboots, and is removed on an instance deletion. The amount of the root
ephemeral volume is defined by the flavor of an instance.</p><p>In addition to the ephemeral root volume, all default types of flavors,
except <code class="literal">m1.tiny</code>, which is the smallest one, provide an additional
ephemeral block device sized between 20 and 160 GB (a configurable value
to suit an environment). It is represented as a raw block device with no
partition table or file system. A cloud-aware operating system can
discover, format, and mount such a storage device. OpenStack Compute
defines the default file system for different operating systems as Ext4
for Linux distributions, VFAT for non-Linux and non-Windows operating
systems, and NTFS for Windows. However, it is possible to specify any
other filesystem type by using <code class="literal">virt_mkfs</code> or
<code class="literal">default_ephemeral_format</code> configuration options.</p><div id="id-1.4.7.5.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>For example, the <code class="literal">cloud-init</code> package included into an Ubuntu's stock
cloud image, by default, formats this space as an Ext4 file system
and mounts it on <code class="literal">/mnt</code>. This is a cloud-init feature, and is not
an OpenStack mechanism. OpenStack only provisions the raw storage.</p></div><p>
          <span class="bold"><strong>Persistent volume</strong></span>
        </p><p>A persistent volume is represented by a persistent virtualized block
device independent of any particular instance, and provided by OpenStack
Block Storage.</p><p>Only a single configured instance can access a persistent volume.
Multiple instances cannot access a persistent volume. This type of
configuration requires a traditional network file system to allow
multiple instances accessing the persistent volume. It also requires a
traditional network file system like NFS, CIFS, or a cluster file system
such as GlusterFS. These systems can be built within an OpenStack
cluster, or provisioned outside of it, but OpenStack software does not
provide these features.</p><p>You can configure a persistent volume as bootable and use it to provide
a persistent virtual instance similar to the traditional non-cloud-based
virtualization system. It is still possible for the resulting instance
to keep ephemeral storage, depending on the flavor selected. In this
case, the root file system can be on the persistent volume, and its
state is maintained, even if the instance is shut down. For more
information about this type of configuration, see <a class="link" href="http://docs.openstack.org/newton/config-reference/block-storage/block-storage-overview.html" target="_blank">Introduction to the
Block Storage service</a>
in the OpenStack Configuration Reference.</p><div id="id-1.4.7.5.7.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>A persistent volume does not provide concurrent access from multiple
instances. That type of configuration requires a traditional network
file system like NFS, or CIFS, or a cluster file system such as
GlusterFS. These systems can be built within an OpenStack cluster,
or provisioned outside of it, but OpenStack software does not
provide these features.</p></div></div><div class="sect2 " id="id-1.4.7.5.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">EC2 compatibility API</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.5.8">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In addition to the native compute API, OpenStack provides an
EC2-compatible API. This API allows EC2 legacy workflows built for EC2
to work with OpenStack.</p><div id="id-1.4.7.5.8.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>Nova in tree EC2-compatible API is deprecated.
The <a class="link" href="http://git.openstack.org/cgit/openstack/ec2-api/" target="_blank">ec2-api project</a>
is working to implement the EC2 API.</p></div><p>You can use numerous third-party tools and language-specific SDKs to
interact with OpenStack clouds. You can use both native and
compatibility APIs. Some of the more popular third-party tools are:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.5.8.5.1"><span class="term ">Euca2ools</span></dt><dd><p>A popular open source command-line tool for interacting with the EC2
API. This is convenient for multi-cloud environments where EC2 is
the common API, or for transitioning from EC2-based clouds to
OpenStack. For more information, see the <a class="link" href="https://docs.eucalyptus.com/eucalyptus/" target="_blank">Eucalyptus
Documentation</a>.</p></dd><dt id="id-1.4.7.5.8.5.2"><span class="term ">Hybridfox</span></dt><dd><p>A Firefox browser add-on that provides a graphical interface to many
popular public and private cloud technologies, including OpenStack.
For more information, see the <a class="link" href="http://code.google.com/p/hybridfox/" target="_blank">hybridfox
site</a>.</p></dd><dt id="id-1.4.7.5.8.5.3"><span class="term ">boto</span></dt><dd><p>Python library for interacting with Amazon Web Services. You can use
this library to access OpenStack through the EC2 compatibility API.
For more     information, see the <a class="link" href="https://github.com/boto/boto" target="_blank">boto project page on
GitHub</a>.</p></dd><dt id="id-1.4.7.5.8.5.4"><span class="term ">fog</span></dt><dd><p>A Ruby cloud services library. It provides methods to interact
with a large number of cloud and virtualization platforms, including
OpenStack. For more information, see the <a class="link" href="https://rubygems.org/gems/fog" target="_blank">fog
site</a>.</p></dd><dt id="id-1.4.7.5.8.5.5"><span class="term ">php-opencloud</span></dt><dd><p>A PHP SDK designed to work with most OpenStack-based cloud
deployments, as well as Rackspace public cloud. For more
information, see the <a class="link" href="http://www.php-opencloud.com" target="_blank">php-opencloud
site</a>.</p></dd></dl></div></div><div class="sect2 " id="id-1.4.7.5.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Building blocks</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.5.9">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In OpenStack the base operating system is usually copied from an image
stored in the OpenStack Image service. This is the most common case and
results in an ephemeral instance that starts from a known template state
and loses all accumulated states on virtual machine deletion. It is also
possible to put an operating system on a persistent volume in the
OpenStack Block Storage volume system. This gives a more traditional
persistent system that accumulates states which are preserved on the
OpenStack Block Storage volume across the deletion and re-creation of
the virtual machine. To get a list of available images on your system,
run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image list
+--------------------------------------+-----------------------------+--------+
| ID                                   | Name                        | Status |
+--------------------------------------+-----------------------------+--------+
| aee1d242-730f-431f-88c1-87630c0f07ba | Ubuntu 14.04 cloudimg amd64 | active |
| 0b27baa1-0ca6-49a7-b3f4-48388e440245 | Ubuntu 14.10 cloudimg amd64 | active |
| df8d56fc-9cea-4dfd-a8d3-28764de3cb08 | jenkins                     | active |
+--------------------------------------+-----------------------------+--------+</pre></div><p>The displayed image attributes are:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.5.9.5.1"><span class="term ">
              <code class="literal">ID</code>
            </span></dt><dd><p>Automatically generated UUID of the image</p></dd><dt id="id-1.4.7.5.9.5.2"><span class="term ">
              <code class="literal">Name</code>
            </span></dt><dd><p>Free form, human-readable name for image</p></dd><dt id="id-1.4.7.5.9.5.3"><span class="term ">
              <code class="literal">Status</code>
            </span></dt><dd><p>The status of the image. Images marked <code class="literal">ACTIVE</code> are available for
use.</p></dd><dt id="id-1.4.7.5.9.5.4"><span class="term ">
              <code class="literal">Server</code>
            </span></dt><dd><p>For images that are created as snapshots of running instances, this
is the UUID of the instance the snapshot derives from. For uploaded
images, this field is blank.</p></dd></dl></div><p>Virtual hardware templates are called <code class="literal">flavors</code>. The default
installation provides five flavors. By default, these are configurable
by admin users, however that behavior can be changed by redefining the
access controls for <code class="literal">compute_extension:flavormanage</code> in
<code class="literal">/etc/nova/policy.json</code> on the <code class="literal">compute-api</code> server.</p><p>For a list of flavors that are available on your system:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor list
+-----+-----------+-------+------+-----------+-------+-----------+
| ID  | Name      |   RAM | Disk | Ephemeral | VCPUs | Is_Public |
+-----+-----------+-------+------+-----------+-------+-----------+
| 1   | m1.tiny   |   512 |    1 |         0 |     1 | True      |
| 2   | m1.small  |  2048 |   20 |         0 |     1 | True      |
| 3   | m1.medium |  4096 |   40 |         0 |     2 | True      |
| 4   | m1.large  |  8192 |   80 |         0 |     4 | True      |
| 5   | m1.xlarge | 16384 |  160 |         0 |     8 | True      |
+-----+-----------+-------+------+-----------+-------+-----------+</pre></div></div><div class="sect2 " id="id-1.4.7.5.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute service architecture</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.5.10">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>These basic categories describe the service architecture and information
about the cloud controller.</p><p>
          <span class="bold"><strong>API server</strong></span>
        </p><p>At the heart of the cloud framework is an API server, which makes
command and control of the hypervisor, storage, and networking
programmatically available to users.</p><p>The API endpoints are basic HTTP web services which handle
authentication, authorization, and basic command and control functions
using various API interfaces under the Amazon, Rackspace, and related
models. This enables API compatibility with multiple existing tool sets
created for interaction with offerings from other vendors. This broad
compatibility prevents vendor lock-in.</p><p>
          <span class="bold"><strong>Message queue</strong></span>
        </p><p>A messaging queue brokers the interaction between compute nodes
(processing), the networking controllers (software which controls
network infrastructure), API endpoints, the scheduler (determines which
physical hardware to allocate to a virtual resource), and similar
components. Communication to and from the cloud controller is handled by
HTTP requests through multiple API endpoints.</p><p>A typical message passing event begins with the API server receiving a
request from a user. The API server authenticates the user and ensures
that they are permitted to issue the subject command. The availability
of objects implicated in the request is evaluated and, if available, the
request is routed to the queuing engine for the relevant workers.
Workers continually listen to the queue based on their role, and
occasionally their type host name. When an applicable work request
arrives on the queue, the worker takes assignment of the task and begins
executing it. Upon completion, a response is dispatched to the queue
which is received by the API server and relayed to the originating user.
Database entries are queried, added, or removed as necessary during the
process.</p><p>
          <span class="bold"><strong>Compute worker</strong></span>
        </p><p>Compute workers manage computing instances on host machines. The API
dispatches commands to compute workers to complete these tasks:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Run instances</p></li><li class="listitem "><p>Delete instances (Terminate instances)</p></li><li class="listitem "><p>Reboot instances</p></li><li class="listitem "><p>Attach volumes</p></li><li class="listitem "><p>Detach volumes</p></li><li class="listitem "><p>Get console output</p></li></ul></div><p>
          <span class="bold"><strong>Network Controller</strong></span>
        </p><p>The Network Controller manages the networking resources on host
machines. The API server dispatches commands through the message queue,
which are subsequently processed by Network Controllers. Specific
operations include:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Allocating fixed IP addresses</p></li><li class="listitem "><p>Configuring VLANs for projects</p></li><li class="listitem "><p>Configuring networks for compute nodes</p></li></ul></div></div></div><div class="sect1 " id="id-1.4.7.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Images and instances</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Virtual machine images contain a virtual disk that holds a
bootable operating system on it. Disk images provide templates for
virtual machine file systems. The Image service controls image storage
and management.</p><p>Instances are the individual virtual machines that run on physical
compute nodes inside the cloud. Users can launch any number of instances
from the same image. Each launched instance runs from a copy of the
base image. Any changes made to the instance do not affect
the base image. Snapshots capture the state of an instances
running disk. Users can create a snapshot, and build a new image based
on these snapshots. The Compute service controls instance, image, and
snapshot storage and management.</p><p>When you launch an instance, you must choose a <code class="literal">flavor</code>, which
represents a set of virtual resources. Flavors define virtual
CPU number, RAM amount available, and ephemeral disks size. Users
must select from the set of available flavors
defined on their cloud. OpenStack provides a number of predefined
flavors that you can edit or add to.</p><div id="id-1.4.7.6.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>For more information about creating and troubleshooting images,
see the <a class="link" href="http://docs.openstack.org/image-guide/" target="_blank">OpenStack Virtual Machine Image
Guide</a>.</p></li><li class="listitem "><p>For more information about image configuration options, see the
<a class="link" href="http://docs.openstack.org/newton/config-reference/image.html" target="_blank">Image services</a>
section of the OpenStack Configuration Reference.</p></li><li class="listitem "><p>For more information about flavors, see <a class="xref" href="bk02ch05.html#compute-flavors" title="5.4.3. Flavors">Section 5.4.3, “Flavors”</a>.</p></li></ul></div></div><p>You can add and remove additional resources from running instances, such
as persistent volume storage, or public IP addresses. The example used
in this chapter is of a typical virtual system within an OpenStack
cloud. It uses the <code class="literal">cinder-volume</code> service, which provides persistent
block storage, instead of the ephemeral storage provided by the selected
instance flavor.</p><p>This diagram shows the system state prior to launching an instance. The
image store has a number of predefined images, supported by the Image
service. Inside the cloud, a compute node contains the
available vCPU, memory, and local disk resources. Additionally, the
<code class="literal">cinder-volume</code> service stores predefined volumes.</p><p>
        <span class="bold"><strong>The base image state with no running instances</strong></span>
      </p><div class="figure" id="id-1.4.7.6.9"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/instance-life-1.png" target="_blank"><img src="images/instance-life-1.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.1: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.9">#</a></h6></div></div><div class="sect2 " id="id-1.4.7.6.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Instance Launch</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.10">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To launch an instance, select an image, flavor, and any optional
attributes. The selected flavor provides a root volume, labeled <code class="literal">vda</code>
in this diagram, and additional ephemeral storage, labeled <code class="literal">vdb</code>. In
this example, the <code class="literal">cinder-volume</code> store is mapped to the third virtual
disk on this instance, <code class="literal">vdc</code>.</p><p>
          <span class="bold"><strong>Instance creation from an image</strong></span>
        </p><div class="figure" id="id-1.4.7.6.10.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/instance-life-2.png" target="_blank"><img src="images/instance-life-2.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.2: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.10.4">#</a></h6></div></div><p>The Image service copies the base image from the image store to the
local disk. The local disk is the first disk that the instance
accesses, which is the root volume labeled <code class="literal">vda</code>. Smaller
instances start faster. Less data needs to be copied across
the network.</p><p>The new empty ephemeral disk is also created, labeled <code class="literal">vdb</code>.
This disk is deleted when you delete the instance.</p><p>The compute node connects to the attached <code class="literal">cinder-volume</code> using iSCSI. The
<code class="literal">cinder-volume</code> is mapped to the third disk, labeled <code class="literal">vdc</code> in this
diagram. After the compute node provisions the vCPU and memory
resources, the instance boots up from root volume <code class="literal">vda</code>. The instance
runs and changes data on the disks (highlighted in red on the diagram).
If the volume store is located on a separate network, the
<code class="literal">my_block_storage_ip</code> option specified in the storage node
configuration file directs image traffic to the compute node.</p><div id="id-1.4.7.6.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Some details in this example scenario might be different in your
environment. For example, you might use a different type of back-end
storage, or different network protocols. One common variant is that
the ephemeral storage used for volumes <code class="literal">vda</code> and <code class="literal">vdb</code> could be
backed by network storage rather than a local disk.</p></div><p>When you delete an instance, the state is reclaimed with the exception
of the persistent volume. The ephemeral storage, whether encrypted or not,
is purged. Memory and vCPU resources are released. The image remains
unchanged throughout this process.</p><div class="figure" id="id-1.4.7.6.10.10"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/instance-life-3.png" target="_blank"><img src="images/instance-life-3.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.3: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.10.10">#</a></h6></div></div></div><div class="sect2 " id="id-1.4.7.6.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image properties and property protection</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.11">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>An image property is a key and value pair that the administrator
or the image owner attaches to an OpenStack Image service image, as
follows:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The administrator defines core properties, such as the image
name.</p></li><li class="listitem "><p>The administrator and the image owner can define additional
properties, such as licensing and billing information.</p></li></ul></div><p>The administrator can configure any property as protected, which
limits which policies or user roles can perform CRUD operations on that
property. Protected properties are generally additional properties to
which only administrators have access.</p><p>For unprotected image properties, the administrator can manage
core properties and the image owner can manage additional properties.</p><p>
          <span class="bold"><strong>To configure property protection</strong></span>
        </p><p>To configure property protection, edit the <code class="literal">policy.json</code> file. This file
can also be used to set policies for Image service actions.</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Define roles or policies in the <code class="literal">policy.json</code> file:</p><div class="verbatim-wrap highlight json"><pre class="screen">{
    "context_is_admin":  "role:admin",
    "default": "",

    "add_image": "",
    "delete_image": "",
    "get_image": "",
    "get_images": "",
    "modify_image": "",
    "publicize_image": "role:admin",
    "copy_from": "",

    "download_image": "",
    "upload_image": "",

    "delete_image_location": "",
    "get_image_location": "",
    "set_image_location": "",

    "add_member": "",
    "delete_member": "",
    "get_member": "",
    "get_members": "",
    "modify_member": "",

    "manage_image_cache": "role:admin",

    "get_task": "",
    "get_tasks": "",
    "add_task": "",
    "modify_task": "",

    "deactivate": "",
    "reactivate": "",

    "get_metadef_namespace": "",
    "get_metadef_namespaces":"",
    "modify_metadef_namespace":"",
    "add_metadef_namespace":"",
    "delete_metadef_namespace":"",

    "get_metadef_object":"",
    "get_metadef_objects":"",
    "modify_metadef_object":"",
    "add_metadef_object":"",

    "list_metadef_resource_types":"",
    "get_metadef_resource_type":"",
    "add_metadef_resource_type_association":"",

    "get_metadef_property":"",
    "get_metadef_properties":"",
    "modify_metadef_property":"",
    "add_metadef_property":"",

    "get_metadef_tag":"",
    "get_metadef_tags":"",
    "modify_metadef_tag":"",
    "add_metadef_tag":"",
    "add_metadef_tags":""
 }</pre></div><p>For each parameter, use <code class="literal">"rule:restricted"</code> to restrict access to all
users or <code class="literal">"role:admin"</code> to limit access to administrator roles.
For example:</p><div class="verbatim-wrap highlight json"><pre class="screen">"download_image":
"upload_image":</pre></div></li><li class="step "><p>Define which roles or policies can manage which properties in a property
protections configuration file. For example:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[x_none_read]
create = context_is_admin
read = !
update = !
delete = !

[x_none_update]
create = context_is_admin
read = context_is_admin
update = !
delete = context_is_admin

[x_none_delete]
create = context_is_admin
read = context_is_admin
update = context_is_admin
delete = !</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>A value of <code class="literal">@</code> allows the corresponding operation for a property.</p></li><li class="listitem "><p>A value of <code class="literal">!</code> disallows the corresponding operation for a
property.</p></li></ul></div></li><li class="step "><p>In the <code class="literal">glance-api.conf</code> file, define the location of a property
protections configuration file.</p><div class="verbatim-wrap highlight ini"><pre class="screen">property_protection_file = {file_name}</pre></div><p>This file contains the rules for property protections and the roles and
policies associated with it.</p><p>By default, property protections are not enforced.</p><p>If you specify a file name value and the file is not found, the
<code class="literal">glance-api</code> service does not start.</p><p>To view a sample configuration file, see
<a class="link" href="http://docs.openstack.org/newton/config-reference/image/glance-api.conf.html" target="_blank">glance-api.conf</a>.</p></li><li class="step "><p>Optionally, in the <code class="literal">glance-api.conf</code> file, specify whether roles or
policies are used in the property protections configuration file</p><div class="verbatim-wrap highlight ini"><pre class="screen">property_protection_rule_format = roles</pre></div><p>The default is <code class="literal">roles</code>.</p><p>To view a sample configuration file, see
<a class="link" href="http://docs.openstack.org/newton/config-reference/image/glance-api.conf.html" target="_blank">glance-api.conf</a>.</p></li></ol></div></div></div><div class="sect2 " id="id-1.4.7.6.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image download: how it works</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.12">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Prior to starting a virtual machine, transfer the virtual machine image
to the compute node from the Image service. How this
works can change depending on the settings chosen for the compute node
and the Image service.</p><p>Typically, the Compute service will use the image identifier passed to
it by the scheduler service and request the image from the Image API.
Though images are not stored in glance—rather in a back end, which could
be Object Storage, a filesystem or any other supported method—the
connection is made from the compute node to the Image service and the
image is transferred over this connection. The Image service streams the
image from the back end to the compute node.</p><p>It is possible to set up the Object Storage node on a separate network,
and still allow image traffic to flow between the compute and object
storage nodes. Configure the <code class="literal">my_block_storage_ip</code> option in the
storage node configuration file to allow block storage traffic to reach
the compute node.</p><p>Certain back ends support a more direct method, where on request the
Image service will return a URL that links directly to the back-end store.
You can download the image using this approach. Currently, the only store
to support the direct download approach is the filesystem store.
Configured the approach using the <code class="literal">filesystems</code> option in
the <code class="literal">image_file_url</code> section of the <code class="literal">nova.conf</code> file on
compute nodes.</p><p>Compute nodes also implement caching of images, meaning that if an image
has been used before it won't necessarily be downloaded every time.
Information on the configuration options for caching on compute nodes
can be found in the <a class="link" href="http://docs.openstack.org/newton/config-reference/" target="_blank">Configuration
Reference</a>.</p></div><div class="sect2 " id="id-1.4.7.6.13"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Instance building blocks</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.13">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In OpenStack, the base operating system is usually copied from an image
stored in the OpenStack Image service. This results in an ephemeral
instance that starts from a known template state and loses all
accumulated states on shutdown.</p><p>You can also put an operating system on a persistent volume in Compute
or the Block Storage volume system. This gives a more traditional,
persistent system that accumulates states that are preserved across
restarts. To get a list of available images on your system, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image list
+--------------------------------------+-----------------------------+--------+
| ID                                   | Name                        | Status |
+--------------------------------------+-----------------------------+--------+
| aee1d242-730f-431f-88c1-87630c0f07ba | Ubuntu 14.04 cloudimg amd64 | active |
+--------------------------------------+-----------------------------+--------+
| 0b27baa1-0ca6-49a7-b3f4-48388e440245 | Ubuntu 14.10 cloudimg amd64 | active |
+--------------------------------------+-----------------------------+--------+
| df8d56fc-9cea-4dfd-a8d3-28764de3cb08 | jenkins                     | active |
+--------------------------------------+-----------------------------+--------+</pre></div><p>The displayed image attributes are:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.6.13.6.1"><span class="term ">
              <code class="literal">ID</code>
            </span></dt><dd><p>Automatically generated UUID of the image.</p></dd><dt id="id-1.4.7.6.13.6.2"><span class="term ">
              <code class="literal">Name</code>
            </span></dt><dd><p>Free form, human-readable name for the image.</p></dd><dt id="id-1.4.7.6.13.6.3"><span class="term ">
              <code class="literal">Status</code>
            </span></dt><dd><p>The status of the image. Images marked <code class="literal">ACTIVE</code> are available for
use.</p></dd><dt id="id-1.4.7.6.13.6.4"><span class="term ">
              <code class="literal">Server</code>
            </span></dt><dd><p>For images that are created as snapshots of running instances, this
is the UUID of the instance the snapshot derives from. For uploaded
images, this field is blank.</p></dd></dl></div><p>Virtual hardware templates are called <code class="literal">flavors</code>. The default
installation provides five predefined flavors.</p><p>For a list of flavors that are available on your system, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor list
+-----+-----------+-------+------+-----------+-------+-----------+
| ID  | Name      |   RAM | Disk | Ephemeral | VCPUs | Is_Public |
+-----+-----------+-------+------+-----------+-------+-----------+
| 1   | m1.tiny   |   512 |    1 |         0 |     1 | True      |
| 2   | m1.small  |  2048 |   20 |         0 |     1 | True      |
| 3   | m1.medium |  4096 |   40 |         0 |     2 | True      |
| 4   | m1.large  |  8192 |   80 |         0 |     4 | True      |
| 5   | m1.xlarge | 16384 |  160 |         0 |     8 | True      |
+-----+-----------+-------+------+-----------+-------+-----------+</pre></div><p>By default, administrative users can configure the flavors. You can
change this behavior by redefining the access controls for
<code class="literal">compute_extension:flavormanage</code> in <code class="literal">/etc/nova/policy.json</code> on the
<code class="literal">compute-api</code> server.</p></div><div class="sect2 " id="id-1.4.7.6.14"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Instance management tools</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.14">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack provides command-line, web interface, and API-based instance
management tools. Third-party management tools are also available, using
either the native API or the provided EC2-compatible API.</p><p>The OpenStack python-novaclient package provides a basic command-line
utility, which uses the <code class="command">nova</code> command. This is available as a native
package for most Linux distributions, or you can install the latest
version using the pip python package installer:</p><div class="verbatim-wrap"><pre class="screen"># pip install python-novaclient</pre></div><p>For more information about python-novaclient and other command-line
tools, see the <a class="link" href="http://docs.openstack.org/user-guide/cli.html" target="_blank">OpenStack End User
Guide</a>.</p></div><div class="sect2 " id="id-1.4.7.6.15"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control where instances run</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.15">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The <a class="link" href="http://docs.openstack.org/newton/config-reference/compute/scheduler.html" target="_blank">Scheduling section</a>
of OpenStack Configuration Reference
provides detailed information on controlling where your instances run,
including ensuring a set of instances run on different compute nodes for
service resiliency or on the same node for high performance
inter-instance communications.</p><p>Administrative users can specify which compute node their instances
run on. To do this, specify the <code class="literal">--availability-zone
AVAILABILITY_ZONE:COMPUTE_HOST</code> parameter.</p></div><div class="sect2 " id="id-1.4.7.6.16"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Launch instances with UEFI</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.6.16">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Unified Extensible Firmware Interface (UEFI) is a standard firmware
designed to replace legacy BIOS. There is a slow but steady trend
for operating systems to move to the UEFI format and, in some cases,
make it their only format.</p><p>
          <span class="bold"><strong>To configure UEFI environment</strong></span>
        </p><p>To successfully launch an instance from an UEFI image in QEMU/KVM
environment, the administrator has to install the following
packages on compute node:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>OVMF, a port of Intel's tianocore firmware to QEMU virtual machine.</p></li><li class="listitem "><p>libvirt, which has been supporting UEFI boot since version 1.2.9.</p></li></ul></div><p>Because default UEFI loader path is <code class="literal">/usr/share/OVMF/OVMF_CODE.fd</code>, the
administrator must create one link to this location after UEFI package
is installed.</p><p>
          <span class="bold"><strong>To upload UEFI images</strong></span>
        </p><p>To launch instances from a UEFI image, the administrator first has to
upload one UEFI image. To do so, <code class="literal">hw_firmware_type</code> property must
be set to <code class="literal">uefi</code> when the image is created. For example:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image create --container-format bare --disk-format qcow2 \
  --property hw_firmware_type=uefi --file /tmp/cloud-uefi.qcow --name uefi</pre></div><p>After that, you can launch instances from this UEFI image.</p></div></div><div class="sect1 " id="id-1.4.7.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking with nova-network</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Understanding the networking configuration options helps you design the
best configuration for your Compute instances.</p><p>You can choose to either install and configure <code class="literal">nova-network</code> or use the
OpenStack Networking service (neutron). This section contains a brief
overview of <code class="literal">nova-network</code>. For more information about OpenStack
Networking, see <a class="xref" href="networking.html" title="Chapter 9. Networking">Chapter 9, <em>Networking</em></a>.</p><div class="sect2 " id="id-1.4.7.7.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking concepts</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.4">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Compute assigns a private IP address to each VM instance. Compute makes
a distinction between fixed IPs and floating IP. Fixed IPs are IP
addresses that are assigned to an instance on creation and stay the same
until the instance is explicitly terminated. Floating IPs are addresses
that can be dynamically associated with an instance. A floating IP
address can be disassociated and associated with another instance at any
time. A user can reserve a floating IP for their project.</p><div id="id-1.4.7.7.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Currently, Compute with <code class="literal">nova-network</code> only supports Linux bridge
networking that allows virtual interfaces to connect to the outside
network through the physical interface.</p></div><p>The network controller with <code class="literal">nova-network</code> provides virtual networks to
enable compute servers to interact with each other and with the public
network. Compute with <code class="literal">nova-network</code> supports the following network modes,
which are implemented as Network Manager types:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.7.4.5.1"><span class="term ">Flat Network Manager</span></dt><dd><p>In this mode, a network administrator specifies a subnet. IP
addresses for VM instances are assigned from the subnet, and then
injected into the image on launch. Each instance receives a fixed IP
address from the pool of available addresses. A system administrator
must create the Linux networking bridge (typically named <code class="literal">br100</code>,
although this is configurable) on the systems running the
<code class="literal">nova-network</code> service. All instances of the system are attached to
the same bridge, which is configured manually by the network
administrator.</p></dd></dl></div><div id="id-1.4.7.7.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Configuration injection currently only works on Linux-style
systems that keep networking configuration in
<code class="literal">/etc/network/interfaces</code>.</p></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.7.4.7.1"><span class="term ">Flat DHCP Network Manager</span></dt><dd><p>In this mode, OpenStack starts a DHCP server (dnsmasq) to allocate
IP addresses to VM instances from the specified subnet, in addition
to manually configuring the networking bridge. IP addresses for VM
instances are assigned from a subnet specified by the network
administrator.</p><p>Like flat mode, all instances are attached to a single bridge on the
compute node. Additionally, a DHCP server configures instances
depending on single-/multi-host mode, alongside each <code class="literal">nova-network</code>.
In this mode, Compute does a bit more configuration. It attempts to
bridge into an Ethernet device (<code class="literal">flat_interface</code>, eth0 by
default). For every instance, Compute allocates a fixed IP address
and configures dnsmasq with the MAC ID and IP address for the VM.
Dnsmasq does not take part in the IP address allocation process, it
only hands out IPs according to the mapping done by Compute.
Instances receive their fixed IPs with the <code class="command">dhcpdiscover</code> command.
These IPs are not assigned to any of the host's network interfaces,
only to the guest-side interface for the VM.</p><p>In any setup with flat networking, the hosts providing the
<code class="literal">nova-network</code> service are responsible for forwarding traffic from the
private network. They also run and configure dnsmasq as a DHCP
server listening on this bridge, usually on IP address 10.0.0.1 (see
<a class="xref" href="bk02ch05.html#compute-dnsmasq" title="5.3.2. DHCP server: dnsmasq">Section 5.3.2, “DHCP server: dnsmasq”</a>). Compute can determine
the NAT entries for each network, although sometimes NAT is not
used, such as when the network has been configured with all public
IPs, or if a hardware router is used (which is a high availability
option). In this case, hosts need to have <code class="literal">br100</code> configured and
physically connected to any other nodes that are hosting VMs. You
must set the <code class="literal">flat_network_bridge</code> option or create networks with
the bridge parameter in order to avoid raising an error. Compute
nodes have iptables or ebtables entries created for each project and
instance to protect against MAC ID or IP address spoofing and ARP
poisoning.</p></dd></dl></div><div id="id-1.4.7.7.4.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>In single-host Flat DHCP mode you will be able to ping VMs
through their fixed IP from the <code class="literal">nova-network</code> node, but you
cannot ping them from the compute nodes. This is expected
behavior.</p></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.7.4.9.1"><span class="term ">VLAN Network Manager</span></dt><dd><p>This is the default mode for OpenStack Compute. In this mode,
Compute creates a VLAN and bridge for each project. For
multiple-machine installations, the VLAN Network Mode requires a
switch that supports VLAN tagging (IEEE 802.1Q). The project gets a
range of private IPs that are only accessible from inside the VLAN.
In order for a user to access the instances in their project, a
special VPN instance (code named <code class="literal">cloudpipe</code>) needs to be created.
Compute generates a certificate and key for the user to access the
VPN and starts the VPN automatically. It provides a private network
segment for each project's instances that can be accessed through a
dedicated VPN connection from the internet. In this mode, each
project gets its own VLAN, Linux networking bridge, and subnet.</p><p>The subnets are specified by the network administrator, and are
assigned dynamically to a project when required. A DHCP server is
started for each VLAN to pass out IP addresses to VM instances from
the subnet assigned to the project. All instances belonging to one
project are bridged into the same VLAN for that project. OpenStack
Compute creates the Linux networking bridges and VLANs when
required.</p></dd></dl></div><p>These network managers can co-exist in a cloud system. However, because
you cannot select the type of network for a given project, you cannot
configure multiple network types in a single Compute installation.</p><p>All network managers configure the network using network drivers. For
example, the Linux L3 driver (<code class="literal">l3.py</code> and <code class="literal">linux_net.py</code>), which
makes use of <code class="literal">iptables</code>, <code class="literal">route</code> and other network management
facilities, and the libvirt <a class="link" href="http://libvirt.org/formatnwfilter.html" target="_blank">network filtering
facilities</a>. The driver is
not tied to any particular network manager; all network managers use the
same driver. The driver usually initializes only when the first VM lands
on this host node.</p><p>All network managers operate in either single-host or multi-host mode.
This choice greatly influences the network configuration. In single-host
mode, a single <code class="literal">nova-network</code> service provides a default gateway for VMs
and hosts a single DHCP server (dnsmasq). In multi-host mode, each
compute node runs its own <code class="literal">nova-network</code> service. In both cases, all
traffic between VMs and the internet flows through <code class="literal">nova-network</code>. Each
mode has benefits and drawbacks. For more on this, see the Network
Topology section in the <a class="link" href="http://docs.openstack.org/ops-guide/arch-network-design.html#network-topology" target="_blank">OpenStack Operations Guide</a>.</p><p>All networking options require network connectivity to be already set up
between OpenStack physical nodes. OpenStack does not configure any
physical network interfaces. All network managers automatically create
VM virtual interfaces. Some network managers can also create network
bridges such as <code class="literal">br100</code>.</p><p>The internal network interface is used for communication with VMs. The
interface should not have an IP address attached to it before OpenStack
installation, it serves only as a fabric where the actual endpoints are
VMs and dnsmasq. Additionally, the internal network interface must be in
<code class="literal">promiscuous</code> mode, so that it can receive packets whose target MAC
address is the guest VM, not the host.</p><p>All machines must have a public and internal network interface
(controlled by these options: <code class="literal">public_interface</code> for the public
interface, and <code class="literal">flat_interface</code> and <code class="literal">vlan_interface</code> for the
internal interface with flat or VLAN managers). This guide refers to the
public network as the external network and the private network as the
internal or project network.</p><p>For flat and flat DHCP modes, use the <code class="command">nova network-create</code> command
to create a network:</p><div class="verbatim-wrap"><pre class="screen">$ nova network-create vmnet \
  --fixed-range-v4 10.0.0.0/16 --fixed-cidr 10.0.20.0/24 --bridge br100</pre></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.7.4.18.1"><span class="term ">This example uses the following parameters:</span></dt><dd><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.7.4.18.1.2.1.1"><span class="term ">
                    <code class="option">--fixed-range-v4</code>
                  </span></dt><dd><p>specifies the network subnet.</p></dd><dt id="id-1.4.7.7.4.18.1.2.1.2"><span class="term ">
                    <code class="option">--fixed-cidr</code>
                  </span></dt><dd><p>specifies a range of fixed IP addresses to allocate,
and can be a subset of the <code class="literal">--fixed-range-v4</code>
argument.</p></dd><dt id="id-1.4.7.7.4.18.1.2.1.3"><span class="term ">
                    <code class="option">--bridge</code>
                  </span></dt><dd><p>specifies the bridge device to which this network is
connected on every compute node.</p></dd></dl></div></dd></dl></div></div><div class="sect2 " id="compute-dnsmasq"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DHCP server: dnsmasq</span> <a title="Permalink" class="permalink" href="bk02ch05.html#compute-dnsmasq">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span>compute-dnsmasq</li></ul></div></div></div></div><p>The Compute service uses
<a class="link" href="http://www.thekelleys.org.uk/dnsmasq/doc.html" target="_blank">dnsmasq</a> as the DHCP
server when using either Flat DHCP Network Manager or VLAN Network
Manager. For Compute to operate in IPv4/IPv6 dual-stack mode, use at
least dnsmasq v2.63. The <code class="literal">nova-network</code> service is responsible for
starting dnsmasq processes.</p><p>The behavior of dnsmasq can be customized by creating a dnsmasq
configuration file. Specify the configuration file using the
<code class="literal">dnsmasq_config_file</code> configuration option:</p><div class="verbatim-wrap highlight ini"><pre class="screen">dnsmasq_config_file=/etc/dnsmasq-nova.conf</pre></div><p>For more information about creating a dnsmasq configuration file, see
the <a class="link" href="http://docs.openstack.org/newton/config-reference/" target="_blank">OpenStack Configuration
Reference</a>,
and <a class="link" href="http://www.thekelleys.org.uk/dnsmasq/docs/dnsmasq.conf.example" target="_blank">the dnsmasq
documentation</a>.</p><p>Dnsmasq also acts as a caching DNS server for instances. You can specify
the DNS server that dnsmasq uses by setting the <code class="literal">dns_server</code>
configuration option in <code class="literal">/etc/nova/nova.conf</code>. This example configures
dnsmasq to use Google's public DNS server:</p><div class="verbatim-wrap highlight ini"><pre class="screen">dns_server=8.8.8.8</pre></div><p>Dnsmasq logs to syslog (typically <code class="literal">/var/log/syslog</code> or
<code class="literal">/var/log/messages</code>, depending on Linux distribution). Logs can be
useful for troubleshooting, especially in a situation where VM instances
boot successfully but are not reachable over the network.</p><p>Administrators can specify the starting point IP address to reserve with
the DHCP server (in the format n.n.n.n) with this command:</p><div class="verbatim-wrap"><pre class="screen">$ nova-manage fixed reserve --address IP_ADDRESS</pre></div><p>This reservation only affects which IP address the VMs start at, not the
fixed IP addresses that <code class="literal">nova-network</code> places on the bridges.</p></div><div class="sect2 " id="id-1.4.7.7.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Compute to use IPv6 addresses</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.6">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>If you are using OpenStack Compute with <code class="literal">nova-network</code>, you can put
Compute into dual-stack mode, so that it uses both IPv4 and IPv6
addresses for communication. In dual-stack mode, instances can acquire
their IPv6 global unicast addresses by using a stateless address
auto-configuration mechanism [RFC 4862/2462]. IPv4/IPv6 dual-stack mode
works with both <code class="literal">VlanManager</code> and <code class="literal">FlatDHCPManager</code> networking
modes.</p><p>In <code class="literal">VlanManager</code> networking mode, each project uses a different 64-bit
global routing prefix. In <code class="literal">FlatDHCPManager</code> mode, all instances use
one 64-bit global routing prefix.</p><p>This configuration was tested with virtual machine images that have an
IPv6 stateless address auto-configuration capability. This capability is
required for any VM to run with an IPv6 address. You must use an EUI-64
address for stateless address auto-configuration. Each node that
executes a <code class="literal">nova-*</code> service must have <code class="literal">python-netaddr</code> and <code class="literal">radvd</code>
installed.</p><p>
          <span class="bold"><strong>Switch into IPv4/IPv6 dual-stack mode</strong></span>
        </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>For every node running a <code class="literal">nova-*</code> service, install python-netaddr:</p><div class="verbatim-wrap"><pre class="screen"># apt-get install python-netaddr</pre></div></li><li class="step "><p>For every node running <code class="literal">nova-network</code>, install <code class="literal">radvd</code> and configure
IPv6 networking:</p><div class="verbatim-wrap"><pre class="screen"># apt-get install radvd
# echo 1 &gt; /proc/sys/net/ipv6/conf/all/forwarding
# echo 0 &gt; /proc/sys/net/ipv6/conf/all/accept_ra</pre></div></li><li class="step "><p>On all nodes, edit the <code class="literal">nova.conf</code> file and specify
<code class="literal">use_ipv6 = True</code>.</p></li><li class="step "><p>Restart all <code class="literal">nova-*</code> services.</p></li></ol></div></div><p>
          <span class="bold"><strong>IPv6 configuration options</strong></span>
        </p><p>You can use the following options with the <code class="command">nova network-create</code>
command:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Add a fixed range for IPv6 addresses to the <code class="command">nova network-create</code>
command. Specify <code class="literal">public</code> or <code class="literal">private</code> after the <code class="literal">network-create</code>
parameter.</p><div class="verbatim-wrap"><pre class="screen">$ nova network-create public --fixed-range-v4 FIXED_RANGE_V4 \
  --vlan VLAN_ID --vpn VPN_START --fixed-range-v6 FIXED_RANGE_V6</pre></div></li><li class="listitem "><p>Set the IPv6 global routing prefix by using the
<code class="literal">--fixed_range_v6</code> parameter. The default value for the parameter
is <code class="literal">fd00::/48</code>.</p><p>When you use <code class="literal">FlatDHCPManager</code>, the command uses the original
<code class="literal">--fixed_range_v6</code> value. For example:</p><div class="verbatim-wrap"><pre class="screen">$ nova network-create public  --fixed-range-v4 10.0.2.0/24 \
  --fixed-range-v6 fd00:1::/48</pre></div></li><li class="listitem "><p>When you use <code class="literal">VlanManager</code>, the command increments the subnet ID
to create subnet prefixes. Guest VMs use this prefix to generate
their IPv6 global unicast addresses. For example:</p><div class="verbatim-wrap"><pre class="screen">$ nova network-create public --fixed-range-v4 10.0.1.0/24 --vlan 100 \
  --vpn 1000 --fixed-range-v6 fd00:1::/48</pre></div></li></ul></div><div class="table" id="id-1.4.7.7.6.10"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.1: </span><span class="name">Description of IPv6 configuration options </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.6.10">#</a></h6></div><div class="table-contents"><table class="table" summary="Description of IPv6 configuration options" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                  <p>Configuration option = Default value</p>
                </th><th>
                  <p>Description</p>
                </th></tr><tr><th>
                  <p>[DEFAULT]</p>
                </th><th> </th></tr></thead><tbody><tr><td>
                  <p>fixed_range_v6 = fd00::/48</p>
                </td><td>
                  <p>(StrOpt) Fixed IPv6 address block</p>
                </td></tr><tr><td>
                  <p>gateway_v6 = None</p>
                </td><td>
                  <p>(StrOpt) Default IPv6 gateway</p>
                </td></tr><tr><td>
                  <p>ipv6_backend = rfc2462</p>
                </td><td>
                  <p>(StrOpt) Backend to use for IPv6 generation</p>
                </td></tr><tr><td>
                  <p>use_ipv6 = False</p>
                </td><td>
                  <p>(BoolOpt) Use IPv6</p>
                </td></tr></tbody></table></div></div></div><div class="sect2 " id="id-1.4.7.7.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metadata service</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Compute uses a metadata service for virtual machine instances to
retrieve instance-specific data. Instances access the metadata service
at <code class="literal">http://169.254.169.254</code>. The metadata service supports two sets of
APIs: an OpenStack metadata API and an EC2-compatible API. Both APIs are
versioned by date.</p><p>To retrieve a list of supported versions for the OpenStack metadata API,
make a GET request to <code class="literal">http://169.254.169.254/openstack</code>:</p><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/openstack
2012-08-10
2013-04-04
2013-10-17
latest</pre></div><p>To list supported versions for the EC2-compatible metadata API, make a
GET request to <code class="literal">http://169.254.169.254</code>:</p><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254
1.0
2007-01-19
2007-03-01
2007-08-29
2007-10-10
2007-12-15
2008-02-01
2008-09-01
2009-04-04
latest</pre></div><p>If you write a consumer for one of these APIs, always attempt to access
the most recent API version supported by your consumer first, then fall
back to an earlier version if the most recent one is not available.</p><p>Metadata from the OpenStack API is distributed in JSON format. To
retrieve the metadata, make a GET request to
<code class="literal">http://169.254.169.254/openstack/2012-08-10/meta_data.json</code>:</p><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/openstack/2012-08-10/meta_data.json</pre></div><div class="verbatim-wrap highlight json"><pre class="screen">{
   "uuid": "d8e02d56-2648-49a3-bf97-6be8f1204f38",
   "availability_zone": "nova",
   "hostname": "test.novalocal",
   "launch_index": 0,
   "meta": {
      "priority": "low",
      "role": "webserver"
   },
   "project_id": "f7ac731cc11f40efbc03a9f9e1d1d21f",
   "public_keys": {
       "mykey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKV\
                 VRNCRX6BlnNbI+USLGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTH\
                 bsiyPCIDOKyeHba4MUJq8Oh5b2i71/3BISpyxTBH/uZDHdslW2a+SrPDCe\
                 uMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated by Nova\n"
   },
   "name": "test"
}</pre></div><p>Instances also retrieve user data (passed as the <code class="literal">user_data</code> parameter
in the API call or by the <code class="literal">--user_data</code> flag in the
<code class="command">openstack server create</code> command) through the metadata service, by making a
GET request to <code class="literal">http://169.254.169.254/openstack/2012-08-10/user_data</code>:</p><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/openstack/2012-08-10/user_data
#!/bin/bash
echo 'Extra user data here'</pre></div><p>The metadata service has an API that is compatible with version
2009-04-04 of the <a class="link" href="http://docs.amazonwebservices.com/AWSEC2/2009-04-04/UserGuide/AESDG-chapter-instancedata.html" target="_blank">Amazon EC2 metadata
service</a>.
This means that virtual machine images designed for EC2 will work
properly with OpenStack.</p><p>The EC2 API exposes a separate URL for each metadata element. Retrieve a
listing of these elements by making a GET query to
<code class="literal">http://169.254.169.254/2009-04-04/meta-data/</code>:</p><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/2009-04-04/meta-data/
ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
hostname
instance-action
instance-id
instance-type
kernel-id
local-hostname
local-ipv4
placement/
public-hostname
public-ipv4
public-keys/
ramdisk-id
reservation-id
security-groups</pre></div><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/2009-04-04/meta-data/block-device-mapping/
ami</pre></div><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/2009-04-04/meta-data/placement/
availability-zone</pre></div><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/2009-04-04/meta-data/public-keys/
0=mykey</pre></div><p>Instances can retrieve the public SSH key (identified by keypair name
when a user requests a new instance) by making a GET request to
<code class="literal">http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key</code>:</p><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKVVRNCRX6BlnNbI+US\
LGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTHbsiyPCIDOKyeHba4MUJq8Oh5b2i71/3B\
ISpyxTBH/uZDHdslW2a+SrPDCeuMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated\
by Nova</pre></div><p>Instances can retrieve user data by making a GET request to
<code class="literal">http://169.254.169.254/2009-04-04/user-data</code>:</p><div class="verbatim-wrap"><pre class="screen">$ curl http://169.254.169.254/2009-04-04/user-data
#!/bin/bash
echo 'Extra user data here'</pre></div><p>The metadata service is implemented by either the <code class="literal">nova-api</code> service or
the <code class="literal">nova-api-metadata</code> service. Note that the <code class="literal">nova-api-metadata</code> service
is generally only used when running in multi-host mode, as it retrieves
instance-specific metadata. If you are running the <code class="literal">nova-api</code> service, you
must have <code class="literal">metadata</code> as one of the elements listed in the
<code class="literal">enabled_apis</code> configuration option in <code class="literal">/etc/nova/nova.conf</code>. The
default <code class="literal">enabled_apis</code> configuration setting includes the metadata
service, so you do not need to modify it.</p><p>Hosts access the service at <code class="literal">169.254.169.254:80</code>, and this is
translated to <code class="literal">metadata_host:metadata_port</code> by an iptables rule
established by the <code class="literal">nova-network</code> service. In multi-host mode, you can set
<code class="literal">metadata_host</code> to <code class="literal">127.0.0.1</code>.</p><p>For instances to reach the metadata service, the <code class="literal">nova-network</code> service
must configure iptables to NAT port <code class="literal">80</code> of the <code class="literal">169.254.169.254</code>
address to the IP address specified in <code class="literal">metadata_host</code> (this defaults
to <code class="literal">$my_ip</code>, which is the IP address of the <code class="literal">nova-network</code> service) and
port specified in <code class="literal">metadata_port</code> (which defaults to <code class="literal">8775</code>) in
<code class="literal">/etc/nova/nova.conf</code>.</p><div id="id-1.4.7.7.7.26" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The <code class="literal">metadata_host</code> configuration option must be an IP address,
not a host name.</p></div><p>The default Compute service settings assume that <code class="literal">nova-network</code> and
<code class="literal">nova-api</code> are running on the same host. If this is not the case, in the
<code class="literal">/etc/nova/nova.conf</code> file on the host running <code class="literal">nova-network</code>, set the
<code class="literal">metadata_host</code> configuration option to the IP address of the host
where <code class="literal">nova-api</code> is running.</p><div class="table" id="id-1.4.7.7.7.28"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.2: </span><span class="name">Description of metadata configuration options </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.7.28">#</a></h6></div><div class="table-contents"><table class="table" summary="Description of metadata configuration options" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                  <p>Configuration option = Default value</p>
                </th><th>
                  <p>Description</p>
                </th></tr><tr><th>
                  <p>[DEFAULT]</p>
                </th><th> </th></tr></thead><tbody><tr><td>
                  <p>metadata_cache_expiration = 15</p>
                </td><td>
                  <p>(IntOpt) Time in seconds to cache metadata; 0 to disable metadata
caching entirely (not recommended). Increasing this should improve
response times of the metadata API when under heavy load. Higher values
may increase memory usage and result in longer times for host metadata
changes to take effect.</p>
                </td></tr><tr><td>
                  <p>metadata_host = $my_ip</p>
                </td><td>
                  <p>(StrOpt) The IP address for the metadata API server</p>
                </td></tr><tr><td>
                  <p>metadata_listen = 0.0.0.0</p>
                </td><td>
                  <p>(StrOpt) The IP address on which the metadata API will listen.</p>
                </td></tr><tr><td>
                  <p>metadata_listen_port = 8775</p>
                </td><td>
                  <p>(IntOpt) The port on which the metadata API will listen.</p>
                </td></tr><tr><td>
                  <p>metadata_manager = nova.api.manager.MetadataManager</p>
                </td><td>
                  <p>(StrOpt) OpenStack metadata service manager</p>
                </td></tr><tr><td>
                  <p>metadata_port = 8775</p>
                </td><td>
                  <p>(IntOpt) The port for the metadata API port</p>
                </td></tr><tr><td>
                  <p>metadata_workers = None</p>
                </td><td>
                  <p>(IntOpt) Number of workers for metadata service. The default will be the number of CPUs available.</p>
                </td></tr><tr><td>
                  <p>vendordata_driver = nova.api.metadata.vendordata_json.JsonFileVendorData</p>
                </td><td>
                  <p>(StrOpt) Driver to use for vendor data</p>
                </td></tr><tr><td>
                  <p>vendordata_jsonfile_path = None</p>
                </td><td>
                  <p>(StrOpt) File to load JSON formatted vendor data from</p>
                </td></tr></tbody></table></div></div></div><div class="sect2 " id="id-1.4.7.7.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable ping and SSH on VMs</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.8">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You need to enable <code class="literal">ping</code> and <code class="literal">ssh</code> on your VMs for network access.
This can be done with either the <code class="command">nova</code> or <code class="command">euca2ools</code>
commands.</p><div id="id-1.4.7.7.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Run these commands as root only if the credentials used to interact
with <code class="literal">nova-api</code> are in <code class="literal">/root/.bashrc</code>. If the EC2 credentials in
the <code class="literal">.bashrc</code> file are for an unprivileged user, you must run
these commands as that user instead.</p></div><p>Enable ping and SSH with <code class="command">openstack security group rule create</code>
commands:</p><div class="verbatim-wrap"><pre class="screen">$ openstack security group rule create default --protocol icmp --dst-port -1:-1 --remote-ip 0.0.0.0/0
$ openstack security group rule create default --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0</pre></div><p>Enable ping and SSH with <code class="literal">euca2ools</code>:</p><div class="verbatim-wrap"><pre class="screen">$ euca-authorize -P icmp -t -1:-1 -s 0.0.0.0/0 default
$ euca-authorize -P tcp -p 22 -s 0.0.0.0/0 default</pre></div><p>If you have run these commands and still cannot ping or SSH your
instances, check the number of running <code class="literal">dnsmasq</code> processes, there
should be two. If not, kill the processes and restart the service with
these commands:</p><div class="verbatim-wrap"><pre class="screen"># killall dnsmasq
# service nova-network restart</pre></div></div><div class="sect2 " id="id-1.4.7.7.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure public (floating) IP addresses</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.9">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>This section describes how to configure floating IP addresses with
<code class="literal">nova-network</code>. For information about doing this with OpenStack
Networking, see <a class="xref" href="networking.html#l3-routing-and-nat" title="9.9.2. L3 routing and NAT">Section 9.9.2, “L3 routing and NAT”</a>.</p><div class="sect3 " id="id-1.4.7.7.9.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Private and public IP addresses</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.9.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In this section, the term floating IP address is used to refer to an IP
address, usually public, that you can dynamically add to a running
virtual instance.</p><p>Every virtual instance is automatically assigned a private IP address.
You can choose to assign a public (or floating) IP address instead.
OpenStack Compute uses network address translation (NAT) to assign
floating IPs to virtual instances.</p><p>To be able to assign a floating IP address, edit the
<code class="literal">/etc/nova/nova.conf</code> file to specify which interface the
<code class="literal">nova-network</code> service should bind public IP addresses to:</p><div class="verbatim-wrap highlight ini"><pre class="screen">public_interface=VLAN100</pre></div><p>If you make changes to the <code class="literal">/etc/nova/nova.conf</code> file while the
<code class="literal">nova-network</code> service is running, you will need to restart the service to
pick up the changes.</p><div id="id-1.4.7.7.9.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Floating IPs are implemented by using a source NAT (SNAT rule in
iptables), so security groups can sometimes display inconsistent
behavior if VMs use their floating IP to communicate with other VMs,
particularly on the same physical host. Traffic from VM to VM across
the fixed network does not have this issue, and so this is the
recommended setup. To ensure that traffic does not get SNATed to the
floating range, explicitly set:</p><div class="verbatim-wrap highlight ini"><pre class="screen">dmz_cidr=x.x.x.x/y</pre></div><p>The <code class="literal">x.x.x.x/y</code> value specifies the range of floating IPs for each
pool of floating IPs that you define. This configuration is also
required if the VMs in the source group have floating IPs.</p></div></div><div class="sect3 " id="id-1.4.7.7.9.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable IP forwarding</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.9.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>IP forwarding is disabled by default on most Linux distributions. You
will need to enable it in order to use floating IPs.</p><div id="id-1.4.7.7.9.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>IP forwarding only needs to be enabled on the nodes that run
<code class="literal">nova-network</code>. However, you will need to enable it on all compute
nodes if you use <code class="literal">multi_host</code> mode.</p></div><p>To check if IP forwarding is enabled, run:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/sys/net/ipv4/ip_forward
0</pre></div><p>Alternatively, run:</p><div class="verbatim-wrap"><pre class="screen">$ sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 0</pre></div><p>In these examples, IP forwarding is disabled.</p><p>To enable IP forwarding dynamically, run:</p><div class="verbatim-wrap"><pre class="screen"># sysctl -w net.ipv4.ip_forward=1</pre></div><p>Alternatively, run:</p><div class="verbatim-wrap"><pre class="screen"># echo 1 &gt; /proc/sys/net/ipv4/ip_forward</pre></div><p>To make the changes permanent, edit the <code class="literal">/etc/sysctl.conf</code> file and
update the IP forwarding setting:</p><div class="verbatim-wrap highlight ini"><pre class="screen">net.ipv4.ip_forward = 1</pre></div><p>Save the file and run this command to apply the changes:</p><div class="verbatim-wrap"><pre class="screen"># sysctl -p</pre></div><p>You can also apply the changes by restarting the network service:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>on Ubuntu, Debian:</p><div class="verbatim-wrap"><pre class="screen"># /etc/init.d/networking restart</pre></div></li><li class="listitem "><p>on RHEL, Fedora, CentOS, openSUSE and SLES:</p><div class="verbatim-wrap"><pre class="screen"># service network restart</pre></div></li></ul></div></div><div class="sect3 " id="id-1.4.7.7.9.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a list of available floating IP addresses</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.9.5">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Compute maintains a list of floating IP addresses that are available for
assigning to instances. Use the <code class="command">nova-manage floating</code> commands
to perform floating IP operations:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Add entries to the list:</p><div class="verbatim-wrap"><pre class="screen"># nova-manage floating create --pool nova --ip_range 68.99.26.170/31</pre></div></li><li class="listitem "><p>List the floating IP addresses in the pool:</p><div class="verbatim-wrap"><pre class="screen"># openstack floating ip list</pre></div></li><li class="listitem "><p>Create specific floating IPs for either a single address or a
subnet:</p><div class="verbatim-wrap"><pre class="screen"># nova-manage floating create --pool POOL_NAME --ip_range CIDR</pre></div></li><li class="listitem "><p>Remove floating IP addresses using the same parameters as the create
command:</p><div class="verbatim-wrap"><pre class="screen"># openstack floating ip delete CIDR</pre></div></li></ul></div><p>For more information about how administrators can associate floating IPs
with instances, see <a class="link" href="http://docs.openstack.org/admin-guide/cli-admin-manage-ip-addresses.html" target="_blank">Manage IP
addresses</a>
in the OpenStack Administrator Guide.</p></div><div class="sect3 " id="id-1.4.7.7.9.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Automatically add floating IPs</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.9.6">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You can configure <code class="literal">nova-network</code> to automatically allocate and assign a
floating IP address to virtual instances when they are launched. Add
this line to the <code class="literal">/etc/nova/nova.conf</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">auto_assign_floating_ip=True</pre></div><p>Save the file, and restart <code class="literal">nova-network</code></p><div id="id-1.4.7.7.9.6.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>If this option is enabled, but all floating IP addresses have
already been allocated, the <code class="command">openstack server create</code>
command will fail.</p></div></div></div><div class="sect2 " id="id-1.4.7.7.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove a network from a project</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.10">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You cannot delete a network that has been associated to a project. This
section describes the procedure for dissociating it so that it can be
deleted.</p><p>In order to disassociate the network, you will need the ID of the
project it has been associated to. To get the project ID, you will need
to be an administrator.</p><p>Disassociate the network from the project using the
<code class="command">nova-manage project scrub</code> command,
with the project ID as the final parameter:</p><div class="verbatim-wrap"><pre class="screen"># nova-manage project scrub --project ID</pre></div></div><div class="sect2 " id="id-1.4.7.7.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple interfaces for instances (multinic)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.11">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The multinic feature allows you to use more than one interface with your
instances. This is useful in several scenarios:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SSL Configurations (VIPs)</p></li><li class="listitem "><p>Services failover/HA</p></li><li class="listitem "><p>Bandwidth Allocation</p></li><li class="listitem "><p>Administrative/Public access to your instances</p></li></ul></div><p>Each VIP represents a separate network with its own IP block. Every
network mode has its own set of changes regarding multinic usage:</p><div class="figure" id="id-1.4.7.7.11.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg" target="_blank"><img src="images/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.4: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.11.5">#</a></h6></div></div><div class="figure" id="id-1.4.7.7.11.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg" target="_blank"><img src="images/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.5: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.11.6">#</a></h6></div></div><div class="figure" id="id-1.4.7.7.11.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg" target="_blank"><img src="images/SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.6: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.11.7">#</a></h6></div></div><div class="sect3 " id="id-1.4.7.7.11.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using multinic</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.11.8">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In order to use multinic, create two networks, and attach them to the
project (named <code class="literal">project</code> on the command line):</p><div class="verbatim-wrap"><pre class="screen">$ nova network-create first-net --fixed-range-v4 20.20.0.0/24 --project-id $your-project
$ nova network-create second-net --fixed-range-v4 20.20.10.0/24 --project-id $your-project</pre></div><p>Each new instance will now receive two IP addresses from their
respective DHCP servers:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server list
+---------+----------+--------+-----------------------------------------+------------+
|ID       | Name     | Status | Networks                                | Image Name |
+---------+----------+--------+-----------------------------------------+------------+
| 1234... | MyServer | ACTIVE | network2=20.20.0.3; private=20.20.10.14 | cirros     |
+---------+----------+--------+-----------------------------------------+------------+</pre></div><div id="id-1.4.7.7.11.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Make sure you start the second interface on the instance, or it
won't be reachable through the second IP.</p></div><p>This example demonstrates how to set up the interfaces within the
instance. This is the configuration that needs to be applied inside the
image.</p><p>Edit the <code class="literal">/etc/network/interfaces</code> file:</p><div class="verbatim-wrap highlight bash"><pre class="screen"># The loopback network interface
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet dhcp</pre></div><p>If the Virtual Network Service Neutron is installed, you can specify the
networks to attach to the interfaces by using the <code class="literal">--nic</code> flag with
the <code class="command">openstack server create</code> command:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server create --image ed8b2a37-5535-4a5f-a615-443513036d71 \
  --flavor 1 --nic net-id=NETWORK1_ID --nic net-id=NETWORK2_ID test-vm1</pre></div></div></div><div class="sect2 " id="id-1.4.7.7.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Networking</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.7.12.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cannot reach floating IPs</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.2.3"><span class="name">Problem</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.2.3">#</a></h5></div><p>You cannot reach your instances through the floating IP address.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.2.5"><span class="name">Solution</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.2.5">#</a></h5></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Check that the default security group allows ICMP (ping) and SSH
(port 22), so that you can reach the instances:</p><div class="verbatim-wrap"><pre class="screen">$ openstack security group rule list default
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| ID                                   | IP Protocol | IP Range  | Port Range      | Remote Security Group |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| 63536865-e5b6-4df1-bac5-ca6d97d8f54d | tcp         | 0.0.0.0/0 | 22:22           | None                  |
| e9d3200f-647a-4293-a9fc-e65ceee189ae | icmp        | 0.0.0.0/0 | type=1:code=-1  | None                  |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+</pre></div></li><li class="listitem "><p>Check the NAT rules have been added to iptables on the node that is
running <code class="literal">nova-network</code>:</p><div class="verbatim-wrap"><pre class="screen"># iptables -L -nv -t nat
-A nova-network-PREROUTING -d 68.99.26.170/32 -j DNAT --to-destination 10.0.0.3
-A nova-network-floating-snat -s 10.0.0.3/32 -j SNAT --to-source 68.99.26.170</pre></div></li><li class="listitem "><p>Check that the public address (<code class="literal">68.99.26.170</code> in
this example), has been added to your public interface. You should
see the address in the listing when you use the <code class="command">ip addr</code> command:</p><div class="verbatim-wrap"><pre class="screen">$ ip addr
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
link/ether xx:xx:xx:17:4b:c2 brd ff:ff:ff:ff:ff:ff
inet 13.22.194.80/24 brd 13.22.194.255 scope global eth0
inet 68.99.26.170/32 scope global eth0
inet6 fe80::82b:2bf:fe1:4b2/64 scope link
valid_lft forever preferred_lft forever</pre></div><div id="id-1.4.7.7.12.2.6.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>You cannot use <code class="literal">SSH</code> to access an instance with a public IP from within
the same server because the routing configuration does not allow
it.</p></div></li><li class="listitem "><p>Use <code class="literal">tcpdump</code> to identify if packets are being routed to the
inbound interface on the compute host. If the packets are reaching
the compute hosts but the connection is failing, the issue may be
that the packet is being dropped by reverse path filtering. Try
disabling reverse-path filtering on the inbound interface. For
example, if the inbound interface is <code class="literal">eth2</code>, run:</p><div class="verbatim-wrap"><pre class="screen"># sysctl -w net.ipv4.conf.ETH2.rp_filter=0</pre></div><p>If this solves the problem, add the following line to
<code class="literal">/etc/sysctl.conf</code> so that the reverse-path filter is persistent:</p><div class="verbatim-wrap highlight ini"><pre class="screen">net.ipv4.conf.rp_filter=0</pre></div></li></ul></div></div><div class="sect3 " id="id-1.4.7.7.12.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Temporarily disable firewall</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.3.3"><span class="name">Problem</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.3.3">#</a></h5></div><p>Networking issues prevent administrators accessing or reaching VM's
through various pathways.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.3.5"><span class="name">Solution</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.3.5">#</a></h5></div><p>You can disable the firewall by setting this option
in <code class="literal">/etc/nova/nova.conf</code>:</p><div class="verbatim-wrap highlight ini"><pre class="screen">firewall_driver=nova.virt.firewall.NoopFirewallDriver</pre></div></div><div class="sect3 " id="id-1.4.7.7.12.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Packet loss from instances to nova-network server (VLANManager mode)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.4.3"><span class="name">Problem</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.4.3">#</a></h5></div><p>If you can access your instances with <code class="literal">SSH</code> but the network to your instance
is slow, or if you find that running certain operations are slower than
they should be (for example, <code class="literal">sudo</code>), packet loss could be occurring
on the connection to the instance.</p><p>Packet loss can be caused by Linux networking configuration settings
related to bridges. Certain settings can cause packets to be dropped
between the VLAN interface (for example, <code class="literal">vlan100</code>) and the associated
bridge interface (for example, <code class="literal">br100</code>) on the host running
<code class="literal">nova-network</code>.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.4.6"><span class="name">Solution</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.4.6">#</a></h5></div><p>One way to check whether this is the problem is to open three terminals
and run the following commands:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>In the first terminal, on the host running <code class="literal">nova-network</code>, use
<code class="literal">tcpdump</code> on the VLAN interface to monitor DNS-related traffic
(UDP, port 53). As root, run:</p><div class="verbatim-wrap"><pre class="screen"># tcpdump -K -p -i vlan100 -v -vv udp port 53</pre></div></li><li class="step "><p>In the second terminal, also on the host running <code class="literal">nova-network</code>, use
<code class="literal">tcpdump</code> to monitor DNS-related traffic on the bridge interface.
As root, run:</p><div class="verbatim-wrap"><pre class="screen"># tcpdump -K -p -i br100 -v -vv udp port 53</pre></div></li><li class="step "><p>In the third terminal, use <code class="literal">SSH</code> to access the instance and generate DNS
requests by using the <code class="command">nslookup</code> command:</p><div class="verbatim-wrap"><pre class="screen">$ nslookup www.google.com</pre></div><p>The symptoms may be intermittent, so try running <code class="command">nslookup</code>
multiple times. If the network configuration is correct, the command
should return immediately each time. If it is not correct, the
command hangs for several seconds before returning.</p></li><li class="step "><p>If the <code class="command">nslookup</code> command sometimes hangs, and there are packets
that appear in the first terminal but not the second, then the
problem may be due to filtering done on the bridges. Try disabling
filtering, and running these commands as root:</p><div class="verbatim-wrap"><pre class="screen"># sysctl -w net.bridge.bridge-nf-call-arptables=0
# sysctl -w net.bridge.bridge-nf-call-iptables=0
# sysctl -w net.bridge.bridge-nf-call-ip6tables=0</pre></div><p>If this solves your issue, add the following line to
<code class="literal">/etc/sysctl.conf</code> so that the changes are persistent:</p><div class="verbatim-wrap highlight ini"><pre class="screen">net.bridge.bridge-nf-call-arptables=0
net.bridge.bridge-nf-call-iptables=0
net.bridge.bridge-nf-call-ip6tables=0</pre></div></li></ol></div></div></div><div class="sect3 " id="id-1.4.7.7.12.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.3.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM: Network connectivity works initially, then fails</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.5">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.5.3"><span class="name">Problem</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.5.3">#</a></h5></div><p>With KVM hypervisors, instances running Ubuntu 12.04 sometimes lose
network connectivity after functioning properly for a period of time.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.7.7.12.5.5"><span class="name">Solution</span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.7.12.5.5">#</a></h5></div><p>Try loading the <code class="literal">vhost_net</code> kernel module as a workaround for this
issue (see <a class="link" href="https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/997978/" target="_blank">bug
#997978</a>)
. This kernel module may also <a class="link" href="http://www.linux-kvm.org/page/VhostNet" target="_blank">improve network
performance</a> on KVM. To load
the kernel module:</p><div class="verbatim-wrap"><pre class="screen"># modprobe vhost_net</pre></div><div id="id-1.4.7.7.12.5.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Loading the module has no effect on running instances.</p></div></div></div></div><div class="sect1 " id="id-1.4.7.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System administration</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To effectively administer compute, you must understand how the different
        installed nodes interact with each other. Compute can be installed in
        many different ways using multiple servers, but generally multiple
        compute nodes control the virtual servers and a cloud controller node
        contains the remaining Compute services.</p><p>The Compute cloud works using a series of daemon processes named <code class="literal">nova-*</code>
        that exist persistently on the host machine. These binaries can all run
        on the same machine or be spread out on multiple boxes in a large
        deployment. The responsibilities of services and drivers are:</p><p>
        <span class="bold"><strong>Services</strong></span>
      </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.8.5.1"><span class="term ">
            <code class="literal">nova-api</code>
          </span></dt><dd><p>receives XML requests and sends them to the rest of the
              system. A WSGI app routes and authenticates requests. Supports the
              EC2 and OpenStack APIs. A <code class="literal">nova.conf</code> configuration file is created
              when Compute is installed.</p></dd><dt id="id-1.4.7.8.5.2"><span class="term ">
            <code class="literal">nova-cert</code>
          </span></dt><dd><p>manages certificates.</p></dd><dt id="id-1.4.7.8.5.3"><span class="term ">
            <code class="literal">nova-compute</code>
          </span></dt><dd><p>manages virtual machines. Loads a Service object, and
              exposes the public methods on ComputeManager through a Remote
              Procedure Call (RPC).</p></dd><dt id="id-1.4.7.8.5.4"><span class="term ">
            <code class="literal">nova-conductor</code>
          </span></dt><dd><p>provides database-access support for compute nodes
              (thereby reducing security risks).</p></dd><dt id="id-1.4.7.8.5.5"><span class="term ">
            <code class="literal">nova-consoleauth</code>
          </span></dt><dd><p>manages console authentication.</p></dd><dt id="id-1.4.7.8.5.6"><span class="term ">
            <code class="literal">nova-objectstore</code>
          </span></dt><dd><p>a simple file-based storage system for images that
              replicates most of the S3 API. It can be replaced with OpenStack
              Image service and either a simple image manager or OpenStack Object
              Storage as the virtual machine image storage facility. It must exist
              on the same node as <code class="literal">nova-compute</code>.</p></dd><dt id="id-1.4.7.8.5.7"><span class="term ">
            <code class="literal">nova-network</code>
          </span></dt><dd><p>manages floating and fixed IPs, DHCP, bridging and
              VLANs. Loads a Service object which exposes the public methods on one
              of the subclasses of NetworkManager. Different networking strategies
              are available by changing the <code class="literal">network_manager</code> configuration
              option to <code class="literal">FlatManager</code>, <code class="literal">FlatDHCPManager</code>, or <code class="literal">VLANManager</code>
              (defaults to <code class="literal">VLANManager</code> if nothing is specified).</p></dd><dt id="id-1.4.7.8.5.8"><span class="term ">
            <code class="literal">nova-scheduler</code>
          </span></dt><dd><p>dispatches requests for new virtual machines to the
              correct node.</p></dd><dt id="id-1.4.7.8.5.9"><span class="term ">
            <code class="literal">nova-novncproxy</code>
          </span></dt><dd><p>provides a VNC proxy for browsers, allowing VNC
              consoles to access virtual machines.</p></dd></dl></div><div id="id-1.4.7.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Some services have drivers that change how the service implements
          its core functionality. For example, the <code class="literal">nova-compute</code> service
          supports drivers that let you choose which hypervisor type it can
          use. <code class="literal">nova-network</code> and <code class="literal">nova-scheduler</code> also have drivers.</p></div><div class="sect2 " id="id-1.4.7.8.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manage Compute users</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Access to the Euca2ools (ec2) API is controlled by an access key and a
secret key. The user's access key needs to be included in the request,
and the request must be signed with the secret key. Upon receipt of API
requests, Compute verifies the signature and runs commands on behalf of
the user.</p><p>To begin using Compute, you must create a user with the Identity
service.</p></div><div class="sect2 " id="id-1.4.7.8.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manage volumes</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.8">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Depending on the setup of your cloud provider, they may give you an
endpoint to use to manage volumes, or there may be an extension under
the covers. In either case, you can use the <code class="literal">openstack</code> CLI to manage
volumes.</p><div class="table" id="id-1.4.7.8.8.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.3: </span><span class="name">openstack volume commands </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.8.3">#</a></h6></div><div class="table-contents"><table class="table" summary="openstack volume commands" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                  <p>Command</p>
                </th><th>
                  <p>Description</p>
                </th></tr></thead><tbody><tr><td>
                  <p>server add volume</p>
                </td><td>
                  <p>Attach a volume to a server.</p>
                </td></tr><tr><td>
                  <p>volume create</p>
                </td><td>
                  <p>Add a new volume.</p>
                </td></tr><tr><td>
                  <p>volume delete</p>
                </td><td>
                  <p>Remove or delete a volume.</p>
                </td></tr><tr><td>
                  <p>server remove volume</p>
                </td><td>
                  <p>Detach or remove a volume from a server.</p>
                </td></tr><tr><td>
                  <p>volume list</p>
                </td><td>
                  <p>List all the volumes.</p>
                </td></tr><tr><td>
                  <p>volume show</p>
                </td><td>
                  <p>Show details about a volume.</p>
                </td></tr><tr><td>
                  <p>snapshot create</p>
                </td><td>
                  <p>Add a new snapshot.</p>
                </td></tr><tr><td>
                  <p>snapshot delete</p>
                </td><td>
                  <p>Remove a snapshot.</p>
                </td></tr><tr><td>
                  <p>snapshot list</p>
                </td><td>
                  <p>List all the snapshots.</p>
                </td></tr><tr><td>
                  <p>snapshot show</p>
                </td><td>
                  <p>Show details about a snapshot.</p>
                </td></tr><tr><td>
                  <p>volume type create</p>
                </td><td>
                  <p>Create a new volume type.</p>
                </td></tr><tr><td>
                  <p>volume type delete</p>
                </td><td>
                  <p>Delete a specific flavor</p>
                </td></tr><tr><td>
                  <p>volume type list</p>
                </td><td>
                  <p>Print a list of available 'volume types'.</p>
                </td></tr></tbody></table></div></div><p>For example, to list IDs and names of volumes, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack volume list
+--------+--------------+-----------+------+-------------+
| ID     | Display Name | Status    | Size | Attached to |
+--------+--------------+-----------+------+-------------+
| 86e6cb | testnfs      | available |    1 |             |
| e389f7 | demo         | available |    1 |             |
+--------+--------------+-----------+------+-------------+</pre></div></div><div class="sect2 " id="compute-flavors"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Flavors</span> <a title="Permalink" class="permalink" href="bk02ch05.html#compute-flavors">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span>compute-flavors</li></ul></div></div></div></div><p>Admin users can use the <code class="command">openstack flavor</code> command to customize and
manage flavors. To see information for this command, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor --help
Command "flavor" matches:
  flavor create
  flavor delete
  flavor list
  flavor set
  flavor show
  flavor unset</pre></div><div id="id-1.4.7.8.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Configuration rights can be delegated to additional users by
redefining the access controls for
<code class="literal">compute_extension:flavormanage</code> in <code class="literal">/etc/nova/policy.json</code>
on the <code class="literal">nova-api</code> server.</p></li><li class="listitem "><p>The Dashboard simulates the ability to modify a flavor
by deleting an existing flavor and creating a new one with the same name.</p></li></ul></div></div><p>Flavors define these elements:</p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                  <p>Element</p>
                </th><th>
                  <p>Description</p>
                </th></tr></thead><tbody><tr><td>
                  <p>Name</p>
                </td><td>
                  <p>A descriptive name. XX.SIZE_NAME is typically not required,
though some third party tools may rely on it.</p>
                </td></tr><tr><td>
                  <p>Memory MB</p>
                </td><td>
                  <p>Instance memory in megabytes.</p>
                </td></tr><tr><td>
                  <p>Disk</p>
                </td><td>
                  <p>Virtual root disk size in gigabytes. This is an ephemeral disk that the base image is copied into. When booting from a persistent volume it is not used. The "0" size is a special case which uses the native base image size as the size of the
ephemeral root volume.</p>
                </td></tr><tr><td>
                  <p>Ephemeral</p>
                </td><td>
                  <p>Specifies the size of a secondary ephemeral data disk. This
is an empty, unformatted disk and exists only for the life of the instance. Default value is <code class="literal">0</code>.</p>
                </td></tr><tr><td>
                  <p>Swap</p>
                </td><td>
                  <p>Optional swap space allocation for the instance. Default
value is <code class="literal">0</code>.</p>
                </td></tr><tr><td>
                  <p>VCPUs</p>
                </td><td>
                  <p>Number of virtual CPUs presented to the instance.</p>
                </td></tr><tr><td>
                  <p>RXTX Factor</p>
                </td><td>
                  <p>Optional property allows created servers to have a different
bandwidth cap than that defined in the network they are attached to. This factor is multiplied by the rxtx_base property of the network. Default value is <code class="literal">1.0</code>. That is, the same
as attached network. This parameter is only available for Xen
or NSX based systems.</p>
                </td></tr><tr><td>
                  <p>Is Public</p>
                </td><td>
                  <p>Boolean value, whether flavor is available to all users or private to the project it was created in. Defaults to <code class="literal">True</code>.</p>
                </td></tr><tr><td>
                  <p>Extra Specs</p>
                </td><td>
                  <p>Key and value pairs that define on which compute nodes a flavor can run. These pairs must match corresponding pairs on the compute nodes. Use to implement special resources, such as flavors that run on only compute nodes with GPU hardware.</p>
                </td></tr></tbody></table></div><div id="id-1.4.7.8.9.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Flavor customization can be limited by the hypervisor in use. For
example the libvirt driver enables quotas on CPUs available to a VM,
disk tuning, bandwidth I/O, watchdog behavior, random number generator
device control, and instance VIF traffic control.</p></div><div class="sect3 " id="id-1.4.7.8.9.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Is Public</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.9.8">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Flavors can be assigned to particular projects. By default, a flavor is public
and available to all projects. Private flavors are only accessible to those on
the access list and are invisible to other projects. To create and assign a
private flavor to a project, run this command:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor create --private p1.medium --id auto --ram 512 --disk 40 --vcpus 4</pre></div></div><div class="sect3 " id="id-1.4.7.8.9.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Extra Specs</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.9.9">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.8.9.9.2.1"><span class="term ">CPU limits</span></dt><dd><p>You can configure the CPU limits with control parameters with the
<code class="literal">nova</code> client. For example, to configure the I/O limit, use:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property quota:read_bytes_sec=10240000 \
    --property quota:write_bytes_sec=10240000</pre></div><p>Use these optional parameters to control weight shares, enforcement
intervals for runtime quotas, and a quota for maximum allowed
bandwidth:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">cpu_shares</code>: Specifies the proportional weighted share for the
domain. If this element is omitted, the service defaults to the
OS provided defaults. There is no unit for the value; it is a
relative measure based on the setting of other VMs. For example,
a VM configured with value 2048 gets twice as much CPU time as a
VM configured with value 1024.</p></li><li class="listitem "><p><code class="literal">cpu_shares_level</code>: On VMware, specifies the allocation level. Can
be <code class="literal">custom</code>, <code class="literal">high</code>, <code class="literal">normal</code>, or <code class="literal">low</code>. If you choose
<code class="literal">custom</code>, set the number of shares using <code class="literal">cpu_shares_share</code>.</p></li><li class="listitem "><p><code class="literal">cpu_period</code>: Specifies the enforcement interval (unit:
microseconds) for QEMU and LXC hypervisors. Within a period, each
VCPU of the domain is not allowed to consume more than the quota
worth of runtime. The value should be in range <code class="literal">[1000, 1000000]</code>.
A period with value 0 means no value.</p></li><li class="listitem "><p><code class="literal">cpu_limit</code>: Specifies the upper limit for VMware machine CPU
allocation in MHz. This parameter ensures that a machine never
uses more than the defined amount of CPU time. It can be used to
enforce a limit on the machine's CPU performance.</p></li><li class="listitem "><p><code class="literal">cpu_reservation</code>: Specifies the guaranteed minimum CPU
reservation in MHz for VMware. This means that if needed, the
machine will definitely get allocated the reserved amount of CPU
cycles.</p></li><li class="listitem "><p><code class="literal">cpu_quota</code>: Specifies the maximum allowed bandwidth (unit:
microseconds). A domain with a negative-value quota indicates
that the domain has infinite bandwidth, which means that it is
not bandwidth controlled. The value should be in range <code class="literal">[1000,
18446744073709551]</code> or less than 0. A quota with value 0 means no
value. You can use this feature to ensure that all vCPUs run at the
same speed. For example:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property quota:cpu_quota=10000 \
    --property quota:cpu_period=20000</pre></div><p>In this example, an instance of <code class="literal">FLAVOR-NAME</code> can only consume
a maximum of 50% CPU of a physical CPU computing capability.</p></li></ul></div></dd><dt id="id-1.4.7.8.9.9.2.2"><span class="term ">Memory limits</span></dt><dd><p>For VMware, you can configure the memory limits with control parameters.</p><p>Use these optional parameters to limit the memory allocation,
guarantee minimum memory reservation, and to specify shares
used in case of resource contention:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">memory_limit</code>: Specifies the upper limit for VMware machine
memory allocation in MB. The utilization of a virtual machine will
not exceed this limit, even if there are available resources. This
is typically used to ensure a consistent performance of
virtual machines independent of available resources.</p></li><li class="listitem "><p><code class="literal">memory_reservation</code>: Specifies the guaranteed minimum memory
reservation in MB for VMware. This means the specified amount of
memory will definitely be allocated to the machine.</p></li><li class="listitem "><p><code class="literal">memory_shares_level</code>: On VMware, specifies the allocation level.
This can be <code class="literal">custom</code>, <code class="literal">high</code>, <code class="literal">normal</code> or <code class="literal">low</code>. If you choose
<code class="literal">custom</code>, set the number of shares using <code class="literal">memory_shares_share</code>.</p></li><li class="listitem "><p><code class="literal">memory_shares_share</code>: Specifies the number of shares allocated
in the event that <code class="literal">custom</code> is used. There is no unit for this
value. It is a relative measure based on the settings for other VMs.
For example:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property quota:memory_shares_level=custom \
    --property quota:memory_shares_share=15</pre></div></li></ul></div></dd><dt id="id-1.4.7.8.9.9.2.3"><span class="term ">Disk I/O limits</span></dt><dd><p>For VMware, you can configure the resource limits for disk
with control parameters.</p><p>Use these optional parameters to limit the disk utilization,
guarantee disk allocation, and to specify shares
used in case of resource contention. This allows the VMware
driver to enable disk allocations for the running instance.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">disk_io_limit</code>: Specifies the upper limit for disk
utilization in I/O per second. The utilization of a
virtual machine will not exceed this limit, even
if there are available resources. The default value
is -1 which indicates unlimited usage.</p></li><li class="listitem "><p><code class="literal">disk_io_reservation</code>: Specifies the guaranteed minimum disk
allocation in terms of <a class="xref" href="bk02go01.html#term-input-output-operations-per-second-iops" title="Input/Output Operations Per Second (IOPS)">Input/Output Operations Per Second (IOPS)</a>.</p></li><li class="listitem "><p><code class="literal">disk_io_shares_level</code>: Specifies the allocation
level. This can be <code class="literal">custom</code>, <code class="literal">high</code>, <code class="literal">normal</code> or <code class="literal">low</code>.
If you choose custom, set the number of shares
using <code class="literal">disk_io_shares_share</code>.</p></li><li class="listitem "><p><code class="literal">disk_io_shares_share</code>: Specifies the number of shares
allocated in the event that <code class="literal">custom</code> is used.
When there is resource contention, this value is used
to determine the resource allocation.</p><p>The example below sets the <code class="literal">disk_io_reservation</code> to 2000 IOPS.</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property quota:disk_io_reservation=2000</pre></div></li></ul></div></dd><dt id="id-1.4.7.8.9.9.2.4"><span class="term ">Disk tuning</span></dt><dd><p>Using disk I/O quotas, you can set maximum disk write to 10 MB per
second for a VM user. For example:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property quota:disk_write_bytes_sec=10485760</pre></div><p>The disk I/O options are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
                      <code class="literal">disk_read_bytes_sec</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">disk_read_iops_sec</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">disk_write_bytes_sec</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">disk_write_iops_sec</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">disk_total_bytes_sec</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">disk_total_iops_sec</code>
                    </p></li></ul></div></dd><dt id="id-1.4.7.8.9.9.2.5"><span class="term ">Bandwidth I/O</span></dt><dd><p>The vif I/O options are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
                      <code class="literal">vif_inbound_average</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">vif_inbound_burst</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">vif_inbound_peak</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">vif_outbound_average</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">vif_outbound_burst</code>
                    </p></li><li class="listitem "><p>
                      <code class="literal">vif_outbound_peak</code>
                    </p></li></ul></div><p>Incoming and outgoing traffic can be shaped independently. The
bandwidth element can have at most, one inbound and at most, one
outbound child element. If you leave any of these child elements
out, no <a class="xref" href="bk02go01.html#term-quality-of-service-qos" title="Quality of Service (QoS)">Quality of Service (QoS)</a> is applied on that traffic
direction. So, if you want to shape only the network's incoming
traffic, use inbound only (and vice versa). Each element has one
mandatory attribute average, which specifies the average bit rate on
the interface being shaped.</p><p>There are also two optional attributes (integer): <code class="literal">peak</code>, which
specifies the maximum rate at which a bridge can send data
(kilobytes/second), and <code class="literal">burst</code>, the amount of bytes that can be
burst at peak speed (kilobytes). The rate is shared equally within
domains connected to the network.</p><p>The example below sets network traffic bandwidth limits for existing
flavor as follows:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Outbound traffic:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>average: 262 Mbps (32768 kilobytes/second)</p></li><li class="listitem "><p>peak: 524 Mbps (65536 kilobytes/second)</p></li><li class="listitem "><p>burst: 65536 kilobytes</p></li></ul></div></li><li class="listitem "><p>Inbound traffic:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>average: 262 Mbps (32768 kilobytes/second)</p></li><li class="listitem "><p>peak: 524 Mbps (65536 kilobytes/second)</p></li><li class="listitem "><p>burst: 65536 kilobytes</p></li></ul></div></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property quota:vif_outbound_average=32768 \
    --property quota:vif_outbound_peak=65536 \
    --property quota:vif_outbound_burst=65536 \
    --property quota:vif_inbound_average=32768 \
    --property quota:vif_inbound_peak=65536 \
    --property quota:vif_inbound_burst=65536</pre></div><div id="id-1.4.7.8.9.9.2.5.2.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>All the speed limit values in above example are specified in
kilobytes/second. And burst values are in kilobytes. Values
were converted using 'Data rate units on
Wikipedia &lt;<a class="link" href="https://en.wikipedia.org/wiki/Data_rate_units" target="_blank">https://en.wikipedia.org/wiki/Data_rate_units</a>&gt;`_.</p></div></dd><dt id="id-1.4.7.8.9.9.2.6"><span class="term ">Watchdog behavior</span></dt><dd><p>For the libvirt driver, you can enable and set the behavior of a
virtual hardware watchdog device for each flavor. Watchdog devices
keep an eye on the guest server, and carry out the configured
action, if the server hangs. The watchdog uses the i6300esb device
(emulating a PCI Intel 6300ESB). If <code class="literal">hw:watchdog_action</code> is not
specified, the watchdog is disabled.</p><p>To set the behavior, use:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME --property hw:watchdog_action=ACTION</pre></div><p>Valid ACTION values are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">disabled</code>: (default) The device is not attached.</p></li><li class="listitem "><p><code class="literal">reset</code>: Forcefully reset the guest.</p></li><li class="listitem "><p><code class="literal">poweroff</code>: Forcefully power off the guest.</p></li><li class="listitem "><p><code class="literal">pause</code>: Pause the guest.</p></li><li class="listitem "><p><code class="literal">none</code>: Only enable the watchdog; do nothing if the server hangs.</p></li></ul></div><div id="id-1.4.7.8.9.9.2.6.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Watchdog behavior set using a specific image's properties will
override behavior set using flavors.</p></div></dd><dt id="id-1.4.7.8.9.9.2.7"><span class="term ">Random-number generator</span></dt><dd><p>If a random-number generator device has been added to the instance
through its image properties, the device can be enabled and
configured using:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property hw_rng:allowed=True \
    --property hw_rng:rate_bytes=RATE-BYTES \
    --property hw_rng:rate_period=RATE-PERIOD</pre></div><p>Where:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>RATE-BYTES: (integer) Allowed amount of bytes that the guest can
read from the host's entropy per period.</p></li><li class="listitem "><p>RATE-PERIOD: (integer) Duration of the read period in seconds.</p></li></ul></div></dd><dt id="id-1.4.7.8.9.9.2.8"><span class="term ">CPU topology</span></dt><dd><p>For the libvirt driver, you can define the topology of the processors
in the virtual machine using properties. The properties with <code class="literal">max</code>
limit the number that can be selected by the user with image properties.</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property hw:cpu_sockets=FLAVOR-SOCKETS \
    --property hw:cpu_cores=FLAVOR-CORES \
    --property hw:cpu_threads=FLAVOR-THREADS \
    --property hw:cpu_max_sockets=FLAVOR-SOCKETS \
    --property hw:cpu_max_cores=FLAVOR-CORES \
    --property hw:cpu_max_threads=FLAVOR-THREADS</pre></div><p>Where:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>FLAVOR-SOCKETS: (integer) The number of sockets for the guest VM. By
default, this is set to the number of vCPUs requested.</p></li><li class="listitem "><p>FLAVOR-CORES: (integer) The number of cores per socket for the guest
VM. By default, this is set to <code class="literal">1</code>.</p></li><li class="listitem "><p>FLAVOR-THREADS: (integer) The number of threads per core for the guest
VM. By default, this is set to <code class="literal">1</code>.</p></li></ul></div></dd><dt id="id-1.4.7.8.9.9.2.9"><span class="term ">CPU pinning policy</span></dt><dd><p>For the libvirt driver, you can pin the virtual CPUs (vCPUs) of instances
to the host's physical CPU cores (pCPUs) using properties. You can further
refine this by stating how hardware CPU threads in a simultaneous
multithreading-based (SMT) architecture be used. These configurations will
result in improved per-instance determinism and performance.</p><div id="id-1.4.7.8.9.9.2.9.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>SMT-based architectures include Intel processors with Hyper-Threading
technology. In these architectures, processor cores share a number of
components with one or more other cores. Cores in such architectures
are commonly referred to as hardware threads, while the cores that a
given core share components with are known as thread siblings.</p></div><div id="id-1.4.7.8.9.9.2.9.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Host aggregates should be used to separate these pinned instances
from unpinned instances as the latter will not respect the resourcing
requirements of the former.</p></div><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property hw:cpu_policy=CPU-POLICY \
    --property hw:cpu_thread_policy=CPU-THREAD-POLICY</pre></div><p>Valid CPU-POLICY values are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">shared</code>: (default) The guest vCPUs will be allowed to freely float
across host pCPUs, albeit potentially constrained by NUMA policy.</p></li><li class="listitem "><p><code class="literal">dedicated</code>: The guest vCPUs will be strictly pinned to a set of host
pCPUs. In the absence of an explicit vCPU topology request, the drivers
typically expose all vCPUs as sockets with one core and one thread.
When strict CPU pinning is in effect the guest CPU topology will be
setup to match the topology of the CPUs to which it is pinned. This
option implies an overcommit ratio of 1.0. For example, if a two vCPU
guest is pinned to a single host core with two threads, then the guest
will get a topology of one socket, one core, two threads.</p></li></ul></div><p>Valid CPU-THREAD-POLICY values are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">prefer</code>: (default) The host may or may not have an SMT architecture.
Where an SMT architecture is present, thread siblings are preferred.</p></li><li class="listitem "><p><code class="literal">isolate</code>: The host must not have an SMT architecture or must emulate
a non-SMT architecture. If the host does not have an SMT architecture,
each vCPU is placed on a different core as expected. If the host does
have an SMT architecture - that is, one or more cores have thread
siblings - then each vCPU is placed on a different physical core. No
vCPUs from other guests are placed on the same core. All but one thread
sibling on each utilized core is therefore guaranteed to be unusable.</p></li><li class="listitem "><p><code class="literal">require</code>: The host must have an SMT architecture. Each vCPU is
allocated on thread siblings. If the host does not have an SMT
architecture, then it is not used. If the host has an SMT architecture,
but not enough cores with free thread siblings are available, then
scheduling fails.</p></li></ul></div><div id="id-1.4.7.8.9.9.2.9.2.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The <code class="literal">hw:cpu_thread_policy</code> option is only valid if <code class="literal">hw:cpu_policy</code>
is set to <code class="literal">dedicated</code>.</p></div></dd><dt id="id-1.4.7.8.9.9.2.10"><span class="term ">NUMA topology</span></dt><dd><p>For the libvirt driver, you can define the host NUMA placement for the
instance vCPU threads as well as the allocation of instance vCPUs and
memory from the host NUMA nodes. For flavors whose memory and vCPU
allocations are larger than the size of NUMA nodes in the compute hosts,
the definition of a NUMA topology allows hosts to better utilize NUMA
and improve performance of the instance OS.</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property hw:numa_nodes=FLAVOR-NODES \
    --property hw:numa_cpus.N=FLAVOR-CORES \
    --property hw:numa_mem.N=FLAVOR-MEMORY</pre></div><p>Where:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>FLAVOR-NODES: (integer) The number of host NUMA nodes to restrict
execution of instance vCPU threads to. If not specified, the vCPU
threads can run on any number of the host NUMA nodes available.</p></li><li class="listitem "><p>N: (integer) The instance NUMA node to apply a given CPU or memory
configuration to, where N is in the range <code class="literal">0</code> to <code class="literal">FLAVOR-NODES</code>
- <code class="literal">1</code>.</p></li><li class="listitem "><p>FLAVOR-CORES: (comma-separated list of integers) A list of instance
vCPUs to map to instance NUMA node N. If not specified, vCPUs are evenly
divided among available NUMA nodes.</p></li><li class="listitem "><p>FLAVOR-MEMORY: (integer) The number of MB of instance memory to map to
instance NUMA node N. If not specified, memory is evenly divided
among available NUMA nodes.</p></li></ul></div><div id="id-1.4.7.8.9.9.2.10.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p><code class="literal">hw:numa_cpus.N</code> and <code class="literal">hw:numa_mem.N</code> are only valid if
<code class="literal">hw:numa_nodes</code> is set. Additionally, they are only required if the
instance's NUMA nodes have an asymmetrical allocation of CPUs and RAM
(important for some NFV workloads).</p></div><div id="id-1.4.7.8.9.9.2.10.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The <code class="literal">N</code> parameter is an index of <span class="emphasis"><em>guest</em></span> NUMA nodes and may not
correspond to <span class="emphasis"><em>host</em></span> NUMA nodes. For example, on a platform with two
NUMA nodes, the scheduler may opt to place guest NUMA node 0, as
referenced in <code class="literal">hw:numa_mem.0</code> on host NUMA node 1 and vice versa.
Similarly, the integers used for <code class="literal">FLAVOR-CORES</code> are indexes of
<span class="emphasis"><em>guest</em></span> vCPUs and may not correspond to <span class="emphasis"><em>host</em></span> CPUs. As such, this
feature cannot be used to constrain instances to specific host CPUs or
NUMA nodes.</p></div><div id="id-1.4.7.8.9.9.2.10.2.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>If the combined values of <code class="literal">hw:numa_cpus.N</code> or <code class="literal">hw:numa_mem.N</code>
are greater than the available number of CPUs or memory respectively,
an exception is raised.</p></div></dd><dt id="id-1.4.7.8.9.9.2.11"><span class="term ">Large pages allocation</span></dt><dd><p>You can configure the size of large pages used to back the VMs.</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property hw:mem_page_size=PAGE_SIZE</pre></div><p>Valid <code class="literal">PAGE_SIZE</code> values are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">small</code>: (default) The smallest page size is used.
Example: 4 KB on x86.</p></li><li class="listitem "><p><code class="literal">large</code>: Only use larger page sizes for guest RAM.
Example: either 2 MB or 1 GB on x86.</p></li><li class="listitem "><p><code class="literal">any</code>: It is left up to the compute driver to decide. In this case,
the libvirt driver might try to find large pages, but fall back to small
pages. Other drivers may choose alternate policies for <code class="literal">any</code>.</p></li><li class="listitem "><p>pagesize: (string) An explicit page size can be set if the workload has
specific requirements. This value can be an integer value for the page
size in KB, or can use any standard suffix.
Example: <code class="literal">4KB</code>, <code class="literal">2MB</code>, <code class="literal">2048</code>, <code class="literal">1GB</code>.</p></li></ul></div><div id="id-1.4.7.8.9.9.2.11.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Large pages can be enabled for guest RAM without any regard to whether
the guest OS will use them or not. If the guest OS chooses not to
use huge pages, it will merely see small pages as before. Conversely,
if a guest OS does intend to use huge pages, it is very important that
the guest RAM be backed by huge pages. Otherwise, the guest OS will not
be getting the performance benefit it is expecting.</p></div></dd><dt id="id-1.4.7.8.9.9.2.12"><span class="term ">PCI passthrough</span></dt><dd><p>You can assign PCI devices to a guest by specifying them in the flavor.</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set FLAVOR-NAME \
    --property pci_passthrough:alias=ALIAS:COUNT</pre></div><p>Where:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>ALIAS: (string) The alias which correspond to a particular PCI device
class as configured in the nova configuration file (see <a class="link" href="http://docs.openstack.org/newton/config-reference/compute/config-options.html" target="_blank">nova.conf
configuration options</a>).</p></li><li class="listitem "><p>COUNT: (integer) The amount of PCI devices of type ALIAS to be assigned
to a guest.</p></li></ul></div></dd></dl></div></div></div><div class="sect2 " id="id-1.4.7.8.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute service node firewall requirements</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.10">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Console connections for virtual machines, whether direct or through a
proxy, are received on ports <code class="literal">5900</code> to <code class="literal">5999</code>. The firewall on each
Compute service node must allow network traffic on these ports.</p><p>This procedure modifies the iptables firewall to allow incoming
connections to the Compute services.</p><p>
          <span class="bold"><strong>Configuring the service-node firewall</strong></span>
        </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Log in to the server that hosts the Compute service, as root.</p></li><li class="step "><p>Edit the <code class="literal">/etc/sysconfig/iptables</code> file, to add an INPUT rule that
allows TCP traffic on ports from <code class="literal">5900</code> to <code class="literal">5999</code>. Make sure the new
rule appears before any INPUT rules that REJECT traffic:</p><div class="verbatim-wrap"><pre class="screen">-A INPUT -p tcp -m multiport --dports 5900:5999 -j ACCEPT</pre></div></li><li class="step "><p>Save the changes to the <code class="literal">/etc/sysconfig/iptables</code> file, and restart the
<code class="literal">iptables</code> service to pick up the changes:</p><div class="verbatim-wrap"><pre class="screen">$ service iptables restart</pre></div></li><li class="step "><p>Repeat this process for each Compute service node.</p></li></ol></div></div></div><div class="sect2 " id="id-1.4.7.8.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Injecting the administrator password</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.11">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Compute can generate a random administrator (root) password and inject
that password into an instance. If this feature is enabled, users can
run <code class="command">ssh</code> to an instance without an <code class="command">ssh</code> keypair.
The random password appears in the output of the
<code class="command">openstack server create</code> command.
You can also view and set the admin password from the dashboard.</p><p>
          <span class="bold"><strong>Password injection using the dashboard</strong></span>
        </p><p>By default, the dashboard will display the <code class="literal">admin</code> password and allow
the user to modify it.</p><p>If you do not want to support password injection, disable the password
fields by editing the dashboard's <code class="literal">local_settings.py</code> file.</p><div class="verbatim-wrap"><pre class="screen">OPENSTACK_HYPERVISOR_FEATURES = {
...
    'can_set_password': False,
}</pre></div><p>
          <span class="bold"><strong>Password injection on libvirt-based hypervisors</strong></span>
        </p><p>For hypervisors that use the libvirt back end (such as KVM, QEMU, and
LXC), admin password injection is disabled by default. To enable it, set
this option in <code class="literal">/etc/nova/nova.conf</code>:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[libvirt]
inject_password=true</pre></div><p>When enabled, Compute will modify the password of the admin account by
editing the <code class="literal">/etc/shadow</code> file inside the virtual machine instance.</p><div id="id-1.4.7.8.11.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Users can only use <code class="command">ssh</code> to access the instance by using the admin
password if the virtual machine image is a Linux distribution, and it has
been configured to allow users to use <code class="command">ssh</code> as the root user. This
is not the case for <a class="link" href="http://uec-images.ubuntu.com" target="_blank">Ubuntu cloud images</a>
which, by default, does not allow users to use <code class="command">ssh</code> to access the
root account.</p></div><p>
          <span class="bold"><strong>Password injection and XenAPI (XenServer/XCP)</strong></span>
        </p><p>When using the XenAPI hypervisor back end, Compute uses the XenAPI agent
to inject passwords into guests. The virtual machine image must be
configured with the agent for password injection to work.</p><p>
          <span class="bold"><strong>Password injection and Windows images (all hypervisors)</strong></span>
        </p><p>For Windows virtual machines, configure the Windows image to retrieve
the admin password on boot by installing an agent such as
<a class="link" href="https://cloudbase.it/cloudbase-init" target="_blank">cloudbase-init</a>.</p></div><div class="sect2 " id="id-1.4.7.8.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manage the cloud</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.12">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>System administrators can use the <code class="command">openstack</code> and
          <code class="command">euca2ools</code> commands to manage their clouds.</p><p>The <code class="literal">openstack</code> client and <code class="literal">euca2ools</code> can be used by all users, though
          specific commands might be restricted by the Identity service.</p><p>
          <span class="bold"><strong>Managing the cloud with the openstack client</strong></span>
        </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>The <code class="literal">python-openstackclient</code> package provides an <code class="literal">openstack</code> shell that
              enables Compute API interactions from the command line. Install the client,
              and provide your user name and password (which can be set as environment
              variables for convenience), for the ability to administer the cloud from
              the command line.</p><p>To install python-openstackclient, follow the instructions in the
              <a class="link" href="http://docs.openstack.org/user-guide/common/cli-install-openstack-command-line-clients.html" target="_blank">OpenStack User Guide</a>.</p></li><li class="step "><p>Confirm the installation was successful:</p><div class="verbatim-wrap"><pre class="screen">$ openstack help
              usage: openstack [--version] [-v | -q] [--log-file LOG_FILE] [-h] [--debug]
              [--os-cloud &lt;cloud-config-name&gt;]
              [--os-region-name &lt;auth-region-name&gt;]
              [--os-cacert &lt;ca-bundle-file&gt;] [--verify | --insecure]
              [--os-default-domain &lt;auth-domain&gt;]
              ...</pre></div><p>Running <code class="command">openstack help</code> returns a list of <code class="literal">openstack</code> commands
              and parameters. To get help for a subcommand, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack help SUBCOMMAND</pre></div><p>For a complete list of <code class="literal">openstack</code> commands and parameters, see the
              <a class="link" href="http://docs.openstack.org/cli-reference/openstack.html" target="_blank">OpenStack Command-Line Reference</a>.</p></li><li class="step "><p>Set the required parameters as environment variables to make running
              commands easier. For example, you can add <code class="literal">--os-username</code> as an
              <code class="literal">openstack</code> option, or set it as an environment variable. To set the user
              name, password, and project as environment variables, use:</p><div class="verbatim-wrap"><pre class="screen">$ export OS_USERNAME=joecool
              $ export OS_PASSWORD=coolword
              $ export OS_TENANT_NAME=coolu</pre></div></li><li class="step "><p>The Identity service gives you an authentication endpoint,
              which Compute recognizes as <code class="literal">OS_AUTH_URL</code>:</p><div class="verbatim-wrap"><pre class="screen">$ export OS_AUTH_URL=http://hostname:5000/v2.0</pre></div></li></ol></div></div><div class="sect3 " id="id-1.4.7.8.12.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing the cloud with euca2ools</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.12.6">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The <code class="literal">euca2ools</code> command-line tool provides a command line interface to
EC2 API calls. For more information, see the <a class="link" href="https://docs.eucalyptus.com/eucalyptus/" target="_blank">Official Eucalyptus Documentation</a>.</p></div><div class="sect3 " id="id-1.4.7.8.12.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Show usage statistics for hosts and instances</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.12.7">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You can show basic statistics on resource usage for hosts and instances.</p><div id="id-1.4.7.8.12.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>For more sophisticated monitoring, see the
<a class="link" href="https://launchpad.net/ceilometer" target="_blank">ceilometer</a> project. You can
also use tools, such as <a class="link" href="http://ganglia.info/" target="_blank">Ganglia</a> or
<a class="link" href="http://graphite.wikidot.com/" target="_blank">Graphite</a>, to gather more detailed
data.</p></div><div class="sect4 " id="id-1.4.7.8.12.7.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Show host usage statistics</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.12.7.4">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The following examples show the host usage statistics for a host called
<code class="literal">devstack</code>.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>List the hosts and the nova-related services that run on them:</p><div class="verbatim-wrap"><pre class="screen">$ openstack host list
+-----------+-------------+----------+
| Host Name | Service     | Zone     |
+-----------+-------------+----------+
| devstack  | conductor   | internal |
| devstack  | compute     | nova     |
| devstack  | cert        | internal |
| devstack  | network     | internal |
| devstack  | scheduler   | internal |
| devstack  | consoleauth | internal |
+-----------+-------------+----------+</pre></div></li><li class="listitem "><p>Get a summary of resource usage of all of the instances running on
the host:</p><div class="verbatim-wrap"><pre class="screen">$ openstack host show devstack
+----------+----------------------------------+-----+-----------+---------+
| Host     | Project                          | CPU | MEMORY MB | DISK GB |
+----------+----------------------------------+-----+-----------+---------+
| devstack | (total)                          | 2   | 4003      | 157     |
| devstack | (used_now)                       | 3   | 5120      | 40      |
| devstack | (used_max)                       | 3   | 4608      | 40      |
| devstack | b70d90d65e464582b6b2161cf3603ced | 1   | 512       | 0       |
| devstack | 66265572db174a7aa66eba661f58eb9e | 2   | 4096      | 40      |
+----------+----------------------------------+-----+-----------+---------+</pre></div><p>The <code class="literal">CPU</code> column shows the sum of the virtual CPUs for instances
running on the host.</p><p>The <code class="literal">MEMORY MB</code> column shows the sum of the memory (in MB)
allocated to the instances that run on the host.</p><p>The <code class="literal">DISK GB</code> column shows the sum of the root and ephemeral disk
sizes (in GB) of the instances that run on the host.</p><p>The row that has the value <code class="literal">used_now</code> in the <code class="literal">PROJECT</code> column
shows the sum of the resources allocated to the instances that run on
the host, plus the resources allocated to the virtual machine of the
host itself.</p><p>The row that has the value <code class="literal">used_max</code> in the <code class="literal">PROJECT</code> column
shows the sum of the resources allocated to the instances that run on
the host.</p><div id="id-1.4.7.8.12.7.4.3.2.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>These values are computed by using information about the flavors of
the instances that run on the hosts. This command does not query the
CPU usage, memory usage, or hard disk usage of the physical host.</p></div></li></ul></div></div><div class="sect4 " id="id-1.4.7.8.12.7.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.6.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Show instance usage statistics</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.12.7.5">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Get CPU, memory, I/O, and network statistics for an instance.</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>List instances:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server list
+----------+----------------------+--------+------------+-------------+------------------+------------+
| ID       | Name                 | Status | Task State | Power State | Networks         | Image Name |
+----------+----------------------+--------+------------+-------------+------------------+------------+
| 84c6e... | myCirrosServer       | ACTIVE | None       | Running     | private=10.0.0.3 | cirros     |
| 8a995... | myInstanceFromVolume | ACTIVE | None       | Running     | private=10.0.0.4 | ubuntu     |
+----------+----------------------+--------+------------+-------------+------------------+------------+</pre></div></li><li class="step "><p>Get diagnostic statistics:</p><div class="verbatim-wrap"><pre class="screen">$ nova diagnostics myCirrosServer
+---------------------------+--------+
| Property                  | Value  |
+---------------------------+--------+
| memory                    | 524288 |
| memory-actual             | 524288 |
| memory-rss                | 6444   |
| tap1fec8fb8-7a_rx         | 22137  |
| tap1fec8fb8-7a_rx_drop    | 0      |
| tap1fec8fb8-7a_rx_errors  | 0      |
| tap1fec8fb8-7a_rx_packets | 166    |
| tap1fec8fb8-7a_tx         | 18032  |
| tap1fec8fb8-7a_tx_drop    | 0      |
| tap1fec8fb8-7a_tx_errors  | 0      |
| tap1fec8fb8-7a_tx_packets | 130    |
| vda_errors                | -1     |
| vda_read                  | 2048   |
| vda_read_req              | 2      |
| vda_write                 | 182272 |
| vda_write_req             | 74     |
+---------------------------+--------+</pre></div></li></ol></div></div></li><li class="listitem "><p>Get summary statistics for each tenant:</p><div class="verbatim-wrap"><pre class="screen">$ openstack usage list
Usage from 2013-06-25 to 2013-07-24:
+---------+---------+--------------+-----------+---------------+
| Project | Servers | RAM MB-Hours | CPU Hours | Disk GB-Hours |
+---------+---------+--------------+-----------+---------------+
| demo    | 1       | 344064.44    | 672.00    | 0.00          |
| stack   | 3       | 671626.76    | 327.94    | 6558.86       |
+---------+---------+--------------+-----------+---------------+</pre></div></li></ul></div></div></div></div><div class="sect2 " id="id-1.4.7.8.13"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.13">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.8.13.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging module</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.13.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Logging behavior can be changed by creating a configuration file. To
specify the configuration file, add this line to the
<code class="literal">/etc/nova/nova.conf</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">log-config=/etc/nova/logging.conf</pre></div><p>To change the logging level, add <code class="literal">DEBUG</code>, <code class="literal">INFO</code>, <code class="literal">WARNING</code>, or
<code class="literal">ERROR</code> as a parameter.</p><p>The logging configuration file is an INI-style configuration file, which
must contain a section called <code class="literal">logger_nova</code>. This controls the
behavior of the logging facility in the <code class="literal">nova-*</code> services. For
example:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[logger_nova]
level = INFO
handlers = stderr
qualname = nova</pre></div><p>This example sets the debugging level to <code class="literal">INFO</code> (which is less verbose
than the default <code class="literal">DEBUG</code> setting).</p><p>For more about the logging configuration syntax, including the
<code class="literal">handlers</code> and <code class="literal">quaname</code> variables, see the
<a class="link" href="http://docs.python.org/release/2.7/library/logging.html#configuration-file-format" target="_blank">Python documentation</a>
on logging configuration files.</p><p>For an example of the <code class="literal">logging.conf</code> file with various defined handlers, see
the <a class="link" href="http://docs.openstack.org/newton/config-reference/" target="_blank">OpenStack Configuration Reference</a>.</p></div><div class="sect3 " id="id-1.4.7.8.13.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Syslog</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.13.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack Compute services can send logging information to syslog. This
is useful if you want to use rsyslog to forward logs to a remote
machine. Separately configure the Compute service (nova), the Identity
service (keystone), the Image service (glance), and, if you are using
it, the Block Storage service (cinder) to send log messages to syslog.
Open these configuration files:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
                <code class="literal">/etc/nova/nova.conf</code>
              </p></li><li class="listitem "><p>
                <code class="literal">/etc/keystone/keystone.conf</code>
              </p></li><li class="listitem "><p>
                <code class="literal">/etc/glance/glance-api.conf</code>
              </p></li><li class="listitem "><p>
                <code class="literal">/etc/glance/glance-registry.conf</code>
              </p></li><li class="listitem "><p>
                <code class="literal">/etc/cinder/cinder.conf</code>
              </p></li></ul></div><p>In each configuration file, add these lines:</p><div class="verbatim-wrap highlight ini"><pre class="screen">debug = False
use_syslog = True
syslog_log_facility = LOG_LOCAL0</pre></div><p>In addition to enabling syslog, these settings also turn off debugging output
from the log.</p><div id="id-1.4.7.8.13.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Although this example uses the same local facility for each service
(<code class="literal">LOG_LOCAL0</code>, which corresponds to syslog facility <code class="literal">LOCAL0</code>),
we recommend that you configure a separate local facility for each
service, as this provides better isolation and more flexibility. For
example, you can capture logging information at different severity
levels for different services. syslog allows you to define up to
eight local facilities, <code class="literal">LOCAL0, LOCAL1, ..., LOCAL7</code>. For more
information, see the syslog documentation.</p></div></div><div class="sect3 " id="id-1.4.7.8.13.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rsyslog</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.13.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>rsyslog is useful for setting up a centralized log server across
multiple machines. This section briefly describe the configuration to
set up an rsyslog server. A full treatment of rsyslog is beyond the
scope of this book. This section assumes rsyslog has already been
installed on your hosts (it is installed by default on most Linux
distributions).</p><p>This example provides a minimal configuration for <code class="literal">/etc/rsyslog.conf</code>
on the log server host, which receives the log files</p><div class="verbatim-wrap"><pre class="screen"># provides TCP syslog reception
$ModLoad imtcp
$InputTCPServerRun 1024</pre></div><p>Add a filter rule to <code class="literal">/etc/rsyslog.conf</code> which looks for a host name.
This example uses COMPUTE_01 as the compute host name:</p><div class="verbatim-wrap highlight ini"><pre class="screen">:hostname, isequal, "COMPUTE_01" /mnt/rsyslog/logs/compute-01.log</pre></div><p>On each compute host, create a file named
<code class="literal">/etc/rsyslog.d/60-nova.conf</code>, with the following content:</p><div class="verbatim-wrap"><pre class="screen"># prevent debug from dnsmasq with the daemon.none parameter
*.*;auth,authpriv.none,daemon.none,local0.none -/var/log/syslog
# Specify a log level of ERROR
local0.error    @@172.20.1.43:1024</pre></div><p>Once you have created the file, restart the <code class="literal">rsyslog</code> service. Error-level
log messages on the compute hosts should now be sent to the log server.</p></div><div class="sect3 " id="id-1.4.7.8.13.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Serial console</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.13.5">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The serial console provides a way to examine kernel output and other
system messages during troubleshooting if the instance lacks network
connectivity.</p><p>Read-only access from server serial console is possible
using the <code class="literal">os-GetSerialOutput</code> server action. Most
cloud images enable this feature by default. For more information, see
<a class="xref" href="bk02ch05.html#compute-common-errors-and-fixes" title="5.5.3. Common errors and fixes for Compute">Section 5.5.3, “Common errors and fixes for Compute”</a>.</p><p>OpenStack Juno and later supports read-write access using the serial
console using the <code class="literal">os-GetSerialConsole</code> server action. This feature
also requires a websocket client to access the serial console.</p><p>
            <span class="bold"><strong>Configuring read-write serial console access</strong></span>
          </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>On a compute node, edit the <code class="literal">/etc/nova/nova.conf</code> file:</p><p>In the <code class="literal">[serial_console]</code> section, enable the serial console:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[serial_console]
...
enabled = true</pre></div></li><li class="step "><p>In the <code class="literal">[serial_console]</code> section, configure the serial console proxy
similar to graphical console proxies:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[serial_console]
...
base_url = ws://controller:6083/
listen = 0.0.0.0
proxyclient_address = MANAGEMENT_INTERFACE_IP_ADDRESS</pre></div><p>The <code class="literal">base_url</code> option specifies the base URL that clients receive from
the API upon requesting a serial console. Typically, this refers to the
host name of the controller node.</p><p>The <code class="literal">listen</code> option specifies the network interface nova-compute
should listen on for virtual console connections. Typically, 0.0.0.0
will enable listening on all interfaces.</p><p>The <code class="literal">proxyclient_address</code> option specifies which network interface the
proxy should connect to. Typically, this refers to the IP address of the
management interface.</p><p>When you enable read-write serial console access, Compute will add
serial console information to the Libvirt XML file for the instance. For
example:</p><div class="verbatim-wrap highlight xml"><pre class="screen">&lt;console type='tcp'&gt;
  &lt;source mode='bind' host='127.0.0.1' service='10000'/&gt;
  &lt;protocol type='raw'/&gt;
  &lt;target type='serial' port='0'/&gt;
  &lt;alias name='serial0'/&gt;
&lt;/console&gt;</pre></div></li></ol></div></div><p>
            <span class="bold"><strong>Accessing the serial console on an instance</strong></span>
          </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Use the <code class="command">nova get-serial-proxy</code> command to retrieve the websocket
URL for the serial console on the instance:</p><div class="verbatim-wrap"><pre class="screen">$ nova get-serial-proxy INSTANCE_NAME</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><tbody><tr><td>
                        <p>Type</p>
                      </td><td>
                        <p>Url</p>
                      </td></tr><tr><td>
                        <p>serial</p>
                      </td><td>
                        <p>ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d</p>
                      </td></tr></tbody></table></div><p>Alternatively, use the API directly:</p><div class="verbatim-wrap"><pre class="screen">$ curl -i 'http://&lt;controller&gt;:8774/v2.1/&lt;tenant_uuid&gt;/servers/
  &lt;instance_uuid&gt;/action' \
  -X POST \
  -H "Accept: application/json" \
  -H "Content-Type: application/json" \
  -H "X-Auth-Project-Id: &lt;project_id&gt;" \
  -H "X-Auth-Token: &lt;auth_token&gt;" \
  -d '{"os-getSerialConsole": {"type": "serial"}}'</pre></div></li><li class="step "><p>Use Python websocket with the URL to generate <code class="literal">.send</code>, <code class="literal">.recv</code>, and
<code class="literal">.fileno</code> methods for serial console access. For example:</p><div class="verbatim-wrap highlight python"><pre class="screen">import websocket
ws = websocket.create_connection(
    'ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d',
    subprotocols=['binary', 'base64'])</pre></div></li></ol></div></div><p>Alternatively, use a <a class="link" href="https://github.com/larsks/novaconsole/" target="_blank">Python websocket client</a>.</p><div id="id-1.4.7.8.13.5.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>When you enable the serial console, typical instance logging using
the <code class="command">nova console-log</code> command is disabled. Kernel output
and other system messages will not be visible unless you are
actively viewing the serial console.</p></div></div></div><div class="sect2 " id="id-1.4.7.8.14"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Secure with rootwrap</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.14">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Rootwrap allows unprivileged users to safely run Compute actions as the
root user. Compute previously used <code class="command">sudo</code> for this purpose, but this
was difficult to maintain, and did not allow advanced filters. The
<code class="command">rootwrap</code> command replaces <code class="command">sudo</code> for Compute.</p><p>To use rootwrap, prefix the Compute command with <code class="command">nova-rootwrap</code>. For
example:</p><div class="verbatim-wrap"><pre class="screen">$ sudo nova-rootwrap /etc/nova/rootwrap.conf command</pre></div><p>A generic <code class="literal">sudoers</code> entry lets the Compute user run <code class="command">nova-rootwrap</code>
as root. The <code class="command">nova-rootwrap</code> code looks for filter definition
directories in its configuration file, and loads command filters from
them. It then checks if the command requested by Compute matches one of
those filters and, if so, executes the command (as root). If no filter
matches, it denies the request.</p><div id="id-1.4.7.8.14.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Be aware of issues with using NFS and root-owned files. The NFS
share must be configured with the <code class="literal">no_root_squash</code> option enabled,
in order for rootwrap to work correctly.</p></div><p>Rootwrap is fully controlled by the root user. The root user
owns the sudoers entry which allows Compute to run a specific
rootwrap executable as root, and only with a specific
configuration file (which should also be owned by root).
The <code class="command">nova-rootwrap</code> command imports the Python
modules it needs from a cleaned, system-default PYTHONPATH.
The root-owned configuration file points to root-owned
filter definition directories, which contain root-owned
filters definition files. This chain ensures that the Compute
user itself is not in control of the configuration or modules
used by the <code class="command">nova-rootwrap</code> executable.</p><div class="sect3 " id="id-1.4.7.8.14.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure rootwrap</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.14.8">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Configure rootwrap in the <code class="literal">rootwrap.conf</code> file. Because
it is in the trusted security path, it must be owned and writable
by only the root user. The <code class="literal">rootwrap_config=entry</code> parameter
specifies the file's location in the sudoers entry and in the
<code class="literal">nova.conf</code> configuration file.</p><p>The <code class="literal">rootwrap.conf</code> file uses an INI file format with these
sections and parameters:</p><div class="table" id="id-1.4.7.8.14.8.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.4: </span><span class="name">rootwrap.conf configuration options </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.14.8.4">#</a></h6></div><div class="table-contents"><table class="table" summary="rootwrap.conf configuration options" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><tbody><tr><td>
                    <p>Configuration option=Default value</p>
                  </td><td>
                    <p>(Type) Description</p>
                  </td></tr><tr><td>
                    <p>[DEFAULT]
filters_path=/etc/nova/rootwrap.d,/usr/share/nova/rootwrap</p>
                  </td><td>
                    <p>(ListOpt) Comma-separated list of directories
containing filter definition files.
Defines where rootwrap filters are stored.
Directories defined on this line should all
exist, and be owned and writable only by the
root user.</p>
                  </td></tr></tbody></table></div></div><p>If the root wrapper is not performing correctly, you can add a
workaround option into the <code class="literal">nova.conf</code> configuration file. This
workaround re-configures the root wrapper configuration to fall back to
running commands as <code class="literal">sudo</code>, and is a Kilo release feature.</p><p>Including this workaround in your configuration file safeguards your
environment from issues that can impair root wrapper performance. Tool
changes that have impacted
<a class="link" href="https://git.openstack.org/cgit/openstack-dev/pbr/" target="_blank">Python Build Reasonableness (PBR)</a>
for example, are a known issue that affects root wrapper performance.</p><p>To set up this workaround, configure the <code class="literal">disable_rootwrap</code> option in
the <code class="literal">[workaround]</code> section of the <code class="literal">nova.conf</code> configuration file.</p><p>The filters definition files contain lists of filters that rootwrap will
use to allow or deny a specific command. They are generally suffixed by
<code class="literal">.filters</code>. Since they are in the trusted security path, they need to
be owned and writable only by the root user. Their location is specified
in the <code class="literal">rootwrap.conf</code> file.</p><p>Filter definition files use an INI file format with a <code class="literal">[Filters]</code>
section and several lines, each with a unique parameter name, which
should be different for each filter you define:</p><div class="table" id="id-1.4.7.8.14.8.10"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.5: </span><span class="name">Filters configuration options </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.14.8.10">#</a></h6></div><div class="table-contents"><table class="table" summary="Filters configuration options" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><tbody><tr><td>
                    <p>Configuration option=Default value</p>
                  </td><td>
                    <p>(Type) Description</p>
                  </td></tr><tr><td>
                    <p>[Filters]
filter_name=kpartx: CommandFilter, /sbin/kpartx, root</p>
                  </td><td>
                    <p>(ListOpt) Comma-separated list containing the filter class to
use, followed by the Filter arguments (which vary depending
on the Filter class selected).</p>
                  </td></tr></tbody></table></div></div></div><div class="sect3 " id="id-1.4.7.8.14.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the rootwrap daemon</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.14.9">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Administrators can use rootwrap daemon support instead of running
rootwrap with <code class="command">sudo</code>. The rootwrap daemon reduces the
overhead and performance loss that results from running
<code class="literal">oslo.rootwrap</code> with <code class="command">sudo</code>. Each call that needs rootwrap
privileges requires a new instance of rootwrap. The daemon
prevents overhead from the repeated calls. The daemon does not support
long running processes, however.</p><p>To enable the rootwrap daemon, set <code class="literal">use_rootwrap_daemon</code> to <code class="literal">True</code>
in the Compute service configuration file.</p></div></div><div class="sect2 " id="section-configuring-compute-migrations"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure migrations</span> <a title="Permalink" class="permalink" href="bk02ch05.html#section-configuring-compute-migrations">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span>section-configuring-compute-migrations</li></ul></div></div></div></div><div id="id-1.4.7.8.15.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Only administrators can perform live migrations. If your cloud
is configured to use cells, you can perform live migration within
but not between cells.</p></div><p>Migration enables an administrator to move a virtual-machine instance
from one compute host to another. This feature is useful when a compute
host requires maintenance. Migration can also be useful to redistribute
the load when many VM instances are running on a specific physical
machine.</p><p>The migration types are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="bold"><strong>Non-live migration</strong></span> (sometimes referred to simply as 'migration').
The instance is shut down for a period of time to be moved to another
hypervisor. In this case, the instance recognizes that it was
rebooted.</p></li><li class="listitem "><p><span class="bold"><strong>Live migration</strong></span> (or 'true live migration'). Almost no instance
downtime. Useful when the instances must be kept running during the
migration. The different types of live migration are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="bold"><strong>Shared storage-based live migration</strong></span>. Both hypervisors have
access to shared storage.</p></li><li class="listitem "><p><span class="bold"><strong>Block live migration</strong></span>. No shared storage is required.
Incompatible with read-only devices such as CD-ROMs and
<a class="link" href="http://docs.openstack.org/user-guide/cli-config-drive.html" target="_blank">Configuration Drive (config_drive)</a>.</p></li><li class="listitem "><p><span class="bold"><strong>Volume-backed live migration</strong></span>. Instances are backed by volumes
rather than ephemeral disk, no shared storage is required, and
migration is supported (currently only available for libvirt-based
hypervisors).</p></li></ul></div></li></ul></div><p>The following sections describe how to configure your hosts and compute
nodes for migrations by using the KVM and XenServer hypervisors.</p><div class="sect3 " id="id-1.4.7.8.15.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM-Libvirt</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.15.7">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect4 " id="configuring-migrations-kvm-shared-storage"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Shared storage</span> <a title="Permalink" class="permalink" href="bk02ch05.html#configuring-migrations-kvm-shared-storage">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span>configuring-migrations-kvm-shared-storage</li></ul></div></div></div></div><p>
              <span class="bold"><strong>Prerequisites</strong></span>
            </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="bold"><strong>Hypervisor:</strong></span> KVM with libvirt</p></li><li class="listitem "><p><span class="bold"><strong>Shared storage:</strong></span><code class="literal">NOVA-INST-DIR/instances/</code> (for example,
<code class="literal">/var/lib/nova/instances</code>) has to be mounted by shared storage.
This guide uses NFS but other options, including the
<a class="link" href="http://gluster.org/community/documentation//index.php/OSConnect" target="_blank">OpenStack Gluster Connector</a>
are available.</p></li><li class="listitem "><p><span class="bold"><strong>Instances:</strong></span> Instance can be migrated with iSCSI-based volumes.</p></li></ul></div><p>
              <span class="bold"><strong>Notes</strong></span>
            </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Because the Compute service does not use the libvirt live
migration functionality by default, guests are suspended before
migration and might experience several minutes of downtime. For
details, see <code class="literal">Enabling true live migration</code>.</p></li><li class="listitem "><p>Compute calculates the amount of downtime required using the RAM size of
the disk being migrated, in accordance with the <code class="literal">live_migration_downtime</code>
configuration parameters. Migration downtime is measured in steps, with an
exponential backoff between each step. This means that the maximum
downtime between each step starts off small, and is increased in ever
larger amounts as Compute waits for the migration to complete. This gives
the guest a chance to complete the migration successfully, with a minimum
amount of downtime.</p></li><li class="listitem "><p>This guide assumes the default value for <code class="literal">instances_path</code> in
your <code class="literal">nova.conf</code> file (<code class="literal">NOVA-INST-DIR/instances</code>). If you
have changed the <code class="literal">state_path</code> or <code class="literal">instances_path</code> variables,
modify the commands accordingly.</p></li><li class="listitem "><p>You must specify <code class="literal">vncserver_listen=0.0.0.0</code> or live migration
will not work correctly.</p></li><li class="listitem "><p>You must specify the <code class="literal">instances_path</code> in each node that runs
<code class="literal">nova-compute</code>. The mount point for <code class="literal">instances_path</code> must be the
same value for each node, or live migration will not work
correctly.</p></li></ul></div></div><div class="sect4 " id="id-1.4.7.8.15.7.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.9.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Compute installation environment</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.15.7.3">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Prepare at least three servers. In this example, we refer to the
servers as <code class="literal">HostA</code>, <code class="literal">HostB</code>, and <code class="literal">HostC</code>:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">HostA</code> is the Cloud Controller, and should run these services:
<code class="literal">nova-api</code>, <code class="literal">nova-scheduler</code>, <code class="literal">nova-network</code>, <code class="literal">cinder-volume</code>,
and <code class="literal">nova-objectstore</code>.</p></li><li class="listitem "><p><code class="literal">HostB</code> and <code class="literal">HostC</code> are the compute nodes that run
<code class="literal">nova-compute</code>.</p></li></ul></div><p>Ensure that <code class="literal">NOVA-INST-DIR</code> (set with <code class="literal">state_path</code> in the
<code class="literal">nova.conf</code> file) is the same on all hosts.</p></li><li class="listitem "><p>In this example, <code class="literal">HostA</code> is the NFSv4 server that exports
<code class="literal">NOVA-INST-DIR/instances</code> directory. <code class="literal">HostB</code> and <code class="literal">HostC</code> are
NFSv4 clients that mount <code class="literal">HostA</code>.</p></li></ul></div><p>
              <span class="bold"><strong>Configuring your system</strong></span>
            </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Configure your DNS or <code class="literal">/etc/hosts</code> and ensure it is consistent across
all hosts. Make sure that the three hosts can perform name resolution
with each other. As a test, use the <code class="command">ping</code> command to ping each host
from one another:</p><div class="verbatim-wrap"><pre class="screen">$ ping HostA
$ ping HostB
$ ping HostC</pre></div></li><li class="step "><p>Ensure that the UID and GID of your Compute and libvirt users are
identical between each of your servers. This ensures that the
permissions on the NFS mount works correctly.</p></li><li class="step "><p>Ensure you can access SSH without a password and without
StrictHostKeyChecking between <code class="literal">HostB</code> and <code class="literal">HostC</code> as <code class="literal">nova</code>
user (set with the owner of <code class="literal">nova-compute</code> service). Direct access
from one compute host to another is needed to copy the VM file
across. It is also needed to detect if the source and target
compute nodes share a storage subsystem.</p></li><li class="step "><p>Export <code class="literal">NOVA-INST-DIR/instances</code> from <code class="literal">HostA</code>, and ensure it is
readable and writable by the Compute user on <code class="literal">HostB</code> and <code class="literal">HostC</code>.</p><p>For more information, see: <a class="link" href="https://help.ubuntu.com/community/SettingUpNFSHowTo" target="_blank">SettingUpNFSHowTo</a>
or <a class="link" href="http://www.cyberciti.biz/faq/centos-fedora-rhel-nfs-v4-configuration/" target="_blank">CentOS/Red Hat: Setup NFS v4.0 File Server</a></p></li><li class="step "><p>Configure the NFS server at <code class="literal">HostA</code> by adding the following line to
the <code class="literal">/etc/exports</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">NOVA-INST-DIR/instances HostA/255.255.0.0(rw,sync,fsid=0,no_root_squash)</pre></div><p>Change the subnet mask (<code class="literal">255.255.0.0</code>) to the appropriate value to
include the IP addresses of <code class="literal">HostB</code> and <code class="literal">HostC</code>. Then restart the
<code class="literal">NFS</code> server:</p><div class="verbatim-wrap"><pre class="screen"># /etc/init.d/nfs-kernel-server restart
# /etc/init.d/idmapd restart</pre></div></li><li class="step "><p>On both compute nodes, enable the <code class="literal">execute/search</code> bit on your shared
directory to allow qemu to be able to use the images within the
directories. On all hosts, run the following command:</p><div class="verbatim-wrap"><pre class="screen">$ chmod o+x NOVA-INST-DIR/instances</pre></div></li><li class="step "><p>Configure NFS on <code class="literal">HostB</code> and <code class="literal">HostC</code> by adding the following line to
the <code class="literal">/etc/fstab</code> file</p><div class="verbatim-wrap"><pre class="screen">HostA:/ /NOVA-INST-DIR/instances nfs4 defaults 0 0</pre></div><p>Ensure that you can mount the exported directory</p><div class="verbatim-wrap"><pre class="screen">$ mount -a -v</pre></div><p>Check that <code class="literal">HostA</code> can see the <code class="literal">NOVA-INST-DIR/instances/</code>
directory</p><div class="verbatim-wrap"><pre class="screen">$ ls -ld NOVA-INST-DIR/instances/
drwxr-xr-x 2 nova nova 4096 2012-05-19 14:34 nova-install-dir/instances/</pre></div><p>Perform the same check on <code class="literal">HostB</code> and <code class="literal">HostC</code>, paying special
attention to the permissions (Compute should be able to write)</p><div class="verbatim-wrap"><pre class="screen">$ ls -ld NOVA-INST-DIR/instances/
drwxr-xr-x 2 nova nova 4096 2012-05-07 14:34 nova-install-dir/instances/

$ df -k
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/sda1            921514972   4180880 870523828   1% /
none                  16498340      1228  16497112   1% /dev
none                  16502856         0  16502856   0% /dev/shm
none                  16502856       368  16502488   1% /var/run
none                  16502856         0  16502856   0% /var/lock
none                  16502856         0  16502856   0% /lib/init/rw
HostA:               921515008 101921792 772783104  12% /var/lib/nova/instances  ( &lt;--- this line is important.)</pre></div></li><li class="step "><p>Update the libvirt configurations so that the calls can be made
securely. These methods enable remote access over TCP and are not
documented here.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SSH tunnel to libvirtd's UNIX socket</p></li><li class="listitem "><p>libvirtd TCP socket, with GSSAPI/Kerberos for auth+data encryption</p></li><li class="listitem "><p>libvirtd TCP socket, with TLS for encryption and x509 client certs
for authentication</p></li><li class="listitem "><p>libvirtd TCP socket, with TLS for encryption and Kerberos for
authentication</p></li></ul></div><p>Restart <code class="literal">libvirt</code>. After you run the command, ensure that libvirt is
successfully restarted</p><div class="verbatim-wrap"><pre class="screen"># stop libvirt-bin &amp;&amp; start libvirt-bin
$ ps -ef | grep libvirt
root 1145 1 0 Nov27 ? 00:00:03 /usr/sbin/libvirtd -d -l\</pre></div></li><li class="step "><p>Configure your firewall to allow libvirt to communicate between nodes.
By default, libvirt listens on TCP port 16509, and an ephemeral TCP
range from 49152 to 49261 is used for the KVM communications. Based on
the secure remote access TCP configuration you chose, be careful which
ports you open, and always understand who has access. For information
about ports that are used with libvirt,
see the <a class="link" href="http://libvirt.org/remote.html#Remote_libvirtd_configuration" target="_blank">libvirt documentation</a>.</p></li><li class="step "><p>Configure the downtime required for the migration by adjusting these
parameters in the <code class="literal">nova.conf</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">live_migration_downtime = 500
live_migration_downtime_steps = 10
live_migration_downtime_delay = 75</pre></div><p>The <code class="literal">live_migration_downtime</code> parameter sets the maximum permitted
downtime for a live migration, in milliseconds. This setting defaults to
500 milliseconds.</p><p>The <code class="literal">live_migration_downtime_steps</code> parameter sets the total number of
incremental steps to reach the maximum downtime value. This setting
defaults to 10 steps.</p><p>The <code class="literal">live_migration_downtime_delay</code> parameter sets the amount of time
to wait between each step, in seconds. This setting defaults to 75 seconds.</p></li><li class="step "><p>You can now configure other options for live migration. In most cases, you
will not need to configure any options. For advanced configuration options,
see the <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/list-of-compute-config-options.html#config_table_nova_livemigration" target="_blank">OpenStack Configuration Reference Guide</a>.</p></li></ol></div></div></div><div class="sect4 " id="id-1.4.7.8.15.7.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.9.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling true live migration</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.15.7.4">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Prior to the Kilo release, the Compute service did not use the libvirt
live migration function by default. To enable this function, add the
following line to the <code class="literal">[libvirt]</code> section of the <code class="literal">nova.conf</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">live_migration_flag=VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_TUNNELLED</pre></div><p>On versions older than Kilo, the Compute service does not use libvirt's
live migration by default because there is a risk that the migration
process will never end. This can happen if the guest operating system
uses blocks on the disk faster than they can be migrated.</p></div><div class="sect4 " id="id-1.4.7.8.15.7.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.9.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block migration</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.15.7.5">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Configuring KVM for block migration is exactly the same as the above
configuration in <a class="xref" href="bk02ch05.html#configuring-migrations-kvm-shared-storage" title="5.4.9.1.1. Shared storage">Section 5.4.9.1.1, “Shared storage”</a>
the section called shared storage, except that <code class="literal">NOVA-INST-DIR/instances</code>
is local to each host rather than shared. No NFS client or server
configuration is required.</p><div id="id-1.4.7.8.15.7.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>To use block migration, you must use the <code class="literal">--block-migrate</code>
parameter with the live migration command.</p></li><li class="listitem "><p>Block migration is incompatible with read-only devices such as
CD-ROMs and <a class="link" href="http://docs.openstack.org/user-guide/cli-config-drive.html" target="_blank">Configuration Drive (config_drive)</a>.</p></li><li class="listitem "><p>Since the ephemeral drives are copied over the network in block
migration, migrations of instances with heavy I/O loads may never
complete if the drives are writing faster than the data can be
copied over the network.</p></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.8.15.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">XenServer</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.15.8">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect4 " id="id-1.4.7.8.15.8.2"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Shared storage</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.15.8.2">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
              <span class="bold"><strong>Prerequisites</strong></span>
            </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="bold"><strong>Compatible XenServer hypervisors</strong></span>. For more information, see the
<a class="link" href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#pooling_homogeneity_requirements" target="_blank">Requirements for Creating Resource Pools</a> section of the XenServer
Administrator's Guide.</p></li><li class="listitem "><p><span class="bold"><strong>Shared storage</strong></span>. An NFS export, visible to all XenServer hosts.</p><div id="id-1.4.7.8.15.8.2.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>For the supported NFS versions, see the
<a class="link" href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#id1002701" target="_blank">NFS VHD</a>
section of the XenServer Administrator's Guide.</p></div></li></ul></div><p>To use shared storage live migration with XenServer hypervisors, the
hosts must be joined to a XenServer pool. To create that pool, a host
aggregate must be created with specific metadata. This metadata is used
by the XAPI plug-ins to establish the pool.</p><p>
              <span class="bold"><strong>Using shared storage live migrations with XenServer Hypervisors</strong></span>
            </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Add an NFS VHD storage to your master XenServer, and set it as the
default storage repository. For more information, see NFS VHD in the
XenServer Administrator's Guide.</p></li><li class="step "><p>Configure all compute nodes to use the default storage repository
(<code class="literal">sr</code>) for pool operations. Add this line to your <code class="literal">nova.conf</code>
configuration files on all compute nodes:</p><div class="verbatim-wrap highlight ini"><pre class="screen">sr_matching_filter=default-sr:true</pre></div></li><li class="step "><p>Create a host aggregate. This command creates the aggregate, and then
displays a table that contains the ID of the new aggregate</p><div class="verbatim-wrap"><pre class="screen">$ openstack aggregate create --zone AVAILABILITY_ZONE POOL_NAME</pre></div><p>Add metadata to the aggregate, to mark it as a hypervisor pool</p><div class="verbatim-wrap"><pre class="screen">$ openstack aggregate set --property hypervisor_pool=true AGGREGATE_ID

$ openstack aggregate set --property operational_state=created AGGREGATE_ID</pre></div><p>Make the first compute node part of that aggregate</p><div class="verbatim-wrap"><pre class="screen">$ openstack aggregate add host AGGREGATE_ID MASTER_COMPUTE_NAME</pre></div><p>The host is now part of a XenServer pool.</p></li><li class="step "><p>Add hosts to the pool</p><div class="verbatim-wrap"><pre class="screen">$ openstack aggregate add host AGGREGATE_ID COMPUTE_HOST_NAME</pre></div><div id="id-1.4.7.8.15.8.2.6.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The added compute node and the host will shut down to join the host
to the XenServer pool. The operation will fail if any server other
than the compute node is running or suspended on the host.</p></div></li></ol></div></div></div><div class="sect4 " id="id-1.4.7.8.15.8.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.9.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block migration</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.15.8.3">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="bold"><strong>Compatible XenServer hypervisors</strong></span>.
The hypervisors must support the Storage XenMotion feature.
See your XenServer manual to make sure your edition
has this feature.</p><div id="id-1.4.7.8.15.8.3.2.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>To use block migration, you must use the <code class="literal">--block-migrate</code>
parameter with the live migration command.</p></li><li class="listitem "><p>Block migration works only with EXT local storage storage
repositories, and the server must not have any volumes attached.</p></li></ul></div></div></li></ul></div></div></div></div><div class="sect2 " id="id-1.4.7.8.16"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrate instances</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.16">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>This section discusses how to migrate running instances from one
OpenStack Compute server to another OpenStack Compute server.</p><p>Before starting a migration, review the Configure migrations section.
<a class="xref" href="bk02ch05.html#section-configuring-compute-migrations" title="5.4.9. Configure migrations">Section 5.4.9, “Configure migrations”</a>.</p><div id="id-1.4.7.8.16.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Although the <code class="command">nova</code> command is called <code class="command">live-migration</code>,
under the default Compute configuration options, the instances
are suspended before migration. For more information, see
<a class="link" href="http://docs.openstack.org/newton/config-reference/compute/config-options.html" target="_blank">Configure migrations</a>.
in the OpenStack Configuration Reference.</p></div><p>
          <span class="bold"><strong>Migrating instances</strong></span>
        </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Check the ID of the instance to be migrated:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server list</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>
                      <p>ID</p>
                    </th><th>
                      <p>Name</p>
                    </th><th>
                      <p>Status</p>
                    </th><th>
                      <p>Networks</p>
                    </th></tr></thead><tbody><tr><td>
                      <p>d1df1b5a-70c4-4fed-98b7-423362f2c47c</p>
                    </td><td>
                      <p>vm1</p>
                    </td><td>
                      <p>ACTIVE</p>
                    </td><td>
                      <p>private=a.b.c.d</p>
                    </td></tr><tr><td>
                      <p>d693db9e-a7cf-45ef-a7c9-b3ecb5f22645</p>
                    </td><td>
                      <p>vm2</p>
                    </td><td>
                      <p>ACTIVE</p>
                    </td><td>
                      <p>private=e.f.g.h</p>
                    </td></tr></tbody></table></div></li><li class="step "><p>Check the information associated with the instance. In this example,
<code class="literal">vm1</code> is running on <code class="literal">HostB</code>:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server show d1df1b5a-70c4-4fed-98b7-423362f2c47c</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                      <p>Property</p>
                    </th><th>
                      <p>Value</p>
                    </th></tr></thead><tbody><tr><td>
                      <p>...</p>
                      <p>OS-EXT-SRV-ATTR:host</p>
                      <p>...</p>
                      <p>flavor</p>
                      <p>id</p>
                      <p>name</p>
                      <p>private network</p>
                      <p>status</p>
                      <p>...</p>
                    </td><td>
                      <p>...</p>
                      <p>HostB</p>
                      <p>...</p>
                      <p>m1.tiny</p>
                      <p>d1df1b5a-70c4-4fed-98b7-423362f2c47c</p>
                      <p>vm1</p>
                      <p>a.b.c.d</p>
                      <p>ACTIVE</p>
                      <p>...</p>
                    </td></tr></tbody></table></div></li><li class="step "><p>Select the compute node the instance will be migrated to. In this
example, we will migrate the instance to <code class="literal">HostC</code>, because
<code class="literal">nova-compute</code> is running on it:</p><div class="table" id="id-1.4.7.8.16.6.3.2"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.6: </span><span class="name">openstack compute service list </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.16.6.3.2">#</a></h6></div><div class="table-contents"><table class="table" summary="openstack compute service list" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>
                      <p>Binary</p>
                    </th><th>
                      <p>Host</p>
                    </th><th>
                      <p>Zone</p>
                    </th><th>
                      <p>Status</p>
                    </th><th>
                      <p>State</p>
                    </th><th>
                      <p>Updated_at</p>
                    </th></tr></thead><tbody><tr><td>
                      <p>nova-consoleauth</p>
                    </td><td>
                      <p>HostA</p>
                    </td><td>
                      <p>internal</p>
                    </td><td>
                      <p>enabled</p>
                    </td><td>
                      <p>up</p>
                    </td><td>
                      <p>2014-03-25T10:33:25.000000</p>
                    </td></tr><tr><td>
                      <p>nova-scheduler</p>
                    </td><td>
                      <p>HostA</p>
                    </td><td>
                      <p>internal</p>
                    </td><td>
                      <p>enabled</p>
                    </td><td>
                      <p>up</p>
                    </td><td>
                      <p>2014-03-25T10:33:25.000000</p>
                    </td></tr><tr><td>
                      <p>nova-conductor</p>
                    </td><td>
                      <p>HostA</p>
                    </td><td>
                      <p>internal</p>
                    </td><td>
                      <p>enabled</p>
                    </td><td>
                      <p>up</p>
                    </td><td>
                      <p>2014-03-25T10:33:27.000000</p>
                    </td></tr><tr><td>
                      <p>nova-compute</p>
                    </td><td>
                      <p>HostB</p>
                    </td><td>
                      <p>nova</p>
                    </td><td>
                      <p>enabled</p>
                    </td><td>
                      <p>up</p>
                    </td><td>
                      <p>2014-03-25T10:33:31.000000</p>
                    </td></tr><tr><td>
                      <p>nova-compute</p>
                    </td><td>
                      <p>HostC</p>
                    </td><td>
                      <p>nova</p>
                    </td><td>
                      <p>enabled</p>
                    </td><td>
                      <p>up</p>
                    </td><td>
                      <p>2014-03-25T10:33:31.000000</p>
                    </td></tr><tr><td>
                      <p>nova-cert</p>
                    </td><td>
                      <p>HostA</p>
                    </td><td>
                      <p>internal</p>
                    </td><td>
                      <p>enabled</p>
                    </td><td>
                      <p>up</p>
                    </td><td>
                      <p>2014-03-25T10:33:31.000000</p>
                    </td></tr></tbody></table></div></div></li><li class="step "><p>Check that <code class="literal">HostC</code> has enough resources for migration:</p><div class="verbatim-wrap"><pre class="screen"># openstack host show HostC</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>
                      <p>HOST</p>
                    </th><th>
                      <p>PROJECT</p>
                    </th><th>
                      <p>cpu</p>
                    </th><th>
                      <p>memory_mb</p>
                    </th><th>
                      <p>disk_gb</p>
                    </th></tr></thead><tbody><tr><td>
                      <p>HostC</p>
                    </td><td>
                      <p>(total)</p>
                    </td><td>
                      <p>16</p>
                    </td><td>
                      <p>32232</p>
                    </td><td>
                      <p>878</p>
                    </td></tr><tr><td>
                      <p>HostC</p>
                    </td><td>
                      <p>(used_now)</p>
                    </td><td>
                      <p>22</p>
                    </td><td>
                      <p>21284</p>
                    </td><td>
                      <p>442</p>
                    </td></tr><tr><td>
                      <p>HostC</p>
                    </td><td>
                      <p>(used_max)</p>
                    </td><td>
                      <p>22</p>
                    </td><td>
                      <p>21284</p>
                    </td><td>
                      <p>422</p>
                    </td></tr><tr><td>
                      <p>HostC</p>
                    </td><td>
                      <p>p1</p>
                    </td><td>
                      <p>22</p>
                    </td><td>
                      <p>21284</p>
                    </td><td>
                      <p>422</p>
                    </td></tr><tr><td>
                      <p>HostC</p>
                    </td><td>
                      <p>p2</p>
                    </td><td>
                      <p>22</p>
                    </td><td>
                      <p>21284</p>
                    </td><td>
                      <p>422</p>
                    </td></tr></tbody></table></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">cpu</code>: Number of CPUs</p></li><li class="listitem "><p><code class="literal">memory_mb</code>: Total amount of memory, in MB</p></li><li class="listitem "><p><code class="literal">disk_gb</code>: Total amount of space for NOVA-INST-DIR/instances, in GB</p></li></ul></div><p>In this table, the first row shows the total amount of resources
available on the physical server. The second line shows the currently
used resources. The third line shows the maximum used resources. The
fourth line and below shows the resources available for each project.</p></li><li class="step "><p>Migrate the instance using the <code class="command">openstack server migrate</code> command:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server migrate SERVER --live HOST_NAME</pre></div><p>In this example, SERVER can be the ID or name of the instance. Another
example:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server migrate d1df1b5a-70c4-4fed-98b7-423362f2c47c --live HostC
Migration of d1df1b5a-70c4-4fed-98b7-423362f2c47c initiated.</pre></div><div id="id-1.4.7.8.16.6.5.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>When using live migration to move workloads between
Icehouse and Juno compute nodes, it may cause data loss
because libvirt live migration with shared block storage
was buggy (potential loss of data) before version 3.32.
This issue can be solved when we upgrade to RPC API version 4.0.</p></div></li><li class="step "><p>Check that the instance has been migrated successfully, using
<code class="command">openstack server list</code>. If the instance is still running on
<code class="literal">HostB</code>, check the log files at <code class="literal">src/dest</code> for <code class="literal">nova-compute</code> and
<code class="literal">nova-scheduler</code> to determine why.</p></li></ol></div></div></div><div class="sect2 " id="id-1.4.7.8.17"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure remote console access</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To provide a remote console or remote desktop access to guest virtual
machines, use VNC or SPICE HTML5 through either the OpenStack dashboard
or the command line. Best practice is to select one or the other to run.</p><div class="sect3 " id="id-1.4.7.8.17.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">About nova-consoleauth</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Both client proxies leverage a shared service to manage token
authentication called <code class="literal">nova-consoleauth</code>. This service must be running for
either proxy to work. Many proxies of either type can be run against a
single <code class="literal">nova-consoleauth</code> service in a cluster configuration.</p><p>Do not confuse the <code class="literal">nova-consoleauth</code> shared service with
<code class="literal">nova-console</code>, which is a XenAPI-specific service that most recent
VNC proxy architectures do not use.</p></div><div class="sect3 " id="id-1.4.7.8.17.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SPICE console</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack Compute supports VNC consoles to guests. The VNC protocol is
fairly limited, lacking support for multiple monitors, bi-directional
audio, reliable cut-and-paste, video streaming and more. SPICE is a new
protocol that aims to address the limitations in VNC and provide good
remote desktop support.</p><p>SPICE support in OpenStack Compute shares a similar architecture to the
VNC implementation. The OpenStack dashboard uses a SPICE-HTML5 widget in
its console tab that communicates to the <code class="literal">nova-spicehtml5proxy</code> service by
using SPICE-over-websockets. The <code class="literal">nova-spicehtml5proxy</code> service
communicates directly with the hypervisor process by using SPICE.</p><p>VNC must be explicitly disabled to get access to the SPICE console. Set
the <code class="literal">vnc_enabled</code> option to <code class="literal">False</code> in the <code class="literal">[DEFAULT]</code> section to
disable the VNC console.</p><p>Use the following options to configure SPICE as the console for
OpenStack Compute:</p><div class="table" id="id-1.4.7.8.17.4.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.7: </span><span class="name">Description of SPICE configuration options </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.4.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Description of SPICE configuration options" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                    <p>
                      <span class="bold"><strong>[spice]</strong></span>
                    </p>
                  </th><th> </th></tr></thead><tbody><tr><td>
                    <p>Spice configuration option = Default value</p>
                  </td><td>
                    <p>Description</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">agent_enabled = True</code>
                    </p>
                  </td><td>
                    <p>(BoolOpt) Enable spice guest agent support</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">enabled = False</code>
                    </p>
                  </td><td>
                    <p>(BoolOpt) Enable spice related features</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">html5proxy_base_url = http://127.0.0.1:6082/spice_auto.html</code>
                    </p>
                  </td><td>
                    <p>(StrOpt) Location of spice HTML5 console proxy, in the form
"<a class="link" href="http://127.0.0.1:6082/spice_auto.html" target="_blank">http://127.0.0.1:6082/spice_auto.html</a>"</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">html5proxy_host = 0.0.0.0</code>
                    </p>
                  </td><td>
                    <p>(StrOpt) Host on which to listen for incoming requests</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">html5proxy_port = 6082</code>
                    </p>
                  </td><td>
                    <p>(IntOpt) Port on which to listen for incoming requests</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">keymap = en-us</code>
                    </p>
                  </td><td>
                    <p>(StrOpt) Keymap for spice</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">server_listen = 127.0.0.1</code>
                    </p>
                  </td><td>
                    <p>(StrOpt) IP address on which instance spice server should listen</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">server_proxyclient_address = 127.0.0.1</code>
                    </p>
                  </td><td>
                    <p>(StrOpt) The address to which proxy clients (like nova-spicehtml5proxy)
should connect</p>
                  </td></tr></tbody></table></div></div></div><div class="sect3 " id="id-1.4.7.8.17.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VNC console proxy</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.5">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The VNC proxy is an OpenStack component that enables compute service
users to access their instances through VNC clients.</p><div id="id-1.4.7.8.17.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The web proxy console URLs do not support the websocket protocol
scheme (ws://) on python versions less than 2.7.4.</p></div><p>The VNC console connection works as follows:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>A user connects to the API and gets an <code class="literal">access_url</code> such as,
<code class="literal">http://ip:port/?token=xyz</code>.</p></li><li class="step "><p>The user pastes the URL in a browser or uses it as a client
parameter.</p></li><li class="step "><p>The browser or client connects to the proxy.</p></li><li class="step "><p>The proxy talks to <code class="literal">nova-consoleauth</code> to authorize the token for the
user, and maps the token to the <span class="emphasis"><em>private</em></span> host and port of the VNC
server for an instance.</p><p>The compute host specifies the address that the proxy should use to
connect through the <code class="literal">nova.conf</code> file option,
<code class="literal">vncserver_proxyclient_address</code>. In this way, the VNC proxy works
as a bridge between the public network and private host network.</p></li><li class="step "><p>The proxy initiates the connection to VNC server and continues to
proxy until the session ends.</p></li></ol></div></div><p>The proxy also tunnels the VNC protocol over WebSockets so that the
<code class="literal">noVNC</code> client can talk to VNC servers. In general, the VNC proxy:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Bridges between the public network where the clients live and the
private network where VNC servers live.</p></li><li class="listitem "><p>Mediates token authentication.</p></li><li class="listitem "><p>Transparently deals with hypervisor-specific connection details to
provide a uniform client experience.</p></li></ul></div><div class="figure" id="id-1.4.7.8.17.5.8"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SCH_5009_V00_NUAC-VNC_OpenStack.png" target="_blank"><img src="images/SCH_5009_V00_NUAC-VNC_OpenStack.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.7: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.5.8">#</a></h6></div></div><div class="sect4 " id="id-1.4.7.8.17.5.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.11.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VNC configuration options</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.5.9">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To customize the VNC console, use the following configuration options in
your <code class="literal">nova.conf</code> file:</p><div id="id-1.4.7.8.17.5.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>To support <a class="xref" href="bk02ch05.html#section-configuring-compute-migrations" title="5.4.9. Configure migrations">Section 5.4.9, “Configure migrations”</a>,
you cannot specify a specific IP address for <code class="literal">vncserver_listen</code>,
because that IP address does not exist on the destination host.</p></div><div class="table" id="id-1.4.7.8.17.5.9.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.8: </span><span class="name">Description of VNC configuration options </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.5.9.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Description of VNC configuration options" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                      <p>Configuration option = Default value</p>
                    </th><th>
                      <p>Description</p>
                    </th></tr></thead><tbody><tr><td>
                      <p>
                        <span class="bold"><strong>[DEFAULT]</strong></span>
                      </p>
                    </td><td> </td></tr><tr><td>
                      <p>
                        <code class="literal">daemon = False</code>
                      </p>
                    </td><td>
                      <p>(BoolOpt) Become a daemon (background process)</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">key = None</code>
                      </p>
                    </td><td>
                      <p>(StrOpt) SSL key file (if separate from cert)</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">novncproxy_host = 0.0.0.0</code>
                      </p>
                    </td><td>
                      <p>(StrOpt) Host on which to listen for incoming requests</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">novncproxy_port = 6080</code>
                      </p>
                    </td><td>
                      <p>(IntOpt) Port on which to listen for incoming requests</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">record = False</code>
                      </p>
                    </td><td>
                      <p>(BoolOpt) Record sessions to FILE.[session_number]</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">source_is_ipv6 = False</code>
                      </p>
                    </td><td>
                      <p>(BoolOpt) Source is ipv6</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">ssl_only = False</code>
                      </p>
                    </td><td>
                      <p>(BoolOpt) Disallow non-encrypted connections</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">web = /usr/share/spice-html5</code>
                      </p>
                    </td><td>
                      <p>(StrOpt) Run webserver on same port. Serve files from DIR.</p>
                    </td></tr><tr><td>
                      <p>
                        <span class="bold"><strong>[vmware]</strong></span>
                      </p>
                    </td><td> </td></tr><tr><td>
                      <p>
                        <code class="literal">vnc_port = 5900</code>
                      </p>
                    </td><td>
                      <p>(IntOpt) VNC starting port</p>
                    </td></tr><tr><td>
                      <p>
                        <code class="literal">vnc_port_total = 10000</code>
                      </p>
                    </td><td>
                      <p>vnc_port_total = 10000</p>
                    </td></tr><tr><td>
                      <p>
                        <span class="bold"><strong>[vnc]</strong></span>
                      </p>
                    </td><td> </td></tr><tr><td>
                      <p>enabled = True</p>
                    </td><td>
                      <p>(BoolOpt) Enable VNC related features</p>
                    </td></tr><tr><td>
                      <p>novncproxy_base_url = <a class="link" href="http://127.0.0.1:6080/vnc_auto.html" target="_blank">http://127.0.0.1:6080/vnc_auto.html</a></p>
                    </td><td>
                      <p>(StrOpt) Location of VNC console proxy, in the form
"<a class="link" href="http://127.0.0.1:6080/vnc_auto.html" target="_blank">http://127.0.0.1:6080/vnc_auto.html</a>"</p>
                    </td></tr><tr><td>
                      <p>vncserver_listen = 127.0.0.1</p>
                    </td><td>
                      <p>(StrOpt) IP address on which instance vncservers should listen</p>
                    </td></tr><tr><td>
                      <p>vncserver_proxyclient_address = 127.0.0.1</p>
                    </td><td>
                      <p>(StrOpt) The address to which proxy clients 
should connect</p>
                    </td></tr></tbody></table></div></div><div id="id-1.4.7.8.17.5.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The <code class="literal">vncserver_proxyclient_address</code> defaults to <code class="literal">127.0.0.1</code>,
which is the address of the compute host that Compute instructs
proxies to use when connecting to instance servers.</p></li><li class="listitem "><p>For all-in-one XenServer domU deployments, set this to
<code class="literal">169.254.0.1.</code></p></li><li class="listitem "><p>For multi-host XenServer domU deployments, set to a <code class="literal">dom0
management IP</code> on the same network as the proxies.</p></li><li class="listitem "><p>For multi-host libvirt deployments, set to a host management IP
on the same network as the proxies.</p></li></ul></div></div></div><div class="sect4 " id="id-1.4.7.8.17.5.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.11.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Typical deployment</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.5.10">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>A typical deployment has the following components:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>A <code class="literal">nova-consoleauth</code> process. Typically runs on the controller host.</p></li><li class="listitem "><p>One or more <code class="literal">nova-novncproxy</code> services. Supports browser-based noVNC
clients. For simple deployments, this service typically runs on the
same machine as <code class="literal">nova-api</code> because it operates as a proxy between the
public network and the private compute host network.</p></li><li class="listitem "><p>One or more compute hosts. These compute hosts must have correctly
configured options, as follows.</p></li></ul></div></div><div class="sect4 " id="id-1.4.7.8.17.5.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.11.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">nova-novncproxy (noVNC)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.5.11">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You must install the noVNC package, which contains the <code class="literal">nova-novncproxy</code>
service. As root, run the following command:</p><div class="verbatim-wrap"><pre class="screen"># apt-get install nova-novncproxy</pre></div><p>The service starts automatically on installation.</p><p>To restart the service, run:</p><div class="verbatim-wrap"><pre class="screen"># service nova-novncproxy restart</pre></div><p>The configuration option parameter should point to your <code class="literal">nova.conf</code>
file, which includes the message queue server address and credentials.</p><p>By default, <code class="literal">nova-novncproxy</code> binds on <code class="literal">0.0.0.0:6080</code>.</p><p>To connect the service to your Compute deployment, add the following
configuration options to your <code class="literal">nova.conf</code> file:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
                  <code class="literal">vncserver_listen=0.0.0.0</code>
                </p><p>Specifies the address on which the VNC service should bind. Make sure
it is assigned one of the compute node interfaces. This address is
the one used by your domain file.</p><div class="verbatim-wrap"><pre class="screen">&lt;graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/&gt;</pre></div><div id="id-1.4.7.8.17.5.11.10.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>To use live migration, use the 0.0.0.0 address.</p></div></li><li class="listitem "><p>
                  <code class="literal">vncserver_proxyclient_address=127.0.0.1</code>
                </p><p>The address of the compute host that Compute instructs proxies to use
when connecting to instance <code class="literal">vncservers</code>.</p></li></ul></div></div><div class="sect4 " id="id-1.4.7.8.17.5.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.11.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Frequently asked questions about VNC access to virtual machines</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.17.5.12">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
                  <span class="bold"><strong>Q: I want VNC support in the OpenStack dashboard. What services do
I need?</strong></span>
                </p><p>A: You need <code class="literal">nova-novncproxy</code>, <code class="literal">nova-consoleauth</code>, and correctly
configured compute hosts.</p></li><li class="listitem "><p>
                  <span class="bold"><strong>Q: When I use ``nova get-vnc-console`` or click on the VNC tab of
the OpenStack dashboard, it hangs. Why?</strong></span>
                </p><p>A: Make sure you are running <code class="literal">nova-consoleauth</code> (in addition to
<code class="literal">nova-novncproxy</code>). The proxies rely on <code class="literal">nova-consoleauth</code> to validate
tokens, and waits for a reply from them until a timeout is reached.</p></li><li class="listitem "><p>
                  <span class="bold"><strong>Q: My VNC proxy worked fine during my all-in-one test, but now it
doesn't work on multi host. Why?</strong></span>
                </p><p>A: The default options work for an all-in-one install, but changes
must be made on your compute hosts once you start to build a cluster.
As an example, suppose you have two servers:</p><div class="verbatim-wrap highlight bash"><pre class="screen">PROXYSERVER (public_ip=172.24.1.1, management_ip=192.168.1.1)
COMPUTESERVER (management_ip=192.168.1.2)</pre></div><p>Your <code class="literal">nova-compute</code> configuration file must set the following values:</p><div class="verbatim-wrap"><pre class="screen"># These flags help construct a connection data structure
vncserver_proxyclient_address=192.168.1.2
novncproxy_base_url=http://172.24.1.1:6080/vnc_auto.html


# This is the address where the underlying vncserver (not the proxy)
# will listen for connections.
vncserver_listen=192.168.1.2</pre></div></li><li class="listitem "><p>
                  <span class="bold"><strong>Q: My noVNC does not work with recent versions of web browsers. Why?</strong></span>
                </p><p>A: Make sure you have installed <code class="literal">python-numpy</code>, which is required
to support a newer version of the WebSocket protocol (HyBi-07+).</p></li><li class="listitem "><p>
                  <span class="bold"><strong>Q: How do I adjust the dimensions of the VNC window image in the
OpenStack dashboard?</strong></span>
                </p><p>A: These values are hard-coded in a Django HTML template. To alter
them, edit the <code class="literal">_detail_vnc.html</code> template file. The location of
this file varies based on Linux distribution. On Ubuntu 14.04, the
file is at
<code class="literal">/usr/share/pyshared/horizon/dashboards/nova/instances/templates/instances/_detail_vnc.html</code>.</p><p>Modify the <code class="literal">width</code> and <code class="literal">height</code> options, as follows:</p><div class="verbatim-wrap"><pre class="screen">&lt;iframe src="{{ vnc_url }}" width="720" height="430"&gt;&lt;/iframe&gt;</pre></div></li><li class="listitem "><p>
                  <span class="bold"><strong>Q: My noVNC connections failed with ValidationError: Origin header
protocol does not match. Why?</strong></span>
                </p><p>A: Make sure the <code class="literal">base_url</code> match your TLS setting. If you are
using https console connections, make sure that the value of
<code class="literal">novncproxy_base_url</code> is set explicitly where the <code class="literal">nova-novncproxy</code>
service is running.</p></li></ul></div></div></div></div><div class="sect2 " id="id-1.4.7.8.18"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Compute service groups</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.18">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The Compute service must know the status of each compute node to
effectively manage and use them. This can include events like a user
launching a new VM, the scheduler sending a request to a live node, or a
query to the ServiceGroup API to determine if a node is live.</p><p>When a compute worker running the nova-compute daemon starts, it calls
the join API to join the compute group. Any service (such as the
scheduler) can query the group's membership and the status of its nodes.
Internally, the ServiceGroup client driver automatically updates the
compute worker status.</p><div class="sect3 " id="id-1.4.7.8.18.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Database ServiceGroup driver</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.18.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>By default, Compute uses the database driver to track if a node is live.
In a compute worker, this driver periodically sends a <code class="literal">db update</code>
command to the database, saying “I'm OK” with a timestamp. Compute uses
a pre-defined timeout (<code class="literal">service_down_time</code>) to determine if a node is
dead.</p><p>The driver has limitations, which can be problematic depending on your
environment. If a lot of compute worker nodes need to be checked, the
database can be put under heavy load, which can cause the timeout to
trigger, and a live node could incorrectly be considered dead. By
default, the timeout is 60 seconds. Reducing the timeout value can help
in this situation, but you must also make the database update more
frequently, which again increases the database workload.</p><p>The database contains data that is both transient (such as whether the
node is alive) and persistent (such as entries for VM owners). With the
ServiceGroup abstraction, Compute can treat each type separately.</p><div class="sect4 " id="id-1.4.7.8.18.4.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ZooKeeper ServiceGroup driver</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.18.4.5">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The ZooKeeper ServiceGroup driver works by using ZooKeeper ephemeral
nodes. ZooKeeper, unlike databases, is a distributed system, with its
load divided among several servers. On a compute worker node, the driver
can establish a ZooKeeper session, then create an ephemeral znode in the
group directory. Ephemeral znodes have the same lifespan as the session.
If the worker node or the nova-compute daemon crashes, or a network
partition is in place between the worker and the ZooKeeper server
quorums, the ephemeral znodes are removed automatically. The driver
can be given group membership by running the <code class="command">ls</code> command in the
group directory.</p><p>The ZooKeeper driver requires the ZooKeeper servers and client
libraries. Setting up ZooKeeper servers is outside the scope of this
guide (for more information, see <a class="link" href="http://zookeeper.apache.org/" target="_blank">Apache Zookeeper</a>). These client-side
Python libraries must be installed on every compute node:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.8.18.4.5.4.1"><span class="term ">
                  <span class="bold"><strong>python-zookeeper</strong></span>
                </span></dt><dd><p>The official Zookeeper Python binding</p></dd><dt id="id-1.4.7.8.18.4.5.4.2"><span class="term ">
                  <span class="bold"><strong>evzookeeper</strong></span>
                </span></dt><dd><p>This library makes the binding work with the eventlet threading model.</p></dd></dl></div><p>This example assumes the ZooKeeper server addresses and ports are
<code class="literal">192.168.2.1:2181</code>, <code class="literal">192.168.2.2:2181</code>, and <code class="literal">192.168.2.3:2181</code>.</p><p>These values in the <code class="literal">/etc/nova/nova.conf</code> file are required on every
node for the ZooKeeper driver:</p><div class="verbatim-wrap highlight ini"><pre class="screen"># Driver for the ServiceGroup service
servicegroup_driver="zk"

[zookeeper]
address="192.168.2.1:2181,192.168.2.2:2181,192.168.2.3:2181"</pre></div></div><div class="sect4 " id="id-1.4.7.8.18.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.12.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Memcache ServiceGroup driver</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.18.4.6">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The memcache ServiceGroup driver uses memcached, a distributed memory
object caching system that is used to increase site performance. For
more details, see <a class="link" href="http://memcached.org/" target="_blank">memcached.org</a>.</p><p>To use the memcache driver, you must install memcached. You might
already have it installed, as the same driver is also used for the
OpenStack Object Storage and OpenStack dashboard. To install
memcached, see the <span class="emphasis"><em>Environment -&gt; Memcached</em></span> section in the
<a class="link" href="http://docs.openstack.org/project-install-guide/newton" target="_blank">Installation Tutorials and Guides</a>
depending on your distribution.</p><p>These values in the <code class="literal">/etc/nova/nova.conf</code> file are required on every
node for the memcache driver:</p><div class="verbatim-wrap highlight ini"><pre class="screen"># Driver for the ServiceGroup service
servicegroup_driver = "mc"

# Memcached servers. Use either a list of memcached servers to use for caching (list value),
# or "&lt;None&gt;" for in-process caching (default).
memcached_servers = &lt;None&gt;

# Timeout; maximum time since last check-in for up service (integer value).
# Helps to define whether a node is dead
service_down_time = 60</pre></div></div></div></div><div class="sect2 " id="id-1.4.7.8.19"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Security hardening</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.19">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack Compute can be integrated with various third-party
technologies to increase security. For more information, see the
<a class="link" href="http://docs.openstack.org/sec/" target="_blank">OpenStack Security Guide</a>.</p><div class="sect3 " id="id-1.4.7.8.19.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trusted compute pools</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.19.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Administrators can designate a group of compute hosts as trusted using
trusted compute pools. The trusted hosts use hardware-based security
features, such as the Intel Trusted Execution Technology (TXT), to
provide an additional level of security. Combined with an external
stand-alone, web-based remote attestation server, cloud providers can
ensure that the compute node runs only software with verified
measurements and can ensure a secure cloud stack.</p><p>Trusted compute pools provide the ability for cloud subscribers to
request services run only on verified compute nodes.</p><p>The remote attestation server performs node verification like this:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Compute nodes boot with Intel TXT technology enabled.</p></li><li class="step "><p>The compute node BIOS, hypervisor, and operating system are measured.</p></li><li class="step "><p>When the attestation server challenges the compute node, the measured
data is sent to the attestation server.</p></li><li class="step "><p>The attestation server verifies the measurements against a known good
database to determine node trustworthiness.</p></li></ol></div></div><p>A description of how to set up an attestation service is beyond the
scope of this document. For an open source project that you can use to
implement an attestation service, see the <a class="link" href="https://github.com/OpenAttestation/OpenAttestation" target="_blank">Open
Attestation</a>
project.</p><div class="figure" id="id-1.4.7.8.19.3.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/OpenStackTrustedComputePool1.png" target="_blank"><img src="images/OpenStackTrustedComputePool1.png" width="" alt="Configuring Compute to use trusted compute pools" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.8: </span><span class="name">Configuring Compute to use trusted compute pools </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.19.3.7">#</a></h6></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Enable scheduling support for trusted compute pools by adding these
lines to the <code class="literal">DEFAULT</code> section of the <code class="literal">/etc/nova/nova.conf</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[DEFAULT]
compute_scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler
scheduler_available_filters=nova.scheduler.filters.all_filters
scheduler_default_filters=AvailabilityZoneFilter,RamFilter,ComputeFilter,TrustedFilter</pre></div></li><li class="step "><p>Specify the connection information for your attestation service by
adding these lines to the <code class="literal">trusted_computing</code> section of the
<code class="literal">/etc/nova/nova.conf</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[trusted_computing]
attestation_server = 10.1.71.206
attestation_port = 8443
# If using OAT v2.0 after, use this port:
# attestation_port = 8181
attestation_server_ca_file = /etc/nova/ssl.10.1.71.206.crt
# If using OAT v1.5, use this api_url:
attestation_api_url = /AttestationService/resources
# If using OAT pre-v1.5, use this api_url:
# attestation_api_url = /OpenAttestationWebServices/V1.0
attestation_auth_blob = i-am-openstack</pre></div><p>In this example:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.8.19.3.8.2.4.1"><span class="term ">server</span></dt><dd><p>Host name or IP address of the host that runs the attestation
service</p></dd><dt id="id-1.4.7.8.19.3.8.2.4.2"><span class="term ">port</span></dt><dd><p>HTTPS port for the attestation service</p></dd><dt id="id-1.4.7.8.19.3.8.2.4.3"><span class="term ">server_ca_file</span></dt><dd><p>Certificate file used to verify the attestation server's identity</p></dd><dt id="id-1.4.7.8.19.3.8.2.4.4"><span class="term ">api_url</span></dt><dd><p>The attestation service's URL path</p></dd><dt id="id-1.4.7.8.19.3.8.2.4.5"><span class="term ">auth_blob</span></dt><dd><p>An authentication blob, required by the attestation service.</p></dd></dl></div></li><li class="step "><p>Save the file, and restart the <code class="literal">nova-compute</code> and <code class="literal">nova-scheduler</code>
service to pick up the changes.</p></li></ol></div></div><p>To customize the trusted compute pools, use these configuration option
settings:</p><div class="table" id="id-1.4.7.8.19.3.10"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.9: </span><span class="name">Description of trusted computing configuration options </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.19.3.10">#</a></h6></div><div class="table-contents"><table class="table" summary="Description of trusted computing configuration options" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                    <p>Configuration option = Default value</p>
                  </th><th>
                    <p>Description</p>
                  </th></tr><tr><th>
                    <p>[trusted_computing]</p>
                  </th><th> </th></tr></thead><tbody><tr><td>
                    <p>attestation_api_url = /OpenAttestationWebServices/V1.0</p>
                  </td><td>
                    <p>(StrOpt) Attestation web API URL</p>
                  </td></tr><tr><td>
                    <p>attestation_auth_blob = None</p>
                  </td><td>
                    <p>(StrOpt) Attestation authorization blob - must change</p>
                  </td></tr><tr><td>
                    <p>attestation_auth_timeout = 60</p>
                  </td><td>
                    <p>(IntOpt) Attestation status cache valid period length</p>
                  </td></tr><tr><td>
                    <p>attestation_insecure_ssl = False</p>
                  </td><td>
                    <p>(BoolOpt) Disable SSL cert verification for Attestation service</p>
                  </td></tr><tr><td>
                    <p>attestation_port = 8443</p>
                  </td><td>
                    <p>(StrOpt) Attestation server port</p>
                  </td></tr><tr><td>
                    <p>attestation_server = None</p>
                  </td><td>
                    <p>(StrOpt) Attestation server HTTP</p>
                  </td></tr><tr><td>
                    <p>attestation_server_ca_file = None</p>
                  </td><td>
                    <p>(StrOpt) Attestation server Cert file for Identity verification</p>
                  </td></tr></tbody></table></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Flavors can be designated as trusted using the
<code class="command">nova flavor-key set</code> command. In this example, the <code class="literal">m1.tiny</code>
flavor is being set as trusted:</p><div class="verbatim-wrap"><pre class="screen">$ nova flavor-key m1.tiny set trust:trusted_host=trusted</pre></div></li><li class="step "><p>You can request that your instance is run on a trusted host by
specifying a trusted flavor when booting the instance:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server create --flavor m1.tiny \
  --key-name myKeypairName --image myImageID newInstanceName</pre></div></li></ol></div></div><div class="figure" id="id-1.4.7.8.19.3.12"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/OpenStackTrustedComputePool2.png" target="_blank"><img src="images/OpenStackTrustedComputePool2.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.9: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.19.3.12">#</a></h6></div></div></div><div class="sect3 " id="id-1.4.7.8.19.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Encrypt Compute metadata traffic</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.19.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
            <span class="bold"><strong>Enabling SSL encryption</strong></span>
          </p><p>OpenStack supports encrypting Compute metadata traffic with HTTPS.
Enable SSL encryption in the <code class="literal">metadata_agent.ini</code> file.</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Enable the HTTPS protocol.</p><div class="verbatim-wrap highlight ini"><pre class="screen">nova_metadata_protocol = https</pre></div></li><li class="step "><p>Determine whether insecure SSL connections are accepted for Compute
metadata server requests. The default value is <code class="literal">False</code>.</p><div class="verbatim-wrap highlight ini"><pre class="screen">nova_metadata_insecure = False</pre></div></li><li class="step "><p>Specify the path to the client certificate.</p><div class="verbatim-wrap highlight ini"><pre class="screen">nova_client_cert = PATH_TO_CERT</pre></div></li><li class="step "><p>Specify the path to the private key.</p><div class="verbatim-wrap highlight ini"><pre class="screen">nova_client_priv_key = PATH_TO_KEY</pre></div></li></ol></div></div></div></div><div class="sect2 " id="id-1.4.7.8.20"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recover from a failed compute node</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.20">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>If you deploy Compute with a shared file system, you can use several methods
to quickly recover from a node failure. This section discusses manual
recovery.</p><div class="sect3 " id="id-1.4.7.8.20.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Evacuate instances</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.20.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>If a hardware malfunction or other error causes the cloud compute node to
fail, you can use the <code class="command">nova evacuate</code> command to evacuate instances.
See the <a class="link" href="http://docs.openstack.org/admin-guide/cli-nova-evacuate.html" target="_blank">OpenStack Administrator Guide</a>.</p></div><div class="sect3 " id="id-1.4.7.8.20.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manual recovery</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.20.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To manually recover a failed compute node:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Identify the VMs on the affected hosts by using a combination of
the <code class="command">openstack server list</code> and <code class="command">openstack server show</code>
commands or the <code class="command">euca-describe-instances</code> command.</p><p>For example, this command displays information about the i-000015b9
instance that runs on the np-rcc54 node:</p><div class="verbatim-wrap"><pre class="screen">$ euca-describe-instances
i-000015b9 at3-ui02 running nectarkey (376, np-rcc54) 0 m1.xxlarge 2012-06-19T00:48:11.000Z 115.146.93.60</pre></div></li><li class="step "><p>Query the Compute database for the status of the host. This example
converts an EC2 API instance ID to an OpenStack ID. If you use the
<code class="command">nova</code> commands, you can substitute the ID directly. This example
output is truncated:</p><div class="verbatim-wrap"><pre class="screen">mysql&gt; SELECT * FROM instances WHERE id = CONV('15b9', 16, 10) \G;
*************************** 1. row ***************************
created_at: 2012-06-19 00:48:11
updated_at: 2012-07-03 00:35:11
deleted_at: NULL
...
id: 5561
...
power_state: 5
vm_state: shutoff
...
hostname: at3-ui02
host: np-rcc54
...
uuid: 3f57699a-e773-4650-a443-b4b37eed5a06
...
task_state: NULL
...</pre></div><div id="id-1.4.7.8.20.4.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Find the credentials for your database in <code class="literal">/etc/nova.conf</code> file.</p></div></li><li class="step "><p>Decide to which compute host to move the affected VM. Run this database
command to move the VM to that host:</p><div class="verbatim-wrap"><pre class="screen">mysql&gt; UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';</pre></div></li><li class="step "><p>If you use a hypervisor that relies on libvirt, such as KVM, update the
<code class="literal">libvirt.xml</code> file in <code class="literal">/var/lib/nova/instances/[instance ID]</code> with
these changes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Change the <code class="literal">DHCPSERVER</code> value to the host IP address of the new
compute host.</p></li><li class="listitem "><p>Update the VNC IP to <code class="literal">0.0.0.0</code>.</p></li></ul></div></li><li class="step "><p>Reboot the VM:</p><div class="verbatim-wrap"><pre class="screen">$ openstack server reboot 3f57699a-e773-4650-a443-b4b37eed5a06</pre></div></li></ol></div></div><p>Typically, the database update and <code class="command">openstack server reboot</code> command
recover a VM from a failed host. However, if problems persist, try one of
these actions:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Use <code class="command">virsh</code> to recreate the network filter configuration.</p></li><li class="listitem "><p>Restart Compute services.</p></li><li class="listitem "><p>Update the <code class="literal">vm_state</code> and <code class="literal">power_state</code> fields in the Compute database.</p></li></ul></div></div><div class="sect3 " id="id-1.4.7.8.20.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recover from a UID/GID mismatch</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.20.5">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Sometimes when you run Compute with a shared file system or an automated
configuration tool, files on your compute node might use the wrong UID or GID.
This UID or GID mismatch can prevent you from running live migrations or
starting virtual machines.</p><p>This procedure runs on <code class="literal">nova-compute</code> hosts, based on the KVM hypervisor:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Set the nova UID to the same number in <code class="literal">/etc/passwd</code> on all hosts. For
example, set the UID to <code class="literal">112</code>.</p><div id="id-1.4.7.8.20.5.4.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Choose UIDs or GIDs that are not in use for other users or groups.</p></div></li><li class="step "><p>Set the <code class="literal">libvirt-qemu</code> UID to the same number in the <code class="literal">/etc/passwd</code> file
on all hosts. For example, set the UID to <code class="literal">119</code>.</p></li><li class="step "><p>Set the <code class="literal">nova</code> group to the same number in the <code class="literal">/etc/group</code> file on all
hosts. For example, set the group to <code class="literal">120</code>.</p></li><li class="step "><p>Set the <code class="literal">libvirtd</code> group to the same number in the <code class="literal">/etc/group</code> file on
all hosts. For example, set the group to <code class="literal">119</code>.</p></li><li class="step "><p>Stop the services on the compute node.</p></li><li class="step "><p>Change all files that the nova user or group owns. For example:</p><div class="verbatim-wrap"><pre class="screen"># find / -uid 108 -exec chown nova {} \;
# note the 108 here is the old nova UID before the change
# find / -gid 120 -exec chgrp nova {} \;</pre></div></li><li class="step "><p>Repeat all steps for the <code class="literal">libvirt-qemu</code> files, if required.</p></li><li class="step "><p>Restart the services.</p></li><li class="step "><p>To verify that all files use the correct IDs, run the <code class="command">find</code>
command.</p></li></ol></div></div></div><div class="sect3 " id="id-1.4.7.8.20.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recover cloud after disaster</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.20.6">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>This section describes how to manage your cloud after a disaster and back up
persistent storage volumes. Backups are mandatory, even outside of disaster
scenarios.</p><p>For a definition of a disaster recovery plan (DRP), see
<a class="link" href="http://en.wikipedia.org/wiki/Disaster_Recovery_Plan" target="_blank">http://en.wikipedia.org/wiki/Disaster_Recovery_Plan</a>.</p><p>A disk crash, network loss, or power failure can affect several components in
your cloud architecture. The worst disaster for a cloud is a power loss. A
power loss affects these components:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>A cloud controller (<code class="literal">nova-api</code>, <code class="literal">nova-objectstore</code>, <code class="literal">nova-network</code>)</p></li><li class="listitem "><p>A compute node (<code class="literal">nova-compute</code>)</p></li><li class="listitem "><p>A storage area network (SAN) used by OpenStack Block Storage
(<code class="literal">cinder-volumes</code>)</p></li></ul></div><p>Before a power loss:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Create an active iSCSI session from the SAN to the cloud controller
(used for the <code class="literal">cinder-volumes</code> LVM's VG).</p></li><li class="listitem "><p>Create an active iSCSI session from the cloud controller to the compute
node (managed by <code class="literal">cinder-volume</code>).</p></li><li class="listitem "><p>Create an iSCSI session for every volume (so 14 EBS volumes requires 14
iSCSI sessions).</p></li><li class="listitem "><p>Create <code class="literal">iptables</code> or <code class="literal">ebtables</code> rules from the cloud controller to the
compute node. This allows access from the cloud controller to the
running instance.</p></li><li class="listitem "><p>Save the current state of the database, the current state of the running
instances, and the attached volumes (mount point, volume ID, volume
status, etc), at least from the cloud controller to the compute node.</p></li></ul></div><p>After power resumes and all hardware components restart:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The iSCSI session from the SAN to the cloud no longer exists.</p></li><li class="listitem "><p>The iSCSI session from the cloud controller to the compute node no
longer exists.</p></li><li class="listitem "><p>nova-network reapplies configurations on boot and, as a result, recreates
the iptables and ebtables from the cloud controller to the compute node.</p></li><li class="listitem "><p>Instances stop running.</p><p>Instances are not lost because neither <code class="literal">destroy</code> nor <code class="literal">terminate</code> ran.
The files for the instances remain on the compute node.</p></li><li class="listitem "><p>The database does not update.</p></li></ul></div><p>
            <span class="bold"><strong>Begin recovery</strong></span>
          </p><div id="id-1.4.7.8.20.6.11" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>Do not add any steps or change the order of steps in this procedure.</p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Check the current relationship between the volume and its instance, so
that you can recreate the attachment.</p><p>Use the <code class="command">openstack volume list</code> command to get this information.
Note that the <code class="command">openstack</code> client can get volume information
from OpenStack Block Storage.</p></li><li class="step "><p>Update the database to clean the stalled state. Do this for every
volume by using these queries:</p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use cinder;
mysql&gt; update volumes set mountpoint=NULL;
mysql&gt; update volumes set status="available" where status &lt;&gt;"error_deleting";
mysql&gt; update volumes set attach_status="detached";
mysql&gt; update volumes set instance_id=0;</pre></div><p>Use <code class="command">openstack volume list</code> command to list all volumes.</p></li><li class="step "><p>Restart the instances by using the
<code class="command">openstack server reboot INSTANCE</code> command.</p><div id="id-1.4.7.8.20.6.12.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>Some instances completely reboot and become reachable, while some might
stop at the plymouth stage. This is expected behavior. DO NOT reboot a
second time.</p><p>Instance state at this stage depends on whether you added an
<code class="literal">/etc/fstab</code> entry for that volume. Images built with the cloud-init
package remain in a <code class="literal">pending</code> state, while others skip the missing
volume and start. You perform this step to ask Compute to reboot every
instance so that the stored state is preserved. It does not matter if
not all instances come up successfully. For more information about
cloud-init, see
<a class="link" href="https://help.ubuntu.com/community/CloudInit/" target="_blank">help.ubuntu.com/community/CloudInit/</a>.</p></div></li><li class="step "><p>If required, run the <code class="command">openstack server add volume</code> command to
reattach the volumes to their respective instances. This example uses
a file of listed volumes to reattach them:</p><div class="verbatim-wrap highlight bash"><pre class="screen">#!/bin/bash

while read line; do
    volume=`echo $line | $CUT -f 1 -d " "`
    instance=`echo $line | $CUT -f 2 -d " "`
    mount_point=`echo $line | $CUT -f 3 -d " "`
        echo "ATTACHING VOLUME FOR INSTANCE - $instance"
    openstack server add volume $instance $volume $mount_point
    sleep 2
done &lt; $volumes_tmp_file</pre></div><p>Instances that were stopped at the plymouth stage now automatically
continue booting and start normally. Instances that previously started
successfully can now see the volume.</p></li><li class="step "><p>Log in to the instances with SSH and reboot them.</p><p>If some services depend on the volume or if a volume has an entry in fstab,
you can now restart the instance. Restart directly from the instance itself
and not through <code class="command">nova</code>:</p><div class="verbatim-wrap"><pre class="screen"># shutdown -r now</pre></div><p>When you plan for and complete a disaster recovery, follow these tips:</p></li></ol></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Use the <code class="literal">errors=remount</code> option in the <code class="literal">fstab</code> file to prevent
data corruption.</p><p>In the event of an I/O error, this option prevents writes to the disk. Add
this configuration option into the cinder-volume server that performs the
iSCSI connection to the SAN and into the instances' <code class="literal">fstab</code> files.</p></li><li class="listitem "><p>Do not add the entry for the SAN's disks to the cinder-volume's
<code class="literal">fstab</code> file.</p><p>Some systems hang on that step, which means you could lose access to
your cloud-controller. To re-run the session manually, run this
command before performing the mount:</p><div class="verbatim-wrap"><pre class="screen"># iscsiadm -m discovery -t st -p $SAN_IP $ iscsiadm -m node --target-name $IQN -p $SAN_IP -l</pre></div></li><li class="listitem "><p>On your instances, if you have the whole <code class="literal">/home/</code> directory on the
disk, leave a user's directory with the user's bash files and the
<code class="literal">authorized_keys</code> file instead of emptying the <code class="literal">/home/</code> directory
and mapping the disk on it.</p><p>This action enables you to connect to the instance without the volume
attached, if you allow only connections through public keys.</p></li></ul></div><p>To script the disaster recovery plan (DRP), use the
<a class="link" href="https://github.com/Razique/BashStuff/blob/master/SYSTEMS/OpenStack/SCR_5006_V00_NUAC-OPENSTACK-DRP-OpenStack.sh" target="_blank">https://github.com/Razique</a> bash script.</p><p>This script completes these steps:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Creates an array for instances and their attached volumes.</p></li><li class="step "><p>Updates the MySQL database.</p></li><li class="step "><p>Restarts all instances with euca2ools.</p></li><li class="step "><p>Reattaches the volumes.</p></li><li class="step "><p>Uses Compute credentials to make an SSH connection into every instance.</p></li></ol></div></div><p>The script includes a <code class="literal">test mode</code>, which enables you to perform the sequence
for only one instance.</p><p>To reproduce the power loss, connect to the compute node that runs that
instance and close the iSCSI session. Do not detach the volume by using the
<code class="command">openstack server remove volume</code> command. You must manually close the
iSCSI session. This example closes an iSCSI session with the number <code class="literal">15</code>:</p><div class="verbatim-wrap"><pre class="screen"># iscsiadm -m session -u -r 15</pre></div><p>Do not forget the <code class="literal">-r</code> option. Otherwise, all sessions close.</p><div id="id-1.4.7.8.20.6.21" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>There is potential for data loss while running instances during
this procedure. If you are using Liberty or earlier, ensure you have the
correct patch and set the options appropriately.</p></div></div></div><div class="sect2 " id="id-1.4.7.8.21"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.4.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Advanced configuration</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack clouds run on platforms that differ greatly in the capabilities that
they provide. By default, the Compute service seeks to abstract the underlying
hardware that it runs on, rather than exposing specifics about the underlying
host platforms. This abstraction manifests itself in many ways. For example,
rather than exposing the types and topologies of CPUs running on hosts, the
service exposes a number of generic CPUs (virtual CPUs, or vCPUs) and allows
for overcommitting of these. In a similar manner, rather than exposing the
individual types of network devices available on hosts, generic
software-powered network ports are provided. These features are designed to
allow high resource utilization and allows the service to provide a generic
cost-effective and highly scalable cloud upon which to build applications.</p><p>This abstraction is beneficial for most workloads. However, there are some
workloads where determinism and per-instance performance are important, if
not vital. In these cases, instances can be expected to deliver near-native
performance. The Compute service provides features to improve individual
instance for these kind of workloads.</p><div class="sect3 " id="id-1.4.7.8.21.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Attaching physical PCI devices to guests</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The PCI passthrough feature in OpenStack allows full access and direct control
of a physical PCI device in guests. This mechanism is generic for any kind of
PCI device, and runs with a Network Interface Card (NIC), Graphics Processing
Unit (GPU), or any other devices that can be attached to a PCI bus. Correct
driver installation is the only requirement for the guest to properly
use the devices.</p><p>Some PCI devices provide Single Root I/O Virtualization and Sharing (SR-IOV)
capabilities. When SR-IOV is used, a physical device is virtualized and appears
as multiple PCI devices. Virtual PCI devices are assigned to the same or
different guests. In the case of PCI passthrough, the full physical device is
assigned to only one guest and cannot be shared.</p><div id="id-1.4.7.8.21.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>For information on attaching virtual SR-IOV devices to guests, refer to the
<a class="link" href="http://docs.openstack.org/newton/networking-guide/config-sriov.html" target="_blank">Networking Guide</a>.</p></div><p>To enable PCI passthrough, follow the steps below:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Configure nova-scheduler (Controller)</p></li><li class="step "><p>Configure nova-api (Controller)**</p></li><li class="step "><p>Configure a flavor (Controller)</p></li><li class="step "><p>Enable PCI passthrough (Compute)</p></li><li class="step "><p>Configure PCI devices in nova-compute (Compute)</p></li></ol></div></div><div id="id-1.4.7.8.21.4.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The PCI device with address <code class="literal">0000:41:00.0</code> is used as an example. This
will differ between environments.</p></div><div class="sect4 " id="id-1.4.7.8.21.4.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure nova-scheduler (Controller)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.4.8">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Configure <code class="literal">nova-scheduler</code> as specified in <a class="link" href="http://docs.openstack.org/newton/networking-guide/config-sriov.html#configure-nova-scheduler-controller" target="_blank">Configure nova-scheduler</a>.</p></li><li class="step "><p>Restart the <code class="literal">nova-scheduler</code> service.</p></li></ol></div></div></div><div class="sect4 " id="id-1.4.7.8.21.4.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure nova-api (Controller)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.4.9">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Specify the PCI alias for the device.</p><p>Configure a PCI alias <code class="literal">a1</code> to request a PCI device with a <code class="literal">vendor_id</code> of
<code class="literal">0x8086</code> and a <code class="literal">product_id</code> of <code class="literal">0x154d</code>. The <code class="literal">vendor_id</code> and
<code class="literal">product_id</code> correspond the PCI device with address <code class="literal">0000:41:00.0</code>.</p><p>Edit <code class="literal">/etc/nova/nova.conf</code>:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[default]
pci_alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</pre></div><p>For more information about the syntax of <code class="literal">pci_alias</code>, refer to <a class="link" href="http://docs.openstack.org/newton/config-reference/compute/config-options.html" target="_blank">nova.conf
configuration options</a>.</p></li><li class="step "><p>Restart the <code class="literal">nova-api</code> service.</p></li></ol></div></div></div><div class="sect4 " id="id-1.4.7.8.21.4.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure a flavor (Controller)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.4.10">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Configure a flavor to request two PCI devices, each with <code class="literal">vendor_id</code> of
<code class="literal">0x8086</code> and <code class="literal">product_id</code> of <code class="literal">0x154d</code>:</p><div class="verbatim-wrap"><pre class="screen"># openstack flavor set m1.large --property "pci_passthrough:alias"="a1:2"</pre></div><p>For more information about the syntax for <code class="literal">pci_passthrough:alias</code>, refer to
<a class="link" href="http://docs.openstack.org/admin-guide/compute-flavors.html" target="_blank">flavor</a>.</p></div><div class="sect4 " id="id-1.4.7.8.21.4.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable PCI passthrough (Compute)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.4.11">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Enable VT-d and IOMMU. For more information, refer to steps one and two in
<a class="link" href="http://docs.openstack.org/newton/networking-guide/config-sriov.html#create-virtual-functions-compute" target="_blank">Create Virtual Functions</a>.</p></div><div class="sect4 " id="id-1.4.7.8.21.4.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure PCI devices (Compute)</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.4.12">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Configure <code class="literal">nova-compute</code> to allow the PCI device to pass through to
VMs. Edit <code class="literal">/etc/nova/nova.conf</code>:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[default]
pci_passthrough_whitelist = { "address": "0000:41:00.0" }</pre></div><p>Alternatively specify multiple PCI devices using whitelisting:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[default]
pci_passthrough_whitelist = { "vendor_id": "8086", "product_id": "10fb" }</pre></div><p>All PCI devices matching the <code class="literal">vendor_id</code> and <code class="literal">product_id</code> are added to
the pool of PCI devices available for passthrough to VMs.</p><p>For more information about the syntax of <code class="literal">pci_passthrough_whitelist</code>,
refer to <a class="link" href="http://docs.openstack.org/newton/config-reference/compute/config-options.html" target="_blank">nova.conf configuration options</a>.</p></li><li class="step "><p>Specify the PCI alias for the device.</p><p>From the Newton release, to resize guest with PCI device, configure the PCI
alias on the compute node as well.</p><p>Configure a PCI alias <code class="literal">a1</code> to request a PCI device with a <code class="literal">vendor_id</code> of
<code class="literal">0x8086</code> and a <code class="literal">product_id</code> of <code class="literal">0x154d</code>. The <code class="literal">vendor_id</code> and
<code class="literal">product_id</code> correspond the PCI device with address <code class="literal">0000:41:00.0</code>.</p><p>Edit <code class="literal">/etc/nova/nova.conf</code>:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[default]
pci_alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</pre></div><p>For more information about the syntax of <code class="literal">pci_alias</code>, refer to <a class="link" href="http://docs.openstack.org/newton/config-reference/compute/config-options.html" target="_blank">nova.conf
configuration options</a>.</p></li><li class="step "><p>Restart the <code class="literal">nova-compute</code> service.</p></li></ol></div></div></div><div class="sect4 " id="id-1.4.7.8.21.4.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create instances with PCI passthrough devices</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.4.13">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The <code class="literal">nova-scheduler</code> selects a destination host that has PCI devices
available with the specified <code class="literal">vendor_id</code> and <code class="literal">product_id</code> that matches the
<code class="literal">pci_alias</code> from the flavor.</p><div class="verbatim-wrap"><pre class="screen"># openstack server create --flavor m1.large --image cirros-0.3.4-x86_64-uec --wait test-pci</pre></div></div></div><div class="sect3 " id="sec-cpu-topo"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CPU topologies</span> <a title="Permalink" class="permalink" href="bk02ch05.html#sec-cpu-topo">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span>sec-cpu-topo</li></ul></div></div></div></div><p>The NUMA topology and CPU pinning features in OpenStack provide high-level
control over how instances run on hypervisor CPUs and the topology of virtual
CPUs available to instances. These features help minimize latency and maximize
performance.</p><div class="sect4 " id="id-1.4.7.8.21.5.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SMP, NUMA, and SMT</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.5.3">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.8.21.5.3.2.1"><span class="term ">Symmetric multiprocessing (SMP)</span></dt><dd><p>SMP is a design found in many modern multi-core systems. In an SMP system,
there are two or more CPUs and these CPUs are connected by some interconnect.
This provides CPUs with equal access to system resources like memory and
input/output ports.</p></dd><dt id="id-1.4.7.8.21.5.3.2.2"><span class="term ">Non-uniform memory access (NUMA)</span></dt><dd><p>NUMA is a derivative of the SMP design that is found in many multi-socket
systems. In a NUMA system, system memory is divided into cells or nodes that
are associated with particular CPUs. Requests for memory on other nodes are
possible through an interconnect bus. However, bandwidth across this shared
bus is limited. As a result, competition for this resource can incur
performance penalties.</p></dd><dt id="id-1.4.7.8.21.5.3.2.3"><span class="term ">Simultaneous Multi-Threading (SMT)</span></dt><dd><p>SMT is a design complementary to SMP. Whereas CPUs in SMP systems share a bus
and some memory, CPUs in SMT systems share many more components. CPUs that
share components are known as thread siblings.  All CPUs appear as usable
CPUs on the system and can execute workloads in parallel. However, as with
NUMA, threads compete for shared resources.</p></dd></dl></div><p>In OpenStack, SMP CPUs are known as <span class="emphasis"><em>cores</em></span>, NUMA cells or nodes are known as
<span class="emphasis"><em>sockets</em></span>, and SMT CPUs are known as <span class="emphasis"><em>threads</em></span>. For example, a quad-socket,
eight core system with Hyper-Threading would have four sockets, eight cores per
socket and two threads per core, for a total of 64 CPUs.</p></div><div class="sect4 " id="id-1.4.7.8.21.5.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing instance NUMA placement policies</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.5.4">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.4.7.8.21.5.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>The functionality described below is currently only supported by the
libvirt/KVM driver.</p></div><p>When running workloads on NUMA hosts, it is important that the vCPUs executing
processes are on the same NUMA node as the memory used by these processes.
This ensures all memory accesses are local to the node and thus do not consume
the limited cross-node memory bandwidth, adding latency to memory accesses.
Similarly, large pages are assigned from memory and benefit from the same
performance improvements as memory allocated using standard pages. Thus, they
also should be local. Finally, PCI devices are directly associated with
specific NUMA nodes for the purposes of DMA. Instances that use PCI or SR-IOV
devices should be placed on the NUMA node associated with these devices.</p><p>By default, an instance floats across all NUMA nodes on a host. NUMA awareness
can be enabled implicitly through the use of huge pages or pinned CPUs or
explicitly through the use of flavor extra specs or image metadata.  In all
cases, the <code class="literal">NUMATopologyFilter</code> filter must be enabled. Details on this
filter are provided in <a class="link" href="http://docs.openstack.org/newton/config-reference/compute/scheduler.html" target="_blank">Scheduling</a> configuration guide.</p><div id="id-1.4.7.8.21.5.4.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>The NUMA node(s) used are normally chosen at random. However, if a PCI
passthrough or SR-IOV device is attached to the instance, then the NUMA
node that the device is associated with will be used. This can provide
important performance improvements. However, booting a large number of
similar instances can result in unbalanced NUMA node usage. Care should
be taken to mitigate this issue. See this <a class="link" href="http://lists.openstack.org/pipermail/openstack-dev/2016-March/090367.html" target="_blank">discussion</a> for more details.</p></div><div id="id-1.4.7.8.21.5.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>Inadequate per-node resources will result in scheduling failures. Resources
that are specific to a node include not only CPUs and memory, but also PCI
and SR-IOV resources. It is not possible to use multiple resources from
different nodes without requesting a multi-node layout. As such, it may be
necessary to ensure PCI or SR-IOV resources are associated with the same
NUMA node or force a multi-node layout.</p></div><p>When used, NUMA awareness allows the operating system of the instance to
intelligently schedule the workloads that it runs and minimize cross-node
memory bandwidth. To restrict an instance's vCPUs to a single host NUMA node,
run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:numa_nodes=1</pre></div><p>Some workloads have very demanding requirements for memory access latency or
bandwidth that exceed the memory bandwidth available from a single NUMA node.
For such workloads, it is beneficial to spread the instance across multiple
host NUMA nodes, even if the instance's RAM/vCPUs could theoretically fit on a
single NUMA node. To force an instance's vCPUs to spread across two host NUMA
nodes, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:numa_nodes=2</pre></div><p>The allocation of instances vCPUs and memory from different host NUMA nodes can
be configured. This allows for asymmetric allocation of vCPUs and memory, which
can be important for some workloads. To spread the 6 vCPUs and 6 GB of memory
of an instance across two NUMA nodes and create an asymmetric 1:2 vCPU and
memory mapping between the two nodes, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:numa_nodes=2
$ openstack flavor set m1.large \  # configure guest node 0
  --property hw:numa_cpus.0=0,1 \
  --property hw:numa_mem.0=2048
$ openstack flavor set m1.large \  # configure guest node 1
  --property hw:numa_cpus.1=2,3,4,5 \
  --property hw:numa_mem.1=4096</pre></div><p>For more information about the syntax for <code class="literal">hw:numa_nodes</code>, <code class="literal">hw:numa_cpus.N</code>
and <code class="literal">hw:num_mem.N</code>, refer to the <a class="link" href="http://docs.openstack.org/admin-guide/compute-flavors.html" target="_blank">Flavors</a> guide.</p></div><div class="sect4 " id="id-1.4.7.8.21.5.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing instance CPU pinning policies</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.5.5">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.4.7.8.21.5.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>The functionality described below is currently only supported by the
libvirt/KVM driver.</p></div><p>By default, instance vCPU processes are not assigned to any particular host
CPU, instead, they float across host CPUs like any other process. This allows
for features like overcommitting of CPUs. In heavily contended systems, this
provides optimal system performance at the expense of performance and latency
for individual instances.</p><p>Some workloads require real-time or near real-time behavior, which is not
possible with the latency introduced by the default CPU policy. For such
workloads, it is beneficial to control which host CPUs are bound to an
instance's vCPUs. This process is known as pinning. No instance with pinned
CPUs can use the CPUs of another pinned instance, thus preventing resource
contention between instances. To configure a flavor to use pinned vCPUs, a
use a dedicated CPU policy. To force this, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:cpu_policy=dedicated</pre></div><div id="id-1.4.7.8.21.5.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>Host aggregates should be used to separate pinned instances from unpinned
instances as the latter will not respect the resourcing requirements of
the former.</p></div><p>When running workloads on SMT hosts, it is important to be aware of the impact
that thread siblings can have. Thread siblings share a number of components
and contention on these components can impact performance. To configure how
to use threads, a CPU thread policy should be specified. For workloads where
sharing benefits performance, use thread siblings. To force this, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=require</pre></div><p>For other workloads where performance is impacted by contention for resources,
use non-thread siblings or non-SMT hosts. To force this, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=isolate</pre></div><p>Finally, for workloads where performance is minimally impacted, use thread
siblings if available. This is the default, but it can be set explicitly:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=prefer</pre></div><p>For more information about the syntax for <code class="literal">hw:cpu_policy</code> and
<code class="literal">hw:cpu_thread_policy</code>, refer to the <a class="link" href="http://docs.openstack.org/admin-guide/compute-flavors.html" target="_blank">Flavors</a> guide.</p><p>Applications are frequently packaged as images. For applications that require
real-time or near real-time behavior, configure image metadata to ensure
created instances are always pinned regardless of flavor. To configure an
image to use pinned vCPUs and avoid thread siblings, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_policy=dedicated \
  --property hw_cpu_thread_policy=isolate</pre></div><p>Image metadata takes precedence over flavor extra specs. Thus, configuring
competing policies causes an exception. By setting a <code class="literal">shared</code> policy
through image metadata, administrators can prevent users configuring CPU
policies in flavors and impacting resource utilization. To configure this
policy, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image set [IMAGE_ID] --property hw_cpu_policy=shared</pre></div><div id="id-1.4.7.8.21.5.5.18" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>There is no correlation required between the NUMA topology exposed in the
instance and how the instance is actually pinned on the host. This is by
design. See this <a class="link" href="https://bugs.launchpad.net/nova/+bug/1466780" target="_blank">invalid bug</a> for more information.</p></div><p>For more information about image metadata, refer to the <a class="link" href="http://docs.openstack.org/image-guide/image-metadata.html" target="_blank">Image metadata</a>
guide.</p></div><div class="sect4 " id="id-1.4.7.8.21.5.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing instance CPU topologies</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.5.6">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.4.7.8.21.5.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>The functionality described below is currently only supported by the
libvirt/KVM driver.</p></div><p>In addition to configuring how an instance is scheduled on host CPUs, it is
possible to configure how CPUs are represented in the instance itself. By
default, when instance NUMA placement is not specified, a topology of N
sockets, each with one core and one thread, is used for an instance, where N
corresponds to the number of instance vCPUs requested. When instance NUMA
placement is specified, the number of sockets is fixed to the number of host
NUMA nodes to use and the total number of instance CPUs is split over these
sockets.</p><p>Some workloads benefit from a custom topology. For example, in some operating
systems, a different license may be needed depending on the number of CPU
sockets. To configure a flavor to use a maximum of two sockets, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:cpu_sockets=2</pre></div><p>Similarly, to configure a flavor to use one core and one thread, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large \
  --property hw:cpu_cores=1 \
  --property hw:cpu_threads=1</pre></div><div id="id-1.4.7.8.21.5.6.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>If specifying all values, the product of sockets multiplied by cores
multiplied by threads must equal the number of instance vCPUs. If specifying
any one of these values or the multiple of two values, the values must be a
factor of the number of instance vCPUs to prevent an exception. For example,
specifying <code class="literal">hw:cpu_sockets=2</code> on a host with an odd number of cores fails.
Similarly, specifying <code class="literal">hw:cpu_cores=2</code> and <code class="literal">hw:cpu_threads=4</code> on a host
with ten cores fails.</p></div><p>For more information about the syntax for <code class="literal">hw:cpu_sockets</code>, <code class="literal">hw:cpu_cores</code>
and <code class="literal">hw:cpu_threads</code>, refer to the <a class="link" href="http://docs.openstack.org/admin-guide/compute-flavors.html" target="_blank">Flavors</a> guide.</p><p>It is also possible to set upper limits on the number of sockets, cores, and
threads used. Unlike the hard values above, it is not necessary for this exact
number to used because it only provides a limit. This can be used to provide
some flexibility in scheduling, while ensuring certains limits are not
exceeded. For example, to ensure no more than two sockets are defined in the
instance topology, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property=hw:cpu_max_sockets=2</pre></div><p>For more information about the syntax for <code class="literal">hw:cpu_max_sockets</code>,
<code class="literal">hw:cpu_max_cores</code>, and <code class="literal">hw:cpu_max_threads</code>, refer to the <a class="link" href="http://docs.openstack.org/admin-guide/compute-flavors.html" target="_blank">Flavors</a>
guide.</p><p>Applications are frequently packaged as images. For applications that prefer
certain CPU topologies, configure image metadata to hint that created instances
should have a given topology regardless of flavor. To configure an image to
request a two-socket, four-core per socket topology, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_sockets=2 \
  --property hw_cpu_cores=4</pre></div><p>To constrain instances to a given limit of sockets, cores or threads, use the
<code class="literal">max_</code> variants. To configure an image to have a maximum of two sockets and a
maximum of one thread, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_max_sockets=2 \
  --property hw_cpu_max_threads=1</pre></div><p>Image metadata takes precedence over flavor extra specs. Configuring competing
constraints causes an exception. By setting a <code class="literal">max</code> value for sockets, cores,
or threads, administrators can prevent users configuring topologies that might,
for example, incur an additional licensing fees.</p><p>For more information about image metadata, refer to the <a class="link" href="http://docs.openstack.org/image-guide/image-metadata.html" target="_blank">Image metadata</a>
guide.</p></div></div><div class="sect3 " id="id-1.4.7.8.21.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.4.15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Huge pages</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.6">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The huge page feature in OpenStack provides important performance improvements
for applications that are highly memory IO-bound.</p><div id="id-1.4.7.8.21.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Huge pages may also be referred to hugepages or large pages, depending on
the source. These terms are synonyms.</p></div><div class="sect4 " id="id-1.4.7.8.21.6.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pages, the TLB and huge pages</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.6.4">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.8.21.6.4.2.1"><span class="term ">Pages</span></dt><dd><p>Physical memory is segmented into a series of contiguous regions called
pages. Each page contains a number of bytes, referred to as the page size.
The system retrieves memory by accessing entire pages, rather than byte by
byte.</p></dd><dt id="id-1.4.7.8.21.6.4.2.2"><span class="term ">Translation Lookaside Buffer (TLB)</span></dt><dd><p>A TLB is used to map the virtual addresses of pages to the physical addresses
in actual memory. The TLB is a cache and is not limitless, storing only the
most recent or frequently accessed pages. During normal operation, processes
will sometimes attempt to retrieve pages that are not stored in the cache.
This is known as a TLB miss and results in a delay as the processor iterates
through the pages themselves to find the missing address mapping.</p></dd><dt id="id-1.4.7.8.21.6.4.2.3"><span class="term ">Huge Pages</span></dt><dd><p>The standard page size in x86 systems is 4 kB. This is optimal for general
purpose computing but larger page sizes - 2 MB and 1 GB - are also available.
These larger page sizes are known as huge pages. Huge pages result in less
efficient memory usage as a process will not generally use all memory
available in each page. However, use of huge pages will result in fewer
overall pages and a reduced risk of TLB misses. For processes that have
significant memory requirements or are memory intensive, the benefits of huge
pages frequently outweigh the drawbacks.</p></dd><dt id="id-1.4.7.8.21.6.4.2.4"><span class="term ">Persistent Huge Pages</span></dt><dd><p>On Linux hosts, persistent huge pages are huge pages that are reserved
upfront. The HugeTLB provides for the mechanism for this upfront
configuration of huge pages. The HugeTLB allows for the allocation of varying
quantities of different huge page sizes. Allocation can be made at boot time
or run time. Refer to the <a class="link" href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_blank">Linux hugetlbfs guide</a> for more information.</p></dd><dt id="id-1.4.7.8.21.6.4.2.5"><span class="term ">Transparent Huge Pages (THP)</span></dt><dd><p>On Linux hosts, transparent huge pages are huge pages that are automatically
provisioned based on process requests. Transparent huge pages are provisioned
on a best effort basis, attempting to provision 2 MB huge pages if available
but falling back to 4 kB small pages if not. However, no upfront
configuration is necessary. Refer to the <a class="link" href="https://www.kernel.org/doc/Documentation/vm/transhuge.txt" target="_blank">Linux THP guide</a> for more
information.</p></dd></dl></div></div><div class="sect4 " id="id-1.4.7.8.21.6.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling huge pages on the host</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.6.5">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Persistent huge pages are required owing to their guaranteed availability.
However, persistent huge pages are not enabled by default in most environments.
The steps for enabling huge pages differ from platform to platform and only the
steps for Linux hosts are described here. On Linux hosts, the number of
persistent huge pages on the host can be queried by checking <code class="literal">/proc/meminfo</code>:</p><div class="verbatim-wrap"><pre class="screen">$ grep Huge /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</pre></div><p>In this instance, there are 0 persistent huge pages (<code class="literal">HugePages_Total</code>) and 0
transparent huge pages (<code class="literal">AnonHugePages</code>) allocated. Huge pages can be
allocated at boot time or run time. Huge pages require a contiguous area of
memory - memory that gets increasingly fragmented the long a host is running.
Identifying contiguous areas of memory is a issue for all huge page sizes, but
it's particularly problematic for larger huge page sizes such as 1 GB huge
pages. Allocating huge pages at boot time will ensure the correct number of huge
pages is always available, while allocating them at run time can fail if memory
has become too fragmented.</p><p>To allocate huge pages at run time, the kernel boot parameters must be extended
to include some huge page-specific parameters. This can be achieved by
modifying <code class="literal">/etc/default/grub</code> and appending the <code class="literal">hugepagesz</code>,
<code class="literal">hugepages</code>, and <code class="literal">transparent_hugepages=never</code> arguments to
<code class="literal">GRUB_CMDLINE_LINUX</code>. To allocate, for example, 2048 persistent 2 MB huge
pages at boot time, run:</p><div class="verbatim-wrap"><pre class="screen"># echo 'GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"' &gt; /etc/default/grub
$ grep GRUB_CMDLINE_LINUX /etc/default/grub
GRUB_CMDLINE_LINUX="..."
GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"</pre></div><div id="id-1.4.7.8.21.6.5.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>Persistent huge pages are not usable by standard host OS processes. Ensure
enough free, non-huge page memory is reserved for these processes.</p></div><p>Reboot the host, then validate that huge pages are now available:</p><div class="verbatim-wrap"><pre class="screen">$ grep "Huge" /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:    2048
HugePages_Free:     2048
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</pre></div><p>There are now 2048 2 MB huge pages totalling 4 GB of huge pages. These huge
pages must be mounted. On most platforms, this happens automatically. To verify
that the huge pages are mounted, run:</p><div class="verbatim-wrap"><pre class="screen"># mount | grep huge
hugetlbfs on /dev/hugepages type hugetlbfs (rw)</pre></div><p>In this instance, the huge pages are mounted at <code class="literal">/dev/hugepages</code>. This mount
point varies from platform to platform. If the above command did not return
anything, the hugepages must be mounted manually. To mount the huge pages at
<code class="literal">/dev/hugepages</code>, run:</p><div class="verbatim-wrap"><pre class="screen"># mkdir -p /dev/hugepages
# mount -t hugetlbfs hugetlbfs /dev/hugepages</pre></div><p>There are many more ways to configure huge pages, including allocating huge
pages at run time, specifying varying allocations for different huge page
sizes, or allocating huge pages from memory affinitized to different NUMA
nodes. For more information on configuring huge pages on Linux hosts, refer to
the <a class="link" href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt" target="_blank">Linux hugetlbfs guide</a>.</p></div><div class="sect4 " id="id-1.4.7.8.21.6.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.4.15.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing instance huge pages allocations</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.8.21.6.6">#</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.4.7.8.21.6.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>The functionality described below is currently only supported by the
libvirt/KVM driver.</p></div><div id="id-1.4.7.8.21.6.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>For performance reasons, configuring huge pages for an instance will
implicitly result in a NUMA topology being configured for the instance.
Configuring a NUMA topology for an instance requires enablement of
<code class="literal">NUMATopologyFilter</code>. Refer to <a class="xref" href="bk02ch05.html#sec-cpu-topo" title="5.4.15.2. CPU topologies">Section 5.4.15.2, “CPU topologies”</a> for more
information.</p></div><p>By default, an instance does not use huge pages for its underlying memory.
However, huge pages can bring important or required performance improvements
for some workloads. Huge pages must be requested explicitly through the use of
flavor extra specs or image metadata. To request an instance use huge pages,
run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:mem_page_size=large</pre></div><p>Different platforms offer different huge page sizes. For example: x86-based
platforms offer 2 MB and 1 GB huge page sizes. Specific huge page sizes can be
also be requested, with or without a unit suffix. The unit suffix must be one
of: Kb(it), Kib(it), Mb(it), Mib(it), Gb(it), Gib(it), Tb(it), Tib(it), KB,
KiB, MB, MiB, GB, GiB, TB, TiB. Where a unit suffix is not provided, Kilobytes
are assumed. To request an instance to use 2 MB huge pages, run one of:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:mem_page_size=2Mb</pre></div><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:mem_page_size=2048</pre></div><p>Enabling huge pages for an instance can have negative consequences for other
instances by consuming limited huge pages resources. To explicitly request
an instance use small pages, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:mem_page_size=small</pre></div><div id="id-1.4.7.8.21.6.6.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Explicitly requesting any page size will still result in a NUMA topology
being applied to the instance, as described earlier in this document.</p></div><p>Finally, to leave the decision of huge or small pages to the compute driver,
run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor set m1.large --property hw:mem_page_size=any</pre></div><p>For more information about the syntax for <code class="literal">hw:mem_page_size</code>, refer to the
<a class="link" href="http://docs.openstack.org/admin-guide/compute-flavors.html" target="_blank">Flavors</a> guide.</p><p>Applications are frequently packaged as images. For applications that require
the IO performance improvements that huge pages provides, configure image
metadata to ensure instances always request the specific page size regardless
of flavor. To configure an image to use 1 GB huge pages, run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image set [IMAGE_ID]  --property hw_mem_page_size=1GB</pre></div><p>Image metadata takes precedence over flavor extra specs. Thus, configuring
competing page sizes causes an exception. By setting a <code class="literal">small</code> page size
through image metadata, administrators can prevent users requesting huge pages
in flavors and impacting resource utilization. To configure this page size,
run:</p><div class="verbatim-wrap"><pre class="screen">$ openstack image set [IMAGE_ID] --property hw_mem_page_size=small</pre></div><div id="id-1.4.7.8.21.6.6.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Explicitly requesting any page size will still result in a NUMA topology
being applied to the instance, as described earlier in this document.</p></div><p>For more information about image metadata, refer to the <a class="link" href="http://docs.openstack.org/image-guide/image-metadata.html" target="_blank">Image metadata</a>
guide.</p></div></div></div></div><div class="sect1 " id="id-1.4.7.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshoot Compute</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Common problems for Compute typically involve misconfigured
networking or credentials that are not sourced properly in the
environment. Also, most flat networking configurations do not
enable <code class="command">ping</code> or <code class="command">ssh</code> from a compute node
to the instances that run on that node. Another common problem
is trying to run 32-bit images on a 64-bit compute node.
This section shows you how to troubleshoot Compute.</p><div class="sect2 " id="id-1.4.7.9.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute service logging</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.3">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Compute stores a log file for each service in
<code class="literal">/var/log/nova</code>. For example, <code class="literal">nova-compute.log</code>
is the log for the <code class="literal">nova-compute</code> service. You can set the
following options to format log strings for the <code class="literal">nova.log</code>
module in the <code class="literal">nova.conf</code> file:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <code class="literal">logging_context_format_string</code>
            </p></li><li class="listitem "><p>
              <code class="literal">logging_default_format_string</code>
            </p></li></ul></div><p>If the log level is set to <code class="literal">debug</code>, you can also specify
<code class="literal">logging_debug_format_suffix</code> to append extra formatting.
For information about what variables are available for the
formatter, see <a class="link" href="http://docs.python.org/library/logging.html#formatter-objects" target="_blank">Formatter Objects</a>.</p><p>You have two logging options for OpenStack Compute based on
configuration settings. In <code class="literal">nova.conf</code>, include the
<code class="literal">logfile</code> option to enable logging. Alternatively you can set
<code class="literal">use_syslog = 1</code> so that the nova daemon logs to syslog.</p></div><div class="sect2 " id="id-1.4.7.9.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Guru Meditation reports</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.4">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>A Guru Meditation report is sent by the Compute service upon receipt of the
<code class="literal">SIGUSR2</code> signal (<code class="literal">SIGUSR1</code> before Mitaka). This report is a
general-purpose error report that includes details about the current state
of the service. The error report is sent to <code class="literal">stderr</code>.</p><p>For example, if you redirect error output to <code class="literal">nova-api-err.log</code>
using <code class="command">nova-api 2&gt;/var/log/nova/nova-api-err.log</code>,
resulting in the process ID 8675, you can then run:</p><div class="verbatim-wrap"><pre class="screen"># kill -USR2 8675</pre></div><p>This command triggers the Guru Meditation report to be printed to
<code class="literal">/var/log/nova/nova-api-err.log</code>.</p><p>The report has the following sections:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Package: Displays information about the package to which the process
belongs, including version information.</p></li><li class="listitem "><p>Threads: Displays stack traces and thread IDs for each of the threads
within the process.</p></li><li class="listitem "><p>Green Threads: Displays stack traces for each of the green threads
within the process (green threads do not have thread IDs).</p></li><li class="listitem "><p>Configuration: Lists all configuration options currently accessible
through the CONF object for the current process.</p></li></ul></div><p>For more information, see <a class="link" href="http://docs.openstack.org/developer/nova/devref/gmr.html" target="_blank">Guru Meditation Reports</a>.</p></div><div class="sect2 " id="compute-common-errors-and-fixes"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Common errors and fixes for Compute</span> <a title="Permalink" class="permalink" href="bk02ch05.html#compute-common-errors-and-fixes">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span>compute-common-errors-and-fixes</li></ul></div></div></div></div><p>The <a class="link" href="http://ask.openstack.org" target="_blank">ask.openstack.org</a> site offers a place to ask
and answer questions, and you can also mark questions as frequently asked
questions. This section describes some errors people have posted previously.
Bugs are constantly being fixed, so online resources are a great way to get
the most up-to-date errors and fixes.</p></div><div class="sect2 " id="id-1.4.7.9.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Credential errors, 401, and 403 forbidden errors</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.6">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.9.6.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.6.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Missing credentials cause a <code class="literal">403 forbidden</code> error.</p></div><div class="sect3 " id="id-1.4.7.9.6.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.6.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To resolve this issue, use one of these methods:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.9.6.3.3.1.1.1"><span class="term ">Manual method</span></dt><dd><p>Gets the <code class="literal">novarc</code> file from the project ZIP file, saves existing
credentials in case of override, and manually sources the <code class="literal">novarc</code>
file.</p></dd></dl></div></li><li class="step "><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.7.9.6.3.3.2.1.1"><span class="term ">Script method</span></dt><dd><p>Generates <code class="literal">novarc</code> from the project ZIP file and sources it for you.</p></dd></dl></div></li></ol></div></div><p>When you run <code class="literal">nova-api</code> the first time, it generates the certificate
authority information, including <code class="literal">openssl.cnf</code>. If you
start the CA services before this, you might not be
able to create your ZIP file. Restart the services.
When your CA information is available, create your ZIP file.</p><p>Also, check your HTTP proxy settings to see whether they cause problems with
<code class="literal">novarc</code> creation.</p></div></div><div class="sect2 " id="id-1.4.7.9.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Instance errors</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.9.7.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.7.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Sometimes a particular instance shows <code class="literal">pending</code> or you cannot SSH to
it. Sometimes the image itself is the problem. For example, when you
use flat manager networking, you do not have a DHCP server and certain
images do not support interface injection; you cannot connect to
them.</p></div><div class="sect3 " id="id-1.4.7.9.7.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.7.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To fix instance errors use an image that does support
this method, such as Ubuntu, which obtains an IP address correctly
with FlatManager network settings.</p><p>To troubleshoot other possible problems with an instance, such as
an instance that stays in a spawning state, check the directory for
the particular instance under <code class="literal">/var/lib/nova/instances</code> on
the <code class="literal">nova-compute</code> host and make sure that these files are present:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
                <code class="literal">libvirt.xml</code>
              </p></li><li class="listitem "><p>
                <code class="literal">disk</code>
              </p></li><li class="listitem "><p>
                <code class="literal">disk-raw</code>
              </p></li><li class="listitem "><p>
                <code class="literal">kernel</code>
              </p></li><li class="listitem "><p>
                <code class="literal">ramdisk</code>
              </p></li><li class="listitem "><p><code class="literal">console.log</code>, after the instance starts.</p></li></ul></div><p>If any files are missing, empty, or very small, the <code class="literal">nova-compute</code>
service did not successfully download the images from the Image service.</p><p>Also check <code class="literal">nova-compute.log</code> for exceptions. Sometimes they do not
appear in the console output.</p><p>Next, check the log file for the instance in the <code class="literal">/var/log/libvirt/qemu</code>
directory to see if it exists and has any useful error messages in it.</p><p>Finally, from the <code class="literal">/var/lib/nova/instances</code> directory for the instance,
see if this command returns an error:</p><div class="verbatim-wrap"><pre class="screen"># virsh create libvirt.xml</pre></div></div></div><div class="sect2 " id="id-1.4.7.9.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Empty log output for Linux instances</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.8">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.9.8.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.8.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You can view the log output of running instances
from either the <span class="guimenu ">Log</span> tab of the dashboard or the output of
<code class="command">nova console-log</code>. In some cases, the log output of a running
Linux instance will be empty or only display a single character (for example,
the <code class="literal">?</code> character).</p><p>This occurs when the Compute service attempts to retrieve the log output
of the instance via a serial console while the instance itself is not
configured to send output to the console.</p></div><div class="sect3 " id="id-1.4.7.9.8.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.8.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To rectify this, append the following parameters to kernel arguments
specified in the instance's boot loader:</p><div class="verbatim-wrap highlight ini"><pre class="screen">console=tty0 console=ttyS0,115200n8</pre></div><p>Upon rebooting, the instance will be configured to send output to the Compute
service.</p></div></div><div class="sect2 " id="id-1.4.7.9.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reset the state of an instance</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.9">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.9.9.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.9.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Instances can remain in an intermediate state, such as <code class="literal">deleting</code>.</p></div><div class="sect3 " id="id-1.4.7.9.9.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.9.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You can use the <code class="command">nova reset-state</code> command to manually reset
the state of an instance to an error state. You can then delete the
instance. For example:</p><div class="verbatim-wrap"><pre class="screen">$ nova reset-state c6bbbf26-b40a-47e7-8d5c-eb17bf65c485
$ openstack server delete c6bbbf26-b40a-47e7-8d5c-eb17bf65c485</pre></div><p>You can also use the <code class="literal">--active</code> parameter to force the instance back
to an active state instead of an error state. For example:</p><div class="verbatim-wrap"><pre class="screen">$ nova reset-state --active c6bbbf26-b40a-47e7-8d5c-eb17bf65c485</pre></div></div></div><div class="sect2 " id="id-1.4.7.9.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Injection problems</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.10">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.9.10.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.10.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Instances may boot slowly, or do not boot. File injection can cause this
problem.</p></div><div class="sect3 " id="id-1.4.7.9.10.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.10.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To disable injection in libvirt, set the following in <code class="literal">nova.conf</code>:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[libvirt]
inject_partition = -2</pre></div><div id="id-1.4.7.9.10.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>If you have not enabled the configuration drive and
you want to make user-specified files available from
the metadata server for to improve performance and
avoid boot failure if injection fails, you must
disable injection.</p></div></div></div><div class="sect2 " id="id-1.4.7.9.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable live snapshotting</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.11">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.7.9.11.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.11.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Administrators using libvirt version <code class="literal">1.2.2</code> may experience problems
with live snapshot creation. Occasionally, libvirt version <code class="literal">1.2.2</code> fails
to create live snapshots under the load of creating concurrent snapshot.</p></div><div class="sect3 " id="id-1.4.7.9.11.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.5.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch05.html#id-1.4.7.9.11.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To effectively disable the libvirt live snapshotting, until the problem
is resolved, configure the <code class="literal">disable_libvirt_livesnapshot</code> option.
You can turn off the live snapshotting mechanism by setting up its value to
<code class="literal">True</code> in the <code class="literal">[workarounds]</code> section of the <code class="literal">nova.conf</code> file:</p><div class="verbatim-wrap highlight ini"><pre class="screen">[workarounds]
disable_libvirt_livesnapshot = True</pre></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="bk02ch06.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 6 </span>Object Storage</span></a><a class="nav-link" href="bk02ch04.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 4 </span>Dashboard</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2022 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>