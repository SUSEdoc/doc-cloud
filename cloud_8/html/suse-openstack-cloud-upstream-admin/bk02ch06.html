<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Object Storage | OpenStack Administrator Guide | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.2.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.81.0 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="OpenStack Administrator Guide" /><meta name="chapter-title" content="Chapter 6. Object Storage" /><meta name="description" content="OpenStack Object Storage (swift) is used for redundant, scalable data storage using clusters of standardized servers to store petabytes of accessible data. It is a long-term storage system for large amounts of static data which can be retrieved and updated. Object Storage uses a distributed architec…" /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" /><link rel="home" href="index.html" title="SUSE OpenStack Cloud Crowbar 8 Documentation" /><link rel="up" href="book-upstream-admin.html" title="OpenStack Administrator Guide" /><link rel="prev" href="bk02ch05.html" title="Chapter 5. Compute" /><link rel="next" href="bk02ch07.html" title="Chapter 7. Block Storage" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="OpenStack Administrator Guide"><span class="book-icon">OpenStack Administrator Guide</span></a><span> › </span><a class="crumb" href="bk02ch06.html">Object Storage</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>OpenStack Administrator Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="bk02ch01.html"><span class="number">1 </span><span class="name">Documentation Conventions</span></a></li><li class="inactive"><a href="bk02ch02.html"><span class="number">2 </span><span class="name">Get started with OpenStack</span></a></li><li class="inactive"><a href="cha-identity.html"><span class="number">3 </span><span class="name">Identity management</span></a></li><li class="inactive"><a href="bk02ch04.html"><span class="number">4 </span><span class="name">Dashboard</span></a></li><li class="inactive"><a href="bk02ch05.html"><span class="number">5 </span><span class="name">Compute</span></a></li><li class="inactive"><a href="bk02ch06.html"><span class="number">6 </span><span class="name">Object Storage</span></a></li><li class="inactive"><a href="bk02ch07.html"><span class="number">7 </span><span class="name">Block Storage</span></a></li><li class="inactive"><a href="bk02ch08.html"><span class="number">8 </span><span class="name">Shared File Systems</span></a></li><li class="inactive"><a href="networking.html"><span class="number">9 </span><span class="name">Networking</span></a></li><li class="inactive"><a href="bk02ch10.html"><span class="number">10 </span><span class="name">Telemetry</span></a></li><li class="inactive"><a href="bk02ch11.html"><span class="number">11 </span><span class="name">Database</span></a></li><li class="inactive"><a href="bk02ch12.html"><span class="number">12 </span><span class="name">Bare Metal</span></a></li><li class="inactive"><a href="bk02ch13.html"><span class="number">13 </span><span class="name">Orchestration</span></a></li><li class="inactive"><a href="osadm-os-cli.html"><span class="number">14 </span><span class="name">OpenStack command-line clients</span></a></li><li class="inactive"><a href="bk02ch15.html"><span class="number">15 </span><span class="name">Cross-project features</span></a></li><li class="inactive"><a href="bk02ch16.html"><span class="number">16 </span><span class="name">Appendix</span></a></li><li class="inactive"><a href="bk02go01.html"><span class="number"> </span><span class="name">Glossary</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 5. Compute" href="bk02ch05.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 7. Block Storage" href="bk02ch07.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="OpenStack Administrator Guide"><span class="book-icon">OpenStack Administrator Guide</span></a><span> › </span><a class="crumb" href="bk02ch06.html">Object Storage</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 5. Compute" href="bk02ch05.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 7. Block Storage" href="bk02ch07.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="id-1.4.8"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname ">SUSE OpenStack Cloud</span> <span class="productnumber ">8</span></div><div><h1 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage</span> <a title="Permalink" class="permalink" href="bk02ch06.html#">#</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.3"><span class="number">6.1 </span><span class="name">Introduction to Object Storage</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.4"><span class="number">6.2 </span><span class="name">Features and benefits</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.5"><span class="number">6.3 </span><span class="name">Object Storage characteristics</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.6"><span class="number">6.4 </span><span class="name">Components</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.7"><span class="number">6.5 </span><span class="name">Ring-builder</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.8"><span class="number">6.6 </span><span class="name">Cluster architecture</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.9"><span class="number">6.7 </span><span class="name">Replication</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.10"><span class="number">6.8 </span><span class="name">Large object support</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.11"><span class="number">6.9 </span><span class="name">Object Auditor</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.12"><span class="number">6.10 </span><span class="name">Erasure coding</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.13"><span class="number">6.11 </span><span class="name">Account reaper</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.14"><span class="number">6.12 </span><span class="name">Configure project-specific image locations with Object Storage</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.15"><span class="number">6.13 </span><span class="name">Object Storage monitoring</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.16"><span class="number">6.14 </span><span class="name">System administration for Object Storage</span></a></span></dt><dt><span class="sect1"><a href="bk02ch06.html#id-1.4.8.17"><span class="number">6.15 </span><span class="name">Troubleshoot Object Storage</span></a></span></dt></dl></div></div><div class="sect1 " id="id-1.4.8.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction to Object Storage</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.3">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>OpenStack Object Storage (swift) is used for redundant, scalable data
storage using clusters of standardized servers to store petabytes of
accessible data. It is a long-term storage system for large amounts of
static data which can be retrieved and updated. Object Storage uses a
distributed architecture
with no central point of control, providing greater scalability,
redundancy, and permanence. Objects are written to multiple hardware
devices, with the OpenStack software responsible for ensuring data
replication and integrity across the cluster. Storage clusters scale
horizontally by adding new nodes. Should a node fail, OpenStack works to
replicate its content from other active nodes. Because OpenStack uses
software logic to ensure data replication and distribution across
different devices, inexpensive commodity hard drives and servers can be
used in lieu of more expensive equipment.</p><p>Object Storage is ideal for cost effective, scale-out storage. It
provides a fully distributed, API-accessible storage platform that can
be integrated directly into applications or used for backup, archiving,
and data retention.</p></div><div class="sect1 " id="id-1.4.8.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Features and benefits</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.4">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                <p>Features</p>
              </th><th>
                <p>Benefits</p>
              </th></tr></thead><tbody><tr><td>
                <p>Leverages commodity hardware</p>
              </td><td>
                <p>No lock-in, lower price/GB.</p>
              </td></tr><tr><td>
                <p>HDD/node failure agnostic</p>
              </td><td>
                <p>Self-healing, reliable, data redundancy protects from failures.</p>
              </td></tr><tr><td>
                <p>Unlimited storage</p>
              </td><td>
                <p>Large and flat namespace, highly scalable read/write access,
able to serve content directly from storage system.</p>
              </td></tr><tr><td>
                <p>Multi-dimensional scalability</p>
              </td><td>
                <p>Scale-out architecture: Scale vertically and
horizontally-distributed storage. Backs up and archives large
amounts of data with linear performance.</p>
              </td></tr><tr><td>
                <p>Account/container/object structure</p>
              </td><td>
                <p>No nesting, not a traditional file system: Optimized for scale,
it scales to multiple petabytes and billions of objects.</p>
              </td></tr><tr><td>
                <p>Built-in replication 3✕ + data redundancy (compared with 2✕ on
RAID)</p>
              </td><td>
                <p>A configurable number of accounts, containers and object copies
for high availability.</p>
              </td></tr><tr><td>
                <p>Easily add capacity (unlike RAID resize)</p>
              </td><td>
                <p>Elastic data scaling with ease.</p>
              </td></tr><tr><td>
                <p>No central database</p>
              </td><td>
                <p>Higher performance, no bottlenecks.</p>
              </td></tr><tr><td>
                <p>RAID not required</p>
              </td><td>
                <p>Handle many small, random reads and writes efficiently.</p>
              </td></tr><tr><td>
                <p>Built-in management utilities</p>
              </td><td>
                <p>Account management: Create, add, verify, and delete users;
Container management: Upload, download, and verify; Monitoring:
Capacity, host, network, log trawling, and cluster health.</p>
              </td></tr><tr><td>
                <p>Drive auditing</p>
              </td><td>
                <p>Detect drive failures preempting data corruption.</p>
              </td></tr><tr><td>
                <p>Expiring objects</p>
              </td><td>
                <p>Users can set an expiration time or a TTL on an object to
control access.</p>
              </td></tr><tr><td>
                <p>Direct object access</p>
              </td><td>
                <p>Enable direct browser access to content, such as for a control
panel.</p>
              </td></tr><tr><td>
                <p>Realtime visibility into client requests</p>
              </td><td>
                <p>Know what users are requesting.</p>
              </td></tr><tr><td>
                <p>Supports S3 API</p>
              </td><td>
                <p>Utilize tools that were designed for the popular S3 API.</p>
              </td></tr><tr><td>
                <p>Restrict containers per account</p>
              </td><td>
                <p>Limit access to control usage by user.</p>
              </td></tr><tr><td>
                <p>Support for NetApp, Nexenta, Solidfire</p>
              </td><td>
                <p>Unified support for block volumes using a variety of storage
systems.</p>
              </td></tr><tr><td>
                <p>Snapshot and backup API for block volumes.</p>
              </td><td>
                <p>Data protection and recovery for VM data.</p>
              </td></tr><tr><td>
                <p>Standalone volume API available</p>
              </td><td>
                <p>Separate endpoint and API for integration with other compute
systems.</p>
              </td></tr><tr><td>
                <p>Integration with Compute</p>
              </td><td>
                <p>Fully integrated with Compute for attaching block volumes and
reporting on usage.</p>
              </td></tr></tbody></table></div></div><div class="sect1 " id="id-1.4.8.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage characteristics</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.5">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The key characteristics of Object Storage are that:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>All objects stored in Object Storage have a URL.</p></li><li class="listitem "><p>All objects stored are replicated 3✕ in as-unique-as-possible zones,
which can be defined as a group of drives, a node, a rack, and so on.</p></li><li class="listitem "><p>All objects have their own metadata.</p></li><li class="listitem "><p>Developers interact with the object storage system through a RESTful
HTTP API.</p></li><li class="listitem "><p>Object data can be located anywhere in the cluster.</p></li><li class="listitem "><p>The cluster scales by adding additional nodes without sacrificing
performance, which allows a more cost-effective linear storage
expansion than fork-lift upgrades.</p></li><li class="listitem "><p>Data does not have to be migrated to an entirely new storage system.</p></li><li class="listitem "><p>New nodes can be added to the cluster without downtime.</p></li><li class="listitem "><p>Failed nodes and disks can be swapped out without downtime.</p></li><li class="listitem "><p>It runs on industry-standard hardware, such as Dell, HP, and
Supermicro.</p></li></ul></div><p>Object Storage (swift)</p><div class="figure" id="id-1.4.8.5.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage.png" target="_blank"><img src="images/objectstorage.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.1: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.5.5">#</a></h6></div></div><p>Developers can either write directly to the Swift API or use one of the
many client libraries that exist for all of the popular programming
languages, such as Java, Python, Ruby, and C#. Amazon S3 and RackSpace
Cloud Files users should be very familiar with Object Storage. Users new
to object storage systems will have to adjust to a different approach
and mindset than those required for a traditional filesystem.</p></div><div class="sect1 " id="id-1.4.8.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Components</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Object Storage uses the following components to deliver high
availability, high durability, and high concurrency:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="bold"><strong>Proxy servers</strong></span> - Handle all of the incoming API requests.</p></li><li class="listitem "><p><span class="bold"><strong>Rings</strong></span> - Map logical names of data to locations on particular
disks.</p></li><li class="listitem "><p><span class="bold"><strong>Zones</strong></span> - Isolate data from other zones. A failure in one zone
does not impact the rest of the cluster as data replicates
across zones.</p></li><li class="listitem "><p><span class="bold"><strong>Accounts and containers</strong></span> - Each account and container are
individual databases that are distributed across the cluster. An
account database contains the list of containers in that account. A
container database contains the list of objects in that container.</p></li><li class="listitem "><p><span class="bold"><strong>Objects</strong></span> - The data itself.</p></li><li class="listitem "><p><span class="bold"><strong>Partitions</strong></span> - A partition stores objects, account databases, and
container databases and helps manage locations where data lives in
the cluster.</p></li></ul></div><p>
        <span class="bold"><strong>Object Storage building blocks</strong></span>
      </p><div class="figure" id="id-1.4.8.6.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-buildingblocks.png" target="_blank"><img src="images/objectstorage-buildingblocks.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.2: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.5">#</a></h6></div></div><div class="sect2 " id="id-1.4.8.6.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Proxy servers</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.6">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Proxy servers are the public face of Object Storage and handle all of
the incoming API requests. Once a proxy server receives a request, it
determines the storage node based on the object's URL, for example:
<a class="link" href="https://swift.example.com/v1/account/container/object" target="_blank">https://swift.example.com/v1/account/container/object</a>. Proxy servers
also coordinate responses, handle failures, and coordinate timestamps.</p><p>Proxy servers use a shared-nothing architecture and can be scaled as
needed based on projected workloads. A minimum of two proxy servers
should be deployed for redundancy. If one proxy server fails, the others
take over.</p><p>For more information concerning proxy server configuration, see
<a class="link" href="http://docs.openstack.org/newton/config-reference/object-storage/proxy-server.html" target="_blank">Configuration Reference</a>.</p></div><div class="sect2 " id="id-1.4.8.6.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rings</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>A ring represents a mapping between the names of entities stored on disks
and their physical locations. There are separate rings for accounts,
containers, and objects. When other components need to perform any
operation on an object, container, or account, they need to interact
with the appropriate ring to determine their location in the cluster.</p><p>The ring maintains this mapping using zones, devices, partitions, and
replicas. Each partition in the ring is replicated, by default, three
times across the cluster, and partition locations are stored in the
mapping maintained by the ring. The ring is also responsible for
determining which devices are used for handoff in failure scenarios.</p><p>Data can be isolated into zones in the ring. Each partition replica is
guaranteed to reside in a different zone. A zone could represent a
drive, a server, a cabinet, a switch, or even a data center.</p><p>The partitions of the ring are equally divided among all of the devices
in the Object Storage installation. When partitions need to be moved
around (for example, if a device is added to the cluster), the ring
ensures that a minimum number of partitions are moved at a time, and
only one replica of a partition is moved at a time.</p><p>You can use weights to balance the distribution of partitions on drives
across the cluster. This can be useful, for example, when differently
sized drives are used in a cluster.</p><p>The ring is used by the proxy server and several background processes
(like replication).</p><p>
          <span class="bold"><strong>The ring</strong></span>
        </p><div class="figure" id="id-1.4.8.6.7.9"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-ring.png" target="_blank"><img src="images/objectstorage-ring.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.3: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.7.9">#</a></h6></div></div><p>These rings are externally managed. The server processes themselves
do not modify the rings, they are instead given new rings modified by
other tools.</p><p>The ring uses a configurable number of bits from an <code class="literal">MD5</code> hash for a path
as a partition index that designates a device. The number of bits kept
from the hash is known as the partition power, and 2 to the partition
power indicates the partition count. Partitioning the full <code class="literal">MD5</code> hash ring
allows other parts of the cluster to work in batches of items at once
which ends up either more efficient or at least less complex than
working with each item separately or the entire cluster all at once.</p><p>Another configurable value is the replica count, which indicates how
many of the partition-device assignments make up a single ring. For a
given partition number, each replica's device will not be in the same
zone as any other replica's device. Zones can be used to group devices
based on physical locations, power separations, network separations, or
any other attribute that would improve the availability of multiple
replicas at the same time.</p></div><div class="sect2 " id="id-1.4.8.6.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Zones</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.8">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Object Storage allows configuring zones in order to isolate failure
boundaries. If possible, each data replica resides in a separate zone.
At the smallest level, a zone could be a single drive or a grouping of a
few drives. If there were five object storage servers, then each server
would represent its own zone. Larger deployments would have an entire
rack (or multiple racks) of object servers, each representing a zone.
The goal of zones is to allow the cluster to tolerate significant
outages of storage servers without losing all replicas of the data.</p><p>
          <span class="bold"><strong>Zones</strong></span>
        </p><div class="figure" id="id-1.4.8.6.8.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-zones.png" target="_blank"><img src="images/objectstorage-zones.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.4: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.8.4">#</a></h6></div></div></div><div class="sect2 " id="id-1.4.8.6.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accounts and containers</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.9">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Each account and container is an individual SQLite database that is
distributed across the cluster. An account database contains the list of
containers in that account. A container database contains the list of
objects in that container.</p><p>
          <span class="bold"><strong>Accounts and containers</strong></span>
        </p><div class="figure" id="id-1.4.8.6.9.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-accountscontainers.png" target="_blank"><img src="images/objectstorage-accountscontainers.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.5: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.9.4">#</a></h6></div></div><p>To keep track of object data locations, each account in the system has a
database that references all of its containers, and each container
database references each object.</p></div><div class="sect2 " id="id-1.4.8.6.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Partitions</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.10">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>A partition is a collection of stored data. This includes account databases,
container databases, and objects. Partitions are core to the replication
system.</p><p>Think of a partition as a bin moving throughout a fulfillment center
warehouse. Individual orders get thrown into the bin. The system treats
that bin as a cohesive entity as it moves throughout the system. A bin
is easier to deal with than many little things. It makes for fewer
moving parts throughout the system.</p><p>System replicators and object uploads/downloads operate on partitions.
As the system scales up, its behavior continues to be predictable
because the number of partitions is a fixed number.</p><p>Implementing a partition is conceptually simple, a partition is just a
directory sitting on a disk with a corresponding hash table of what it
contains.</p><p>
          <span class="bold"><strong>Partitions</strong></span>
        </p><div class="figure" id="id-1.4.8.6.10.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-partitions.png" target="_blank"><img src="images/objectstorage-partitions.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.6: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.10.7">#</a></h6></div></div></div><div class="sect2 " id="id-1.4.8.6.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replicators</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.11">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In order to ensure that there are three copies of the data everywhere,
replicators continuously examine each partition. For each local
partition, the replicator compares it against the replicated copies in
the other zones to see if there are any differences.</p><p>The replicator knows if replication needs to take place by examining
hashes. A hash file is created for each partition, which contains hashes
of each directory in the partition. Each of the three hash files is
compared. For a given partition, the hash files for each of the
partition's copies are compared. If the hashes are different, then it is
time to replicate, and the directory that needs to be replicated is
copied over.</p><p>This is where partitions come in handy. With fewer things in the system,
larger chunks of data are transferred around (rather than lots of little
TCP connections, which is inefficient) and there is a consistent number
of hashes to compare.</p><p>The cluster eventually has a consistent behavior where the newest data
has a priority.</p><p>
          <span class="bold"><strong>Replication</strong></span>
        </p><div class="figure" id="id-1.4.8.6.11.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-replication.png" target="_blank"><img src="images/objectstorage-replication.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.7: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.11.7">#</a></h6></div></div><p>If a zone goes down, one of the nodes containing a replica notices and
proactively copies data to a handoff location.</p></div><div class="sect2 " id="id-1.4.8.6.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use cases</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.12">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The following sections show use cases for object uploads and downloads
and introduce the components.</p><div class="sect3 " id="id-1.4.8.6.12.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.4.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upload</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.12.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>A client uses the REST API to make a HTTP request to PUT an object into
an existing container. The cluster receives the request. First, the
system must figure out where the data is going to go. To do this, the
account name, container name, and object name are all used to determine
the partition where this object should live.</p><p>Then a lookup in the ring figures out which storage nodes contain the
partitions in question.</p><p>The data is then sent to each storage node where it is placed in the
appropriate partition. At least two of the three writes must be
successful before the client is notified that the upload was successful.</p><p>Next, the container database is updated asynchronously to reflect that
there is a new object in it.</p><p>
            <span class="bold"><strong>Object Storage in use</strong></span>
          </p><div class="figure" id="id-1.4.8.6.12.3.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-usecase.png" target="_blank"><img src="images/objectstorage-usecase.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.8: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.12.3.7">#</a></h6></div></div></div><div class="sect3 " id="id-1.4.8.6.12.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.4.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Download</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.6.12.4">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>A request comes in for an account/container/object. Using the same
consistent hashing, the partition name is generated. A lookup in the
ring reveals which storage nodes contain that partition. A request is
made to one of the storage nodes to fetch the object and, if that fails,
requests are made to the other nodes.</p></div></div></div><div class="sect1 " id="id-1.4.8.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ring-builder</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.7">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Use the swift-ring-builder utility to build and manage rings. This
utility assigns partitions to devices and writes an optimized Python
structure to a gzipped, serialized file on disk for transmission to the
servers. The server processes occasionally check the modification time
of the file and reload in-memory copies of the ring structure as needed.
If you use a slightly older version of the ring, one of the three
replicas for a partition subset will be incorrect because of the way the
ring-builder manages changes to the ring. You can work around this
issue.</p><p>The ring-builder also keeps its own builder file with the ring
information and additional data required to build future rings. It is
very important to keep multiple backup copies of these builder files.
One option is to copy the builder files out to every server while
copying the ring files themselves. Another is to upload the builder
files into the cluster itself. If you lose the builder file, you have to
create a new ring from scratch. Nearly all partitions would be assigned
to different devices and, therefore, nearly all of the stored data would
have to be replicated to new locations. So, recovery from a builder file
loss is possible, but data would be unreachable for an extended time.</p><div class="sect2 " id="id-1.4.8.7.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ring data structure</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.7.4">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The ring data structure consists of three top level fields: a list of
devices in the cluster, a list of lists of device ids indicating
partition to device assignments, and an integer indicating the number of
bits to shift an MD5 hash to calculate the partition for the hash.</p></div><div class="sect2 " id="id-1.4.8.7.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Partition assignment list</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.7.5">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>This is a list of <code class="literal">array('H')</code> of devices ids. The outermost list
contains an <code class="literal">array('H')</code> for each replica. Each <code class="literal">array('H')</code> has a
length equal to the partition count for the ring. Each integer in the
<code class="literal">array('H')</code> is an index into the above list of devices. The partition
list is known internally to the Ring class as <code class="literal">_replica2part2dev_id</code>.</p><p>So, to create a list of device dictionaries assigned to a partition, the
Python code would look like:</p><div class="verbatim-wrap highlight python"><pre class="screen">devices = [self.devs[part2dev_id[partition]] for
part2dev_id in self._replica2part2dev_id]</pre></div><p>That code is a little simplistic because it does not account for the
removal of duplicate devices. If a ring has more replicas than devices,
a partition will have more than one replica on a device.</p><p><code class="literal">array('H')</code> is used for memory conservation as there may be millions
of partitions.</p></div><div class="sect2 " id="id-1.4.8.7.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overload</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.7.6">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The ring builder tries to keep replicas as far apart as possible while
still respecting device weights. When it can not do both, the overload
factor determines what happens. Each device takes an extra
fraction of its desired partitions to allow for replica dispersion;
after that extra fraction is exhausted, replicas are placed closer
together than optimal.</p><p>The overload factor lets the operator trade off replica
dispersion (durability) against data dispersion (uniform disk usage).</p><p>The default overload factor is 0, so device weights are strictly
followed.</p><p>With an overload factor of 0.1, each device accepts 10% more
partitions than it otherwise would, but only if it needs to maintain
partition dispersion.</p><p>For example, consider a 3-node cluster of machines with equal-size disks;
node A has 12 disks, node B has 12 disks, and node C has
11 disks. The ring has an overload factor of 0.1 (10%).</p><p>Without the overload, some partitions would end up with replicas only
on nodes A and B. However, with the overload, every device can accept
up to 10% more partitions for the sake of dispersion. The
missing disk in C means there is one disk's worth of partitions
to spread across the remaining 11 disks, which gives each
disk in C an extra 9.09% load. Since this is less than the 10%
overload, there is one replica of each partition on each node.</p><p>However, this does mean that the disks in node C have more data
than the disks in nodes A and B. If 80% full is the warning
threshold for the cluster, node C's disks reach 80% full while A
and B's disks are only 72.7% full.</p></div><div class="sect2 " id="id-1.4.8.7.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replica counts</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.7.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>To support the gradual change in replica counts, a ring can have a real
number of replicas and is not restricted to an integer number of
replicas.</p><p>A fractional replica count is for the whole ring and not for individual
partitions. It indicates the average number of replicas for each
partition. For example, a replica count of 3.2 means that 20 percent of
partitions have four replicas and 80 percent have three replicas.</p><p>The replica count is adjustable. For example:</p><div class="verbatim-wrap"><pre class="screen">$ swift-ring-builder account.builder set_replicas 4
$ swift-ring-builder account.builder rebalance</pre></div><p>Removing unneeded replicas saves on
the cost of disks.</p><p>You can gradually increase the replica count at a rate that does not
adversely affect cluster performance. For example:</p><div class="verbatim-wrap"><pre class="screen">$ swift-ring-builder object.builder set_replicas 3.01
$ swift-ring-builder object.builder rebalance
&lt;distribute rings and wait&gt;...

$ swift-ring-builder object.builder set_replicas 3.02
$ swift-ring-builder object.builder rebalance
&lt;distribute rings and wait&gt;...</pre></div><p>Changes take effect after the ring is rebalanced. Therefore, if you
intend to change from 3 replicas to 3.01 but you accidentally type
2.01, no data is lost.</p><p>Additionally, the <code class="command">swift-ring-builder X.builder create</code> command can
now take a decimal argument for the number of replicas.</p></div><div class="sect2 " id="id-1.4.8.7.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Partition shift value</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.7.8">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The partition shift value is known internally to the Ring class as
<code class="literal">_part_shift</code>. This value is used to shift an MD5 hash to calculate
the partition where the data for that hash should reside. Only the top
four bytes of the hash is used in this process. For example, to compute
the partition for the <code class="literal">/account/container/object</code> path using Python:</p><div class="verbatim-wrap highlight python"><pre class="screen">partition = unpack_from('&gt;I',
md5('/account/container/object').digest())[0] &gt;&gt;
self._part_shift</pre></div><p>For a ring generated with part_power P, the partition shift value is
<code class="literal">32 - P</code>.</p></div><div class="sect2 " id="id-1.4.8.7.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Build the ring</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.7.9">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The ring builder process includes these high-level steps:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>The utility calculates the number of partitions to assign to each
device based on the weight of the device. For example, for a
partition at the power of 20, the ring has 1,048,576 partitions. One
thousand devices of equal weight each want 1,048.576 partitions. The
devices are sorted by the number of partitions they desire and kept
in order throughout the initialization process.</p><div id="id-1.4.8.7.9.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>Each device is also assigned a random tiebreaker value that is
used when two devices desire the same number of partitions. This
tiebreaker is not stored on disk anywhere, and so two different
rings created with the same parameters will have different
partition assignments. For repeatable partition assignments,
<code class="literal">RingBuilder.rebalance()</code> takes an optional seed value that
seeds the Python pseudo-random number generator.</p></div></li><li class="step "><p>The ring builder assigns each partition replica to the device that
requires most partitions at that point while keeping it as far away
as possible from other replicas. The ring builder searches for a
device in a different zone, or on a different server. If it does not
find one, it looks for a device with no replicas. Finally, if all
options are exhausted, the ring builder assigns the replica to the
device that has the fewest replicas already assigned.</p><div id="id-1.4.8.7.9.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The ring builder assigns multiple replicas to one device only if
the ring has fewer devices than it has replicas.</p></div></li><li class="step "><p>When building a new ring from an old ring, the ring builder
recalculates the desired number of partitions that each device wants.</p></li><li class="step "><p>The ring builder unassigns partitions and gathers these partitions
for reassignment, as follows:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The ring builder unassigns any assigned partitions from any
removed devices and adds these partitions to the gathered list.</p></li><li class="listitem "><p>The ring builder unassigns any partition replicas that can be
spread out for better durability and adds these partitions to the
gathered list.</p></li><li class="listitem "><p>The ring builder unassigns random partitions from any devices that
have more partitions than they need and adds these partitions to
the gathered list.</p></li></ul></div></li><li class="step "><p>The ring builder reassigns the gathered partitions to devices by
using a similar method to the one described previously.</p></li><li class="step "><p>When the ring builder reassigns a replica to a partition, the ring
builder records the time of the reassignment. The ring builder uses
this value when it gathers partitions for reassignment so that no
partition is moved twice in a configurable amount of time. The
RingBuilder class knows this configurable amount of time as
<code class="literal">min_part_hours</code>. The ring builder ignores this restriction for
replicas of partitions on removed devices because removal of a device
happens on device failure only, and reassignment is the only choice.</p></li></ol></div></div><p>These steps do not always perfectly rebalance a ring due to the random
nature of gathering partitions for reassignment. To help reach a more
balanced ring, the rebalance process is repeated until near perfect
(less than 1 percent off) or when the balance does not improve by at
least 1 percent (indicating we probably cannot get perfect balance due
to wildly imbalanced zones or too many partitions recently moved).</p></div></div><div class="sect1 " id="id-1.4.8.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster architecture</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.8">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect2 " id="id-1.4.8.8.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access tier</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.8.2">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Large-scale deployments segment off an access tier, which is considered
the Object Storage system's central hub. The access tier fields the
incoming API requests from clients and moves data in and out of the
system. This tier consists of front-end load balancers, ssl-terminators,
and authentication services. It runs the (distributed) brain of the
Object Storage system: the proxy server processes.</p><div id="id-1.4.8.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>If you want to use OpenStack Identity API v3 for authentication, you
have the following options available in <code class="literal">/etc/swift/dispersion.conf</code>:
<code class="literal">auth_version</code>, <code class="literal">user_domain_name</code>, <code class="literal">project_domain_name</code>,
and <code class="literal">project_name</code>.</p></div><p>
          <span class="bold"><strong>Object Storage architecture</strong></span>
        </p><div class="figure" id="id-1.4.8.8.2.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-arch.png" target="_blank"><img src="images/objectstorage-arch.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.9: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.8.2.5">#</a></h6></div></div><p>Because access servers are collocated in their own tier, you can scale
out read/write access regardless of the storage capacity. For example,
if a cluster is on the public Internet, requires SSL termination, and
has a high demand for data access, you can provision many access
servers. However, if the cluster is on a private network and used
primarily for archival purposes, you need fewer access servers.</p><p>Since this is an HTTP addressable storage service, you may incorporate a
load balancer into the access tier.</p><p>Typically, the tier consists of a collection of 1U servers. These
machines use a moderate amount of RAM and are network I/O intensive.
Since these systems field each incoming API request, you should
provision them with two high-throughput (10GbE) interfaces - one for the
incoming <code class="literal">front-end</code> requests and the other for the <code class="literal">back-end</code> access to
the object storage nodes to put and fetch data.</p><div class="sect3 " id="id-1.4.8.8.2.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Factors to consider</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.8.2.9">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>For most publicly facing deployments as well as private deployments
available across a wide-reaching corporate network, you use SSL to
encrypt traffic to the client. SSL adds significant processing load to
establish sessions between clients, which is why you have to provision
more capacity in the access layer. SSL may not be required for private
deployments on trusted networks.</p></div></div><div class="sect2 " id="id-1.4.8.8.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage nodes</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.8.3">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In most configurations, each of the five zones should have an equal
amount of storage capacity. Storage nodes use a reasonable amount of
memory and CPU. Metadata needs to be readily available to return objects
quickly. The object stores run services not only to field incoming
requests from the access tier, but to also run replicators, auditors,
and reapers. You can provision object stores provisioned with single
gigabit or 10 gigabit network interface depending on the expected
workload and desired performance.</p><p>
          <span class="bold"><strong>Object Storage (swift)</strong></span>
        </p><div class="figure" id="id-1.4.8.8.3.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/objectstorage-nodes.png" target="_blank"><img src="images/objectstorage-nodes.png" width="" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.10: </span><span class="name"> </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.8.3.4">#</a></h6></div></div><p>Currently, a 2 TB or 3 TB SATA disk delivers good performance for the
price. You can use desktop-grade drives if you have responsive remote
hands in the datacenter and enterprise-grade drives if you don't.</p><div class="sect3 " id="id-1.4.8.8.3.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Factors to consider</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.8.3.6">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You should keep in mind the desired I/O performance for single-threaded
requests. This system does not use RAID, so a single disk handles each
request for an object. Disk performance impacts single-threaded response
rates.</p><p>To achieve apparent higher throughput, the object storage system is
designed to handle concurrent uploads/downloads. The network I/O
capacity (1GbE, bonded 1GbE pair, or 10GbE) should match your desired
concurrent throughput needs for reads and writes.</p></div></div></div><div class="sect1 " id="id-1.4.8.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replication</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.9">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Because each replica in Object Storage functions independently and
clients generally require only a simple majority of nodes to respond to
consider an operation successful, transient failures like network
partitions can quickly cause replicas to diverge. These differences are
eventually reconciled by asynchronous, peer-to-peer replicator
processes. The replicator processes traverse their local file systems
and concurrently perform operations in a manner that balances load
across physical disks.</p><p>Replication uses a push model, with records and files generally only
being copied from local to remote replicas. This is important because
data on the node might not belong there (as in the case of hand offs and
ring changes), and a replicator cannot know which data it should pull in
from elsewhere in the cluster. Any node that contains data must ensure
that data gets to where it belongs. The ring handles replica placement.</p><p>To replicate deletions in addition to creations, every deleted record or
file in the system is marked by a tombstone. The replication process
cleans up tombstones after a time period known as the <code class="literal">consistency
window</code>. This window defines the duration of the replication and how
long transient failure can remove a node from the cluster. Tombstone
cleanup must be tied to replication to reach replica convergence.</p><p>If a replicator detects that a remote drive has failed, the replicator
uses the <code class="literal">get_more_nodes</code> interface for the ring to choose an
alternate node with which to synchronize. The replicator can maintain
desired levels of replication during disk failures, though some replicas
might not be in an immediately usable location.</p><div id="id-1.4.8.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>The replicator does not maintain desired levels of replication when
failures such as entire node failures occur; most failures are
transient.</p></div><p>The main replication types are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.8.9.8.1.1.1"><span class="term ">Database replication</span></dt><dd><p>Replicates containers and objects.</p></dd></dl></div></li><li class="listitem "><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.8.9.8.2.1.1"><span class="term ">Object replication</span></dt><dd><p>Replicates object data.</p></dd></dl></div></li></ul></div><div class="sect2 " id="id-1.4.8.9.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Database replication</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.9.9">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Database replication completes a low-cost hash comparison to determine
whether two replicas already match. Normally, this check can quickly
verify that most databases in the system are already synchronized. If
the hashes differ, the replicator synchronizes the databases by sharing
records added since the last synchronization point.</p><p>This synchronization point is a high water mark that notes the last
record at which two databases were known to be synchronized, and is
stored in each database as a tuple of the remote database ID and record
ID. Database IDs are unique across all replicas of the database, and
record IDs are monotonically increasing integers. After all new records
are pushed to the remote database, the entire synchronization table of
the local database is pushed, so the remote database can guarantee that
it is synchronized with everything with which the local database was
previously synchronized.</p><p>If a replica is missing, the whole local database file is transmitted to
the peer by using rsync(1) and is assigned a new unique ID.</p><p>In practice, database replication can process hundreds of databases per
concurrency setting per second (up to the number of available CPUs or
disks) and is bound by the number of database transactions that must be
performed.</p></div><div class="sect2 " id="id-1.4.8.9.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object replication</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.9.10">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The initial implementation of object replication performed an rsync to
push data from a local partition to all remote servers where it was
expected to reside. While this worked at small scale, replication times
skyrocketed once directory structures could no longer be held in RAM.
This scheme was modified to save a hash of the contents for each suffix
directory to a per-partition hashes file. The hash for a suffix
directory is no longer valid when the contents of that suffix directory
is modified.</p><p>The object replication process reads in hash files and calculates any
invalidated hashes. Then, it transmits the hashes to each remote server
that should hold the partition, and only suffix directories with
differing hashes on the remote server are rsynced. After pushing files
to the remote server, the replication process notifies it to recalculate
hashes for the rsynced suffix directories.</p><p>The number of uncached directories that object replication must
traverse, usually as a result of invalidated suffix directory hashes,
impedes performance. To provide acceptable replication speeds, object
replication is designed to invalidate around 2 percent of the hash space
on a normal node each day.</p></div></div><div class="sect1 " id="id-1.4.8.10"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Large object support</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.10">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Object Storage (swift) uses segmentation to support the upload of large
objects. By default, Object Storage limits the download size of a single
object to 5GB. Using segmentation, uploading a single object is virtually
unlimited. The segmentation process works by fragmenting the object,
and automatically creating a file that sends the segments together as
a single object. This option offers greater upload speed with the possibility
of parallel uploads.</p><div class="sect2 " id="id-1.4.8.10.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Large objects</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.10.3">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The large object is comprised of two types of objects:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="bold"><strong>Segment objects</strong></span> store the object content. You can divide your
content into segments, and upload each segment into its own segment
object. Segment objects do not have any special features. You create,
update, download, and delete segment objects just as you would normal
objects.</p></li><li class="listitem "><p>A <span class="bold"><strong>manifest object</strong></span> links the segment objects into one logical
large object. When you download a manifest object, Object Storage
concatenates and returns the contents of the segment objects in the
response body of the request. The manifest object types are:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
                  <span class="bold"><strong>Static large objects</strong></span>
                </p></li><li class="listitem "><p>
                  <span class="bold"><strong>Dynamic large objects</strong></span>
                </p></li></ul></div></li></ul></div><p>To find out more information on large object support, see <a class="link" href="http://docs.openstack.org/user-guide/cli-swift-large-object-creation.html" target="_blank">Large objects</a>
in the OpenStack End User Guide, or <a class="link" href="http://docs.openstack.org/developer/swift/overview_large_objects.html" target="_blank">Large Object Support</a>
in the developer documentation.</p></div></div><div class="sect1 " id="id-1.4.8.11"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Auditor</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.11">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>On system failures, the XFS file system can sometimes truncate files it is
trying to write and produce zero-byte files. The object-auditor will catch
these problems but in the case of a system crash it is advisable to run
an extra, less rate limited sweep, to check for these specific files.
You can run this command as follows:</p><div class="verbatim-wrap"><pre class="screen">$ swift-object-auditor /path/to/object-server/config/file.conf once -z 1000</pre></div><div id="id-1.4.8.11.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>"-z" means to only check for zero-byte files at 1000 files per second.</p></div><p>It is useful to run the object auditor on a specific device or set of devices.
You can run the object-auditor once as follows:</p><div class="verbatim-wrap"><pre class="screen">$ swift-object-auditor /path/to/object-server/config/file.conf once \
  --devices=sda,sdb</pre></div><div id="id-1.4.8.11.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>This will run the object auditor on only the <code class="literal">sda</code> and <code class="literal">sdb</code> devices.
This parameter accepts a comma-separated list of values.</p></div></div><div class="sect1 " id="id-1.4.8.12"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Erasure coding</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.12">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Erasure coding is a set of algorithms that allows the reconstruction of
missing data from a set of original data. In theory, erasure coding uses
less capacity with similar durability characteristics as replicas.
From an application perspective, erasure coding support is transparent.
Object Storage (swift) implements erasure coding as a Storage Policy.
See <a class="link" href="http://docs.openstack.org/developer/swift/overview_policies.html" target="_blank">Storage Policies</a>
for more details.</p><p>There is no external API related to erasure coding. Create a container using a
Storage Policy; the interaction with the cluster is the same as any
other durability policy. Because support implements as a Storage Policy,
you can isolate all storage devices that associate with your cluster's
erasure coding capability. It is entirely possible to share devices between
storage policies, but for erasure coding it may make more sense to use
not only separate devices but possibly even entire nodes dedicated for erasure
coding.</p><div id="id-1.4.8.12.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>The erasure code support in Object Storage is considered beta in Kilo.
Most major functionality is included, but it has not been tested or
validated at large scale. This feature relies on <code class="literal">ssync</code> for durability.
We recommend deployers do extensive testing and not deploy production
data using an erasure code storage policy.
If any bugs are found during testing, please report them to
<a class="link" href="https://bugs.launchpad.net/swift" target="_blank">https://bugs.launchpad.net/swift</a></p></div></div><div class="sect1 " id="id-1.4.8.13"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Account reaper</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.13">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The purpose of the account reaper is to remove data from the deleted accounts.</p><p>A reseller marks an account for deletion by issuing a <code class="literal">DELETE</code> request
on the account's storage URL. This action sets the <code class="literal">status</code> column of
the account_stat table in the account database and replicas to
<code class="literal">DELETED</code>, marking the account's data for deletion.</p><p>Typically, a specific retention time or undelete are not provided.
However, you can set a <code class="literal">delay_reaping</code> value in the
<code class="literal">[account-reaper]</code> section of the <code class="literal">account-server.conf</code> file to
delay the actual deletion of data. At this time, to undelete you have to update
the account database replicas directly, set the status column to an
empty string and update the put_timestamp to be greater than the
delete_timestamp.</p><div id="id-1.4.8.13.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>It is on the development to-do list to write a utility that performs
this task, preferably through a REST call.</p></div><p>The account reaper runs on each account server and scans the server
occasionally for account databases marked for deletion. It only fires up
on the accounts for which the server is the primary node, so that
multiple account servers aren't trying to do it simultaneously. Using
multiple servers to delete one account might improve the deletion speed
but requires coordination to avoid duplication. Speed really is not a
big concern with data deletion, and large accounts aren't deleted often.</p><p>Deleting an account is simple. For each account container, all objects
are deleted and then the container is deleted. Deletion requests that
fail will not stop the overall process but will cause the overall
process to fail eventually (for example, if an object delete times out,
you will not be able to delete the container or the account). The
account reaper keeps trying to delete an account until it is empty, at
which point the database reclaim process within the db_replicator will
remove the database files.</p><p>A persistent error state may prevent the deletion of an object or
container. If this happens, you will see a message in the log, for example:</p><div class="verbatim-wrap"><pre class="screen">Account &lt;name&gt; has not been reaped since &lt;date&gt;</pre></div><p>You can control when this is logged with the <code class="literal">reap_warn_after</code> value in the
<code class="literal">[account-reaper]</code> section of the <code class="literal">account-server.conf</code> file.
The default value is 30 days.</p></div><div class="sect1 " id="id-1.4.8.14"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure project-specific image locations with Object Storage</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.14">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>For some deployers, it is not ideal to store all images in one place to
enable all projects and users to access them. You can configure the Image
service to store image data in project-specific image locations. Then,
only the following projects can use the Image service to access the
created image:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The project who owns the image</p></li><li class="listitem "><p>Projects that are defined in <code class="literal">swift_store_admin_tenants</code> and that
have admin-level accounts</p></li></ul></div><p>
        <span class="bold"><strong>To configure project-specific image locations</strong></span>
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Configure swift as your <code class="literal">default_store</code> in the
<code class="literal">glance-api.conf</code> file.</p></li><li class="step "><p>Set these configuration options in the <code class="literal">glance-api.conf</code> file:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.8.14.5.2.2.1.1.1"><span class="term ">swift_store_multi_tenant</span></dt><dd><p>Set to <code class="literal">True</code> to enable tenant-specific storage locations.
Default is <code class="literal">False</code>.</p></dd></dl></div></li><li class="listitem "><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.8.14.5.2.2.2.1.1"><span class="term ">swift_store_admin_tenants</span></dt><dd><p>Specify a list of tenant IDs that can grant read and write access to all
Object Storage containers that are created by the Image service.</p></dd></dl></div></li></ul></div></li></ol></div></div><p>With this configuration, images are stored in an Object Storage service
(swift) endpoint that is pulled from the service catalog for the
authenticated user.</p></div><div class="sect1 " id="id-1.4.8.15"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage monitoring</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.15">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.4.8.15.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>This section was excerpted from a blog post by <a class="link" href="http://swiftstack.com/blog/2012/04/11/swift-monitoring-with-statsd" target="_blank">Darrell
Bishop</a> and
has since been edited.</p></div><p>An OpenStack Object Storage cluster is a collection of many daemons that
work together across many nodes. With so many different components, you
must be able to tell what is going on inside the cluster. Tracking
server-level meters like CPU utilization, load, memory consumption, disk
usage and utilization, and so on is necessary, but not sufficient.</p><div class="sect2 " id="id-1.4.8.15.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Recon</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.15.4">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The Swift Recon middleware (see
<a class="link" href="http://swift.openstack.org/admin_guide.html#cluster-telemetry-and-monitoring" target="_blank">Defining Storage Policies</a>)
provides general machine statistics, such as load average, socket
statistics, <code class="literal">/proc/meminfo</code> contents, as well as Swift-specific meters:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The <code class="literal">MD5</code> sum of each ring file.</p></li><li class="listitem "><p>The most recent object replication time.</p></li><li class="listitem "><p>Count of each type of quarantined file: Account, container, or
object.</p></li><li class="listitem "><p>Count of "async_pendings" (deferred container updates) on disk.</p></li></ul></div><p>Swift Recon is middleware that is installed in the object servers
pipeline and takes one required option: A local cache directory. To
track <code class="literal">async_pendings</code>, you must set up an additional cron job for
each object server. You access data by either sending HTTP requests
directly to the object server or using the <code class="literal">swift-recon</code> command-line
client.</p><p>There are Object Storage cluster statistics but the typical
server meters overlap with existing server monitoring systems. To get
the Swift-specific meters into a monitoring system, they must be polled.
Swift Recon acts as a middleware meters collector. The
process that feeds meters to your statistics system, such as
<code class="literal">collectd</code> and <code class="literal">gmond</code>, should already run on the storage node.
You can choose to either talk to Swift Recon or collect the meters
directly.</p></div><div class="sect2 " id="id-1.4.8.15.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift-Informant</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.15.5">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Swift-Informant middleware (see
<a class="link" href="https://github.com/pandemicsyn/swift-informant" target="_blank">swift-informant</a>) has
real-time visibility into Object Storage client requests. It sits in the
pipeline for the proxy server, and after each request to the proxy server it
sends three meters to a <code class="literal">StatsD</code> server:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>A counter increment for a meter like <code class="literal">obj.GET.200</code> or
<code class="literal">cont.PUT.404</code>.</p></li><li class="listitem "><p>Timing data for a meter like <code class="literal">acct.GET.200</code> or <code class="literal">obj.GET.200</code>.
[The README says the meters look like <code class="literal">duration.acct.GET.200</code>, but
I do not see the <code class="literal">duration</code> in the code. I am not sure what the
Etsy server does but our StatsD server turns timing meters into five
derivative meters with new segments appended, so it probably works as
coded. The first meter turns into <code class="literal">acct.GET.200.lower</code>,
<code class="literal">acct.GET.200.upper</code>, <code class="literal">acct.GET.200.mean</code>,
<code class="literal">acct.GET.200.upper_90</code>, and <code class="literal">acct.GET.200.count</code>].</p></li><li class="listitem "><p>A counter increase by the bytes transferred for a meter like
<code class="literal">tfer.obj.PUT.201</code>.</p></li></ul></div><p>This is used for receiving information on the quality of service clients
experience with the timing meters, as well as sensing the volume of the
various modifications of a request server type, command, and response
code. Swift-Informant requires no change to core Object
Storage code because it is implemented as middleware. However, it gives
no insight into the workings of the cluster past the proxy server.
If the responsiveness of one storage node degrades, you can only see
that some of the requests are bad, either as high latency or error
status codes.</p></div><div class="sect2 " id="id-1.4.8.15.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Statsdlog</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.15.6">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The <a class="link" href="https://github.com/pandemicsyn/statsdlog" target="_blank">Statsdlog</a>
project increments StatsD counters based on logged events. Like
Swift-Informant, it is also non-intrusive, however statsdlog can track
events from all Object Storage daemons, not just proxy-server. The
daemon listens to a UDP stream of syslog messages, and StatsD counters
are incremented when a log line matches a regular expression. Meter
names are mapped to regex match patterns in a JSON file, allowing
flexible configuration of what meters are extracted from the log stream.</p><p>Currently, only the first matching regex triggers a StatsD counter
increment, and the counter is always incremented by one. There is no way
to increment a counter by more than one or send timing data to StatsD
based on the log line content. The tool could be extended to handle more
meters for each line and data extraction, including timing data. But a
coupling would still exist between the log textual format and the log
parsing regexes, which would themselves be more complex to support
multiple matches for each line and data extraction. Also, log processing
introduces a delay between the triggering event and sending the data to
StatsD. It would be preferable to increment error counters where they
occur and send timing data as soon as it is known to avoid coupling
between a log string and a parsing regex and prevent a time delay
between events and sending data to StatsD.</p><p>The next section describes another method for gathering Object Storage
operational meters.</p></div><div class="sect2 " id="id-1.4.8.15.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift StatsD logging</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.15.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>StatsD (see <a class="link" href="http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/" target="_blank">Measure Anything, Measure Everything</a>)
was designed for application code to be deeply instrumented. Meters are
sent in real-time by the code that just noticed or did something. The
overhead of sending a meter is extremely low: a <code class="literal">sendto</code> of one UDP
packet. If that overhead is still too high, the StatsD client library
can send only a random portion of samples and StatsD approximates the
actual number when flushing meters upstream.</p><p>To avoid the problems inherent with middleware-based monitoring and
after-the-fact log processing, the sending of StatsD meters is
integrated into Object Storage itself. The submitted change set (see
<a class="link" href="https://review.openstack.org/#change,6058" target="_blank">https://review.openstack.org/#change,6058</a>) currently reports 124 meters
across 15 Object Storage daemons and the tempauth middleware. Details of
the meters tracked are in the <a class="link" href="http://docs.openstack.org/developer/swift/admin_guide.html" target="_blank">Administrator's
Guide</a>.</p><p>The sending of meters is integrated with the logging framework. To
enable, configure <code class="literal">log_statsd_host</code> in the relevant config file. You
can also specify the port and a default sample rate. The specified
default sample rate is used unless a specific call to a statsd logging
method (see the list below) overrides it. Currently, no logging calls
override the sample rate, but it is conceivable that some meters may
require accuracy (<code class="literal">sample_rate=1</code>) while others may not.</p><div class="verbatim-wrap highlight ini"><pre class="screen">[DEFAULT]
     ...
log_statsd_host = 127.0.0.1
log_statsd_port = 8125
log_statsd_default_sample_rate = 1</pre></div><p>Then the LogAdapter object returned by <code class="literal">get_logger()</code>, usually stored
in <code class="literal">self.logger</code>, has these new methods:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">set_statsd_prefix(self, prefix)</code> Sets the client library stat
prefix value which gets prefixed to every meter. The default prefix
is the <code class="literal">name</code> of the logger such as <code class="literal">object-server</code>,
<code class="literal">container-auditor</code>, and so on. This is currently used to turn
<code class="literal">proxy-server</code> into one of <code class="literal">proxy-server.Account</code>,
<code class="literal">proxy-server.Container</code>, or <code class="literal">proxy-server.Object</code> as soon as the
Controller object is determined and instantiated for the request.</p></li><li class="listitem "><p><code class="literal">update_stats(self, metric, amount, sample_rate=1)</code> Increments
the supplied meter by the given amount. This is used when you need
to add or subtract more that one from a counter, like incrementing
<code class="literal">suffix.hashes</code> by the number of computed hashes in the object
replicator.</p></li><li class="listitem "><p><code class="literal">increment(self, metric, sample_rate=1)</code> Increments the given counter
meter by one.</p></li><li class="listitem "><p><code class="literal">decrement(self, metric, sample_rate=1)</code> Lowers the given counter
meter by one.</p></li><li class="listitem "><p><code class="literal">timing(self, metric, timing_ms, sample_rate=1)</code> Record that the
given meter took the supplied number of milliseconds.</p></li><li class="listitem "><p><code class="literal">timing_since(self, metric, orig_time, sample_rate=1)</code>
Convenience method to record a timing meter whose value is "now"
minus an existing timestamp.</p></li></ul></div><div id="id-1.4.8.15.7.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>These logging methods may safely be called anywhere you have a
logger object. If StatsD logging has not been configured, the methods
are no-ops. This avoids messy conditional logic each place a meter is
recorded. These example usages show the new logging methods:</p><div class="verbatim-wrap highlight python"><pre class="screen"># swift/obj/replicator.py
def update(self, job):
     # ...
    begin = time.time()
    try:
        hashed, local_hash = tpool.execute(tpooled_get_hashes, job['path'],
                do_listdir=(self.replication_count % 10) == 0,
                reclaim_age=self.reclaim_age)
        # See tpooled_get_hashes "Hack".
        if isinstance(hashed, BaseException):
            raise hashed
        self.suffix_hash += hashed
        self.logger.update_stats('suffix.hashes', hashed)
        # ...
    finally:
        self.partition_times.append(time.time() - begin)
        self.logger.timing_since('partition.update.timing', begin)</pre></div><div class="verbatim-wrap highlight python"><pre class="screen"># swift/container/updater.py
def process_container(self, dbfile):
    # ...
    start_time = time.time()
    # ...
        for event in events:
            if 200 &lt;= event.wait() &lt; 300:
                successes += 1
            else:
                failures += 1
        if successes &gt; failures:
          self.logger.increment('successes')
            # ...
        else:
            self.logger.increment('failures')
            # ...
        # Only track timing data for attempted updates:
        self.logger.timing_since('timing', start_time)
    else:
        self.logger.increment('no_changes')
        self.no_changes += 1</pre></div></div></div></div><div class="sect1 " id="id-1.4.8.16"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System administration for Object Storage</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.16">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>By understanding Object Storage concepts, you can better monitor and
administer your storage solution. The majority of the administration
information is maintained in developer documentation at
<a class="link" href="http://docs.openstack.org/developer/swift/" target="_blank">docs.openstack.org/developer/swift/</a>.</p><p>See the <a class="link" href="http://docs.openstack.org/newton/config-reference/object-storage.html" target="_blank">OpenStack Configuration Reference</a>
for a list of configuration options for Object Storage.</p></div><div class="sect1 " id="id-1.4.8.17"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshoot Object Storage</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>For Object Storage, everything is logged in <code class="literal">/var/log/syslog</code> (or
<code class="literal">messages</code> on some distros). Several settings enable further
customization of logging, such as <code class="literal">log_name</code>, <code class="literal">log_facility</code>, and
<code class="literal">log_level</code>, within the object server configuration files.</p><div class="sect2 " id="id-1.4.8.17.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Drive failure</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.3">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.8.17.3.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.3.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Drive failure can prevent Object Storage performing replication.</p></div><div class="sect3 " id="id-1.4.8.17.3.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.3.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In the event that a drive has failed, the first step is to make sure the
drive is unmounted. This will make it easier for Object Storage to work
around the failure until it has been resolved. If the drive is going to
be replaced immediately, then it is just best to replace the drive,
format it, remount it, and let replication fill it up.</p><p>If you cannot replace the drive immediately, then it is best to leave it
unmounted, and remove the drive from the ring. This will allow all the
replicas that were on that drive to be replicated elsewhere until the
drive is replaced. Once the drive is replaced, it can be re-added to the
ring.</p><p>You can look at error messages in the <code class="literal">/var/log/kern.log</code> file for
hints of drive failure.</p></div></div><div class="sect2 " id="id-1.4.8.17.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server failure</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.4">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.8.17.4.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.4.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The server is potentially offline, and may have failed, or require a
reboot.</p></div><div class="sect3 " id="id-1.4.8.17.4.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.4.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>If a server is having hardware issues, it is a good idea to make sure
the Object Storage services are not running. This will allow Object
Storage to work around the failure while you troubleshoot.</p><p>If the server just needs a reboot, or a small amount of work that should
only last a couple of hours, then it is probably best to let Object
Storage work around the failure and get the machine fixed and back
online. When the machine comes back online, replication will make sure
that anything that is missing during the downtime will get updated.</p><p>If the server has more serious issues, then it is probably best to
remove all of the server's devices from the ring. Once the server has
been repaired and is back online, the server's devices can be added back
into the ring. It is important that the devices are reformatted before
putting them back into the ring as it is likely to be responsible for a
different set of partitions than before.</p></div></div><div class="sect2 " id="id-1.4.8.17.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Detect failed drives</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.5">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.8.17.5.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.5.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>When drives fail, it can be difficult to detect that a drive has failed,
and the details of the failure.</p></div><div class="sect3 " id="id-1.4.8.17.5.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.5.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>It has been our experience that when a drive is about to fail, error
messages appear in the <code class="literal">/var/log/kern.log</code> file. There is a script called
<code class="literal">swift-drive-audit</code> that can be run via cron to watch for bad drives. If
errors are detected, it will unmount the bad drive, so that Object
Storage can work around it. The script takes a configuration file with
the following settings:</p><div class="table" id="id-1.4.8.17.5.3.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.1: </span><span class="name">Description of configuration options for [drive-audit] in drive-audit.conf </span><a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.5.3.3">#</a></h6></div><div class="table-contents"><table class="table" summary="Description of configuration options for [drive-audit] in drive-audit.conf" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>
                    <p>Configuration option = Default value</p>
                  </th><th>
                    <p>Description</p>
                  </th></tr></thead><tbody><tr><td>
                    <p>
                      <code class="literal">device_dir = /srv/node</code>
                    </p>
                  </td><td>
                    <p>Directory devices are mounted under</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">error_limit = 1</code>
                    </p>
                  </td><td>
                    <p>Number of errors to find before a device is unmounted</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">log_address = /dev/log</code>
                    </p>
                  </td><td>
                    <p>Location where syslog sends the logs to</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">log_facility = LOG_LOCAL0</code>
                    </p>
                  </td><td>
                    <p>Syslog log facility</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">log_file_pattern = /var/log/kern.*[!.][!g][!z]</code>
                    </p>
                  </td><td>
                    <p>Location of the log file with globbing pattern to check against device
errors locate device blocks with errors in the log file</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">log_level = INFO</code>
                    </p>
                  </td><td>
                    <p>Logging level</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">log_max_line_length = 0</code>
                    </p>
                  </td><td>
                    <p>Caps the length of log lines to the value given; no limit if set to 0,
the default.</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">log_to_console = False</code>
                    </p>
                  </td><td>
                    <p>No help text available for this option.</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">minutes = 60</code>
                    </p>
                  </td><td>
                    <p>Number of minutes to look back in <code class="literal">/var/log/kern.log</code></p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">recon_cache_path = /var/cache/swift</code>
                    </p>
                  </td><td>
                    <p>Directory where stats for a few items will be stored</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">regex_pattern_1 = \berror\b.*\b(dm-[0-9]{1,2}\d?)\b</code>
                    </p>
                  </td><td>
                    <p>No help text available for this option.</p>
                  </td></tr><tr><td>
                    <p>
                      <code class="literal">unmount_failed_device = True</code>
                    </p>
                  </td><td>
                    <p>No help text available for this option.</p>
                  </td></tr></tbody></table></div></div><div id="id-1.4.8.17.5.3.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>This script has only been tested on Ubuntu 10.04; use with caution on
other operating systems in production.</p></div></div></div><div class="sect2 " id="id-1.4.8.17.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Emergency recovery of ring builder files</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.6">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3 " id="id-1.4.8.17.6.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Problem</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.6.2">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>An emergency might prevent a successful backup from restoring the
cluster to operational status.</p></div><div class="sect3 " id="id-1.4.8.17.6.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.15.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution</span> <a title="Permalink" class="permalink" href="bk02ch06.html#id-1.4.8.17.6.3">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bk_openstack_admin.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>You should always keep a backup of swift ring builder files. However, if
an emergency occurs, this procedure may assist in returning your cluster
to an operational state.</p><p>Using existing swift tools, there is no way to recover a builder file
from a <code class="literal">ring.gz</code> file. However, if you have a knowledge of Python, it
is possible to construct a builder file that is pretty close to the one
you have lost.</p><div id="id-1.4.8.17.6.3.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>This procedure is a last-resort for emergency circumstances. It
requires knowledge of the swift python code and may not succeed.</p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>Load the ring and a new ringbuilder object in a Python REPL:</p><div class="verbatim-wrap highlight python"><pre class="screen">&gt;&gt;&gt; from swift.common.ring import RingData, RingBuilder
&gt;&gt;&gt; ring = RingData.load('/path/to/account.ring.gz')</pre></div></li><li class="step "><p>Start copying the data we have in the ring into the builder:</p><div class="verbatim-wrap highlight python"><pre class="screen">&gt;&gt;&gt; import math
&gt;&gt;&gt; partitions = len(ring._replica2part2dev_id[0])
&gt;&gt;&gt; replicas = len(ring._replica2part2dev_id)

&gt;&gt;&gt; builder = RingBuilder(int(math.log(partitions, 2)), replicas, 1)
&gt;&gt;&gt; builder.devs = ring.devs
&gt;&gt;&gt; builder._replica2part2dev = ring._replica2part2dev_id
&gt;&gt;&gt; builder._last_part_moves_epoch = 0
&gt;&gt;&gt; from array import array
&gt;&gt;&gt; builder._last_part_moves = array('B', (0 for _ in xrange(partitions)))
&gt;&gt;&gt; builder._set_parts_wanted()
&gt;&gt;&gt; for d in builder._iter_devs():
            d['parts'] = 0
&gt;&gt;&gt; for p2d in builder._replica2part2dev:
            for dev_id in p2d:
                builder.devs[dev_id]['parts'] += 1

This is the extent of the recoverable fields.</pre></div></li><li class="step "><p>For <code class="literal">min_part_hours</code> you either have to remember what the value you
used was, or just make up a new one:</p><div class="verbatim-wrap highlight python"><pre class="screen">&gt;&gt;&gt; builder.change_min_part_hours(24) # or whatever you want it to be</pre></div></li><li class="step "><p>Validate the builder. If this raises an exception, check your
previous code:</p><div class="verbatim-wrap highlight python"><pre class="screen">&gt;&gt;&gt; builder.validate()</pre></div></li><li class="step "><p>After it validates, save the builder and create a new <code class="literal">account.builder</code>:</p><div class="verbatim-wrap highlight python"><pre class="screen">&gt;&gt;&gt; import pickle
&gt;&gt;&gt; pickle.dump(builder.to_dict(), open('account.builder', 'wb'), protocol=2)
&gt;&gt;&gt; exit ()</pre></div></li><li class="step "><p>You should now have a file called <code class="literal">account.builder</code> in the current
working directory. Run
<code class="command">swift-ring-builder account.builder write_ring</code> and compare the new
<code class="literal">account.ring.gz</code> to the <code class="literal">account.ring.gz</code> that you started
from. They probably are not byte-for-byte identical, but if you load them
in a REPL and their <code class="literal">_replica2part2dev_id</code> and <code class="literal">devs</code> attributes are
the same (or nearly so), then you are in good shape.</p></li><li class="step "><p>Repeat the procedure for <code class="literal">container.ring.gz</code> and
<code class="literal">object.ring.gz</code>, and you might get usable builder files.</p></li></ol></div></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="bk02ch07.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 7 </span>Block Storage</span></a><a class="nav-link" href="bk02ch05.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 5 </span>Compute</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>