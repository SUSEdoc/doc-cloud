<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Example Configurations | Planning an Installation with Cloud Lifecycle Manager | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="Planning an Installation with Cloud Lifecycle Manager" /><meta name="chapter-title" content="Chapter 9. Example Configurations" /><meta name="description" content="The SUSE OpenStack Cloud 8 system ships with a collection of pre-qualified example configurations. These are designed to help you to get up and running quickly with a minimum number of configuration changes." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" /><link rel="home" href="index.html" title="Documentation" /><link rel="up" href="architecture.html" title="Part II. Cloud Lifecycle Manager Overview" /><link rel="prev" href="cpinfofiles.html" title="Chapter 8. Configuration Processor Information Files" /><link rel="next" href="modify-compute-input-model.html" title="Chapter 10. Modifying Example Configurations for Compute Nodes" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-planning.html">Planning an Installation with Cloud Lifecycle Manager</a><span> › </span><a class="crumb" href="architecture.html">Cloud Lifecycle Manager Overview</a><span> › </span><a class="crumb" href="example-configurations.html">Example Configurations</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Planning an Installation with Cloud Lifecycle Manager</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="planning-index.html"><span class="number">I </span><span class="name">Planning</span></a><ol><li class="inactive"><a href="register-suse-overview.html"><span class="number">1 </span><span class="name">Registering SLES</span></a></li><li class="inactive"><a href="min-hardware.html"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></li><li class="inactive"><a href="idg-planning-planning-recommended-hardware-minimums-xml-1.html"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></li><li class="inactive"><a href="HP3-0HA.html"><span class="number">4 </span><span class="name">High Availability</span></a></li></ol></li><li class="inactive"><a href="architecture.html"><span class="number">II </span><span class="name">Cloud Lifecycle Manager Overview</span></a><ol><li class="inactive"><a href="cha-input-model-intro-concept.html"><span class="number">5 </span><span class="name">Input Model</span></a></li><li class="inactive"><a href="configurationobjects.html"><span class="number">6 </span><span class="name">Configuration Objects</span></a></li><li class="inactive"><a href="othertopics.html"><span class="number">7 </span><span class="name">Other Topics</span></a></li><li class="inactive"><a href="cpinfofiles.html"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></li><li class="inactive"><a href="example-configurations.html"><span class="number">9 </span><span class="name">Example Configurations</span></a></li><li class="inactive"><a href="modify-compute-input-model.html"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></li><li class="inactive"><a href="modify-input-model.html"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></li><li class="inactive"><a href="alternative-configurations.html"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></li></ol></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 8. Configuration Processor Information Files" href="cpinfofiles.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 10. Modifying Example Configurations for Compute Nodes" href="modify-compute-input-model.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-planning.html">Planning an Installation with Cloud Lifecycle Manager</a><span> › </span><a class="crumb" href="architecture.html">Cloud Lifecycle Manager Overview</a><span> › </span><a class="crumb" href="example-configurations.html">Example Configurations</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 8. Configuration Processor Information Files" href="cpinfofiles.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 10. Modifying Example Configurations for Compute Nodes" href="modify-compute-input-model.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="example-configurations"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></div><div><h2 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Configurations</span> <a title="Permalink" class="permalink" href="example-configurations.html#">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>example-configurations</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="example-configurations.html#example-configs"><span class="number">9.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span></a></span></dt><dt><span class="section"><a href="example-configurations.html#alternative"><span class="number">9.2 </span><span class="name">Alternative Configurations</span></a></span></dt><dt><span class="section"><a href="example-configurations.html#kvm-examples"><span class="number">9.3 </span><span class="name">KVM Examples</span></a></span></dt><dt><span class="section"><a href="example-configurations.html#esx-examples"><span class="number">9.4 </span><span class="name">ESX Examples</span></a></span></dt><dt><span class="section"><a href="example-configurations.html#swift-examples"><span class="number">9.5 </span><span class="name">Swift Examples</span></a></span></dt><dt><span class="section"><a href="example-configurations.html#ironic-examples"><span class="number">9.6 </span><span class="name">Ironic Examples</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> system ships with a collection of pre-qualified example
  configurations. These are designed to help you to get up and running quickly
  with a minimum number of configuration changes.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input model allows a wide variety of configuration parameters
  that can, at first glance, appear daunting. The example configurations are
  designed to simplify this process by providing pre-built and pre-qualified
  examples that need only a minimum number of modifications to get started.
 </p><div class="sect1" id="example-configs"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span> <a title="Permalink" class="permalink" href="example-configurations.html#example-configs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>example-configs</li></ul></div></div></div></div><p>
   This section briefly describes the various example configurations and their
   capabilities. It also describes in detail, for the entry-scale-kvm
   example, how you can adapt the input model to work in your environment.
  </p><p>
   The following pre-qualified examples are shipped with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Location</th></tr></thead><tbody><tr><td>
      <a class="xref" href="example-configurations.html#entry-scale-kvm" title="9.3.1. Entry-Scale Cloud">Section 9.3.1, “Entry-Scale Cloud”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td>
      <a class="xref" href="example-configurations.html#entry-scale-kvm-mml" title="9.3.2. Entry Scale Cloud with Metering and Monitoring Services">Section 9.3.2, “Entry Scale Cloud with Metering and Monitoring Services”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td><a class="xref" href="example-configurations.html#entry-scale-kvm-esx" title="9.4.1. Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors">Section 9.4.1, “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</a>
     </td><td><code class="filename">~/openstack/examples/entry-scale-esx-kvm</code>
     </td></tr><tr><td>
      <a class="xref" href="example-configurations.html#entry-scale-kvm-esx-mml" title="9.4.2. Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors">Section 9.4.2, “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-esx-kvm-mml</code>
     </td></tr><tr><td>
      <a class="xref" href="example-configurations.html#entryscale-swift" title="9.5.1. Entry-scale Swift Model">Section 9.5.1, “Entry-scale Swift Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td>
      <a class="xref" href="example-configurations.html#entryscale-ironic" title="9.6.1. Entry-Scale Cloud with Ironic Flat Network">Section 9.6.1, “Entry-Scale Cloud with Ironic Flat Network”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td>
      <a class="xref" href="example-configurations.html#entryscale-ironic-multi-tenancy" title="9.6.2. Entry-Scale Cloud with Ironic Multi-Tenancy">Section 9.6.2, “Entry-Scale Cloud with Ironic Multi-Tenancy”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td><a class="xref" href="example-configurations.html#mid-scale-kvm" title="9.3.3. Single-Region Mid-Size Model">Section 9.3.3, “Single-Region Mid-Size Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div><p>
   The entry-scale systems are designed to provide an entry-level solution that
   can be scaled from a small number of nodes to a moderately high node count
   (approximately 100 compute nodes, for example).
  </p><p>
   In the mid-scale model, the cloud control plane is subdivided into a number
   of dedicated service clusters to provide more processing power for
   individual control plane elements. This enables a greater number of
   resources to be supported (compute nodes, Swift object servers). This model
   also shows how a segmented network can be expressed in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> model.
  </p></div><div class="sect1" id="alternative"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Configurations</span> <a title="Permalink" class="permalink" href="example-configurations.html#alternative">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>alternative</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> there are alternative configurations that we recommend
   for specific purposes and this section we will outline them.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="alternative-configurations.html#standalone-deployer" title="12.1. Using a Dedicated Cloud Lifecycle Manager Node">Section 12.1, “Using a Dedicated Cloud Lifecycle Manager Node”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="alternative-configurations.html#without-dvr" title="12.2. Configuring SUSE OpenStack Cloud without DVR">Section 12.2, “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="alternative-configurations.html#without-l3agent" title="12.3. Configuring SUSE OpenStack Cloud with Provider VLANs and Physical Routers Only">Section 12.3, “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="alternative-configurations.html#twosystems" title="12.4. Considerations When Installing Two Systems on One Subnet">Section 12.4, “Considerations When Installing Two Systems on One Subnet”</a>
    </p></li></ul></div><p>
   The Ironic multi-tenancy feature uses Neutron to manage the tenant
   networks. The interaction between Neutron and the physical switch is
   facilitated by Neutron's Modular Layer 2 (ML2) plugin. The Neutron ML2
   plugin supports drivers to interact with various networks, as each vendor
   may have their own extensions. Those drivers are referred to as <span class="emphasis"><em>Neutron ML2
   mechanism drivers</em></span>, or simply <span class="emphasis"><em>mechanism drivers</em></span>.
  </p><p>
   The Ironic multi-tenancy feature has been validated using <span class="productname">OpenStack</span>
   genericswitch mechanism driver. However, if the given physical switch
   requires a different mechanism driver, you must update the input model
   accordingly. To update the input model with a custom ML2 mechanism driver,
   specify the relevant information in the
   <code class="literal">multi_tenancy_switch_config:</code> section of the
   <code class="filename">data/ironic/ironic_config.yml</code> file.
  </p></div><div class="sect1" id="kvm-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM Examples</span> <a title="Permalink" class="permalink" href="example-configurations.html#kvm-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-kvm_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-kvm_examples.xml</li><li><span class="ds-label">ID: </span>kvm-examples</li></ul></div></div></div></div><div class="sect2" id="entry-scale-kvm"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud</span> <a title="Permalink" class="permalink" href="example-configurations.html#entry-scale-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-entry-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm</li></ul></div></div></div></div><p>
  This example deploys an entry-scale cloud.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.2.3.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     Keystone, Nova API, Glance API, Neutron API, Horizon, and Heat
     API.
    </p></dd><dt id="id-1.3.4.7.6.2.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.6.2.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> One node of type
       <code class="literal">COMPUTE-ROLE</code> runs Nova Compute and associated
       services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       resources are provided by the control plane.
      </p></li></ul></div><p>
     Additional resource nodes can be added to the configuration.
    </p></dd><dt id="id-1.3.4.7.6.2.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> This network provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This network is used
       for all internal traffic between the cloud services. It is also used to
       install and configure the nodes. The network needs to be on an untagged
       VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.6.2.3.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by Swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file
    </p></dd></dl></div></div><div class="sect2" id="entry-scale-kvm-mml"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry Scale Cloud with Metering and Monitoring Services</span> <a title="Permalink" class="permalink" href="example-configurations.html#entry-scale-kvm-mml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-entry-scale-kvm-mml.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-mml.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-mml</li></ul></div></div></div></div><p>
  This example deploys an entry-scale cloud that provides metering and
  monitoring services and runs the database and messaging services in their own
  cluster.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.3.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Cluster1</strong></span> 2 nodes of type
       <code class="literal">CONTROLLER-ROLE</code> run the core OpenStack services, such
       as Keystone, Nova API, Glance API, Neutron API, Horizon, and
       Heat API.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster2</strong></span> 3 nodes of type
       <code class="literal">MTRMON-ROLE</code>, run the OpenStack services for metering
       and monitoring (for example, Ceilometer, Monasca and Logging).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster3</strong></span> 3 nodes of type
       <code class="literal">DBMQ-ROLE</code> that run clustered database and RabbitMQ
       services to support the cloud infrastructure. 3 nodes are required for
       high availability.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.6.3.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code>file.
    </p></dd><dt id="id-1.3.4.7.6.3.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> 1 node of type
       <code class="literal">COMPUTE-ROLE</code> runs Nova Compute and associated
       services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       resources are provided by the control plane.
      </p></li></ul></div><p>
     Additional resource nodes can be added to the configuration.
    </p></dd><dt id="id-1.3.4.7.6.3.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that is used for all internal traffic between the cloud services. It is
       also used to install and configure the nodes. The network needs to be on
       an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.6.3.3.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB of capacity. In addition,
     the example configures one additional disk depending on the role of
     the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Core Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> is
       configured to be used by Swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>DBMQ Controllers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used by the database and RabbitMQ.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage.
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="mid-scale-kvm"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Mid-Size Model</span> <a title="Permalink" class="permalink" href="example-configurations.html#mid-scale-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-mid-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-mid-scale-kvm.xml</li><li><span class="ds-label">ID: </span>mid-scale-kvm</li></ul></div></div></div></div><p>
  The mid-size model is intended as a template for a moderate sized cloud. The
  Control plane is made up of multiple server clusters to provide sufficient
  computational, network and IOPS capacity for a mid-size production style
  cloud.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.4.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Core Cluster</strong></span> runs core OpenStack
       Services, such as Keystone, Nova API, Glance API, Neutron API,
       Horizon, and Heat API. Default configuration is two nodes of role
       type <code class="literal">CORE-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Metering and Monitoring Cluster</strong></span> runs
       the OpenStack Services for metering and monitoring (for example,
       Ceilometer, Monasca and logging). Default configuration is three
       nodes of role type <code class="literal">MTRMON-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Database and Message Queue Cluster</strong></span> runs
       clustered MariaDB and RabbitMQ services to support the Ardana cloud
       infrastructure. Default configuration is three nodes of role type
       <code class="literal">DBMQ-ROLE</code>. Three nodes are required for high
       availability.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Swift PAC Cluster</strong></span> runs the Swift
       Proxy, Account and Container services. Default configuration is three
       nodes of role type <code class="literal">SWPAC-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Neutron Agent Cluster</strong></span> Runs Neutron
       VPN (L3), DHCP, Metadata and OpenVswitch agents. Default configuration
       is two nodes of role type <code class="literal">NEUTRON-ROLE</code>.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.6.4.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.6.4.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> runs Nova Compute and
       associated services. Runs on nodes of role type
       <code class="literal">COMPUTE-ROLE</code>. This model lists 3 nodes. 1 node is the
       minimum requirement.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> 3 nodes of type
       <code class="literal">SOWBJ-ROLE</code> run the Swift Object service. The
       minimum node count should match your Swift replica count.
      </p></li></ul></div><p>
     The minimum node count required to run this model unmodified is 19 nodes.
     This can be reduced by consolidating services on the control plane
     clusters.
    </p></dd><dt id="id-1.3.4.7.6.4.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Internal API</strong></span> This network is used
       within the cloud for API access between services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> This network provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This network is used
       for all internal traffic between the cloud services. It is also used to
       install and configure the nodes. The network needs to be on an untagged
       VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>SWIFT</strong></span> This network is used for internal
       Swift communications between the Swift nodes.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd></dl></div><div class="sect3" id="id-1.3.4.7.6.4.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adapting the Mid-Size Model to Fit Your Environment</span> <a title="Permalink" class="permalink" href="example-configurations.html#id-1.3.4.7.6.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-mid-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-mid-scale-kvm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The minimum set of changes you need to make to adapt the model for your
   environment are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Update <code class="filename">servers.yml</code> to list the details of your
     baremetal servers.
    </p></li><li class="listitem "><p>
     Update the <code class="filename">networks.yml</code> file to replace network CIDRs
     and VLANs with site specific values.
    </p></li><li class="listitem "><p>
     Update the <code class="filename">nic_mappings.yml</code> file to ensure that
     network devices are mapped to the correct physical port(s).
    </p></li><li class="listitem "><p>
     Review the disk models (<code class="filename">disks_*.yml</code>) and confirm that
     the associated servers have the number of disks required by the disk
     model. The device names in the disk models might need to be adjusted to
     match the probe order of your servers. The default number of disks for the
     Swift nodes (3 disks) is set low on purpose to facilitate deployment on
     generic hardware. For production scale Swift the servers should have
     more disks. For example, 6 on SWPAC nodes and 12 on SWOBJ nodes. If you
     allocate more Swift disks then you should review the ring power in the
     Swift ring configuration. This is documented in the Swift section.
     Disk models are provided as follows:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       DISK SET CONTROLLER: Minimum 1 disk
      </p></li><li class="listitem "><p>
       DISK SET DBMQ: Minimum 3 disks
      </p></li><li class="listitem "><p>
       DISK SET COMPUTE: Minimum 2 disks
      </p></li><li class="listitem "><p>
       DISK SET SWPAC: Minimum 3 disks
      </p></li><li class="listitem "><p>
       DISK SET SWOBJ: Minimum 3 disks
      </p></li></ul></div></li><li class="listitem "><p>
     Update the <code class="filename">netinterfaces.yml</code> file to match the server
     NICs used in your configuration. This file has a separate interface model
     definition for each of the following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       INTERFACE SET CONTROLLER
      </p></li><li class="listitem "><p>
       INTERFACE SET DBMQ
      </p></li><li class="listitem "><p>
       INTERFACE SET SWPAC
      </p></li><li class="listitem "><p>
       INTERFACE SET SWOBJ
      </p></li><li class="listitem "><p>
       INTERFACE SET COMPUTE
      </p></li></ul></div></li></ul></div></div></div></div><div class="sect1" id="esx-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX Examples</span> <a title="Permalink" class="permalink" href="example-configurations.html#esx-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-esx_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-esx_examples.xml</li><li><span class="ds-label">ID: </span>esx-examples</li></ul></div></div></div></div><div class="sect2" id="entry-scale-kvm-esx"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors</span> <a title="Permalink" class="permalink" href="example-configurations.html#entry-scale-kvm-esx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-entry-scale-kvm-esx.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-esx.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-esx</li></ul></div></div></div></div><p>
  This example deploys a cloud which mixes KVM and ESX hypervisors.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.7.2.3.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     Keystone, Nova API, Glance API, Neutron API, Horizon, and Heat
     API.
    </p></dd><dt id="id-1.3.4.7.7.2.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.7.2.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Compute:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>KVM</strong></span> runs Nova Computes and
         associated services. It runs on nodes of role type
         <code class="literal">COMPUTE-ROLE</code>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>ESX</strong></span> provides ESX Compute services. OS
         and software on this node is installed by user.
        </p></li></ul></div></li></ul></div></dd><dt id="id-1.3.4.7.7.2.3.4"><span class="term ">ESX Resource Requirements</span></dt><dd><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       User needs to supply vSphere server
      </p></li><li class="listitem "><p>
       User needs to deploy the ovsvapp network resources using the
       vSphere GUI (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 15 “Installing ESX Computes and OVSvAPP”, Section 15.8 “Configuring the Required Distributed vSwitches and Port Groups”, Section 15.8.2 “Creating ESXi MGMT DVS and Required Portgroup”</span>) by running the
       <code class="literal">neutron-create-ovsvapp-resources.yml</code> playbook
       (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 15 “Installing ESX Computes and OVSvAPP”, Section 15.8 “Configuring the Required Distributed vSwitches and Port Groups”, Section 15.8.3 “Configuring OVSvApp Network Resources Using Ansible-Playbook”</span>) or via Python-Networking-vSphere
       (<span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 15 “Installing ESX Computes and OVSvAPP”, Section 15.8 “Configuring the Required Distributed vSwitches and Port Groups”, Section 15.8.4 “Configuring OVSVAPP Using Python-Networking-vSphere”</span>)
      </p><p>
       The following DVS and DVPGs need to be created and configured for each
       cluster in each ESX hypervisor that will host an OvsVapp appliance. The
       settings for each DVS and DVPG are specific to your system and network
       policies. A JSON file example is provided in the documentation, but it
       needs to be edited to match your requirements.
      </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><tbody><tr><td><span class="bold"><strong>DVS</strong></span></td><td><span class="bold"><strong>Port Groups assigned to DVS</strong></span></td></tr><tr><td>MGMT</td><td>MGMT-PG, ESX-CONF-PG, GUEST-PG</td></tr><tr><td>TRUNK</td><td>TRUNK-PG</td></tr></tbody></table></div></li><li class="listitem "><p>
       User needs to deploy ovsvapp appliance (<code class="literal">OVSVAPP-ROLE</code>)
       and nova-proxy appliance (<code class="literal">ESX-COMPUTE-ROLE</code>)
      </p></li><li class="listitem "><p>
       User needs to add required information related to compute proxy and
       OVSvApp Nodes
      </p></li></ol></div></dd><dt id="id-1.3.4.7.7.2.3.5"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span>network connected to the
       lifecycle-manager and the IPMI ports of all nodes, except the ESX
       hypervisors.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for
       making requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that
       provides access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> The network
       used for all internal traffic between the cloud services. It is also
       used to install and configure the nodes. The network needs to be on an
       untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This network carries
       traffic between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>SES</strong></span> This is the network that
       control-plane and compute-node clients use to talk to the external SUSE Enterprise Storage.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>TRUNK</strong></span> is the network that is used
       to apply security group rules on tenant traffic. It is managed by the
       cloud admin and is restricted to the vCenter environment.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>ESX-CONF-NET</strong></span> network is used
       only to configure the ESX compute nodes in the cloud. This network
       should be different from the network used with PXE to stand up the cloud
       control-plane.
      </p></li></ul></div><p>
     This example's set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.7.2.3.6"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In addition,
     the example configures additional disk depending on the node's role:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by Swift
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="entry-scale-kvm-esx-mml"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors</span> <a title="Permalink" class="permalink" href="example-configurations.html#entry-scale-kvm-esx-mml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-entry-scale-kvm-esx-mml.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-esx-mml.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-esx-mml</li></ul></div></div></div></div><p>
  This example deploys a cloud which mixes KVM and ESX hypervisors, provides
  metering and monitoring services, and runs the database and messaging
  services in their own cluster.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.7.3.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Cluster1</strong></span> 2 nodes of type
       <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such
       as Keystone, Nova API, Glance API, Neutron API, Horizon, and
       Heat API.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster2</strong></span> 3 nodes of type
       <code class="literal">MTRMON-ROLE</code>, run the <span class="productname">OpenStack</span> services for metering
       and monitoring (for example, Ceilometer, Monasca and Logging).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster3</strong></span> 3 nodes of type
       <code class="literal">DBMQ-ROLE</code>, run clustered database and RabbitMQ
       services to support the cloud infrastructure. 3 nodes are required for
       high availability.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.7.3.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.7.3.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Compute:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>KVM</strong></span> runs Nova Computes and
         associated services. It runs on nodes of role type
         <code class="literal">COMPUTE-ROLE</code>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>ESX</strong></span> provides ESX Compute services. OS
         and software on this node is installed by user.
        </p></li></ul></div></li></ul></div></dd><dt id="id-1.3.4.7.7.3.3.4"><span class="term "> ESX Resource Requirements </span></dt><dd><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       User needs to supply vSphere server
      </p></li><li class="listitem "><p>
       User needs to deploy the ovsvapp network resources using the
       vSphere GUI or by running the
       <code class="literal">neutron-create-ovsvapp-resources.yml</code> playbook
      </p><p>
       The following DVS and DVPGs need to be created and configured for each
       cluster in each ESX hypervisor that will host an OvsVapp appliance. The
       settings for each DVS and DVPG are specific to your system and network
       policies. A JSON file example is provided in the documentation, but it
       needs to be edited to match your requirements.
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         ESX-CONF (DVS and DVPG) connected to ovsvapp eth0 and compute-proxy
         eth0
        </p></li><li class="listitem "><p>
         MANAGEMENT (DVS and DVPG) connected to ovsvapp eth1, eth2, eth3 and compute-proxy
         eth1
        </p></li></ul></div></li><li class="listitem "><p>
       User needs to deploy ovsvapp appliance (<code class="literal">OVSVAPP-ROLE</code>)
       and nova-proxy appliance (<code class="literal">ESX-COMPUTE-ROLE</code>)
      </p></li><li class="listitem "><p>
       User needs to add required information related to compute proxy and
       OVSvApp Nodes
      </p></li></ol></div></dd><dt id="id-1.3.4.7.7.3.3.5"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span>network connected to the
       lifecycle-manager and the IPMI ports of all nodes, except the ESX
       hypervisors.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for
       making requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that
       provides access to VMs (via floating IP addresses).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This
       network is used for all internal traffic between the cloud services. It
       is also used to install and configure the nodes. The network needs to be
       on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This is the network that will
       carry traffic between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>TRUNK</strong></span> is the network that will be used
       to apply security group rules on tenant traffic. It is managed by the
       cloud admin and is restricted to the vCenter environment.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>ESX-CONF-NET</strong></span> network is used
       only to configure the ESX compute nodes in the cloud. This network
       should be different from the network used with PXE to stand up the cloud
       control-plane.
      </p></li></ul></div><p>
     This example's set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.7.3.3.6"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition, the example configures additional disk depending on the node's
     role:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by Swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file
    </p></dd></dl></div></div></div><div class="sect1" id="swift-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Examples</span> <a title="Permalink" class="permalink" href="example-configurations.html#swift-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-swift_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-swift_examples.xml</li><li><span class="ds-label">ID: </span>swift-examples</li></ul></div></div></div></div><div class="sect2" id="entryscale-swift"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-scale Swift Model</span> <a title="Permalink" class="permalink" href="example-configurations.html#entryscale-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-entryscale_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_swift.xml</li><li><span class="ds-label">ID: </span>entryscale-swift</li></ul></div></div></div></div><p>
  This example shows how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can be configured to provide a Swift-only
  configuration, consisting of three controllers and one or more Swift object
  servers.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-examples-entry_scale_swift.png" target="_blank"><img src="images/media-examples-entry_scale_swift.png" width="" /></a></div></div><p>
  The example requires the following networks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>External API</strong></span> - The network for making
    requests to the cloud.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Swift</strong></span> - The network for all data traffic
    between the Swift services.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Management</strong></span> - This network that is used for
    all internal traffic between the cloud services, including node
    provisioning. This network must be on an untagged VLAN.
   </p></li></ul></div><p>
  All of these networks are configured to be presented via a pair of bonded
  NICs. The example also enables provider VLANs to be configured in Neutron on
  this interface.
 </p><p>
  In the diagram "External Routing" refers to whatever routing you want to
  provide to allow users to access the External API. "Internal Routing" refers
  to whatever routing you want to provide to allow administrators to access the
  Management network.
 </p><p>
  If you are using <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to install the operating system, then an IPMI
  network connected to the IPMI ports of all servers and routable from the
  Cloud Lifecycle Manager is also required for BIOS and power management of the node during the
  operating system installation process.
 </p><p>
  In the example the controllers use one disk for the operating system and two
  disks for Swift proxy and account storage. The Swift object servers use one
  disk for the operating system and four disks for Swift storage. These values
  can be modified to suit your environment.
 </p><p>
  These recommended minimums are based on the included with the base
  installation and are suitable only for demo environments. For production
  systems you will want to consider your capacity and performance requirements
  when making decisions about your hardware.
 </p><p>
  The <code class="literal">entry-scale-swift</code> example runs the Swift proxy,
  account and container services on the three controller servers. However, it
  is possible to extend the model to include the Swift proxy, account and
  container services on dedicated servers (typically referred to as the Swift
  proxy servers). If you are using this model, we have included the recommended
  Swift proxy servers specs in the table below.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">Server Hardware - Minimum Requirements and
            Recommendations</th></tr><tr><th>Disk </th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Swift account/container data drive
        </p></li></ul></div>
     </td><td>64 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Swift Object</td><td>swobj</td><td>3</td><td>
      <p>
       If using x3 replication only:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
      <p>
       If using Erasure Codes only or a mix of x3 replication and Erasure
       Codes:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         6 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
     </td><td>32 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Swift Proxy, Account, and Container</td><td>swpac</td><td>3</td><td>2 x 600 GB (minimum, see considerations at bottom of page for more details)</td><td>64 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr></tbody></table></div><div id="id-1.3.4.7.8.2.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The disk speeds (RPM) chosen should be consistent within the same ring or
   storage policy. It is best to not use disks with mixed disk speeds within the
   same Swift ring.
  </p></div><p>
  <span class="bold"><strong>Considerations for your Swift object and proxy,
  account, container servers RAM and disk capacity needs</strong></span>
 </p><p>
  Swift can have a diverse number of hardware configurations. For example, a
  Swift object server may have just a few disks (minimum of 6 for erasure
  codes) or up to 70 and beyond. The memory requirement needs to be increased
  as more disks are added. The general rule of thumb for memory needed is 0.5
  GB per TB of storage. For example, a system with 24 hard drives at 8TB each,
  giving a total capacity of 192TB, should use 96GB of RAM. However, this does
  not work well for a system with a small number of small hard drives or a very
  large number of very large drives. So, if after calculating the memory given
  this guideline, if the answer is less than 32GB then go with 32GB of memory
  minimum and if the answer is over 256GB then use 256GB maximum, no need to
  use more memory than that.
 </p><p>
  When considering the capacity needs for the Swift proxy, account, and
  container (PAC) servers, you should calculate 2% of the total raw storage
  size of your object servers to specify the storage required for the PAC
  servers. So, for example, if you were using the example we provided earlier
  and you had an object server setup of 24 hard drives with 8TB each for a
  total of 192TB and you had a total of 6 object servers, that would give a raw
  total of 1152TB. So you would take 2% of that, which is 23TB, and ensure that
  much storage capacity was available on your Swift proxy, account, and
  container (PAC) server cluster. If you had a cluster of three Swift PAC
  servers, that would be ~8TB each.
 </p><p>
  Another general rule of thumb is that if you are expecting to have more than
  a million objects in a container then you should consider using SSDs on the
  Swift PAC servers rather than HDDs.
 </p></div></div><div class="sect1" id="ironic-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Examples</span> <a title="Permalink" class="permalink" href="example-configurations.html#ironic-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-ironic_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-ironic_examples.xml</li><li><span class="ds-label">ID: </span>ironic-examples</li></ul></div></div></div></div><div class="sect2" id="entryscale-ironic"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud with Ironic Flat Network</span> <a title="Permalink" class="permalink" href="example-configurations.html#entryscale-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_ironic.xml</li><li><span class="ds-label">ID: </span>entryscale-ironic</li></ul></div></div></div></div><p>
  This example deploys an entry scale cloud that uses the Ironic service to
  provision physical machines through the Compute services API.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-exampleconfigs-entry_scale_ironic.png" target="_blank"><img src="images/media-hos.docs-exampleconfigs-entry_scale_ironic.png" width="" /></a></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.9.2.4.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core OpenStack services, such
     as Keystone, Nova API, Glance API, Neutron API, Horizon, and
     Heat API.
    </p></dd><dt id="id-1.3.4.7.9.2.4.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.9.2.4.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Ironic Compute</strong></span> One node of type
       <code class="literal">IRONIC-COMPUTE-ROLE</code> runs nova-compute,
       nova-compute-ironic, and other supporting services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       resources are provided by the control plane.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.9.2.4.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> This is the network that
       users will use to make requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that will be used for all internal traffic between the cloud services.
       This network is also used to install and configure the nodes. The
       network needs to be on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This is the flat network that
       will carry traffic between bare metal instances within the cloud. It is
       also used to PXE boot said bare metal instances and install the
       operating system selected by tenants.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">GUEST</code> network for the bare metal instances to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     modified to match your system.
    </p></dd><dt id="id-1.3.4.7.9.2.4.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code>
       configured to be used by Swift.
      </p></li></ul></div><p>
     Additional discs can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="entryscale-ironic-multi-tenancy"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud with Ironic Multi-Tenancy</span> <a title="Permalink" class="permalink" href="example-configurations.html#entryscale-ironic-multi-tenancy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-architecture-examples-entryscale_ironic_multi_tenancy.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_ironic_multi_tenancy.xml</li><li><span class="ds-label">ID: </span>entryscale-ironic-multi-tenancy</li></ul></div></div></div></div><p>
  This example deploys an entry scale cloud that uses the Ironic service to
  provision physical machines through the Compute services API and supports
  multi tenancy.
 </p><div class="figure" id="multi-tenancy"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ironic-Entry-ScaleIronicMultiTenancy.png" target="_blank"><img src="images/media-ironic-Entry-ScaleIronicMultiTenancy.png" width="" alt="Entry-scale Cloud with Ironic Muti-Tenancy" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 9.1: </span><span class="name">Entry-scale Cloud with Ironic Muti-Tenancy </span><a title="Permalink" class="permalink" href="example-configurations.html#multi-tenancy">#</a></h6></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.9.3.4.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     Keystone, Nova API, Glance API, Neutron API, Horizon, and Heat
     API.
    </p></dd><dt id="id-1.3.4.7.9.3.4.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code>file.
    </p></dd><dt id="id-1.3.4.7.9.3.4.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Ironic Compute</strong></span> One node of type
       <code class="literal">IRONIC-COMPUTE-ROLE</code> runs nova-compute,
       nova-compute-ironic, and other supporting services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal Swift
       Resources are provided by the control plane.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.9.3.4.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       deployer and the IPMI ports of all nodes.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> network is used to make
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that will be used for all internal traffic between the cloud services.
       This network is also used to install and configure the controller nodes.
       The network needs to be on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Provisioning</strong></span> is the network used to PXE
       boot the Ironic nodes and install the operating system selected by
       tenants. This network needs to be tagged on the switch for control
       plane/Ironic compute nodes. For Ironic bare metal nodes, VLAN
       configuration on the switch will be set by Neutron driver.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Tenant VLANs</strong></span> The range of VLAN IDs
       should be reserved for use by Ironic and set in the cloud configuration.
       It is configured as untagged on control plane nodes, therefore it cannot
       be combined with management network on the same network interface.
      </p></li></ul></div><p>
     The following access should be allowed by routing/firewall:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Access from Management network to IPMI. Used during cloud
       installation and during Ironic bare metal node provisioning.
      </p></li><li class="listitem "><p>
       Access from Management network to switch management network. Used by
       neutron driver.
      </p></li><li class="listitem "><p>
       The <code class="literal">EXTERNAL API</code> network must be reachable from the
       tenant networks if you want bare metal nodes to be able to make API
       calls to the cloud.
      </p></li></ul></div><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses <code class="filename">hed3</code> for Management and External API
     traffic, and <code class="filename">hed4</code> for provisioning and tenant network
     traffic. If you need to modify these assignments for your environment,
     they are defined in <code class="filename">data/net_interfaces.yml</code>.
    </p></dd><dt id="id-1.3.4.7.9.3.4.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code>
       configured to be used by Swift.
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="modify-compute-input-model.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 10 </span>Modifying Example Configurations for Compute Nodes</span></a><a class="nav-link" href="cpinfofiles.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 8 </span>Configuration Processor Information Files</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>