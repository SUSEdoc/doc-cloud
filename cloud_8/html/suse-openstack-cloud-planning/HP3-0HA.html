<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>High Availability | Planning an Installation with Cloud Lifecycle Manager | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.2.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.81.0 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="Planning an Installation with Cloud Lifecycle Manager" /><meta name="chapter-title" content="Chapter 4. High Availability" /><meta name="description" content="This chapter covers High Availability concepts overview and cloud infrastructure." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" /><link rel="home" href="index.html" title="Documentation" /><link rel="up" href="planning-index.html" title="Part I. Planning" /><link rel="prev" href="idg-planning-planning-recommended-hardware-minimums-xml-1.html" title="Chapter 3. Recommended Hardware Minimums for the Example Configurations" /><link rel="next" href="architecture.html" title="Part II. Cloud Lifecycle Manager Overview" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-planning.html">Planning an Installation with Cloud Lifecycle Manager</a><span> › </span><a class="crumb" href="planning-index.html">Planning</a><span> › </span><a class="crumb" href="HP3-0HA.html">High Availability</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Planning an Installation with Cloud Lifecycle Manager</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="planning-index.html"><span class="number">I </span><span class="name">Planning</span></a><ol><li class="inactive"><a href="register-suse-overview.html"><span class="number">1 </span><span class="name">Registering SLES</span></a></li><li class="inactive"><a href="min-hardware.html"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></li><li class="inactive"><a href="idg-planning-planning-recommended-hardware-minimums-xml-1.html"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></li><li class="inactive"><a href="HP3-0HA.html"><span class="number">4 </span><span class="name">High Availability</span></a></li></ol></li><li class="inactive"><a href="architecture.html"><span class="number">II </span><span class="name">Cloud Lifecycle Manager Overview</span></a><ol><li class="inactive"><a href="cha-input-model-intro-concept.html"><span class="number">5 </span><span class="name">Input Model</span></a></li><li class="inactive"><a href="configurationobjects.html"><span class="number">6 </span><span class="name">Configuration Objects</span></a></li><li class="inactive"><a href="othertopics.html"><span class="number">7 </span><span class="name">Other Topics</span></a></li><li class="inactive"><a href="cpinfofiles.html"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></li><li class="inactive"><a href="example-configurations.html"><span class="number">9 </span><span class="name">Example Configurations</span></a></li><li class="inactive"><a href="modify-compute-input-model.html"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></li><li class="inactive"><a href="modify-input-model.html"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></li><li class="inactive"><a href="alternative-configurations.html"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></li></ol></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 3. Recommended Hardware Minimums for the Example Configurations" href="idg-planning-planning-recommended-hardware-minimums-xml-1.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Part II. Cloud Lifecycle Manager Overview" href="architecture.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-planning.html">Planning an Installation with Cloud Lifecycle Manager</a><span> › </span><a class="crumb" href="planning-index.html">Planning</a><span> › </span><a class="crumb" href="HP3-0HA.html">High Availability</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 3. Recommended Hardware Minimums for the Example Configurations" href="idg-planning-planning-recommended-hardware-minimums-xml-1.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Part II. Cloud Lifecycle Manager Overview" href="architecture.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="HP3-0HA"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></div><div><h2 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>HP3-0HA</li></ul></div></div><div><div class="abstract"><p>
    This chapter covers High Availability concepts overview and cloud
    infrastructure.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="HP3-0HA.html#concepts-overview"><span class="number">4.1 </span><span class="name">High Availability Concepts Overview</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#highly-available-cloud-infrastructure"><span class="number">4.2 </span><span class="name">Highly Available Cloud Infrastructure</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#high-availablity-controllers"><span class="number">4.3 </span><span class="name">High Availability of Controllers</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#CVR"><span class="number">4.4 </span><span class="name">High Availability Routing - Centralized</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#DVR"><span class="number">4.5 </span><span class="name">High Availability Routing - Distributed</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#availability-zones"><span class="number">4.6 </span><span class="name">Availability Zones</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#compute-kvm"><span class="number">4.7 </span><span class="name">Compute with KVM</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#nova-availability-zones"><span class="number">4.8 </span><span class="name">Nova Availability Zones</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#compute-esx"><span class="number">4.9 </span><span class="name">Compute with ESX Hypervisor</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#cinder-availability-zones"><span class="number">4.10 </span><span class="name">Cinder Availability Zones</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#object-storage-swift"><span class="number">4.11 </span><span class="name">Object Storage with Swift</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#highly-available-app-workloads"><span class="number">4.12 </span><span class="name">Highly Available Cloud Applications and Workloads</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#what-not-ha"><span class="number">4.13 </span><span class="name">What is not Highly Available?</span></a></span></dt><dt><span class="section"><a href="HP3-0HA.html#more-information"><span class="number">4.14 </span><span class="name">More Information</span></a></span></dt></dl></div></div><div class="sect1" id="concepts-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Concepts Overview</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#concepts-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>concepts-overview</li></ul></div></div></div></div><p>
   A highly available (HA) cloud ensures that a minimum level of cloud
   resources are always available on request, which results in uninterrupted
   operations for users.
  </p><p>
   In order to achieve this high availability of infrastructure and workloads,
   we define the scope of HA to be limited to protecting these only against
   single points of failure (SPOF). Single points of failure include:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Hardware SPOFs</strong></span>: Hardware failures can
     take the form of server failures, memory going bad, power failures,
     hypervisors crashing, hard disks dying, NIC cards breaking, switch ports
     failing, network cables loosening, and so forth.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Software SPOFs</strong></span>: Server processes can
     crash due to software defects, out-of-memory conditions, operating system
     kernel panic, and so forth.
    </p></li></ul></div><p>
   By design, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> strives to create a system architecture resilient to
   SPOFs, and does not attempt to automatically protect the system against
   multiple cascading levels of failures; such cascading failures will result
   in an unpredictable state. The cloud operator is encouraged to recover and
   restore any failed component as soon as the first level of failure occurs.
  </p></div><div class="sect1" id="highly-available-cloud-infrastructure"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Highly Available Cloud Infrastructure</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#highly-available-cloud-infrastructure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>highly-available-cloud-infrastructure</li></ul></div></div></div></div><p>
   The highly available cloud infrastructure consists of the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     High Availability of Controllers
    </p></li><li class="listitem "><p>
     Availability Zones
    </p></li><li class="listitem "><p>
     Compute with KVM
    </p></li><li class="listitem "><p>
     Nova Availability Zones
    </p></li><li class="listitem "><p>
     Compute with ESX
    </p></li><li class="listitem "><p>
     Object Storage with Swift
    </p></li></ul></div></div><div class="sect1" id="high-availablity-controllers"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability of Controllers</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#high-availablity-controllers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>high-availablity-controllers</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer deploys highly available configurations of OpenStack
   cloud services, resilient against single points of failure.
  </p><p>
   The high availability of the controller components comes in two main forms.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Many services are stateless and multiple instances are run across the
     control plane in active-active mode. The API services (nova-api,
     cinder-api, etc.) are accessed through the HA proxy load balancer whereas
     the internal services (nova-scheduler, cinder-scheduler, etc.), are
     accessed through the message broker. These services use the database
     cluster to persist any data.
    </p><div id="id-1.3.3.5.5.4.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The HA proxy load balancer is also run in active-active mode and
      keepalived (used for Virtual IP (VIP) Management) is run in active-active
      mode, with only one keepalived instance holding the VIP at any one point
      in time.
     </p></div></li><li class="listitem "><p>
     The high availability of the message queue service and the database
     service is achieved by running these in a clustered mode across the three
     nodes of the control plane: RabbitMQ cluster with Mirrored Queues and
     MariaDB Galera cluster.
    </p></li></ul></div><div class="figure" id="ControlPlane1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HPE_HA_Flow.png" target="_blank"><img src="images/media-ha30-HPE_HA_Flow.png" width="" alt="HA Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.1: </span><span class="name">HA Architecture </span><a title="Permalink" class="permalink" href="HP3-0HA.html#ControlPlane1">#</a></h6></div></div><p>
   The above diagram illustrates the HA architecture with the focus on VIP
   management and load balancing. It only shows a subset of active-active API
   instances and does not show examples of other services such as
   nova-scheduler, cinder-scheduler, etc.
  </p><p>
   In the above diagram, requests from an OpenStack client to the API services
   are sent to VIP and port combination; for example, 192.0.2.26:8774 for a
   Nova request. The load balancer listens for requests on that VIP and port.
   When it receives a request, it selects one of the controller nodes
   configured for handling Nova requests, in this particular case, and then
   forwards the request to the IP of the selected controller node on the same
   port.
  </p><p>
   The nova-api service, which is listening for requests on the IP of its host
   machine, then receives the request and deals with it accordingly. The
   database service is also accessed through the load balancer. RabbitMQ, on
   the other hand, is not currently accessed through VIP/HA proxy as the
   clients are configured with the set of nodes in the RabbitMQ cluster and
   failover between cluster nodes is automatically handled by the clients.
  </p></div><div class="sect1" id="CVR"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Routing - Centralized</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#CVR">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>CVR</li></ul></div></div></div></div><p>
   Incorporating High Availability into a system involves implementing
   redundancies in the component that is being made highly available. In
   Centralized Virtual Router (CVR), that element is the Layer 3 agent (L3
   agent). By making L3 agent highly available, upon failure all HA routers are
   migrated from the primary L3 agent to a secondary L3 agent. The
   implementation efficiency of an HA subsystem is measured by the number of
   packets that are lost when the secondary L3 agent is made the master.
  </p><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the primary and secondary L3 agents run continuously, and
   failover involves a rapid switchover of mastership to the secondary agent
   (IEFT RFC 5798). The failover essentially involves a switchover from an
   already running master to an already running slave. This substantially
   reduces the latency of the HA. The mechanism used by the master and the
   slave to implement a failover is implemented using Linux’s pacemaker HA
   resource manager. This CRM (Cluster resource manager) uses VRRP (Virtual
   Router Redundancy Protocol) to implement the HA mechanism. VRRP is a
   industry standard protocol and defined in RFC 5798.
  </p><div class="figure" id="Layer3HA"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HPE_HA_Layer-3HA.png" target="_blank"><img src="images/media-ha30-HPE_HA_Layer-3HA.png" width="" alt="Layer-3 HA" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.2: </span><span class="name">Layer-3 HA </span><a title="Permalink" class="permalink" href="HP3-0HA.html#Layer3HA">#</a></h6></div></div><p>
   L3 HA uses of VRRP comes with several benefits.
  </p><p>
   The primary benefit is that the failover mechanism does not involve
   interprocess communication overhead. Such overhead would be in the order of
   10s of seconds. By not using an RPC mechanism to invoke the secondary agent
   to assume the primary agents role enables VRRP to achieve failover within
   1-2 seconds.
  </p><p>
   In VRRP, the primary and secondary routers are all active. As the routers
   are running, it is a matter of making the router aware of its primary/master
   status. This switchover takes less than 2 seconds instead of 60+ seconds it
   would have taken to start a backup router and failover.
  </p><p>
   The failover depends upon a heartbeat link between the primary and
   secondary. That link in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses keepalived package of the
   pacemaker resource manager. The heartbeats are sent at a 2 second intervals
   between the primary and secondary. As per the VRRP protocol, if the
   secondary does not hear from the master after 3 intervals, it assumes the
   function of the primary.
  </p><p>
   Further, all the routable IP addresses, that is the VIPs (virtual IPs) are
   assigned to the primary agent.
  </p></div><div class="sect1" id="DVR"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Routing - Distributed</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#DVR">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>DVR</li></ul></div></div></div></div><p>
   The OpenStack Distributed Virtual Router (DVR) function delivers HA through
   its distributed architecture. The one centralized function remaining is
   source network address translation (SNAT), where high availability is
   provided by DVR SNAT HA.
  </p><p>
   DVR SNAT HA is enabled on a per router basis and requires that two or more
   L3 agents capable of providing SNAT services be running on the system. If a
   minimum number of L3 agents is configured to 1 or lower, the neutron server
   will fail to start and a log message will be created. The L3 Agents must be
   running on a control-plane node, L3 agents running on a compute node do not
   provide SNAT services.
  </p></div><div class="sect1" id="availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Availability Zones</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#availability-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>availability-zones</li></ul></div></div></div></div><div class="figure" id="DeploymentZones"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HA_AvailabilityZones_3.png" target="_blank"><img src="images/media-ha30-HA_AvailabilityZones_3.png" width="" alt="Availability Zones" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.3: </span><span class="name">Availability Zones </span><a title="Permalink" class="permalink" href="HP3-0HA.html#DeploymentZones">#</a></h6></div></div><p>
   While planning your OpenStack deployment, you should decide on how to zone
   various types of nodes - such as compute, block storage, and object storage.
   For example, you may decide to place all servers in the same rack in the
   same zone. For larger deployments, you may plan more elaborate redundancy
   schemes for redundant power, network ISP connection, and even physical
   firewalling between zones (<span class="emphasis"><em>this aspect is outside the scope of
   this document</em></span>).
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> offers APIs, CLIs and Horizon UIs for the administrator to define
   and user to consume, availability zones for Nova, Cinder and Swift services.
   This section outlines the process to deploy specific types of nodes to
   specific physical servers, and makes a statement of available support for
   these types of availability zones in the current release.
  </p><div id="id-1.3.3.5.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is deployed in a single availability zone upon
    installation. Multiple availability zones can be configured by an
    administrator post-install, if required. Refer to <a class="link" href="https://docs.openstack.org/openstack-ansible/pike/admin/maintenance-tasks/scale-environment.html" target="_blank">OpenStack
    Docs:Scaling your environment</a>
   </p></div></div><div class="sect1" id="compute-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute with KVM</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#compute-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>compute-kvm</li></ul></div></div></div></div><p>
   You can deploy your KVM nova-compute nodes either during initial
   installation or by adding compute nodes post initial installation.
  </p><p>
   While adding compute nodes post initial installation, you can specify the
   target physical servers for deploying the compute nodes.
  </p><p>
   Learn more about adding compute nodes in
   <span class="intraxref">Book “Operations Guide”, Chapter 13 “System Maintenance”, Section 13.1 “Planned System Maintenance”, Section 13.1.3 “Planned Compute Maintenance”, Section 13.1.3.4 “Adding Compute Node”</span>.
  </p></div><div class="sect1" id="nova-availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nova Availability Zones</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#nova-availability-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>nova-availability-zones</li></ul></div></div></div></div><p>
   Nova host aggregates and Nova availability zones can be used to segregate
   Nova compute nodes across different failure zones.
  </p></div><div class="sect1" id="compute-esx"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute with ESX Hypervisor</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#compute-esx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>compute-esx</li></ul></div></div></div></div><p>
   Compute nodes deployed on ESX Hypervisor can be made highly available using
   the HA feature of VMware ESX Clusters. For more information on VMware
   HA, please refer to your VMware ESX documentation.
  </p></div><div class="sect1" id="cinder-availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cinder Availability Zones</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#cinder-availability-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>cinder-availability-zones</li></ul></div></div></div></div><p>
   Cinder availability zones are not supported for general consumption in the
   current release.
  </p></div><div class="sect1" id="object-storage-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage with Swift</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#object-storage-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>object-storage-swift</li></ul></div></div></div></div><p>
   High availability in Swift is achieved at two levels.
  </p><p>
   <span class="bold"><strong>Control Plane</strong></span>
  </p><p>
   The Swift API is served by multiple Swift proxy nodes. Client requests are
   directed to all Swift proxy nodes by the HA Proxy load balancer in
   round-robin fashion. The HA Proxy load balancer regularly checks the node is
   responding, so that if it fails, traffic is directed to the remaining nodes.
   The Swift service will continue to operate and respond to client requests as
   long as at least one Swift proxy server is running.
  </p><p>
   If a Swift proxy node fails in the middle of a transaction, the transaction
   fails. However it is standard practice for Swift clients to retry
   operations. This is transparent to applications that use the
   python-swiftclient library.
  </p><p>
   The entry-scale example cloud models contain three Swift proxy nodes.
   However, it is possible to add additional clusters with additional Swift
   proxy nodes to handle a larger workload or to provide additional resiliency.
  </p><p>
   <span class="bold"><strong>Data</strong></span>
  </p><p>
   Multiple replicas of all data is stored. This happens for account, container
   and object data. The example cloud models recommend a replica count of
   three. However, you may change this to a higher value if needed.
  </p><p>
   When Swift stores different replicas of the same item on disk, it ensures
   that as far as possible, each replica is stored in a different zone, server
   or drive. This means that if a single server of disk drives fails, there
   should be two copies of the item on other servers or disk drives.
  </p><p>
   If a disk drive is failed, Swift will continue to store three replicas. The
   replicas that would normally be stored on the failed drive are “handed
   off” to another drive on the system. When the failed drive is replaced,
   the data on that drive is reconstructed by the replication process. The
   replication process re-creates the <span class="quote">“<span class="quote ">missing</span>”</span> replicas by
   copying them to the drive using one of the other remaining replicas. While
   this is happening, Swift can continue to store and retrieve data.
  </p></div><div class="sect1" id="highly-available-app-workloads"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Highly Available Cloud Applications and Workloads</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#highly-available-app-workloads">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>highly-available-app-workloads</li></ul></div></div></div></div><p>
   Projects writing applications to be deployed in the cloud must be aware of
   the cloud architecture and potential points of failure and architect their
   applications accordingly for high availability.
  </p><p>
   Some guidelines for consideration:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Assume intermittent failures and plan for retries
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>OpenStack Service APIs</strong></span>: invocations can
       fail - you should carefully evaluate the response of each invocation,
       and retry in case of failures.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span>: VMs can die - monitor and
       restart them
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Network</strong></span>: Network calls can fail - retry
       should be successful
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Storage</strong></span>: Storage connection can hiccup
       - retry should be successful
      </p></li></ul></div></li><li class="listitem "><p>
     Build redundancy into your application tiers
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Replicate VMs containing stateless services such as Web application tier
       or Web service API tier and put them behind load balancers (you must
       implement your own HA Proxy type load balancer in your application VMs
       until <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> delivers the LBaaS service).
      </p></li><li class="listitem "><p>
       Boot the replicated VMs into different Nova availability zones.
      </p></li><li class="listitem "><p>
       If your VM stores state information on its local disk (Ephemeral
       Storage), and you cannot afford to lose it, then boot the VM off a
       Cinder volume.
      </p></li><li class="listitem "><p>
       Take periodic snapshots of the VM which will back it up to Swift through
       Glance.
      </p></li><li class="listitem "><p>
       Your data on ephemeral may get corrupted (but not your backup data in
       Swift and not your data on Cinder volumes).
      </p></li><li class="listitem "><p>
       Take regular snapshots of Cinder volumes and also back up Cinder volumes
       or your data exports into Swift.
      </p></li></ul></div></li><li class="listitem "><p>
     Instead of rolling your own highly available stateful services, use
     readily available <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> platform services such as Designate, the DNS
     service.
    </p></li></ol></div></div><div class="sect1" id="what-not-ha"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is not Highly Available?</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#what-not-ha">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>what-not-ha</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.3.5.15.2.1"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
      The Cloud Lifecycle Manager in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is not highly available. The Cloud Lifecycle Manager state/data are
      all maintained in a filesystem and are backed up by the Freezer service.
      In case of Cloud Lifecycle Manager failure, the state/data can be recovered from the
      backup.
     </p></dd><dt id="id-1.3.3.5.15.2.2"><span class="term ">Control Plane</span></dt><dd><p>
      High availability (HA) is supported for the Network Services LBaaS and
      FWaaS. HA is <span class="bold"><strong>not</strong></span> supported for VPNaaS.
     </p></dd><dt id="id-1.3.3.5.15.2.3"><span class="term ">Nova-consoleauth</span></dt><dd><p>
      Nova-consoleauth is a singleton service, it can only run on a single node
      at a time. While nova-consoleauth is not high availability, some work has
      been done to provide the ability to switch nova-consoleauth to another
      controller node in case of a failure.
      
     </p></dd><dt id="id-1.3.3.5.15.2.4"><span class="term ">Cinder Volume and Backup Services</span></dt><dd><p>
      Cinder Volume and Backup Services are not high availability and started
      on one controller node at a time.
      More information on Cinder Volume and Backup Services can
      be found in <span class="intraxref">Book “Operations Guide”, Chapter 7 “Managing Block Storage”, Section 7.1 “Managing Block Storage using Cinder”, Section 7.1.3 “Managing Cinder Volume and Backup Services”</span>.
     </p></dd><dt id="id-1.3.3.5.15.2.5"><span class="term ">Keystone Cron Jobs</span></dt><dd><p>
      The Keystone cron job is a singleton service, which can only run on a
      single node at a time. A manual setup process for this job will be
      required in case of a node failure.
      More information on enabling the cron job for Keystone on
      the other nodes can be found in <span class="intraxref">Book “Operations Guide”, Chapter 4 “Managing Identity”, Section 4.12 “Identity Service Notes and Limitations”, Section 4.12.4 “System cron jobs need setup”</span>.
     </p></dd></dl></div></div><div class="sect1" id="more-information"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="HP3-0HA.html#more-information">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>more-information</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="link" href="https://docs.openstack.org/ha-guide/" target="_blank">OpenStack
     High-availability Guide</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://12factor.net/" target="_blank">12-Factor Apps</a>
    </p></li></ul></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="architecture.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Part II </span>Cloud Lifecycle Manager Overview</span></a><a class="nav-link" href="idg-planning-planning-recommended-hardware-minimums-xml-1.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 3 </span>Recommended Hardware Minimums for the Example Configurations</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>