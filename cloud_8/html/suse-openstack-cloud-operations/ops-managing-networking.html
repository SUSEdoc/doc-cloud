<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Managing Networking | Operations Guide | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.2.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.81.0 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="Operations Guide" /><meta name="chapter-title" content="Chapter 9. Managing Networking" /><meta name="description" content="Information about managing and configuring the Networking service." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" /><link rel="home" href="index.html" title="Documentation" /><link rel="up" href="book-operations.html" title="Operations Guide" /><link rel="prev" href="ops-managing-objectstorage.html" title="Chapter 8. Managing Object Storage" /><link rel="next" href="ops-managing-dashboards.html" title="Chapter 10. Managing the Dashboard" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-operations.html">Operations Guide</a><span> › </span><a class="crumb" href="ops-managing-networking.html">Managing Networking</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Operations Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="gettingstarted-ops.html"><span class="number">1 </span><span class="name">Operations Overview</span></a></li><li class="inactive"><a href="tutorials.html"><span class="number">2 </span><span class="name">Tutorials</span></a></li><li class="inactive"><a href="third-party-integrations.html"><span class="number">3 </span><span class="name">Third-Party Integrations</span></a></li><li class="inactive"><a href="ops-managing-identity.html"><span class="number">4 </span><span class="name">Managing Identity</span></a></li><li class="inactive"><a href="ops-managing-compute.html"><span class="number">5 </span><span class="name">Managing Compute</span></a></li><li class="inactive"><a href="ops-managing-esx.html"><span class="number">6 </span><span class="name">Managing ESX</span></a></li><li class="inactive"><a href="ops-managing-blockstorage.html"><span class="number">7 </span><span class="name">Managing Block Storage</span></a></li><li class="inactive"><a href="ops-managing-objectstorage.html"><span class="number">8 </span><span class="name">Managing Object Storage</span></a></li><li class="inactive"><a href="ops-managing-networking.html"><span class="number">9 </span><span class="name">Managing Networking</span></a></li><li class="inactive"><a href="ops-managing-dashboards.html"><span class="number">10 </span><span class="name">Managing the Dashboard</span></a></li><li class="inactive"><a href="ops-managing-orchestration.html"><span class="number">11 </span><span class="name">Managing Orchestration</span></a></li><li class="inactive"><a href="topic-ttn-5fg-4v.html"><span class="number">12 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li class="inactive"><a href="system-maintenance.html"><span class="number">13 </span><span class="name">System Maintenance</span></a></li><li class="inactive"><a href="bura-overview.html"><span class="number">14 </span><span class="name">Backup and Restore</span></a></li><li class="inactive"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html"><span class="number">15 </span><span class="name">Troubleshooting Issues</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 8. Managing Object Storage" href="ops-managing-objectstorage.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 10. Managing the Dashboard" href="ops-managing-dashboards.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-operations.html">Operations Guide</a><span> › </span><a class="crumb" href="ops-managing-networking.html">Managing Networking</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 8. Managing Object Storage" href="ops-managing-objectstorage.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 10. Managing the Dashboard" href="ops-managing-dashboards.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="ops-managing-networking"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></div><div><h1 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Networking</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-managing_networking.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_networking.xml</li><li><span class="ds-label">ID: </span>ops-managing-networking</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="ops-managing-networking.html#topic-gll-nsn-15"><span class="number">9.1 </span><span class="name">Configuring the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Firewall</span></a></span></dt><dt><span class="section"><a href="ops-managing-networking.html#DesignateOverview"><span class="number">9.2 </span><span class="name">DNS Service Overview</span></a></span></dt><dt><span class="section"><a href="ops-managing-networking.html#neutron-overview"><span class="number">9.3 </span><span class="name">Networking Service Overview</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Networking service.
 </p><div class="sect1" id="topic-gll-nsn-15"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Firewall</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#topic-gll-nsn-15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>topic-gll-nsn-15</li></ul></div></div></div></div><p>
  The following instructions provide information about how to identify and
  modify the overall <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall that is configured in front of the
  control services. This firewall is administered only by a cloud admin and is
  not available for tenant use for private network firewall services.
 </p><p>
  During the installation process, the configuration processor will
  automatically generate "allow" firewall rules for each server based on the
  services deployed and block all other ports. These are populated in
  <code class="literal">~/openstack/my_cloud/info/firewall_info.yml</code>, which includes
  a list of all the ports by network, including the addresses on which the
  ports will be opened. This is described in more detail in
  <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”, Section 5.2 “Concepts”, Section 5.2.10 “Networking”, Section 5.2.10.5 “Firewall Configuration”</span>.
 </p><p>
  The <code class="literal">firewall_rules.yml</code> file in the input model allows you
  to define additional rules for each network group. You can read more about
  this in <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.15 “Firewall Rules”</span>.
 </p><p>
  The purpose of this document is to show you how to make post-installation
  changes to the firewall rules if the need arises.
 </p><div id="id-1.6.11.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   This process is not to be confused with Firewall-as-a-Service (see
   <span class="intraxref">Book “User Guide”, Chapter 14 “Using Firewall as a Service (FWaaS)”</span>),
   which is a separate service that enables the ability for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tenants
   to create north-south, network-level firewalls to provide stateful
   protection to all instances in a private, tenant network. This service is
   optional and is tenant-configured.
  </p></div><div class="sect2" id="idg-all-operations-configure-firewall-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Changes to the Firewall Rules</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-operations-configure-firewall-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configure-firewall-xml-7</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your
     <code class="literal">~/openstack/my_cloud/definition/data/firewall_rules.yml</code>
     file and add the lines necessary to allow the port(s) needed through the
     firewall.
    </p><p>
     In this example we are going to open up port range 5900-5905 to allow VNC
     traffic through the firewall:
    </p><div class="verbatim-wrap"><pre class="screen">  - name: VNC
    network-groups:
  - MANAGEMENT
    rules:
     - type: allow
       remote-ip-prefix:  0.0.0.0/0
       port-range-min: 5900
       port-range-max: 5905
       protocol: tcp</pre></div><div id="id-1.6.11.3.7.2.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The example above shows a <code class="literal">remote-ip-prefix</code> of
      <code class="literal">0.0.0.0/0</code> which opens the ports up to all IP ranges.
      To be more secure you can specify your local IP address CIDR you will
      be running the VNC connect from.
     </p></div></li><li class="listitem "><p>
     Commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "firewall rule update"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create the deployment directory structure:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Change to the deployment directory and run the
     <code class="literal">osconfig-iptables-deploy.yml</code> playbook to update your
     iptable rules to allow VNC:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-iptables-deploy.yml</pre></div></li></ol></div><p>
   You can repeat these steps as needed to add, remove, or edit any of these
   firewall rules.
  </p></div></div><div class="sect1" id="DesignateOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#DesignateOverview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span>DesignateOverview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS service provides multi-tenant Domain Name Service with REST
  API management for domain and records.
 </p><div id="id-1.6.11.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   The DNS Service is not intended to be used as an
   <span class="emphasis"><em>internal</em></span> or
   <span class="emphasis"><em>private</em></span> DNS service. The name records in
   DNSaaS should be treated as public information that anyone could query.
   There are controls to prevent tenants from creating records for domains they
   do not own. TSIG provides a <span class="bold"><strong>T</strong></span>ransaction
   <span class="bold"><strong>SIG</strong></span> nature to ensure integrity during zone
   transfer to other DNS servers.
  </p></div><div class="sect2" id="id-1.6.11.4.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For more information about Designate REST APIs, see the <span class="productname">OpenStack</span> REST
     API Documentation at
     <a class="link" href="http://docs.openstack.org/developer/designate/rest.html" target="_blank">http://docs.openstack.org/developer/designate/rest.html</a>.
    </p></li><li class="listitem "><p>
     For a glossary of terms for Designate, see the <span class="productname">OpenStack</span> glossary at
     <a class="link" href="http://docs.openstack.org/developer/designate/glossary.html" target="_blank">http://docs.openstack.org/developer/designate/glossary.html</a>.
    </p></li></ul></div></div><div class="sect2" id="DesignateInitialConfig"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designate Initial Configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#DesignateInitialConfig">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>DesignateInitialConfig</li></ul></div></div></div></div><p>
  After the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
  has been completed, Designate requires initial configuration to operate.
 </p><div class="sect3" id="sec-designate-identify"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying Name Server Public IPs</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-identify">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-identify</li></ul></div></div></div></div><p>
   Depending on the back-end, the method used to identify the name servers'
   public IPs will differ.
  </p><div class="sect4" id="sec-designate-infoblox"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">InfoBlox</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-infoblox">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-infoblox</li></ul></div></div></div></div><p>
    InfoBlox will act as your public name servers, consult the InfoBlox
    management UI to identify the IPs.
   </p></div><div class="sect4" id="sec-designate-powerdns-bind"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">PowerDNS or BIND Back-end</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-powerdns-bind">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-powerdns-bind</li></ul></div></div></div></div><p>
    You can find the name server IPs in <code class="filename">/etc/hosts</code> by
    looking for the <code class="literal">ext-api</code> addresses, which are the
    addresses of the controllers. For example:
   </p><div class="verbatim-wrap"><pre class="screen">192.168.10.1 example-cp1-c1-m1-extapi
192.168.10.2 example-cp1-c1-m2-extapi
192.168.10.3 example-cp1-c1-m3-extapi</pre></div></div><div class="sect4" id="sec-designate-a-record"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Name Server A Records</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-a-record">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-a-record</li></ul></div></div></div></div><p>
    Each name server requires a public name, for example
    <code class="literal">ns1.example.com.</code>, to which Designate-managed domains will
    be delegated. There are two common locations where these may be registered,
    either within a zone hosted on Designate itself, or within a zone hosted on a
    external DNS service.
   </p><p>
    <span class="bold"><strong>If you are using an externally managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
      For each name server public IP, create the necessary A records in the
      external system.
     </p></li></ul></div></div><p>
    <span class="bold"><strong>If you are using a Designate-managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the zone in Designate which will contain the records:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack zone create --email hostmaster@example.com example.com.
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| action         | CREATE                               |
| created_at     | 2016-03-09T13:16:41.000000           |
| description    | None                                 |
| email          | hostmaster@example.com               |
| id             | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
| masters        |                                      |
| name           | example.com.                         |
| pool_id        | 794ccc2c-d751-44fe-b57f-8894c9f5c842 |
| project_id     | a194d740818942a8bea6f3674e0a3d71     |
| serial         | 1457529400                           |
| status         | PENDING                              |
| transferred_at | None                                 |
| ttl            | 3600                                 |
| type           | PRIMARY                              |
| updated_at     | None                                 |
| version        | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step "><p>
      For each name server public IP, create an A record. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset create --records 192.168.10.1 --type A example.com. ns1.example.com.
+-------------+--------------------------------------+
| Field       | Value                                |
+-------------+--------------------------------------+
| action      | CREATE                               |
| created_at  | 2016-03-09T13:18:36.000000           |
| description | None                                 |
| id          | 09e962ed-6915-441a-a5a1-e8d93c3239b6 |
| name        | ns1.example.com.                     |
| records     | 192.168.10.1                         |
| status      | PENDING                              |
| ttl         | None                                 |
| type        | A                                    |
| updated_at  | None                                 |
| version     | 1                                    |
| zone_id     | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
+-------------+--------------------------------------+</pre></div></li><li class="step "><p>
      When records have been added, list the record sets in the zone to
      validate:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset list example.com.
+--------------+------------------+------+---------------------------------------------------+
| id           | name             | type | records                                           |
+--------------+------------------+------+---------------------------------------------------+
| 2d6cf...655b | example.com.     | SOA  | ns1.example.com. hostmaster.example.com 145...600 |
| 33466...bd9c | example.com.     | NS   | ns1.example.com.                                  |
| da98c...bc2f | example.com.     | NS   | ns2.example.com.                                  |
| 672ee...74dd | example.com.     | NS   | ns3.example.com.                                  |
| 09e96...39b6 | ns1.example.com. | A    | 192.168.10.1                                      |
| bca4f...a752 | ns2.example.com. | A    | 192.168.10.2                                      |
| 0f123...2117 | ns3.example.com. | A    | 192.168.10.3                                      |
+--------------+------------------+------+---------------------------------------------------+</pre></div></li><li class="step "><p>
      Contact your domain registrar requesting <span class="emphasis"><em>Glue
      Records</em></span> to be registered in the
      <code class="literal">com.</code> zone for the nameserver and public
      IP address pairs above. If you are using a sub-zone of an existing
      company zone (for example, <code class="literal">ns1.cloud.mycompany.com.</code>),
      the Glue must be placed in the <code class="literal">mycompany.com.</code> zone.
     </p></li></ol></div></div></div><div class="sect4" id="sec-designate-more"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.2.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-more</li></ul></div></div></div></div><p>
    For additional DNS integration and configuration information, see the
    <span class="productname">OpenStack</span> Designate documentation at
    <a class="link" href="https://docs.openstack.org/designate/pike/index.html" target="_blank">https://docs.openstack.org/designate/pike/index.html</a>.
   </p><p>
    For more information on creating servers, domains and examples, see the
    <span class="productname">OpenStack</span> REST API documentation at
    <a class="link" href="https://developer.openstack.org/api-ref/dns/" target="_blank">https://developer.openstack.org/api-ref/dns/</a>.
   </p></div></div></div><div class="sect2" id="DesignateMonitoringSupport"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#DesignateMonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>DesignateMonitoringSupport</li></ul></div></div></div></div><div class="sect3" id="MonitoringSupport"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#MonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>MonitoringSupport</li></ul></div></div></div></div><p>
   Additional monitoring support for the DNS Service (Designate) has been added
   to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   In the Networking section of the Operations Console, you can see alarms for all of
   the DNS Services (Designate), such as designate-zone-manager, designate-api,
   designate-pool-manager, designate-mdns, and designate-central after running
   <code class="literal">designate-stop.yml</code>.
  </p><p>
   You can run <code class="literal">designate-start.yml</code> to start the DNS Services
   back up and the alarms will change from a red status to green and be removed
   from the <span class="bold"><strong>New Alarms</strong></span> panel of the
   Operations Console.
  </p><p>
   An example of the generated alarms from the Operations Console is provided below
   after running <code class="literal">designate-stop.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">ALARM:  STATE:  ALARM ID:  LAST CHECK:  DIMENSION:
Process Check
0f221056-1b0e-4507-9a28-2e42561fac3e 2016-10-03T10:06:32.106Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-zone-manager,
component=designate-zone-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
50dc4c7b-6fae-416c-9388-6194d2cfc837 2016-10-03T10:04:32.086Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-api,
component=designate-api,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
55cf49cd-1189-4d07-aaf4-09ed08463044 2016-10-03T10:05:32.109Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-pool-manager,
component=designate-pool-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
c4ab7a2e-19d7-4eb2-a9e9-26d3b14465ea 2016-10-03T10:06:32.105Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-mdns,
component=designate-mdns,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm
HTTP Status
c6349bbf-4fd1-461a-9932-434169b86ce5 2016-10-03T10:05:01.731Z service=dns,
cluster=cluster1,
url=http://100.60.90.3:9001/,
hostname=ardana-cp1-c1-m3-mgmt,
component=designate-api,
control_plane=control-plane-1,
api_endpoint=internal,
cloud_name=entry-scale-kvm,
monitored_host_type=instance

Process Check
ec2c32c8-3b91-4656-be70-27ff0c271c89 2016-10-03T10:04:32.082Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-central,
component=designate-central,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm</pre></div></div></div></div><div class="sect1" id="neutron-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Service Overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#neutron-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>neutron-overview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Networking is a virtual networking service that leverages the
  OpenStack Neutron service to provide network connectivity and addressing to
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Compute service devices.
 </p><p>
  The Networking service also provides an API to configure and manage a variety
  of network services.
 </p><p>
  You can use the Networking service to connect guest servers or you can define
  and configure your own virtual network topology.
 </p><div class="sect2" id="installing-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Networking service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#installing-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>installing-the-networking-service</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Network Administrators are responsible for planning for the Neutron
   networking service, and once installed, to configure the service to meet the
   needs of their cloud network users.
  </p></div><div class="sect2" id="working-with-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Working with the Networking service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#working-with-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>working-with-the-networking-service</li></ul></div></div></div></div><p>
   To perform tasks using the Networking service, you can use the dashboard,
   API or CLI.
  </p></div><div class="sect2" id="restarting"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring the Networking service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#restarting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>restarting</li></ul></div></div></div></div><p>
   If you change any of the network configuration after installation, it is
   recommended that you reconfigure the Networking service by running the
   neutron-reconfigure playbook.
  </p><p>
   On the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></div><div class="sect2" id="for-more-information"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#for-more-information">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>for-more-information</li></ul></div></div></div></div><p>
   For information on how to operate your cloud we suggest you read the
   <a class="link" href="http://docs.openstack.org/ops/" target="_blank">OpenStack Operations
   Guide</a>. The <span class="emphasis"><em>Architecture</em></span> section contains useful
   information about how an OpenStack Cloud is put together. However, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   takes care of these details for you. The <span class="emphasis"><em>Operations</em></span>
   section contains information on how to manage the system.
  </p></div><div class="sect2" id="neutron-external-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron External Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#neutron-external-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>neutron-external-networks</li></ul></div></div></div></div><div class="sect3" id="id-1.6.11.5.9.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">External networks overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.9.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This topic explains how to create a Neutron external network.
  </p><p>
   External networks provide access to the internet.
  </p><p>
   The typical use is to provide an IP address that can be used to reach a VM
   from an external network which can be a public network like the internet or
   a network that is private to an organization.
  </p></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Ansible Playbook</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-neutron-external-networks-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-4</li></ul></div></div></div></div><p>
   This playbook will query the Networking service for an existing external
   network, and then create a new one if you do not already have one. The
   resulting external network will have the name <code class="literal">ext-net</code>
   with a subnet matching the CIDR you specify in the command below.
  </p><p>
   If you need to specify more granularity, for example specifying an
   allocation pool for the subnet, use the
   <a class="xref" href="ops-managing-networking.html#idg-all-networking-neutron-external-networks-xml-6" title="9.3.5.3. Using the NeutronClient CLI">Section 9.3.5.3, “Using the NeutronClient CLI”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&lt;CIDR&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e EXT_NET_CIDR=&lt;CIDR&gt;</code>
       </p>
      </td><td>
       <p>
        Optional. You can use this switch to specify the external network CIDR.
        If you choose not to use this switch, or use a wrong value, the VMs
        will not be accessible over the network.
       </p>
       <p>
        This CIDR will be from the <code class="literal">EXTERNAL VM</code> network.
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the NeutronClient CLI</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-neutron-external-networks-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-6</li></ul></div></div></div></div><p>
   For more granularity you can utilize the Neutron command line tool to create
   your external network.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the Admin creds:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
     Create the external network and then the subnet using these commands
     below.
    </p><p>
     Creating the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron net-create --router:external &lt;external-network-name&gt;</pre></div><p>
     Creating the subnet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron subnet-create <em class="replaceable ">EXTERNAL-NETWORK-NAME</em> <em class="replaceable ">CIDR</em> --gateway <em class="replaceable ">GATEWAY</em> --allocation-pool start=<em class="replaceable ">IP_START</em>,end=<em class="replaceable ">IP_END</em> [--disable-dhcp]</pre></div><p>
     Where:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>external-network-name</td><td>
         <p>
          This is the name given to your external network. This is a unique
          value that you will choose. The value <code class="literal">ext-net</code> is
          usually used.
         </p>
        </td></tr><tr><td>CIDR</td><td>
         <p>
          You can use this switch to specify the external network CIDR. If you
          choose not to use this switch, or use a wrong value, the VMs will not
          be accessible over the network.
         </p>
         <p>
          This CIDR will be from the EXTERNAL VM network.
         </p>
        </td></tr><tr><td>--gateway</td><td>
         <p>
          Optional switch to specify the gateway IP for your subnet. If this is
          not included then it will choose the first available IP.
         </p>
        </td></tr><tr><td>
         <p>
          --allocation-pool start end
         </p>
        </td><td>
         <p>
          Optional switch to specify a start and end IP address to use as the
          allocation pool for this subnet.
         </p>
        </td></tr><tr><td>--disable-dhcp</td><td>
         <p>
          Optional switch if you want to disable DHCP on this subnet. If this
          is not specified then DHCP will be enabled.
         </p>
        </td></tr></tbody></table></div></li></ol></div></div><div class="sect3" id="MultipleExternalNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple External Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#MultipleExternalNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>MultipleExternalNetworks</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides the ability to have multiple external networks, by using
   the Network Service (Neutron) provider networks for external networks. You
   can configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to allow the use of provider VLANs as external
   networks by following these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Do NOT include the
     <code class="literal">neutron.l3_agent.external_network_bridge</code> tag in the
     network_groups definition for your cloud. This results in the
     <code class="literal">l3_agent.ini external_network_bridge</code> being set to an
     empty value (rather than the traditional br-ex).
    </p></li><li class="listitem "><p>
     Configure your cloud to use provider VLANs, by specifying the
     <code class="literal">provider_physical_network</code> tag on one of the
     network_groups defined for your cloud.
    </p><p>
     For example, to run provider VLANS over the EXAMPLE network group: (some
     attributes omitted for brevity)
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:

  - name: EXAMPLE
    tags:
      - neutron.networks.vlan:
          provider-physical-network: physnet1</pre></div></li><li class="listitem "><p>
     After the cloud has been deployed, you can create external networks using
     provider VLANs.
    </p><p>
     For example, using the Network Service CLI:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create external network 1 on vlan101
      </p><div class="verbatim-wrap"><pre class="screen">neutron net-create --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 101 ext-net1 --router:external true</pre></div></li><li class="listitem "><p>
       Create external network 2 on vlan102
      </p><div class="verbatim-wrap"><pre class="screen">neutron net-create --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 102 ext-net2 --router:external true</pre></div></li></ol></div></li></ol></div></div></div><div class="sect2" id="neutron-provider-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron Provider Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#neutron-provider-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>neutron-provider-networks</li></ul></div></div></div></div><p>
  This topic explains how to create a Neutron provider network.
 </p><p>
  A provider network is a virtual network created in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that
  is consumed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services. The distinctive element of a provider
  network is that it does not create a virtual router; rather, it depends on
  L3 routing that is provided by the infrastructure.
 </p><p>
  A provider network is created by adding the specification to the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  input model. It consists of at least one network and one or more subnets.
 </p><div class="sect3" id="id-1.6.11.5.10.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input model</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The input model is the primary mechanism a cloud admin uses in defining a
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation. It exists as a directory with a data subdirectory that
   contains YAML files. By convention, any service that creates a Neutron
   provider network will create a subdirectory under the data directory and the
   name of the subdirectory shall be the project name. For example, the Octavia
   project will use Neutron provider networks so it will have a subdirectory
   named 'octavia' and the config file that specifies the neutron network will
   exist in that subdirectory.
  </p><div class="verbatim-wrap"><pre class="screen">├── cloudConfig.yml
    ├── data
    │   ├── control_plane.yml
    │   ├── disks_compute.yml
    │   ├── disks_controller_1TB.yml
    │   ├── disks_controller.yml
    │   ├── firewall_rules.yml
    │   ├── net_interfaces.yml
    │   ├── network_groups.yml
    │   ├── networks.yml
    │   ├── neutron
    │   │   └── neutron_config.yml
    │   ├── nic_mappings.yml
    │   ├── server_groups.yml
    │   ├── server_roles.yml
    │   ├── servers.yml
    │   ├── swift
    │   │   └── rings.yml
    │   └── octavia
    │       └── octavia_config.yml
    ├── README.html
    └── README.md</pre></div></div><div class="sect3" id="id-1.6.11.5.10.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network/Subnet specification</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The elements required in the input model for you to define a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     name
    </p></li><li class="listitem "><p>
     network_type
    </p></li><li class="listitem "><p>
     physical_network
    </p></li></ul></div><p>
   Elements that are optional when defining a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     segmentation_id
    </p></li><li class="listitem "><p>
     shared
    </p></li></ul></div><p>
   Required elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     cidr
    </p></li></ul></div><p>
   Optional elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     allocation_pools which will require start and end addresses
    </p></li><li class="listitem "><p>
     host_routes which will require a destination and nexthop
    </p></li><li class="listitem "><p>
     gateway_ip
    </p></li><li class="listitem "><p>
     no_gateway
    </p></li><li class="listitem "><p>
     enable-dhcp
    </p></li></ul></div><p>
   NOTE: Only IPv4 is supported at the present time.
  </p></div><div class="sect3" id="id-1.6.11.5.10.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the network values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Attribute</th><th>Required/optional</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>name</td><td>Required</td><td> </td><td> </td></tr><tr><td>network_type</td><td>Required</td><td>flat, vlan, vxlan</td><td>The type of desired network</td></tr><tr><td>physical_network</td><td>Required</td><td>Valid</td><td>Name of physical network that is overlayed with the virtual network</td></tr><tr><td>segmentation_id</td><td>Optional</td><td>vlan or vxlan ranges</td><td>VLAN id for vlan or tunnel id for vxlan</td></tr><tr><td>shared</td><td>Optional</td><td>True</td><td>Shared by all projects or private to a single project</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.10.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Subnet details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the subnet values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>cidr</td><td>Required</td><td>Valid CIDR range</td><td>for example, 172.30.0.0/24</td></tr><tr><td>allocation_pools</td><td>Optional</td><td>See allocation_pools table below</td><td> </td></tr><tr><td>host_routes</td><td>Optional</td><td>See host_routes table below</td><td> </td></tr><tr><td>gateway_ip</td><td>Optional</td><td>Valid IP addr</td><td>Subnet gateway to other nets</td></tr><tr><td>no_gateway</td><td>Optional</td><td>True</td><td>No distribution of gateway</td></tr><tr><td>enable-dhcp</td><td>Optional</td><td>True</td><td>Enable dhcp for this subnet</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.10.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ALLOCATION_POOLS details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains allocation pool settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>start</td><td>Required</td><td>Valid IP addr</td><td>First ip address in pool</td></tr><tr><td>end</td><td>Required</td><td>Valid IP addr</td><td>Last ip address in pool</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.10.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HOST_ROUTES details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains host route settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>destination</td><td>Required</td><td>Valid CIDR</td><td>Destination subnet</td></tr><tr><td>nexthop</td><td>Required</td><td>Valid IP addr</td><td>Hop to take to destination subnet</td></tr></tbody></table></div><div id="id-1.6.11.5.10.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Multiple destination/nexthop values can be used.
   </p></div></div><div class="sect3" id="id-1.6.11.5.10.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examples</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following examples show the configuration file settings for Neutron and
   Octavia.
  </p><p>
   <span class="bold"><strong>Octavia configuration</strong></span>
  </p><p>
   This file defines the mapping. It does not need to be edited unless you want
   to change the name of your VLAN.
  </p><p>
   Path:
   <code class="literal">~/openstack/my_cloud/definition/data/octavia/octavia_config.yml</code>
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name: OCTAVIA-CONFIG-CP1
      services:
        - octavia
      data:
        amp_network_name: OCTAVIA-MGMT-NET</pre></div><p>
   <span class="bold"><strong>Neutron configuration</strong></span>
  </p><p>
   Input your network configuration information for your provider VLANs in
   <code class="literal">neutron_config.yml</code> found here:
  </p><p>
   <code class="literal">~/openstack/my_cloud/definition/data/neutron/</code>.
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  NEUTRON-CONFIG-CP1
      services:
        - neutron
      data:
        neutron_provider_networks:
        - name: OCTAVIA-MGMT-NET
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 2754
          cidr: 10.13.189.0/24
          no_gateway:  True
          enable_dhcp: True
          allocation_pools:
            - start: 10.13.189.4
              end: 10.13.189.252
          host_routes:
            # route to MANAGEMENT-NET
            - destination: 10.13.111.128/26
              nexthop:  10.13.189.5</pre></div></div><div class="sect3" id="id-1.6.11.5.10.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Implementing your changes</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.10.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "configuring provider network"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Then continue with your clean cloud installation.
    </p></li><li class="listitem "><p>
     If you are only adding a Neutron Provider network to an existing model,
     then run the neutron-deploy.yml playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-deploy.yml</pre></div></li></ol></div></div><div class="sect3" id="MultipleProviderNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Provider Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#MultipleProviderNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>MultipleProviderNetworks</li></ul></div></div></div></div><p>
   The physical network infrastructure must be configured to convey the
   provider VLAN traffic as tagged VLANs to the cloud compute nodes and network
   service network nodes. Configuration of the physical network infrastructure
   is outside the scope of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> software.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> automates the server networking configuration and the
   Network Service configuration based on information in the cloud definition.
   To configure the system for provider VLANs, specify the<span class="emphasis"><em>
   neutron.networks.vlan</em></span> tag with a
   <span class="emphasis"><em>provider-physical-network</em></span> attribute on one or more
   network groups. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:

        - name: NET_GROUP_A
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet1

        - name: NET_GROUP_B
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet2</pre></div><p>
   A network group is associated with a server network interface via an
   interface model. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">interface-models:
        - name: INTERFACE_SET_X
        network-interfaces:
        - device:
        name: bond0
        network-groups:
        - NET_GROUP_A
        - device:
        name: eth3
        network-groups:
        - NET_GROUP_B</pre></div><p>
   A network group used for provider VLANs may contain only a single <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   network, because that VLAN must span all compute nodes and any Network
   Service network nodes/controllers (that is, it is a single L2 segment). The
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> network must be defined with tagged-vlan false, otherwise a Linux
   VLAN network interface will be created. For example:
  </p><div class="verbatim-wrap"><pre class="screen">networks:

        - name: NET_A
        tagged-vlan: false
        network-group: NET_GROUP_A

        - name: NET_B
        tagged-vlan: false
        network-group: NET_GROUP_B</pre></div><p>
   When the cloud is deployed, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> will create the appropriate
   bridges on the servers, and set the appropriate attributes in the Neutron
   configuration files (for example, bridge_mappings).
  </p><p>
   After the cloud has been deployed, create Network Service network objects
   for each provider VLAN. For example, using the Network Service CLI:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron net-create --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 101 mynet101
<code class="prompt user">ardana &gt; </code>neutron net-create --provider:network_type vlan --provider:physical_network physnet2 --provider:segmentation_id 234 mynet234</pre></div></div><div class="sect3" id="idg-all-networking-neutron-provider-networks-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-neutron-provider-networks-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-provider-networks-xml-10</li></ul></div></div></div></div><p>
   For more information on the Network Service command-line interface (CLI),
   see the OpenStack networking command-line client reference:
   <a class="link" href="http://docs.openstack.org/cli-reference/content/neutronclient_commands.html" target="_blank">http://docs.openstack.org/cli-reference/content/neutronclient_commands.html</a>
  </p></div></div><div class="sect2" id="ipam"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using IPAM Drivers in the Networking Service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#ipam">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span>ipam</li></ul></div></div></div></div><p>
  This topic describes how to choose and implement an IPAM driver.
 </p><div class="sect3" id="id-1.6.11.5.11.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting and implementing an IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.11.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Beginning with the Liberty release, OpenStack networking includes a
   pluggable interface for the IP Address Management (IPAM) function. This
   interface creates a driver framework for the allocation and de-allocation of
   subnets and IP addresses, enabling the integration of alternate IPAM
   implementations or third-party IP Address Management systems.
  </p><p>
   There are three possible IPAM driver options:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Non-pluggable driver. This option is the default when the ipam_driver
     parameter is not specified in neutron.conf.
    </p></li><li class="listitem "><p>
     Pluggable reference IPAM driver. The pluggable IPAM driver interface was
     introduced in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> (OpenStack Liberty). It is a refactoring of
     the Kilo non-pluggable driver to use the new pluggable interface. The
     setting in neutron.conf to specify this driver is <code class="literal">ipam_driver =
     internal</code>.
    </p></li><li class="listitem "><p>
     Pluggable Infoblox IPAM driver. The pluggable Infoblox IPAM driver is a
     third-party implementation of the pluggable IPAM interface. the
     corresponding setting in neutron.conf to specify this driver is
     <code class="literal">ipam_driver =
     networking_infoblox.ipam.driver.InfobloxPool</code>.
    </p><div id="id-1.6.11.5.11.3.4.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can use either the non-pluggable IPAM driver or a pluggable one.
      However, you cannot use both.
     </p></div></li></ul></div></div><div class="sect3" id="id-1.6.11.5.11.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Pluggable reference IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To indicate that you want to use the Pluggable reference IPAM driver, the
   only parameter needed is "ipam_driver." You can set it by looking for the
   following commented line in the
   <code class="literal">neutron.conf.j2</code> template (ipam_driver = internal)
   uncommenting it, and committing the file. After following the standard
   steps to deploy Neutron, Neutron will be configured to run using the
   Pluggable reference IPAM driver.
  </p><p>
   As stated, the file you must edit is <code class="literal">neutron.conf.j2</code> on
   the Cloud Lifecycle Manager in the directory
   <code class="literal">~/openstack/my_cloud/config/neutron</code>. Here is the relevant
   section where you can see the <code class="literal">ipam_driver</code> parameter
   commented out:
  </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
  ...
  l3_ha_net_cidr = 169.254.192.0/18

  # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
  # ipam_driver = internal
  ...</pre></div><p>
   After uncommenting the line <code class="literal">ipam_driver = internal</code>,
   commit the file using git commit from the <code class="literal">openstack/my_cloud</code>
   directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m 'My config for enabling the internal IPAM Driver'</pre></div><p>
   Then follow the steps to deploy <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in the
   <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Preface “Installation Overview”</span> appropriate to your cloud configuration.
  </p><div id="id-1.6.11.5.11.4.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    Currently there is no migration path from the non-pluggable driver to a
    pluggable IPAM driver because changes are needed to database tables and
    Neutron currently cannot make those changes.
   </p></div></div><div class="sect3" id="id-1.6.11.5.11.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As suggested above, using the Infoblox IPAM driver requires changes to
   existing parameters in <code class="literal">nova.conf</code> and
   <code class="literal">neutron.conf</code>. If you want to use the infoblox appliance,
   you will need to add the "infoblox service-component" to the service-role
   containing the neutron API server. To use the infoblox appliance for IPAM,
   both the agent <span class="emphasis"><em>and</em></span> the Infoblox IPAM driver are
   required. The <code class="literal">infoblox-ipam-agent</code> should be deployed on
   the same node where the neutron-server component is running. Usually this is
   a Controller node.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Have the Infoblox appliance running on the management network (the
     Infoblox appliance admin or the datacenter administrator should know how
     to perform this step).
    </p></li><li class="listitem "><p>
     Change the control plane definition to add
     i<code class="literal">nfoblox-ipam-agent</code> as a service in the controller node
     cluster (see change in bold). Make the changes in
     <code class="literal">control_plane.yml</code> found here:
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  control-planes:
    - name: ccp
      control-plane-prefix: ccp
 ...
      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: ARDANA-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - lifecycle-manager
        - name: cluster1
          cluster-prefix: c1
          server-role: CONTROLLER-ROLE
          member-count: 3
          allocation-policy: strict
          service-components:
            - ntp-server
...
            - neutron-server
            <span class="bold"><strong>- infoblox-ipam-agent</strong></span>
...
            - designate-client
            - powerdns
      resources:
        - name: compute
          resource-prefix: comp
          server-role: COMPUTE-ROLE
          allocation-policy: any</pre></div></li><li class="listitem "><p>
     Modify the
     <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> file
     on the controller node to comment and uncomment the lines noted below to
     enable use with the Infoblox appliance:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
            ...
            l3_ha_net_cidr = 169.254.192.0/18


            # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
            # ipam_driver = internal


            # Comment out the line below if the Infoblox IPAM Driver is to be used
            # notification_driver = messaging

            # Uncomment the lines below if the Infoblox IPAM driver is to be used
            ipam_driver = networking_infoblox.ipam.driver.InfobloxPool
            notification_driver = messagingv2


            # Modify the infoblox sections below to suit your cloud environment

            [infoblox]
            cloud_data_center_id = 1
            # This name of this section is formed by "infoblox-dc:&lt;infoblox.cloud_data_center_id&gt;"
            # If cloud_data_center_id is 1, then the section name is "infoblox-dc:1"

            [infoblox-dc:0]
            http_request_timeout = 120
            http_pool_maxsize = 100
            http_pool_connections = 100
            ssl_verify = False
            wapi_version = 2.2
            admin_user_name = admin
            admin_password = infoblox
            grid_master_name = infoblox.localdomain
            grid_master_host = 1.2.3.4


            [QUOTAS]
            ...</pre></div></li><li class="listitem "><p>
     Change <code class="literal">nova.conf.j2</code> to replace the notification driver
     "messaging" to "messagingv2"
    </p><div class="verbatim-wrap"><pre class="screen"> ...

 # Oslo messaging
 notification_driver = log

 #  Note:
 #  If the infoblox-ipam-agent is to be deployed in the cloud, change the
 #  notification_driver setting from "messaging" to "messagingv2".
 notification_driver = messagingv2
 notification_topics = notifications

 # Policy
 ...</pre></div></li><li class="listitem "><p>
     Commit the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud
<code class="prompt user">ardana &gt; </code>git commit –a –m 'My config for enabling the Infoblox IPAM driver'</pre></div></li><li class="listitem "><p>
     Deploy the cloud with the changes. Due to changes to the
     control_plane.yml, you will need to rerun the config-processor-run.yml
     playbook if you have run it already during the install process.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.11.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration parameters for using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Changes required in the notification parameters in nova.conf:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in nova.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>notify_on_state_change</td><td>DEFAULT</td><td>None</td><td>vm_and_task_state</td><td>
       <p>
        Send compute.instance.update notifications on instance state changes.
       </p>
       <p>
        Vm_and_task_state means notify on vm and task state changes.
       </p>
       <p>
        Infoblox requires the value to be vm_state (notify on vm state change).
       </p>
       <p>
        <span class="bold"><strong> Thus NO CHANGE is needed for infoblox</strong></span>
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>empty list</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>NO CHANGE is needed for infoblox.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notifications to be
        "notifications"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>None</td><td>messaging</td><td>
       <p>
        <span class="bold"><strong>Change needed.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notification driver to be
        "messagingv2".
       </p>
      </td></tr></tbody></table></div><p>
   Changes to existing parameters in neutron.conf
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>ipam_driver</td><td>DEFAULT</td><td>None</td><td>
       <p>
        None
       </p>
       <p>
        (param is undeclared in neutron.conf)
       </p>
      </td><td>
       <p>
        Pluggable IPAM driver to be used by Neutron API server.
       </p>
       <p>
        For infoblox, the value is
        "networking_infoblox.ipam.driver.InfobloxPool"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>empty list</td><td>messaging</td><td>
       <p>
        The driver used to send notifications from the Neutron API server to
        the Neutron agents.
       </p>
       <p>
        The installation guide for networking-infoblox calls for the
        notification_driver to be "messagingv2"
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>None</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>No change needed</strong></span>.
       </p>
       <p>
        The row is here show the changes in the Neutron parameters described in
        the installation guide for networking-infoblox
       </p>
      </td></tr></tbody></table></div><p>
   Parameters specific to the Networking Infoblox Driver. All the parameters
   for the Infoblox IPAM driver must be defined in neutron.conf.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>cloud_data_center_id</td><td>infoblox</td><td>0</td><td>ID for selecting a particular grid from one or more grids to serve networks in
                the Infoblox back end</td></tr><tr><td>ipam_agent_workers</td><td>infoblox</td><td>1</td><td>Number of Infoblox IPAM agent works to run</td></tr><tr><td>grid_master_host</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>IP address of the grid master. WAPI requests are sent to the
                grid_master_host</td></tr><tr><td>ssl_verify</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>False</td><td>Ensure whether WAPI requests sent over HTTPS require SSL verification</td></tr><tr><td>WAPI Version</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>1.4</td><td>The WAPI version. Value should be 2.2.</td></tr><tr><td>admin_user_name</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user name to access the grid master or cloud platform appliance</td></tr><tr><td>admin_password</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user password</td></tr><tr><td>http_pool_connections</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_pool_maxsize</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_request_timeout</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>120</td><td> </td></tr></tbody></table></div><p>
  The diagram below shows Nova compute sending notification to the
  infoblox-ipam-agent
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networking-ipam.png" target="_blank"><img src="images/media-networking-ipam.png" width="" /></a></div></div></div><div class="sect3" id="id-1.6.11.5.11.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     There is no IPAM migration path from non-pluggable to pluggable IPAM
     driver
     (<a class="link" href="https://bugs.launchpad.net/neutron/+bug/1516156" target="_blank">https://bugs.launchpad.net/neutron/+bug/1516156</a>).
     This means there is no way to reconfigure the Neutron database if you
     wanted to change Neutron to use a pluggable IPAM driver. Unless you change
     the default of non-pluggable IPAM configuration to a pluggable driver at
     install time, you will have no other opportunity to make that change
     because reconfiguration of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>from using the default
     non-pluggable IPAM configuration to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> using a pluggable IPAM
     driver is not supported.
    </p></li><li class="listitem "><p>
     Upgrade from previous versions of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> to use a
     pluggable IPAM driver is not supported.
    </p></li><li class="listitem "><p>
     The Infoblox appliance does not allow for overlapping IPs. For example,
     only one tenant can have a CIDR of 10.0.0.0/24.
    </p></li><li class="listitem "><p>
     The infoblox IPAM driver fails the creation of a subnet when a there is no
     gateway-ip supplied. For example, the command "neutron subnet-create ...
     --no-gateway ..." will fail.
    </p></li></ul></div></div></div><div class="sect2" id="HP2-0LBaaSAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Load Balancing as a Service (LBaaS)</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#HP2-0LBaaSAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>HP2-0LBaaSAdmin</li></ul></div></div></div></div><p>
  <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> LBaaS Configuration</strong></span>
 </p><p>
  Load Balancing as a Service (LBaaS) is an advanced networking service that
  allows load balancing of multi-node environments. It provides the ability to
  spread requests across multiple servers thereby reducing the load on any
  single server. This document describes the installation steps for LBaaS v1
  (see prerequisites) and the configuration for LBaaS v1 and v2.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> can support either LBaaS v1 or LBaaS v2 to allow for wide
  ranging customer requirements. If the decision is made to utilize LBaaS v1 it
  is highly unlikely that you will be able to perform an on-line upgrade of the
  service to v2 after the fact as the internal data structures are
  significantly different. Should you wish to attempt an upgrade, support will
  be needed from <span class="phrase"><span class="phrase">Sales Engineering</span></span> and your chosen load balancer partner.
 </p><div id="id-1.6.11.5.12.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   The LBaaS architecture is based on a driver model to support different load
   balancers. LBaaS-compatible drivers are provided by load balancer vendors
   including F5 and Citrix. A new software load balancer driver was introduced
   in the OpenStack Liberty release called "<span class="emphasis"><em>Octavia</em></span>". The
   Octavia driver deploys a software load balancer called HAProxy. Octavia is
   the default load balancing provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> for LBaaS V2.
   
   Until Octavia is configured the creation of load balancers will fail with an
   error. Please refer to <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span> document for
   information on installing Octavia.
  </p></div><div id="id-1.6.11.5.12.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   Before upgrading to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, contact F5 and
   <span class="phrase"><span class="phrase">SUSE</span></span> to determine which F5 drivers have been certified for use with
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Loading drivers not certified by <span class="phrase"><span class="phrase">SUSE</span></span> may result in
   failure of your cloud deployment.
  </p></div><p>
  LBaaS V2 offers with <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span> a software load
  balancing solution that supports both a highly available control plane and
  data plane. However, should an external hardware load balancer be selected
  the cloud operation can achieve additional performance and availability.
 </p><p>
  <span class="bold"><strong>LBaaS v1</strong></span>
 </p><p>
  Reasons to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    You must be able to configure LBaaS via Horizon.
   </p></li><li class="listitem "><p>
    Your hardware load balancer vendor does not currently support LBaaS v2.
   </p></li></ol></div><p>
  Reasons not to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    No active development is being performed on this API in the OpenStack
    community. (Security fixes are still being worked upon).
   </p></li><li class="listitem "><p>
    It does not allow for multiple ports on the same VIP (for example, to
    support both port 80 and 443 on a single VIP).
   </p></li><li class="listitem "><p>
    It will never be able to support TLS termination/re-encryption at the load
    balancer.
   </p></li><li class="listitem "><p>
    It will never be able to support L7 rules for load balancing.
   </p></li><li class="listitem "><p>
    LBaaS v1 will likely become officially deprecated by the OpenStack
    community at the Tokyo (October 2015) summit.
   </p></li></ol></div><p>
  <span class="bold"><strong>LBaaS v2</strong></span>
 </p><p>
  Reasons to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your vendor already has a driver that supports LBaaS v2. Many hardware load
    balancer vendors already support LBaaS v2 and this list is growing all the
    time.
   </p></li><li class="listitem "><p>
    You intend to script your load balancer creation and management so a UI is
    not important right now (Horizon support will be added in a future
    release).
   </p></li><li class="listitem "><p>
    You intend to support TLS termination at the load balancer.
   </p></li><li class="listitem "><p>
    You intend to use the Octavia software load balancer (adding HA and
    scalability).
   </p></li><li class="listitem "><p>
    You do not want to take your load balancers offline to perform
    subsequent LBaaS upgrades.
   </p></li><li class="listitem "><p>
    You intend in future releases to need L7 load balancing.
   </p></li></ol></div><p>
  Reasons not to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your LBaaS vendor does not have a v2 driver.
   </p></li><li class="listitem "><p>
    You must be able to manage your load balancers from Horizon.
   </p></li><li class="listitem "><p>
    You have legacy software which utilizes the LBaaS v1 API.
   </p></li></ol></div><p>
  LBaaS v1 requires configuration changes prior to installation and is not
  recommended. LBaaS v2 is installed by default with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and requires
  minimal configuration to start the service.
 </p><div id="id-1.6.11.5.12.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   Only LBaaS V2 API currently supports load balancer failover with Octavia.
   However, in LBaaS V1 and if Octavia is not deployed when a load balancer is
   deleted it will need to be manually recreated. LBaaS v2 API includes
   automatic failover of a deployed load balancer with Octavia. More
   information about this driver can be found in
   <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span>.
  </p></div><div class="sect3" id="idg-all-networking-lbaas-admin-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-lbaas-admin-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-lbaas-admin-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> LBaaS v1</strong></span>
  </p><p>
   Installing LBaaS v1
  </p><div id="id-1.6.11.5.12.20.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    It is not recommended that LBaaS v1 is used in a production environment. It
    is recommended you use LBaaS v2. If you do deploy LBaaS v1, the upgrade to
    LBaaS v2 is non-trivial and may require the use of professional services.
   </p></div><div id="id-1.6.11.5.12.20.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you need to run LBaaS v1 instead of the default LBaaS v2, you should
    make appropriate installation preparations during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
    since LBaaS v2 is the default. If you have selected to install and use
    LBaaS v1 you will replace the <code class="filename">control_plane.yml</code>
    directories and <code class="filename">neutron.conf.j2</code> file to use version 1.
   </p></div><p>
   Before you modify the control_plane.yml file, it is recommended that you
   back up the original version of this file. Once you have backed them up,
   modify the control_plane.yml file.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit ~/openstack/my_cloud/definition/data/control_plane.yml - depending on
     your installation the control_plane.yml file might be in a different
     location.
    </p></li><li class="listitem "><p>
     In the section specifying the compute nodes (resources/compute) replace
     neutron-lbaasv2-agent with neutron-lbaas-agent - there will only be one
     occurrence in that file.
    </p></li><li class="listitem "><p>
     Save the modified file.
    </p></li><li class="listitem "><p>
     Follow the steps in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span> to commit and apply the
     changes.
    </p></li><li class="listitem "><p>
     To test the installation follow the steps outlined in
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span> after you have created a suitable subnet,
     see: <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 27 “UI Verification”, Section 27.4 “Creating an External Network”</span>.
    </p></li></ol></div><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> LBaaS v2</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must be installed for LBaaS v2.
    </p></li><li class="listitem "><p>
     Follow the instructions to install <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 31 “Configuring Load Balancer as a Service”</span>
    </p></li></ol></div></div></div><div class="sect2" id="OctaviaAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer: Octavia Driver Administration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#OctaviaAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>OctaviaAdmin</li></ul></div></div></div></div><p>
  This document provides the instructions on how to enable and manage various
  components of the Load Balancer Octavia driver if that driver is enabled.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#Alerts" title="9.3.9.1. Monasca Alerts">Section 9.3.9.1, “Monasca Alerts”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#Tuning" title="9.3.9.2. Tuning Octavia Installation">Section 9.3.9.2, “Tuning Octavia Installation”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Homogeneous Compute Configuration
     </p></li><li class="listitem "><p>
      Octavia and Floating IP's
     </p></li><li class="listitem "><p>
      Configuration Files
     </p></li><li class="listitem "><p>
      Spare Pools
     </p></li></ul></div></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#Amphora" title="9.3.9.3. Managing Amphora">Section 9.3.9.3, “Managing Amphora”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Updating the Cryptographic Certificates
     </p></li><li class="listitem "><p>
      Accessing VM information in Nova
     </p></li><li class="listitem "><p>
      Initiating Failover of an Amphora VM
     </p></li></ul></div></li></ul></div><div class="sect3" id="Alerts"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Alerts</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#Alerts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Alerts</li></ul></div></div></div></div><p>
   The Monasca-agent has the following Octavia-related plugins:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Process checks – checks if octavia processes are running. When it
     starts, it detects which processes are running and then monitors them.
    </p></li><li class="listitem "><p>
     http_connect check – checks if it can connect to octavia api servers.
    </p></li></ul></div><p>
   Alerts are displayed in the Operations Console. For more information see
   <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span>.
  </p></div><div class="sect3" id="Tuning"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning Octavia Installation</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#Tuning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Tuning</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Homogeneous Compute Configuration</strong></span>
  </p><p>
   Octavia works only with homogeneous compute node configurations. Currently,
   Octavia does not support multiple nova flavors. If Octavia needs to be
   supported on multiple compute nodes, then all the compute nodes should carry
   same set of physnets (which will be used for Octavia).
  </p><p>
   <span class="bold"><strong>Octavia and Floating IPs</strong></span>
  </p><p>
   Due to a Neutron limitation Octavia will only work with CVR routers. Another
   option is to use VLAN provider networks which do not require a router.
  </p><p>
   You cannot currently assign a floating IP address as the VIP (user facing)
   address for a load balancer created by the Octavia driver if the underlying
   Neutron network is configured to support Distributed Virtual Router (DVR).
   The Octavia driver uses a Neutron function known as
   <span class="emphasis"><em>allowed address pairs</em></span>
   to support load balancer fail over.
  </p><p>
   There is currently a Neutron bug that does not support this function in a
   DVR configuration
  </p><p>
   <span class="bold"><strong>Octavia Configuration Files</strong></span>
  </p><p>
   The system comes pre-tuned and should not need any adjustments for most
   customers. If in rare instances manual tuning is needed, follow these steps:
  </p><div id="id-1.6.11.5.13.5.10" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Changes might be lost during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> upgrades.
   </p></div><p>
   Edit the Octavia configuration files in
   <code class="literal">my_cloud/config/octavia</code>. It is recommended that any
   changes be made in all of the Octavia configuration files.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     octavia-api.conf.j2
    </p></li><li class="listitem "><p>
     octavia-health-manager.conf.j2
    </p></li><li class="listitem "><p>
     octavia-housekeeping.conf.j2
    </p></li><li class="listitem "><p>
     octavia-worker.conf.j2
    </p></li></ul></div><p>
   After the changes are made to the configuration files, redeploy the service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit changes to git.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My Octavia Config"</pre></div></li><li class="listitem "><p>
     Run the configuration processor and ready deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Octavia reconfigure.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>Spare Pools</strong></span>
  </p><p>
   The Octavia driver provides support for creating spare pools of the HAProxy
   software installed in VMs. This means instead of creating a new load
   balancer when loads increase, create new load balancer calls will pull a
   load balancer from the spare pool. The spare pools feature consumes
   resources, therefore the load balancers in the spares pool has been set to
   0, which is the default and also disables the feature.
  </p><p>
   Reasons to enable a load balancing spare pool in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     You expect a large number of load balancers to be provisioned all at once
     (puppet scripts, or ansible scripts) and you want them to come up quickly.
    </p></li><li class="listitem "><p>
     You want to reduce the wait time a customer has while requesting a new
     load balancer.
    </p></li></ol></div><p>
   To increase the number of load balancers in your spares pool, edit the
   Octavia configuration files by uncommenting the
   <code class="literal">spare_amphora_pool_size</code> and adding the number of load
   balancers you would like to include in your spares pool.
  </p><div class="verbatim-wrap"><pre class="screen"># Pool size for the spare pool
# spare_amphora_pool_size = 0</pre></div><div id="id-1.6.11.5.13.5.21" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> the spare pool cannot be used to speed up fail
    overs. If a load balancer fails in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, Octavia will always provision
    a new VM to replace that failed load balancer.
   </p></div></div><div class="sect3" id="Amphora"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Amphora</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#Amphora">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Amphora</li></ul></div></div></div></div><p>
   Octavia starts a separate VM for each load balancing function. These VMs are
   called amphora.
  </p><p>
   <span class="bold"><strong>Updating the Cryptographic Certificates</strong></span>
  </p><p>
   Octavia uses two-way SSL encryption for communication between amphora and
   the control plane. Octavia keeps track of the certificates on the amphora
   and will automatically recycle them. The certificates on the control plane
   are valid for one year after installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   You can check on the status of the certificate by logging into the
   controller node as root and running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /opt/stack/service/octavia-<em class="replaceable ">SOME UUID</em>/etc/certs/
openssl x509 -in client.pem  -text –noout</pre></div><p>
   This prints the certificate out where you can check on the expiration dates.
  </p><p>
   To renew the certificates, reconfigure Octavia. Reconfiguring causes Octavia
   to automatically generate new certificates and deploy them to the controller
   hosts.
  </p><p>
   On the Cloud Lifecycle Manager execute octavia-reconfigure:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div><p>
   <span class="bold"><strong>Accessing VM information in Nova</strong></span>
  </p><p>
   You can use <code class="literal">openstack project list</code> as an administrative
   user to obtain information about the tenant or project-id of the Octavia
   project. In the example below, the Octavia project has a project-id of
   <code class="literal">37fd6e4feac14741b6e75aba14aea833</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack project list
+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 055071d8f25d450ea0b981ca67f7ccee | glance-swift     |
| 37fd6e4feac14741b6e75aba14aea833 | octavia          |
| 4b431ae087ef4bd285bc887da6405b12 | swift-monitor    |
| 8ecf2bb5754646ae97989ba6cba08607 | swift-dispersion |
| b6bd581f8d9a48e18c86008301d40b26 | services         |
| bfcada17189e4bc7b22a9072d663b52d | cinderinternal   |
| c410223059354dd19964063ef7d63eca | monitor          |
| d43bc229f513494189422d88709b7b73 | admin            |
| d5a80541ba324c54aeae58ac3de95f77 | demo             |
| ea6e039d973e4a58bbe42ee08eaf6a7a | backup           |
+----------------------------------+------------------+</pre></div><p>
   You can then use <code class="literal">nova list --tenant &lt;project-id&gt;</code> to
   list the VMs for the Octavia tenant. Take particular note of the IP address
   on the OCTAVIA-MGMT-NET; in the example below it is
   <code class="literal">172.30.1.11</code>. For additional nova command-line options see
   <a class="xref" href="ops-managing-networking.html#idg-all-networking-octavia-admin-xml-10" title="9.3.9.5. For More Information">Section 9.3.9.5, “For More Information”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.11 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div><div id="id-1.6.11.5.13.6.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    The Amphora VMs do not have SSH or any other access. In the rare case that
    there is a problem with the underlying load balancer the whole amphora will
    need to be replaced.
   </p></div><p>
   <span class="bold"><strong>Initiating Failover of an Amphora VM</strong></span>
  </p><p>
   Under normal operations Octavia will monitor the health of the amphora
   constantly and automatically fail them over if there are any issues. This
   helps to minimize any potential downtime for load balancer users. There are,
   however, a few cases a failover needs to be initiated manually:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The Loadbalancer has become unresponsive and Octavia has not detected an
     error.
    </p></li><li class="listitem "><p>
     A new image has become available and existing load balancers need to start
     using the new image.
    </p></li><li class="listitem "><p>
     The cryptographic certificates to control and/or the HMAC password to
     verify Health information of the amphora have been compromised.
    </p></li></ol></div><p>
   To minimize the impact for end users we will keep the existing load balancer
   working until shortly before the new one has been provisioned. There will be
   a short interruption for the load balancing service so keep that in mind
   when scheduling the failovers. To achieve that follow these steps (assuming
   the management ip from the previous step):
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Assign the IP to a SHELL variable for better readability.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export MGM_IP=172.30.1.11</pre></div></li><li class="listitem "><p>
     Identify the port of the vm on the management network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-list | grep $MGM_IP
| 0b0301b9-4ee8-4fb6-a47c-2690594173f4 |                                                   | fa:16:3e:d7:50:92 |
{"subnet_id": "3e0de487-e255-4fc3-84b8-60e08564c5b7", "ip_address": "172.30.1.11"} |</pre></div></li><li class="listitem "><p>
     Disable the port to initiate a failover. Note the load balancer will still
     function but cannot be controlled any longer by Octavia.
    </p><div id="id-1.6.11.5.13.6.21.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Changes after disabling the port will result in errors.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-update --admin-state-up False 0b0301b9-4ee8-4fb6-a47c-2690594173f4
Updated port: 0b0301b9-4ee8-4fb6-a47c-2690594173f4</pre></div></li><li class="listitem "><p>
     You can check to see if the amphora failed over with <code class="literal">nova list
     --tenant &lt;project-id&gt;</code>. This may take some time and in some
     cases may need to be repeated several times. You can tell that the
     failover has been successful by the changed IP on the management network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.12 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div></li></ol></div><div id="id-1.6.11.5.13.6.22" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Do not issue too many failovers at once. In a big installation you might be
    tempted to initiate several failovers in parallel for instance to speed up
    an update of amphora images. This will put a strain on the nova service and
    depending on the size of your installation you might need to throttle the
    failover rate.
   </p></div></div><div class="sect3" id="octavia-admin-delete"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing load balancers</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#octavia-admin-delete">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>octavia-admin-delete</li></ul></div></div></div></div><p>
     The following procedures demonstrate how to delete a load
     balancer that is in the <code class="literal">ERROR</code>,
     <code class="literal">PENDING_CREATE</code>, or
     <code class="literal">PENDING_DELETE</code> state.
   </p><div class="procedure " id="id-1.6.11.5.13.7.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 9.1: </span><span class="name">
       Manually deleting load balancers created with neutron lbaasv2
       (in an upgrade/migration scenario)
      </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.13.7.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Query the Neutron service for the loadbalancer ID:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron lbaas-loadbalancer-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| id                                   | name    | tenant_id                        | vip_address  | provisioning_status | provider |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | test-lb | d62a1510b0f54b5693566fb8afeb5e33 | 192.168.1.10 | ERROR               | haproxy  |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+</pre></div></li><li class="step "><p>
         Connect to the neutron database:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use ovs_neutron</pre></div></li><li class="step "><p>
         Get the pools and healthmonitors associated with the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, healthmonitor_id, loadbalancer_id from lbaas_pools where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | healthmonitor_id                     | loadbalancer_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 26c0384b-fc76-4943-83e5-9de40dd1c78c | 323a3c4b-8083-41e1-b1d9-04e1fef1a331 | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 |
+--------------------------------------+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
         Get the members associated with the pool:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, pool_id from lbaas_members where pool_id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';
+--------------------------------------+--------------------------------------+
| id                                   | pool_id                              |
+--------------------------------------+--------------------------------------+
| 6730f6c1-634c-4371-9df5-1a880662acc9 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
| 06f0cfc9-379a-4e3d-ab31-cdba1580afc2 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
         Delete the pool members:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_members where id = '6730f6c1-634c-4371-9df5-1a880662acc9';
mysql&gt; delete from lbaas_members where id = '06f0cfc9-379a-4e3d-ab31-cdba1580afc2';</pre></div></li><li class="step "><p>
         Find and delete the listener associated with the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, loadbalancer_id, default_pool_id from lbaas_listeners where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | loadbalancer_id                      | default_pool_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 3283f589-8464-43b3-96e0-399377642e0a | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+--------------------------------------+
mysql&gt; delete from lbaas_listeners where id = '3283f589-8464-43b3-96e0-399377642e0a';</pre></div></li><li class="step "><p>
         Delete the pool associated with the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_pools where id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';</pre></div></li><li class="step "><p>
         Delete the healthmonitor associated with the pool:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_healthmonitors where id = '323a3c4b-8083-41e1-b1d9-04e1fef1a331';</pre></div></li><li class="step "><p>
         Delete the loadbalancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_loadbalancer_statistics where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
mysql&gt; delete from lbaas_loadbalancers where id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';</pre></div></li></ol></div></div><div class="procedure " id="id-1.6.11.5.13.7.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 9.2: </span><span class="name">Manually Deleting Load Balancers Created With Octavia </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.13.7.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Query the Octavia service for the loadbalancer ID:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer list --column id --column name --column provisioning_status
+--------------------------------------+---------+---------------------+
| id                                   | name    | provisioning_status |
+--------------------------------------+---------+---------------------+
| d8ac085d-e077-4af2-b47a-bdec0c162928 | test-lb | ERROR               |
+--------------------------------------+---------+---------------------+</pre></div></li><li class="step "><p>
         Query the Octavia service for the amphora IDs (in this
         example we use <code class="literal">ACTIVE/STANDBY</code> topology with 1 spare Amphora):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer amphora list
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| id                                   | loadbalancer_id                      | status    | role   | lb_network_ip | ha_ip       |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| 6dc66d41-e4b6-4c33-945d-563f8b26e675 | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | BACKUP | 172.30.1.7    | 192.168.1.8 |
| 1b195602-3b14-4352-b355-5c4a70e200cf | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | MASTER | 172.30.1.6    | 192.168.1.8 |
| b2ee14df-8ac6-4bb0-a8d3-3f378dbc2509 | None                                 | READY     | None   | 172.30.1.20   | None        |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+</pre></div></li><li class="step "><p>
         Query the Octavia service for the loadbalancer pools:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer pool list
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| id                                   | name      | project_id                       | provisioning_status | protocol | lb_algorithm | admin_state_up |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| 39c4c791-6e66-4dd5-9b80-14ea11152bb5 | test-pool | 86fba765e67f430b83437f2f25225b65 | ACTIVE              | TCP      | ROUND_ROBIN  | True           |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+</pre></div></li><li class="step "><p>
         Connect to the octavia database:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use octavia</pre></div></li><li class="step "><p>
         Delete any listeners, pools, health monitors, and members
         from the load balancer:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from listener where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
mysql&gt; delete from health_monitor where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from member where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from pool where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
         Delete the amphora entries in the database:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from amphora_health where amphora_id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; update amphora set status = 'DELETED' where id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; delete from amphora_health where amphora_id = '1b195602-3b14-4352-b355-5c4a70e200cf';
mysql&gt; update amphora set status = 'DELETED' where id = '1b195602-3b14-4352-b355-5c4a70e200cf';</pre></div></li><li class="step "><p>
         Delete the load balancer instance:
       </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; update load_balancer set provisioning_status = 'DELETED' where id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
         The following script automates the above steps:
       </p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

if (( $# != 1 )); then
  echo "Please specify a loadbalancer ID"
  exit 1
fi

LB_ID=$1

set -u -e -x

readarray -t AMPHORAE &lt; &lt;(openstack loadbalancer amphora list \
  --format value \
  --column id \
  --column loadbalancer_id \
  | grep ${LB_ID} \
  | cut -d ' ' -f 1)

readarray -t POOLS &lt; &lt;(openstack loadbalancer show ${LB_ID} \
  --format value \
  --column pools)

mysql octavia --execute "delete from listener where load_balancer_id = '${LB_ID}';"
for p in "${POOLS[@]}"; do
  mysql octavia --execute "delete from health_monitor where pool_id = '${p}';"
  mysql octavia --execute "delete from member where pool_id = '${p}';"
done
mysql octavia --execute "delete from pool where load_balancer_id = '${LB_ID}';"
for a in "${AMPHORAE[@]}"; do
  mysql octavia --execute "delete from amphora_health where amphora_id = '${a}';"
  mysql octavia --execute "update amphora set status = 'DELETED' where id = '${a}';"
done
mysql octavia --execute "update load_balancer set provisioning_status = 'DELETED' where id = '${LB_ID}';"</pre></div></li></ol></div></div></div><div class="sect3" id="idg-all-networking-octavia-admin-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-octavia-admin-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-octavia-admin-xml-10</li></ul></div></div></div></div><p>
   For more information on the Nova command-line client, see the
   <a class="link" href="http://docs.openstack.org/cli-reference/nova.html" target="_blank">OpenStack
   Compute command-line client</a> guide.
  </p><p>
   For more information on Octavia terminology, see the
   <a class="link" href="http://docs.octavia.io/review/master/main/glossary.html" target="_blank">OpenStack
   Octavia Glossary</a>
  </p></div></div><div class="sect2" id="cha-network-rbac"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Role-based Access Control in Neutron</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#cha-network-rbac">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>cha-network-rbac</li></ul></div></div></div></div><p>
   This topic explains how to achieve more granular access control for your
   Neutron networks.
  </p><p>
   Previously in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, a network object was either private to a project or
   could be used by all projects. If the network's shared attribute was True,
   then the network could be used by every project in the cloud. If false, only
   the members of the owning project could use it. There was no way for the
   network to be shared by only a subset of the projects.
  </p><p>
  <span class="phrase">Neutron Role Based Access Control (RBAC) solves this problem for
   networks. Now the network owner can create RBAC policies that give network
   access to target projects. Members of a targeted project can use the
   network named in the RBAC policy the same way as if the network was owned
   by the project. Constraints are described in the
   section</span>
  <a class="xref" href="ops-managing-networking.html#sec-network-rbac-limitation" title="9.3.10.10. Limitations">Section 9.3.10.10, “Limitations”</a>.
 </p><p>
   With RBAC you are able to let another tenant use a network that you created,
   but as the owner of the network, you need to create the subnet and the
   router for the network.
  </p><div class="sect3" id="id-1.6.11.5.14.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Network</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create demo-net
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 9c801954-ec7f-4a65-82f8-e313120aabc4 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an RBAC Policy</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here we will create an RBAC policy where a member of the project called
   'demo' will share the network with members of project 'demo2'
  </p><p>
   To create the RBAC policy, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create  --target-project <em class="replaceable ">DEMO2-PROJECT-ID</em> --type network --action access_as_shared demo-net</pre></div><p>
   Here is an example where the <em class="replaceable ">DEMO2-PROJECT-ID</em> is
   5a582af8b44b422fafcd4545bd2b7eb5
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --target-tenant 5a582af8b44b422fafcd4545bd2b7eb5 \
  --type network --action access_as_shared demo-net</pre></div></div><div class="sect3" id="id-1.6.11.5.14.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing RBACs</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To list all the RBAC rules/policies, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac list
+--------------------------------------+-------------+--------------------------------------+
| ID                                   | Object Type | Object ID                            |
+--------------------------------------+-------------+--------------------------------------+
| 0fdec7f0-9b94-42b4-a4cd-b291d04282c1 | network     | 7cd94877-4276-488d-b682-7328fc85d721 |
+--------------------------------------+-------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing the Attributes of an RBAC</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To see the attributes of a specific RBAC policy, run
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0fd89dcb-9809-4a5e-adc1-39dd676cb386 |
| object_id     | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b |
| object_type   | network                              |
| target_tenant | 5a582af8b44b422fafcd4545bd2b7eb5     |
| tenant_id     | 75eb5efae5764682bca2fede6f4d8c6f     |
+---------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deleting an RBAC Policy</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To delete an RBAC policy, run <code class="literal">openstack network rbac delete</code> passing the policy id:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">Deleted rbac_policy: 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div></div><div class="sect3" id="id-1.6.11.5.14.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sharing a Network with All Tenants</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Either the administrator or the network owner can make a network shareable
   by all tenants.
  </p><p>
   The administrator can make a tenant's network shareable by all tenants.
   To make the network <code class="literal">demo-shareall-net</code> accessible by all
   tenants in the cloud:
  </p><p>
   To share a network with all tenants:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of all projects
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
      which produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 1be57778b61645a7a1c07ca0ac488f9e | demo             |
| 5346676226274cd2b3e3862c2d5ceadd | admin            |
| 749a557b2b9c482ca047e8f4abf348cd | swift-monitor    |
| 8284a83df4df429fb04996c59f9a314b | swift-dispersion |
| c7a74026ed8d4345a48a3860048dcb39 | demo-sharee      |
| e771266d937440828372090c4f99a995 | glance-swift     |
| f43fb69f107b4b109d22431766b85f20 | services         |
+----------------------------------+------------------+</pre></div></li><li class="step "><p>
     Get a list of networks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><p>
     This produces the following list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-------------------+----------------------------------------------------+
| id                                   | name              | subnets                                            |
+--------------------------------------+-------------------+----------------------------------------------------+
| f50f9a63-c048-444d-939d-370cb0af1387 | ext-net           | ef3873db-fc7a-4085-8454-5566fb5578ea 172.31.0.0/16 |
| 9fb676f5-137e-4646-ac6e-db675a885fd3 | demo-net          | 18fb0b77-fc8b-4f8d-9172-ee47869f92cc 10.0.1.0/24   |
| 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e | demo-shareall-net | 2bbc85a9-3ffe-464c-944b-2476c7804877 10.0.250.0/24 |
| 73f946ee-bd2b-42e9-87e4-87f19edd0682 | demo-share-subset | c088b0ef-f541-42a7-b4b9-6ef3c9921e44 10.0.2.0/24   |
+--------------------------------------+-------------------+----------------------------------------------------+</pre></div></li><li class="step "><p>
     Set the network you want to share to a shared value of True:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network set --share 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     You should see the following output:
    </p><div class="verbatim-wrap"><pre class="screen">Updated network: 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div></li><li class="step "><p>
     Check the attributes of that network by running the following command
     using the ID of the network in question:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network show 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     The output will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | None                                 |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     As the owner of the <code class="literal">demo-shareall-net</code> network, view
     the RBAC attributes for
     <code class="literal">demo-shareall-net</code>
     (<code class="literal">id=8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</code>) by first
     getting an RBAC list:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_USERNAME ; echo $OS_PROJECT_NAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network rbac list</pre></div><p>
     This produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+--------------------------------------+
| id                                   | object_id                            |
+--------------------------------------+--------------------------------------+
| ...                                                                         |
| 3e078293-f55d-461c-9a0b-67b5dae321e8 | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     View the RBAC information:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 3e078293-f55d-461c-9a0b-67b5dae321e8

+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 3e078293-f55d-461c-9a0b-67b5dae321e8 |
| object_id     | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li><li class="step "><p>
     With network RBAC, the owner of the network can also make the network
     shareable by all tenants. First create the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_PROJECT_NAME ; echo $OS_USERNAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network create test-net</pre></div><p>
     The network is created:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T18:04:25Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | test-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1073                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T18:04:25Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create the RBAC. It is important that the asterisk is surrounded by
     single-quotes to prevent the shell from expanding it to all files in the
     current directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --type network \
  --action access_as_shared --target-project '*' test-net</pre></div><p>
     Here are the resulting RBAC attributes:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0b797cc6-debc-48a1-bf9d-d294b077d0d9 |
| object_id     | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.6.11.5.14.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project (<code class="literal">demo2</code>) View of Networks and Subnets</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Note that the owner of the network and subnet is not the tenant named
   <code class="literal">demo2</code>. Both the network and subnet are owned by tenant <code class="literal">demo</code>.
   <code class="literal">Demo2</code>members cannot create subnets of the network. They also cannot
   modify or delete subnets owned by <code class="literal">demo</code>.
  </p><p>
   As the tenant <code class="literal">demo2</code>, you can get a list of neutron networks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-----------+--------------------------------------------------+
| id                                   | name      | subnets                                          |
+--------------------------------------+-----------+--------------------------------------------------+
| f60f3896-2854-4f20-b03f-584a0dcce7a6 | ext-net   | 50e39973-b2e3-466b-81c9-31f4d83d990b             |
| c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | demo-net  | d9b765da-45eb-4543-be96-1b69a00a2556 10.0.1.0/24 |
   ...
+--------------------------------------+-----------+--------------------------------------------------+</pre></div><p>
   And get a list of subnets:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet list --network c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+---------+--------------------------------------+---------------+
| ID                                   | Name    | Network                              | Subnet        |
+--------------------------------------+---------+--------------------------------------+---------------+
| a806f28b-ad66-47f1-b280-a1caa9beb832 | ext-net | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | 10.0.1.0/24   |
+--------------------------------------+---------+--------------------------------------+---------------+</pre></div><p>
To show details of the subnet:
</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet show d9b765da-45eb-4543-be96-1b69a00a2556</pre></div><div class="verbatim-wrap"><pre class="screen">+-------------------+--------------------------------------------+
| Field             | Value                                      |
+-------------------+--------------------------------------------+
| allocation_pools  | {"start": "10.0.1.2", "end": "10.0.1.254"} |
| cidr              | 10.0.1.0/24                                |
| dns_nameservers   |                                            |
| enable_dhcp       | True                                       |
| gateway_ip        | 10.0.1.1                                   |
| host_routes       |                                            |
| id                | d9b765da-45eb-4543-be96-1b69a00a2556       |
| ip_version        | 4                                          |
| ipv6_address_mode |                                            |
| ipv6_ra_mode      |                                            |
| name              | sb-demo-net                                |
| network_id        | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b       |
| subnetpool_id     |                                            |
| tenant_id         | 75eb5efae5764682bca2fede6f4d8c6f           |
+-------------------+--------------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project: Creating a Port Using demo-net</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The owner of the port is <code class="literal">demo2</code>. Members of the network owner project
   (<code class="literal">demo</code>) will not see this port.
  </p><p>
   Running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><p>
   Creates a new port:
  </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             |                                                                                                     |
| device_owner          |                                                                                                     |
| dns_assignment        | {"hostname": "host-10-0-1-10", "ip_address": "10.0.1.10", "fqdn": "host-10-0-1-10.openstacklocal."} |
| dns_name              |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.10"}                    |
| id                    | 03ef2dce-20dc-47e5-9160-942320b4e503                                                                |
| mac_address           | fa:16:3e:27:8d:ca                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | DOWN                                                                                                |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="id-1.6.11.5.14.14"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project Booting a VM Using Demo-Net</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.14.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here the tenant <code class="literal">demo2</code> boots a VM that uses the <code class="literal">demo-net</code> shared network:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor 1 --image $OS_IMAGE --nic net-id=c3d55c21-d8c9-4ee5-944b-560b7e0ea33b demo2-vm-using-demo-net-nic</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------------------------------+
| Property                             | Value                                          |
+--------------------------------------+------------------------------------------------+
| OS-EXT-AZ:availability_zone          |                                                |
| OS-EXT-STS:power_state               | 0                                              |
| OS-EXT-STS:task_state                | scheduling                                     |
| OS-EXT-STS:vm_state                  | building                                       |
| OS-SRV-USG:launched_at               | -                                              |
| OS-SRV-USG:terminated_at             | -                                              |
| accessIPv4                           |                                                |
| accessIPv6                           |                                                |
| adminPass                            | sS9uSv9PT79F                                   |
| config_drive                         |                                                |
| created                              | 2016-01-04T19:23:24Z                           |
| flavor                               | m1.tiny (1)                                    |
| hostId                               |                                                |
| id                                   | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a           |
| image                                | cirros-0.3.3-x86_64 (6ae23432-8636-4e...1efc5) |
| key_name                             | -                                              |
| metadata                             | {}                                             |
| name                                 | demo2-vm-using-demo-net-nic                    |
| os-extended-volumes:volumes_attached | []                                             |
| progress                             | 0                                              |
| security_groups                      | default                                        |
| status                               | BUILD                                          |
| tenant_id                            | 5a582af8b44b422fafcd4545bd2b7eb5               |
| updated                              | 2016-01-04T19:23:24Z                           |
| user_id                              | a0e6427b036344fdb47162987cb0cee5               |
+--------------------------------------+------------------------------------------------+</pre></div><p>
   Run openstack server list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list</pre></div><p>
   See the VM running:
  </p><div class="verbatim-wrap"><pre class="screen">+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| ID                | Name                        | Status | Task State | Power State | Networks           |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| 3a4dc...a7c2dca2a | demo2-vm-using-demo-net-nic | ACTIVE | -          | Running     | demo-net=10.0.1.11 |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+</pre></div><p>
   Run openstack port list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-list --device-id 3a4dc44a-027b-45e9-acf8-054a7c2dca2a</pre></div><p>
   View the subnet:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------------+------+-------------------+-------------------------------------------------------------------+
| id                  | name | mac_address       | fixed_ips                                                         |
+---------------------+------+-------------------+-------------------------------------------------------------------+
| 7d14ef8b-9...80348f |      | fa:16:3e:75:32:8e | {"subnet_id": "d9b765da-45...00a2556", "ip_address": "10.0.1.11"} |
+---------------------+------+-------------------+-------------------------------------------------------------------+</pre></div><p>
   Run neutron port-show:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 7d14ef8b-9d48-4310-8c02-00c74d80348f</pre></div><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a                                                                |
| device_owner          | compute:None                                                                                        |
| dns_assignment        | {"hostname": "host-10-0-1-11", "ip_address": "10.0.1.11", "fqdn": "host-10-0-1-11.openstacklocal."} |
| dns_name              |                                                                                                     |
| extra_dhcp_opts       |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.11"}                    |
| id                    | 7d14ef8b-9d48-4310-8c02-00c74d80348f                                                                |
| mac_address           | fa:16:3e:75:32:8e                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | ACTIVE                                                                                              |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="sec-network-rbac-limitation"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.10.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-network-rbac-limitation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>sec-network-rbac-limitation</li></ul></div></div></div></div><p>
   Note the following limitations of RBAC in Neutron.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Neutron network is the only supported RBAC Neutron object type.
    </p></li><li class="listitem "><p>
     The "access_as_external" action is not supported – even though it is
     listed as a valid action by python-neutronclient.
    </p></li><li class="listitem "><p>
     The neutron-api server will not accept action value of
     'access_as_external'. The <code class="literal">access_as_external</code> definition
     is not found in the specs.
    </p></li><li class="listitem "><p>
     The target project users cannot create, modify, or delete subnets on
     networks that have RBAC policies.
    </p></li><li class="listitem "><p>
     The subnet of a network that has an RBAC policy cannot be added as an
     interface of a target tenant's router. For example, the command
     <code class="literal">neutron router-interface-add tgt-tenant-router &lt;sb-demo-net
     uuid&gt;</code> will error out.
    </p></li><li class="listitem "><p>
     The security group rules on the network owner do not apply to other
     projects that can use the network.
    </p></li><li class="listitem "><p>
     A user in target project can boot up VMs using a VNIC using the shared
     network. The user of the target project can assign a floating IP (FIP) to
     the VM. The target project must have SG rules that allows SSH and/or ICMP
     for VM connectivity.
    </p></li><li class="listitem "><p>
     Neutron RBAC creation and management are currently not supported in
     Horizon. For now, the Neutron CLI has to be used to manage RBAC rules.
    </p></li><li class="listitem "><p>
     A RBAC rule tells Neutron whether a tenant can access a network (Allow).
     Currently there is no DENY action.
    </p></li><li class="listitem "><p>
     Port creation on a shared network fails if <code class="literal">--fixed-ip</code>
     is specified in the <code class="literal">neutron port-create</code> command.
     
     
    </p></li></ul></div></div></div><div class="sect2" id="configureMTU"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Maximum Transmission Units in Neutron</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#configureMTU">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>configureMTU</li></ul></div></div></div></div><p>
  This topic explains how you can configure MTUs, what to look out for, and
  the results and implications of changing the default MTU settings. It is
  important to note that every network within a network group will have the
  same MTU.
 </p><div id="id-1.6.11.5.15.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   An MTU change will not affect existing networks that have had VMs created
   on them. It will only take effect on new networks created after the
   reconfiguration process.
  </p></div><div class="sect3" id="id-1.6.11.5.15.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.15.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A Maximum Transmission Unit, or MTU is the maximum packet size (in bytes)
   that a network device can or is configured to handle. There are a number of
   places in your cloud where MTU configuration is relevant: the physical
   interfaces managed and configured by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the virtual
   interfaces created by Neutron and Nova for Neutron networking, and the
   interfaces inside the VMs.
  </p><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces </strong></span>
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces include the physical interfaces and the
   bonds, bridges, and VLANs created on top of them. The MTU for these
   interfaces is configured via the 'mtu' property of a network group. Because
   multiple network groups can be mapped to one physical interface, there may
   have to be some resolution of differing MTUs between the untagged and tagged
   VLANs on the same physical interface. For instance, if one untagged VLAN,
   vlan101 (with an MTU of 1500) and a tagged VLAN vlan201 (with an MTU of
   9000) are both on one interface (eth0), this means that eth0 can handle
   1500, but the VLAN interface which is created on top of eth0 (that is,
   <code class="literal">vlan201@eth0</code>) wants 9000. However, vlan201 cannot have a
   higher MTU than eth0, so vlan201 will be limited to 1500 when it is brought
   up, and fragmentation will result.
  </p><p>
   In general, a VLAN interface MTU must be lower than or equal to the base
   device MTU. If they are different, as in the case above, the MTU of eth0 can
   be overridden and raised to 9000, but in any case the discrepancy will have
   to be reconciled.
  </p><p>
   <span class="bold"><strong>Neutron/Nova interfaces </strong></span>
  </p><p>
   Neutron/Nova interfaces include the virtual devices created by Neutron and
   Nova during the normal process of realizing a Neutron network/router and
   booting a VM on it (qr-*, qg-*, tap-*, qvo-*, qvb-*, etc.). There is
   currently no support in Neutron/Nova for per-network MTUs in which every
   interface along the path for a particular Neutron network has the correct
   MTU for that network. There is, however, support for globally changing the
   MTU of devices created by Neutron/Nova (see network_device_mtu below). This
   means that if you want to enable jumbo frames for any set of VMs, you will
   have to enable it for all your VMs. You cannot just enable them for a
   particular Neutron network.
  </p><p>
   <span class="bold"><strong>VM interfaces</strong></span>
  </p><p>
   VMs typically get their MTU via DHCP advertisement, which means that the
   dnsmasq processes spawned by the neutron-dhcp-agent actually advertise a
   particular MTU to the VMs. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the DHCP server advertises to
   all VMS a 1400 MTU via a forced setting in dnsmasq-neutron.conf. This is
   suboptimal for every network type (vxlan, flat, vlan, etc) but it does
   prevent fragmentation of a VM's packets due to encapsulation.
  </p><p>
   For instance, if you set the new *-mtu configuration options to a default of
   1500 and create a VXLAN network, it will be given an MTU of 1450 (with the
   remaining 50 bytes used by the VXLAN encapsulation header) and will
   advertise a 1450 MTU to any VM booted on that network. If you create a
   provider VLAN network, it will have an MTU of 1500 and will advertise 1500
   to booted VMs on the network. It should be noted that this default starting
   point for MTU calculation and advertisement is also global, meaning you
   cannot have an MTU of 8950 on one VXLAN network and 1450 on another. However,
   you can have provider physical networks with different MTUs by using the
   physical_network_mtus config option, but Nova still requires a global MTU
   option for the interfaces it creates, thus you cannot really take advantage
   of that configuration option.
  </p></div><div class="sect3" id="id-1.6.11.5.15.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network settings in the input model</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.15.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   MTU can be set as an attribute of a network group in network_groups.yml.
   Note that this applies only to KVM. That setting means that every network in
   the network group will be assigned the specified MTU. The MTU value must be
   set individually for each network group. For example:
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:
        - name: GUEST
        mtu: 9000
        ...

        - name: EXTERNAL-API
        mtu: 9000
        ...

        - name: EXTERNAL-VM
        mtu: 9000
        ...</pre></div></div><div class="sect3" id="id-1.6.11.5.15.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Infrastructure support for jumbo frames</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.15.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you want to use jumbo frames, or frames with an MTU of 9000 or more, the
   physical switches and routers that make up the infrastructure of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation must be configured to support them. To realize the advantages,
   generally all devices in the same broadcast domain must have the same MTU.
  </p><p>
   If you want to configure jumbo frames on compute and controller nodes, then
   all switches joining the compute and controller nodes must have jumbo frames
   enabled. Similarly, the "infrastructure gateway" through which the external
   VM network flows, commonly known as the default route for the external VM
   VLAN, must also have the same MTU configured.
  </p><p>
   You can also consider anything in the same broadcast domain to be anything
   in the same VLAN or anything in the same IP subnet.
  </p></div><div class="sect3" id="id-1.6.11.5.15.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling end-to-end jumbo frames for a VM</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.15.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add an 'mtu' attribute to all the network groups in your model. Note that
     adding the MTU for the network groups will only affect the configuration
     for physical network interfaces.
    </p><p>
     To add the mtu attribute, find the YAML file that contains your
     network-groups entry. We will assume it is network_groups.yml, unless you
     have changed it. Whatever the file is named, it will be found in
     ~/openstack/my_cloud/definition/data/.
    </p><p>
     To edit these files, begin by checking out the
     <span class="bold"><strong>site</strong></span> branch on the Cloud Lifecycle Manager
     node. You may already be on that branch. If so, you will remain there.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div><p>
     Then begin editing the files. In network_groups.yml, add mtu: 9000
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:
            - name: GUEST
            hostname-suffix: guest
            mtu: 9000
            tags:
            - neutron.networks.vxlan</pre></div><p>
     This will set the physical interface managed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> that has
     the GUEST network group tag assigned to it. This can be found in the
     interfaces_set.yml file under the interface-models section.
    </p></li><li class="step "><p>
     Next, edit the layer 2 agent config file, ml2_conf.ini.j2, found in
     ~/openstack/my_cloud/config/neutron/ to set the path_mtu to 0, this ensures
     that global_physnet_mtu is used.
    </p><div class="verbatim-wrap"><pre class="screen">[ml2]
...
path_mtu = 0</pre></div></li><li class="step "><p>
     Next, edit neutron.conf.j2 found in ~/openstack/my_cloud/config/neutron/ to
     set advertise_mtu (to true) and global_physnet_mtu to 9000 under
     [DEFAULT]:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
advertise_mtu = True
global_physnet_mtu = 9000</pre></div><p>
     This allows Neutron to advertise the optimal MTU to instances (based upon
     global_physnet_mtu minus the encapsulation size).
    </p></li><li class="step "><p>
     Next, remove the "dhcp-option-force=26,1400" line from
     <code class="filename">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>.
    </p></li><li class="step "><p>
     OvS will set <code class="literal">br-int</code> to the value of the lowest physical
     interface. If you are using Jumbo frames on some of your networks,
     <code class="literal">br-int</code> on the controllers may be set to 1500 instead of
     9000. Work around this condition by running:
    </p><div class="verbatim-wrap"><pre class="screen">ovs-vsctl set int br-int mtu_request=9000</pre></div></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has not been deployed yet, do normal deployment and skip to
     step 8.
    </p></li><li class="step "><p>
     Assuming it has been deployed already, continue here:
    </p><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     and ready the deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Then run the network_interface-reconfigure.yml playbook, changing
     directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div><p>
     Then run neutron-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div><p>
     Then nova-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div><p>
     Note: adding/changing network-group mtu settings will likely require a
     network restart during network_interface-reconfigure.yml.
    </p></li><li class="step "><p>
     Follow the normal process for creating a Neutron network and booting a VM
     or two. In this example, if a VXLAN network is created and a VM is booted
     on it, the VM will have an MTU of 8950, with the remaining 50 bytes used
     by the VXLAN encapsulation header.
    </p></li><li class="step "><p>
     Test and verify that the VM can send and receive jumbo frames without
     fragmentation. You can use ping. For example, to test an MTU of 9000 using
     VXLAN:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ping –M do –s 8950 <em class="replaceable ">YOUR_VM_FLOATING_IP</em></pre></div><p>
     Just substitute your actual floating IP address for the
     <em class="replaceable ">YOUR_VM_FLOATING_IP</em>.
    </p></li></ol></div></div></div><div class="sect3" id="optimal-mtu"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Optimal MTU Advertisement Feature</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#optimal-mtu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>optimal-mtu</li></ul></div></div></div></div><p>
   To enable the optimal MTU feature, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
     to <span class="bold"><strong>remove</strong></span> advertise_mtu variable under
     [DEFAULT]
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
advertise_mtu = False #remove this</pre></div></li><li class="step "><p>
     Remove the <code class="literal">dhcp-option-force=26,1400</code> line from
     <code class="literal">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>
    </p></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has already been deployed, follow the remaining steps,
     otherwise follow the normal deployment procedures.
    </p></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run ready deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">network_interface-reconfigure.yml</code> playbook,
     changing directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div></li><li class="step "><p>
     Run neutron-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div id="id-1.6.11.5.15.8.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    If you are upgrading an existing deployment, attention must be paid to
    avoid creating MTU mismatch between network interfaces in preexisting VMs
    and that of VMs created after upgrade. If you do have an MTU mismatch, then
    the new VMs (having interface with 1500 minus the underlay protocol
    overhead) will not be able to have L2 connectivity with preexisting VMs
    (with 1400 MTU due to dhcp-option-force).
   </p></div></div></div><div class="sect2" id="topic-shy-ksv-jw"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve Network Peformance with Isolated Metadata Settings</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#topic-shy-ksv-jw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-isolated_metadata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-isolated_metadata.xml</li><li><span class="ds-label">ID: </span>topic-shy-ksv-jw</li></ul></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, Neutron currently sets <code class="literal">enable_isolated_metadata =
  True</code> by default in <code class="literal">dhcp_agent.ini</code> because
  several services require isolated networks (Neutron networks without a
  router). This has the effect of spawning a neutron-ns-metadata-proxy process
  on one of the controller nodes for every active Neutron network.
 </p><p>
  In environments that create many Neutron networks, these extra
  <code class="literal">neutron-ns-metadata-proxy</code> processes can quickly eat up a
  lot of memory on the controllers, which does not scale well.
 </p><p>
  For deployments that do not require isolated metadata (that is, they do not
  require the Platform Services and will always create networks with an
  attached router), you can set <code class="literal">enable_isolated_metadata =
  False</code> in dhcp_agent.ini to reduce Neutron memory usage on
  controllers, allowing a greater number of active Neutron networks.
 </p><p>
  Note that the <code class="literal">dhcp_agent.ini.j2</code> template is found in
  <code class="literal">~/openstack/my_cloud/config/neutron</code> on the Cloud Lifecycle Manager
  node. The edit can be made there and the standard deployment can be run if
  this is install time. In a deployed cloud, run the Neutron reconfiguration
  procedure outlined here:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    First check out the site branch:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/neutron
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="step "><p>
    Edit the <code class="literal">dhcp_agent.ini.j2</code> file to change the
    <code class="literal">enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</code>
    line in the <code class="literal">[DEFAULT]</code> section to read:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = False</pre></div></li><li class="step "><p>
    Commit the file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
    Run the <code class="literal">ready-deployment.yml</code> playbook from
    <code class="literal">~/openstack/ardana/ansible</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Then run the <code class="literal">neutron-reconfigure.yml</code> playbook, changing
    directories first:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="moving-from-dvr-deployments"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Moving from DVR deployments to non_DVR</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#moving-from-dvr-deployments">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-moving_from_dvr_deployments.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-moving_from_dvr_deployments.xml</li><li><span class="ds-label">ID: </span>moving-from-dvr-deployments</li></ul></div></div></div></div><p>
  If you have an older deployment of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which is using DVR as a default
  and you are attempting to move to non_DVR, make sure you follow these steps:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Remove all your existing DVR routers and their workloads. Make sure to
    remove interfaces, floating ips and gateways, if applicable.
   </p><div class="verbatim-wrap"><pre class="screen">neutron router-interface-delete <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">SUBNET-NAME</em>/<em class="replaceable ">SUBNET-ID</em>
neutron floatingip-disassociate <em class="replaceable ">FLOATINGIP-ID</em> <em class="replaceable ">PRIVATE-PORT-ID</em>
neutron router-gateway-clear <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">-NET-NAME</em>/<em class="replaceable ">EXT-NET-ID</em></pre></div></li><li class="listitem "><p>
    Then delete the router.
   </p><div class="verbatim-wrap"><pre class="screen">neutron router-delete <em class="replaceable ">ROUTER-NAME</em></pre></div></li><li class="listitem "><p>
    Before you create any non_DVR router make sure that l3-agents and
    metadata-agents are not running in any compute host. You can run the
    command <code class="literal">neutron agent-list</code> to see if there are any
    neutron-l3-agent running in any compute-host in your deployment.
   </p><p>
    You must disable neutron-l3-agent and neutron-metadata-agent on every
    compute host by running the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron agent-list
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | availability_zone | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| 208f6aea-3d45-4b89-bf42-f45a51b05f29 | Loadbalancerv2 agent | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-lbaasv2-agent     |
| 810f0ae7-63aa-4ee3-952d-69837b4b2fe4 | L3 agent             | ardana-cp1-comp0001-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
| 89ac17ba-2f43-428a-98fa-b3698646543d | Metadata agent       | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| f602edce-1d2a-4c8a-ba56-fa41103d4e17 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
...
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+

$ neutron agent-update 810f0ae7-63aa-4ee3-952d-69837b4b2fe4 --admin-state-down
Updated agent: 810f0ae7-63aa-4ee3-952d-69837b4b2fe4

$ neutron agent-update 89ac17ba-2f43-428a-98fa-b3698646543d --admin-state-down
Updated agent: 89ac17ba-2f43-428a-98fa-b3698646543d</pre></div><div id="id-1.6.11.5.17.3.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Only L3 and Metadata agents were disabled.
     </p></div></li><li class="listitem "><p>
    Once L3 and metadata neutron agents are stopped, follow steps 1 through 7
    in the document <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 12 “Alternative Configurations”, Section 12.2 “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR”</span> and then run the
    <code class="literal">neutron-reconfigure.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="dpdk"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVS-DPDK Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span>dpdk</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a version of Open vSwitch (OVS) that is built with the
  Data Plane Development Kit (DPDK) and includes a QEMU hypervisor which
  supports vhost-user.
 </p><p>
  The OVS-DPDK package modifes the OVS fast path, which is normally performed
  in kernel space, and allows it to run in userspace so there is no context
  switch to the kernel for processing network packets.
 </p><p>
  The EAL component of DPDK supports mapping the Network Interface Card (NIC)
  registers directly into userspace. The DPDK provides a Poll Mode Driver
  (PMD) that can access the NIC hardware from userspace and uses polling
  instead of interrupts to avoid the user to kernel transition.
 </p><p>
  The PMD maps the shared address space of the VM that is provided by the
  vhost-user capability of QEMU. The vhost-user mode causes Neutron to create
  a Unix domain socket that allows communication between the PMD and QEMU. The
  PMD uses this in order to acquire the file descriptors to the pre-allocated
  VM memory. This allows the PMD to directly access the VM memory space and
  perform a fast zero-copy of network packets directly into and out of the VMs
  virtio_net vring.
 </p><p>
  This yields performance improvements in the time it takes to process network
  packets.
 </p><div class="sect3" id="id-1.6.11.5.18.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage considerations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The target for a DPDK Open vSwitch is VM performance and VMs only run on
   compute nodes so the following considerations are compute node specific.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     You are required to <a class="xref" href="ops-managing-networking.html#hugepages" title="9.3.14.3. Configuring Hugepages for DPDK in Neutron Networks">Section 9.3.14.3, “Configuring Hugepages for DPDK in Neutron Networks”</a> in order to use DPDK with
     VMs. The memory to be used must be allocated at boot time so you must know
     beforehand how many VMs will be scheduled on a node. Also, for NUMA
     considerations, you want those hugepages on the same NUMA node as the NIC.
     A VM maps its entire address space into a hugepage.
    </p></li><li class="listitem "><p>
     For maximum performance you must reserve logical cores for DPDK poll mode
     driver (PMD) usage and for hypervisor (QEMU) usage. This keeps the Linux
     kernel from scheduling processes on those cores. The PMD threads will go
     to 100% cpu utilization since it uses polling of the hardware instead of
     interrupts. There will be at least 2 cores dedicated to PMD threads. Each
     VM will have a core dedicated to it although for less performance VMs can
     share cores.
    </p></li><li class="listitem "><p>
     VMs can use the virtio_net or the virtio_pmd drivers. There is also a PMD
     for an emulated e1000.
    </p></li><li class="listitem "><p>
     Only VMs that use hugepages can be sucessfully launched on a DPDK enabled
     NIC. If there is a need to support both DPDK and non-DPDK based VMs an
     additional port managed by the Linux kernel must exist.
    </p></li><li class="listitem "><p>
     OVS/DPDK does not support jumbo frames. Please review
     <a class="link" href="https://github.com/openvswitch/ovs/blob/branch-2.5/INSTALL.DPDK.md#restrictions" target="_blank">https://github.com/openvswitch/ovs/blob/branch-2.5/INSTALL.DPDK.md#restrictions</a>
     for restrictions.
    </p></li><li class="listitem "><p>
     The Open vSwitch firewall driver in networking-ovs-dpdk repo is stateless
     and not a stateful one that would use iptables and conntrack. In the past,
     Neutron core has declined to pull in stateless type FW.
     <a class="link" href="https://bugs.launchpad.net/neutron/+bug/1531205" target="_blank">https://bugs.launchpad.net/neutron/+bug/1531205</a>
     The native firewall driver is stateful, which is why conntrack was added
     to Open vSwitch. But this is not supported on DPDK and will not be until OVS
     2.6.
    </p></li></ol></div></div><div class="sect3" id="id-1.6.11.5.18.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   See the following topics for more information:
  </p></div><div class="sect3" id="hugepages"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Hugepages for DPDK in Neutron Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#hugepages">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span>hugepages</li></ul></div></div></div></div><p>
  To take advantage of DPDK and its network
  performance enhancements, enable hugepages first.
 </p><p>
  With hugepages, physical RAM is reserved at boot time and dedicated to a
  virtual machine. Only that virtual machine and Open vSwitch can use this
  specifically allocated RAM. The host OS cannot access it. This memory is
  contiguous, and because of its larger size, reduces the number of entries in
  the memory map and number of times it must be read.
 </p><p>
  The hugepage reservation is made in <code class="literal">/etc/default/grub</code>,
  but this is handled by the Cloud Lifecycle Manager.
 </p><p>
  In addition to hugepages, to use DPDK, CPU isolation is required. This is
  achieved with the 'isolcups' command in
  <code class="literal">/etc/default/grub</code>, but this is also managed by the
  Cloud Lifecycle Manager using a new input model file.
 </p><p>
  The two new input model files introduced with this release to help you
  configure the necessary settings and persist them are:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    memory_models.yml (for hugepages)
   </p></li><li class="listitem "><p>
    cpu_models.yml (for CPU isolation)
   </p></li></ul></div><div class="sect4" id="id-1.6.11.5.18.9.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">memory_models.yml</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.9.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In this file you set your huge page size along with the number of such
   huge-page allocations.
  </p><div class="verbatim-wrap"><pre class="screen"> ---
  product:
    version: 2

  memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 1G
      huge-pages:
        - size: 1G
          count: 24
          numa-node: 0
        - size: 1G
          count: 24
          numa-node: 1
        - size: 1G
          count: 48</pre></div></div><div class="sect4" id="id-1.6.11.5.18.9.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">cpu_models.yml</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.9.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  cpu-models:

    - name: COMPUTE-CPU
      assignments:
       - components:
           - nova-compute-kvm
         cpu:
           - processor-ids: 3-5,12-17
             role: vm

       - components:
           - openvswitch
         cpu:
           - processor-ids: 0
             role: eal
           - processor-ids: 1-2
             role: pmd</pre></div></div><div class="sect4" id="id-1.6.11.5.18.9.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NUMA memory allocation</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.9.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As mentioned above, the memory used for hugepages is locked down at boot
   time by an entry in <code class="literal">/etc/default/grub</code>. As an admin, you
   can specify in the input model how to arrange this memory on NUMA nodes. It
   can be spread across NUMA nodes or you can specify where you want it. For
   example, if you have only one NIC, you would probably want all the hugepages
   memory to be on the NUMA node closest to that NIC.


  </p><p>
   If you do not specify the <code class="literal">numa-node</code> settings in the
   <code class="literal">memory_models.yml</code> input model file and use only the last
   entry indicating "size: 1G" and "count: 48" then this memory is spread
   evenly across all NUMA nodes.
  </p><p>
   Also note that the hugepage service runs once at boot time and then goes to
   an inactive state so you should not expect to see it running. If you decide
   to make changes to the NUMA memory allocation, you will need to reboot the
   compute node for the changes to take effect.
  </p></div></div><div class="sect3" id="dpdk-setup"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Setup for Neutron Networking</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk-setup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span>dpdk-setup</li></ul></div></div></div></div><div class="sect4" id="id-1.6.11.5.18.10.2"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware requirements</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Intel-based compute node. DPDK is not available on AMD-based systems.
    </p></li><li class="listitem "><p>
     The following BIOS settings must be enabled for DL360 Gen9:
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       Virtualization Technology
      </p></li><li class="listitem "><p>
       Intel(R) VT-d
      </p></li><li class="listitem "><p>
       PCI-PT (Also see <a class="xref" href="ops-managing-networking.html#pcipt-gen9" title="9.3.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 9.3.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>)
      </p></li></ol></div></li><li class="listitem "><p>
     Need adequate host memory to allow for hugepages. The examples below use
     1G hugepages for the VMs
    </p></li></ul></div></div><div class="sect4" id="id-1.6.11.5.18.10.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     DPDK is supported on SLES only.
    </p></li><li class="listitem "><p>
     Applies to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> only.
    </p></li><li class="listitem "><p>
     Tenant network can be untagged vlan or untagged vxlan
    </p></li><li class="listitem "><p>
     DPDK port names must be of the form 'dpdk&lt;portid&gt;' where port id is
     sequential and starts at 0
    </p></li><li class="listitem "><p>
     No support for converting DPDK ports to non DPDK ports without rebooting
     compute node.
    </p></li><li class="listitem "><p>
     No security group support, need userspace conntrack.
    </p></li><li class="listitem "><p>
     No jumbo frame support.
    </p></li></ul></div></div><div class="sect4" id="id-1.6.11.5.18.10.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup instructions</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.18.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   These setup instructions and example model are for a three-host system.
   There is one controller with Cloud Lifecycle Manager in cloud control plane and
   two compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After initial run of site.yml all compute nodes must be rebooted to pick
     up changes in grub for hugepages and isolcpus
    </p></li><li class="listitem "><p>
     Changes to non-uniform memory access (NUMA) memory, isolcpu, or network
     devices must be followed by a reboot of compute nodes
    </p></li><li class="listitem "><p>
     Run sudo reboot to pick up libvirt change and hugepage/isocpus grub
     changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo reboot</pre></div></li><li class="listitem "><p>
     Use the bash script below to configure nova aggregates, neutron networks,
     a new flavor, etc. And then it will spin up two VMs.
    </p></li></ol></div><p>
   <span class="bold"><strong>VM spin-up instructions</strong></span>
  </p><p>
   Before running the spin up script you need to get a copy of the cirros image
   to your Cloud Lifecycle Manager node. You can manually scp a copy of the cirros
   image to the system. You can copy it locallly with wget like so
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img</pre></div><p>
   Save the following shell script in the home directory and run it. This
   should spin up two VMs, one on each compute node.
  </p><div id="id-1.6.11.5.18.10.4.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    Make sure to change all network-specific information in the script to match
    your environment.
   </p></div><div class="verbatim-wrap"><pre class="screen">#!/usr/bin/env bash

source service.osrc

######## register glance image
glance image-create --name='cirros' --container-format=bare --disk-format=qcow2 &lt; ~/cirros-0.3.4-x86_64-disk.img

####### create nova aggregate and flavor for dpdk

MI_NAME=dpdk

nova aggregate-create $MI_NAME nova
nova aggregate-add-host $MI_NAME openstack-cp-comp0001-mgmt
nova aggregate-add-host $MI_NAME openstack-cp-comp0002-mgmt
nova aggregate-set-metadata $MI_NAME pinned=true

nova flavor-create $MI_NAME 6 1024 20 1
nova flavor-key $MI_NAME set hw:cpu_policy=dedicated
nova flavor-key $MI_NAME set aggregate_instance_extra_specs:pinned=true
nova flavor-key $MI_NAME set hw:mem_page_size=1048576

######## sec groups NOTE: no sec groups supported on DPDK.  This is in case we do non-DPDK compute hosts.
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0

########  nova keys
nova keypair-add mykey &gt;mykey.pem
chmod 400 mykey.pem

######## create neutron external network
neutron net-create ext-net --router:external --os-endpoint-type internalURL
neutron subnet-create ext-net 10.231.0.0/19 --gateway_ip=10.231.0.1  --ip-version=4 --disable-dhcp  --allocation-pool start=10.231.17.0,end=10.231.17.255

########  neutron network
neutron net-create mynet1
neutron subnet-create mynet1 10.1.1.0/24 --name mysubnet1
neutron router-create myrouter1
neutron router-interface-add myrouter1 mysubnet1
neutron router-gateway-set myrouter1 ext-net
export MYNET=$(neutron net-list|grep mynet|awk '{print $2}')

######## spin up 2 VMs, 1 on each compute
nova boot --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0001-mgmt vm1
nova boot --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0002-mgmt vm2

######## create floating ip and attach to instance
export MYFIP1=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm1 ${MYFIP1}

export MYFIP2=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm2 ${MYFIP2}

nova list</pre></div></div></div><div class="sect3" id="dpdk-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Configurations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>dpdk-config</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#base-config" title="9.3.14.5.1. Base configuration">Section 9.3.14.5.1, “Base configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#common-perf" title="9.3.14.5.2. Performance considerations common to all NIC types">Section 9.3.14.5.2, “Performance considerations common to all NIC types”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#multiqueue-config" title="9.3.14.5.3. Multiqueue configuration">Section 9.3.14.5.3, “Multiqueue configuration”</a>
   </p></li></ul></div><div class="sect4" id="base-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Base configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#base-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>base-config</li></ul></div></div></div></div><p>
   The following is specific to DL360 Gen9 and BIOS configuration as detailed
   in <a class="xref" href="ops-managing-networking.html#dpdk-setup" title="9.3.14.4. DPDK Setup for Neutron Networking">Section 9.3.14.4, “DPDK Setup for Neutron Networking”</a>.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     EAL cores - 1, isolate: False in cpu-models
    </p></li><li class="listitem "><p>
     PMD cores - 1 per NIC port
    </p></li><li class="listitem "><p>
     Hugepages - 1G per PMD thread
    </p></li><li class="listitem "><p>
     Memory channels - 4
    </p></li><li class="listitem "><p>
     Global rx queues - based on needs
    </p></li></ul></div></div><div class="sect4" id="common-perf"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance considerations common to all NIC types</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#common-perf">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>common-perf</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Compute host core frequency</strong></span>
  </p><p>
   Host CPUs should be running at maximum performance. The following is a
   script to set that. Note that in this case there are 24 cores. This needs to
   be modified to fit your environment. For a HP DL360 Gen9, the BIOS should be
   configured to use "OS Control Mode" which can be found on the iLO Power
   Settings page.
  </p><div class="verbatim-wrap"><pre class="screen">for i in `seq 0 23`; do echo "performance" &gt; /sys/devices/system/cpu/cpu$i/cpufreq/scaling_governor; done</pre></div><p>
   <span class="bold"><strong>IO non-posted prefetch</strong></span>
  </p><p>
   The DL360 Gen9 should have the IO non-posted prefetch disabled. Experimental
   evidence shows this yields an additional 6-8% performance boost.
  </p></div><div class="sect4" id="multiqueue-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiqueue configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#multiqueue-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>multiqueue-config</li></ul></div></div></div></div><p>
   In order to use multiqueue, a property must be applied to the Glance image
   and a setting inside the resulting VM must be applied. In this example we
   create a 4 vCPU flavor for DPDK using 1G hugepages.
  </p><div class="verbatim-wrap"><pre class="screen">MI_NAME=dpdk

nova aggregate-create $MI_NAME nova
nova aggregate-add-host $MI_NAME openstack-cp-comp0001-mgmt
nova aggregate-add-host $MI_NAME openstack-cp-comp0002-mgmt
nova aggregate-set-metadata $MI_NAME pinned=true

nova flavor-create $MI_NAME 6 1024 20 4
nova flavor-key $MI_NAME set hw:cpu_policy=dedicated
nova flavor-key $MI_NAME set aggregate_instance_extra_specs:pinned=true
nova flavor-key $MI_NAME set hw:mem_page_size=1048576</pre></div><p>
   And set the hw_vif_multiqueue_enabled property on the Glance image
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image set --property hw_vif_multiqueue_enabled=true <em class="replaceable ">IMAGE UUID</em></pre></div><p>
   Once the VM is booted using the flavor above, inside the VM, choose the
   number of combined rx and tx queues to be equal to the number of vCPUs
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ethtool -L eth0 combined 4</pre></div><p>
   On the hypervisor you can verify that multiqueue has been properly set by
   looking at the qemu process
  </p><div class="verbatim-wrap"><pre class="screen">-netdev type=vhost-user,id=hostnet0,chardev=charnet0,queues=4 -device virtio-net-pci,mq=on,vectors=10,</pre></div><p>
   Here you can see that 'mq=on' and vectors=10. The formula for vectors is
   2*num_queues+2
  </p></div></div><div class="sect3" id="dpdk-troubleshooting"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.14.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting DPDK</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>dpdk-troubleshooting</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#hardware" title="9.3.14.6.1. Hardware configuration">Section 9.3.14.6.1, “Hardware configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#system" title="9.3.14.6.2. System configuration">Section 9.3.14.6.2, “System configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#inputModel" title="9.3.14.6.3. Input model configuration">Section 9.3.14.6.3, “Input model configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#reboot" title="9.3.14.6.4. Reboot requirements">Section 9.3.14.6.4, “Reboot requirements”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#software" title="9.3.14.6.5. Software configuration">Section 9.3.14.6.5, “Software configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#runtime" title="9.3.14.6.6. DPDK runtime">Section 9.3.14.6.6, “DPDK runtime”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#errors" title="9.3.14.6.7. Errors">Section 9.3.14.6.7, “Errors”</a>
   </p></li></ul></div><div class="sect4" id="hardware"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#hardware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>hardware</li></ul></div></div></div></div><p>
   Because there are several variations of hardware, it is up to you to verify
   that the hardware is configured properly.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only Intel based compute nodes are supported. There is no DPDK available
     for AMD-based CPUs.
    </p></li><li class="listitem "><p>
     PCI-PT must be enabled for the NIC that will be used with DPDK.
    </p></li><li class="listitem "><p>
     When using Intel Niantic and the igb_uio driver, the VT-d must be enabled
     in the BIOS.
    </p></li><li class="listitem "><p>
     For DL360 Gen9 systems, the BIOS shared-memory
     <a class="xref" href="ops-managing-networking.html#pcipt-gen9" title="9.3.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 9.3.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>.
    </p></li><li class="listitem "><p>
     Adequate memory must be available for <a class="xref" href="ops-managing-networking.html#hugepages" title="9.3.14.3. Configuring Hugepages for DPDK in Neutron Networks">Section 9.3.14.3, “Configuring Hugepages for DPDK in Neutron Networks”</a> usage.
    </p></li><li class="listitem "><p>
     Hyper-threading can be enabled but is not required for base functionality.
    </p></li><li class="listitem "><p>
     Determine the PCI slot that the DPDK NIC(s) are installed in to
     determine the associated NUMA node.
    </p></li><li class="listitem "><p>
     Only the Intel Haswell, Broadwell, and Skylake microarchitectures are
     supported.
     
     Intel Sandy Bridge is not supported.
    </p></li></ul></div></div><div class="sect4" id="system"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#system">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>system</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only SLES12-SP3 compute nodes are supported.
    </p></li><li class="listitem "><p>
     If a NIC port is used with PCI-PT, SRIOV-only, or PCI-PT+SRIOV, then it
     cannot be used with DPDK. They are mutually exclusive. This is because DPDK
     depends on an OvS bridge which does not exist if you use any combination of
     PCI-PT and SRIOV. You can use DPDK, SRIOV-only, and PCI-PT on difference
     interfaces of the same server.
    </p></li><li class="listitem "><p>
     There is an association between the PCI slot for the NIC and a NUMA node.
     Make sure to use logical CPU cores that are on the NUMA node associated to
     the NIC. Use the following to determine which CPUs are on which NUMA node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lscpu

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz
Stepping:              2
CPU MHz:               1200.000
CPU max MHz:           1800.0000
CPU min MHz:           1200.0000
BogoMIPS:              3597.06
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0-11,24-35
NUMA node1 CPU(s):     12-23,36-47</pre></div></li></ul></div></div><div class="sect4" id="inputModel"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#inputModel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>inputModel</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If you do not specify a driver for a DPDK device, the igb_uio will be
     selected as default.
    </p></li><li class="listitem "><p>
     DPDK devices must be named <code class="literal">dpdk&lt;port-id&gt;</code> where
     the port-id starts at 0 and increments sequentially.
    </p></li><li class="listitem "><p>
     Tenant networks supported are untagged VXLAN and VLAN.
    </p></li><li class="listitem "><p>
     Jumbo Frames MTU does not work with DPDK OvS. There is an upstream patch
     most likely showing up in OvS 2.6 and it cannot be backported due to
     changes this patch relies upon.
    </p></li><li class="listitem "><p>
     Sample VXLAN model
    </p></li><li class="listitem "><p>
     Sample VLAN model
    </p></li></ul></div></div><div class="sect4" id="reboot"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reboot requirements</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#reboot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>reboot</li></ul></div></div></div></div><p>
   A reboot of a compute node must be performed when an input model change
   causes the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After the initial <code class="filename">site.yml</code> play on a new <span class="productname">OpenStack</span>
     environment
    </p></li><li class="listitem "><p>
     Changes to an existing <span class="productname">OpenStack</span> environment that modify the
     <code class="literal">/etc/default/grub</code> file, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       hugepage allocations
      </p></li><li class="listitem "><p>
       CPU isolation
      </p></li><li class="listitem "><p>
       iommu changes
      </p></li></ul></div></li><li class="listitem "><p>
     Changes to a NIC port usage type, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       moving from DPDK to any combination of PCI-PT and SRIOV
      </p></li><li class="listitem "><p>
       moving from DPDK to kernel based eth driver
      </p></li></ul></div></li></ol></div></div><div class="sect4" id="software"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#software">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>software</li></ul></div></div></div></div><p>
   The input model is processed by the Configuration Processor which eventually
   results in changes to the OS. There are several files that should be checked
   to verify the proper settings were applied. In addition, after the inital
   site.yml play is run all compute nodes must be rebooted in order to pickup
   changes to the <code class="filename">/etc/default/grub</code> file for hugepage
   reservation, CPU isolation and iommu settings.
  </p><p>
   <span class="bold"><strong>Kernel settings</strong></span>
  </p><p>
   Check <code class="filename">/etc/default/grub</code> for the following
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     hugepages
    </p></li><li class="listitem "><p>
     CPU isolation
    </p></li><li class="listitem "><p>
     that iommu is in passthru mode if the igb_uio driver is in use
    </p></li></ol></div><p>
   <span class="bold"><strong>Open vSwitch settings</strong></span>
  </p><p>
   Check <code class="filename">/etc/default/openvswitch-switchf</code> for
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     using the <code class="literal">--dpdk</code> option
    </p></li><li class="listitem "><p>
     core 0 set aside for EAL and kernel to share
    </p></li><li class="listitem "><p>
     cores assigned to PMD drivers, at least two for each DPDK device
    </p></li><li class="listitem "><p>
     verify that memory is reserved with socket-mem option
    </p></li><li class="listitem "><p>
     Once
     <a class="link" href="https://jira.hpcloud.net/browse/VNETCORE-2509" target="_blank">VNETCORE-2509</a>
     merges also verify that the umask is 022 and the group is libvirt-qemu
    </p></li></ol></div><p>
   <span class="bold"><strong>DPDK settings</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     check <code class="filename">/etc/dpdk/interfacesf</code> for the correct DPDK devices
    </p></li></ol></div></div><div class="sect4" id="runtime"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK runtime</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#runtime">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>runtime</li></ul></div></div></div></div><p>
   All non-bonded DPDK devices will be added to individual OvS bridges. The
   bridges will be named <code class="literal">br-dpdk0</code>,
   <code class="literal">br-dpdk1</code>, etc. The name of the OvS bridge for bonded DPDK
   devices will be <code class="literal">br-dpdkbond0</code>,
   <code class="literal">br-dpdkbond1</code>, etc.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Since each PMD thread is in a polling loop, it will use 100% of the CPU.
     Thus for two PMDs you would expect to see the ovs-vswitchd process running
     at 200%. This can be verified by running
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>top

top - 16:45:42 up 4 days, 22:24,  1 user,  load average: 2.03, 2.10, 2.14
Tasks: 384 total,   2 running, 382 sleeping,   0 stopped,   0 zombie
%Cpu(s):  9.0 us,  0.2 sy,  0.0 ni, 90.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  13171580+total, 10356851+used, 28147296 free,   257196 buffers
KiB Swap:        0 total,        0 used,        0 free.  1085868 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 1522 root      10 -10 6475196 287780  10192 S 200.4  0.2  14250:20 ovs-vswitchd</pre></div></li><li class="listitem "><p>
     Verify that <code class="literal">ovs-vswitchd</code> is running with
    </p><div class="verbatim-wrap"><pre class="screen">--dpdk option. ps -ef | grep ovs-vswitchd</pre></div></li><li class="listitem "><p>
     PMD thread(s) are started when a DPDK port is added to an OvS bridge.
     Verify the port is on the bridge.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ovs-vsctl show</pre></div></li><li class="listitem "><p>
     A DPDK port cannot be added to an OvS bridge unless it is bound to a
     driver. Verify that the DPDK port is bound.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo dpdk_nic_bind -s</pre></div></li><li class="listitem "><p>
     Verify that the proper number of hugepages is on the correct NUMA node
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo virsh freepages --all</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo grep -R "" /sys/kernel/mm/hugepages/ /proc/sys/vm/*huge*</pre></div></li><li class="listitem "><p>
     Verify that the VM and the DPDK PMD threads have both mapped the same
     hugepage(s)
    </p><div class="verbatim-wrap"><pre class="screen"># this will yield 2 process ids, use the 2nd one
<code class="prompt user">tux &gt; </code>sudo ps -ef | grep ovs-vswitchd
<code class="prompt user">tux &gt; </code>sudo ls -l /proc/<em class="replaceable ">PROCESS-ID</em>/fd | grep huge

# if running more than 1 VM you will need to figure out which one to use
<code class="prompt user">tux &gt; </code>sudo ps -ef | grep qemu
<code class="prompt user">tux &gt; </code>sudo ls -l /proc/<em class="replaceable ">PROCESS-ID</em>/fd | grep huge</pre></div></li></ol></div></div><div class="sect4" id="errors"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.3.14.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Errors</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#errors">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>errors</li></ul></div></div></div></div><p>
   <span class="bold"><strong>VM does not get fixed IP</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     DPDK Poll Mode drivers (PMD) communicates with the VM by direct access of
     the VM hugepage. If a VM is not created using hugepages
     (see <a class="xref" href="ops-managing-networking.html#hugepages" title="9.3.14.3. Configuring Hugepages for DPDK in Neutron Networks">Section 9.3.14.3, “Configuring Hugepages for DPDK in Neutron Networks”</a>),
     there is no way for DPDK to communicate with the VM and the VM will never
     be connected to the network.
    </p></li><li class="listitem "><p>
     It has been observed that the DPDK communication with VM fails if the
     shared-memory is not disabled in BIOS for DL360 Gen9.
    </p></li></ol></div><p>
   <span class="bold"><strong>Vestiges of non-existent DPDK devices</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Incorrect input models that do not use the correct DPDK device name or
     do not use sequential port IDs starting at 0 may leave non-existent devices
     in the OvS database. While this does not affect proper functionality it may
     be confusing.
    </p></li></ol></div><p>
   <span class="bold"><strong>Startup issues</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Running the following will help diagnose startup issues with ovs-vswitchd:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo journalctl -u openvswitch.service --all</pre></div></li></ol></div></div></div></div><div class="sect2" id="sr-iov"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV and PCI Passthrough Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sr-iov">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>sr-iov</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports both single-root I/O virtualization (SR-IOV) and PCI
  passthrough (PCIPT). Both technologies provide for better network
  performance.


 </p><p>
  This improves network I/O, decreases latency, and reduces processor overhead.
 </p><div class="sect3" id="id-1.6.11.5.19.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) Ethernet
   interface is a physical PCI Ethernet NIC that implements hardware-based
   virtualization mechanisms to expose multiple virtual network interfaces that
   can be used by one or more virtual machines simultaneously. With SR-IOV
   based NICs, the traditional virtual bridge is no longer required. Each
   SR-IOV port is associated with a virtual function (VF).
  </p><p>
   When compared with a PCI Passthtrough Ethernet interface, an SR-IOV Ethernet
   interface:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Provides benefits similar to those of a PCI Passthtrough Ethernet
     interface, including lower latency packet processing.
    </p></li><li class="listitem "><p>
     Scales up more easily in a virtualized environment by providing multiple
     VFs that can be attached to multiple virtual machine interfaces.
    </p></li><li class="listitem "><p>
     Shares the same limitations, including the lack of support for LAG, QoS,
     ACL, and live migration.
    </p></li><li class="listitem "><p>
     Has the same requirements regarding the VLAN configuration of the access
     switches.
    </p></li></ul></div><p>
   The process for configuring SR-IOV includes creating a VLAN provider network
   and subnet, then attaching VMs to that network.
  </p><p>
   With SR-IOV based NICs, the traditional virtual bridge is no longer
   required. Each SR-IOV port is associated with a virtual function (VF)
  </p></div><div class="sect3" id="id-1.6.11.5.19.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">PCI passthrough Ethernet interfaces</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A passthrough Ethernet interface is a physical PCI Ethernet NIC on a compute
   node to which a virtual machine is granted direct access. PCI passthrough
   allows a VM to have direct access to the hardware without being brokered by
   the hypervisor. This minimizes packet processing delays but at the same time
   demands special operational considerations. For all purposes, a PCI
   passthrough interface behaves as if it were physically attached to the
   virtual machine. Therefore any potential throughput limitations coming from
   the virtualized environment, such as the ones introduced by internal copying
   of data buffers, are eliminated. However, by bypassing the virtualized
   environment, the use of PCI passthrough Ethernet devices introduces several
   restrictions that must be taken into consideration. They include:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     no support for LAG, QoS, ACL, or host interface monitoring
    </p></li><li class="listitem "><p>
     no support for live migration
    </p></li><li class="listitem "><p>
     no access to the compute node's OVS switch
    </p></li></ul></div><p>
   A passthrough interface bypasses the compute node's OVS switch completely,
   and is attached instead directly to the provider network's access switch.
   Therefore, proper routing of traffic to connect the passthrough interface to
   a particular tenant network depends entirely on the VLAN tagging options
   configured on both the passthrough interface and the access port on the
   switch (TOR).
  </p><p>
   The access switch routes incoming traffic based on a VLAN ID, which
   ultimately determines the tenant network to which the traffic belongs. The
   VLAN ID is either explicit, as found in incoming tagged packets, or
   implicit, as defined by the access port's default VLAN ID when the incoming
   packets are untagged. In both cases the access switch must be configured to
   process the proper VLAN ID, which therefore has to be known in advance
  </p></div><div class="sect3" id="pci-passthrough"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Leveraging PCI Passthrough</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#pci-passthrough">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>pci-passthrough</li></ul></div></div></div></div><p>
   Two parts are necessary to leverage PCI passthrough on a SUSE <span class="productname">OpenStack</span> Cloud 8
   Compute Node: preparing the Compute Node, preparing Nova and Glance.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Preparing the Compute Node</strong></span>
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       There should be no kernel drivers or binaries with direct access to the
       PCI device. If there are kernel modules, they should be blacklisted.
      </p><p>
       For example, it is common to have a <code class="literal">nouveau</code> driver
       from when the node was installed. This driver is a graphics driver for
       Nvidia-based GPUs. It must be blacklisted as shown in this example.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/nouveau-default.conf</pre></div><p>
       The file location and its contents are important; the filename is your
       choice. Other drivers can be blacklisted in the same manner, possibly
       including Nvidia drivers.
      </p></li><li class="step "><p>
       On the host, <code class="literal">iommu_groups</code> should be enabled. To check
       if IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments)
.....</pre></div><p>
       To modify the kernel cmdline as suggested in the warning, edit the file
       <code class="filename">/etc/default/grub</code> and append
       <code class="literal">intel_iommu=on</code> to the
       <code class="literal">GRUB_CMDLINE_LINUX_DEFAULT</code> variable. Then run
       <code class="literal">update-bootloader</code>.
      </p><p>
       A reboot will be required for <code class="literal">iommu_groups</code> to be
       enabled.
      </p></li><li class="step "><p>
       After the reboot, check that IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: PASS
.....</pre></div></li><li class="step "><p>
       Confirm IOMMU groups are available by finding the group associated with
       your PCI device (for example Nvidia GPU):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lspci -nn | grep -i nvidia
08:00.0 VGA compatible controller [0300]: NVIDIA Corporation GT218 [NVS 300] [10de:10d8] (rev a2)
08:00.1 Audio device [0403]: NVIDIA Corporation High Definition Audio Controller [10de:0be3] (rev a1)</pre></div><p>
       In this example, <code class="literal">08:00.0</code> and
       <code class="literal">08:00.1</code> are addresses of the PCI device. The vendorID
       is <code class="literal">10de</code>. The productIDs are <code class="literal">10d8</code>
       and <code class="literal">0be3</code>.
      </p></li><li class="step "><p>
       Confirm that the devices are available for passthrough:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -ld /sys/kernel/iommu_groups/*/devices/*08:00.?/
drwxr-xr-x 3 root root 0 Feb 14 13:05 /sys/kernel/iommu_groups/20/devices/0000:08:00.0/
drwxr-xr-x 3 root root 0 Feb 19 16:09 /sys/kernel/iommu_groups/20/devices/0000:08:00.1/</pre></div><div id="id-1.6.11.5.19.6.3.1.2.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        With PCI passthrough, only an entire IOMMU group can be passed. Parts
        of the group cannot be passed. In this example, the IOMMU group is
        <code class="literal">20</code>.
       </p></div></li></ol></div></div></li><li class="listitem "><p>
     <span class="bold"><strong>Preparing Nova and Glance for
     passthrough</strong></span>
    </p><p>
     Information about configuring Nova and Glance is available in the
     documentation at
     <a class="link" href="https://docs.openstack.org/nova/pike/admin/pci-passthrough.html" target="_blank">https://docs.openstack.org/nova/pike/admin/pci-passthrough.html</a>.
     Both <code class="literal">nova-compute</code> and <code class="literal">nova-scheduler</code>
     must be configured.
    </p></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Intel 82599 Devices</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="table" id="intel-82599-table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 9.1: </span><span class="name">Intel 82599 devices supported with SRIOV and PCIPT </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#intel-82599-table">#</a></h6></div><div class="table-contents"><table class="table" summary="Intel 82599 devices supported with SRIOV and PCIPT" border="1"><colgroup><col align="center" class="c1" /><col align="center" class="c2" /><col align="center" class="c3" /></colgroup><thead><tr><th align="center">Vendor</th><th align="center">Device</th><th align="center">Title</th></tr></thead><tbody><tr><td align="center">Intel Corporation</td><td align="center">10f8</td><td align="center">82599 10 Gigabit Dual Port Backplane Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10f9</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fb</td><td align="center">82599ES 10-Gigabit SFI/SFP+ Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fc</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr></tbody></table></div></div></div><div class="sect3" id="id-1.6.11.5.19.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SRIOV PCIPT configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you plan to take advantage of SR-IOV support in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> you will need to
   plan in advance to meet the following requirements:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use one of the supported NIC cards:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       HP Ethernet 10Gb 2-port 560FLR-SFP+ Adapter (Intel Niantic). Product
       part number: 665243-B21 -- Same part number for the following card
       options:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         FlexLOM card
        </p></li><li class="listitem "><p>
         PCI slot adapter card
        </p></li></ul></div></li></ul></div></li><li class="listitem "><p>
     Identify the NIC ports to be used for PCI Passthrough devices and SRIOV
     devices from each compute node
    </p></li><li class="listitem "><p>
     Ensure that:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       SRIOV is enabled in the BIOS
      </p></li><li class="listitem "><p>
       HP Shared memory is disabled in the BIOS on the compute nodes.
      </p></li><li class="listitem "><p>
       The Intel boot agent is disabled on the compute
       (<a class="xref" href="ops-managing-networking.html#bootutil" title="9.3.15.11. Intel bootutils">Section 9.3.15.11, “Intel bootutils”</a> can be used to perform this)
      </p></li></ul></div><div id="id-1.6.11.5.19.8.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Because of Intel driver limitations, you cannot use a NIC port as an
      SRIOV NIC as well as a physical NIC. Using the physical function to carry
      the normal tenant traffic through the OVS bridge at the same time as
      assigning the VFs from the same NIC device as passthrough to the guest VM
      is not supported.
     </p></div></li></ol></div><p>
   If the above prerequisites are met, then SR-IOV or PCIPT can be reconfigured
   at any time. There is no need to do it at install time.
  </p></div><div class="sect3" id="id-1.6.11.5.19.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment use cases</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are typical use cases that should cover your particular needs:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     A device on the host needs to be enabled for both PCI-passthrough and
     PCI-SRIOV during deployment. At run time Nova decides whether to use
     physical functions or virtual function depending on vnic_type of the port
     used for booting the VM.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-passthrough.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-SRIOV virtual
     functions.
    </p></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model updates</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> provides various options for the user to configure the
   network for tenant VMs. These options have been enhanced to support SRIOV
   and PCIPT.
  </p><p>
   the Cloud Lifecycle Manager input model changes to support SRIOV and PCIPT are as follows. If
   you were familiar with the configuration settings previously, you will
   notice these changes.
  </p><p>
   <span class="bold"><strong>net_interfaces.yml:</strong></span> This file defines the
   interface details of the nodes. In it, the following fields have been added
   under the compute node interface section:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>sriov_only: </td><td>
       <p>
        Indicates that only SR-IOV be enabled on the interface. This should be
        set to true if you want to dedicate the NIC interface to support only
        SR-IOV functionality.
       </p>
      </td></tr><tr><td>pci-pt: </td><td>
       <p>
        When this value is set to true, it indicates that PCIPT should be
        enabled on the interface.
       </p>
      </td></tr><tr><td>vf-count: </td><td>
       <p>
        Indicates the number of VFs to be configured on a given interface.
       </p>
      </td></tr></tbody></table></div><p>
   In control_plane.yml, under Compute resource neutron-sriov-nic-agent has
   been added as service components
  </p><p>
   under resources:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td> Compute</td></tr><tr><td>resource-prefix:</td><td> Comp</td></tr><tr><td>server-role:</td><td>COMPUTE-ROLE</td></tr><tr><td>allocation-policy:</td><td> Any</td></tr><tr><td>min-count:</td><td> 0</td></tr><tr><td>service-components:</td><td>ntp-client</td></tr><tr><td> </td><td>nova-compute</td></tr><tr><td> </td><td>nova-compute-kvm</td></tr><tr><td> </td><td>neutron-l3-agent</td></tr><tr><td> </td><td>neutron-metadata-agent</td></tr><tr><td> </td><td>neutron-openvswitch-agent</td></tr><tr><td> </td><td>neutron-lbaasv2-agent</td></tr><tr><td> </td><td>- neutron-sriov-nic-agent*</td></tr></tbody></table></div><p>
   <span class="bold"><strong>nic_device_data.yml:</strong></span> This is the new file
   added with this release to support SRIOV and PCIPT configuration details. It
   contains information about the specifics of a nic, and is found here:
   <code class="literal">~/openstack/ardana/services/osconfig/nic_device_data.yml</code>.
   The fields in this file are as follows.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>nic-device-types:</strong></span> The nic-device-types
     section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the nic-device-types that will be referenced in
          nic_mappings.yml
         </p>
        </td></tr><tr><td>family:</td><td>
         <p>
          The name of the nic-device-families to be used with this
          nic_device_type
         </p>
        </td></tr><tr><td>device_id:</td><td>
         <p>
          Device ID as specified by the vendor for the particular NIC
         </p>
        </td></tr><tr><td>type:</td><td>
         <p>
          The value of this field can be "simple-port" or "multi-port". If a
          single bus address is assigned to more than one nic it will be
          multi-port, else if there is a one-to one mapping between bus address
          and the nic then it will be simple-port.
         </p>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     <span class="bold"><strong>nic-device-families:</strong></span> The
     nic-device-families section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the device family that can be used for reference in
          nic-device-types.
         </p>
        </td></tr><tr><td>vendor-id: </td><td>
         <p>
          Vendor ID of the NIC
         </p>
        </td></tr><tr><td>config-script:</td><td>
         <p>
          A script file used to create the virtual functions (VF) on the
          Compute node.
         </p>
        </td></tr><tr><td>driver:</td><td>
         <p>
          Indicates the NIC driver that needs to be used.
         </p>
        </td></tr><tr><td>vf-count-type:</td><td>
         <p>
          This value can be either "port" or "driver".
         </p>
        </td></tr><tr><td>“port”:</td><td>
         <p>
          Indicates that the device supports per-port virtual function (VF)
          counts.
         </p>
        </td></tr><tr><td>“driver:”</td><td>
         <p>
          Indicates that all ports using the same driver will be configured
          with the same number of VFs, whether or not the interface model
          specifies a vf-count attribute for the port. If two or more ports
          specify different vf-count values, the config processor errors out.
         </p>
        </td></tr><tr><td>Max-vf-count:</td><td>
         <p>
          This field indicates the maximum VFs that can be configured on an
          interface as defined by the vendor.
         </p>
        </td></tr></tbody></table></div></li></ol></div><p>
   <span class="bold"><strong>control_plane.yml:</strong></span> This file provides the
   information about the services to be run on a particular node. To support
   SR-IOV on a particular compute node, you must run neutron-sriov-nic-agent on
   that node.
  </p><p>
   <span class="bold"><strong>Mapping the use cases with various fields in input
   model</strong></span>
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /><col class="col7" /></colgroup><thead><tr><th> </th><th>Vf-count</th><th>SR-IOV</th><th>PCIPT</th><th>OVS bridge</th><th>Can be NIC bonded</th><th>Use case</th></tr></thead><tbody><tr><td>sriov-only: true</td><td>Mandatory</td><td>Yes</td><td>No</td><td>No</td><td>No</td><td>Dedicated to SRIOV</td></tr><tr><td>pci-pt : true</td><td>Not Specified</td><td>No</td><td>Yes</td><td>No</td><td>No</td><td>Dedicated to PCI-PT</td></tr><tr><td>pci-pt : true</td><td>Specified</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>PCI-PT or SRIOV</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Specified</td><td>Yes</td><td>No</td><td>Yes</td><td>No</td><td>SRIOV with PF used by host</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Not Specified</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td><td>Traditional/Usual use case</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.19.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Mappings between nic_mappings.yml and net_interfaces.yml</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following diagram shows which fields in nic_mappings.yml map to
   corresponding fields in net_interfaces.yml:
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-sriov_pcpit.png" target="_blank"><img src="images/media-sriov_pcpit.png" width="" /></a></div></div></div><div class="sect3" id="id-1.6.11.5.19.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Use Cases for Intel</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Nic-device-types and nic-device-families</strong></span>
     with Intel 82559 with ixgbe as the driver.
    </p><div class="verbatim-wrap"><pre class="screen">nic-device-types:
    - name: ''8086:10fb
      family: INTEL-82599
      device-id: '10fb'
      type: simple-port
nic-device-families:
    # Niantic
    - name: INTEL-82599
      vendor-id: '8086'
      config-script: intel-82599.sh
      driver: ixgbe
      vf-count-type: port
      max-vf-count: 63</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       sriov-only: true
       vf-count: 6
     network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the PCIPT-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       pci-pt: true
    network-groups:
     - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV and
     PCIPT use case
    </p><div class="verbatim-wrap"><pre class="screen"> - name: COMPUTE-INTERFACES
    - name: hed1
      device:
        name: hed1
        pci-pt: true
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for SRIOV and Normal
     Virtio use case
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
        name: hed1
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for PCI-PT
     (<code class="literal">hed1</code> and <code class="literal">hed4</code> refer to the DUAL
     ports of the PCI-PT NIC)
    </p><div class="verbatim-wrap"><pre class="screen">    - name: COMPUTE-PCI-INTERFACES
      network-interfaces:
      - name: hed3
        device:
          name: hed3
        network-groups:
          - MANAGEMENT
          - EXTERNAL-VM
        forced-network-groups:
          - EXTERNAL-API
      - name: hed1
        device:
          name: hed1
          pci-pt: true
        network-groups:
          - GUEST
      - name: hed4
        device:
          name: hed4
          pci-pt: true
        network-groups:
          - GUEST</pre></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Launching Virtual Machines</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Provisioning a VM with SR-IOV NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a Neutron port with <code class="literal">vnic_type = direct</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create $net_id --name sriov_port --binding:vnic_type direct</pre></div></li><li class="step "><p>
     Boot a VM with the created <code class="literal">port-id</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova boot --flavor m1.large --image ubuntu_14.04 --nic port-id=$port_id test-sriov</pre></div></li></ol></div></div><p>
   Provisioning a VM with PCI-PT NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create two Neutron ports with <code class="literal">vnic_type =
     direct-physical</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create net1 --name pci-port1 --vnic_type=direct-physical
neutron port-create net1 --name pci-port2  --vnic_type=direct-physical</pre></div></li><li class="step "><p>
     Boot a VM with the created ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova boot --flavor 4 --image opensuse --nic port-id pci-port1-port-id \
--nic port-id pci-port2-port-id vm1-pci-passthrough</pre></div></li></ol></div></div><p>
   If PCI-PT VM gets stuck (hangs) at boot time when using an Intel NIC, the
   boot agent should be disabled.
  </p></div><div class="sect3" id="bootutil"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Intel bootutils</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#bootutil">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>bootutil</li></ul></div></div></div></div><p>
   When Intel cards are used for PCI-PT, a tenant VM can get stuck at boot
   time. When this happens, you should download Intel bootutils and use it to
   should disable bootagent.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Download Preebot.tar.gz from
     <a class="link" href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers" target="_blank">https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers</a>
    </p></li><li class="listitem "><p>
     Untar the <code class="literal">Preboot.tar.gz</code> on the compute node where the
     PCI-PT VM is to be hosted.
    </p></li><li class="listitem "><p>
     Go to ~/APPS/BootUtil/Linux_x64
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/APPS/BootUtil/Linux_x64</pre></div><p>
     and run following command
    </p><div class="verbatim-wrap"><pre class="screen">./bootutil64e -BOOTENABLE disable -all</pre></div></li><li class="listitem "><p>
     Boot the PCI-PT VM and it should boot without getting stuck.
    </p><div id="id-1.6.11.5.19.14.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Here even though VM console shows VM getting stuck at PXE boot, it is not
      related to BIOS PXE settings.
     </p></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.19.15"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making input model changes and implementing PCI PT and SR-IOV</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To implent the configuration you require, log into the Cloud Lifecycle Manager node and update
   the Cloud Lifecycle Manager model files to enable SR-IOV or PCIPT following the relevent use
   case explained above. You will need to edit
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     net_interfaces.yml
    </p></li><li class="listitem "><p>
     nic_device_data.yml
    </p></li><li class="listitem "><p>
     control_plane.yml
    </p></li></ul></div><p>
   To make the edits,
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Check out the site branch of the local git repository and change to the
     correct directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div></li><li class="listitem "><p>
     Open each file in vim or another editor and make the necessary changes.
     Save each file, then commit to the local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="listitem "><p>
     Here you will have the Cloud Lifecycle Manager enable your changes by running the necessary
     playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div><div id="id-1.6.11.5.19.15.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    After running the site.yml playbook above, you must reboot the compute
    nodes that are configured with Intel PCI devices.
   </p></div><div id="id-1.6.11.5.19.15.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    When a VM is running on an SRIOV port on a given compute node,
    reconfiguration is not supported.
   </p></div><p>
   You can set the number of virtual functions that must be enabled on a
   compute node at install time. You can update the number of virtual functions
   after deployment. If any VMs have been spawned before you change the number
   of virtual functions, those VMs may lose connectivity. Therefore, it is
   always recommended that if any virtual function is used by any tenant VM,
   you should not reconfigure the virtual functions. Instead, you should
   delete/migrate all the VMs on that NIC before reconfiguring the number of
   virtual functions.
  </p></div><div class="sect3" id="id-1.6.11.5.19.16"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.19.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Security groups are not applicable for PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Live migration is not supported for VMs with PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Rate limiting (QoS) is not applicable on SRIOV and PCI-PT ports.
    </p></li><li class="listitem "><p>
     SRIOV/PCIPT is not supported for VxLAN network.
    </p></li><li class="listitem "><p>
     DVR is not supported with SRIOV/PCIPT.
    </p></li><li class="listitem "><p>
     For Intel cards, the same NIC cannot be used for both SRIOV and normal VM
     boot.
    </p></li><li class="listitem "><p>
     Current upstream OpenStack code does not support this hot plugin of
     SRIOV/PCIPT interface using the nova <code class="literal">attach_interface</code>
     command. See <a class="link" href="https://review.openstack.org/#/c/139910/" target="_blank">https://review.openstack.org/#/c/139910/</a>
     for more information.
    </p></li><li class="listitem "><p>
     Neutron port-update when admin state is down will not work.
    </p></li><li class="listitem "><p>
     SLES Compute Nodes with dual-port PCI-PT NICs, both ports should always be
     passed in the VM. It is not possible to split the dual port and pass
     through just a single port.
    </p></li></ul></div></div><div class="sect3" id="pcipt-gen9"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.15.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling PCI-PT on HPE DL360 Gen 9 Servers</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#pcipt-gen9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-enabling_pcipt_on_gen9.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-enabling_pcipt_on_gen9.xml</li><li><span class="ds-label">ID: </span>pcipt-gen9</li></ul></div></div></div></div><p>
  The HPE DL360 Gen 9 and HPE ProLiant systems with Intel processors use a
  region of system memory for sideband communication of management
  information. The BIOS sets up Reserved Memory Region Reporting (RMRR) to
  report these memory regions and devices to the operating system. There is a
  conflict between the Linux kernel and RMRR which causes problems with PCI
  pass-through (PCI-PT). This is needed for IOMMU use by DPDK. Note that this
  does not affect SR-IOV.
 </p><p>
  In order to enable PCI-PT on the HPE DL360 Gen 9 you must have a version of
  firmware that supports setting this and you must change a BIOS setting.
 </p><p>
  To begin, get the latest firmware and install it on your compute nodes.
 </p><p>
  Once the firmware has been updated:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Reboot the server and press <span class="keycap">F9</span> (system utilities) during POST (power on
    self test)
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">System Configuration</span>
   </p></li><li class="listitem "><p>
    Select the NIC for which you want to enable PCI-PT
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">Device Level Configuration</span>
   </p></li><li class="listitem "><p>
    Disable the shared memory feature in the BIOS.
   </p></li><li class="listitem "><p>
    Save the changes and reboot server
   </p></li></ol></div></div></div><div class="sect2" id="vlan-aware"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up VLAN-Aware VMs</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#vlan-aware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>vlan-aware</li></ul></div></div></div></div><p>
  Creating a VM with a trunk port will allow a VM to gain connectivity to one
  or more networks over the same virtual NIC (vNIC) through the use VLAN
  interfaces in the guest VM. Connectivity to different networks can be added
  and removed dynamically through the use of subports. The network of the
  parent port will be presented to the VM as the untagged VLAN, and the
  networks of the child ports will be presented to the VM as the tagged VLANs
  (the VIDs of which can be chosen arbitrarily as long as they are unique to
  that trunk). The VM will send/receive VLAN-tagged traffic over the subports,
  and Neutron will mux/demux the traffic onto the subport's corresponding
  network. This is not to be confused with VLAN transparency where a VM can
  pass VLAN-tagged traffic transparently across the network without
  interference from Neutron. VLAN transparency is not supported.
 </p><div class="sect3" id="id-1.6.11.5.20.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Terminology</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.20.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Trunk</strong></span>: a resource that logically
     represents a trunked vNIC and references a parent port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Parent port</strong></span>: a Neutron port that a Trunk
     is referenced to. Its network is presented as the untagged VLAN.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Subport</strong></span>: a resource that logically
     represents a tagged VLAN port on a Trunk. A Subport references a child
     port and consists of the
     &lt;port&gt;,&lt;segmentation-type&gt;,&lt;segmentation-id&gt; tuple.
     Currently only the 'vlan' segmentation type is supported.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Child port</strong></span>: a Neutron port that a Subport
     is referenced to. Its network is presented as a tagged VLAN based upon the
     segmentation-id used when creating/adding a Subport.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy VM</strong></span>: a VM that does not use a trunk
     port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy port</strong></span>: a Neutron port that is not
     used in a Trunk.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>VLAN-aware VM</strong></span>: a VM that uses at least
     one trunk port.
    </p></li></ul></div></div><div class="sect3" id="id-1.6.11.5.20.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trunk CLI reference</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> Command</th><th>Action</th></tr></thead><tbody><tr><td>network trunk create </td><td>Create a trunk.</td></tr><tr><td>network trunk delete </td><td>Delete a given trunk.</td></tr><tr><td>network trunk list </td><td>List all trunks.</td></tr><tr><td>network trunk show </td><td>Show information of a given trunk.</td></tr><tr><td>network trunk set </td><td>Add subports to a given trunk.</td></tr><tr><td>network subport list </td><td>List all subports for a given trunk.</td></tr><tr><td>network trunk unset</td><td>Remove subports from a given trunk.</td></tr><tr><td>network trunk set</td><td>Update trunk properties.</td></tr></tbody></table></div></div><div class="sect3" id="id-1.6.11.5.20.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling VLAN-aware VM capability</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> to
     add the "trunk" service_plugin:
    </p><div class="verbatim-wrap"><pre class="screen">service_plugins = {{ neutron_service_plugins }},trunk</pre></div></li><li class="listitem "><p>
     Edit <code class="literal">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
     to enable the noop firewall driver:
    </p><div class="verbatim-wrap"><pre class="screen">[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><div id="id-1.6.11.5.20.5.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      This is a manual configuration step because it must be made apparent that
      this step disables Neutron security groups completely. The default
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall_driver is
      <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewall
      Driver</code> which does not implement security groups for trunk
      ports. Optionally, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver may still be used
      (that is, skip this step), which would provide security groups for legacy
      VMs but not for VLAN-aware VMs. However, this mixed environment is not
      recommended. For more information, see <a class="xref" href="ops-managing-networking.html#firewall" title="9.3.16.6. Firewall issues">Section 9.3.16.6, “Firewall issues”</a>.
     </p></div></li><li class="listitem "><p>
     Commit the configuration changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enable vlan-aware VMs"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/</pre></div></li><li class="listitem "><p>
     If this is an initial deployment, continue the rest of normal deployment
     process:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li><li class="listitem "><p>
     If the cloud has already been deployed and this is a reconfiguration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.6.11.5.20.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Cases</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.20.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Creating a trunk port</strong></span>
  </p><p>
   Assume that a number of Neutron networks/subnets already exist: private,
   foo-net, and bar-net. This will create a trunk with two subports allocated
   to it. The parent port will be on the "private" network, while the two child
   ports will be on "foo-net" and "bar-net", respectively:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create a port that will function as the trunk's parent port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create --name trunkparent private</pre></div></li><li class="listitem "><p>
     Create ports that will function as the child ports to be used in subports:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-create --name subport1 foo-net
<code class="prompt user">ardana &gt; </code>neutron port-create --name subport2 bar-net</pre></div></li><li class="listitem "><p>
     Create a trunk port using the <code class="literal">openstack network trunk
     create</code> command, passing the parent port created in step 1 and
     child ports created in step 2:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent --subport port=subport1,segmentation-type=vlan,segmentation-id=1 --subport port=subport2,segmentation-type=vlan,segmentation-id=2 mytrunk
+-----------------+-----------------------------------------------------------------------------------------------+
| Field           | Value                                                                                         |
+-----------------+-----------------------------------------------------------------------------------------------+
| admin_state_up  | UP                                                                                            |
| created_at      | 2017-06-02T21:49:59Z                                                                          |
| description     |                                                                                               |
| id              | bd822ebd-33d5-423e-8731-dfe16dcebac2                                                          |
| name            | mytrunk                                                                                       |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358                                                          |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| revision_number | 1                                                                                             |
| status          | DOWN                                                                                          |
| sub_ports       | port_id='9d25abcf-d8a4-4272-9436-75735d2d39dc', segmentation_id='1', segmentation_type='vlan' |
|                 | port_id='e3c38cb2-0567-4501-9602-c7a78300461e', segmentation_id='2', segmentation_type='vlan' |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| updated_at      | 2017-06-02T21:49:59Z                                                                          |
+-----------------+-----------------------------------------------------------------------------------------------+

$ openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div><p>
     Optionally, a trunk may be created without subports (they can be added
     later):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent mytrunk
+-----------------+--------------------------------------+
| Field           | Value                                |
+-----------------+--------------------------------------+
| admin_state_up  | UP                                   |
| created_at      | 2017-06-02T21:45:35Z                 |
| description     |                                      |
| id              | eb8a3c7d-9f0a-42db-b26a-ca15c2b38e6e |
| name            | mytrunk                              |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358 |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9     |
| revision_number | 1                                    |
| status          | DOWN                                 |
| sub_ports       |                                      |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9     |
| updated_at      | 2017-06-02T21:45:35Z                 |
+-----------------+--------------------------------------+</pre></div><p>
     A port that is already bound (that is, already in use by a VM) cannot be
     upgraded to a trunk port. The port must be unbound to be eligible for
     use as a trunk's parent port. When adding subports to a trunk, the child
     ports must be unbound as well.
    </p></li></ol></div><p>
   <span class="bold"><strong>Checking a port's trunk details</strong></span>
  </p><p>
   Once a trunk has been created, its parent port will show the
   <code class="literal">trunk_details</code> attribute, which consists of the
   <code class="literal">trunk_id</code> and list of subport dictionaries:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-show -F trunk_details trunkparent
+---------------+-------------------------------------------------------------------------------------+
| Field         | Value                                                                               |
+---------------+-------------------------------------------------------------------------------------+
| trunk_details | {"trunk_id": "bd822ebd-33d5-423e-8731-dfe16dcebac2", "sub_ports":                   |
|               | [{"segmentation_id": 2, "port_id": "e3c38cb2-0567-4501-9602-c7a78300461e",          |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:11:90:d2"},                   |
|               | {"segmentation_id": 1, "port_id": "9d25abcf-d8a4-4272-9436-75735d2d39dc",           |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:ff:de:73"}]}                  |
+---------------+-------------------------------------------------------------------------------------+</pre></div><p>
   Ports that are not trunk parent ports will not have a
   <code class="literal">trunk_details</code> field:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-show -F trunk_details subport1
need more than 0 values to unpack</pre></div><p>
   <span class="bold"><strong>Adding subports to a trunk</strong></span>
  </p><p>
   Assuming a trunk and new child port have been created already, the
   <code class="literal">trunk-subport-add</code> command will add one or more subports
   to the trunk.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Run <code class="literal">openstack network trunk set</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk set --subport port=subport3,segmentation-type=vlan,segmentation-id=3 mytrunk</pre></div></li><li class="listitem "><p>
     Run <code class="literal">openstack network subport list</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
| bf958742-dbf9-467f-b889-9f8f2d6414ad | vlan              |               3 |
+--------------------------------------+-------------------+-----------------+</pre></div></li></ol></div><div id="id-1.6.11.5.20.6.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    The <code class="literal">--subport</code> option may be repeated multiple times in
    order to add multiple subports at a time.
   </p></div><p>
   <span class="bold"><strong>Removing subports from a trunk</strong></span>
  </p><p>
   To remove a subport from a trunk, use <code class="literal">openstack network trunk
   unset</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk unset --subport subport3 mytrunk</pre></div><p>
   <span class="bold"><strong>Deleting a trunk port</strong></span>
  </p><p>
   To delete a trunk port, use the <code class="literal">openstack network trunk
   delete</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk delete mytrunk</pre></div><p>
   Once a trunk has been created successfully, its parent port may be passed to
   the <code class="literal">nova boot</code> command, which will make the VM VLAN-aware:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova boot --image ubuntu-server --flavor 1 --nic port-id=239f8807-be2e-4732-9de6-c64519f46358 vlan-aware-vm</pre></div><div id="id-1.6.11.5.20.6.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    A trunk cannot be deleted until its parent port is unbound. Mainly, this
    means you must delete the VM using the trunk port before you are allowed to
    delete the trunk.
   </p></div></div><div class="sect3" id="id-1.6.11.5.20.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VLAN-aware VM network configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.6.11.5.20.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This section illustrates how to configure the VLAN interfaces inside a
   VLAN-aware VM based upon the subports allocated to the trunk port being
   used.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Run <code class="literal">openstack network trunk subport list</code> to see the
     VLAN IDs in use on the trunk port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div></li><li class="listitem "><p>
     Run <code class="literal">neutron port-show</code> on the child port to get its
     mac_address:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port-show -F mac_address 08848e38-50e6-4d22-900c-b21b07886fb7
+-------------+-------------------+
| Field       | Value             |
+-------------+-------------------+
| mac_address | fa:16:3e:08:24:61 |
+-------------+-------------------+</pre></div></li><li class="listitem "><p>
     Log into the VLAN-aware VM and run the following commands to set up the
     VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2
$ sudo ip link set dev ens3.2 up</pre></div></li><li class="listitem "><p>
     Note the usage of the mac_address from step 2 and VLAN ID from step 1 in
     configuring the VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2</pre></div></li><li class="listitem "><p>
     Trigger a DHCP request for the new vlan interface to verify connectivity
     and retrieve its IP address. On an Ubuntu VM, this might be:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo dhclient ens3.2
<code class="prompt user">tux &gt; </code>sudo ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:8d:77:39 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.5/24 brd 10.10.10.255 scope global ens3
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe8d:7739/64 scope link
       valid_lft forever preferred_lft forever
3: ens3.2@ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:11:90:d2 brd ff:ff:ff:ff:ff:ff
    inet 10.10.12.7/24 brd 10.10.12.255 scope global ens3.2
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe11:90d2/64 scope link
       valid_lft forever preferred_lft forever</pre></div></li></ol></div></div><div class="sect3" id="firewall"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.16.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall issues</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#firewall">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/develop/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>firewall</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver is
   <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</code>.
   This default does not implement security groups for VLAN-aware VMs, but it
   does implement security groups for legacy VMs. For this reason, it is
   recommended to disable Neutron security groups altogether when using
   VLAN-aware VMs. To do so, set:
  </p><div class="verbatim-wrap"><pre class="screen">firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><p>
   Doing this will prevent having a mix of firewalled and non-firewalled VMs in
   the same environment, but it should be done with caution because all VMs
   would be non-firewalled.
  </p></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="ops-managing-dashboards.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 10 </span>Managing the Dashboard</span></a><a class="nav-link" href="ops-managing-objectstorage.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 8 </span>Managing Object Storage</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>