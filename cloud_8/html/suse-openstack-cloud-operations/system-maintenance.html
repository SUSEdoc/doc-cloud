<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>System Maintenance | Operations Guide | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.2.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.81.0 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="Operations Guide" /><meta name="chapter-title" content="Chapter 13. System Maintenance" /><meta name="description" content="Information about managing and configuring your cloud as well as procedures for performing node maintenance." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" /><link rel="home" href="index.html" title="Documentation" /><link rel="up" href="book-operations.html" title="Operations Guide" /><link rel="prev" href="topic-ttn-5fg-4v.html" title="Chapter 12. Managing Monitoring, Logging, and Usage Reporting" /><link rel="next" href="bura-overview.html" title="Chapter 14. Backup and Restore" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Operations Guide"><span class="book-icon">Operations Guide</span></a><span> › </span><a class="crumb" href="system-maintenance.html">System Maintenance</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Operations Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="gettingstarted-ops.html"><span class="number">1 </span><span class="name">Operations Overview</span></a></li><li class="inactive"><a href="tutorials.html"><span class="number">2 </span><span class="name">Tutorials</span></a></li><li class="inactive"><a href="third-party-integrations.html"><span class="number">3 </span><span class="name">Third-Party Integrations</span></a></li><li class="inactive"><a href="ops-managing-identity.html"><span class="number">4 </span><span class="name">Managing Identity</span></a></li><li class="inactive"><a href="ops-managing-compute.html"><span class="number">5 </span><span class="name">Managing Compute</span></a></li><li class="inactive"><a href="ops-managing-esx.html"><span class="number">6 </span><span class="name">Managing ESX</span></a></li><li class="inactive"><a href="ops-managing-blockstorage.html"><span class="number">7 </span><span class="name">Managing Block Storage</span></a></li><li class="inactive"><a href="ops-managing-objectstorage.html"><span class="number">8 </span><span class="name">Managing Object Storage</span></a></li><li class="inactive"><a href="ops-managing-networking.html"><span class="number">9 </span><span class="name">Managing Networking</span></a></li><li class="inactive"><a href="ops-managing-dashboards.html"><span class="number">10 </span><span class="name">Managing the Dashboard</span></a></li><li class="inactive"><a href="ops-managing-orchestration.html"><span class="number">11 </span><span class="name">Managing Orchestration</span></a></li><li class="inactive"><a href="topic-ttn-5fg-4v.html"><span class="number">12 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li class="inactive"><a href="system-maintenance.html"><span class="number">13 </span><span class="name">System Maintenance</span></a></li><li class="inactive"><a href="bura-overview.html"><span class="number">14 </span><span class="name">Backup and Restore</span></a></li><li class="inactive"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html"><span class="number">15 </span><span class="name">Troubleshooting Issues</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 12. Managing Monitoring, Logging, and Usage Reporting" href="topic-ttn-5fg-4v.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 14. Backup and Restore" href="bura-overview.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Operations Guide"><span class="book-icon">Operations Guide</span></a><span> › </span><a class="crumb" href="system-maintenance.html">System Maintenance</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 12. Managing Monitoring, Logging, and Usage Reporting" href="topic-ttn-5fg-4v.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 14. Backup and Restore" href="bura-overview.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="system-maintenance"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></div><div><h1 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-system_maintenance.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-system_maintenance.xml</li><li><span class="ds-label">ID: </span>system-maintenance</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="system-maintenance.html#planned-maintenance"><span class="number">13.1 </span><span class="name">Planned System Maintenance</span></a></span></dt><dt><span class="section"><a href="system-maintenance.html#unplanned-maintenance"><span class="number">13.2 </span><span class="name">Unplanned System Maintenance</span></a></span></dt><dt><span class="section"><a href="system-maintenance.html#maintenance-update"><span class="number">13.3 </span><span class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span></a></span></dt><dt><span class="section"><a href="system-maintenance.html#deploy-ptf"><span class="number">13.4 </span><span class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span></a></span></dt><dt><span class="section"><a href="system-maintenance.html#database-maintenance"><span class="number">13.5 </span><span class="name">Periodic OpenStack Maintenance Tasks</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring your cloud as well as procedures
  for performing node maintenance.
 </p><p>
  This section contains the following sections to help you manage, configure,
  and maintain your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
 </p><div class="sect1" id="planned-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned System Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-maintenance">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-planned_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-planned_maintenance.xml</li><li><span class="ds-label">ID: </span>planned-maintenance</li></ul></div></div></div></div><p>
  Planned maintenance tasks for your cloud. See sections below for:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="system-maintenance.html#cont-planned" title="13.1.2. Planned Control Plane Maintenance">Section 13.1.2, “Planned Control Plane Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="system-maintenance.html#comp-planned" title="13.1.3. Planned Compute Maintenance">Section 13.1.3, “Planned Compute Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="system-maintenance.html#planned-maintenance-task-for-networking-nodes" title="13.1.4. Planned Network Maintenance">Section 13.1.4, “Planned Network Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="system-maintenance.html#storage-maintenance" title="13.1.5. Planned Storage Maintenance">Section 13.1.5, “Planned Storage Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="system-maintenance.html#mariadb-manual-update" title="13.1.6. Updating MariaDB with Galera">Section 13.1.6, “Updating MariaDB with Galera”</a>
   </p></li></ul></div><div class="sect2" id="sysmn-gen"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Whole Cloud Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#sysmn-gen">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-general_procedures.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-general_procedures.xml</li><li><span class="ds-label">ID: </span>sysmn-gen</li></ul></div></div></div></div><p>
  Planned maintenance procedures for your whole cloud.
 </p><div class="sect3" id="stop-restart"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bringing Down Your Cloud: Services Down Method</span> <a title="Permalink" class="permalink" href="system-maintenance.html#stop-restart">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_down.xml</li><li><span class="ds-label">ID: </span>stop-restart</li></ul></div></div></div></div><div id="id-1.6.15.4.4.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
   If you have a planned maintenance and need to bring down your entire cloud,
   update and reboot all nodes in the cloud one by one. Start with the deployer
   node, then follow the order recommended in <a class="xref" href="system-maintenance.html#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>. This method will bring down all of your services.
  </p></div><p>
  If you wish to use a method utilizing rolling reboots where your cloud
  services will continue running then see <a class="xref" href="system-maintenance.html#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>.
 </p><p>
  To perform backups prior to these steps, visit the backup and
  restore pages first at <a class="xref" href="bura-overview.html" title="Chapter 14. Backup and Restore">Chapter 14, <em>Backup and Restore</em></a>.
 </p><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-down-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Gracefully Bringing Down and Restarting Your Cloud Environment</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-down-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_down.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-down-xml-7</li></ul></div></div></div></div><p>
   You will do the following steps from your Cloud Lifecycle Manager.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Gracefully shut down your cloud by running the
     <code class="literal">ardana-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml</pre></div></li><li class="step "><p>
     Shut down your nodes. You should shut down your controller nodes last and
     bring them up first after the maintenance.
    </p><p>
     There are multiple ways you can do this:
    </p><ol type="a" class="substeps "><li class="step "><p>
       You can SSH to each node and use <code class="literal">sudo reboot -f</code> to
       reboot the node.
      </p></li><li class="step "><p>
       From the Cloud Lifecycle Manager, you can use the
       <code class="literal">bm-power-down.yml</code> and
       <code class="literal">bm-power-up.yml</code> playbooks.
      </p></li><li class="step "><p>
       You can shut down the nodes and then physically restart them either via a
       power button or the IPMI.
      </p></li></ol></li><li class="step "><p>
     Perform the necessary maintenance.
    </p></li><li class="step "><p>
     After the maintenance is complete, power your Cloud Lifecycle Manager back up
     and then SSH to it.
    </p></li><li class="step "><p>
     Determine the current power status of the nodes in your environment:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-status.yml</pre></div></li><li class="step "><p>
     If necessary, power up any nodes that are not already powered up, ensuring
     that you power up your controller nodes first. You can target specific
     nodes with the <code class="literal">-e nodelist=&lt;node_name&gt;</code> switch.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-up.yml [-e nodelist=&lt;node_name&gt;]</pre></div><div id="id-1.6.15.4.4.3.5.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     Obtain the <code class="literal">&lt;node_name&gt;</code> by using the
     <code class="command">sudo cobbler system list</code> command from the Cloud Lifecycle Manager.
    </p></div></li><li class="step "><p>
     Bring the databases back up:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step "><p>
     Gracefully bring up your cloud services by running the
     <code class="literal">ardana-start.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml</pre></div></li><li class="step "><p>
     Pause for a few minutes and give the cloud environment time to come up
     completely and then verify the status of the individual services using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li><li class="step "><p>
     If any services did not start properly, you can run playbooks for the
     specific services having issues.
    </p><p>
     For example:
    </p><p>
     If RabbitMQ fails, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-start.yml</pre></div><p>
     You can check the status of RabbitMQ afterwards with this:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div><p>
     If the recovery had failed, you can run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</pre></div><p>
     Each of the other services have playbooks in the
     <code class="literal">~/scratch/ansible/next/ardana/ansible</code> directory in the
     format of <code class="literal">&lt;service&gt;-start.yml</code> that you can run.
     One example, for the compute service, is
     <code class="literal">nova-start.yml</code>.
    </p></li><li class="step "><p>
     Continue checking the status of your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> cloud services until
     there are no more failed or unreachable nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></div></div><div class="sect3" id="rebootNodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rolling Reboot of the Cloud</span> <a title="Permalink" class="permalink" href="system-maintenance.html#rebootNodes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>rebootNodes</li></ul></div></div></div></div><p>
  If you have a planned maintenance and need to bring down your entire cloud
  and restart services while minimizing downtime, follow the steps here to
  safely restart your cloud. If you do not mind your services being down, then
  another option for planned maintenance can be found at
  <a class="xref" href="system-maintenance.html#stop-restart" title="13.1.1.1. Bringing Down Your Cloud: Services Down Method">Section 13.1.1.1, “Bringing Down Your Cloud: Services Down Method”</a>.
 </p><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended node reboot order</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-5</li></ul></div></div></div></div><p>
   To ensure that rebooted nodes reintegrate into the cluster, the key is
   having enough time between controller reboots.
  </p><p>
   The recommended way to achieve this is as follows:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Reboot controller nodes one-by-one with a suitable interval in between. If
     you alternate between controllers and compute nodes you will gain more
     time between the controller reboots.
    </p></li><li class="step "><p>
     Reboot of compute nodes (if present in your cloud).
    </p></li><li class="step "><p>
     Reboot of Swift nodes (if present in your cloud).
    </p></li><li class="step "><p>
     Reboot of ESX nodes (if present in your cloud).
    </p></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting controller nodes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Turn off the Keystone Fernet Token-Signing Key
   Rotation</strong></span>
  </p><p>
   Before rebooting any controller node, you need to ensure that the Keystone
   Fernet token-signing key rotation is turned off. Run the following command:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-stop-fernet-auto-rotation.yml</pre></div><p>
   <span class="bold"><strong>Migrate singleton services first</strong></span>
  </p><div id="id-1.6.15.4.4.4.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you have previously rebooted your Cloud Lifecycle Manager for any reason, ensure that
    the <code class="systemitem">apache2</code> service is running before
    continuing. To start the <code class="systemitem">apache2</code> service, use
    this command:
   </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start apache2</pre></div></div><p>
   The first consideration before rebooting any controller nodes is that there
   are a few services that run as singletons (non-HA), thus they will be
   unavailable while the controller they run on is down. Typically this is a
   very small window, but if you want to retain the service during the reboot
   of that server you should take special action to maintain service, such as
   migrating the service.
  </p><p>
   For these steps, if your singleton services are running on controller1 and
   you move them to controller2, then ensure you move them back to controller1
   before proceeding to reboot controller2.
  </p><p>
   <span class="bold"><strong>For the <code class="literal">cinder-volume</code> singleton
   service:</strong></span>
  </p><p>
   Execute the following command on each controller node to determine which
   node is hosting the cinder-volume singleton. It should be running on only
   one node:
  </p><div class="verbatim-wrap"><pre class="screen">ps auxww | grep cinder-volume | grep -v grep</pre></div><p>
   Run the <code class="literal">cinder-migrate-volume.yml</code> playbook - details
   about the Cinder volume and backup migration instructions can be found in
   <a class="xref" href="ops-managing-blockstorage.html#sec-operation-manage-block-storage" title="7.1.3. Managing Cinder Volume and Backup Services">Section 7.1.3, “Managing Cinder Volume and Backup Services”</a>.
  </p><p>
   <span class="bold"><strong>For the <code class="literal">nova-consoleauth</code> singleton
   service:</strong></span>
  </p><p>
   The <code class="literal">nova-consoleauth</code> component runs by default on the
   first controller node, that is, the host with
   <code class="literal">consoleauth_host_index=0</code>. To move it to another
   controller node before rebooting controller 0, run the ansible playbook
   <code class="literal">nova-start.yml</code> and pass it the index of the next
   controller node. For example, to move it to controller 2 (index of 1), run:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</pre></div><p>
   After you run this command you may now see two instances of the
   <code class="literal">nova-consoleauth</code> service, which will show as being in
   <code class="literal">disabled</code> state, when you run the <code class="literal">nova
   service-list</code> command. You can then delete the service using these
   steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Obtain the service ID for the duplicated nova-consoleauth service:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| Id | Binary           | Host                      | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 10 | nova-conductor   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:47.000000 | -               |
| 13 | nova-conductor   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 16 | nova-scheduler   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:39.000000 | -               |
| 19 | nova-scheduler   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 22 | nova-scheduler   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:44.000000 | -               |
| 25 | nova-consoleauth | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:45.000000 | -               |
| 49 | nova-compute     | ...a-cp1-comp0001-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 52 | nova-compute     | ...a-cp1-comp0002-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 55 | nova-compute     | ...a-cp1-comp0003-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:43.000000 | -               |
<span class="bold"><strong>| 70 | nova-consoleauth | ...a-cp1-c1-m3-mgmt    | internal | disabled | down  | 2016-08-25T12:10:40.000000 | -               |</strong></span>
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+</pre></div></li><li class="step "><p>
     Delete the disabled duplicate service with this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete &lt;service_ID&gt;</pre></div><p>
     Given the example in the previous step, the command could be:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete 70</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>For the SNAT namespace singleton service:</strong></span>
  </p><p>
   If you reboot the controller node hosting the SNAT namespace service on it,
   Compute instances without floating IPs will lose network connectivity when
   that controller is rebooted. To prevent this from happening, you can use
   these steps to determine which controller node is hosting the SNAT namespace
   service and migrate it to one of the other controller nodes while that node
   is rebooted.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Locate the SNAT node where the router is providing the active
     <code class="literal">snat_service</code>:
    </p><ol type="a" class="substeps "><li class="step "><p>
       From the Cloud Lifecycle Manager, list out your ports to determine which port
       is serving as the router gateway:
      </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
neutron port-list --device_owner network:router_gateway</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-list --device_owner network:router_gateway
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                           |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| 287746e6-7d82-4b2c-914c-191954eba342 |      | fa:16:3e:2e:26:ac | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"} |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
       Look at the details of this port to determine what the
       <code class="literal">binding:host_id</code> value is, which will point to the
       host in which the port is bound to:
      </p><div class="verbatim-wrap"><pre class="screen">neutron port-show &lt;port_id&gt;</pre></div><p>
       Example, with the value you need in bold:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m2-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div><p>
       In this example, the <code class="literal">ardana-cp1-c1-m2-mgmt</code> is the
       node hosting the SNAT namespace service.
      </p></li></ol></li><li class="step "><p>
     SSH to the node hosting the SNAT namespace service and check the SNAT
     namespace, specifying the router_id that has the interface with the subnet
     that you are interested in:
    </p><div class="verbatim-wrap"><pre class="screen">ssh &lt;IP_of_SNAT_namespace_host&gt;
sudo ip netns exec snat-&lt;router_ID&gt; bash</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns exec snat-e122ea3f-90c5-4662-bf4a-3889f677aacf bash</pre></div></li><li class="step "><p>
     Obtain the ID for the L3 Agent for the node hosting your SNAT namespace:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
neutron agent-list</pre></div><p>
     Example, with the entry you need given the examples above:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron agent-list
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 0126bbbf-5758-4fd0-84a8-7af4d93614b8 | DHCP agent           | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| 33dec174-3602-41d5-b7f8-a25fd8ff6341 | Metadata agent       | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-metadata-agent    |
| 3bc28451-c895-437b-999d-fdcff259b016 | L3 agent             | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 4af1a941-61c1-4e74-9ec1-961cebd6097b | L3 agent             | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-l3-agent          |
| 58f01f34-b6ca-4186-ac38-b56ee376ffeb | Loadbalancerv2 agent | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-lbaasv2-agent     |
| 65bcb3a0-4039-4d9d-911c-5bb790953297 | Open vSwitch agent   | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| 6981c0e5-5314-4ccd-bbad-98ace7db7784 | L3 agent             | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 7df9fa0b-5f41-411f-a532-591e6db04ff1 | Metadata agent       | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-metadata-agent    |
| 92880ab4-b47c-436c-976a-a605daa8779a | Metadata agent       | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-metadata-agent    |
<span class="bold"><strong>| a209c67d-c00f-4a00-b31c-0db30e9ec661 | L3 agent             | ardana-cp1-c1-m2-mgmt</strong></span>    | :-)   | True           | neutron-vpn-agent         |
| a9467f7e-ec62-4134-826f-366292c1f2d0 | DHCP agent           | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| b13350df-f61d-40ec-b0a3-c7c647e60f75 | Open vSwitch agent   | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| d4c07683-e8b0-4a2b-9d31-b5b0107b0b31 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-openvswitch-agent |
| e91d7f3f-147f-4ad2-8751-837b936801e3 | Open vSwitch agent   | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| f33015c8-f4e4-4505-b19b-5a1915b6e22a | DHCP agent           | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| fe43c0e9-f1db-4b67-a474-77936f7acebf | Metadata agent       | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-metadata-agent    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+</pre></div></li><li class="step "><p>
     Also obtain the ID for the L3 Agent of the node you are going to move the
     SNAT namespace service to using the same commands as the previous step.
    </p></li><li class="step "><p>
     Use these commands to move the SNAT namespace service, with the
     <code class="literal">router_id</code> being the same value as the ID for router:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Remove the L3 Agent for the old host:
      </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-remove &lt;agent_id_of_snat_namespace_host&gt; &lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron l3-agent-router-remove a209c67d-c00f-4a00-b31c-0db30e9ec661 e122ea3f-90c5-4662-bf4a-3889f677aacf
Removed router e122ea3f-90c5-4662-bf4a-3889f677aacf from L3 agent</pre></div></li><li class="step "><p>
       Remove the SNAT namespace:
      </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns delete snat-&lt;router_id&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ sudo ip netns delete snat-e122ea3f-90c5-4662-bf4a-3889f677aacf</pre></div></li><li class="step "><p>
       Create a new L3 Agent for the new host:
      </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-add &lt;agent_id_of_new_snat_namespace_host&gt; &lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ neutron l3-agent-router-add 3bc28451-c895-437b-999d-fdcff259b016 e122ea3f-90c5-4662-bf4a-3889f677aacf
Added router e122ea3f-90c5-4662-bf4a-3889f677aacf to L3 agent</pre></div></li></ol><p>
     Confirm that it has been moved by listing the details of your port from step
     1b above, noting the value of <code class="literal">binding:host_id</code> which
     should be updated to the host you moved your SNAT namespace to:
    </p><div class="verbatim-wrap"><pre class="screen">neutron port-show &lt;port_ID&gt;</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m1-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Reboot the controllers</strong></span>
  </p><p>
   In order to reboot the controller nodes, you must first retrieve a list of
   nodes in your cloud running control plane services.
  </p><div class="verbatim-wrap"><pre class="screen">for i in $(grep -w cluster-prefix ~/openstack/my_cloud/definition/data/control_plane.yml | awk '{print $2}'); do grep $i ~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts | grep ansible_ssh_host | awk '{print $1}'; done</pre></div><p>
   Then perform the following steps from your Cloud Lifecycle Manager for each of
   your controller nodes:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If any singleton services are active on this node, they will be
     unavailable while the node is down. If you want to retain the service
     during the reboot, you should take special action to maintain the service,
     such as migrating the service as appropriate as noted above.
    </p></li><li class="step "><p>
     Stop all services on the controller node that you are rebooting first:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     Reboot the controller node, e.g. run the following command on the
     controller itself:
    </p><div class="verbatim-wrap"><pre class="screen">sudo reboot</pre></div><p>
     Note that the current node being rebooted could be hosting the lifecycle
     manager.
    </p></li><li class="step "><p>
     Wait for the controller node to become ssh-able and allow an additional
     minimum of five minutes for the controller node to settle. Start all
     services on the controller node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     Verify that the status of all services on that is OK on the controller
     node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml --limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     When above start operation has completed successfully, you may proceed to
     the next controller node. Ensure that you migrate your singleton services
     off the node first.
    </p></li></ol></div></div><div id="id-1.6.15.4.4.4.4.26" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    It is important that you not begin the reboot procedure for a new
    controller node until the reboot of the previous controller node has been
    completed successfully (that is, the ardana-status playbook has completed
    without error).
   </p></div><p>
   <span class="bold"><strong>
    Reenable the Keystone Fernet Token-Signing Key Rotation
   </strong></span>
  </p><p>
   After all the controller nodes are successfully updated and back online, you
   need to re-enable the Keystone Fernet token-signing key rotation job by
   running the <code class="filename">keystone-reconfigure.yml</code> playbook. On the
   deployer, run:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-9"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting compute nodes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-9">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-9</li></ul></div></div></div></div><p>
   To reboot a compute node the following operations will need to be performed:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Disable provisioning of the node to take the node offline to prevent
     further instances being scheduled to the node during the reboot.
    </p></li><li class="listitem "><p>
     Identify instances that exist on the compute node, and then either:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Live migrate the instances off the node before actioning the reboot. OR
      </p></li><li class="listitem "><p>
       Stop the instances
      </p></li></ul></div></li><li class="listitem "><p>
     Reboot the node
    </p></li><li class="listitem "><p>
     Restart the Nova services
    </p></li></ul></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Disable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "&lt;describe reason&gt;" &lt;node name&gt; nova-compute</pre></div><p>
     If the node has existing instances running on it these instances will need
     to be migrated or stopped prior to re-booting the node.
    </p></li><li class="step "><p>
     Live migrate existing instances. Identify the instances on the compute
     node. Note: The following command must be run with nova admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="step "><p>
     Migrate or Stop the instances on the compute node.
    </p><p>
     Migrate the instances off the node by running one of the following
     commands for each of the instances:
    </p><p>
     If your instance is booted from a volume and has any number of Cinder
     volume attached, use the nova live-migration command:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only, you can use the
     --block-migrate option:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     Note: The [&lt;target compute host&gt;] option is optional. If you do not
     specify a target host then the nova scheduler will choose a node for you.
    </p><p>
     OR
    </p><p>
     Stop the instances on the node by running the following command for each
     of the instances:
    </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance-uuid&gt;</pre></div></li><li class="step "><p>
     Stop all services on the Compute node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;compute node&gt;</pre></div></li><li class="step "><p>
     SSH to your Compute nodes and reboot them:
    </p><div class="verbatim-wrap"><pre class="screen">sudo reboot</pre></div><p>
     The operating system cleanly shuts down services and then automatically
     reboots. If you want to be very thorough, run your backup jobs just before
     you reboot.
    </p></li><li class="step "><p>
     Run the ardana-start.yml playbook from the Cloud Lifecycle Manager. If needed, use
     the bm-power-up.yml playbook to restart the node. Specify just the node(s)
     you want to start in the 'nodelist' parameter arguments, that is,
     nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node&gt;</pre></div></li><li class="step "><p>
     Execute the <span class="bold"><strong>ardana-start.yml </strong></span>playbook.
     Specifying the node(s) you want to start in the 'limit' parameter
     arguments. This parameter accepts wildcard arguments and also
     '@&lt;filename&gt;' to process all hosts listed in the file.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;compute node&gt;</pre></div></li><li class="step "><p>
     Re-enable provisioning on the node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;node-name&gt; nova-compute</pre></div></li><li class="step "><p>
     Restart any instances you stopped.
    </p><div class="verbatim-wrap"><pre class="screen">nova start &lt;instance-uuid&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-10"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting Swift nodes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-10">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-10</li></ul></div></div></div></div><p>
   If your Swift services are on controller node, please follow the controller
   node reboot instructions above.
  </p><p>
   For a dedicated Swift PAC cluster or Swift Object resource node:
  </p><p>
   For each Swift host
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Stop all services on the Swift node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;Swift node&gt;</pre></div></li><li class="step "><p>
     Reboot the Swift node by running the following command on the Swift node
     itself:
    </p><div class="verbatim-wrap"><pre class="screen">sudo reboot</pre></div></li><li class="step "><p>
     Wait for the node to become ssh-able and then start all services on the
     Swift node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;swift node&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Get list of status playbooks</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-14">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-14</li></ul></div></div></div></div><p>
   Running the following command will yield a list of status playbooks:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ls *status*</pre></div><p>
   Here is the list:
  </p><div class="verbatim-wrap"><pre class="screen">ls *status*
bm-power-status.yml          heat-status.yml      logging-producer-status.yml
ceilometer-status.yml        FND-AP2-status.yml   ardana-status.yml
FND-CLU-status.yml           horizon-status.yml   logging-status.yml
cinder-status.yml            freezer-status.yml   ironic-status.yml
cmc-status.yml               glance-status.yml    keystone-status.yml
galera-status.yml            memcached-status.yml nova-status.yml
logging-server-status.yml    monasca-status.yml   ops-console-status.yml
monasca-agent-status.yml     neutron-status.yml   rabbitmq-status.yml
swift-status.yml             zookeeper-status.yml</pre></div></div></div></div><div class="sect2" id="cont-planned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Control Plane Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#cont-planned">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-cont_planned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-cont_planned.xml</li><li><span class="ds-label">ID: </span>cont-planned</li></ul></div></div></div></div><p>
  Planned maintenance tasks for controller nodes such as full cloud reboots and
  replacing controller nodes.
 </p><div class="sect3" id="replacing-controller"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Controller Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#replacing-controller">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-replace_controller.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_controller.xml</li><li><span class="ds-label">ID: </span>replacing-controller</li></ul></div></div></div></div><p>
  This section outlines steps for replacing a controller node in your
  environment.
 </p><p>
  For <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you must have three controller nodes.
  Therefore, adding or removing nodes is not an option. However, if you need to
  repair or replace a controller node, you may do so by following the steps
  outlined here. Note that to run any playbooks whatsoever for cloud
  maintenance, you will always run the steps from the Cloud Lifecycle Manager.
 </p><p>
  These steps will depend on whether you need to replace a shared lifecycle
  manager/controller node or whether this is a standalone controller node.
 </p><p>Keep in mind while performing the following tasks:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Do not add entries for a new server. Instead, update the entries for
    the broken one.
   </p></li><li class="listitem "><p>
    Be aware that all management commands are run on the node where the
    Cloud Lifecycle Manager is running.
   </p></li></ul></div><div class="sect4" id="replace-shared-lm"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Shared Cloud Lifecycle Manager/Controller Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace-shared-lm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-replace_shared_lm.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_shared_lm.xml</li><li><span class="ds-label">ID: </span>replace-shared-lm</li></ul></div></div></div></div><p>
  If the controller node you need to replace was also being used as your
  Cloud Lifecycle Manager then use these steps below. If this is not a shared
  controller then skip to the next section.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that you previously had
    loaded on your Cloud Lifecycle Manager, you will need to download and install the
    lifecycle management software using the instructions from the installation
    guide:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 3 “Installing the Cloud Lifecycle Manager server”, Section 3.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span>
     </p></li><li class="listitem "><p>
      To restore your data, see <a class="xref" href="system-maintenance.html#pit-lifecyclemanager-recovery" title="13.2.2.2.3. Point-in-time Cloud Lifecycle Manager Recovery">Section 13.2.2.2.3, “Point-in-time Cloud Lifecycle Manager Recovery”</a>
     </p></li></ol></div></li><li class="step "><p>
    On the new node, update your cloud model with the new
    <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields to
    reflect the attributes of the node. Do not change the
    <code class="literal">id</code>, <code class="literal">ip-addr</code>, <code class="literal">role</code>,
    or <code class="literal">server-group</code> settings.
   </p><div id="id-1.6.15.4.5.3.7.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div></li><li class="step "><p>
    Get the <span class="bold"><strong>servers.yml</strong></span> file stored in git:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div><p>
    then change, as necessary, the <code class="literal">mac-addr</code>,
    <code class="literal">ilo-ip</code>, <code class="literal">ilo-password</code>, and
    <code class="literal">ilo-user</code> fields of this existing controller node. Save
    and commit the change
   </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "repaired node X"</pre></div></li><li class="step "><p>
    Run the configuration processor as follows:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
    Then run ready-deployment:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Deploy Cobbler:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div><div id="id-1.6.15.4.5.3.7.3.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    After this step you may see failures because MariaDB has not finished
    syncing. If so, rerun this step.
   </p></div></li><li class="step "><p>
    Delete the haproxy user:
   </p><div class="verbatim-wrap"><pre class="screen">sudo userdel haproxy</pre></div></li><li class="step "><p>
    Install the software on your new Cloud Lifecycle Manager/controller node with
    these three playbooks:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller-hostname&gt;,&lt;first-proxy-hostname&gt;</pre></div></li><li class="step "><p>
    During the replacement of the node there will be alarms that show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></div><div class="sect4" id="replace-dedicated-lm"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Standalone Controller Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace-dedicated-lm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-replace_dedicated_lm.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_dedicated_lm.xml</li><li><span class="ds-label">ID: </span>replace-dedicated-lm</li></ul></div></div></div></div><p>
  If the controller node you need to replace is not also being used as the
  Cloud Lifecycle Manager, follow the steps below.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Update your cloud model, specifically the <code class="literal">servers.yml</code>
    file, with the new <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields
    where these have changed. Do not change the <code class="literal">id</code>,
    <code class="literal">ip-addr</code>, <code class="literal">role</code>, or
    <code class="literal">server-group</code> settings.
   </p></li><li class="step "><p>
    Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, as follows:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Remove the old controller node(s) from Cobbler. You can list out the
    systems in Cobbler currently with this command:
   </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div><p>
    and then remove the old controller nodes with this command:
   </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name &lt;node&gt;</pre></div></li><li class="step "><p>
    Remove the SSH key of the old controller node from the known hosts file.
    You will specify the <code class="literal">ip-addr</code> value:
   </p><div class="verbatim-wrap"><pre class="screen">ssh-keygen -f "~/.ssh/known_hosts" -R &lt;ip_addr&gt;</pre></div><p>
    You should see a response similar to this one:
   </p><div class="verbatim-wrap"><pre class="screen">ardana@ardana-cp1-c1-m1-mgmt:~/openstack/ardana/ansible$ ssh-keygen -f "~/.ssh/known_hosts" -R 10.13.111.135
# Host 10.13.111.135 found: line 6 type ECDSA
~/.ssh/known_hosts updated.
Original contents retained as ~/.ssh/known_hosts.old</pre></div></li><li class="step "><p>
    Run the cobbler-deploy playbook to add the new controller node:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
    Image the new node(s) by using the bm-reimage playbook. You will specify
    the name for the node in Cobbler in the command:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node-name&gt;</pre></div><div id="id-1.6.15.4.5.3.8.3.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     You must ensure that the old controller node is powered off before
     completing this step. This is because the new controller node will re-use
     the original IP address.
    </p></div></li><li class="step "><p>
    Configure the necessary keys used for the database etc:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml</pre></div></li><li class="step "><p>
    Run osconfig on the replacement controller node. For example:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;</pre></div></li><li class="step "><p>
    If the controller being replaced is the Swift ring builder (see
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a>) you need to restore the Swift ring
    builder files to the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory.
    See <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a> for details.
   </p></li><li class="step "><p>
    Run the ardana-deploy playbook on the replacement controller.
   </p><p>
    If the node being replaced is the Swift ring builder server then you only
    need to use the <code class="literal">--limit</code> switch for that node, otherwise
    you need to specify the hostname of your Swift ringer builder server and
    the hostname of the node being replaced.
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True
--limit=&lt;controller-hostname&gt;,&lt;swift-ring-builder-hostname&gt;</pre></div><div id="id-1.6.15.4.5.3.8.3.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     If you receive a Keystone failure when running this playbook, it is
     likely due to Fernet keys being out of sync. This problem can be corrected
     by running the <code class="filename">keystone-reconfigure.yml</code> playbook to
     re-sync the Fernet keys.
    </p><p>
     In this situation, do not use the <code class="literal">--limit</code> option when
     running <code class="filename">keystone-reconfigure.yml</code>. In order to re-sync
     Fernet keys, all the controller nodes must be in the play.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></div><div id="id-1.6.15.4.5.3.8.3.13.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
     If you receive a RabbitMQ failure when running this playbook, review
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#recoverrabbit" title="15.2.1. Understanding and Recovering RabbitMQ after Failure">Section 15.2.1, “Understanding and Recovering RabbitMQ after Failure”</a> for how to resolve the issue and then
     re-run the ardana-deploy playbook.
    </p></div></li><li class="step "><p>
    During the replacement of the node there will be alarms that show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="comp-planned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Compute Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#comp-planned">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-comp_planned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-comp_planned.xml</li><li><span class="ds-label">ID: </span>comp-planned</li></ul></div></div></div></div><p>
  Planned maintenance tasks for compute nodes.
 </p><div class="sect3" id="planned-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Maintenance for a Compute Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-computenode">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-planned_computenode.xml</li><li><span class="ds-label">ID: </span>planned-computenode</li></ul></div></div></div></div><p>
  If one or more of your compute nodes needs hardware maintenance and you can
  schedule a planned maintenance then this procedure should be followed.
 </p><div class="sect4" id="id-1.6.15.4.6.3.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing planned maintenance on a compute node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.6.3.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-planned_computenode.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you have planned maintenance to perform on a compute node, you have to
   take it offline, repair it, and restart it. To do so, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-list | grep compute</pre></div><p>
     The following example shows two compute nodes:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova host-list | grep compute
| ardana-cp1-comp0001-mgmt | compute     | AZ1      |
| ardana-cp1-comp0002-mgmt | compute     | AZ2      |</pre></div></li><li class="step "><p>
     Disable provisioning on the compute node, which will prevent additional
     instances from being spawned on it:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "Maintenance mode" &lt;hostname&gt; nova-compute</pre></div><div id="id-1.6.15.4.6.3.3.3.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Make sure you re-enable provisioning after the maintenance is complete
      if you want to continue to be able to spawn instances on the node. You
      can do this with the command:
     </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;hostname&gt; nova-compute</pre></div></div></li><li class="step "><p>
     At this point you have two choices:
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       <span class="bold"><strong>Live migration</strong></span>: This option enables you
       to migrate the instances off the compute node with minimal downtime so
       you can perform the maintenance without risk of losing data.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Stop/start the instances</strong></span>: Issuing
       <code class="literal">nova stop</code> commands to each of the instances will halt
       them. This option lets you do maintenance and then start the instances
       back up, as long as no disk failures occur on the compute node data
       disks. This method involves downtime for the length of the maintenance.
      </p></li></ol></div><p>
     If you choose the live migration route, See
     <a class="xref" href="system-maintenance.html#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a> for more details. Skip to step #6
     after you finish live migration.
    </p><p>
     If you choose the stop start method, continue on.
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       List all of the instances on the node so you can issue stop commands to
       them:
      </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
       Issue the <code class="literal">nova stop</code> command against each of the
       instances:
      </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance uuid&gt;</pre></div></li><li class="listitem "><p>
       Confirm that the instances are stopped. If stoppage was successful you
       should see the instances in a <code class="literal">SHUTOFF</code> state, as shown
       here:
      </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status  | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>SHUTOFF</strong></span> | -          | Shutdown    | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+</pre></div></li><li class="listitem "><p>
       Do your required maintenance. If this maintenance does not take down the
       disks completely then you should be able to list the instances again
       after the repair and confirm that they are still in their
       <code class="literal">SHUTOFF</code> state:
      </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
       Start the instances back up using this command:
      </p><div class="verbatim-wrap"><pre class="screen">nova start &lt;instance uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ nova start ef31c453-f046-4355-9bd3-11e774b1772f
Request to start server ef31c453-f046-4355-9bd3-11e774b1772f has been accepted.</pre></div></li><li class="listitem "><p>
       Confirm that the instances started back up. If restarting is successful
       you should see the instances in an <code class="literal">ACTIVE</code> state, as
       shown here:
      </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>ACTIVE</strong></span> | -          | Running     | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="listitem "><p>
       If the <code class="literal">nova start</code> fails, you can try doing a hard
       reboot:
      </p><div class="verbatim-wrap"><pre class="screen">nova reboot --hard &lt;instance uuid&gt;</pre></div><p>
       If this does not resolve the issue you may want to contact support.
      </p></li></ol></div></li><li class="step "><p>
     Reenable provisioning when the node is fixed:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;hostname&gt; nova-compute</pre></div></li></ol></div></div></div></div><div class="sect3" id="reboot-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting a Compute Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#reboot-computenode">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-reboot_computenode.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-reboot_computenode.xml</li><li><span class="ds-label">ID: </span>reboot-computenode</li></ul></div></div></div></div><p>
  If all you need to do is reboot a Compute node, the following steps can be
  used.
 </p><p>
  You can choose to live migrate all Compute instances off the node prior to
  the reboot. Any instances that remain will be restarted when the node is
  rebooted. This playbook will ensure that all services on the Compute node are
  restarted properly.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Reboot the Compute node(s) with the following playbook.
   </p><p>
    You can specify either single or multiple Compute nodes using the
    <code class="literal">--limit</code> switch.
   </p><p>
    An optional reboot wait time can also be specified. If no reboot wait time
    is specified it will default to 300 seconds.
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-compute-reboot.yml --limit [compute_node_or_list] [-e nova_reboot_wait_timeout=(seconds)]</pre></div><div id="id-1.6.15.4.6.4.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
     If the Compute node fails to reboot, you should troubleshoot this issue
     separately as this playbook will not attempt to recover after a failed
     reboot.
    </p></div></li></ol></div></div></div><div class="sect3" id="liveInstMigration"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Live Migration of Instances</span> <a title="Permalink" class="permalink" href="system-maintenance.html#liveInstMigration">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>liveInstMigration</li></ul></div></div></div></div><p>
  Live migration allows you to move active compute instances between compute
  nodes, allowing for less downtime during maintenance.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Nova offers a set of commands that allow you to move compute
  instances between compute hosts. Which command you use will depend on the
  state of the host, what operating system is on the host, what type of storage
  the instances are using, and whether you want to migrate a single instance or
  all of the instances off of the host. We will describe these options on this
  page as well as give you step-by-step instructions for performing them.
 </p><div class="sect4" id="options"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migration Options</span> <a title="Permalink" class="permalink" href="system-maintenance.html#options">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>options</li></ul></div></div></div></div><p>
   <span class="bold"><strong>If your compute node has failed</strong></span>
  </p><p>
   A compute host failure could be caused by hardware failure, such as the data
   disk needing to be replaced, power has been lost, or any other type of
   failure which requires that you replace the baremetal host. In this
   scenario, the instances on the compute node are unrecoverable and any data
   on the local ephemeral storage is lost. If you are utilizing block storage
   volumes, either as a boot device or as additional storage, they should be
   unaffected.
  </p><p>
   In these cases you will want to use one of the Nova evacuate commands, which
   will cause Nova to rebuild the instances on other hosts.
  </p><p>
   This table describes each of the evacuate options for failed compute nodes:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">nova evacuate &lt;instance&gt; &lt;hostname&gt;</code>
       </p>
      </td><td>
       <p>
        This command is used to evacuate a single instance from a failed host.
        You specify the compute instance UUID and the target host you want to
        evacuate it to. If no host is specified then the Nova scheduler will
        choose one for you.
       </p>
       <p>
        See <code class="literal">nova help evacuate</code> for more information and
        syntax. Further details can also be seen in the OpenStack documentation
        at
        <a class="link" href="http://docs.openstack.org/admin-guide/cli_nova_evacuate.html" target="_blank">http://docs.openstack.org/admin-guide/cli_nova_evacuate.html</a>.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate &lt;hostname&gt; --target_host
        &lt;target_hostname&gt;</code>
       </p>
      </td><td>
       <p>
        This command is used to evacuate all instances from a failed host. You
        specify the hostname of the compute host you want to evacuate.
        Optionally you can specify a target host. If no target host is
        specified then the Nova scheduler will choose a target host for each
        instance.
       </p>
       <p>
        See <code class="literal">nova help host-evacuate</code> for more information and
        syntax.
       </p>
      </td></tr></tbody></table></div><p>
   If your compute host is active, powered on and the data disks are in working order
   you can utilize the migration commands to migrate your compute instances.
   There are two migration features, "cold" migration (also referred to simply
   as "migration") and live migration. Migration and live migration are two
   different functions.
  </p><p>
   <span class="bold"><strong>Cold migration</strong></span> is used to copy an instances
   data in a <code class="literal">SHUTOFF</code> status from one compute host to
   another. It does this using passwordless SSH access which has security
   concerns associated with it. For this reason, the <code class="literal">nova
   migrate</code> function has been disabled by default but you have the
   ability to enable this feature if you would like. Details on how to do this
   can be found in <a class="xref" href="ops-managing-compute.html#enabling-the-nova-resize" title="5.4. Enabling the Nova Resize and Migrate Features">Section 5.4, “Enabling the Nova Resize and Migrate Features”</a>.
  </p><p>
   <span class="bold"><strong>Live migration</strong></span> can be performed on
   instances in either an <code class="literal">ACTIVE</code> or
   <code class="literal">PAUSED</code> state and uses the QEMU hypervisor to manage the
   copy of the running processes and associated resources to the destination
   compute host using the hypervisors own protocol and thus is a more secure
   method and allows for less downtime. There may be a short network outage,
   usually a few milliseconds but could be up to a few seconds if your compute
   instances are busy, during a live migration. Also there may be some
   performance degredation during the process.
  </p><p>
   The compute host must remain powered on during the migration process.
  </p><p>
   Both the cold migration and live migration options will honor Nova group
   policies, which includes affinity settings. There is a limitation to keep in
   mind if you use group policies and that is discussed in the
   <a class="xref" href="system-maintenance.html#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a> section.
  </p><p>
   This table describes each of the migration options for active compute nodes:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Command</th><th>Description</th><th>SLES</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">nova migrate &lt;instance_uuid&gt;</code>
       </p>
      </td><td>
       <p>
        Used to cold migrate a single instance from a compute host. The
        <code class="literal">nova-scheduler</code> will choose the new host.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td> </td></tr><tr><td>
       <p>
        <code class="literal">nova host-servers-migrate &lt;hostname&gt;</code>
       </p>
      </td><td>
       <p>
        Used to cold migrate all instances off a specified host to other
        available hosts, chosen by the <code class="literal">nova-scheduler</code>.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td> </td></tr><tr><td>
       <p>
        <code class="literal">nova live-migration &lt;instance_uuid&gt; [&lt;target
        host&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova live-migration --block-migrate &lt;instance_uuid&gt;
        [&lt;target host&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate-live &lt;hostname&gt; [--target-host
        &lt;target_hostname&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate-live --block-migrate &lt;hostname&gt;
        [--target-host &lt;target_hostname&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr></tbody></table></div></div><div class="sect4" id="idg-all-operations-maintenance-live-migration-xml-10"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations of these Features</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-live-migration-xml-10">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-live-migration-xml-10</li></ul></div></div></div></div><p>
   There are limitations that may impact your use of this feature:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     To use live migration, your compute instances must be in either an
     <code class="literal">ACTIVE</code> or <code class="literal">PAUSED</code> state on the
     compute host. If you have instances in a <code class="literal">SHUTOFF</code> state
     then cold migration should be used.
    </p></li><li class="listitem "><p>
     Instances in a <code class="literal">Paused</code> state cannot be live migrated
     using the Horizon dashboard. You will need to utilize the NovaClient CLI
     to perform these.
    </p></li><li class="listitem "><p>
     Both cold migration and live migration honor an instance's group policies.
     If you are utilizing an affinity policy and are migrating multiple
     instances you may run into an error stating no hosts are available to
     migrate to. To work around this issue you should specify a target host
     when migrating these instances, which will bypass the
     <code class="literal">nova-scheduler</code>. You should ensure that the target host
     you choose has the resources available to host the instances.
    </p></li><li class="listitem "><p>
     The <code class="literal">nova host-evacuate-live</code> command will produce an
     error if you have a compute host that has a mix of instances that use
     local ephemeral storage and instances that are booted from a block storage
     volume or have any number of block storage volumes attached. If you have a
     mix of these instance types, you may need to run the command twice,
     utilizing the <code class="literal">--block-migrate</code> option. This is described
     in further detail in <a class="xref" href="system-maintenance.html#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a>.
    </p></li><li class="listitem "><p>
     Instances on KVM hosts can only be live migrated to other KVM hosts.
    </p></li><li class="listitem "><p>
     The migration options described in this document are not available on ESX
     compute hosts.
    </p></li><li class="listitem "><p>
     Ensure that you read and take into account any other limitations that
     exist in the release notes. See the release notes for
     more details.
    </p></li></ul></div></div><div class="sect4" id="liveMigrate"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing a Live Migration</span> <a title="Permalink" class="permalink" href="system-maintenance.html#liveMigrate">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>liveMigrate</li></ul></div></div></div></div><p>
   Cloud administrators can perform a migration on an instance using either the
   Horizon dashboard, API, or CLI. Instances in a <code class="literal">Paused</code>
   state cannot be live migrated using the Horizon GUI. You will need to
   utilize the CLI to perform these.
  </p><p>
   We have documented different scenarios:
  </p></div><div class="sect4" id="failed-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating instances off of a failed compute host</span> <a title="Permalink" class="permalink" href="system-maintenance.html#failed-host">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>failed-host</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     If the compute node is not already powered off, do so with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.6.15.4.6.5.7.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The value for <code class="literal">&lt;node_name&gt;</code> will be the name that
      Cobbler has when you run <code class="command">sudo cobbler system list</code> from
      the Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     Source the admin credentials necessary to run administrative commands
     against the Nova API:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Force the <code class="literal">nova-compute</code> service to go down on the
     compute node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-force-down <em class="replaceable ">HOSTNAME</em> nova-compute</pre></div><div id="id-1.6.15.4.6.5.7.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The value for <em class="replaceable ">HOSTNAME</em> can be obtained by
      using <code class="command">nova host-list</code> from the Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     Evacuate the instances off of the failed compute node. This will cause the
     nova-scheduler to rebuild the instances on other valid hosts. Any local
     ephemeral data on the instances is lost.
    </p><p>
     For single instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova evacuate &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
     For all instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate &lt;hostname&gt; [--target_host &lt;target_hostname&gt;]</pre></div></li><li class="step "><p>
     When you have repaired the failed node and start it back up again, when
     the <code class="command">nova-compute</code> process starts again, it will clean
     up the evacuated instances.
    </p></li></ol></div></div></div><div class="sect4" id="active-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating instances off of an active compute host</span> <a title="Permalink" class="permalink" href="system-maintenance.html#active-host">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>active-host</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Migrating instances using the Horizon
   dashboard</strong></span>
  </p><p>
   The Horizon dashboard offers a GUI method for performing live migrations.
   Instances in a <code class="literal">Paused</code> state will not provide you the live
   migration option in Horizon so you will need to use the CLI instructions in
   the next section to perform these.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Horizon dashboard with admin credentials.
    </p></li><li class="step "><p>
     Navigate to the menu <span class="guimenu ">Admin</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Instances</span>.
    </p></li><li class="step "><p>
     Next to the instance you want to migrate, select the drop down menu and
     choose the <span class="guimenu ">Live Migrate Instance</span> option.
    </p></li><li class="step "><p>
     In the Live Migrate wizard you will see the compute host the instance
     currently resides on and then a drop down menu that allows you to choose
     the compute host you want to migrate the instance to. Select a destination
     host from that menu. You also have two checkboxes for additional options,
     which are described below:
    </p><p>
     <span class="guimenu ">Disk Over Commit</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will allow you to override the check that occurs to ensure the
     destination host has the available disk space to host the instance.
    </p><p>
     <span class="guimenu ">Block Migration</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will migrate the local disks by using block migration. Use this
     option if you are only using ephemeral storage on your instances. If you
     are using block storage for your instance then ensure this box is not
     checked.
    </p></li><li class="step "><p>
     To begin the live migration, click <span class="guimenu ">Submit</span>.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Migrating instances using the NovaClient
   CLI</strong></span>
  </p><p>
   To perform migrations from the command-line, use the NovaClient.
   The Cloud Lifecycle Manager node in your cloud environment should have
   the NovaClient already installed. If you will be accessing your environment
   through a different method, ensure that the NovaClient is
   installed. You can do so using Python's <code class="command">pip</code> package
   manager.
   
   
  </p><p>
   To run the commands in the steps below, you need administrator
   credentials. From the Cloud Lifecycle Manager, you can source the
   <code class="filename">service.osrc</code> file which is provided that has the
   necessary credentials:
  </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div><p>
   Here are the steps to perform:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Identify the instances on the compute node you wish to migrate:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --all-tenants --host &lt;hostname&gt;</pre></div><p>
     Example showing a host with a single compute instance on it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> nova list --host ardana-cp1-comp0001-mgmt --all-tenants
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| 553ba508-2d75-4513-b69a-f6a2a08d04e3 | test | 193548a949c146dfa1f051088e141f0b | ACTIVE | -          | Running     | adminnetwork=10.0.0.5 |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="step "><p>
     When using live migration you can either specify a target host that the
     instance will be migrated to or you can omit the target to allow the
     nova-scheduler to choose a node for you. If you want to get a list of
     available hosts you can use this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-list</pre></div></li><li class="step "><p>
     Migrate the instance(s) on the compute node using the notes below.
    </p><p>
     If your instance is booted from a block storage volume or has any number
     of block storage volumes attached, use the <code class="literal">nova
     live-migration</code> command with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only or if your instance
     has a mix of ephemeral disk(s) and block storage volume(s), you should use
     the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><div id="id-1.6.15.4.6.5.8.10.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">[&lt;target compute host&gt;]</code> option is
      optional. If you do not specify a target host then the nova scheduler
      will choose a node for you.
     </p></div><p>
     <span class="bold"><strong>Multiple instances</strong></span>
    </p><p>
     If you want to live migrate all of the instances off a single compute host
     you can utilize the <code class="literal">nova host-evacuate-live</code> command.
    </p><p>
     Issue the host-evacuate-live command, which will begin the live migration
     process.
    </p><p>
     If all of the instances on the host are using at least one local
     (ephemeral) disk, you should use this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live --block-migrate &lt;hostname&gt;</pre></div><p>
     Alternatively, if all of the instances are only using block storage
     volumes then omit the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt;</pre></div><div id="id-1.6.15.4.6.5.8.10.4.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can either let the nova-scheduler choose a suitable target host or
      you can specify one using the
      <code class="literal">--target-host &lt;hostname&gt;</code> switch. See
      <code class="command">nova help host-evacuate-live</code> for details.
     </p></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-live-migration-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting migration or host evacuate issues</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-live-migration-xml-14">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-live-migration-xml-14</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                                                                                        |
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 95a7ded8-ebfc-4848-9090-2df378c88a4c | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-9fd79670-a780-40ed-a515-c14e28e0a0a7)     |
| 13ab4ef7-0623-4d00-bb5a-5bb2f1214be4 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration cannot be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-26834267-c3ec-4f8b-83cc-5193d6a394d6)     |
+--------------------------------------+-------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from local storage and
   you are not specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live <span class="bold"><strong>--block-migrate</strong></span> &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live --block-migrate ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| e9874122-c5dc-406f-9039-217d9258c020 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-60b1196e-84a0-4b71-9e49-96d6f1358e1a)     |
| 84a02b42-9527-47ac-bed9-8fde1f98e3fe | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-0cdf1198-5dbd-40f4-9e0c-e94aa1065112)     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from a block storage
   volume and you are specifying <code class="literal">--block-migrate</code> in your
   command. Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration 2a13ffe6-e269-4d75-8e46-624fec7a5da0 ardana-cp1-comp0002-mgmt
ERROR (BadRequest): ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-158dd415-0bb7-4613-8529-6689265387e7)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from local storage and you are not
   specifying <code class="literal">--block-migrate</code> in your command. Re-attempt
   the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration <span class="bold"><strong>--block-migrate</strong></span> &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration --block-migrate 84a02b42-9527-47ac-bed9-8fde1f98e3fe ardana-cp1-comp0001-mgmt
ERROR (BadRequest): ardana-cp1-comp0002-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-51fee8d6-6561-4afc-b0c9-7afa7dc43a5b)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from a block storage volume and you
   are specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div></div></div><div class="sect3" id="adding-compute-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Compute Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#adding-compute-nodes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-adding_compute_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-adding_compute_nodes.xml</li><li><span class="ds-label">ID: </span>adding-compute-nodes</li></ul></div></div></div></div><p>
  Adding a Compute Node allows you to add capacity.
 </p><div class="sect4" id="add-sles-compute"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a SLES Compute Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#add-sles-compute">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>add-sles-compute</li></ul></div></div></div></div><p>
  Adding a SLES compute node allows you to add additional capacity for more
  virtual machines.
 </p><p>
  You may have a need to add additional SLES compute hosts for more virtual
  machine capacity or another purpose and these steps will help you achieve
  this.
 </p><p>
  There are two methods you can use to add SLES compute hosts to your
  environment:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Adding SLES pre-installed compute hosts. This method does not require the
    SLES ISO be on the Cloud Lifecycle Manager to complete.
   </p></li><li class="listitem "><p>
    Using the provided Ansible playbooks and Cobbler, SLES will be installed on
    your new compute hosts. This method requires that you provided a SUSE Linux Enterprise Server 12 SP3
    ISO during the initial installation of your cloud, following the
    instructions at <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”, Section 19.1 “SLES Compute Node Installation Overview”</span>.
   </p><p>
    If you want to use the provided Ansible playbooks and Cobbler to setup and
    configure your SLES hosts and you did not have the SUSE Linux Enterprise Server 12 SP3 ISO on your
    Cloud Lifecycle Manager during your initial installation then ensure you look at
    the note at the top of that section before proceeding.
   </p></li></ol></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.3.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-add-sles-compute-xml-5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-5</li></ul></div></div></div></div><p>
   You need to ensure your input model files are properly setup for SLES
   compute host clusters. This must be done during the installation process of
   your cloud and is discussed further at <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”, Section 19.3 “Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes”</span> and
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
  </p></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.3.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a SLES compute node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-add-sles-compute-xml-6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-6</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Adding pre-installed SLES compute hosts</strong></span>
  </p><p>
   This method requires that you have SUSE Linux Enterprise Server 12 SP3 pre-installed on the
   baremetal host prior to beginning these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure you have SUSE Linux Enterprise Server 12 SP3 pre-installed on your baremetal host.
    </p></li><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in the
     format. Note that we left out the IPMI details because they will not be
     needed since you pre-installed the SLES OS on your host(s).
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.6.15.4.6.6.3.7.4.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem "><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See for <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> more details.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation.
    </p><div id="id-1.6.15.4.6.6.3.7.4.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="filename">wipe_disks.yml</code> playbook is only meant to be run
      on systems immediately after running
      <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
      not wipe all of the expected partitions.
     </p></div><p>
     The location of <code class="literal">hostname</code> is
     <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts</code>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the compute host deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Adding SLES compute hosts with Ansible playbooks and
   Cobbler</strong></span>
  </p><p>
   These steps will show you how to add the new SLES compute host to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><p>
   If you did not have the SUSE Linux Enterprise Server 12 SP3 ISO available on your Cloud Lifecycle Manager
   during your initial installation, it must be installed before proceeding
   further. Instructions can be found in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”</span>.
  </p><p>
   When you are prepared to continue, use these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="step "><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in this
     format:
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1
  mac-addr: e8:39:35:21:32:4e
  ilo-ip: 10.1.192.36
  ilo-password: password
  ilo-user: admin
  distro-id: sles12sp3-x86_64</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.6.15.4.6.6.3.7.9.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="step "><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     The following playbook confirms that your servers are accessible over their IPMI ports.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml -e nodelist=compute4</pre></div></li><li class="step "><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
     Run the following playbook, ensuring that you specify only your UEFI
     SLES nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div><div id="id-1.6.15.4.6.6.3.7.9.10.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
       If you do not know the <code class="literal">&lt;node name&gt;</code>, you can
       get it by using <code class="command">sudo cobbler system list</code>.
      </p></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your hosts are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div><div id="id-1.6.15.4.6.6.3.7.9.12.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You can obtain the <code class="literal">&lt;hostname&gt;</code> from the file
      <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.
     </p></div></li><li class="step "><p>
     You should verify that the netmask, bootproto, and other necessary
     settings are correct and if they are not then re-do them. See
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 19 “Installing SLES Compute”</span> for details.
    </p></li><li class="step "><p>
     Complete the compute host deployment with these playbooks. For the last
     one, ensure you specify the compute hosts you are added with the
     <code class="literal">--limit</code> switch:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-8"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.3.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a new SLES compute node to monitoring</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-add-sles-compute-xml-8">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-8</li></ul></div></div></div></div><p>
   If you want to add a new Compute node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></div></div></div><div class="sect3" id="remove-compute-node"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Compute Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-compute-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-compute-node</li></ul></div></div></div></div><p>
  Removing a Compute node allows you to remove capacity.
 </p><p>
  You may have a need to remove a Compute node and these steps will help you
  achieve this.
 </p><div class="sect4" id="disable-provisioning"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable Provisioning on the Compute Host</span> <a title="Permalink" class="permalink" href="system-maintenance.html#disable-provisioning">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>disable-provisioning</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of the Nova services running which will provide us with the
     details we need to disable the provisioning on the Compute host you are
     wanting to remove:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -               |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -               |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -               |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | -               |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</pre></div></li><li class="step "><p>
     Disable the Nova service on the Compute node you are wanting to remove
     which will ensure it is taken out of the scheduling rotation:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "&lt;enter reason here&gt;" &lt;node hostname&gt; nova-compute</pre></div><p>
     Here is an example if I wanted to remove the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> in the output above:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-disable --reason "hardware reallocation" ardana-cp1-comp0002-mgmt nova-compute
+--------------------------+--------------+----------+-----------------------+
| Host                     | Binary       | Status   | Disabled Reason       |
+--------------------------+--------------+----------+-----------------------+
| ardana-cp1-comp0002-mgmt | nova-compute | disabled | hardware reallocation |
+--------------------------+--------------+----------+-----------------------+</pre></div></li></ol></div></div></div><div class="sect4" id="remove-az"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from its Availability Zone</span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-az">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-az</li></ul></div></div></div></div><p>
   If you configured the Compute host to be part of an availability zone, these
   steps will show you how to remove it.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of the Nova services running which will provide us with the
     details we need to remove a Compute node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+</pre></div></li><li class="step "><p>
     You can remove the Compute host from the availability zone it was a part
     of with this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova aggregate-remove-host &lt;availability zone&gt; &lt;nova hostname&gt;</pre></div><p>
     So for the same example as the previous step, the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> host was in the
     <code class="literal">AZ2</code> availability zone so I would use this command to
     remove it:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova aggregate-remove-host AZ2 ardana-cp1-comp0002-mgmt
Host ardana-cp1-comp0002-mgmt has been successfully removed from aggregate 4
+----+------+-------------------+-------+-------------------------+
| Id | Name | Availability Zone | Hosts | Metadata                |
+----+------+-------------------+-------+-------------------------+
| 4  | AZ2  | AZ2               |       | 'availability_zone=AZ2' |
+----+------+-------------------+-------+-------------------------+</pre></div></li><li class="step "><p>
     You can confirm the last two steps completed successfully by running
     another <code class="literal">nova service-list</code>.
    </p><p>
     Here is an example which confirms that the node has been disabled and that
     it has been removed from the availability zone. I have highlighted these:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div></li></ol></div></div></div><div class="sect4" id="live-migration"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Live Migration to Move Any Instances on this Host to Other Hosts</span> <a title="Permalink" class="permalink" href="system-maintenance.html#live-migration">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>live-migration</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     You will need to verify if the Compute node is currently hosting any
     instances on it. You can do this with the command below:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host=&lt;nova hostname&gt; --all_tenants=1</pre></div><p>
     Here is an example below which shows that we have a single running
     instance on this node currently:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | ACTIVE | -          | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+</pre></div></li><li class="step "><p>
     You will likely want to migrate this instance off of this node before
     removing it. You can do this with the live migration functionality within
     Nova. The command will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;nova instance ID&gt;</pre></div><p>
     Here is an example using the instance in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration --block-migrate 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9</pre></div><p>
     You can check the status of the migration using the same command from the
     previous step:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status    | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | MIGRATING | migrating  | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+</pre></div></li><li class="step "><p>
     Run nova list again
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1</pre></div><p>
     to see that the running instance has been migrated:
    </p><div class="verbatim-wrap"><pre class="screen">+----+------+-----------+--------+------------+-------------+----------+
| ID | Name | Tenant ID | Status | Task State | Power State | Networks |
+----+------+-----------+--------+------------+-------------+----------+
+----+------+-----------+--------+------------+-------------+----------+</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.4.6.7.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable Neutron Agents on Node to be Removed</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.6.7.7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You should also locate and disable or remove neutron agents. To see the
   neutron agents running:
  </p><div class="verbatim-wrap"><pre class="screen">$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-update --admin-state-down 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-update --admin-state-down dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-update --admin-state-down f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></div><div class="sect4" id="shutdown-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Shut down or Stop the Nova and Neutron Services on the Compute Host</span> <a title="Permalink" class="permalink" href="system-maintenance.html#shutdown-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>shutdown-node</li></ul></div></div></div></div><p>
   To perform this step you have a few options. You can SSH into the Compute
   host and run the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop nova-compute</pre></div><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop neutron-*</pre></div><p>
   Because the Neutron agent self-registers against Neutron server, you may
   want to prevent the following services from coming back online. Here is how
   you can get the list:
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl list-units neutron-* --all</pre></div><p>
   Here are the results:
  </p><div class="verbatim-wrap"><pre class="screen">UNIT                                  LOAD        ACTIVE     SUB      DESCRIPTION
neutron-common-rundir.service         loaded      inactive   dead     Create /var/run/neutron
•neutron-dhcp-agent.service         not-found     inactive   dead     neutron-dhcp-agent.service
neutron-l3-agent.service              loaded      inactive   dead     neutron-l3-agent Service
neutron-lbaasv2-agent.service         loaded      inactive   dead     neutron-lbaasv2-agent Service
neutron-metadata-agent.service        loaded      inactive   dead     neutron-metadata-agent Service
•neutron-openvswitch-agent.service    loaded      failed     failed   neutron-openvswitch-agent Service
neutron-ovs-cleanup.service           loaded      inactive   dead     Neutron OVS Cleanup Service

        LOAD   = Reflects whether the unit definition was properly loaded.
        ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
        SUB    = The low-level unit activation state, values depend on unit type.

        7 loaded units listed.
        To show all installed unit files use 'systemctl list-unit-files'.</pre></div><p>
   For each loaded service issue the command
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl disable &lt;service-name&gt;</pre></div><p>
   In the above example that would be each service, <span class="emphasis"><em>except neutron-dhcp-agent.service
   </em></span>
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl disable neutron-common-rundir neutron-l3-agent neutron-lbaasv2-agent neutron-metadata-agent neutron-openvswitch-agent</pre></div><p>
   Now you can shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen">sudo shutdown now</pre></div><p>
   OR
  </p><p>
   From the Cloud Lifecycle Manager you can use the
   <code class="literal">bm-power-down.yml</code> playbook to shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&lt;node name&gt;</pre></div><div id="id-1.6.15.4.6.7.8.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   The <code class="literal">&lt;node name&gt;</code> value will be the value
   corresponding to this node in Cobbler. You can run
   <code class="command">sudo cobbler system list</code> to retrieve these names.
  </p></div></div><div class="sect4" id="delete-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete the Compute Host from Nova</span> <a title="Permalink" class="permalink" href="system-maintenance.html#delete-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>delete-node</li></ul></div></div></div></div><p>
   Retrieve the list of Nova services:
  </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
   Here is an example highlighting the Compute host we're going to remove:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div><p>
   Delete the host from Nova using the command below:
  </p><div class="verbatim-wrap"><pre class="screen">nova service-delete &lt;service ID&gt;</pre></div><p>
   Following our example above, you would use:
  </p><div class="verbatim-wrap"><pre class="screen">nova service-delete 37</pre></div><p>
   Use the command below to confirm that the Compute host has been completely
   removed from Nova:
  </p><div class="verbatim-wrap"><pre class="screen">nova hypervisor-list</pre></div></div><div class="sect4" id="deletefromneutron"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete the Compute Host from Neutron</span> <a title="Permalink" class="permalink" href="system-maintenance.html#deletefromneutron">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>deletefromneutron</li></ul></div></div></div></div><p>
   Multiple Neutron agents are running on the compute node. You have to remove
   all of the agents running on the node using the "neutron agent-delete"
   command. In the example below, the l3-agent, openvswitch-agent and
   metadata-agent are running:
  </p><div class="verbatim-wrap"><pre class="screen">$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-delete AGENT_ID

$ neutron agent-delete 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-delete dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-delete f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></div><div class="sect4" id="remove-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from the servers.yml File and Run the Configuration Processor</span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-node</li></ul></div></div></div></div><p>
   Complete these steps from the Cloud Lifecycle Manager to remove the Compute node:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="step "><p>
     Edit your <code class="literal">servers.yml</code> file in the location below to
     remove references to the Compute node(s) you want to remove:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div></li><li class="step "><p>
     You may also need to edit your <code class="literal">control_plane.yml</code> file
     to update the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code> if you used
     those to ensure they reflect the proper number of nodes you are using.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "Remove node &lt;name&gt;"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     To free up the resources when running the configuration processor, use
     the switches <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code>. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div></div><div class="sect4" id="remove-cobbler"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from Cobbler</span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-cobbler">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-cobbler</li></ul></div></div></div></div><p>
   Complete these steps to remove the node from Cobbler:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Confirm the system name in Cobbler with this command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="step "><p>
     Remove the system from Cobbler using this command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name=&lt;node&gt;</pre></div></li><li class="step "><p>
     Run the <code class="literal">cobbler-deploy.yml</code> playbook to complete the
     process:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-compute-remove-compute-node-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.3.5.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from Monitoring</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-remove-compute-node-xml-14">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-remove-compute-node-xml-14</li></ul></div></div></div></div><p>
   Once you have removed the Compute nodes, the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
    To find all Monasca API servers 
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cat /etc/haproxy/haproxy.cfg | grep MON
listen ardana-cp1-vip-public-MON-API-extapi-8070
    bind ardana-cp1-vip-public-MON-API-extapi:8070  ssl crt /etc/ssl/private//my-public-cert-entry-scale                                          
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5        
listen ardana-cp1-vip-MON-API-mgmt-8070
    bind ardana-cp1-vip-MON-API-mgmt:8070  ssl crt /etc/ssl/private//ardana-internal-cert                                          
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5</pre></div><p>In above example <code class="literal">ardana-cp1-c1-m1-mgmt</code>,<code class="literal">ardana-cp1-c1-m2-mgmt</code>,
  <code class="literal">ardana-cp1-c1-m3-mgmt</code> are Monasa API servers</p><p>
   You will want to SSH to each of the Monasca API servers and edit the
   <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to remove
   references to the Compute node you removed. This will require
   <code class="literal">sudo</code> access. The entries will look similar to the one
   below:
  </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: ardana-cp1-comp0001-mgmt
  name: ardana-cp1-comp0001-mgmt ping</pre></div><p>
   Once you have removed the references on each of your Monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the Compute node references removed and the monasca-agent restarted,
   you can then delete the corresponding alarm to finish this process. To do so
   we recommend using the Monasca CLI which should be installed on each of your
   Monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=&lt;compute node deleted&gt;</pre></div><p>
   For example, if your Compute node looked like the example above then you
   would use this command to get the alarm ID:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div></div></div></div><div class="sect2" id="planned-maintenance-task-for-networking-nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Network Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-maintenance-task-for-networking-nodes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-networking_nodes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking_nodes.xml</li><li><span class="ds-label">ID: </span>planned-maintenance-task-for-networking-nodes</li></ul></div></div></div></div><p>
  Planned maintenance task for networking nodes.
 </p><div class="sect3" id="add-network-node"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Neutron Network Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#add-network-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>add-network-node</li></ul></div></div></div></div><p>
  Adding an additional Neutron networking node allows you to increase the
  performance of your cloud.
 </p><p>
  You may have a need to add an additional Neutron network node for increased
  performance or another purpose and these steps will help you achieve this.
 </p><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-networking-add-network-node-xml-6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-6</li></ul></div></div></div></div><p>
   If you are using the mid-scale model then your networking nodes are already
   separate and the roles are defined. If you are not already using this model
   and wish to add separate networking nodes then you need to ensure that those
   roles are defined. You can look in the <code class="literal">~/openstack/examples</code>
   folder on your Cloud Lifecycle Manager for the mid-scale example model files which
   show how to do this. We have also added the basic edits that need to be made
   below:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In your <code class="literal">server_roles.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/server_roles.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-ROLE
  interface-model: NEUTRON-INTERFACES
  disk-model: NEUTRON-DISKS</pre></div></li><li class="listitem "><p>
     In your <code class="literal">net_interfaces.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-INTERFACES</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/net_interfaces.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-INTERFACES
  network-interfaces:
  - device:
      name: hed3
    name: hed3
    network-groups:
    - EXTERNAL-VM
    - GUEST
    - MANAGEMENT</pre></div></li><li class="listitem "><p>
     Create a <code class="literal">disks_neutron.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-DISKS</code> defined in it.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_neutron.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  product:
    version: 2

  disk-models:
  - name: NEUTRON-DISKS
    volume-groups:
      - name: ardana-vg
        physical-volumes:
         - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></li><li class="listitem "><p>
     Modify your <code class="literal">control_plane.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined as well as the Neutron services
     added.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/control_plane.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  - allocation-policy: strict
    cluster-prefix: neut
    member-count: 1
    name: neut
    server-role: NEUTRON-ROLE
    service-components:
    - ntp-client
    - neutron-vpn-agent
    - neutron-dhcp-agent
    - neutron-metadata-agent
    - neutron-openvswitch-agent</pre></div></li></ol></div><p>
   You should also have one or more baremetal servers that meet the minimum
   hardware requirements for a network node which are documented in the
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span>.
  </p></div><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a network node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-networking-add-network-node-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-7</li></ul></div></div></div></div><p>
   These steps will show you how to add the new network node to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="listitem "><p>
     In the same directory, edit your <code class="literal">servers.yml</code> file to
     include the details about your new network node(s).
    </p><p>
     For example, if you already had a cluster of three network nodes and
     needed to add a fourth one you would add your details to the bottom of the
     file in this format:
    </p><div class="verbatim-wrap"><pre class="screen"># network nodes
- id: neut3
  ip-addr: 10.13.111.137
  role: NEUTRON-ROLE
  server-group: RACK2
  mac-addr: "5c:b9:01:89:b6:18"
  nic-mapping: HP-DL360-6PORT
  ip-addr: 10.243.140.22
  ilo-ip: 10.1.12.91
  ilo-password: password
  ilo-user: admin</pre></div><div id="id-1.6.15.4.7.3.5.3.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this node does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem "><p>
     In your <code class="literal">control_plane.yml</code> file you will need to check
     the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code>, if you
     specified them, to ensure that they match up with your new total node
     count. So for example, if you had previously specified
     <code class="literal">member-count: 3</code> and are adding a fourth network node,
     you will need to change that value to <code class="literal">member-count: 4</code>.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "Add new networking node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;hostname&gt;</pre></div><div id="id-1.6.15.4.7.3.5.3.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      If you do not know the <code class="literal">&lt;hostname&gt;</code>, you can
      get it by using <code class="command">sudo cobbler system list</code>.
     </p></div></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Configure the operating system on the new networking node with this
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the networking node deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">site.yml</code> playbook with the required tag so that
     all other services become aware of the new node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li></ol></div></div><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-8"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New Network Node to Monitoring</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-networking-add-network-node-xml-8">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-8</li></ul></div></div></div></div><p>
   If you want to add a new networking node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></div></div></div><div class="sect2" id="storage-maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Storage Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#storage-maintenance">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-storage_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-storage_maintenance.xml</li><li><span class="ds-label">ID: </span>storage-maintenance</li></ul></div></div></div></div><p>
  Planned maintenance procedures for Swift storage nodes.
 </p><div class="sect3" id="planned-maintenance-for-swift-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Maintenance Tasks for Swift Nodes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-maintenance-for-swift-nodes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift_nodes.xml</li><li><span class="ds-label">ID: </span>planned-maintenance-for-swift-nodes</li></ul></div></div></div></div><p>
  Planned maintenance tasks including recovering, adding, and removing Swift
  nodes.
 </p><div class="sect4" id="sec-swift-add-object-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Swift Object Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#sec-swift-add-object-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_object_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-add-object-node</li></ul></div></div></div></div><p>
  Adding additional object nodes allows you to increase capacity.
 </p><p>
  This topic describes how to add additional Swift object server nodes to an
  existing system.
 </p><div class="sect5" id="id-1.6.15.4.8.3.3.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To add a new node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.8.3.3.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_object_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="literal">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="literal">servers.yml
   file</code> by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager node.
    </p></li><li class="listitem "><p>
     Get the <code class="literal">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem "><p>
     If not already done, set the <code class="literal">weight-step</code> attribute. For
     instructions, see <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Add the details of new nodes to the <code class="literal">servers.yml</code> file.
     In the following example only one new server
     <span class="bold"><strong>swobj4</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="literal">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swobj4
  role: SWOBJ_ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div></li><li class="listitem "><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.6.15.4.8.3.3.4.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 18 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Configure Cobbler to include the new node, and then reimage the node (if
     you are adding several nodes, use a comma-separated list with the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swobj4</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj4</pre></div><div id="id-1.6.15.4.8.3.3.4.4.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">server-id</code>.
     </p></div></li><li class="listitem "><p>
     Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     The hostname of the newly added server can be found in the list generated
     from the output of the following command:
    </p><div class="verbatim-wrap"><pre class="screen">grep hostname ~/openstack/my_cloud/info/server_info.yml</pre></div><p>
     For example, for <span class="bold"><strong>swobj4</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-swobj0004-mgmt</strong></span>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-swobj0004-mgmt</pre></div></li><li class="listitem "><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem "><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swobj4</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem "><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml</pre></div></li></ol></div></div></div><div class="sect4" id="adding-proxy"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Swift Proxy, Account, Container (PAC) Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#adding-proxy">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_pac_node.xml</li><li><span class="ds-label">ID: </span>adding-proxy</li></ul></div></div></div></div><p>
  Steps for adding additional PAC nodes to your Swift system.
 </p><p>
  This topic describes how to add additional Swift proxy, account, and
  container (PAC) servers to an existing system.
 </p><div class="sect5" id="id-1.6.15.4.8.3.4.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a new node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.8.3.4.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_pac_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="literal">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="literal">servers.yml
   file</code> by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Get the <code class="literal">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem "><p>
     If not already done, set the weight-step attribute. For instructions, see
     <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Add details of new nodes to the <code class="literal">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swpac6
  role: SWPAC-ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div><p>
     In the above example, only one new server
     <span class="bold"><strong>swpac6</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="literal">servers.yml</code> file.
    </p><p>
     In the entry-scale configurations there is no dedicated Swift PAC cluster.
     Instead, there is a cluster using servers that have a role of
     <code class="literal">CONTROLLER-ROLE</code>. You cannot add
     <code class="literal">swpac4</code> to this cluster because that would change the
     <code class="literal">member-count</code>. If your system does not already have a
     dedicated Swift PAC cluster you will need to add it to the configuration
     files. For details on how to do this, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.7 “Creating a Swift Proxy, Account, and Container (PAC) Cluster”</span>.
    </p><p>
     If using a new PAC nodes you must add the PAC node's configuration details
     in the following yaml files:
    </p><div class="verbatim-wrap"><pre class="screen">control_plane.yml
disks_pac.yml
net_interfaces.yml
servers.yml
server_roles.yml</pre></div><p>
     You can see a good example of this in the example configurations for the
     mid-scale model in the <code class="literal">~/openstack/examples/mid-scale-kvm</code>
     directory.
    </p><p>
     The following steps assume that you have already created a dedicated Swift
     PAC cluster and that it has two members
     (<span class="bold"><strong>swpac4</strong></span> and
     <span class="bold"><strong>swpac5</strong></span>).
    </p></li><li class="listitem "><p>
     Increase the member count of the Swift PAC cluster, as appropriate. For
     example, if you are adding <span class="bold"><strong>swpac6</strong></span> and you
     previously had two Swift PAC nodes, the increased member count should be 3
     as shown in the following example:
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1

  . . .
  clusters:
  . . .
     - name: ....
       cluster-prefix: c2
       server-role: SWPAC-ROLE
       member-count: 3
   . . .</pre></div></li><li class="listitem "><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.6.15.4.8.3.4.4.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 18 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Configure Cobbler to include the new node and reimage the node (if you are
     adding several nodes, use a comma-separated list for the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swpac6</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swpac6</pre></div><div id="id-1.6.15.4.8.3.4.4.4.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">server-id</code>.
     </p></div></li><li class="listitem "><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     For example, for <span class="bold"><strong>swpac6</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-c2-m3-mgmt</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-c2-m3-mgmt</pre></div></li><li class="listitem "><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem "><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swpac6</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem "><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div></div><div class="sect4" id="add-swift-disk"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Additional Disks to a Swift Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#add-swift-disk">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_disk.xml</li><li><span class="ds-label">ID: </span>add-swift-disk</li></ul></div></div></div></div><p>
  Steps for adding additional disks to any nodes hosting Swift services.
 </p><p>
  You may have a need to add additional disks to a node for Swift usage and we
  can show you how. These steps work for adding additional disks to Swift
  object or proxy, account, container (PAC) nodes. It can also apply to adding
  additional disks to a controller node that is hosting the Swift service, like
  you would see if you are using one of the entry-scale example models.
 </p><p>
  Read through the notes below before beginning the process.
 </p><p>
  You can add multiple disks at the same time, there is no need to do it one at
  a time.
 </p><div id="id-1.6.15.4.8.3.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important: Add the Same Number of Disks</h6><p>
   You must add the <span class="emphasis"><em>same</em></span> number of disks to each server
   that the disk model applies to. For example, if you have a single cluster of
   three Swift servers and you want to increase capacity and decide to add two
   additional disks, you must add two to each of your three Swift servers.
  </p></div><div class="sect5" id="id-1.6.15.4.8.3.5.7"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding additional disks to your Swift servers</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.8.3.5.7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_disk.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Verify the general health of the Swift system and that it is safe to
     rebalance your rings. See <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details
     on how to do this.
    </p></li><li class="step "><p>
     Perform the disk maintenance.
    </p><ol type="a" class="substeps "><li class="step " id="st-swift-add-disk-shutdown"><p>
       Shut down the first Swift server you wish to add disks to.
      </p></li><li class="step "><p>
       Add the additional disks to the physical server. The disk drives that
       are added should be clean. They should either contain no partitions or a
       single partition the size of the entire disk. It should not contain a
       file system or any volume groups. Failure to comply will cause errors
       and the disk will not be added.
      </p><p>
       For more details, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.
      </p></li><li class="step "><p>
       Power the server on.
      </p></li><li class="step "><p>
       While the server was shutdown, data that normally would have been placed
       on the server is placed elsewhere. When the server is rebooted, the
       Swift replication process will move that data back onto the server.
       Monitor the replication process to determine when it is complete. See
       <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details on how to do this.
      </p></li><li class="step "><p>
       Repeat the steps from <a class="xref" href="system-maintenance.html#st-swift-add-disk-shutdown" title="Step 2.a">Step 2.a</a> for
       each of the Swift servers you are adding the disks to, one at a time.
      </p><div id="id-1.6.15.4.8.3.5.7.2.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         If the additional disks can be added to the Swift servers online
         (for example, via hotplugging) then there is no need to perform the
         last two steps.
         
        </p></div></li></ol></li><li class="step "><p>
     On the Cloud Lifecycle Manager, update your cloud configuration with the details
     of your additional disks.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Edit the disk configuration file that correlates to the type of server
       you are adding your new disks to.
      </p><p>
       Path to the typical disk configuration files:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_swobj.yml
~/openstack/my_cloud/definition/data/disks_swpac.yml
~/openstack/my_cloud/definition/data/disks_controller_*.yml</pre></div><p>
       Example showing the addition of a single new disk, indicated by the
       <code class="literal">/dev/sdd</code>, in bold:
      </p><div class="verbatim-wrap"><pre class="screen">device-groups:
  - name: SwiftObject
    devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"
      <span class="bold"><strong>- name: "/dev/sdd"</strong></span>
    consumer:
      name: swift
      ...</pre></div><div id="id-1.6.15.4.8.3.5.7.2.3.2.1.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         For more details on how the disk model works, see
         <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”</span>.
        </p></div></li><li class="step "><p>
       Configure the Swift weight-step value in the
       <code class="literal">~/openstack/my_cloud/definition/data/swift/rings.yml</code>
       file. See <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for details on how to do
       this.
      </p></li><li class="step "><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "adding additional Swift disks"</pre></div></li><li class="step "><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step "><p>
     Run the <code class="literal">osconfig-run.yml</code> playbook against the Swift
     nodes you have added disks to. Use the <code class="literal">--limit</code> switch
     to target the specific nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostnames&gt;</pre></div><p>
     You can use a wildcard when specifying the hostnames with the
     <code class="literal">--limit</code> switch. If you added disks to all of the Swift
     servers in your environment and they all have the same prefix (for
     example, <code class="literal">ardana-cp1-swobj...</code>) then you can use a
     wildcard like <code class="literal">ardana-cp1-swobj*</code>. If you only added
     disks to a set of nodes but not all of them, you can use a comma
     deliminated list and enter the hostnames of each of the nodes you added
     disks to.
    </p></li><li class="step "><p>
     Validate your Swift configuration with this playbook which will also
     provide details of each drive being added:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div></li><li class="step "><p>
     Verify that Swift services are running on all of your servers:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li><li class="step "><p>
     If everything looks okay with the Swift status, then apply the changes to
     your Swift rings with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="step "><p>
     At this point your Swift rings will begin rebalancing. You should wait
     until replication has completed or min-part-hours has elapsed (whichever
     is longer), as described in <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> and then
     follow the "Weight Change Phase of Ring Rebalance" process as described in
     <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div></div></div><div class="sect4" id="remove-swift-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Swift Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-swift-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>remove-swift-node</li></ul></div></div></div></div><p>
  Removal process for both Swift Object and PAC nodes.
 </p><p>
  You can use this process when you want to remove one or more Swift nodes
  permanently. This process applies to both Swift Proxy, Account, Container
  (PAC) nodes and Swift Object nodes.
 </p><div class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting the Pass-through Attributes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-swift-removing-swift-node-xml-6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-removing-swift-node-xml-6</li></ul></div></div></div></div><p>
   This process will remove the Swift node's drives from the rings and move it
   to the remaining nodes in your cluster.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Ensure that the weight-step attribute is set. See
     <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="8.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 8.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for more details.
    </p></li><li class="listitem "><p>
     Add the pass-through definition to your input model, specifying the server
     ID (as opposed to the server name). It is easiest to include in your
     <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file
     since your server IDs are already listed in that file. For more
     information about pass-through, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.17 “Pass Through”</span>.
    </p><p>
     Here is the general format:
    </p><div class="verbatim-wrap"><pre class="screen">pass-through:
  servers:
    - id: &lt;server-id&gt;
      data:
          &lt;subsystem&gt;:
                &lt;subsystem-attributes&gt;</pre></div><p>
     Here is an example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  <span class="bold"><strong>pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            drain: yes</strong></span></pre></div><p>
     By setting this pass-through attribute, you indicate that the system
     should reduce the weight of the server's drives. The weight reduction is
     determined by the weight-step attribute as described in the previous step.
     This process is known as "draining", where you remove the Swift data from
     the node in preparation for removing the node.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Swift deploy playbook to perform the first ring rebuild. This will
     remove some of the partitions from all drives on the node you are
     removing:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until the replication has completed. For further details, see
     <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem "><p>
     Determine whether all of the partitions have been removed from all drives
     on the Swift node you are removing. You can do this by SSH'ing into the
     first account server node and using these commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/swiftlm/cloud1/cp1/builder_dir/
sudo swift-ring-builder &lt;ring_name&gt;.builder</pre></div><p>
     For example, if the node you are removing was part of the object-o ring
     the command would be:
    </p><div class="verbatim-wrap"><pre class="screen">sudo swift-ring-builder object-0.builder</pre></div><p>
     Check the output. You will need to know the IP address of the server being
     drained. In the example below, the number of partitions of the drives on
     192.168.245.3 has reached zero for the object-0 ring:
    </p><div class="verbatim-wrap"><pre class="screen">$ cd /etc/swiftlm/cloud1/cp1/builder_dir/
$ sudo swift-ring-builder object-0.builder
account.builder, build version 6
4096 partitions, 3.000000 replicas, 1 regions, 1 zones, 6 devices, 0.00 balance, 0.00 dispersion
The minimum number of hours before a partition can be reassigned is 16
The overload factor is 0.00% (0.000000)
Devices:    id  region  zone      ip address  port  replication ip  replication port      name weight partitions balance meta
             0       1     1   192.168.245.3  6002   192.168.245.3              6002     disk0   0.00          0   -0.00 padawan-ccp-c1-m1:disk0:/dev/sdc
             1       1     1   192.168.245.3  6002   192.168.245.3              6002     disk1   0.00          0   -0.00 padawan-ccp-c1-m1:disk1:/dev/sdd
             2       1     1   192.168.245.4  6002   192.168.245.4              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m2:disk0:/dev/sdc
             3       1     1   192.168.245.4  6002   192.168.245.4              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m2:disk1:/dev/sdd
             4       1     1   192.168.245.5  6002   192.168.245.5              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m3:disk0:/dev/sdc
             5       1     1   192.168.245.5  6002   192.168.245.5              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m3:disk1:/dev/sdd</pre></div></li><li class="listitem "><p>
     If the number of partitions is zero for the server on all rings, you can
     move to the next step, otherwise continue the ring rebalance cycle by
     repeating steps 7-9 until the weight has reached zero.
    </p></li><li class="listitem "><p>
     If the number of partitions is zero for the server on all rings, you can
     remove the Swift nodes' drives from all rings. Edit the pass-through data
     you created in step #3 and set the <code class="literal">remove</code> attribute as
     shown in this example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            <span class="bold"><strong>remove: yes</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Swift deploy playbook to rebuild the rings by removing the server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     At this stage, the server has been removed from the rings and the data
     that was originally stored on the server has been replicated in a balanced
     way to the other servers in the system. You can proceed to the next phase.
    </p></li></ol></div></div><div class="sect5" id="sec-swift-disable-node"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Disable Swift on a Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#sec-swift-disable-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-disable-node</li></ul></div></div></div></div><p>
   The next phase in this process will disable the Swift service on the node.
   In this example, <span class="bold"><strong>swobj4</strong></span> is the node being
   removed from Swift.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Stop Swift services on the node using the
     <code class="literal">swift-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong>&lt;hostname&gt;</strong></span></pre></div><div id="id-1.6.15.4.8.3.6.5.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      When using the <code class="literal">--limit</code> argument, you must specify the
      full hostname (for example: <span class="emphasis"><em>ardana-cp1-swobj0004</em></span>) or
      use the wild card <code class="literal">*</code> (for example,
      <span class="emphasis"><em>*swobj4*</em></span>).
     </p></div><p>
     The following example uses the <code class="literal">swift-stop.yml</code> playbook
     to stop Swift services on
     <span class="bold"><strong>ardana-cp1-swobj0004</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong>ardana-cp1-swobj0004</strong></span></pre></div></li><li class="listitem "><p>
     Remove the configuration files.
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-swobj4-mgmt sudo rm -R /etc/swift</pre></div><div id="id-1.6.15.4.8.3.6.5.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      Do not run any other playbooks until you have finished the process
      described in <a class="xref" href="system-maintenance.html#sec-swift-remove-node-model" title="13.1.5.1.4.3. To Remove a Node from the Input Model">Section 13.1.5.1.4.3, “To Remove a Node from the Input Model”</a>. Otherwise,
      these playbooks may recreate <code class="filename">/etc/swift</code> and
      restart Swift on <span class="emphasis"><em>swobj4</em></span>. If you accidentally run a
      playbook, repeat the process in <a class="xref" href="system-maintenance.html#sec-swift-disable-node" title="13.1.5.1.4.2. To Disable Swift on a Node">Section 13.1.5.1.4.2, “To Disable Swift on a Node”</a>.
     </p></div></li></ol></div></div><div class="sect5" id="sec-swift-remove-node-model"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Remove a Node from the Input Model</span> <a title="Permalink" class="permalink" href="system-maintenance.html#sec-swift-remove-node-model">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-remove-node-model</li></ul></div></div></div></div><p>
   Use the following steps to finish the process of removing the Swift node.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file and remove the entry for the node
     (<span class="bold"><strong>swobj4</strong></span> in this example).
    </p></li><li class="listitem "><p>
     If this was a SWPAC node, reduce the member-count attribute by 1 in the
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file. For SWOBJ nodes, no such action is needed.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     You may want to use the <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> switches to free up the resources
     when running the configuration processor. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Validate the changes you have made to the configuration files using the
     playbook below before proceeding further:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them in your configuration files and repeat
     steps 3-5 again until no more errors occur before going to the next step.
    </p><p>
     For more details on how to interpret and resolve errors, see
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-input-swift-error" title="15.6.2.3. Interpreting Swift Input Model Validation Errors">Section 15.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>
    </p></li><li class="listitem "><p>
     Remove the node from Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name=swobj4</pre></div></li><li class="listitem "><p>
     Run the Cobbler deploy playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     The final step will depend on what type of Swift node you are removing.
    </p><p>
     If the node was a SWPAC node, run the <code class="literal">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div><p>
     If the node was a SWOBJ node, run the <code class="literal">swift-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished. For more details, see
     <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="8.5.4. Determining When to Rebalance and Deploy a New Ring">Section 8.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
    </p></li><li class="listitem "><p>
     You may need to continue to rebalance the rings. For instructions, see
     <span class="bold"><strong>Final Rebalance Phase </strong></span> at
     <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="8.5.5. Applying Input Model Changes to Existing Rings">Section 8.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div><div class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-9"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Swift Node from Monitoring</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-swift-removing-swift-node-xml-9">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-removing-swift-node-xml-9</li></ul></div></div></div></div><p>
   Once you have removed the Swift node(s), the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
   You will want to SSH to each of the Monasca API servers and edit the
   <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to remove
   references to the Swift node(s) you removed. This will require
   <code class="literal">sudo</code> access.
  </p><p>
   Once you have removed the references on each of your Monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the Swift node references removed and the monasca-agent restarted, you
   can then delete the corresponding alarm to finish this process. To do so we
   recommend using the Monasca CLI which should be installed on each of your
   Monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=&lt;swift node deleted&gt;</pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div></div></div><div class="sect4" id="replace-swift-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Swift Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace-swift-node">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_swift_node.xml</li><li><span class="ds-label">ID: </span>replace-swift-node</li></ul></div></div></div></div><p>
  Maintenance steps for replacing a failed Swift node in your environment.
 </p><p>
  This process is used when you want to replace a failed Swift node in your
  cloud.
 </p><div id="id-1.6.15.4.8.3.7.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   If it applies to the server, do not skip step 10. If you do, the system will
   overwrite the existing rings with new rings. This will not cause data loss,
   but, potentially, will move most objects in your system to new locations and
   may make data unavailable until the replication process has completed.
  </p></div><div class="sect5" id="id-1.6.15.4.8.3.7.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace a Swift node in your environment</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.8.3.7.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update your cloud configuration with the details of your replacement Swift
     node.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Edit your <code class="literal">servers.yml</code> file to include the details
       (MAC address, IPMI user, password, and IP address (IPME) if these
       have changed) about your replacement Swift node.
      </p><div id="id-1.6.15.4.8.3.7.5.2.2.2.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        Do not change the server's IP address (that is,
        <code class="literal">ip-addr</code>).
       </p></div><p>
       Path to file:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div><p>
       Example showing the fields to edit, in bold:
      </p><div class="verbatim-wrap"><pre class="screen"> - id: swobj5
   role: SWOBJ-ROLE
   server-group: rack2
   <span class="bold"><strong>mac-addr: 8c:dc:d4:b5:cb:bd</strong></span>
   nic-mapping: HP-DL360-6PORT
   ip-addr: 10.243.131.10
   <span class="bold"><strong>ilo-ip: 10.1.12.88
   ilo-user: iLOuser
   ilo-password: iLOpass</strong></span>
   ...</pre></div></li><li class="listitem "><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "replacing a Swift node"</pre></div></li><li class="listitem "><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></li><li class="listitem "><p>
     Update Cobbler and reimage your replacement Swift node:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Obtain the name in Cobbler for your node you wish to remove. You will
       use this value to replace <code class="literal">&lt;node name&gt;</code> in future
       steps.
      </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
       Remove the replaced Swift node from Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name &lt;node name&gt;</pre></div></li><li class="listitem "><p>
       Re-run the <code class="literal">cobbler-deploy.yml</code> playbook to add the
       replaced node:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
       Reimage the node using this playbook:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div></li><li class="listitem "><p>
     Complete the deployment of your replacement Swift node.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Obtain the hostname for your new Swift node. You will use this value to
       replace <code class="literal">&lt;hostname&gt;</code> in future steps.
      </p><div class="verbatim-wrap"><pre class="screen">cat ~/openstack/my_cloud/info/server_info.yml</pre></div></li><li class="listitem "><p>
       Configure the operating system on your replacement Swift node:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
       If this is the Swift ring builder server, restore the Swift ring builder
       files to the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For
       more information and instructions, see
       <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a> and
       <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
      </p></li><li class="listitem "><p>
       Configure services on the node using the
       <code class="literal">ardana-deploy.yml</code> playbook. If you have used an
       encryption password when running the configuration processor, include
       the <code class="literal">--ask-vault-pass</code> argument.
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit &lt;hostname&gt;</pre></div></li></ol></div></li></ol></div></div></div><div class="sect4" id="replacing-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.5.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing Drives in a Swift Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#replacing-disks">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span>replacing-disks</li></ul></div></div></div></div><p>
  Maintenance steps for replacing drives in a Swift node.
 </p><p>
  This process is used when you want to remove a failed hard drive
  from Swift node and replace it with a new one.
 </p><p>
  There are two different classes of drives in a Swift node that needs to be
  replaced; the operating system disk drive (generally
  <span class="bold"><strong>/dev/sda</strong></span>) and storage disk drives. There are
  different procedures for the replacement of each class of drive to bring the
  node back to normal.
 </p><div class="sect5" id="id-1.6.15.4.8.3.8.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Replace the Operating System Disk Drive</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.8.3.8.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After the operating system disk drive is replaced, the node must be
   reimaged.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update your Cobbler profile:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     Reimage the node using this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server name&gt;</pre></div><p>
     In the example below <span class="bold"><strong>swobj2</strong></span> server is
     reimaged:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj2</pre></div></li><li class="listitem "><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     In the following example, for <span class="bold"><strong>swobj2</strong></span>, the
     hostname is <span class="bold"><strong>ardana-cp1-swobj0002</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit ardana-cp1-swobj0002*</pre></div></li><li class="listitem "><p>
     If this is the first server running the swift-proxy service, restore the
     Swift Ring Builder files to the
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For more
     information and instructions, see <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a> and
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
    </p></li><li class="listitem "><p>
     Configure services on the node using the <code class="literal">ardana-deploy.yml</code>
     playbook. If you have used an encryption password when running the
     configuration processor include the <code class="literal">--ask-vault-pass</code>
     argument.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass \
  --limit &lt;hostname&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit ardana-cp1-swobj0002*</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.4.8.3.8.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.5.1.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Replace a Storage Disk Drive</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.4.8.3.8.6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After a storage drive is replaced, there is no need to reimage the server.
   Instead, run the <code class="literal">swift-reconfigure.yml</code> playbook.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log onto the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit &lt;hostname&gt;</pre></div><p>
     In following example, the server used is
     <span class="bold"><strong>swobj2</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit ardana-cp1-swobj0002-mgmt</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="mariadb-manual-update"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating MariaDB with Galera</span> <a title="Permalink" class="permalink" href="system-maintenance.html#mariadb-manual-update">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-mariadb-manual-update.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-mariadb-manual-update.xml</li><li><span class="ds-label">ID: </span>mariadb-manual-update</li></ul></div></div></div></div><p>
   Updating MariaDB with Galera must be done manually. Updates are not
   installed automatically. In particular, this situation applies to upgrades
   to MariaDB 10.2.17 or higher from MariaDB 10.2.16 or earlier. See <a class="link" href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/" target="_blank">MariaDB
   10.2.22 Release Notes - Notable Changes</a>.
  </p><p>
   Using the CLI, update MariaDB with the following procedure:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Mark Galera as unmanaged:
    </p><div class="verbatim-wrap"><pre class="screen">crm resource unmanage galera</pre></div><p>
     Or put the whole cluster into maintenance mode:
    </p><div class="verbatim-wrap"><pre class="screen">crm configure property maintenance-mode=true</pre></div></li><li class="step "><p>
     Pick a node other than the one currently targeted by the loadbalancer and
     stop MariaDB on that node:
    </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-demote -r galera -V</pre></div></li><li class="step "><p>
     Perform updates:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Uninstall the old versions of MariaDB and the Galera wsrep provider.
      </p></li><li class="step "><p>
       Install the new versions of MariaDB and the Galera wsrep
       provider. Select the appropriate instructions at <a class="link" href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/" target="_blank">Installing
       MariaDB with zypper</a>.
      </p></li><li class="step "><p>
       Change configuration options if necessary.
      </p></li></ol></li><li class="step "><p>
       Start MariaDB on the node.
      </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-promote -r galera -V</pre></div></li><li class="step "><p>
       Run <code class="command">mysql_upgrade</code> with the
       <code class="literal">--skip-write-binlog</code> option.
      </p></li><li class="step "><p>
       On the other nodes, repeat the process detailed above: stop MariaDB,
       perform updates, start MariaDB, run <code class="command">mysql_upgrade</code>.
      </p></li><li class="step "><p>
       Mark Galera as managed:
      </p><div class="verbatim-wrap"><pre class="screen">crm resource manage galera</pre></div><p>
       Or take the cluster out of maintenance mode.
      </p></li></ol></div></div></div></div><div class="sect1" id="unplanned-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned System Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#unplanned-maintenance">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-unplanned_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-unplanned_maintenance.xml</li><li><span class="ds-label">ID: </span>unplanned-maintenance</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for your cloud.
 </p><div class="sect2" id="whole-unplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Whole Cloud Recovery Procedures</span> <a title="Permalink" class="permalink" href="system-maintenance.html#whole-unplanned">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-whole_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-whole_unplanned.xml</li><li><span class="ds-label">ID: </span>whole-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance procedures for your whole cloud.
 </p><div class="sect3" id="full-recovery"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Full Disaster Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#full-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span>full-recovery</li></ul></div></div></div></div><p>
  In this disaster scenario, you have lost everything in the cloud, including
  Swift.
 </p><div class="sect4" id="id-1.6.15.5.3.3.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup:</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.3.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Restoring from a Swift backup is not possible because Swift is gone.
  </p></div><div class="sect4" id="id-1.6.15.5.3.3.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup:</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.3.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file so it contains the same information as it had
     previously:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div></li><li class="listitem "><p>
     On the Cloud Lifecycle Manager copy the following files:
    </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/hp-ci/openstack/* ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     Run this playbook to restore the Cloud Lifecycle Manager helper:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/
ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="listitem "><p>
     Run as root, and change directories:
    </p><div class="verbatim-wrap"><pre class="screen">sudo su
cd /root/deployer_restore_helper/</pre></div></li><li class="listitem "><p>
     Execute the restore:
    </p><div class="verbatim-wrap"><pre class="screen">./deployer_restore_script.sh</pre></div></li><li class="listitem "><p>
     Run this playbook to deploy your cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml -e '{ "freezer_backup_jobs_upload": false }'</pre></div></li><li class="listitem "><p>
     You can now perform the procedures to restore MySQL and Swift. Once
     everything is restored, re-enable the backups from the Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div><div class="sect3" id="full-recovery-test"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Full Disaster Recovery Test</span> <a title="Permalink" class="permalink" href="system-maintenance.html#full-recovery-test">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span>full-recovery-test</li></ul></div></div></div></div><p>
  Full Disaster Recovery Test
 </p><div class="sect4" id="id-1.6.15.5.3.4.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> platform</p><p>An external server to store backups to via SSH</p></div><div class="sect4" id="id-1.6.15.5.3.4.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Goals</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Here is a high level view of how we expect to test the disaster recovery of the platform.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Backup the control plane using Freezer to an SSH target</p></li><li class="listitem "><p>Backup the Cassandra Database</p></li><li class="listitem "><p>Re-install Controller 1 with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO</p></li><li class="listitem "><p>Use Freezer to recover deployment data (model …)</p></li><li class="listitem "><p>Re-install <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> on Controller 1, 2, 3</p></li><li class="listitem "><p>Recover the Cassandra Database</p></li><li class="listitem "><p>Recover the backup of the MariaDB database</p></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Description of the testing environment</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>The testing environment is very similar to the Entry Scale model.</p><p>It used 5 servers: 3 Controllers and 2 computes.</p><p>The controller node have three disks. The first one is reserved for the system, while others are used for swift.</p><div id="id-1.6.15.5.3.4.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>During this Disaster Recovery exercise, we have saved the data on disk 2 and 3 of the swift controllers.</p><p>This allow to restore the swift objects after the recovery.</p><p>If these disks were to be wiped as well, swift data would be lost but the procedure would not change.</p><p>The only difference is that Glance images would be lost and they will have to be re-uploaded.</p></div></div><div class="sect4" id="id-1.6.15.5.3.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disaster recovery test note</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>If it is not specified otherwise, all the commands should be executed on controller 1, which is also the deployer node.</p></div><div class="sect4" id="id-1.6.15.5.3.4.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Disaster testing</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In order to validate the procedure after recovery, we need to create some workloads.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
         Source the service credential file
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
         Copy an image to the platform and create a Glance image with it.
         In this example, Cirros is used
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image create --disk-format raw --container-format bare --public --file ~/cirros-0.3.5-x86_64-disk.img cirros</pre></div></li><li class="listitem "><p>Create a network</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create test_net</pre></div></li><li class="listitem "><p>Create a subnet</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron subnet-create 07c35d11-13f9-41d4-8289-fa92147b1d44 192.168.42.0/24 --name test_subnet</pre></div></li><li class="listitem "><p>Create some instances</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create server_1 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_2 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_3 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_4 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_5 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server list</pre></div></li><li class="listitem "><p>Create containers and objects</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift upload container_1 ~/service.osrc
var/lib/ardana/service.osrc

<code class="prompt user">ardana &gt; </code>swift upload container_1 ~/backup.osrc
swift upload container_1 ~/backup.osrc

<code class="prompt user">ardana &gt; </code>swift list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation of the backup server</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.8">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Preparation of the backup server</p><div class="sect5" id="id-1.6.15.5.3.4.8.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation to store Freezer backups</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.8.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In this example, we want to store the backups on the server 192.168.69.132</p><p>Freezer will connect with the user backupuser on port 22 and store the backups in the <code class="filename">/mnt/backups/</code> directory.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Connect to the backup server</p></li><li class="listitem "><p>Create the user</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>useradd backupuser --create-home --home-dir /mnt/backups/</pre></div></li><li class="listitem "><p>Switch to that user</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>su backupuser</pre></div></li><li class="listitem "><p>Create the SSH keypair</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ssh-keygen -t rsa
&gt; # Just leave the default for the first question and do not set any passphrase
&gt; Generating public/private rsa key pair.
&gt; Enter file in which to save the key (/mnt/backups//.ssh/id_rsa):
&gt; Created directory '/mnt/backups//.ssh'.
&gt; Enter passphrase (empty for no passphrase):
&gt; Enter same passphrase again:
&gt; Your identification has been saved in /mnt/backups//.ssh/id_rsa
&gt; Your public key has been saved in /mnt/backups//.ssh/id_rsa.pub
&gt; The key fingerprint is:
&gt; a9:08:ae:ee:3c:57:62:31:d2:52:77:a7:4e:37:d1:28 backupuser@padawan-ccp-c0-m1-mgmt
&gt; The key's randomart image is:
&gt; +---[RSA 2048]----+
&gt; |          o      |
&gt; |   . . E + .     |
&gt; |  o . . + .      |
&gt; | o +   o +       |
&gt; |  + o o S .      |
&gt; | . + o o         |
&gt; |  o + .          |
&gt; |.o .             |
&gt; |++o              |
&gt; +-----------------+</pre></div></li><li class="listitem "><p>Add the public key to the list of the keys authorized to connect to that user on this server</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa.pub &gt;&gt; /mnt/backups/.ssh/authorized_keys</pre></div></li><li class="listitem "><p>Print the private key. This is what we will use for the backup configuration (ssh_credentials.yml file)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa

&gt; -----BEGIN RSA PRIVATE KEY-----
&gt; MIIEogIBAAKCAQEAvjwKu6f940IVGHpUj3ffl3eKXACgVr3L5s9UJnb15+zV3K5L
&gt; BZuor8MLvwtskSkgdXNrpPZhNCsWSkryJff5I335Jhr/e5o03Yy+RqIMrJAIa0X5
&gt; ...
&gt; ...
&gt; ...
&gt; iBKVKGPhOnn4ve3dDqy3q7fS5sivTqCrpaYtByJmPrcJNjb2K7VMLNvgLamK/AbL
&gt; qpSTZjicKZCCl+J2+8lrKAaDWqWtIjSUs29kCL78QmaPOgEvfsw=
&gt; -----END RSA PRIVATE KEY-----</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.8.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation to store Cassandra backups</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.8.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>In this example, we want to store the backups on the server 192.168.69.132. We will store the backups in the <code class="filename">/mnt/backups/cassandra_backups/</code> directory.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Create a directory on the backup server to store cassandra backups</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>mkdir /mnt/backups/cassandra_backups</pre></div></li><li class="listitem "><p>Copy private ssh key from backupserver to all controller nodes</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>scp /mnt/backups/.ssh/id_rsa ardana@<em class="replaceable ">CONTROLLER</em>:~/.ssh/id_rsa_backup
         Password:
         id_rsa     100% 1675     1.6KB/s   00:00</pre></div><p><em class="replaceable ">Replace CONTROLLER with each control node e.g. doc-cp1-c1-m1-mgmt, doc-cp1-c1-m2-mgmt etc</em></p></li><li class="listitem "><p>Login to each controller node and copy private ssh key to the root user's .ssh directory</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp /var/lib/ardana/.ssh/id_rsa_backup /root/.ssh/</pre></div></li><li class="listitem "><p>Verify that you can ssh to backup server as backup user using the private key</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ssh -i ~/.ssh/id_rsa_backup backupuser@doc-cp1-comp0001-mgmt</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.5.3.4.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Perform Backups for disaster recovery test</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.9">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Perform Backups for disaster recovery</p><div class="sect5" id="id-1.6.15.5.3.4.9.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Execute backup of Cassandra</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.9.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Execute backup of Cassandra</p><p>Create cassandra-backup-extserver.sh script on all
     controller nodes where Cassandra runs, which can be determined
     by running this command on deployer</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible FND-CDB --list-hosts</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &gt; ~/cassandra-backup-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra/data/data
NODETOOL=/usr/bin/nodetool

# e.g. cassandra-snp-2018-06-26-1003
SNAPSHOT_NAME=cassandra-snp-\$(date +%F-%H%M)
HOST_NAME=\$(/bin/hostname)_

# Take a snapshot of cassandra database
\$NODETOOL snapshot -t \$SNAPSHOT_NAME monasca

# Collect a list of directories that make up the snapshot
SNAPSHOT_DIR_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)
for d in \$SNAPSHOT_DIR_LIST
  do
    # copy snapshot directories to external server
    rsync -avR -e "ssh -i /root/.ssh/id_rsa_backup" \$d \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME
  done

\$NODETOOL clearsnapshot monasca
EOF</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod +x ~/cassandra-backup-extserver.sh</pre></div><p>Execute following steps on all the controller nodes</p><div id="id-1.6.15.5.3.4.9.3.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>/usr/local/sbin/cassandra-backup-extserver.sh should be
       executed on all the three controller nodes at the same time
       (within seconds of each other) for a successful backup
       </p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit /usr/local/sbin/cassandra-backup-extserver.sh script</p><p>
          Set
          <code class="literal">BACKUP_USER</code> and <code class="literal">BACKUP_SERVER</code>
          to the desired backup user (for example,
          <code class="systemitem">backupuser</code>) and desired
          backup server (for example, <code class="literal">192.168.68.132</code>),
          respectively.
         </p><div class="verbatim-wrap"><pre class="screen">BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</pre></div></li><li class="listitem "><p>Execute ~/cassandra-backup-extserver.sh</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>~/cassandra-backup-extserver.sh (on all controller nodes which are also cassandra nodes)

Requested creating snapshot(s) for [monasca] with snapshot name [cassandra-snp-2018-06-28-0251] and options {skipFlush=false}
Snapshot directory: cassandra-snp-2018-06-28-0251
sending incremental file list
created directory /mnt/backups/cassandra_backups//doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
/var/
/var/cassandra/
/var/cassandra/data/
/var/cassandra/data/data/
/var/cassandra/data/data/monasca/

...
...
...

/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-Summary.db
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-TOC.txt
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/schema.cql
sent 173,691 bytes  received 531 bytes  116,148.00 bytes/sec
total size is 171,378  speedup is 0.98
Requested clearing snapshot(s) for [monasca]</pre></div></li><li class="listitem "><p>Verify cassandra backup directory on backup server</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306
drwxr-xr-x 3 backupuser users 4096 Jun 28 02:51 doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
drwxr-xr-x 8 backupuser users 4096 Jun 27 20:56 ..

$backupuser@backupserver&gt; du -shx /mnt/backups/cassandra_backups/*
6.2G    /mnt/backups/cassandra_backups/doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
6.3G    /mnt/backups/cassandra_backups/doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.9.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Execute backup of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.9.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Execute backup of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit the configuration file for SSH backups (be careful to format the private key as requested: pipe on the first line and two spaces indentation). The private key is the key we created on the backup server earlier.</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/my_cloud/config/freezer/ssh_credentials.yml

$ cat ~/openstack/my_cloud/config/freezer/ssh_credentials.yml
freezer_ssh_host: 192.168.69.132
freezer_ssh_port: 22
freezer_ssh_username: backupuser
freezer_ssh_base_dir: /mnt/backups
freezer_ssh_private_key: |
  -----BEGIN RSA PRIVATE KEY-----
  MIIEowIBAAKCAQEAyzhZ+F+sXQp70N8zCDDb6ORKAxreT/qD4zAetjOTuBoFlGb8
  pRBY79t9vNp7qvrKaXHBfb1OkKzhqyUwEqNcC9bdngABbb8KkCq+OkfDSAZRrmja
  wa5PzgtSaZcSJm9jQcF04Fq19mZY2BLK3OJL4qISp1DmN3ZthgJcpksYid2G3YG+
  bY/EogrQrdgHfcyLaoEkiBWQSBTEENKTKFBB2jFQYdmif3KaeJySv9cJqihmyotB
  s5YTdvB5Zn/fFCKG66THhKnIm19NftbJcKc+Y3Z/ZX4W9SpMSj5dL2YW0Y176mLy
  gMLyZK9u5k+fVjYLqY7XlVAFalv9+HZsvQ3OQQIDAQABAoIBACfUkqXAsrrFrEDj
  DlCDqwZ5gBwdrwcD9ceYjdxuPXyu9PsCOHBtxNC2N23FcMmxP+zs09y+NuDaUZzG
  vCZbCFZ1tZgbLiyBbiOVjRVFLXw3aNkDSiT98jxTMcLqTi9kU5L2xN6YSOPTaYRo
  IoSqge8YjwlmLMkgGBVU7y3UuCmE/Rylclb1EI9mMPElTF+87tYK9IyA2QbIJm/w
  4aZugSZa3PwUvKGG/TCJVD+JfrZ1kCz6MFnNS1jYT/cQ6nzLsQx7UuYLgpvTMDK6
  Fjq63TmVg9Z1urTB4dqhxzpDbTNfJrV55MuA/z9/qFHs649tFB1/hCsG3EqWcDnP
  mcv79nECgYEA9WdOsDnnCI1bamKA0XZxovb2rpYZyRakv3GujjqDrYTI97zoG+Gh
  gLcD1EMLnLLQWAkDTITIf8eurkVLKzhb1xlN0Z4xCLs7ukgMetlVWfNrcYEkzGa8
  wec7n1LfHcH5BNjjancRH0Q1Xcc2K7UgGe2iw/Iw67wlJ8i5j2Wq3sUCgYEA0/6/
  irdJzFB/9aTC8SFWbqj1DdyrpjJPm4yZeXkRAdn2GeLU2jefqPtxYwMCB1goeORc
  gQLspQpxeDvLdiQod1Y1aTAGYOcZOyAatIlOqiI40y3Mmj8YU/KnL7NMkaYBCrJh
  aW//xo+l20dz52pONzLFjw1tW9vhCsG1QlrCaU0CgYB03qUn4ft4JDHUAWNN3fWS
  YcDrNkrDbIg7MD2sOIu7WFCJQyrbFGJgtUgaj295SeNU+b3bdCU0TXmQPynkRGvg
  jYl0+bxqZxizx1pCKzytoPKbVKCcw5TDV4caglIFjvoz58KuUlQSKt6rcZMHz7Oh
  BX4NiUrpCWo8fyh39Tgh7QKBgEUajm92Tc0XFI8LNSyK9HTACJmLLDzRu5d13nV1
  XHDhDtLjWQUFCrt3sz9WNKwWNaMqtWisfl1SKSjLPQh2wuYbqO9v4zRlQJlAXtQo
  yga1fxZ/oGlLVe/PcmYfKT91AHPvL8fB5XthSexPv11ZDsP5feKiutots47hE+fc
  U/ElAoGBAItNX4jpUfnaOj0mR0L+2R2XNmC5b4PrMhH/+XRRdSr1t76+RJ23MDwf
  SV3u3/30eS7Ch2OV9o9lr0sjMKRgBsLZcaSmKp9K0j/sotwBl0+C4nauZMUKDXqg
  uGCyWeTQdAOD9QblzGoWy6g3ZI+XZWQIMt0pH38d/ZRbuSUk5o5v
  -----END RSA PRIVATE KEY-----</pre></div></li><li class="listitem "><p>Save the modifications in the GIT repository</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "SSH backup configuration"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>Create the Freezer jobs</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li><li class="listitem "><p>Wait until all the SSH backup jobs have finished running</p><p>Freezer backup jobs are scheduled at interval specified in job specification</p><p>You will have to wait for the scheduled time interval for the backup job to run</p><p>To find the interval:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep SSH

| 34c1364692f64a328c38d54b95753844 | Ardana Default: deployer backup to SSH      |         7 | success | scheduled |       |            |
| 944154642f624bb7b9ff12c573a70577 | Ardana Default: swift backup to SSH         |         1 | success | scheduled |       |            |
| 22c6bab7ac4d43debcd4f5a9c4c4bb19 | Ardana Default: mysql backup to SSH         |         1 | success | scheduled |       |            |

<code class="prompt user">ardana &gt; </code>freezer job-show 944154642f624bb7b9ff12c573a70577
+-------------+---------------------------------------------------------------------------------+
| Field       | Value                                                                           |
+-------------+---------------------------------------------------------------------------------+
| Job ID      | 944154642f624bb7b9ff12c573a70577                                                |
| Client ID   | ardana-qe201-cp1-c1-m1-mgmt                                                     |
| User ID     | 33a6a77adc4b4799a79a4c3bd40f680d                                                |
| Session ID  |                                                                                 |
| Description | Ardana Default: swift backup to SSH                                             |
| Actions     | [{u'action_id': u'e8373b03ca4b41fdafd83f9ba7734bfa',                            |
|             |   u'freezer_action': {u'action': u'backup',                                     |
|             |                       u'backup_name': u'freezer_swift_builder_dir_backup',      |
|             |                       u'container': u'/mnt/backups/freezer_rings_backups',      |
|             |                       u'log_config_append': u'/etc/freezer/agent-logging.conf', |
|             |                       u'max_level': 14,                                         |
|             |                       u'path_to_backup': u'/etc/swiftlm/',                      |
|             |                       u'remove_older_than': 90,                                 |
|             |                       u'snapshot': True,                                        |
|             |                       u'ssh_host': u'192.168.69.132',                           |
|             |                       u'ssh_key': u'/etc/freezer/ssh_key',                      |
|             |                       u'ssh_port': u'22',                                       |
|             |                       u'ssh_username': u'backupuser',                           |
|             |                       u'storage': u'ssh'},                                      |
|             |   u'max_retries': 5,                                                            |
|             |   u'max_retries_interval': 60,                                                  |
|             |   u'user_id': u'33a6a77adc4b4799a79a4c3bd40f680d'}]                             |
| Start Date  |                                                                                 |
| End Date    |                                                                                 |
| Interval    | 24 hours                                                                        |
+-------------+---------------------------------------------------------------------------------+</pre></div><p>Swift SSH backup job has Interval of 24 hours, so the next backup would run after 24 hours.</p><p>In the default installation Interval for various backup jobs are:</p><div class="table" id="FreezerBackupJobScheduleTable"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.1: </span><span class="name">Default Interval for Freezer backup jobs </span><a title="Permalink" class="permalink" href="system-maintenance.html#FreezerBackupJobScheduleTable">#</a></h6></div><div class="table-contents"><table class="table" summary="Default Interval for Freezer backup jobs" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Job Name</th><th>Interval</th></tr></thead><tbody><tr><td>Ardana Default: deployer backup to SSH</td><td>48 hours</td></tr><tr><td>Ardana Default: mysql backup to SSH</td><td>12 hours</td></tr><tr><td>Ardana Default: swift backup to SSH</td><td>24 hours</td></tr></tbody></table></div></div><p>You will have to wait for as long as 48 hours for all the backup jobs to run</p></li><li class="listitem "><p>On the backup server, you can verify that the backup files are present</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ls -lah  /mnt/backups/
total 16
drwxr-xr-x 2 backupuser users 4096 Jun 27  2017 bin
drwxr-xr-x 2 backupuser users 4096 Jun 29 14:04 freezer_database_backups
drwxr-xr-x 2 backupuser users 4096 Jun 29 14:05 freezer_lifecycle_manager_backups
drwxr-xr-x 2 backupuser users 4096 Jun 29 14:05 freezer_rings_backups</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>du -shx *
4.0K    bin
509M    freezer_audit_logs_backups
2.8G    freezer_database_backups
24G     freezer_lifecycle_manager_backups
160K    freezer_rings_backups</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.5.3.4.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore of the first controller</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.10">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Restore of the first controller</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit the SSH backup configuration (re-enter the same information as earlier)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div></li><li class="listitem "><p>Execute the restore helper. When prompted, enter the hostname the first controller had. In this example: <code class="literal">doc-cp1-c1-m1-mgmt</code></p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="listitem "><p>Execute the restore. When prompted, leave the first value empty (none) and validate the restore by typing 'yes'.</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo su
cd /root/deployer_restore_helper/
./deployer_restore_script.sh</pre></div></li><li class="listitem "><p>Create a restore file for Swift rings</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nano swift_rings_restore.ini
<code class="prompt user">ardana &gt; </code>cat swift_rings_restore.ini</pre></div><p>Help:</p><div class="verbatim-wrap"><pre class="screen">[default]
action = restore
storage = ssh
# backup server ip
ssh_host = <em class="replaceable ">192.168.69.132</em>
# username to connect to the backup server
ssh_username = <em class="replaceable ">backupuser</em>
ssh_key = /etc/freezer/ssh_key
# base directory for backups on the backup server 
container = <em class="replaceable ">/mnt/backups/freezer_ring_backups</em>
backup_name = freezer_swift_builder_dir_backup
restore_abs_path = /etc/swiftlm
log_file = /var/log/freezer-agent/freezer-agent.log
# hostname that the controller
hostname = <em class="replaceable ">doc-cp1-c1-m1-mgmt</em>
overwrite = True</pre></div></li><li class="listitem "><p>Execute the restore of the swift rings</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer-agent --config ./swift_rings_restore.ini</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-deployment of controllers 1, 2 and 3</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.11">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Re-deployment of controllers 1, 2 and 3</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Change back to the default ardana user</p></li><li class="listitem "><p>Deactivate the freezer backup jobs (otherwise empty backups would be added on top of the current good backups)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nano ~/openstack/my_cloud/config/freezer/activate_jobs.yml
<code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/config/freezer/activate_jobs.yml

# If set to false, We wont create backups jobs.
freezer_create_backup_jobs: false

# If set to false, We wont create restore jobs.
freezer_create_restore_jobs: true</pre></div></li><li class="listitem "><p>Save the modification in the GIT repository</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "De-Activate SSH backup jobs during re-deployment"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>Run the cobbler-deploy.yml playbook</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.xml</pre></div></li><li class="listitem "><p>Run the bm-reimage.yml playbook limited to the second and third controller</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2,controller3</pre></div><p>controller2 and controller3 names can vary. You can use the bm-power-status.yml playbook in order to check the cobbler names of these nodes.</p></li><li class="listitem "><p>Run the site.yml playbook limited to the three controllers and localhost. In this example, this means: doc-cp1-c1-m1-mgmt, doc-cp1-c1-m2-mgmt, doc-cp1-c1-m3-mgmt and localhost</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cassandra database restore</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.12">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Cassandra database restore</p><p>Create a script cassandra-restore-extserver.sh on all
       controller nodes</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &gt; ~/cassandra-restore-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra
NODETOOL=/usr/bin/nodetool

HOST_NAME=\$(/bin/hostname)_

#Get snapshot name from command line.
if [ -z "\$*"  ]
then
  echo "usage \$0 &lt;snapshot to restore&gt;"
  exit 1
fi
SNAPSHOT_NAME=\$1

# restore
rsync -av -e "ssh -i /root/.ssh/id_rsa_backup" \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME/ /

# set ownership of newley restored files
chown -R cassandra:cassandra \$DATA_DIR

# Get a list of snapshot directories that have files to be restored.
RESTORE_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)

# use RESTORE_LIST to move snapshot files back into place of database.
for d in \$RESTORE_LIST
do
  cd \$d
  mv * ../..
  KEYSPACE=\$(pwd | rev | cut -d '/' -f4 | rev)
  TABLE_NAME=\$(pwd | rev | cut -d '/' -f3 |rev | cut -d '-' -f1)
  \$NODETOOL refresh \$KEYSPACE \$TABLE_NAME
done
cd
# Cleanup snapshot directories
\$NODETOOL clearsnapshot \$KEYSPACE
EOF</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod +x ~/cassandra-restore-extserver.sh</pre></div><p>Execute following steps on all the controller nodes</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Edit ~/cassandra-restore-extserver.sh script</p><p>
               Set
               <em class="replaceable ">BACKUP_USER</em>,<em class="replaceable ">BACKUP_SERVER</em>
               to the desired backup user (for example,
               <code class="systemitem">backupuser</code>) and the
               desired backup server (for example,
               <code class="literal">192.168.68.132</code>), respectively.
              </p><div class="verbatim-wrap"><pre class="screen">BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</pre></div></li><li class="listitem "><p>Execute ~/cassandra-restore-extserver.sh <em class="replaceable ">SNAPSHOT_NAME</em></p><p>You will have to find out <em class="replaceable ">SNAPSHOT_NAME</em> from listing of /mnt/backups/cassandra_backups.
              All the directories are of format <em class="replaceable ">HOST</em>_<em class="replaceable ">SNAPSHOT_NAME</em></p><div class="verbatim-wrap"><pre class="screen">ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>~/cassandra-restore-extserver.sh cassandra-snp-2018-06-28-0306

receiving incremental file list
./
var/
var/cassandra/
var/cassandra/data/
var/cassandra/data/data/
var/cassandra/data/data/monasca/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/manifest.json
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-CompressionInfo.db
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-Data.db
...
...
...
/usr/bin/nodetool clearsnapshot monasca</pre></div></li></ol></div></div><div class="sect4" id="id-1.6.15.5.3.4.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Databases restore</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.13">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Databases restore</p><div class="sect5" id="id-1.6.15.5.3.4.13.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MariaDB database restore</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.13.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>MariaDB database restore</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Source the backup credentials file</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/backup.osrc</pre></div></li><li class="listitem "><p>List Freezer jobs</p><p>Gather the id of the job corresponding to the first controller and with the description. For example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep "mysql restore from SSH"
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| Job ID                           | Description                                 | # Actions | Result  | Status    | Event | Session ID |
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| 64715c6ce8ed40e1b346136083923260 | Ardana Default: mysql restore from SSH      |         1 |         | stop      |       |            |

<code class="prompt user">ardana &gt; </code>freezer job-show 64715c6ce8ed40e1b346136083923260
+-------------+---------------------------------------------------------------------------------+
| Field       | Value                                                                           |
+-------------+---------------------------------------------------------------------------------+
| Job ID      | 64715c6ce8ed40e1b346136083923260                                                |
| Client ID   | doc-cp1-c1-m1-mgmt                                                     |
| User ID     | 33a6a77adc4b4799a79a4c3bd40f680d                                                |
| Session ID  |                                                                                 |
| Description | Ardana Default: mysql restore from SSH                                          |
| Actions     | [{u'action_id': u'19dfb0b1851e41c682716ecc6990b25b',                            |
|             |   u'freezer_action': {u'action': u'restore',                                    |
|             |                       u'backup_name': u'freezer_mysql_backup',                  |
|             |                       u'container': u'/mnt/backups/freezer_database_backups',   |
|             |                       u'hostname': u'doc-cp1-c1-m1-mgmt',              |
|             |                       u'log_config_append': u'/etc/freezer/agent-logging.conf', |
|             |                       u'restore_abs_path': u'/tmp/mysql_restore/',              |
|             |                       u'ssh_host': u'192.168.69.132',                           |
|             |                       u'ssh_key': u'/etc/freezer/ssh_key',                      |
|             |                       u'ssh_port': u'22',                                       |
|             |                       u'ssh_username': u'backupuser',                           |
|             |                       u'storage': u'ssh'},                                      |
|             |   u'max_retries': 5,                                                            |
|             |   u'max_retries_interval': 60,                                                  |
|             |   u'user_id': u'33a6a77adc4b4799a79a4c3bd40f680d'}]                             |
| Start Date  |                                                                                 |
| End Date    |                                                                                 |
| Interval    |                                                                                 |
+-------------+---------------------------------------------------------------------------------+</pre></div></li><li class="listitem "><p>Start the job using its id</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start 64715c6ce8ed40e1b346136083923260
Start request sent for job 64715c6ce8ed40e1b346136083923260</pre></div></li><li class="listitem "><p>Wait for the job result to be success</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep "mysql restore from SSH"
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| Job ID                           | Description                                 | # Actions | Result  | Status    | Event | Session ID |
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| 64715c6ce8ed40e1b346136083923260 | Ardana Default: mysql restore from SSH      |         1 |         | running      |       |            |</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list | grep "mysql restore from SSH"
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| Job ID                           | Description                                 | # Actions | Result  | Status    | Event | Session ID |
+----------------------------------+---------------------------------------------+-----------+---------+-----------+-------+------------+
| 64715c6ce8ed40e1b346136083923260 | Ardana Default: mysql restore from SSH      |         1 | success | completed |       |            |</pre></div></li><li class="listitem "><p>Verify that the files have been restored on the controller</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*

16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li><li class="listitem "><p>Repeat steps 2-5 on the other two controllers where the 
               MariaDB/Galera database is running, which can be 
               determined by running below command on deployer</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible FND-MDB --list-hosts</pre></div></li><li class="listitem "><p>Stop <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers (replace the hostnames of the controllers in the command)</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="listitem "><p>Clean the mysql directory and copy the restored backup on all three controllers where MariaDB/Galera database is running</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cd /var/lib/mysql/
<code class="prompt user">root # </code>rm -rf ./*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* ./</pre></div><p>Switch back to the ardana user once the copy is finished</p></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.13.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.13.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Restart the MariaDB Database</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div><p>
             On the deployer node, execute the
             <code class="filename">galera-bootstrap.yml</code> playbook which will
             automatically determine the log sequence number, bootstrap the main node,
             and start the database cluster.
           </p><p>
             If this process fails to recover the database cluster, please refer to
             <a class="xref" href="system-maintenance.html#mysql" title="13.2.2.1.2. Recovering the MariaDB Database">Section 13.2.2.1.2, “Recovering the MariaDB Database”</a>. There Scenario 3 covers the process of manually
             starting the database.
           </p></li><li class="listitem "><p>
            Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services limited to the three controllers (replace the
            the hostnames of the controllers in the command).
          </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-start.yml \
 --limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="listitem "><p>Re-configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div><div class="sect5" id="id-1.6.15.5.3.4.13.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.1.2.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-enable SSH backups</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.13.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Re-enable SSH backups</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Re-activate Freezer backup jobs</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/my_cloud/config/freezer/activate_jobs.yml
<code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/config/freezer/activate_jobs.yml

# If set to false, We wont create backups jobs.
freezer_create_backup_jobs: true

# If set to false, We wont create restore jobs.
freezer_create_restore_jobs: true</pre></div></li><li class="listitem "><p>Save the modifications in the GIT repository</p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/
git add -A
git commit -a -m “Re-Activate SSH backup jobs”
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>Create Freezer jobs</p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.6.15.5.3.4.14"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.1.2.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post restore testing</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.3.4.14">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Post restore testing</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Source the service credential file</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>Swift</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>swift list
container_1
volumebackups

<code class="prompt user">ardana &gt; </code>swift list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc

<code class="prompt user">ardana &gt; </code>swift download container_1 /tmp/backup.osrc</pre></div></li><li class="listitem "><p>Neutron</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list
+--------------------------------------+---------------------+--------------------------------------+
| ID                                   | Name                | Subnets                              |
+--------------------------------------+---------------------+--------------------------------------+
| 07c35d11-13f9-41d4-8289-fa92147b1d44 | test-net             | 02d5ca3b-1133-4a74-a9ab-1f1dc2853ec8|
+--------------------------------------+---------------------+--------------------------------------+</pre></div></li><li class="listitem "><p>Glance</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image list
+--------------------------------------+----------------------+--------+
| ID                                   | Name                 | Status |
+--------------------------------------+----------------------+--------+
| 411a0363-7f4b-4bbc-889c-b9614e2da52e | cirros-0.4.0-x86_64  | active |
+--------------------------------------+----------------------+--------+
<code class="prompt user">ardana &gt; </code>openstack image save --file /tmp/cirros f751c39b-f1e3-4f02-8332-3886826889ba
<code class="prompt user">ardana &gt; </code>ls -lah /tmp/cirros
-rw-r--r-- 1 ardana ardana 12716032 Jul  2 20:52 /tmp/cirros</pre></div></li><li class="listitem "><p>Nova</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list

<code class="prompt user">ardana &gt; </code>openstack server list

<code class="prompt user">ardana &gt; </code>openstack server create server_6 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e  --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
+-------------------------------------+------------------------------------------------------------+
| Field                               | Value                                                      |
+-------------------------------------+------------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                     |
| OS-EXT-AZ:availability_zone         |                                                            |
| OS-EXT-SRV-ATTR:host                | None                                                       |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                                       |
| OS-EXT-SRV-ATTR:instance_name       |                                                            |
| OS-EXT-STS:power_state              | NOSTATE                                                    |
| OS-EXT-STS:task_state               | scheduling                                                 |
| OS-EXT-STS:vm_state                 | building                                                   |
| OS-SRV-USG:launched_at              | None                                                       |
| OS-SRV-USG:terminated_at            | None                                                       |
| accessIPv4                          |                                                            |
| accessIPv6                          |                                                            |
| addresses                           |                                                            |
| adminPass                           | iJBoBaj53oUd                                               |
| config_drive                        |                                                            |
| created                             | 2018-07-02T21:02:01Z                                       |
| flavor                              | m1.small (2)                                               |
| hostId                              |                                                            |
| id                                  | ce7689ff-23bf-4fe9-b2a9-922d4aa9412c                       |
| image                               | cirros-0.4.0-x86_64 (f751c39b-f1e3-4f02-8332-3886826889ba) |
| key_name                            | None                                                       |
| name                                | server_6                                                   |
| progress                            | 0                                                          |
| project_id                          | cca416004124432592b2949a5c5d9949                           |
| properties                          |                                                            |
| security_groups                     | name='default'                                             |
| status                              | BUILD                                                      |
| updated                             | 2018-07-02T21:02:01Z                                       |
| user_id                             | 8cb1168776d24390b44c3aaa0720b532                           |
| volumes_attached                    |                                                            |
+-------------------------------------+------------------------------------------------------------+

<code class="prompt user">ardana &gt; </code>openstack server list
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ID                                   | Name     | Status | Networks                        | Image               | Flavor    |
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ce7689ff-23bf-4fe9-b2a9-922d4aa9412c | server_6 | ACTIVE | n1=1.1.1.8                      | cirros-0.4.0-x86_64 | m1.small  |

<code class="prompt user">ardana &gt; </code>openstack server delete ce7689ff-23bf-4fe9-b2a9-922d4aa9412c</pre></div></li></ol></div></div></div></div><div class="sect2" id="cont-ungplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Control Plane Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#cont-ungplanned">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-cont_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-cont_unplanned.xml</li><li><span class="ds-label">ID: </span>cont-ungplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for controller nodes such as recovery from power
  failure.
 </p><div class="sect3" id="recover-downed-cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting Controller Nodes After a Reboot</span> <a title="Permalink" class="permalink" href="system-maintenance.html#recover-downed-cluster">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>recover-downed-cluster</li></ul></div></div></div></div><p>
  Steps to follow if one or more of your controller nodes lose network
  connectivity or power, which includes if the node is either rebooted or needs
  hardware maintenance.
 </p><p>
  When a controller node is rebooted, needs hardware maintenance, loses
  network connectivity or loses power, these steps will help you recover the
  node.
 </p><p>
  These steps may also be used if the Host Status (ping) alarm is triggered
  for one or more of your controller nodes.
 </p><div class="sect4" id="idg-all-operations-maintenance-controller-restart-controller-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-controller-restart-controller-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-controller-restart-controller-xml-7</li></ul></div></div></div></div><p>
   The following conditions must be true in order to perform these steps
   successfully:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Each of your controller nodes should be powered on.
    </p></li><li class="listitem "><p>
     Each of your controller nodes should have network connectivity, verified
     by SSH connectivity from the Cloud Lifecycle Manager to them.
    </p></li><li class="listitem "><p>
     The operator who performs these steps will need access to the lifecycle
     manager.
    </p></li></ul></div></div><div class="sect4" id="mysql"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering the MariaDB Database</span> <a title="Permalink" class="permalink" href="system-maintenance.html#mysql">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>mysql</li></ul></div></div></div></div><p>
   The recovery process for your MariaDB database cluster will depend on how
   many of your controller nodes need to be recovered. We will cover two
   scenarios:
  </p><p>
   <span class="bold"><strong>Scenario 1: Recovering one or two of your controller
   nodes but not the entire cluster</strong></span>
  </p><p>
   Follow these steps to recover one or two of your controller nodes but not the
   entire cluster, then use these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Ensure the controller nodes have power and are booted to the command
     prompt.
    </p></li><li class="step "><p>
     If the MariaDB service is not started, start it with this command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo service mysql start</pre></div></li><li class="step "><p>
     If MariaDB fails to start, proceed to the next section which covers the
     bootstrap process.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Scenario 2: Recovering the entire controller cluster
   with the bootstrap playbook</strong></span>
  </p><p>
   If the scenario above failed or if you need to recover your entire control
   plane cluster, use the process below to recover the MariaDB database.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Make sure no <code class="literal">mysqld</code> daemon is running on any node in
     the cluster before you continue with the steps in this procedure. If there
     is a <code class="literal">mysqld</code> daemon running, then use the command below
     to shut down the daemon.
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop mysql</pre></div><p>
     If the mysqld daemon does not go down following the service stop, then
     kill the daemon using <code class="literal">kill -9</code> before continuing.
    </p></li><li class="step "><p>
     On the deployer node, execute the
     <code class="filename">galera-bootstrap.yml</code> playbook which will
     automatically determine the log sequence number, bootstrap the main node,
     and start the database cluster.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li></ol></div></div></div><div class="sect4" id="hlm"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting Services on the Controller Nodes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#hlm">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>hlm</li></ul></div></div></div></div><p>
   From the Cloud Lifecycle Manager you should execute the
   <code class="literal">ardana-start.yml</code> playbook for each node that was brought
   down so the services can be started back up.
  </p><p>
   If you have a dedicated (separate) Cloud Lifecycle Manager node you can use this
   syntax:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;</pre></div><p>
   If you have a shared Cloud Lifecycle Manager/controller setup and need to restart
   services on this shared node, you can use <code class="literal">localhost</code> to
   indicate the shared node, like this:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;,localhost</pre></div><div id="id-1.6.15.5.4.3.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you leave off the <code class="literal">--limit</code> switch, the playbook will
    be run against all nodes.
   </p></div></div><div class="sect4" id="monasca"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Monitoring Agents</span> <a title="Permalink" class="permalink" href="system-maintenance.html#monasca">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>monasca</li></ul></div></div></div></div><p>
   As part of the recovery process, you should also restart the
   <code class="literal">monasca-agent</code> and these steps will show you how:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Stop the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml</pre></div></li><li class="listitem "><p>
     Restart the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml</pre></div></li><li class="listitem "><p>
     You can then confirm the status of the <code class="literal">monasca-agent</code>
     with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div></div><div class="sect3" id="recovering-controller-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering the Control Plane</span> <a title="Permalink" class="permalink" href="system-maintenance.html#recovering-controller-nodes">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-recovering_controller_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-recovering_controller_nodes.xml</li><li><span class="ds-label">ID: </span>recovering-controller-nodes</li></ul></div></div></div></div><p>
  If one or more of your controller nodes has experienced data or disk
  corruption due to power loss or hardware failure and you need perform
  disaster recovery then we provide different scenarios for how to resolve them
  to get your cloud recovered.
 </p><p>
  If one or more of your controller nodes has experienced data or disk
  corruption due to power-loss or hardware failure and you need perform
  disaster recovery then we provide different scenarios for how to resolve
  them to get your cloud recovered.
 </p><div id="id-1.6.15.5.4.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   You should have backed up <code class="filename">/etc/group</code> of the Cloud Lifecycle Manager
   manually after installation. While recovering a Cloud Lifecycle Manager node, manually copy
   the <code class="filename">/etc/group</code> file from a backup of the old Cloud Lifecycle Manager.
  </p></div><div class="sect4" id="pit-database-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time MariaDB Database Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#pit-database-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span>pit-database-recovery</li></ul></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, cloud controller nodes,
  and compute nodes) but you want to restore the MariaDB database to a
  previous state.
 </p><div class="sect5" id="id-1.6.15.5.4.4.5.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.5.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Determine which node is the first host member in the
     <code class="literal">FND-MDB</code> group, which will be the first node hosting the
     MariaDB service in your cloud. You can do this by using these commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>grep -A1 FND-MDB--first-member hosts/verb_hosts</pre></div><p>The result will be similar to the following example:
</p><div class="verbatim-wrap"><pre class="screen">[FND-MDB--first-member:children]
ardana002-cp1-c1-m1</pre></div><p>
     In this example, the host name of the node is
     <code class="literal">ardana002-cp1-c1-m1</code>
    </p></li><li class="step " id="mariadb-nodes"><p>
     Find the host IP address which will be used to log in.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana002-cp1-c1-m1
10.84.43.82      ardana002-cp1-c1-m1-extapi ardana002-cp1-c1-m1-extapi
192.168.24.21    ardana002-cp1-c1-m1-mgmt ardana002-cp1-c1-m1-mgmt
10.1.2.1         ardana002-cp1-c1-m1-guest ardana002-cp1-c1-m1-guest
10.84.65.3       ardana002-cp1-c1-m1-EXTERNAL-VM ardana002-cp1-c1-m1-external-vm</pre></div><p>
     In this example, <code class="literal">192.168.24.21</code> is the IP address for
     the host.
    </p></li><li class="step "><p>
     SSH into the host.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh ardana@192.168.24.21</pre></div></li><li class="step "><p>
     Source the backup file.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source /var/lib/ardana/backup.osrc</pre></div></li><li class="step "><p>
     Find the <code class="literal">Client ID</code> for the host name from the beginning
     of this procedure ( <code class="literal">ardana002-cp1-c1-m1</code> ) in this
     example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
     In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
     ID</code> are the same: <code class="literal">ardana002-cp1-c1-m1-mgmt</code>.
    </p></li><li class="step "><p>
     List the jobs
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
     Using the example in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C ardana002-cp1-c1-m1-mgmt</pre></div></li><li class="step "><p>
     Get the corresponding job id for <code class="literal">Ardana Default: mysql restore
     from Swift</code>.
    </p></li><li class="step "><p>
     Launch the restore process with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB-ID</em></pre></div></li><li class="step "><p>
     This will take some time. You can follow the progress by running
     <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code>. Wait
     until the restore job is finished before doing the next step.
    </p></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step "><p>
     Log back in to the first node running the MariaDB service, the same node
     as in <a class="xref" href="system-maintenance.html#mariadb-nodes" title="Step 3">Step 3</a>.
    </p></li><li class="step "><p>
     Clean the MariaDB directory using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm -r /var/lib/mysql/*</pre></div></li><li class="step "><p>
     Copy the restored files back to the MariaDB directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</pre></div></li><li class="step "><p>
     Log in to each of the other nodes in your MariaDB cluster, which were
     determined in <a class="xref" href="system-maintenance.html#mariadb-nodes" title="Step 3">Step 3</a>. Remove the
     <code class="literal">grastate.dat</code> file from each of them.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm /var/lib/mysql/grastate.dat</pre></div><div id="id-1.6.15.5.4.4.5.3.2.16.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
      Do not remove this file from the first node in your MariaDB cluster.
      Ensure you only do this from the other cluster nodes.
     </p></div></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Start the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.5.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.5.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Follow the same procedure as the one for Swift but select the job
   <code class="literal">Ardana Default: mysql restore from SSH</code>.
  </p></div><div class="sect5" id="id-1.6.15.5.4.4.5.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore MariaDB manually</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.5.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If restoring MariaDB fails during the procedure outlined above, you can
   follow this procedure to manually restore MariaDB:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the MariaDB cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step "><p>
     On all of the nodes running the MariaDB service, which should be all of
     your controller nodes, run the following command to purge the old
     database:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo rm -r /var/lib/mysql/*</pre></div></li><li class="step "><p>
     On the first node running the MariaDB service restore the backup with
     the command below. If you have already restored to a temporary directory,
     copy the files again.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</pre></div></li><li class="step "><p>
     If you need to restore the files manually from SSH, follow these steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Create the <code class="literal">/root/mysql_restore.ini</code> file with the
       contents below. Be careful to substitute the <code class="literal">{{ values
       }}</code>. Note that the SSH information refers to the SSH server you
       configured for backup before installing.
      </p><div class="verbatim-wrap"><pre class="screen">[default]
action = restore
storage = ssh
ssh_host = {{ freezer_ssh_host }}
ssh_username = {{ freezer_ssh_username }}
container = {{ freezer_ssh_base_dir }}/freezer_mysql_backup
ssh_key = /etc/freezer/ssh_key
backup_name = freezer_mysql_backup
restore_abs_path = /var/lib/mysql/
log_file = /var/log/freezer-agent/freezer-agent.log
hostname = {{ hostname of the first MariaDB node }}</pre></div></li><li class="step "><p>
       Execute the restore job:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer-agent --config /root/mysql_restore.ini</pre></div></li></ol></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Start the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step "><p>
     After approximately 10-15 minutes, the output of the
     <code class="literal">percona-status.yml</code> playbook should show all the
     MariaDB nodes in sync. MariaDB cluster status can be checked using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div><p>
     An example output is as follows:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] *************
  ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m2-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m3-mgmt] =&gt; {
  "msg": "mysql is synced."
  }</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.5.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time Cassandra Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.5.6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A node may have been removed either due to an intentional action in the
   Cloud Lifecycle Manager Admin UI or as a result of a fatal hardware event that requires a
   server to be replaced. In either case, the entry for the failed or deleted
   node should be removed from Cassandra before a new node is brought up.
  </p><p>
   The following steps should be taken before enabling and deploying the
   replacement node.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Determine the IP address of the node that was removed or is being replaced.
    </p></li><li class="step "><p>
     On one of the functional Cassandra control plane nodes, log in as the
     <code class="literal">ardana</code> user.
    </p></li><li class="step "><p>
     Run the command <code class="command">nodetool status</code> to display a list of
     Cassandra nodes.
    </p></li><li class="step "><p>
     If the node that has been removed (no IP address matches that of the
     removed node) is not in the list, skip the next step.
    </p></li><li class="step "><p>
     If the node that was removed is still in the list, copy its node
     <em class="replaceable ">ID</em>.
    </p></li><li class="step "><p>
     Run the command <code class="command">nodetool removenode
     <em class="replaceable ">ID</em></code>.
    </p></li></ol></div></div><p>
   After any obsolete node entries have been removed, the replacement node can
   be deployed as usual (for more information, see <a class="xref" href="system-maintenance.html#cont-planned" title="13.1.2. Planned Control Plane Maintenance">Section 13.1.2, “Planned Control Plane Maintenance”</a>). The new Cassandra node will be able to join the
   cluster and replicate data.
  </p><p>
   For more information, please consult <a class="link" href="http://cassandra.apache.org/doc/latest/operating/topo_changes.html" target="_blank">the
   Cassandra documentation</a>.
   </p></div></div><div class="sect4" id="pit-swiftrings-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time Swift Rings Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#pit-swiftrings-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>pit-swiftrings-recovery</li></ul></div></div></div></div><p>
  In this situation, everything is still running (Cloud Lifecycle Manager, control plane nodes,
  and compute nodes) but you want to restore your Swift rings to a previous
  state.
 </p><div id="id-1.6.15.5.4.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   Freezer backs up and restores Swift rings only, not Swift data.
  </p></div><div class="sect5" id="restore-swift-bu"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#restore-swift-bu">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>restore-swift-bu</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="swift-nodes"><p>
     Log in to the first Swift Proxy (<code class="literal">SWF-PRX[0]</code>) node.
    </p><p>
     To find the first Swift Proxy node:
    </p><ol type="a" class="substeps "><li class="step "><p>
       On the Cloud Lifecycle Manager
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd  ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit SWF-PRX[0]</pre></div><p>
       At the end of the output, you will see something like the following
       example:
      </p><div class="verbatim-wrap"><pre class="screen">...
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:max-latency: 0.679254770279 (at 1529352109.66)'
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:avg-latency: 0.679254770279 (at 1529352109.66)'

PLAY RECAP ********************************************************************
ardana-qe102-cp1-c1-m1 : ok=12 changed=0 unreachable=0 failed=0```</pre></div></li><li class="step "><p>
       Find the first node name and its IP address. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana-qe102-cp1-c1-m1</pre></div></li></ol></li><li class="step "><p>
     Source the backup environment file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source /var/lib/ardana/backup.osrc</pre></div></li><li class="step "><p>
     Find the client id.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
     In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
     ID</code> are the same: <code class="literal">ardana002-cp1-c1-m1-mgmt</code>.
    </p></li><li class="step "><p>
     List the jobs
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
     Using the example in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C ardana002-cp1-c1-m1-mgmt</pre></div></li><li class="step "><p>
     Get the corresponding job id for <code class="literal">Ardana Default: swift restore
     from Swift</code> in the <code class="literal">Description</code> column.
    </p></li><li class="step "><p>
     Launch the restore job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB-ID</em></pre></div></li><li class="step "><p>
     This will take some time. You can follow the progress by running
     <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code> Wait
     until the restore job is finished before doing the next step.
    </p></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the Swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml</pre></div></li><li class="step "><p>
     Log back in to the first Swift Proxy (<code class="literal">SWF-PRX[0]</code>)
     node, which was determined in <a class="xref" href="system-maintenance.html#swift-nodes" title="Step 1">Step 1</a>.
    </p></li><li class="step "><p>
     Copy the restored files.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Reconfigure the Swift service:\
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.6.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.6.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Follow almost the same procedure as for Swift in the section immediately
   preceding this one: <a class="xref" href="system-maintenance.html#restore-swift-bu" title="13.2.2.2.2.1. Restore from a Swift backup">Section 13.2.2.2.2.1, “Restore from a Swift backup”</a>. The only change is
   that the restore job uses a different job id. Get the corresponding job id
   for <code class="literal">Ardana Default: Swift restore from SSH</code> in the
   <code class="literal">Description</code> column.
  </p></div></div><div class="sect4" id="pit-lifecyclemanager-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-time Cloud Lifecycle Manager Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#pit-lifecyclemanager-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-pit_lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span>pit-lifecyclemanager-recovery</li></ul></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, controller nodes, and
  compute nodes) but you want to restore the Cloud Lifecycle Manager to a previous state.
 </p><div class="procedure " id="restore-swift-ssh-bu"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 13.1: </span><span class="name">Restoring from a Swift or SSH Backup </span><a title="Permalink" class="permalink" href="system-maintenance.html#restore-swift-ssh-bu">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Source the backup environment file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>source /var/lib/ardana/backup.osrc</pre></div></li><li class="step "><p>
    Find the <code class="literal">Client ID</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
    In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
    ID</code> are the same: <code class="literal">ardana002-cp1-c1-m1-mgmt</code>.
   </p></li><li class="step "><p>
    List the jobs
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
    Using the example in the previous step:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer job-list -C ardana002-cp1-c1-m1-mgmt</pre></div></li><li class="step "><p>
    Find the correct job ID:
   </p><p><span class="formalpara-title">SSH Backups: </span>
     Get the id corresponding to the job id for <code class="literal">Ardana Default:
     deployer restore from SSH</code>.
    </p><p>
    or
   </p><p><span class="formalpara-title">Swift Backups. </span>
     Get the id corresponding to the job id for <code class="literal">Ardana Default:
     deployer restore from Swift</code>.
    </p></li><li class="step "><p>
    Stop the Dayzero UI:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop dayzero</pre></div></li><li class="step "><p>
    Launch the restore job:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>freezer job-start <em class="replaceable ">JOB ID</em></pre></div></li><li class="step "><p>
    This will take some time. You can follow the progress by running
    <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code>. Wait
    until the restore job is finished before doing the next step.
   </p></li><li class="step "><p>
    Start the Dayzero UI:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl start dayzero</pre></div></li></ol></div></div></div><div class="sect4" id="lifecyclemanager-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Disaster Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#lifecyclemanager-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span>lifecyclemanager-recovery</li></ul></div></div></div></div><p>
  In this scenario everything is still running (controller nodes and compute
  nodes) but you have lost either a dedicated Cloud Lifecycle Manager or a shared
  Cloud Lifecycle Manager/controller node.
 </p><p>
  To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that you previously had
  loaded on your Cloud Lifecycle Manager, you will need to download and install the
  lifecycle management software using the instructions from the
  <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 3 “Installing the Cloud Lifecycle Manager server”, Section 3.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span> before proceeding further.
 </p><div class="sect5" id="id-1.6.15.5.4.4.8.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.8.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Install the freezer-agent using the following playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="step "><p>
     Access one of the other controller or compute nodes in your environment to
     perform the following steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Retrieve the <code class="filename">/var/lib/ardana/backup.osrc</code> file and
       copy it to the <code class="filename">/var/lib/ardana/</code> directory on the
       Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       Copy all the files in the
       <code class="literal">/opt/stack/service/freezer-api/etc/</code> directory to
       the same directory on the Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       Copy all the files in the <code class="literal">/var/lib/ca-certificates</code>
       directory to the same directory on the Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       Retrieve the <code class="literal">/etc/hosts</code> file and replace the one
       found on the Cloud Lifecycle Manager.
      </p></li></ol></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the value for <code class="literal">client_id</code> in the following file to
     contain the hostname of your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/service/freezer-api/etc/freezer-api.conf</pre></div></li><li class="step "><p>
     Update your ca-certificates:
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step "><p>
     Edit the <code class="literal">/etc/hosts</code> file, ensuring you edit the
     127.0.0.1 line so it points to <code class="literal">ardana</code>:
    </p><div class="verbatim-wrap"><pre class="screen">127.0.0.1       localhost ardana
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, source the backup user credentials:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/backup.osrc</pre></div></li><li class="step "><p>
     Find the <code class="literal">Client ID</code>
     (<code class="literal">ardana002-cp1-c0-m1-mgmt</code>) for the host name as done in
     previous procedures (see <a class="xref" href="system-maintenance.html#restore-swift-ssh-bu" title="Restoring from a Swift or SSH Backup">Procedure 13.1, “Restoring from a Swift or SSH Backup”</a>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</pre></div><p>
     In this example, the <code class="literal">hostname</code> and the <code class="literal">Client
     ID</code> are the same: <code class="literal">ardana002-cp1-c0-m1-mgmt</code>.
    </p></li><li class="step "><p>
     List the Freezer jobs
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">CLIENT ID</em></pre></div><p>
     Using the example in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-list -C ardana002-cp1-c0-m1-mgmt</pre></div></li><li class="step "><p>
     Get the id of the job corresponding to <code class="literal">Ardana Default: deployer
     backup to Swift</code>. Stop that job so the freezer scheduler does not
     begin making backups when started.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-stop <em class="replaceable ">JOB-ID</em></pre></div><p>
     If it is present, also stop the Cloud Lifecycle Manager's SSH backup.
    </p></li><li class="step "><p>
     Start the freezer scheduler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start openstack-freezer-scheduler</pre></div></li><li class="step "><p>
     Get the id of the job corresponding to <code class="literal">Ardana Default: deployer
     restore from Swift</code> and launch that job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB-ID</em></pre></div><p>
     This will take some time. You can follow the progress by running
     <code class="command">tail -f /var/log/freezer/freezer-scheduler.log</code>. Wait
     until the restore job is finished before doing the next step.
    </p></li><li class="step "><p>
     When the job completes, the previous Cloud Lifecycle Manager contents should be
     restored to your home directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~
<code class="prompt user">ardana &gt; </code>ls</pre></div></li><li class="step "><p>
     If you are using Cobbler, restore your Cobbler configuration with these
     steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Remove the following files:
      </p><div class="verbatim-wrap"><pre class="screen">sudo rm -rf /var/lib/cobbler
sudo rm -rf /srv/www/cobbler</pre></div></li><li class="step "><p>
       Deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
       Set the <code class="literal">netboot-enabled</code> flag for each of your nodes
       with this command:
      </p><div class="verbatim-wrap"><pre class="screen">for h in $(sudo cobbler system list)
do
  sudo cobbler system edit --name=$h --netboot-enabled=0
done</pre></div></li></ol></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready_deployment.yml</pre></div></li><li class="step "><p>
     If you are using a dedicated Cloud Lifecycle Manager, follow these steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       re-run the deployment to ensure the Cloud Lifecycle Manager is in the correct
       state:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</pre></div></li></ol></li><li class="step "><p>
     If you are using a shared Cloud Lifecycle Manager/controller, follow these
     steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       If the node is also a Cloud Lifecycle Manager hypervisor, run the following commands to
       recreate the virtual machines that were lost:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-hypervisor-setup.yml --limit &lt;this node&gt;</pre></div></li><li class="step "><p>
       If the node that was lost (or one of the VMs that it hosts) was a member
       of the RabbitMQ cluster then you need to remove the record of the old
       node, by running the following command <span class="bold"><strong>on any one
       of the other cluster members</strong></span>. In this example the nodes are
       called <code class="literal">cloud-cp1-rmq-mysql-m*-mgmt</code> but you need to
       use the correct names for your system, which you can find in
       <code class="literal">/etc/hosts</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ssh cloud-cp1-rmq-mysql-m3-mgmt sudo rabbitmqctl forget_cluster_node \
rabbit@cloud-cp1-rmq-mysql-m1-mgmt</pre></div></li><li class="step "><p>
       Run the <code class="literal">site.yml</code> against the complete cloud to
       reinstall and rebuild the services that were lost. If you replaced one
       of the RabbitMQ cluster members then you will need to add the
       <code class="literal">-e</code> flag shown below, to nominate a new master node
       for the cluster, otherwise you can omit it.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml -e \
rabbit_primary_hostname=cloud-cp1-rmq-mysql-m3</pre></div></li></ol></li></ol></div></div></div><div class="sect5" id="id-1.6.15.5.4.4.8.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.8.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the Cloud Lifecycle Manager, edit the following file so it contains the same
     information as it did previously:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/openstack/my_cloud/config/freezer/ssh_credentials.yml</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, copy the following files, change directories,
     and run the playbook _deployer_restore_helper.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/hp-ci/openstack/* ~/openstack/my_cloud/definition/
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</pre></div></li><li class="step "><p>
     Perform the restore. First become root and change directories:
    </p><div class="verbatim-wrap"><pre class="screen">sudo su
<code class="prompt user">root # </code>cd /root/deployer_restore_helper/</pre></div></li><li class="step "><p>
     Execute the restore job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./deployer_restore_script.sh</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready_deployment.yml</pre></div></li><li class="step "><p>
     When the Cloud Lifecycle Manager is restored, re-run the deployment to ensure
     the Cloud Lifecycle Manager is in the correct state:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</pre></div></li></ol></div></div></div></div><div class="sect4" id="onetwo-controller-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">One or Two Controller Node Disaster Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#onetwo-controller-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-onetwo_controller_recovery.xml</li><li><span class="ds-label">ID: </span>onetwo-controller-recovery</li></ul></div></div></div></div><p>
  This scenario makes the following assumptions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Your Cloud Lifecycle Manager is still intact and working.
   </p></li><li class="listitem "><p>
    One or two of your controller nodes went down, but not the entire cluster.
   </p></li><li class="listitem "><p>
    The node needs to be rebuilt from scratch, not simply rebooted.
   </p></li></ul></div><div class="sect5" id="id-1.6.15.5.4.4.9.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps to recovering one or two controller nodes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.9.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-onetwo_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that your node has power and all of the hardware is functioning.
    </p></li><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Verify that all of the information in your
     <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file is
     correct for your controller node. You may need to replace the existing
     information if you had to either replacement your entire controller node
     or just pieces of it.
    </p></li><li class="listitem "><p>
     If you made changes to your <code class="literal">servers.yml</code> file then
     commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "editing controller information"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Ensure that Cobbler has the correct system information:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       If you replaced your controller node with a completely new machine, you
       need to verify that Cobbler has the correct list of controller nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem "><p>
       Remove any controller nodes from Cobbler that no longer exist:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name=&lt;node&gt;</pre></div></li><li class="listitem "><p>
       Add the new node into Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></li><li class="listitem "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.6.15.5.4.4.9.4.2.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      If you do not know the <code class="literal">&lt;node name&gt;</code> already,
      you can get it by using <code class="command">sudo cobbler system list</code>.
     </p></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See the Persisted Server Allocations section in for
     information on how this works.
    </p></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;controller_node_hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the rebuilding of your controller node with the two playbooks
     below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="three-controller-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Three Control Plane Node Disaster Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#three-controller-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span>three-controller-recovery</li></ul></div></div></div></div><p>
  In this scenario, all control plane nodes are destroyed which need to be
  rebuilt or replaced.
 </p><div class="sect5" id="id-1.6.15.5.4.4.10.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a Swift backup:</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.10.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Restoring from a Swift backup is not possible because Swift is gone.
  </p></div><div class="sect5" id="id-1.6.15.5.4.4.10.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.10.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Disable the default backup job(s) by editing the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible/roles/freezer-jobs/defaults/activate.yml</pre></div><p>
     Set the value for <code class="literal">freezer_create_backup_jobs</code> to
     <code class="literal">false</code>:
    </p><div class="verbatim-wrap"><pre class="screen"># If set to false, We won't create backups jobs.
freezer_create_backup_jobs: false</pre></div></li><li class="listitem "><p>
     Deploy the control plane nodes, using the values for your control plane
     node hostnames:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit \
  <em class="replaceable ">CONTROL_PLANE_HOSTNAME1</em>,<em class="replaceable ">CONTROL_PLANE_HOSTNAME2</em>, \
  <em class="replaceable ">CONTROL_PLANE_HOSTNAME3</em> -e rebuild=True</pre></div><p>
     For example, if you were using the default values from the example model
     files your command would look like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml \
    --limit ardana-ccp-c1-m1-mgmt,ardana-ccp-c1-m2-mgmt,ardana-ccp-c1-m3-mgmt \
    -e rebuild=True</pre></div><div id="id-1.6.15.5.4.4.10.4.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">-e rebuild=True</code> is only used on a single control
      plane node when there are other controllers available to pull
      configuration data
      from. This will cause the MariaDB database to be reinitialized, which is
      the only choice if there are no additional control nodes.
     </p></div></li><li class="listitem "><p>
     Restore the MariaDB backup on the first controller node.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       List the Freezer jobs:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>freezer job-list -C <em class="replaceable ">FIRST_CONTROLLER_NODE</em></pre></div></li><li class="listitem "><p>
       Run the <code class="literal">Ardana Default: mysql restore from SSH</code> job for your first
       controller node, replacing the <code class="literal">JOB_ID</code> for that job:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>freezer job-start <em class="replaceable ">JOB_ID</em></pre></div></li></ol></div></li><li class="listitem "><p>
     You can monitor the restore job by connecting to your first controller
     node via SSH and running the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh <em class="replaceable ">FIRST_CONTROLLER_NODE</em>
<code class="prompt user">ardana &gt; </code>sudo su
<code class="prompt user">root # </code>tail -n 100 /var/log/freezer/freezer-scheduler.log</pre></div></li><li class="listitem "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Stop MySQL:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="listitem "><p>
     Log back in to the first controller node and move the following files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh <em class="replaceable ">FIRST_CONTROLLER_NODE</em>
<code class="prompt user">ardana &gt; </code>sudo su
<code class="prompt user">root # </code>rm -rf /var/lib/mysql/*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* /var/lib/mysql/</pre></div></li><li class="listitem "><p>
     Log back in to the Cloud Lifecycle Manager and bootstrap MySQL:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="listitem "><p>
     Verify the status of MySQL:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div></li><li class="listitem "><p>
     Re-enable the default backup job(s) by editing the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/freezer-jobs/defaults/activate.yml</pre></div><p>
     Set the value for <code class="literal">freezer_create_backup_jobs</code> to
     <code class="literal">true</code>:
    </p><div class="verbatim-wrap"><pre class="screen"># If set to false, We won't create backups jobs.
freezer_create_backup_jobs: true</pre></div></li><li class="listitem "><p>
     Run this playbook to deploy the backup jobs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _freezer_manage_jobs.yml</pre></div></li></ol></div></div></div><div class="sect4" id="swiftrings-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.2.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Rings Recovery</span> <a title="Permalink" class="permalink" href="system-maintenance.html#swiftrings-recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>swiftrings-recovery</li></ul></div></div></div></div><p>
  To recover your Swift rings in the event of a disaster, follow the procedure
  that applies to your situation: either recover the rings from one Swift node
  if possible, or use the SSH backup that you have set up.
 </p><p>
  To recover your Swift rings in the event of a disaster, follow the procedure
  that applies to your situation: either recover the rings from one Swift node
  if possible, or use the SSH backup that you have set up.
 </p><div class="sect5" id="id-1.6.15.5.4.4.11.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from the Swift deployment backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.11.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   See <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
  </p></div><div class="sect5" id="id-1.6.15.5.4.4.11.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.2.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from the SSH Freezer backup</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.5.4.4.11.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In the very specific use case where you lost all system disks of all object
   nodes, and Swift proxy nodes are corrupted, you can recover the rings
   because a copy of the Swift rings is stored in Freezer. This means that
   Swift data is still there (the disks used by Swift needs to be still
   accessible).
  </p><p>
   Recover the rings with these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to a node that has the freezer-agent installed.
    </p></li><li class="listitem "><p>
     Become root:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo su</pre></div></li><li class="listitem "><p>
     Create the temporary directory to restore your files to:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /tmp/swift_builder_dir_restore/</pre></div></li><li class="listitem "><p>
     Create a restore file with the following content:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &lt;&lt; EOF &gt; ./restore_config.ini
[default]
action = restore
storage = ssh
compression = bzip2
restore_abs_path = /tmp/swift_builder_dir_restore/
ssh_key = /etc/freezer/ssh_key
ssh_host = &lt;freezer_ssh_host&gt;
ssh_port = &lt;freezer_ssh_port&gt;
ssh_user name = &lt;freezer_ssh_user name&gt;
container = &lt;freezer_ssh_base_rid&gt;/freezer_swift_backup_name = freezer_swift_builder_backup
hostname = &lt;hostname of the old first Swift-Proxy (SWF-PRX[0])&gt;
EOF</pre></div></li><li class="listitem "><p>
     Edit the file and replace all &lt;tags&gt; with the right information.
    </p><div class="verbatim-wrap"><pre class="screen">vim ./restore_config.ini</pre></div><p>
     You will also need to put the SSH key used to do the backups in
     /etc/freezer/ssh_key and remember to set the right permissions: 600.
    </p></li><li class="listitem "><p>
     Execute the restore job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>freezer-agent --config ./restore_config.ini</pre></div><p>
     You now have the Swift rings in
     <code class="literal">/tmp/swift_builder_dir_restore/</code>
    </p></li><li class="listitem "><p>
     If the SWF-PRX[0] is already deployed, copy the contents of the restored
     directory (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>) to
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code> on the SWF-PRX[0] Then from
     the Cloud Lifecycle Manager run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     If the SWF-ACC[0] is<span class="bold"><strong> not </strong></span>deployed, from
     the Cloud Lifecycle Manager run these playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;SWF-ACC[0]-hostname&gt;</pre></div></li><li class="listitem "><p>
     Copy the contents of the restored directory
     (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>) to
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code> on the SWF-ACC[0] You will
     have to create the directories :
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="listitem "><p>
     From the Cloud Lifecycle Manager, run the <code class="filename">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="unplanned-compute-maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Compute Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#unplanned-compute-maintenance">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute_maintenance.xml</li><li><span class="ds-label">ID: </span>unplanned-compute-maintenance</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks including recovering compute nodes.
 </p><div class="sect3" id="recover-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering a Compute Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#recover-computenode">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>recover-computenode</li></ul></div></div></div></div><p>
  If one or more of your compute nodes has experienced an issue such as power
  loss or hardware failure, then you need to perform disaster recovery. Here we
  provide different scenarios and how to resolve them to get your cloud
  repaired.
 </p><p>
  Typical scenarios in which you will need to recover a compute node include
  the following:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The node has failed, either because it has shut down has a hardware
    failure, or for another reason.
   </p></li><li class="listitem "><p>
    The node is working but the <code class="literal">nova-compute</code> process is not
    responding, thus instances are working but you cannot manage them (for
    example to delete, reboot, and attach/detach volumes).
   </p></li><li class="listitem "><p>
    The node is fully operational but monitoring indicates a potential issue
    (such as disk errors) that require down time to fix.
   </p></li></ul></div><div class="sect4" id="idg-all-operations-maintenance-compute-recover-compute-node-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What to do if your compute node is down</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-recover-compute-node-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-recover-compute-node-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Compute node has power but is not powered
   on</strong></span>
  </p><p>
   If your compute node has power but is not powered on, use these steps to
   restore the node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Obtain the name for your compute node in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Compute node is powered on but services are not
   running on it</strong></span>
  </p><p>
   If your compute node is powered on but you are unsure if services are
   running, you can use these steps to ensure that they are running:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Confirm the status of the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-status.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     You can start the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div><div class="sect4" id="unplanned"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scenarios involving disk failures on your compute nodes</span> <a title="Permalink" class="permalink" href="system-maintenance.html#unplanned">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>unplanned</li></ul></div></div></div></div><p>
   Your compute nodes should have a minimum of two disks, one that is used for
   the operating system and one that is used as the data disk. These are
   defined during the installation of your cloud, in the
   <code class="literal">~/openstack/my_cloud/definition/data/disks_compute.yml</code> file
   on the Cloud Lifecycle Manager. The data disk(s) are where the
   <code class="literal">nova-compute</code> service lives. Recovery scenarios will
   depend on whether one or the other, or both, of these disks experienced
   failures.
  </p><p>
   <span class="bold"><strong>If your operating system disk failed but the data
   disk(s) are okay</strong></span>
  </p><p>
   If you have had issues with the physical volume that nodes your operating
   system you need to ensure that your physical volume is restored and then you
   can use the following steps to restore the operating system:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="listitem "><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-list | grep compute</pre></div></li><li class="listitem "><p>
     Obtain the status of the <code class="literal">nova-compute</code> service on that
     node:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list --host &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     You will likely want to disable provisioning on that node to ensure that
     <code class="literal">nova-scheduler</code> does not attempt to place any additional
     instances on the node while you are repairing it:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-disable --reason "node is being rebuilt" &lt;hostname&gt; nova-compute</pre></div></li><li class="listitem "><p>
     Obtain the status of the instances on the compute node:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
     Before continuing, you should either evacuate all of the instances off
     your compute node or shut them down. If the instances are booted from
     volumes, then you can use the <code class="literal">nova evacuate</code> or
     <code class="literal">nova host-evacuate</code> commands to do this. See
     <a class="xref" href="system-maintenance.html#liveInstMigration" title="13.1.3.3. Live Migration of Instances">Section 13.1.3.3, “Live Migration of Instances”</a> for more details on how to do this.
    </p><p>
     If your instances are not booted from volumes, you will need to stop the
     instances using the <code class="literal">nova stop</code> command. Because the
     <code class="literal">nova-compute</code> service is not running on the node you
     will not see the instance status change, but the <code class="literal">Task
     State</code> for the instance should change to
     <code class="literal">powering-off</code>.
    </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance_uuid&gt;</pre></div><p>
     Verify the status of each of the instances using these commands, verifying
     the <code class="literal">Task State</code> states <code class="literal">powering-off</code>:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova show &lt;instance_uuid&gt;</pre></div></li><li class="listitem "><p>
     At this point you should be ready with a functioning hard disk in the node
     that you can use for the operating system. Follow these steps:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Obtain the name for your compute node in Cobbler, which you will use in
       subsequent commands when <code class="literal">&lt;node_name&gt;</code> is
       requested:
      </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
       Reimage the compute node with this playbook:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div></li><li class="listitem "><p>
     Once reimaging is complete, use the following playbook to configure the
     operating system and start up services:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     You should then ensure any instances on the recovered node are in an
     <code class="literal">ACTIVE</code> state. If they are not then use the
     <code class="literal">nova start</code> command to bring them to the
     <code class="literal">ACTIVE</code> state:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova start &lt;instance_uuid&gt;</pre></div></li><li class="listitem "><p>
     Reenable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-enable &lt;hostname&gt; nova-compute</pre></div></li><li class="listitem "><p>
     Start any instances that you had stopped previously:
    </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova start &lt;instance_uuid&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>If your data disk(s) failed but the operating system
   disk is okay OR if all drives failed</strong></span>
  </p><p>
   In this scenario your instances on the node are lost. First, follow steps 1
   to 5 and 8 to 9 in the previous scenario.
  </p><p>
   After that is complete, use the <code class="literal">nova rebuild</code> command to
   respawn your instances, which will also ensure that they receive the same IP
   address:
  </p><div class="verbatim-wrap"><pre class="screen">nova list --host &lt;hostname&gt; --all-tenants
nova rebuild &lt;instance_uuid&gt;</pre></div></div></div></div><div class="sect2" id="storage-unplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Storage Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#storage-unplanned">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-storage_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-storage_unplanned.xml</li><li><span class="ds-label">ID: </span>storage-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for storage nodes.
 </p><div class="sect3" id="swift-storage-unplanned"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Swift Storage Maintenance</span> <a title="Permalink" class="permalink" href="system-maintenance.html#swift-storage-unplanned">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-swift_storage_unplanned.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-swift_storage_unplanned.xml</li><li><span class="ds-label">ID: </span>swift-storage-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for Swift storage nodes.
 </p><div class="sect4" id="recover-swiftnode"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering a Swift Node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#recover-swiftnode">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>recover-swiftnode</li></ul></div></div></div></div><p>
  If one or more of your Swift Object or PAC nodes has experienced an issue,
  such as power loss or hardware failure, and you need to perform disaster
  recovery then we provide different scenarios and how to resolve them to get
  your cloud repaired.
 </p><p>
  Typical scenarios in which you will need to repair a Swift object or PAC
  node include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The node has either shut down or been rebooted.
   </p></li><li class="listitem "><p>
    The entire node has failed and needs to be replaced.
   </p></li><li class="listitem "><p>
    A disk drive has failed and must be replaced.
   </p></li></ul></div><div class="sect5" id="idg-all-operations-maintenance-swift-recover-swift-node-xml-5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.4.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What to do if your Swift host has shut down or rebooted</span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-swift-recover-swift-node-xml-5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-recover-swift-node-xml-5</li></ul></div></div></div></div><p>
   If your Swift host has power but is not powered on, from the lifecycle
   manager you can run this playbook:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Obtain the name for your Swift host in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div><p>
   Once the node is booted up, Swift should start automatically. You can verify
   this with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div><p>
   Any alarms that have triggered due to the host going down should clear
   within 10 minutes. See <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> if further
   assistance is needed with the alarms.
  </p></div><div class="sect5" id="replace"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.4.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace your Swift node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>replace</li></ul></div></div></div></div><p>
   If your Swift node has irreparable damage and you need to replace the entire
   node in your environment, see <a class="xref" href="system-maintenance.html#replace-swift-node" title="13.1.5.1.5. Replacing a Swift Node">Section 13.1.5.1.5, “Replacing a Swift Node”</a> for
   details on how to do this.
  </p></div><div class="sect5" id="disk-replacement"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.2.4.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace a hard disk in your Swift node</span> <a title="Permalink" class="permalink" href="system-maintenance.html#disk-replacement">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>disk-replacement</li></ul></div></div></div></div><p>
   If you need to do a hard drive replacement in your Swift node, see
   <a class="xref" href="system-maintenance.html#replacing-disks" title="13.1.5.1.6. Replacing Drives in a Swift Node">Section 13.1.5.1.6, “Replacing Drives in a Swift Node”</a> for details on how to do this.
  </p></div></div></div></div></div><div class="sect1" id="maintenance-update"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span> <a title="Permalink" class="permalink" href="system-maintenance.html#maintenance-update">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span>maintenance-update</li></ul></div></div></div></div><div class="procedure " id="id-1.6.15.6.2"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 13.2: </span><span class="name">Preparing for Update </span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.6.2">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Ensure that the update repositories have been properly set up on all nodes.
    The easiest way to provide the required repositories on the Cloud Lifecycle Manager Server is
    to set up an SMT server as described in
    <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 4 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”</span>. Alternatives to setting up an
    SMT server are described in <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 5 “Software Repository Setup”</span>.
   </p></li><li class="step "><p>
    Read the Release Notes for the security and maintenance updates that will
    be installed.
   </p></li><li class="step "><p>
    Have a backup strategy in place. For further information, see
    <a class="xref" href="bura-overview.html" title="Chapter 14. Backup and Restore">Chapter 14, <em>Backup and Restore</em></a>.
   </p></li><li class="step "><p>
    Ensure that you have a known starting state by resolving any unexpected
    alarms.
   </p></li><li class="step "><p>
    Determine if you need to reboot your cloud after updating the software.
    Rebooting is highly recommended to ensure that all affected services are
    restarted. Reboot may be required after installing Linux kernel updates,
    but it can be skipped if the impact on running services is non-existent or
    well understood.
   </p></li><li class="step "><p>
    Review steps in <a class="xref" href="system-maintenance.html#add-network-node" title="13.1.4.1. Adding a Neutron Network Node">Section 13.1.4.1, “Adding a Neutron Network Node”</a> and
    <a class="xref" href="system-maintenance.html#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a> to minimize the impact on existing
    workloads. These steps are critical when the Neutron services are not
    provided via external SDN controllers.
   </p></li><li class="step "><p>
    Before the update, prepare your working loads by consolidating all of your
    instances to one or more Compute Nodes. After the update is complete on the
    324
    evacuated Compute Nodes, reboot them and move the images from the remaining
    Compute Nodes to the newly booted ones. Then, update the remaining
    Compute Nodes.
   </p></li></ol></div></div><div class="sect2" id="perform-update"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing the Update</span> <a title="Permalink" class="permalink" href="system-maintenance.html#perform-update">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span>perform-update</li></ul></div></div></div></div><p>
     Before you proceed, get the status of all your services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div><p>
     If status check returns an error for a specific service, run the
     <code class="filename"><em class="replaceable ">SERVICE</em>-reconfigure.yml</code>
     playbook. Then run the
     <code class="filename"><em class="replaceable ">SERVICE</em>-status.yml</code>
     playbook to check that the issue has been resolved.
    </p><p>
     Update and reboot all nodes in the cloud one by one. Start with the
     deployer node, then follow the order recommended in
     <a class="xref" href="system-maintenance.html#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>.
    </p><div id="id-1.6.15.6.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The described workflow also covers cases in which the deployer node is
      also provisioned as an active cloud node.
     </p></div><p>
     To minimize the impact on the existing workloads, the node should first be
     prepared for an update and a subsequent reboot by following the steps
     leading up to stopping services listed in
     <a class="xref" href="system-maintenance.html#rebootNodes" title="13.1.1.2. Rolling Reboot of the Cloud">Section 13.1.1.2, “Rolling Reboot of the Cloud”</a>, such as migrating singleton agents on
     Control Nodes and evacuating Compute Nodes. Do not stop services running on
     the node, as they need to be running during the update.
    </p><div class="procedure " id="id-1.6.15.6.3.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 13.3: </span><span class="name">Update Instructions </span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.6.3.8">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Install all available security and maintenance updates on the deployer
       using the <code class="command">zypper patch</code> command.
      </p></li><li class="step "><p>
       Initialize the Cloud Lifecycle Manager and prepare the update playbooks.
      </p><ol type="a" class="substeps "><li class="step "><p>
         Run the <code class="systemitem">ardana-init</code> initialization script to
         update the deployer.
        </p></li><li class="step "><p>
         Redeploy cobbler:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
         Run the configuration processor:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
         Update your deployment directory:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step "><p>
       Installation and management of updates can be automated with the
       following playbooks:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <code class="filename">ardana-update-pkgs.yml</code>
        </p></li><li class="listitem "><p>
         <code class="filename">ardana-update.yml</code>
        </p></li><li class="listitem "><p>
         <code class="filename">ardana-update-status.yml</code>
        </p><div id="id-1.6.15.6.3.8.4.2.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
          Some playbooks are being deprecated. To determine how your system is
          affected, run:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>rpm -qa ardana-ansible</pre></div><p>
          The result will be <code class="literal">ardana-ansible-8.0+git.</code>
          followed by a version number string.
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            If the first part of the version number string is greater than or
            equal to 1553878455 (for example, ardana-ansible-8.0+git.<span class="bold"><strong>1553878455</strong></span>.7439e04), use the newly
            introduced parameters:
           </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <code class="literal">pending_clm_update</code>
             </p></li><li class="listitem "><p>
              <code class="literal">pending_service_update</code>
             </p></li><li class="listitem "><p>
              <code class="literal">pending_system_reboot</code>
             </p></li></ul></div></li><li class="listitem "><p>
            If the first part of the version number string is less than 1553878455
            (for example, ardana-ansible-8.0+git.<span class="bold"><strong>1552032267</strong></span>.5298d45), use the following
            parameters:
           </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <code class="literal">update_status_var</code>
             </p></li><li class="listitem "><p>
              <code class="literal">update_status_set</code>
             </p></li><li class="listitem "><p>
              <code class="literal">update_status_reset</code>
             </p></li></ul></div></li></ul></div></div></li><li class="listitem "><p>
         <code class="filename">ardana-reboot.yml</code>
        </p></li></ul></div></li><li class="step "><p>
       Confirm version changes by running <code class="literal">hostnamectl</code>
       before and after running the <code class="literal">ardana-update-pkgs</code>
       playbook on each node.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>hostnamectl</pre></div><p>
       Notice that the <code class="literal">Boot ID:</code> and
       <code class="literal">Kernel:</code> information has changed.
      </p></li><li class="step "><p>
       By default, the <code class="filename">ardana-update-pkgs.yml</code> playbook
       will install patches and updates that do not require a system
       reboot. Patches and updates that <span class="bold"><strong>do</strong></span>
       require a system reboot will be installed later in this process.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div><p>
       There may be a delay in the playbook output at the following task while
       updates are pulled from the deployer.
      </p><div class="verbatim-wrap"><pre class="screen">TASK: [ardana-upgrade-tools | pkg-update | Download and install
package updates] ***</pre></div></li><li class="step "><p>
       After running the <code class="filename">ardana-update-pkgs.yml</code> playbook
       to install patches and updates not requiring reboot, check the status of
       remaining tasks.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       To install patches that require reboot, run the
       <code class="filename">ardana-update-pkgs.yml</code> playbook with the parameter
       <code class="literal">-e zypper_update_include_reboot_patches=true</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit  <em class="replaceable ">TARGET_NODE_NAME</em> \
-e zypper_update_include_reboot_patches=true</pre></div><p>
	If the output of <code class="filename">ardana-update-pkgs.yml</code> indicates 
	that a reboot is required, run <code class="filename">ardana-reboot.yml</code> 
	<span class="emphasis"><em>after</em></span> completing the <code class="filename">ardana-update.yml</code> 
	step below. Running <code class="filename">ardana-reboot.yml</code>
	will cause cloud service interruption.
      </p><div id="id-1.6.15.6.3.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
        To update a single package (for example, apply a PTF on a single node
        or on all nodes), run <code class="command">zypper update
        <em class="replaceable ">PACKAGE</em></code>.
       </p><p>
        To install all package updates using <code class="command">zypper update</code>.
       </p></div></li><li class="step "><p>
       Update services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       If indicated by the <code class="filename">ardana-update-status.yml</code>
       playbook, reboot the node.
      </p><p>
       There may also be a warning to reboot after running the
       <code class="filename">ardana-update-pkgs.yml</code>.
      </p><p>
       This check can be overridden by setting the
       <code class="literal">SKIP_UPDATE_REBOOT_CHECKS</code> environment variable or the
       <code class="literal">skip_update_reboot_checks</code> Ansible variable.
        </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-reboot.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       To recheck pending system reboot status at a later time, run the
       following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2</pre></div></li><li class="step "><p>
       The pending system reboot status can be reset by running:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2 \
-e pending_system_reboot=off</pre></div></li><li class="step "><p>
       Multiple servers can be patched at the same time with
       <code class="filename">ardana-update-pkgs.yml</code> by setting the option
       <code class="literal">-e skip_single_host_checks=true</code>.
      </p><div id="id-1.6.15.6.3.8.13.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
        When patching multiple servers at the same time, take care not to
        compromise HA capability by updating an entire cluster (controller,
        database, monitor, logging) at the same time.
       </p></div><p>
       If multiple nodes are specified on the command line (with
       <code class="literal">--limit</code>), services on those servers will experience
       outages as the packages are shutdown and updated.  On Compute Nodes (or
       group of Compute Nodes) migrate the workload off if you plan to update
       it. The same applies to Control Nodes: move singleton services off of the
       control plane node that will be updated.
      </p><div id="id-1.6.15.6.3.8.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
        Do not reboot all of your controllers at the same time.
       </p></div></li><li class="step "><p>
       When the node comes up after the reboot, run the
      <code class="filename">spark-start.yml</code> file:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml</pre></div></li><li class="step "><p>
       Verify that Spark is running on all Control Nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-status.yml</pre></div></li><li class="step "><p>
       After all nodes have been updated, check the status of all services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.6.15.6.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Summary of the Update Playbooks</span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.6.15.6.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.6.15.6.4.2.1"><span class="term ">ardana-update-pkgs.yml</span></dt><dd><p>
      Top-level playbook automates the installation of package updates on
      a single node. It also works for multiple nodes, if the single-node
      restriction is overridden by setting the SKIP_SINGLE_HOST_CHECKS
      environment variable <code class="literal">ardana-update-pkgs.yml -e
      skip_single_host_checks=true</code>.
     </p><p>
      Provide the following <code class="literal">-e</code> options to modify default
      behavior:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">zypper_update_method</code> (default: patch)
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">patch</code> will install all patches for the
          system. Patches are intended for specific bug and security fixes.
         </p></li><li class="listitem "><p>
          <code class="literal">update</code> will install all packages that have a
          higher version number than the installed packages.
         </p></li><li class="listitem "><p>
          <code class="literal">dist-upgrade</code> replaces each package installed with
          the version from the repository and deletes packages not available in
          the repositories.
         </p></li></ul></div></li><li class="listitem "><p>
        <code class="literal">zypper_update_repositories</code> (default: all) restricts
        the list of repositories used
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_gpg_checks</code> (default: true) enables GPG
        checks. If set to <code class="literal">true</code>, checks if packages are
        correctly signed.
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_licenses_agree</code> (default: false)
        automatically agrees with licenses. If set to <code class="literal">true</code>,
        zypper automatically accepts third party licenses.
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_include_reboot_patches</code> (default:
        false) includes patches that require reboot. Setting this to
        <code class="literal">true</code> installs patches that require a reboot (such as
        kernel or glibc updates).
       </p></li></ul></div></dd><dt id="id-1.6.15.6.4.2.2"><span class="term ">ardana-update.yml</span></dt><dd><p>
      Top level playbook that automates the update of all the services. Runs
      on all nodes by default, or can be limited to a single node by adding
      <code class="literal">--limit <em class="replaceable ">nodename</em></code>.
     </p></dd><dt id="id-1.6.15.6.4.2.3"><span class="term ">ardana-reboot.yml</span></dt><dd><p>
      Top-level playbook that automates the steps required to reboot a node. It
      includes pre-boot and post-boot phases, which can be extended to include
      additional checks.
     </p></dd><dt id="id-1.6.15.6.4.2.4"><span class="term ">ardana-update-status.yml</span></dt><dd><p>
      This playbook can be used to check or reset the update-related status
      variables maintained by the update playbooks. The main reason for having
      this mechanism is to allow the update status to be checked at any point
      during the update procedure. It is also used heavily by the automation
      scripts to orchestrate installing maintenance updates on multiple nodes.
     </p></dd></dl></div></div></div><div class="sect1" id="deploy-ptf"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span> <a title="Permalink" class="permalink" href="system-maintenance.html#deploy-ptf">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-deploy-ptf.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-deploy-ptf.xml</li><li><span class="ds-label">ID: </span>deploy-ptf</li></ul></div></div></div></div><p>
  Occasionally, in order to fix a given issue, SUSE will provide a set of
  packages known as a Program Temporary Fix (PTF). Such a PTF is fully
  supported by SUSE until the Maintenance Update containing a permanent fix has
  been released via the regular Update repositories. Customers running PTF
  fixes will be notified through the related Service Request when a permanent
  patch for a PTF has been released.
 </p><p>
  Use the following steps to deploy a PTF:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    When SUSE has developed a PTF, you will receive a URL for that PTF. You
    should download the packages from the location provided by SUSE Support
    to a temporary location on the Cloud Lifecycle Manager. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>tmpdir=`mktemp -d`
<code class="prompt user">ardana &gt; </code>cd $tmpdir
<code class="prompt user">ardana &gt; </code>sudo wget --no-directories --recursive --reject "index.html*"\
--user=<em class="replaceable ">USER_NAME</em> \
--password=<em class="replaceable ">PASSWORD</em> \
--no-parent https://ptf.suse.com/54321aaaa...dddd12345/cloud8/042171/x86_64/20181030</pre></div></li><li class="step "><p>
    Remove any old data from the PTF repository, such as a listing for a PTF
    repository from a migration or when previous product patches were
    installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -rf /srv/www/suse-12.3/x86_64/repos/PTF/*</pre></div></li><li class="step "><p>
    Move packages from the temporary download location to the PTF repository
    directory on the CLM Server. This example is for a Neutron PTF.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo mkdir -p /srv/www/suse-12.3/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo mv $tmpdir/*
   /srv/www/suse-12.3/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo chown --recursive root:root /srv/www/suse-12.3/x86_64/repos/PTF/*
<code class="prompt user">ardana &gt; </code>rmdir $tmpdir</pre></div></li><li class="step "><p>
    Create or update the repository metadata:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo /usr/local/sbin/createrepo-cloud-ptf
Spawning worker 0 with 2 pkgs
Workers Finished
Saving Primary metadata
Saving file lists metadata
Saving other metadata</pre></div></li><li class="step "><p>
    Refresh the PTF repository before installing package updates on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper refresh --force --repo PTF
Forcing raw metadata refresh
Retrieving repository 'PTF' metadata
..........................................[d
one]
Forcing building of repository cache
Building repository 'PTF' cache ..........................................[done]
Specified repositories have been refreshed.</pre></div></li><li class="step "><p>
    The PTF shows as available on the deployer.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --repo PTF
Loading repository data...
Reading installed packages...

S | Name                          | Summary                                 | Type
--+-------------------------------+-----------------------------------------+--------
  | python-neutronclient          | Python API and CLI for OpenStack Neutron | package
i | venv-openstack-neutron-x86_64 | Python virtualenv for OpenStack Neutron | package</pre></div></li><li class="step "><p>
    Install the PTF venv packages on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper dup  --from PTF
Refreshing service
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

The following package is going to be upgraded:
  venv-openstack-neutron-x86_64

The following package has no support information from its vendor:
  venv-openstack-neutron-x86_64

1 package to upgrade.
Overall download size: 64.2 MiB. Already cached: 0 B. After the operation, additional 6.9 KiB will be used.
Continue? [y/n/...? shows all options] (y): y
Retrieving package venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ... (1/1),  64.2 MiB ( 64.6 MiB unpacked)
Retrieving: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm ....[done]
Checking for file conflicts: ..............................................................[done]
(1/1) Installing: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ....[done]
Additional rpm output:
warning
warning: /var/cache/zypp/packages/PTF/noarch/venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm: Header V3 DSA/SHA1 Signature, key ID b37b98a9: NOKEY</pre></div></li><li class="step "><p>
    Validate the venv tarball has been installed into the deployment directory:(note:the packages file under that dir shows the registered tarballs that will be used for the services, which should align with the installed venv RPM)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/ardana_packager/ardana-8/sles_venv/x86_64
total 898952
drwxr-xr-x 2 root root     4096 Oct 30 16:10 .
...
-rw-r--r-- 1 root root 67688160 Oct 30 12:44 neutron-20181030T124310Z.tgz &lt;&lt;&lt;
-rw-r--r-- 1 root root 64674087 Aug 14 16:14 nova-20180814T161306Z.tgz
-rw-r--r-- 1 root root 45378897 Aug 14 16:09 octavia-20180814T160839Z.tgz
-rw-r--r-- 1 root root     1879 Oct 30 16:10 packages
-rw-r--r-- 1 root root 27186008 Apr 26  2018 swift-20180426T230541Z.tgz</pre></div></li><li class="step "><p>
    Install the non-venv PTF packages on the Compute Node
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml --extra-vars '{"zypper_update_method": "update", "zypper_update_repositories": ["PTF"]}' --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that the upgraded package
    has been installed on <code class="literal">comp0001-mgmt</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --detail python-neutronclient
Loading repository data...
Reading installed packages...

S | Name                 | Type     | Version                         | Arch   | Repository
--+----------------------+----------+---------------------------------+--------+--------------------------------------
i | python-neutronclient | package  | 6.5.1-4.361.042171.0.PTF.102473 | noarch | PTF
  | python-neutronclient | package  | 6.5.0-4.361                     | noarch | SUSE-OPENSTACK-CLOUD-x86_64-GM-DVD1</pre></div></li><li class="step "><p>
    Running the ardana update playbook will distribute the PTF venv packages to
    the cloud server. Then you can find them loaded in the virtual environment
    directory with the other venvs.
   </p><p>
    The Compute Node before running the update playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 24
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step "><p>
    Run the update.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that an additional virtual environment
    has been installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 28
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x  9 root root 4096 Oct 30 12:43 neutron-20181030T124310Z &lt;&lt;&lt; New venv installed
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step "><p>
    The PTF may also have <code class="literal">RPM</code> package updates in addition to
    venv updates. To complete the update, follow the instructions at <a class="xref" href="system-maintenance.html#perform-update" title="13.3.1. Performing the Update">Section 13.3.1, “Performing the Update”</a>.
   </p></li></ol></div></div></div><div class="sect1" id="database-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Periodic OpenStack Maintenance Tasks</span> <a title="Permalink" class="permalink" href="system-maintenance.html#database-maintenance">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-maintenance-database_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-database_maintenance.xml</li><li><span class="ds-label">ID: </span>database-maintenance</li></ul></div></div></div></div><p>
    Heat-manage helps manage Heat specific database operations. The associated
    database should be periodically purged to save space. The following should
    be setup as a cron job on the servers where the heat service is running at
    <code class="literal">/etc/cron.weekly/local-cleanup-heat</code>
    with the following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su heat -s /bin/bash -c "/usr/bin/heat-manage purge_deleted -g days 14" || :</pre></div><p>
     nova-manage db archive_deleted_rows command will move deleted rows
     from production tables to shadow tables. Including
     <code class="literal">--until-complete</code> will make the command run continuously
     until all deleted rows are archived. It is recommended to setup this task
     as <code class="literal">/etc/cron.weekly/local-cleanup-nova</code>
     on the servers where the nova service is running, with the
     following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su nova -s /bin/bash -c "/usr/bin/nova-manage db archive_deleted_rows --until-complete" || :</pre></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="bura-overview.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 14 </span>Backup and Restore</span></a><a class="nav-link" href="topic-ttn-5fg-4v.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 12 </span>Managing Monitoring, Logging, and Usage Reporting</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>