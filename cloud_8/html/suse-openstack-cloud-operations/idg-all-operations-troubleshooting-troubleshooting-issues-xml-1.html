<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Troubleshooting Issues | Operations Guide | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.2.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.81.0 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="Operations Guide" /><meta name="chapter-title" content="Chapter 15. Troubleshooting Issues" /><meta name="description" content="Troubleshooting and support processes for solving issues in your environment." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" /><link rel="home" href="index.html" title="Documentation" /><link rel="up" href="book-operations.html" title="Operations Guide" /><link rel="prev" href="bura-overview.html" title="Chapter 14. Backup and Restore" /><link rel="next" href="book-user.html" title="User Guide" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.svg" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Operations Guide"><span class="book-icon">Operations Guide</span></a><span> › </span><a class="crumb" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html">Troubleshooting Issues</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Operations Guide</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="gettingstarted-ops.html"><span class="number">1 </span><span class="name">Operations Overview</span></a></li><li class="inactive"><a href="tutorials.html"><span class="number">2 </span><span class="name">Tutorials</span></a></li><li class="inactive"><a href="third-party-integrations.html"><span class="number">3 </span><span class="name">Third-Party Integrations</span></a></li><li class="inactive"><a href="ops-managing-identity.html"><span class="number">4 </span><span class="name">Managing Identity</span></a></li><li class="inactive"><a href="ops-managing-compute.html"><span class="number">5 </span><span class="name">Managing Compute</span></a></li><li class="inactive"><a href="ops-managing-esx.html"><span class="number">6 </span><span class="name">Managing ESX</span></a></li><li class="inactive"><a href="ops-managing-blockstorage.html"><span class="number">7 </span><span class="name">Managing Block Storage</span></a></li><li class="inactive"><a href="ops-managing-objectstorage.html"><span class="number">8 </span><span class="name">Managing Object Storage</span></a></li><li class="inactive"><a href="ops-managing-networking.html"><span class="number">9 </span><span class="name">Managing Networking</span></a></li><li class="inactive"><a href="ops-managing-dashboards.html"><span class="number">10 </span><span class="name">Managing the Dashboard</span></a></li><li class="inactive"><a href="ops-managing-orchestration.html"><span class="number">11 </span><span class="name">Managing Orchestration</span></a></li><li class="inactive"><a href="topic-ttn-5fg-4v.html"><span class="number">12 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li class="inactive"><a href="system-maintenance.html"><span class="number">13 </span><span class="name">System Maintenance</span></a></li><li class="inactive"><a href="bura-overview.html"><span class="number">14 </span><span class="name">Backup and Restore</span></a></li><li class="inactive"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html"><span class="number">15 </span><span class="name">Troubleshooting Issues</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 14. Backup and Restore" href="bura-overview.html"><span class="prev-icon">←</span></a><span class="tool-spacer"><span class="next-icon">→</span></span></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Operations Guide"><span class="book-icon">Operations Guide</span></a><span> › </span><a class="crumb" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html">Troubleshooting Issues</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 14. Backup and Restore" href="bura-overview.html"><span class="prev-icon">←</span></a><span class="tool-spacer"><span class="next-icon">→</span></span></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber "><span class="phrase"><span class="phrase">8</span></span></span></div><div><h1 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Issues</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-troubleshooting_issues.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_issues.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-issues-xml-1</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#general-troubleshooting"><span class="number">15.1 </span><span class="name">General Troubleshooting</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-controlplane"><span class="number">15.2 </span><span class="name">Control Plane Troubleshooting</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#ts-compute"><span class="number">15.3 </span><span class="name">Troubleshooting Compute Service</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#neutron-troubleshooting"><span class="number">15.4 </span><span class="name">Network Service Troubleshooting</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-glance"><span class="number">15.5 </span><span class="name">Troubleshooting the Image (Glance) Service</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-storage"><span class="number">15.6 </span><span class="name">Storage Troubleshooting</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#monitoring-logging-usage-reporting"><span class="number">15.7 </span><span class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-ly3-yyr-st"><span class="number">15.8 </span><span class="name">Backup and Restore Troubleshooting</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-orchestration"><span class="number">15.9 </span><span class="name">Orchestration Troubleshooting</span></a></span></dt><dt><span class="section"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-tools"><span class="number">15.10 </span><span class="name">Troubleshooting Tools</span></a></span></dt></dl></div></div><p>
  Troubleshooting and support processes for solving issues in your environment.
 </p><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
 </p><div class="sect1" id="general-troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#general-troubleshooting">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-general_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-general_troubleshooting.xml</li><li><span class="ds-label">ID: </span>general-troubleshooting</li></ul></div></div></div></div><p>
  General troubleshooting procedures for resolving your cloud issues including
  steps for resolving service alarms and support contact information.
 </p><p>
  Before contacting support to help you with a problem on SUSE <span class="productname">OpenStack</span> Cloud, we recommend
  gathering as much information as possible about your system and the
  problem. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with a tool called
  <code class="command">supportconfig</code>. It gathers system information such as the
  current kernel version being used, the hardware, RPM database, partitions,
  and other items. <code class="command">supportconfig</code> also collects the most
  important log files. This information assists support staff to identify and
  solve your problem.
 </p><p>
  Always run <code class="command">supportconfig</code> on the Cloud Lifecycle Manager and on the
  Control Node(s). If a Compute Node or a Storage Node is part of the problem, run
  <code class="command">supportconfig</code> on the affected node as well. For details on
  how to run <code class="command">supportconfig</code>, see
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-adm-support</a>.
 </p><div class="sect2" id="alarmdefinitions"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Resolution Procedures</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-alarm_resolutions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-alarm_resolutions.xml</li><li><span class="ds-label">ID: </span>alarmdefinitions</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a monitoring solution based on OpenStack’s Monasca
  service. This service provides monitoring and metrics for all OpenStack
  components, as well as much of the underlying system. By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  comes with a set of alarms that provide coverage of the primary systems. In
  addition, you can define alarms based on threshold values for any metrics
  defined in the system. You can view alarm information in the Operations
  Console. You can also receive or deliver this information to others by
  configuring email or other mechanisms. Alarms provide information about
  whether a component failed and is affecting the system, and also what
  condition triggered the alarm.
 </p><p>
  Here is a list of the included service-specific alarms and the recommended
  troubleshooting steps. We have organized these alarms by the section of the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console, they are organized in as well as the
  <code class="literal">service</code> dimension defined.
 </p><div class="sect3" id="compute-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#compute-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>compute-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Compute section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.4.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: COMPUTE</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.4.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> This is a <code class="literal">nova-api</code> health check.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
        </p>
       </td><td valign="top">Restart the <code class="literal">nova-api</code> process on the affected
     node. Review the <code class="literal">nova-api.log</code> files. Try to connect
     locally to the http port that is found in the dimension field of the alarm
     to see if the connection is accepted.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Host Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Alarms when the specified host is down or not reachable.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The host is down, has been rebooted, or has network
         connectivity issues.
        </p>
       </td><td valign="top">If it is a single host, attempt to restart the system. If it is
     multiple hosts, investigate networking issues.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Bound Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: <code class="literal">process_name=nova-api</code> This alarm
         checks that the number of processes found is in a predefined range.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process crashed or too many processes running
        </p>
       </td><td valign="top">Stop all the processes and restart the nova-api process on the
       affected host.  Review the system and nova-api logs.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Separate alarms for each of these Nova services,
         specified by the <code class="literal">component</code> dimension:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           nova-api
          </p></li><li class="listitem "><p>
           nova-cert
          </p></li><li class="listitem "><p>
           nova-compute
          </p></li><li class="listitem "><p>
           nova-consoleauth
          </p></li><li class="listitem "><p>
           nova-conductor
          </p></li><li class="listitem "><p>
           nova-scheduler
          </p></li><li class="listitem "><p>
           nova-novncproxy
          </p></li></ul></div>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process specified by the <code class="literal">component</code>
         dimension has crashed on the host specified by the
         <code class="literal">hostname</code> dimension.
        </p>
       </td><td valign="top">
        <p>
         Restart the process on the affected node using these steps:
        </p>
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
           Log in to the Cloud Lifecycle Manager.
          </p></li><li class="step "><p>
           Use the Nova start playbook against the affected node:
          </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
        <p>
         Review the associated logs. The logs will be in the format of
         <code class="literal">&lt;service&gt;.log</code>, such as
         <code class="literal">nova-compute.log</code> or
         <code class="literal">nova-scheduler.log</code>.
        </p>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: nova.heartbeat</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Check that all services are sending heartbeats.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process for service specified in the alarm has crashed
         or is hung and not reporting its status to the database. Alternatively
         it may be the service is fine but an issue with messaging or the
         database which means the status is not being updated correctly.
        </p>
       </td><td valign="top">Restart the affected service. If the service is reporting OK the
     issue may be with RabbitMQ or MySQL. In that case, check the alarms for
     those services.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Service log directory consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a service set to
         <code class="literal">DEBUG</code> instead of <code class="literal">INFO</code> level.
         Another reason could be due to a repeating error message filling up
         the log files. Finally, it could be due to log rotate not configured
         properly so old log files are not being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at the
     logs. If <code class="literal">DEBUG</code> log entries exist, set the logging level
     to <code class="literal">INFO</code>. If the logs are repeatedly logging an error
     message, do what is needed to resolve the error. If old log files exist,
     configure log rotate to remove them. You could also choose to remove old
     log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.4.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: IMAGE-SERVICE in Compute section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.4.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Separate alarms for
         each of these Glance services, specified by the
         <code class="literal">component</code> dimension:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          glance-api
         </p></li><li class="listitem "><p>
          glance-registry
         </p></li></ul></div>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> API is unresponsive.
        </p>
       </td><td valign="top">
        <p>
         Restart the process on the affected node using these steps:
        </p>
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Glance start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p></td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Service log directory consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a service set to
         <code class="literal">DEBUG</code> instead of <code class="literal">INFO</code>
         level. Another reason could be due to a repeating error message
         filling up the log files. Finally, it could be due to log rotate not
         configured properly so old log files are not being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.4.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BAREMETAL in Compute section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.4.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified process is not running: <code class="literal">process_name = ironic-api</code>
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The Ironic API is unresponsive.
        </p>
       </td><td valign="top">
        <p>
        Restart the <code class="literal">ironic-api</code> process with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the affected host via SSH.
         </p></li><li class="step "><p>
          Restart the <code class="literal">ironic-api</code> process with this command:
         </p><div class="verbatim-wrap"><pre class="screen">sudo service ironic-api restart</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified process is not running: <code class="literal">process_name = ironic-conductor</code>
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The
         <code class="literal">ironic-conductor</code> process has crashed.
        </p>
       </td><td valign="top">
        <p>
        Restart the <code class="literal">ironic-conductor</code> process with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Source your <code class="literal">admin</code> user credentials:
         </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
          Locate the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants | grep mess</pre></div></li><li class="step "><p>
          SSH to the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">sudo -u ardana ssh &lt;IP_ADDRESS&gt;</pre></div></li><li class="step "><p>
          Stop the <code class="literal">ironic-conductor</code> process by using this
          playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</pre></div></li><li class="step "><p>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified HTTP endpoint is down or not reachable.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The API is unresponsive.
        </p>
       </td><td valign="top">
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Source your <code class="literal">admin</code> user credentials:
         </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
          Locate the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants | grep mess</pre></div></li><li class="step "><p>
          SSH to the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">sudo -u ardana ssh &lt;IP_ADDRESS&gt;</pre></div></li><li class="step "><p>
          Stop the <code class="literal">ironic-api</code> process by using this
          playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</pre></div></li><li class="step "><p>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Service log directory
         consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
         service set to <code class="literal">DEBUG</code> instead of
         <code class="literal">INFO</code> level. Another reason could be due to a
         repeating error message filling up the log files. Finally, it could be
         due to log rotate not configured properly so old log files are not
         being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="storage-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#storage-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>storage-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Storage section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.5.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OBJECT-STORAGE</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.5.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swiftlm-scan monitor</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        <code class="literal">swiftlm-scan</code> cannot execute a monitoring task.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">swiftlm-scan</code> program is used to monitor and measure
        a number of metrics. If it is unable to monitor or measure something,
        it raises this alarm.
       </p>
      </td><td valign="top">
       <p>
        Click on the alarm to examine the <code class="literal">Details</code> field and
        look for a <code class="literal">msg</code> field. The text may explain the error
        problem. To view/confirm this, you can also log into the host specified
        by the <code class="literal">hostname</code> dimension, and then run this
        command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo swiftlm-scan | python -mjson.tool</pre></div>
       <p>
        The <code class="literal">msg</code> field is contained in the
        <code class="literal">value_meta</code> item.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift account replicator last</strong></span>
        completed in 12 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if an
        <code class="literal">account-replicator</code> process did not complete a
        replication cycle within the last 12 hours.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the <code class="literal">account-replication</code> process is stuck.
       </p>
      </td><td valign="top">
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift container replicator last</strong></span>
        completed in 12 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a
        container-replicator process did not complete a replication cycle
        within the last 12 hours
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the container-replication process is stuck.
       </p>
      </td><td valign="top">
       <p>
        SSH to the affected host and restart the process with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo systemctl restart swift-container-replicator</pre></div>
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift object replicator last</strong></span>
        completed in 24 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if an
        object-replicator process did not complete a replication cycle within
        the last 24 hours
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the object-replication process is stuck.
       </p>
      </td><td valign="top">
       <p>
        SSH to the affected host and restart the process with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo systemctl restart swift-account-replicator</pre></div>
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift configuration file</strong></span>
        ownership
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        files/directories in <code class="literal">/etc/swift</code> are not owned by
        Swift.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> For files in
        <code class="literal">/etc/swift</code>, somebody may have manually edited or
        created a file.
       </p>
      </td><td valign="top">
       <p>
        For files in <code class="literal">/etc/swift</code>, use this command to change
        the file ownership:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo chown swift.swift /etc/swift/, /etc/swift/*</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift data filesystem ownership</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if files or
        directories in <code class="literal">/srv/node</code> are not owned by Swift.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> For directories in
        <code class="literal">/srv/node/*</code>, it may happen that the root partition
        was reimaged or reinstalled and the UID assigned to the Swift user
        change. The directories and files would then not be owned by the UID
        assigned to the Swift user.
       </p>
      </td><td valign="top">
       <p>
        For directories and files in <code class="filename">/srv/node/*</code>, compare
        the swift UID of this system and other systems and the UID of the owner
        of <code class="filename">/srv/node/*</code>. If possible, make the UID of the
        Swift user match the directories or files. Otherwise, change the
        ownership of all files and directories under the
        <code class="filename">/srv/node</code> path using a similar <code class="command">chown
        swift.swift</code> command as above.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Drive URE errors detected</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        <code class="literal">swift-drive-audit</code> reports an unrecoverable read
        error on a drive used by the Swift service.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> An unrecoverable read
        error occurred when Swift attempted to access a directory.
       </p>
      </td><td valign="top">
       <p>
        The UREs reported only apply to file system metadata (that is,
        directory structures). For UREs in object files, the Swift system
        automatically deletes the file and replicates a fresh copy from one of
        the other replicas.
       </p>
       <p>
        UREs are a normal feature of large disk drives. It does not mean that
        the drive has failed. However, if you get regular UREs on a specific
        drive, then this may indicate that the drive has indeed failed and
        should be replaced.
       </p>
       <p>
        You can use standard XFS repair actions to correct the UREs in the file
        system.
       </p>
       <p>
        If the XFS repair fails, you should wipe the GPT table as follows
        (where &lt;drive_name&gt; is replaced by the actual drive name):
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo dd if=/dev/zero of=/dev/sd&lt;drive_name&gt; \
bs=$((1024*1024)) count=1</pre></div>
       <p>
        Then follow the steps below which will reformat the drive, remount it,
        and restart Swift services on the affected node.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift reconfigure playbook, specifying the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _swift-configure.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        It is safe to reformat drives containing Swift data because Swift
        maintains other copies of the data (usually, Swift is configured to
        have three replicas of all data).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift service</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a Swift
        process, specified by the <code class="literal">component</code> field, is not
        running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A daemon specified by
        the <code class="literal">component</code> dimension on the host specified by the
        <code class="literal">hostname</code> dimension has stopped running.
       </p>
      </td><td valign="top">
       <p>
        Examine the <code class="filename">/var/log/swift/swift.log</code> file for
        possible error messages related the Swift process. The process in
        question is listed in the alarm dimensions in the
        <code class="literal">component</code> dimension.
       </p>
       <p>
        Restart Swift processes by running the
        <code class="filename">swift-start.yml</code> playbook, with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift filesystem mount point</strong></span>
        status
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a file
        system/drive used by Swift is not correctly mounted.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The device specified by
        the <code class="literal">device</code> dimension is not correctly mounted at the
        mountpoint specified by the <code class="literal">mount</code> dimension.
       </p>
       <p>
        The most probable cause is that the drive has failed or that it had a
        temporary failure during the boot process and remained unmounted.
       </p>
       <p>
        Other possible causes are a file system corruption that prevents the
        device from being mounted.
       </p>
      </td><td valign="top">
       <p>
        Reboot the node and see if the file system remains unmounted.
       </p>
       <p>
        If the file system is corrupt, see the process used for the "Drive URE
        errors" alarm to wipe and reformat the drive.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift uptime-monitor status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the
        swiftlm-uptime-monitor has errors using Keystone (<code class="literal">keystone-get-token</code>),
        Swift (<code class="literal">rest-api</code>) or Swift's healthcheck.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        swiftlm-uptime-monitor cannot get a token from Keystone or cannot get a
        successful response from the Swift Object-Storage API.
       </p>
      </td><td valign="top">
       <p>
        Check that the Keystone service is running:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check the status of the Keystone service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-status.yml</pre></div></li><li class="step "><p>
          If it is not running, start the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-start.yml</pre></div></li><li class="step "><p>
          Contact the support team if further assistance troubleshooting the
          Keystone service is needed.
         </p></li></ol></div></div>
       <p>
        Check that Swift is running:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check the status of the Keystone service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li><li class="step "><p>
          If it is not running, start the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml</pre></div></li></ol></div></div>
       <p>
        Restart the swiftlm-uptime-monitor as follows:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log into the first server running the swift-proxy-server service. Use
          this playbook below to determine whcih host this is:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml
--limit SWF-PRX[0]</pre></div></li><li class="step "><p>
          Restart the swiftlm-uptime-monitor with this command:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart swiftlm-uptime-monitor</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift Keystone server connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the Keystone service (used for token validation)
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Identity service
        (Keystone) server may be down. Another possible cause is that the
        network between the host reporting the problem and the Keystone server
        or the <code class="literal">haproxy</code> process is not forwarding requests to
        Keystone.
       </p>
      </td><td valign="top">
       <p>
        The <code class="literal">URL</code> dimension contains the name of the virtual
        IP address. Use cURL or a similar program to confirm that a connection
        can or cannot be made to the virtual IP address. Check that
        <code class="literal">haproxy</code> is running. Check that the Keystone service
        is working.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift service listening on ip</strong></span>
        and port
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a Swift
        service is not listening on the correct port or ip.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Swift service may be
        down.
       </p>
      </td><td valign="top">
       <p>
        Verify the status of the Swift service on the affected host, as
        specified by the <code class="literal">hostname</code> dimension.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift status playbook to confirm status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If an issue is determined, you can stop and restart the Swift service
        with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the Swift service on the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts Swift-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Restart the Swift service on the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift rings checksum</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the Swift rings
        checksums do not match on all hosts.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Swift ring files
        must be the same on every node. The files are located in
        <code class="filename">/etc/swift/*.ring.gz</code>.
       </p>
       <p>
        If you have just changed any of the rings and you are still deploying
        the change, it is normal for this alarm to trigger.
       </p>
      </td><td valign="top">
       <p>
        If you have just changed any of your Swift rings, if you wait until the
        changes complete then this alarm will likely clear on its own. If it
        does not, then continue with these steps.
       </p>
       <p>
        Use <code class="command">sudo swift-recon --md5</code> to find which node has
        outdated rings.
       </p>
       <p>
        Run the <code class="filename">swift-reconfigure.yml</code> playbook, using the
        steps below. This deploys the same set of rings to every node.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift memcached server connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the specified memcached server.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The server may be down.
        The memcached daemon running the server may have stopped.
       </p>
      </td><td valign="top">
       <p>
        If the server is down, restart it.
       </p>
       <p>
        If memcached has stopped, you can restart it by using the
        <code class="filename">memcached-start.yml</code> playbook, using the steps
        below. If this fails, rebooting the node will restart the process.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the memcached start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts memcached-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If the server is running and memcached is running, there may be a
        network problem blocking port 11211.
       </p>
       <p>
        If you see sporadic alarms on different servers, the system may be
        running out of resources. Contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for advice.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift individual disk usage
        exceeds 80%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a disk drive
        used by Swift exceeds 80% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Generally all disk
        drives will fill roughly at the same rate. If an individual disk drive
        becomes filled faster than other drives it can indicate a problem with
        the replication process.
       </p>
      </td><td valign="top">
       <p>
        If many or most of your disk drives are 80% full, you need to add more
        nodes to your system or delete existing objects.
       </p>
       <p>
        If one disk drive is noticeably (more than 30%) more utilized than the
        average of other disk drives, check that Swift processes are working on
        the server (use the steps below) and also look for alarms related to
        the host. Otherwise continue to monitor the situation.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift individual disk usage exceeds
        90%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a disk drive
        used by Swift exceeds 90% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Generally all disk
        drives will fill roughly at the same rate. If an individual disk drive
        becomes filled faster than other drives it can indicate a problem with
        the replication process.
       </p>
      </td><td valign="top">
       <p>
        If one disk drive is noticeably (more than 30%) more utilized than the
        average of other disk drives, check that Swift processes are working on
        the server, using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the Swift status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li></ol></div></div>
       <p>
        Also look for alarms related to the host. An individual disk drive
        filling can indicate a problem with the replication process.
       </p>
       <p>
        Restart Swift on that host using the <code class="literal">--limit</code>
        argument to target the host:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the Swift service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Start the Swift service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If the utilization does not return to similar values as other disk
        drives, you can reformat the disk drive. You should only do this if the
        average utilization of all disk drives is less than 80%. To format a
        disk drive contact <span class="phrase"><span class="phrase">Sales Engineering</span></span> for instructions.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift total disk usage exceeds
        80%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the average
        disk utilization of Swift disk drives exceeds 80% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number and size of
        objects in your system is beginning to fill the available disk space.
        Account and container storage is included in disk utilization. However,
        this generally consumes 1-2% of space compared to objects, so object
        storage is the dominate consumer of disk space.
       </p>
      </td><td valign="top">
       <p>
        You need to add more nodes to your system or delete existing objects to
        remain under 80% utilization.
       </p>
       <p>
        If you delete a project/account, the objects in that account are not
        removed until a week later by the <code class="literal">account-reaper</code>
        process, so this is not a good way of quickly freeing up space.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift total disk usage exceeds
        90%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the average
        disk utilization of Swift disk drives exceeds 90% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number and size of
        objects in your system is beginning to fill the available disk space.
        Account and container storage is included in disk utilization. However,
        this generally consumes 1-2% of space compared to objects, so object
        storage is the dominate consumer of disk space.
       </p>
      </td><td valign="top">
       <p>
        If your disk drives are 90% full, you must immediately stop all
        applications that put new objects into the system. At that point you
        can either delete objects or add more servers.
       </p>
       <p>
        Using the steps below, set the <code class="literal">fallocate_reserve</code>
        value to a value higher than the currently available space on disk
        drives. This will prevent more objects being created.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Edit the configuration files below and change the value for
          <code class="literal">fallocate_reserve</code> to a value higher than the
          currently available space on the disk drives:
         </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/swift/account-server.conf.j2
~/openstack/my_cloud/config/swift/container-server.conf.j2
~/openstack/my_cloud/config/swift/object-server.conf.j2</pre></div></li><li class="step "><p>
          Commit the changes to git:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "changing Swift fallocate_reserve value"</pre></div></li><li class="step "><p>
          Run the configuration processor:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
          Update your deployment directory:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
          Run the Swift reconfigure playbook to deploy the change:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div>
       <p>
        If you allow your file systems to become full, you will be unable to
        delete objects or add more nodes to the system. This is because the
        system needs some free space to handle the replication process when
        adding nodes. With no free space, the replication process cannot work.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift service per-minute
        availability</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the Swift
        service reports unavailable for the previous minute.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">swiftlm-uptime-monitor</code> service runs on the first
        proxy server. It monitors the Swift endpoint and reports latency data.
        If the endpoint stops reporting, it generates this alarm.
       </p>
      </td><td valign="top">
       <p>
        There are many reasons why the endpoint may stop running. Check:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Is <code class="literal">haproxy</code> running on the control nodes?
         </p></li><li class="listitem "><p>
          Is <code class="literal">swift-proxy-server</code> running on the Swift proxy
          servers?
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift rsync connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the specified rsync server
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The rsync daemon on the
        specified node cannot be contacted. The most probable cause is that the
        node is down. The rsync service might also have been stopped on the
        node.
       </p>
      </td><td valign="top">
       <p>
        Reboot the server if it is down.
       </p>
       <p>
        Attempt to restart rsync with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">systemctl restart rsync.service</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift smart array controller
        status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Smart Array.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Smart Array or Smart
        HBA controller has a fault or a component of the controller (such as a
        battery) is failed or caching is disabled.
       </p>
       <p>
        The HPE Smart Storage Administrator (HPE SSA) CLI component will have
        to be installed for SSACLI status to be reported. HPE-specific binaries
        that are not based on open source are distributed directly from and
        supported by HPE. To download and install the SSACLI utility, please
        refer to:
        <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the controllers:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; controller show all detail</pre></div>
       <p>
        For hardware failures (such as failed battery), replace the failed
        component. If the cache is disabled, reenable the cache.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift physical drive status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Physical Drive.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>A disk drive on the
        server has failed or has warnings.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported and run these commands to find out the status of
        the drive:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Swift logical drive status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Logical Drive.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A LUN on the server is
        degraded or has failed.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the LUN:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> If the
        <code class="literal">service</code> dimension is
        <code class="literal">object-store</code>, see the description of the "Swift
        Service" alarm for possible causes.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">service</code> dimension is
        <code class="literal">object-storage</code>, see the description of the "Swift
        Service" alarm for possible mitigation tasks.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> If the
        <code class="literal">service</code> dimension is
        <code class="literal">object-store</code>, see the description of the "Swift host
        socket connect" alarm for possible causes.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">service</code> dimension is
        <code class="literal">object-storage</code>, see the description of the "Swift
        host socket connect" alarm for possible mitigation tasks.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">
       <p>
        Find the service that is consuming too much disk space. Look at the
        logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
        level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
        error message, do what is needed to resolve the error. If old log files
        exist, configure log rotate to remove them. You could also choose to
        remove old log files by hand after backing them up if needed.
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.5.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BLOCK-STORAGE in Storage section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.5.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these Cinder services, specified by the <code class="literal">component</code>
        dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          cinder-api
         </p></li><li class="listitem "><p>
          cinder-backup
         </p></li><li class="listitem "><p>
          cinder-scheduler
         </p></li><li class="listitem "><p>
          cinder-volume
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node. Review the associated logs.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the <code class="filename">cinder-start.yml</code> playbook to start the
          process back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-start.yml
--limit &lt;hostname&gt;</pre></div><div id="id-1.6.17.4.5.5.4.2.1.4.1.2.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
           The <code class="literal">--limit &lt;hostname&gt;</code> switch is optional.
           If it is included, then the <code class="literal">&lt;hostname&gt;</code> you
           should use is the host where the alarm was raised.
          </p></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name=cinder-backup</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Alert may be incorrect if the service has migrated. Validate that the
        service is intended to be running on this node before restarting the
        service. Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name=cinder-scheduler</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node. Review the associated logs.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the <code class="filename">cinder-start.yml</code> playbook to start the
          process back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-start.yml \
--limit &lt;hostname&gt;</pre></div><div id="id-1.6.17.4.5.5.4.2.1.4.3.2.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
           The <code class="literal">--limit &lt;hostname&gt;</code> switch is optional.
           If it is included, then the <code class="literal">&lt;hostname&gt;</code> you
           should use is the host where the alarm was raised.
          </p></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name=cinder-volume</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Alert may be incorrect if the service has migrated. Validate that the
        service is intended to be running on this node before restarting the
        service. Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Cinder backup running
        &lt;hostname&gt; check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Cinder backup singleton
        check.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Backup process is one of
        the following:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          It is running on a node it should not be on
         </p></li><li class="listitem "><p>
          It is not running on a node it should be on
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Run the <code class="filename">cinder-migrate-volume.yml</code> playbook to
        migrate the volume and back up to the correct node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook to migrate the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Cinder volume running
        &lt;hostname&gt; check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Cinder volume singleton
        check.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">cinder-volume</code> process is either:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          running on a node it should not be on, or
         </p></li><li class="listitem "><p>
          not running on a node it should be on
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Run the <code class="filename">cinder-migrate-volume.yml</code> playbook to
        migrate the volume and backup to correct node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook to migrate the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Storage faulty lun check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if local LUNs on
        your HPE servers using smartarray are not OK.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A LUN on the server is
        degraded or has failed.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the LUN:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Storage faulty drive check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the local disk
        drives on your HPE servers using smartarray are not OK.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A disk drive on the
        server has failed or has warnings.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported and run these commands to find out the status of
        the drive:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">
       <p>
        Find the service that is consuming too much disk space. Look at the
        logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
        level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
        error message, do what is needed to resolve the error. If old log files
        exist, configure log rotate to remove them. You could also choose to
        remove old log files by hand after backing them up if needed.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="networking-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#networking-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>networking-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Networking section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.6.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: NETWORKING</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.6.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. Separate alarms for each of these Neutron
        services, specified by the <code class="literal">component</code> dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         ipsec/charon
        </p></li><li class="listitem "><p>
         neutron-openvswitch-agent
        </p></li><li class="listitem "><p>
         neutron-l3-agent
        </p></li><li class="listitem "><p>
         neutron-dhcp-agent
        </p></li><li class="listitem "><p>
         neutron-metadata-agent
        </p></li><li class="listitem "><p>
         neutron-server
        </p></li><li class="listitem "><p>
         neutron-vpn-agent
        </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Check the status of the networking status:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li><li class="step "><p>
         Make note of the failed service names and the affected hosts which you
         will use to review the logs later.
        </p></li><li class="step "><p>
         Using the affected hostname(s) from the previous output, run the
         Neutron start playbook to restart the services:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-start.yml \
--limit &lt;hostname&gt;</pre></div><div id="id-1.6.17.4.5.6.3.2.1.4.1.2.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
          You can pass multiple hostnames with
          <code class="literal">--limit</code> option by separating them with a colon
          <code class="literal">:</code>.
         </p></div></li><li class="step "><p>
         Check the status of the networking service again:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li><li class="step "><p>
         Once all services are back up, you can SSH to the affected host(s) and
         review the logs in the location below for any errors around the time
         that the alarm triggered:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/&lt;service_name&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = neutron-rootwrap</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Currently <code class="literal">neutron-rootwrap</code> is only used to run
       <code class="literal">ovsdb-client</code>. To restart this process, use these
       steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         SSH to the affected host(s).
        </p></li><li class="step "><p>
         Restart the process:
        </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart neutron-openvswitch-agent</pre></div></li><li class="step "><p>
         Review the logs at the location below for errors:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/neutron-openvswitch-agent.log</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> neutron api health check
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process is stuck if the
        <code class="literal">neutron-server</code> Process Check is not OK.
       </p>
      </td><td valign="top">
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         SSH to the affected host(s).
        </p></li><li class="step "><p>
         Run this command to restart the <code class="literal">neutron-server</code>
         process:
        </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart neutron-server</pre></div></li><li class="step "><p>
         Review the logs at the location below for errors:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/neutron-server.log</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> neutron api health check
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The node
        crashed. Alternatively, only connectivity might have been lost if the
        local node HTTP Status is OK or UNKNOWN.
       </p>
      </td><td valign="top">Reboot the node if it crashed or diagnose the networking
      connectivity failures between the local and remote nodes. Review the
      logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Directory Log Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.6.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: DNS in Networking section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.6.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-zone-manager</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-ZMG'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-zone-manager.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-pool-manager</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-PMG'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-pool-manager.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-central</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-CEN'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-central.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-api</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-API'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-api.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-mdns</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen">         <code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-MDN'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-mdns.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        designate-api</code> This alarm will also have the
        <code class="literal">api_endpoint</code> and
        <code class="literal">monitored_host_types</code> dimensions defined. The likely
        cause and mitigation steps are the same for both.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The API is unresponsive.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-API,DES-CEN'</pre></div></li></ol></div></div>
      <p>
       Review the logs located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-api.log
/var/log/designate/designate-central.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Directory Log Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.6.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BIND in Networking section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.6.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = pdns_server</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the PowerDNS start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts bind-start.yml</pre></div></li></ol></div></div>
      <p>
       Review the log located at, querying against <code class="literal">process =
       pdns_server</code>:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/syslog</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = named</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Bind start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts bind-start.yml</pre></div></li></ol></div></div>
      <p>
       Review the log located at, querying against <code class="literal">process =
       named</code>:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/syslog</pre></div>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="identity-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#identity-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-identity_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-identity_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>identity-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Identity section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.7.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: IDENTITY-SERVICE</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.7.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-identity_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-identity_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the Keystone public endpoint directly.
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
api_endpoint=public</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Keystone service is
        down on the affected node.
       </p>
      </td><td valign="top">
       <p>
        Restart the Keystone service on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Keystone start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the Keystone admin endpoint directly
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
api_endpoint=admin</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Keystone service is
        down on the affected node.
       </p>
      </td><td valign="top">
       <p>
        Restart the Keystone service on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Keystone start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the Keystone admin endpoint via the virtual IP address (HAProxy)
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
monitored_host_type=vip</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Keystone service is
        unreachable via the virtual IP address.
       </p>
      </td><td valign="top">
       <p>
        If neither the <code class="literal">api_endpoint=public</code> or
        <code class="literal">api_endpoint=admin</code> alarms are triggering at the same
        time then there is likely a problem with haproxy.
       </p>
       <p>
        You can restart the haproxy service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use this playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these Glance services, specified by the <code class="literal">component</code>
        dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          keystone-main
         </p></li><li class="listitem "><p>
          keystone admin
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        You can restart the Keystone service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use this playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the logs in <code class="literal">/var/log/keystone</code> on the affected
        node.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="telemetry-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Telemetry Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#telemetry-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>telemetry-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Telemetry section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.8.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: TELEMETRY</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.8.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the
        <code class="literal">ceilometer-agent-notification</code> process is not
        running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs on the alarming host in the following location for the
        cause:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/ceilometer/ceilometer-agent-notification-json.log</pre></div>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Ceilometer start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the
        <code class="literal">ceilometer-polling</code> process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs on the alarming host in the following location for the
        cause:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/ceilometer/ceilometer-polling-json.log</pre></div>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Ceilometer start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: METERING in Telemetry section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.8.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: KAFKA in Telemetry section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.8.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Persister Metric Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the Persister
        consumer group is not keeping up with the incoming messages on the
        metric topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Verify that all of the monasca-persister services are up with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager
         </p></li><li class="step "><p>
          Verify that all of the <code class="literal">monasca-persister</code> services
          are up with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Determining which alarms are
        firing can help diagnose likely causes. For example, if the alarm is
        alerting all on one machine it could be the machine. If one topic
        across multiple machines it is likely the consumers of that topic, etc.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Alarm Transition Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        consumer group is not keeping up with the incoming messages on the
        alarm state transition topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Check that monasca-thresh and monasca-notification are up.
       </p>
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Which alarms are firing can help
        diagnose likely causes. For example:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If all alarms are on the same machine, the machine could be at fault.
         </p></li><li class="listitem "><p>
          If one topic is shared across multiple machines, the consumers of
          that topic are likely at fault.
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Kronos Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the Kronos
        consumer group is not keeping up with the incoming messages on the
        metric topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Which alarms are firing can help
        diagnose likely causes. For example:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If all alarms are on the same machine, the machine could be at fault.
         </p></li><li class="listitem "><p>
          If one topic is shared across multiple machines, the consumers of
          that topic are likely at fault.
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = kafka.Kafka</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the kafka service with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Start the kafka service back up with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li></ol></div></div>
       <p>
        Review the logs in <code class="filename">/var/log/kafka/server.log</code>
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: LOGGING in Telemetry section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.8.6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Beaver Memory Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Beaver is using more
        memory than expected. This may indicate that it cannot forward messages
        and its queue is filling up. If you continue to see this, see the
        troubleshooting guide.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Overloaded system or
        services with memory leaks.
       </p>
      </td><td valign="top">Log on to the reporting host to investigate high memory users.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Audit Log Partition Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="literal">/var/audit</code> disk space usage has crossed low
        watermark. If the high watermark is reached, logrotate will be run to
        free up disk space. If needed, adjust:
       </p>
       <div class="verbatim-wrap"><pre class="screen">var_audit_low_watermark_percent</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to DEBUG instead of INFO level. Another reason could be due
        to a repeating error message filling up the log files. Finally, it
        could be due to log rotate not configured properly so old log files are
        not being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If DEBUG log entries exist, set the logging level to INFO. If
       the logs are repeatedly logging an error message, do what is needed to
       resolve the error. If old log files exist, configure log rotate to
       remove them. You could also choose to remove old log files by hand after
       backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Audit Log Partition High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="literal">/var/audit</code> volume is running low on disk space.
        Logrotate will be run now to free up space. If needed, adjust:
       </p>
       <div class="verbatim-wrap"><pre class="screen">var_audit_high_watermark_percent</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to DEBUG instead of INFO level. Another reason could be due
        to a repeating error message filling up the log files. Finally, it
        could be due to log rotate not configured properly so old log files are
        not being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If DEBUG log entries exist, set the logging level to INFO. If
       the logs are repeatedly logging an error message, do what is needed to
       resolve the error. If old log files exist, configure log rotate to
       remove them. You could also choose to remove old log files by hand after
       backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Unassigned Shards</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> component =
        elasticsearch; Elasticsearch unassigned shards count is greater than
        0.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Environment could be
        misconfigured.
       </p>
      </td><td valign="top">
       <p>
        To find the unassigned shards, run the following command on the Cloud Lifecycle Manager
        from the <code class="filename">~/scratch/ansible/next/ardana/ansible</code>
        directory:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a \
"curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</pre></div>
       <p>
        This shows which shards are unassigned, like this:
       </p>
<div class="verbatim-wrap"><pre class="screen">logstash-2015.10.21 4 p UNASSIGNED ... 10.240.75.10 NodeName</pre></div>
       <p>
        The last column shows the name that Elasticsearch uses for the node
        that the unassigned shards are on. To find the actual host name, run:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a \
"curl localhost:9200/_nodes/_all/name?pretty -s"</pre></div>
       <p>
        When you find the host name, take the following steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Make sure the node is not out of disk space, and free up space if
          needed.
         </p></li><li class="step "><p>
          Restart the node (use caution, as this may affect other services as
          well).
         </p></li><li class="step "><p>
          Make sure all versions of Elasticsearch are the same:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR -m shell -a \
"curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</pre></div></li><li class="step "><p>
          Contact customer support.
         </p></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Number of Log Entries</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Elasticsearch Number of
        Log Entries: <code class="literal">component = elasticsearch;</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number of log
        entries may get too large.
       </p>
      </td><td valign="top">Older versions of Kibana (version 3 and earlier) may hang if the
       number of log entries is too large (for example, above 40,000), and the
       page size would need to be small enough (about 20,000 results), because
       if it is larger (for example, 200,000), it may hang the browser, but
       Kibana 4 should not have this issue.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Field Data Evictions</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Elasticsearch Field
        Data Evictions count is greater than 0: <code class="literal">component =
        elasticsearch</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Field Data Evictions may
        be found even though it is nowhere near the limit set.
       </p>
      </td><td valign="top">
       <p>
        The <code class="literal">elasticsearch_indices_fielddata_cache_size</code> is
        set to <code class="literal">unbounded</code> by default. If this is set by the
        user to a value that is insufficient, you may need to increase this
        configuration parameter or set it to <code class="literal">unbounded</code> and
        run a reconfigure using the steps below:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Edit the configuration file below and change the value for
          <code class="literal">elasticsearch_indices_fielddata_cache_size</code> to your
          desired value:
         </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/main.yml</pre></div></li><li class="step "><p>
          Commit the changes to git:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Elasticsearch fielddata cache size"</pre></div></li><li class="step "><p>
          Run the configuration processor:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
          Update your deployment directory:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
          Run the Logging reconfigure playbook to deploy the change:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these logging services, specified by the
        <code class="literal">process_name</code> dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          elasticsearch
         </p></li><li class="listitem "><p>
          logstash
         </p></li><li class="listitem "><p>
          beaver
         </p></li><li class="listitem "><p>
          apache2
         </p></li><li class="listitem "><p>
          kibana
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        On the affected node, attempt to restart the process.
       </p>
       <p>
        If the <code class="command">elasticsearch</code> process has crashed, use:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart elasticsearch</pre></div>
       <p>
        If the logstash process has crashed, use:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart logstash</pre></div>
       <p>
        The rest of the processes can be restarted using similar commands,
        listed here:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart beaver
<code class="prompt user">ardana &gt; </code>sudo systemctl restart apache2
<code class="prompt user">ardana &gt; </code>sudo systemctl restart kibana</pre></div>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MONASCA-TRANSFORM in Telemetry section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.8.7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">process_name =
        pyspark</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart process on affected node. Review logs.
       </p>
       <p>
        Child process of <code class="literal">spark-worker</code> but created once the
        <code class="literal">monasca-transform</code> process begins processing streams.
        If the process fails on one node only, along with the pyspark process,
        it is likely that the <code class="literal">spark-worker</code> has failed to
        connect to the elected leader of the <code class="literal">spark-master</code>
        service. In this case the <code class="literal">spark-worker</code> service
        should be started on the affected node. If on multiple nodes check the
        <code class="literal">spark-worker</code>, <code class="literal">spark-master</code> and
        <code class="literal">monasca-transform</code> services and logs. If the
        <code class="literal">monasca-transform</code> or <code class="literal">spark</code>
        services have been interrupted this process may not re-appear for up to
        ten minutes (the stream processing interval).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name =
org.apache.spark.executor.CoarseGrainedExecutorBackend</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart process on affected node. Review logs.
       </p>
       <p>
        Child process of <code class="literal">spark-worker</code> but created once the
        <code class="literal">monasca-transform</code> process begins processing streams.
        If the process fails on one node only, along with the pyspark process,
        it is likely that the <code class="literal">spark-worker</code> has failed to
        connect to the elected leader of the <code class="literal">spark-master</code>
        service. In this case the <code class="literal">spark-worker</code> service
        should be started on the affected node. If on multiple nodes check the
        <code class="literal">spark-worker</code>, <code class="literal">spark-master</code> and
        <code class="literal">monasca-transform</code> services and logs. If the
        <code class="literal">monasca-transform</code> or <code class="literal">spark</code>
        services have been interrupted this process may not re-appear for up to
        ten minutes (the stream processing interval).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">process_name =
        monasca-transform</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">Restart the service on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.8.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MONITORING in Telemetery section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.8.8">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Persister Health Check
        <code class="literal">component = monasca-persister</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed
        or a dependency is out.
       </p>
      </td><td valign="top">
       <p>
        If the process has crashed, restart it using the steps below. If a
        dependent service is down, address that issue.
       </p>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags persister</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> API Health Check
        <code class="literal">component = monasca-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed
        or a dependency is out.
       </p>
      </td><td valign="top">
       <p>
        If the process has crashed, restart it using the steps below. If a
        dependent service is down, address that issue.
       </p>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Monasca Agent Collection Time</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the elapsed
        time the <code class="literal">monasca-agent</code> takes to collect metrics is
        high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy load on the box or
        a stuck agent plug-in.
       </p>
      </td><td valign="top">
       <p>
        Address the load issue on the machine. If needed, restart the agent
        using the steps below:
       </p>
       <p>
        Restart the agent on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-agent</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">component = kafka</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if Kafka is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Verify that Kafka is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = monasca-notification</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags notification</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags notification</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags notification</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = monasca-agent</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the agent on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-agent</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = monasca-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        &gt;Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name =
        monasca-persister</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags persister</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.nimbus
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="filename">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.6.17.4.5.8.8.2.1.4.9.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart <code class="literal">monasca-thresh</code>, if necessary, with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-thresh</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.supervisor
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="literal">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.6.17.4.5.8.8.2.1.4.10.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart monasca-thresh with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the monasca-thresh service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Start the monasca-thresh service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.worker
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="literal">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.6.17.4.5.8.8.2.1.4.11.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart <code class="literal">monasca-thresh</code> with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the <code class="literal">monasca-thresh</code> service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Start the <code class="literal">monasca-thresh</code> service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal"></code>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = monasca-thresh
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-thresh</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Use the Monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="console-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Console Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#console-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-console_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-console_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>console-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Console section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: HTTP Status</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span>
       <code class="literal">service=ops-console</code>
      </p>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> The Operations Console is
       unresponsive
      </p>
     </td><td valign="top">
      <p>
       Review logs in <code class="filename">/var/log/ops-console</code> and logs in
       <code class="filename">/var/log/apache2</code>. Restart ops-console by running
       the following commands on the Cloud Lifecycle Manager:
      </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-start.yml</pre></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: Process Check</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when the specified
       process is not running:
       <code class="literal">process_name=leia-leia_monitor</code>
      </p>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Process crashed or
       unresponsive.
      </p>
     </td><td valign="top">
      <p>
       Review logs in <code class="filename">/var/log/ops-console</code>. Restart
       ops-console by running the following commands on the Cloud Lifecycle Manager:
      </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-start.yml</pre></div>
     </td></tr></tbody></table></div></div><div class="sect3" id="system-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#system-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-system_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-system_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>system-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the System section and are set up per
  <code class="literal">hostname</code> and/or <code class="literal">mount_point</code>.
 </p><div class="sect4" id="id-1.6.17.4.5.10.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: SYSTEM</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.10.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-system_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-system_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: CPU Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on high CPU usage.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy load or runaway
        processes.
       </p>
      </td><td valign="top">Log onto the reporting host and diagnose the heavy CPU usage.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        elasticsearch</code> Elasticsearch disk low watermark. Backup
        indices. If high watermark is reached, indices will be deleted. Adjust
        curator_low_watermark_percent, curator_high_watermark_percent, and
        elasticsearch_max_total_indices_size_in_bytes if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Running out of disk
        space for <code class="filename">/var/lib/elasticsearch</code>.
       </p>
      </td><td valign="top">
       <p>
        Free up space by removing indices (backing them up first if desired).
        Alternatively, adjust <code class="literal">curator_low_watermark_percent</code>,
        <code class="literal">curator_high_watermark_percent</code>, and/or
        <code class="literal">elasticsearch_max_total_indices_size_in_bytes</code> if
        needed.
       </p>
       <p>
        For more information about how to back up your centralized logs, see
        <a class="xref" href="topic-ttn-5fg-4v.html#central-log-configure-settings" title="12.2.5. Configuring Centralized Logging">Section 12.2.5, “Configuring Centralized Logging”</a>.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        elasticsearch</code> Elasticsearch disk high watermark. Attempting
        to delete indices to free disk space. Adjust
        <code class="literal">curator_low_watermark_percent</code>,
        <code class="literal">curator_high_watermark_percent</code>, and
        <code class="literal">elasticsearch_max_total_indices_size_in_bytes</code> if
        needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Running out of disk
        space for <code class="filename">/var/lib/elasticsearch</code>
       </p>
      </td><td valign="top">
       <p>
        Verify that disk space was freed up by the curator. If needed, free up
        additional space by removing indices (backing them up first if
        desired). Alternatively, adjust curator_low_watermark_percent,
        curator_high_watermark_percent, and/or
        elasticsearch_max_total_indices_size_in_bytes if needed.
       </p>
       <p>
        For more information about how to back up your centralized logs, see
        <a class="xref" href="topic-ttn-5fg-4v.html#central-log-configure-settings" title="12.2.5. Configuring Centralized Logging">Section 12.2.5, “Configuring Centralized Logging”</a>.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Log Partition Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="filename">/var/log</code> disk space usage has crossed the low
        watermark. If the high watermark is reached,
        <code class="literal">logrotate</code> will be run to free up disk space. Adjust
        <code class="literal">var_log_low_watermark_percent</code> if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Log Partition High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="filename">/var/log</code> volume is running low on disk space.
        Logrotate will be run now to free up space. Adjust
        <code class="literal">var_log_high_watermark_percent</code> if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Crash Dump Count</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if it receives any
        metrics with <code class="literal">crash.dump_count</code> &gt; 0
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> When a crash dump is
        generated by kdump, the crash dump file is put into the
        <code class="filename">/var/crash</code> directory by default. Any crash dump
        files in this directory will cause the
        <code class="literal">crash.dump_count</code> metric to show a value greater than
        0.
       </p>
      </td><td valign="top">
       <p>
        Analyze the crash dump file(s) located in
        <code class="filename">/var/crash</code> on the host that generated the alarm to
        try to determine if a service or hardware caused the crash.
       </p>
       <p>
        Move the file to a new location so that a developer can take a look at
        it. Make sure all of the processes are back up after the crash (run the
        <code class="literal">&lt;service&gt;-status.yml</code> playbooks). When the
        <code class="filename">/var/crash</code> directory is empty, the <code class="literal">Crash
        Dump Count</code> alarm should transition back to OK.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Disk Inode Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Nearly out of inodes for
        a partition, as indicated by the <code class="literal">mount_point</code>
        reported.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Many files on the disk.
       </p>
      </td><td valign="top">Investigate cleanup of data or migration to other partitions.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Disk Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> High disk usage, as
        indicated by the <code class="literal">mount_point</code> reported.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Large files on the disk.
       </p>
      </td><td valign="top">
       <p>
        Investigate cleanup of data or migration to other partitions.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Host Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alerts when a host is
        unreachable. <code class="literal">test_type = ping</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Host or network is down.
       </p>
      </td><td valign="top">If a single host, attempt to restart the system. If multiple
      hosts, investigate network issues.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Memory Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> High memory usage.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Overloaded system or
        services with memory leaks.
       </p>
      </td><td valign="top">Log onto the reporting host to investigate high memory users.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Network Errors</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on a high network
        error rate.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Bad network or cabling.
       </p>
      </td><td valign="top">Take this host out of service until the network can be fixed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: NTP Time Sync</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the NTP time
        offset is high.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and check if the ntp service is running.
       </p>
       <p>
        If it is running, then use these steps:
       </p>
       <div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
          Stop the service:
         </p><div class="verbatim-wrap"><pre class="screen">service ntpd stop</pre></div></li><li class="listitem "><p>
          Resynchronize the node's time:
         </p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/ntpdate -b  &lt;ntp-server&gt;</pre></div></li><li class="listitem "><p>
          Restart the ntp service:
         </p><div class="verbatim-wrap"><pre class="screen">service ntp start</pre></div></li><li class="listitem "><p>
          Restart rsyslog:
         </p><div class="verbatim-wrap"><pre class="screen">service rsyslog restart</pre></div></li></ol></div>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="other-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Services Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#other-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>other-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Other Services section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Operations Console.
 </p><div class="sect4" id="id-1.6.17.4.5.11.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: APACHE</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.3">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Apache Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on failure to
        reach the Apache status endpoint.
       </p>
      </td><td valign="top"> </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = apache2</code>
       </p>
      </td><td valign="top">If the Apache process goes down, connect to the affected node via
      SSH and restart it with this command: <code class="command">sudo systemctl restart
      apache2</code>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Apache Idle Worker Count</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when there are no
        idle workers in the Apache server.
       </p>
      </td><td valign="top"> </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BACKUP in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = freezer-scheduler</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">Restart the process on the affected node. Review the associated logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable: <code class="literal">process_name =
        freezer-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> see
        <code class="literal">Description</code>
       </p>
      </td><td valign="top">see <code class="literal">Description</code></td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: HAPROXY in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.5">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = haproxy</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> HA Proxy is not running
        on this machine.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook on the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ARDANA-UX-SERVICES in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
      </td><td valign="top"> </td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: KEY-MANAGER in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal"></code>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = barbican-api</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Barbican start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
<div class="verbatim-wrap"><pre class="screen">component = barbican-api
api_endpoint = public or internal</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The endpoint is not
        responsive, it may be down.
       </p>
      </td><td valign="top">
       <p>
        For the HTTP Status alarms for the public and internal endpoints,
        restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the barbican service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Restart the barbican service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Examine the logs in <code class="filename">/var/log/barbican/</code> for
        possible error messages.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
<div class="verbatim-wrap"><pre class="screen">component = barbican-api
monitored_host_type = vip</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Barbican API on the
        admin virtual IP is down.
       </p>
      </td><td valign="top">This alarm is verifying access to the Barbican API via the virtual
      IP address (HAProxy). If this check is failing but the other two HTTP
      Status alarms for the key-manager service are not then the issue is
      likely with HAProxy so you should view the alarms for that service. If
      the other two HTTP Status alarms are alerting as well then restart
      Barbican using the steps listed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MYSQL in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.8">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: MySQL Slow Query Rate</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the slow
        query rate is high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The system load is too
        high.
       </p>
      </td><td valign="top">This could be an indication of near capacity limits or an exposed
      bad query. First, check overall system load and then investigate MySQL
      details.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> MySQL crashed.
       </p>
      </td><td valign="top">Restart MySQL on the affected node.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OCTAVIA in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.9">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. There are individual alarms for each of these
        processes:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          octavia-worker
         </p></li><li class="listitem "><p>
          octavia-housekeeping
         </p></li><li class="listitem "><p>
          octavia-api
         </p></li><li class="listitem "><p>
          octavia-health-manager
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Octavia start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The <code class="literal">octavia-api
        </code>process could be down or you could be experiencing an issue
        with either haproxy or another network related issue.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">octavia-api</code> process is down, restart it on
        the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Octavia start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If it is not the <code class="literal">octavia-process</code> that is the issue,
        then check if there is an issue with <code class="literal">haproxy</code> or
        possibly a network issue and troubleshoot accordingly.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ORCHESTRATION in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.10">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. There are individual alarms for each of these
        processes:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          heat-api
         </p></li><li class="listitem "><p>
          heat-api-cfn
         </p></li><li class="listitem "><p>
          heat-api-cloudwatch
         </p></li><li class="listitem "><p>
          heat-engine
         </p></li></ul></div>
       <p>
        heat-api process check on each node
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop all the Heat processes:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li><li class="step "><p>
          Start the Heat processes back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li></ol></div></div>
       <p>
        Review the relevant log at the following locations on the affected
        node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log
/var/log/heat/heat-engine.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          heat-api
         </p></li><li class="listitem "><p>
          heat-api-cfn
         </p></li><li class="listitem "><p>
          heat-api-cloudwatch
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Restart the Heat service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop all the Heat processes:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li><li class="step "><p>
          Start the Heat processes back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li></ol></div></div>
       <p>
        Review the relevant log at the following locations on the affected
        node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OVSVAPP-SERVICEVM in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.11">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span>Alarms when the specified
        process is not running:
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = ovs-vswitchd
process_name = neutron-ovsvapp-agent
process_name = ovsdb-server</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: RABBITMQ in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.12">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = rabbitmq
process_name = epmd</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: SPARK in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.13">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = org.apache.spark.deploy.master.Master
process_name = org.apache.spark.deploy.worker.Worker</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.14"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: WEB-UI in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.14">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Apache is not running or
        there is a misconfiguration.
       </p>
      </td><td valign="top">Check that Apache is running; investigate Horizon logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.6.17.4.5.11.15"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.8.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ZOOKEEPER in Other Services section</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.5.11.15">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name =
        org.apache.zookeeper.server</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">Restart the process on the affected node. Review the associated
      logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: ZooKeeper Latency</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the ZooKeeper
        latency is high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy system load.
       </p>
      </td><td valign="top">Check the individual system as well as activity across the entire
      service.</td></tr></tbody></table></div></div></div><div class="sect3" id="esx-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX vCenter Plugin Alarms</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#esx-alarmdefinitions">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-esx_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-esx_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>esx-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms relate to your ESX cluster, if you are utilizing one.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster CPU Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when average of CPU
       usage for a particular cluster exceeds 90% continuously for 3 polling
       cycles.
      </p>
      <p>
       Alarm will have the following dimension:
      </p>
<div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Virtual machines are
       consuming more than 90% of allocated vCPUs.
      </p>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Reduce the load on virtual machines with high consumption by
         restarting/stopping one or more services.
        </p></li><li class="listitem "><p>
         Add more vCPUs to the host(s) attached to the cluster.
        </p></li></ul></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster Disk Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span>
      </p>
      <div class="itemizedlist " id="ul-ctc-lpy-5x"><ul class="itemizedlist"><li class="listitem "><p>
         Alarms when the total size of the all shared datastores attached to
         the cluster exceeds 90% of their total allocated capacity.
        </p></li></ul></div>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Or in the case of cluster having a single host, the size of non-shared
         datastore exceeds 90% of its allocated capacity.
        </p></li><li class="listitem "><p>
         Alarm will have the following dimension:
        </p><div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div></li></ul></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span>
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Virtual machines occupying the storage.
        </p></li><li class="listitem "><p>
         Large file or image being copied on the datastore(s).
        </p></li></ul></div>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Check the virtual machines that are consuming more disk space. Delete
         unnecessary files.
        </p></li><li class="listitem "><p>
         Delete unnecessary files and images from database(s).
        </p></li><li class="listitem "><p>
         Add storage to the datastore(s).
        </p></li></ul></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster Memory Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when average of RAM
       memory usage for a particular cluster, exceeds 90% continuously for 3
       polling cycles.
      </p>
      <p>
       Alarm will have the following dimension:
      </p>
<div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Virtual machines are
       consuming more than 90% of their total allocated memory.
      </p>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Reduce the load on virtual machines with high consumption by
         restarting or stopping one or more services.
        </p></li><li class="listitem "><p>
         Add more memory to the host(s) attached to the cluster.
        </p></li></ul></div>
     </td></tr></tbody></table></div></div></div><div class="sect2" id="topic-ask-vgb-dv"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support Resources</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-ask-vgb-dv">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span>topic-ask-vgb-dv</li></ul></div></div></div></div><p>
  To solve issues in your cloud, consult the Knowledge Base or contact
  <span class="phrase"><span class="phrase">Sales Engineering</span></span>.
 </p><div class="sect3" id="kb"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Knowledge Base</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#kb">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span>kb</li></ul></div></div></div></div><p>
   Support information is available at the SUSE Support page <a class="link" href="https://www.suse.com/products/suse-openstack-cloud/" target="_blank">https://www.suse.com/products/suse-openstack-cloud/</a>. This page
   offers access to the Knowledge Base, forums and documentation.
  </p></div><div class="sect3" id="id-1.6.17.4.6.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Contacting SUSE Support</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.4.6.4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The central location for information about accessing and using SUSE
   Technical Support is available at <a class="link" href="https://www.suse.com/support/handbook/" target="_blank">https://www.suse.com/support/handbook/</a>. This page has
   guidelines and links to many online support services, such as support
   account management, incident reporting, issue reporting, feature requests,
   training, consulting.
  </p></div></div></div><div class="sect1" id="troubleshooting-controlplane"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-controlplane">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_controlplane.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_controlplane.xml</li><li><span class="ds-label">ID: </span>troubleshooting-controlplane</li></ul></div></div></div></div><p>
  Troubleshooting procedures for control plane services.
 </p><div class="sect2" id="recoverrabbit"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding and Recovering RabbitMQ after Failure</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#recoverrabbit">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>recoverrabbit</li></ul></div></div></div></div><p>
  RabbitMQ is the message queue service that runs on each of your controller
  nodes and brokers communication between multiple services in your
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud environment. It is important for cloud operators to
  understand how different troubleshooting scenarios affect RabbitMQ so they
  can minimize downtime in their environments. We are going to discuss multiple
  scenarios and how it affects RabbitMQ. We will also explain how you can
  recover from them if there are issues.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-recover-rabbit-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How upgrades affect RabbitMQ</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-recover-rabbit-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-recover-rabbit-xml-7</li></ul></div></div></div></div><p>
   There are two types of upgrades within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> -- major and minor. The
   effect that the upgrade process has on RabbitMQ depends on these types.
  </p><p>
   A major upgrade is defined by an erlang change or major version upgrade of
   RabbitMQ. A minor upgrade would be an upgrade where RabbitMQ stays within
   the same version, such as v3.4.3 to v.3.4.6.
  </p><p>
   During both types of upgrades there may be minor blips in the authentication
   process of client services as the accounts are recreated.
  </p><p>
   <span class="bold"><strong>RabbitMQ during a major upgrade</strong></span>
  </p><p>
   There will be a RabbitMQ service outage while the upgrade is performed.
  </p><p>
   During the upgrade, high availability consistency is compromised -- all but
   the primary node will go down and will be reset, meaning their database
   copies are deleted. The primary node is not taken down until the last step
   and then it is upgrade. The database of users and permissions is maintained
   during this process. Then the other nodes are brought back into the cluster
   and resynchronized.
  </p><p>
   <span class="bold"><strong>RabbitMQ during a minor upgrade</strong></span>
  </p><p>
   Minor upgrades are performed node by node. This "rolling" process means
   there should be no overall service outage because each node is taken out of
   its cluster in turn, its database is reset, and then it is added back to the
   cluster and resynchronized.
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-recover-rabbit-xml-8"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How RabbitMQ is affected by other operational processes</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-recover-rabbit-xml-8">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-recover-rabbit-xml-8</li></ul></div></div></div></div><p>
   There are operational tasks, such as <a class="xref" href="system-maintenance.html#stop-restart" title="13.1.1.1. Bringing Down Your Cloud: Services Down Method">Section 13.1.1.1, “Bringing Down Your Cloud: Services Down Method”</a>, where
   you use the <code class="filename">ardana-stop.yml</code> and
   <code class="filename">ardana-start.yml</code> playbooks to gracefully restart your cloud.
   If you use these playbooks, and there are no errors associated with them
   forcing you to troubleshoot further, then RabbitMQ is brought down
   gracefully and brought back up. There is nothing special to note regarding
   RabbitMQ in these normal operational processes.
  </p><p>
   However, there are other scenarios where an understanding of RabbitMQ is
   important when a graceful shutdown did not occur.
  </p><p>
   These examples that follow assume you are using one of the entry-scale
   models where RabbitMQ is hosted on your controller node cluster. If you are
   using a mid-scale model or have a dedicated cluster that RabbitMQ lives on
   you may need to alter the steps accordingly. To determine which nodes
   RabbitMQ is on you can use the <code class="filename">rabbit-status.yml</code> playbook
   from your Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div><p>
   <span class="bold"><strong>Your entire control plane cluster goes down</strong></span>
  </p><p>
   If you have a scenario where all of your controller nodes went down, either
   manually or via another process such as a power outage, then an
   understanding of how RabbitMQ should be brought back up is important. Follow
   these steps to recover RabbitMQ on your controller node cluster in these
   cases:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The order in which the nodes went down is key here. Locate the last node
     to go down as this will be used as the primary node when bringing the
     RabbitMQ cluster back up. You can review the timestamps in the
     <code class="filename">/var/log/rabbitmq</code> log file to determine what the last
     node was.
    </p><div id="id-1.6.17.5.3.4.8.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">primary</code> status of a node is transient, it only applies for the
      duration that this process is running. There is no long-term distinction
      between any of the nodes in your cluster. The primary node is simply
      the one that owns the RabbitMQ configuration database that will be
      synchronized across the cluster.
     </p></div></li><li class="step "><p>
     Run the <code class="filename">ardana-start.yml</code> playbook specifying the primary
     node (aka the last node down determined in the first step):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;hostname&gt;</pre></div><div id="id-1.6.17.5.3.4.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The <code class="literal">&lt;hostname&gt;</code> value will be the "shortname" for
      your node, as found in the <code class="filename">/etc/hosts</code> file.
     </p></div></li></ol></div></div><p>
   <span class="bold"><strong>If one of your controller nodes goes down</strong></span>
  </p><p>
   First step here is to determine whether the controller that went down is the
   primary RabbitMQ host or not. The primary host is going to be the first host
   member in the <code class="literal">FND-RMQ</code> group in the file below on your
   Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</pre></div><p>
   In this example below, <code class="literal">ardana-cp1-c1-m1-mgmt</code> would be the
   primary:
  </p><div class="verbatim-wrap"><pre class="screen">[FND-RMQ-ccp-cluster1:children]
ardana-cp1-c1-m1-mgmt
ardana-cp1-c1-m2-mgmt
ardana-cp1-c1-m3-mgmt</pre></div><p>
   If your primary RabbitMQ controller node has gone down and you need to bring
   it back up, you can follow these steps. In this playbook you are using the
   <code class="literal">rabbit_primary_hostname</code> parameter to specify the hostname
   for one of the other controller nodes in your environment hosting RabbitMQ,
   which will service as the primary node in the recovery. You will also use
   the <code class="literal">--limit</code> parameter to specify the controller node you
   are attempting to bring back up.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_bringing_up&gt;</pre></div><p>
   If the node you need to bring back is <span class="bold"><strong>not</strong></span>
   the primary RabbitMQ node then you can just run the
   <code class="filename">ardana-start.yml</code> playbook with the
   <code class="literal">--limit</code> parameter and your node should recover:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_bringing_up&gt;</pre></div><p>
   <span class="bold"><strong>If you are replacing one or more of your controller
   nodes</strong></span>
  </p><p>
   The same general process noted above is used if you are removing or
   replacing one or more of your controller nodes.
  </p><p>
   If your node needs minor hardware repairs, but does not need to be replaced
   with a new node, you should use the <code class="filename">ardana-stop.yml</code> playbook
   with the <code class="literal">--limit</code> parameter to stop services on that node
   prior to removing it from the cluster.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the <code class="filename">rabbitmq-stop.yml</code> playbook, specifying the
     hostname of the node you are removing, which will remove the node from the
     RabbitMQ cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</pre></div></li><li class="step "><p>
     Run the <code class="filename">ardana-stop.yml</code> playbook, again specifying the
     hostname of the node you are removing, which will stop the rest of the
     services and prepare it to be removed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</pre></div></li></ol></div></div><p>
   If your node cannot be repaired and needs to be replaced with another
   baremetal node, any references to the replaced node must be removed from the
   RabbitMQ cluster. This is because RabbitMQ associates a cookie with each
   node in the cluster which is derived, in part, by the specific hardware.  So
   it is possible to replace a hard drive in a node. However changing a
   motherboard or replacing the node with another node entirely may cause
   RabbitMQ to stop working. When this happens, the running RabbitMQ cluster
   must be edited from a running RabbitMQ node. The following steps show how to
   do this.
  </p><p>
   In this example, controller 3 is the node being replaced with the following
   steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="step "><p>
     SSH to a running RabbitMQ cluster node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh cloud-cp1-rmq-mysql-m1-mgmt</pre></div></li><li class="step "><p>
     Force the cluster to forget the node you are removing (in this example,
     the controller 3 node).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl forget_cluster_node \
rabbit@cloud-cp1-rmq-mysql-m3-mgmt</pre></div></li><li class="step "><p>
     Confirm that the node has been removed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl cluster_status</pre></div></li><li class="step "><p>
     On the replacement node, information and services related to RabbitMQ must be removed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl stop rabbitmq-server
<code class="prompt user">ardana &gt; </code>sudo systemctl stop epmd.socket&gt;</pre></div></li><li class="step "><p>
     Verify that the epmd service has stopped (kill it if it is still running).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ps -eaf | grep epmd.</pre></div></li><li class="step "><p>
     Remove the Mnesia database directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -rf /var/lib/rabbitmq/mnesia</pre></div></li><li class="step "><p>
     Restart the RabbitMQ server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl start rabbitmq-server</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, run the <code class="filename">ardana-start.yml</code> playbook.
    </p></li></ol></div></div><p>
   If the node you are removing/replacing is your primary host then when you
   are adding it to your cluster then you will want to ensure that you specify
   a new primary host when doing so, as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_adding&gt;</pre></div><p>
   If the node you are removing/replacing is not your primary host then you can
   add it as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_adding&gt;</pre></div><p>
   <span class="bold"><strong>If one of your controller nodes has rebooted or
   temporarily lost power</strong></span>
  </p><p>
   After a single reboot, RabbitMQ will not automatically restart. This is by
   design to protect your RabbitMQ cluster. To restart RabbitMQ, you should
   follow the process below.
  </p><p>
   If the rebooted node was your primary RabbitMQ host, you will specify a
   different primary hostname using one of the other nodes in your cluster:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_rebooted&gt;</pre></div><p>
   If the rebooted node was not the primary RabbitMQ host then you can just
   start it back up with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_that_rebooted&gt;</pre></div></div><div class="sect3" id="recovery"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering RabbitMQ</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#recovery">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>recovery</li></ul></div></div></div></div><p>
   In this section we will show you how to check the status of RabbitMQ and how
   to do a variety of disaster recovery procedures.
  </p><p>
   <span class="bold"><strong>Verifying the status of RabbitMQ</strong></span>
  </p><p>
   You can verify the status of RabbitMQ on each of your controller nodes by
   using the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the <code class="filename">rabbitmq-status.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div></li><li class="step "><p>
     If all is well, you should see an output similar to the following:
    </p><div class="verbatim-wrap"><pre class="screen">PLAY RECAP ********************************************************************
rabbitmq | status | Check RabbitMQ running hosts in cluster ------------- 2.12s
rabbitmq | status | Check RabbitMQ service running ---------------------- 1.69s
rabbitmq | status | Report status of RabbitMQ --------------------------- 0.32s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 4.36s
ardana-cp1-c1-m1-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m2-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m3-mgmt  : ok=2    changed=0    unreachable=0    failed=0</pre></div></li></ol></div></div><p>
   If one or more of your controller nodes are having RabbitMQ issues then
   continue reading, looking for the scenario that best matches yours.
  </p><p>
   <span class="bold"><strong>RabbitMQ recovery after a small network
   outage</strong></span>
  </p><p>
   In the case of a transient network outage, the version of RabbitMQ included
   with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is likely to recover automatically without any further
   action needed. However, if yours does not and the
   <code class="filename">rabbitmq-status.yml</code> playbook is reporting an issue then
   use the scenarios below to resolve your issues.
  </p><p>
   <span class="bold"><strong>All of your controller nodes have gone down and using
   other methods have not brought RabbitMQ back up</strong></span>
  </p><p>
   If your RabbitMQ cluster is irrecoverable and you need rapid service
   recovery because other methods either cannot resolve the issue or you do not
   have time to investigate more nuanced approaches then we provide a disaster
   recovery playbook for you to use. This playbook will tear down and reset any
   RabbitMQ services. This does have an extreme effect on your services. The
   process will ensure that the RabbitMQ cluster is recreated.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the RabbitMQ disaster recovery playbook. This generally takes around
     two minutes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</pre></div></li><li class="step "><p>
     Run the reconfigure playbooks for both Cinder (Block Storage) and Heat
     (Orchestration), if those services are present in your cloud. These
     services are affected when the fan-out queues are not recovered correctly.
     The reconfigure generally takes around five minutes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-server-configure.yml</pre></div></li><li class="step "><p>
     If you need to do a safe recovery of all the services in your environment
     then you can use this playbook. This is a more lengthy process as all
     services are inspected.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>One of your controller nodes has gone down and using
   other methods have not brought RabbitMQ back up</strong></span>
  </p><p>
   This disaster recovery procedure has the same caveats as the preceding one,
   but the steps differ.
  </p><p>
   If your primary RabbitMQ controller node has gone down and you need to
   perform a disaster recovery, use this playbook from your Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_needs_recovered&gt;</pre></div><p>
   If the controller node is not your primary, you can use this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml --limit &lt;hostname_of_node_that_needs_recovered&gt;</pre></div><p>
   No reconfigure playbooks are needed because all of the fan-out exchanges are
   maintained by the running members of your RabbitMQ cluster.
  </p></div></div></div><div class="sect1" id="ts-compute"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Compute Service</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#ts-compute">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>ts-compute</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Nova service.
 </p><p>
  Nova offers scalable, on-demand, self-service access to compute resources.
  You can use this guide to help with known issues and troubleshooting of Nova
  services.
 </p><div class="sect2" id="idg-all-operations-troubleshooting-ts-compute-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How can I reset the state of a compute instance?</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-ts-compute-xml-6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-compute-xml-6</li></ul></div></div></div></div><p>
   If you have an instance that is stuck in a non-Active state, such as
   <code class="literal">Deleting</code> or <code class="literal">Rebooting</code> and you want to
   reset the state so you can interact with the instance again, there is a way
   to do this.
  </p><p>
   The Nova command-line tool (also known as the Nova CLI or python-novaclient)
   has a command, <code class="literal">nova reset-state</code>, that allows you to reset
   the state of a server.
  </p><p>
   Here is the content of the help information about the command which shows
   the syntax:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova help reset-state
        usage: nova reset-state [--active] &lt;server&gt; [&lt;server&gt; ...]

        Reset the state of a server.

        Positional arguments:
        &lt;server&gt;  Name or ID of server(s).

        Optional arguments:
        --active  Request the server be reset to "active" state instead of "error"
        state (the default).</pre></div><p>
   If you had an instance that was stuck in a <code class="literal">Rebooting</code>
   state you would use this command to reset it back to
   <code class="literal">Active</code>:
  </p><div class="verbatim-wrap"><pre class="screen">nova reset-state --active &lt;instance_id&gt;</pre></div></div><div class="sect2" id="nova-consoleauth-troubleshoot"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting nova-consoleauth</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#nova-consoleauth-troubleshoot">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>nova-consoleauth-troubleshoot</li></ul></div></div></div></div><p>
   The nova-consoleauth service runs by default on the first controller node,
   that is, the host with <code class="literal">consoleauth_host_index=0</code>. If
   nova-consoleauth fails on the first controller node, you can switch it to
   another controller node by running the ansible playbook nova-start.yml and
   passing it the index of the next controller node.
  </p><p>
   The command to switch nova-consoleauth to another controller node
   (controller 2 for instance) is:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</pre></div><p>
   After you run this command you may now see two instances of the
   <code class="literal">nova-consoleauth</code> service, which will show as being in
   <code class="literal">disabled</code> state, when you run the <code class="literal">nova
   service-list</code> command. You can then delete the service using these
   steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Obtain the service ID for the duplicated nova-consoleauth service:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-list</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova service-list
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| Id | Binary           | Host                      | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 10 | nova-conductor   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:47.000000 | -               |
| 13 | nova-conductor   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 16 | nova-scheduler   | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:39.000000 | -               |
| 19 | nova-scheduler   | ...a-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 22 | nova-scheduler   | ...a-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:44.000000 | -               |
| 25 | nova-consoleauth | ...a-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2016-08-25T12:11:45.000000 | -               |
| 49 | nova-compute     | ...a-cp1-comp0001-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:48.000000 | -               |
| 52 | nova-compute     | ...a-cp1-comp0002-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:41.000000 | -               |
| 55 | nova-compute     | ...a-cp1-comp0003-mgmt | nova     | enabled  | up    | 2016-08-25T12:11:43.000000 | -               |
<span class="bold"><strong>| 70 | nova-consoleauth | ...a-cp1-c1-m3-mgmt    | internal | disabled | down  | 2016-08-25T12:10:40.000000 | -               |</strong></span>
+----+------------------+---------------------------+----------+----------+-------+----------------------------+-----------------+</pre></div></li><li class="step "><p>
     Delete the disabled duplicate service with this command:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete &lt;service_ID&gt;</pre></div><p>
     Given the example in the previous step, the command could be:
    </p><div class="verbatim-wrap"><pre class="screen">nova service-delete 70</pre></div></li></ol></div></div></div><div class="sect2" id="ts-resize"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the migrate or resize functions in Nova post-installation when using encryption</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#ts-resize">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>ts-resize</li></ul></div></div></div></div><p>
   If you have used encryption for your data when running the configuration
   processor during your cloud deployment and are enabling the Nova resize and
   migrate functionality after the initial installation, there is an issue that
   arises if you have made additional configuration changes that required you to
   run the configuration processor before enabling these features.
  </p><p>
   You will only experience an issue if you have enabled encryption. If you
   haven't enabled encryption, then there is no need to follow the procedure
   below. If you are using encryption and you have made a configuration change
   and run the configuration processor after your initial install or upgrade,
   and you have run the <code class="filename">ready-deployment.yml</code> playbook, and
   you want to enable migrate or resize in Nova, then the following steps will
   allow you to proceed. Note that the ansible vault key referred to below is
   the encryption key that you have provided to the configuration processor.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Checkout the ansible branch of your local git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git checkout ansible</pre></div></li><li class="step "><p>
     Do a git log, and pick the previous commit:
    </p><div class="verbatim-wrap"><pre class="screen">git log</pre></div><p>
     In this example below, the commit is
     <code class="literal">ac54d619b4fd84b497c7797ec61d989b64b9edb3</code>:
    </p><div class="verbatim-wrap"><pre class="screen">$ git log

              commit 69f95002f9bad0b17f48687e4d97b2a791476c6a
              Merge: 439a85e ac54d61
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 09:08:55 2016 +0000

              Merging promotion of saved output

              commit 439a85e209aeeca3ab54d1a9184efb01604dbbbb
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 09:08:24 2016 +0000

              Saved output from CP run on 1d3976dac4fd7e2e78afad8d23f7b64f9d138778

              commit <span class="bold"><strong>ac54d619b4fd84b497c7797ec61d989b64b9edb3</strong></span>
              Merge: a794083 66ffe07
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 08:32:04 2016 +0000

              Merging promotion of saved output</pre></div></li><li class="step "><p>
     Checkout the commit:
    </p><div class="verbatim-wrap"><pre class="screen">git checkout &lt;commit_ID&gt;</pre></div><p>
     Using the same example above, here is the command:
    </p><div class="verbatim-wrap"><pre class="screen">$ git checkout ac54d619b4fd84b497c7797ec61d989b64b9edb3
              Note: checking out 'ac54d619b4fd84b497c7797ec61d989b64b9edb3'.

              You are in 'detached HEAD' state. You can look around, make experimental
              changes and commit them, and you can discard any commits you make in this
              state without impacting any branches by performing another checkout.

              If you want to create a new branch to retain commits you create, you may
              do so (now or later) by using -b with the checkout command again. Example:

              git checkout -b new_branch_name

              HEAD is now at ac54d61... Merging promotion of saved output</pre></div></li><li class="step "><p>
     Change to the ansible output directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/stage/ansible/group_vars/</pre></div></li><li class="step "><p>
     View the <code class="literal">group_vars</code> file from the ansible vault - it
     will be of the form below, with your compute cluster name being the
     indicator:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;cloud name&gt;-&lt;control plane name&gt;-&lt;compute cluster name&gt;</pre></div><p>
     View this group_vars file from the ansible vault with this command which
     will prompt you for your vault password:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-vault view &lt;group_vars_file&gt;</pre></div></li><li class="step "><p>
     Search the contents of this file for the <code class="literal">nova_ssh_key</code>
     section which will contain both the private and public SSH keys which you
     should then save into a temporary file so you can use it in a later step.
    </p><p>
     Here is an example snippet, with the bold part being what you need to
     save:
    </p><div class="verbatim-wrap"><pre class="screen">NOV_KVM:
                vars:
                <span class="bold"><strong>              nova_ssh_key:
                  private: '-----BEGIN RSA PRIVATE KEY-----
                  MIIEpAIBAAKCAQEAv/hhekzykD2K8HnVNBKZcJWYrVlUyb6gR8cvE6hbh2ISzooA
                  jQc3xgglIwpt5TuwpTY3LL0C4PEHObxy9WwqXTHBZp8jg/02RzD02bEcZ1WT49x7
                  Rj8f5+S1zutHlDv7PwEIMZPAHA8lihfGFG5o+QHUmsUHgjShkWPdHXw1+6mCO9V/
                  eJVZb3nDbiunMOBvyyk364w+fSzes4UDkmCq8joDa5KkpTgQK6xfw5auEosyrh8D
                  zocN/JSdr6xStlT6yY8naWziXr7p/QhG44RPD9SSD7dhkyJh+bdCfoFVGdjmF8yA
                  h5DlcLu9QhbJ/scb7yMP84W4L5GwvuWCCFJTHQIDAQABAoIBAQCCH5O7ecMFoKG4
                  JW0uMdlOJijqf93oLk2oucwgUANSvlivJX4AGj9k/YpmuSAKvS4cnqZBrhDwdpCG
                  Q0XNM7d3mk1VCVPimNWc5gNiOBpftPNdBcuNryYqYq4WBwdq5EmGyGVMbbFPk7jH
                  ZRwAJ2MCPoplKl7PlGtcCMwNu29AGNaxCQEZFmztXcEFdMrfpTh3kuBI536pBlEi
                  Srh23mRILn0nvLXMAHwo94S6bI3JOQSK1DBCwtA52r5YgX0nkZbi2MvHISY1TXBw
                  SiWgzqW8dakzVu9UNif9nTDyaJDpU0kr0/LWtBQNdcpXnDSkHGjjnIm2pJVBC+QJ
                  SM9o8h1lAoGBANjGHtG762+dNPEUUkSNWVwd7tvzW9CZY35iMR0Rlux4PO+OXwNq
                  agldHeUpgG1MPl1ya+rkf0GD62Uf4LHTDgaEkUfiXkYtcJwHbjOnj3EjZLXaYMX2
                  LYBE0bMKUkQCBdYtCvZmo6+dfC2DBEWPEhvWi7zf7o0CJ9260aS4UHJzAoGBAOK1
                  P//K7HBWXvKpY1yV2KSCEBEoiM9NA9+RYcLkNtIy/4rIk9ShLdCJQVWWgDfDTfso
                  sJKc5S0OtOsRcomvv3OIQD1PvZVfZJLKpgKkt20/w7RwfJkYC/jSjQpzgDpZdKRU
                  vRY8P5iryptleyImeqV+Vhf+1kcH8t5VQMUU2XAvAoGATpfeOqqIXMpBlJqKjUI2
                  QNi1bleYVVQXp43QQrrK3mdlqHEU77cYRNbW7OwUHQyEm/rNN7eqj8VVhi99lttv
                  fVt5FPf0uDrnVhq3kNDSh/GOJQTNC1kK/DN3WBOI6hFVrmZcUCO8ewJ9MD8NQG7z
                  4NXzigIiiktayuBd+/u7ZxMCgYEAm6X7KaBlkn8KMypuyIsssU2GwHEG9OSYay9C
                  Ym8S4GAZKGyrakm6zbjefWeV4jMZ3/1AtXg4tCWrutRAwh1CoYyDJlUQAXT79Phi
                  39+8+6nSsJimQunKlmvgX7OK7wSp24U+SPzWYPhZYzVaQ8kNXYAOlezlquDfMxxv
                  GqBE5QsCgYA8K2p/z2kGXCNjdMrEM02reeE2J1Ft8DS/iiXjg35PX7WVIZ31KCBk
                  wgYTWq0Fwo2W/EoJVl2o74qQTHK0Bs+FTnR2nkVF3htEOAW2YXQTTN2rEsHmlQqE
                  A9iGTNwm9hvzbvrWeXtx8Zk/6aYfsXCoxq193KglS40shOCaXzWX0w==
                  -----END RSA PRIVATE KEY-----'
                  public: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/+GF6TPKQPYrwedU0Epl
                  wlZitWVTJvqBHxy8TqFuHYhLOigCNBzfGCCUjCm3lO7ClNjcsvQLg8Qc5vHL1bCpdMc
                  FmnyOD/TZHMPTZsRxnVZPj3HtGPx/n5LXO60eUO/s/AQgxk8AcDyWKF8YUbmj5Ad
                  SaxQeCNKGRY90dfDX7qYI71X94lVlvecNuK6cw4G/LKTfrjD59LN6zhQOSYKryOgNrkq
                  SlOBArrF/Dlq4SizKuHwPOhw38lJ2vrFK2VPrJjydpbOJevun9CEbjhE8P1JIPt2GTImH5t0
                  J+gVUZ2OYXzICHkOVwu71CFsn+xxvvIw/zhbgvkbC+5YIIUlMd
                  Generated Key for Nova User</strong></span>
                NTP_CLI:</pre></div></li><li class="step "><p>
     Switch back to the <code class="literal">site</code> branch by checking it out:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git checkout site</pre></div></li><li class="step "><p>
     Navigate to your group_vars directory in this branch:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/group_vars</pre></div></li><li class="step "><p>
     Edit your compute group_vars file, which will prompt you for your vault
     password:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-vault edit &lt;group_vars_file&gt;
              Vault password:
              Decryption successful</pre></div></li><li class="step "><p>
     Search the contents of this file for the <code class="literal">nova_ssh_key</code>
     section and replace the private and public keys with the contents that you
     had saved in a temporary file in step #7 earlier.
    </p></li><li class="step "><p>
     Remove the temporary file that you created earlier. You are now ready to
     run the deployment. For information about enabling Nova resizing and
     migration, see <a class="xref" href="ops-managing-compute.html#enabling-the-nova-resize" title="5.4. Enabling the Nova Resize and Migrate Features">Section 5.4, “Enabling the Nova Resize and Migrate Features”</a>.
    </p></li></ol></div></div></div><div class="sect2" id="idg-all-operations-troubleshooting-ts-compute-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (ESX)</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-ts-compute-xml-9">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-compute-xml-9</li></ul></div></div></div></div><p>

   <span class="bold"><strong>Unable to Create Instance Snapshot when Instance is
   Active</strong></span>
  </p><p>
   There is a known issue with VMWare vCenter where if you have a compute
   instance in <code class="literal">Active</code> state you will receive the error below
   when attempting to take a snapshot of it:
  </p><div class="verbatim-wrap"><pre class="screen">An error occurred while saving the snapshot: Failed to quiesce the virtual machine</pre></div><p>
   The workaround for this issue is to stop the instance. Here are steps to
   achieve this using the command line tool:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Stop the instance using the NovaClient:
    </p><div class="verbatim-wrap"><pre class="screen">nova stop &lt;instance UUID&gt;</pre></div></li><li class="step "><p>
     Take the snapshot of the instance.
    </p></li><li class="step "><p>
     Start the instance back up:
    </p><div class="verbatim-wrap"><pre class="screen">nova start &lt;instance UUID&gt;</pre></div></li></ol></div></div></div></div><div class="sect1" id="neutron-troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Service Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#neutron-troubleshooting">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>neutron-troubleshooting</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Networking service.
 </p><div class="sect2" id="network-fail-troublshoot"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Network failures</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#network-fail-troublshoot">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>network-fail-troublshoot</li></ul></div></div></div></div><p>
   <span class="bold"><strong>CVR HA - Split-brain result of failover of L3 agent
   when master comes back up</strong></span> This situation is specific to when L3
   HA is configured and a network failure occurs to the node hosting the
   currently active l3 agent. L3 HA is intended to provide HA in situations
   where the l3-agent crashes or the node hosting an l3-agent crashes/restarts.
   In the case of a physical networking issue which isolates the active l3
   agent, the stand-by l3-agent takes over but when the physical networking
   issue is resolved, traffic to the VMs is disrupted due to a "split-brain"
   situation in which traffic is split over the two L3 agents. The solution is
   to restart the L3-agent that was originally the master.
  </p><p>
   <span class="bold"><strong>OVSvApp loses connectivity with vCenter</strong></span> If
   the OVSvApp loses connectivity with the vCenter cluster, you will receive
   the following errors:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The OVSvApp VM will go into ERROR state
    </p></li><li class="step "><p>
     The OVSvApp VM will not get IP address
    </p></li></ol></div></div><p>
   When you see these symptoms:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Restart the OVSvApp agent on the OVSvApp VM.
    </p></li><li class="step "><p>
     Execute the following command to restart the Network (Neutron) service:
    </p><div class="verbatim-wrap"><pre class="screen">sudo service neutron-ovsvapp-agent restart</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Fail over a plain CVR router because the node became
   unavailable:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of l3 agent UUIDs which can be used in the commands that follow
    </p><div class="verbatim-wrap"><pre class="screen"> neutron agent-list | grep l3</pre></div></li><li class="step "><p>
     Determine the current host
    </p><div class="verbatim-wrap"><pre class="screen"> neutron l3-agent-list-hosting-router &lt;router uuid&gt;</pre></div></li><li class="step "><p>
     Remove the router from the current host
    </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-remove &lt;current l3 agent uuid&gt; &lt;router uuid&gt;</pre></div></li><li class="step "><p>
     Add the router to a new host
    </p><div class="verbatim-wrap"><pre class="screen">neutron l3-agent-router-add &lt;new l3 agent uuid&gt; &lt;router uuid&gt;</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Trouble setting maximum transmission units
   (MTU)</strong></span> <a class="xref" href="ops-managing-networking.html#configureMTU" title="9.3.11. Configuring Maximum Transmission Units in Neutron">Section 9.3.11, “Configuring Maximum Transmission Units in Neutron”</a>
  </p><p>
   <span class="bold"><strong>Floating IP on allowed_address_pair port with
   DVR-routed networks</strong></span> <code class="literal">allowed_address_pair</code>
  </p><p>
   <span class="bold"><strong>You may notice this issue:</strong></span> If you have an
   <code class="literal">allowed_address_pair</code> associated with multiple virtual
   machine (VM) ports, and if all the VM ports are ACTIVE, then the
   <code class="literal">allowed_address_pair</code> port binding will have the last
   ACTIVE VM's binding host as its bound host.
  </p><p>
   <span class="bold"><strong>In addition, you may notice</strong></span> that if the
   floating IP is assigned to the <code class="literal">allowed_address_pair</code> that
   is bound to multiple VMs that are ACTIVE, then the floating IP will not work
   with DVR routers. This is different from the centralized router behavior
   where it can handle unbound <code class="literal">allowed_address_pair</code> ports
   that are associated with floating IPs.
  </p><p>
   Currently we support <code class="literal">allowed_address_pair</code> ports with DVR
   only if they have floating IPs enabled, and have just one ACTIVE port.
  </p><p>
   Using the CLI, you can follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a network to add the host to:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron net-create vrrp-net</pre></div></li><li class="step "><p>
     Attach a subnet to that network with a specified allocation-pool range:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron subnet-create  --name vrrp-subnet --allocation-pool start=10.0.0.2,end=10.0.0.200 vrrp-net 10.0.0.0/24</pre></div></li><li class="step "><p>
     Create a router, uplink the vrrp-subnet to it, and attach the router to an
     upstream network called public:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron router-create router1
$ neutron router-interface-add router1 vrrp-subnet
$ neutron router-gateway-set router1 public</pre></div><p>
     Create a security group called vrrp-sec-group and add ingress rules to
     allow ICMP and TCP port 80 and 22:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron security-group-create vrrp-sec-group
$ neutron security-group-rule-create  --protocol icmp vrrp-sec-group
$ neutron security-group-rule-create  --protocol tcp  --port-range-min80 --port-range-max80 vrrp-sec-group
$ neutron security-group-rule-create  --protocol tcp  --port-range-min22 --port-range-max22 vrrp-sec-group</pre></div></li><li class="step "><p>
     Next, boot two instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova boot --num-instances 2 --image ubuntu-12.04 --flavor 1 --nic net-id=24e92ee1-8ae4-4c23-90af-accb3919f4d1 vrrp-node --security_groups vrrp-sec-group</pre></div></li><li class="step "><p>
     When you create two instances, make sure that both the instances are not
     in ACTIVE state before you associate the
     <code class="literal">allowed_address_pair</code>. The instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ nova list
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| ID                                   | Name                                            | Status | Task State | Power State | Networks                                               |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| 15b70af7-2628-4906-a877-39753082f84f | vrrp-node-15b70af7-2628-4906-a877-39753082f84f | ACTIVE  | -          | Running     | vrrp-net=10.0.0.3                                      |
| e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | vrrp-node-e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | DOWN    | -          | Running     | vrrp-net=10.0.0.4                                      |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+</pre></div></li><li class="step "><p>
     Create a port in the VRRP IP range that was left out of the ip-allocation
     range:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-create --fixed-ip ip_address=10.0.0.201 --security-group vrrp-sec-group vrrp-net
Created a new port:
+-----------------------+-----------------------------------------------------------------------------------+
| Field                 | Value                                                                             |
+-----------------------+-----------------------------------------------------------------------------------+
| admin_state_up        | True                                                                              |
| allowed_address_pairs |                                                                                   |
| device_id             |                                                                                   |
| device_owner          |                                                                                   |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                              |
| mac_address           | fa:16:3e:20:67:9f                                                                 |
| name                  |                                                                                   |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                              |
| port_security_enabled | True                                                                              |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                              |
| status                | DOWN                                                                              |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                  |
+-----------------------+-----------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Another thing to cross check after you associate the allowed_address_pair
     port to the VM port, is whether the
     <code class="literal">allowed_address_pair</code> port has inherited the VM's host
     binding:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron --os-username admin --os-password ZIy9xitH55 --os-tenant-name admin port-show f5a252b2-701f-40e9-a314-59ef9b5ed7de
+-----------------------+--------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                  |
+-----------------------+--------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                   |
| allowed_address_pairs |                                                                                                        |
| {color:red}binding:host_id{color} | ...-cp1-comp0001-mgmt                                                                      |
| binding:profile       | {}                                                                                                     |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                         |
| binding:vif_type      | ovs                                                                                                    |
| binding:vnic_type     | normal                                                                                                 |
| device_id             |                                                                                                        |
| device_owner          | compute:None                                                                                           |
| dns_assignment        | {"hostname": "host-10-0-0-201", "ip_address": "10.0.0.201", "fqdn": "host-10-0-0-201.openstacklocal."} |
| dns_name              |                                                                                                        |
| extra_dhcp_opts       |                                                                                                        |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"}                      |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                                                   |
| mac_address           | fa:16:3e:20:67:9f                                                                                      |
| name                  |                                                                                                        |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                                                   |
| port_security_enabled | True                                                                                                   |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                                                   |
| status                | DOWN                                                                                                   |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                                       |
+-----------------------+--------------------------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Note that you were allocated a port with the IP address 10.0.0.201 as
     requested. Next, associate a floating IP to this port to be able to access
     it publicly:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron floatingip-create --port-id=6239f501-e902-4b02-8d5c-69062896a2dd public
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.0.0.201                           |
| floating_ip_address | 10.36.12.139                         |
| floating_network_id | 3696c581-9474-4c57-aaa0-b6c70f2529b0 |
| id                  | a26931de-bc94-4fd8-a8b9-c5d4031667e9 |
| port_id             | 6239f501-e902-4b02-8d5c-69062896a2dd |
| router_id           | 178fde65-e9e7-4d84-a218-b1cc7c7b09c7 |
| tenant_id           | d4e4332d5f8c4a8eab9fcb1345406cb0     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Now update the ports attached to your VRRP instances to include this IP
     address as an allowed-address-pair so they will be able to send traffic
     out using this address. First find the ports attached to these instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-list -- --network_id=24e92ee1-8ae4-4c23-90af-accb3919f4d1
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                         |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d |      | fa:16:3e:7a:7b:18 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"}   |
| 14f57a85-35af-4edb-8bec-6f81beb9db88 |      | fa:16:3e:2f:7e:ee | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.2"}   |
| 6239f501-e902-4b02-8d5c-69062896a2dd |      | fa:16:3e:20:67:9f | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| 87094048-3832-472e-a100-7f9b45829da5 |      | fa:16:3e:b3:38:30 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.1"}   |
| c080dbeb-491e-46e2-ab7e-192e7627d050 |      | fa:16:3e:88:2e:e2 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.3"}   |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Add this address to the ports c080dbeb-491e-46e2-ab7e-192e7627d050 and
     12bf9ea4-4845-4e2c-b511-3b8b1ad7291d which are 10.0.0.3 and 10.0.0.4 (your
     vrrp-node instances):
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-update  c080dbeb-491e-46e2-ab7e-192e7627d050 --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201
$ neutron port-update  12bf9ea4-4845-4e2c-b511-3b8b1ad7291d --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201</pre></div></li><li class="step "><p>
     The allowed-address-pair 10.0.0.201 now shows up on the port:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron port-show12bf9ea4-4845-4e2c-b511-3b8b1ad7291d
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs | {"ip_address": "10.0.0.201", "mac_address": "fa:16:3e:7a:7b:18"}                |
| device_id             | e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6                                            |
| device_owner          | compute:None                                                                    |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"} |
| id                    | 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d                                            |
| mac_address           | fa:16:3e:7a:7b:18                                                               |
| name                  |                                                                                 |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                            |
| port_security_enabled | True                                                                            |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                |</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>OpenStack traffic that must traverse VXLAN tunnel
   dropped when using HPE 5930 switch</strong></span> Cause: UDP destination port
   4789 is conflicting with OpenStack VXLAN traffic.
  </p><p>
   There is a configuration setting you can use in the switch to configure the
   port number the HPN kit will use for its own VXLAN tunnels. Setting this to
   a port number other than the one Neutron will use by default (4789) will
   keep the HPN kit from absconding with Neutron's VXLAN traffic. Specifically:
  </p><p>
   <span class="bold"><strong>Parameters: </strong></span>
  </p><p>
   port-number: Specifies a UDP port number in the range of 1 to 65535. As a
   best practice, specify a port number in the range of 1024 to 65535 to avoid
   conflict with well-known ports.
  </p><p>
   <span class="bold"><strong>Usage guidelines:</strong></span>
  </p><p>
   You must configure the same destination UDP port number on all VTEPs in a
   VXLAN.
  </p><p>
   Examples
  </p><div class="verbatim-wrap"><pre class="screen"># Set the destination UDP port number to 6666 for VXLAN packets.
&lt;Sysname&gt; system-view
[Sysname] vxlan udp-port 6666</pre></div><p>
   Use vxlan udp-port to configure the destination UDP port number of VXLAN
   packets.   Mandatory for all VXLAN packets to specify a UDP port Default
   The destination UDP port number is 4789 for VXLAN packets.
  </p><p>
   OVS can be configured to use a different port number itself:
  </p><div class="verbatim-wrap"><pre class="screen"># (IntOpt) The port number to utilize if tunnel_types includes 'vxlan'. By
# default, this will make use of the Open vSwitch default value of '4789' if
# not specified.
#
# vxlan_udp_port =
# Example: vxlan_udp_port = 8472
#</pre></div><div class="sect3" id="DOCS-3584"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: PCI-PT virtual machine gets stuck at boot</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#DOCS-3584">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>DOCS-3584</li></ul></div></div></div></div><p>
    If you are using a machine that uses Intel NICs, if the PCI-PT virtual
    machine gets stuck at boot, the boot agent should be disabled.
   </p><p>
    When Intel cards are used for PCI-PT, sometimes the tenant virtual machine
    gets stuck at boot. If this happens, you should download Intel bootutils
    and use it to disable the bootagent.
   </p><p>
    Use the following steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download <code class="literal">preebot.tar.gz</code> from the
      <a class="link" href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers" target="_blank">Intel
      website</a>.
     </p></li><li class="step "><p>
      Untar the <code class="literal">preboot.tar.gz</code> file on the compute host
      where the PCI-PT virtual machine is to be hosted.
     </p></li><li class="step "><p>
      Go to path <code class="literal">~/APPS/BootUtil/Linux_x64</code> and then run
      following command:
     </p><div class="verbatim-wrap"><pre class="screen">./bootutil64e -BOOTENABLE disable -all</pre></div></li><li class="step "><p>
      Now boot the PCI-PT virtual machine and it should boot without getting
      stuck.
     </p></li></ol></div></div></div></div></div><div class="sect1" id="troubleshooting-glance"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting the Image (Glance) Service</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-glance">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_image.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_image.xml</li><li><span class="ds-label">ID: </span>troubleshooting-glance</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Glance service. We have
  gathered some of the common issues and troubleshooting steps that will help
  when resolving issues that occur with the Glance service.
 </p><div class="sect2" id="image-upload"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Images Created in Horizon UI Get Stuck in a Queued State</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#image-upload">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_image.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_image.xml</li><li><span class="ds-label">ID: </span>image-upload</li></ul></div></div></div></div><p>
   When creating a new image in the Horizon UI you will see the option for
   <code class="literal">Image Location</code> which allows you to enter a HTTP source to
   use when creating a new image for your cloud. However, this option is
   disabled by default for security reasons. This results in any new images
   created via this method getting stuck in a <code class="literal">Queued</code> state.
  </p><p>
   We cannot guarantee the security of any third party sites you use as image
   sources and the traffic goes over HTTP (non-SSL) traffic.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> You will need your cloud
   administrator to enable the HTTP store option in Glance for your cloud.
  </p><p>
   Here are the steps to enable this option:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the file below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/GLA-API/templates/glance-api.conf.j2</pre></div></li><li class="step "><p>
     Locate the Glance store options and add the <code class="literal">http</code> value
     in the <code class="literal">stores</code> field. It will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}</pre></div><p>
     Change this to:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}<span class="bold"><strong>,http</strong></span></pre></div></li><li class="step "><p>
     Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "adding HTTP option to Glance store list"</pre></div></li><li class="step "><p>
     Run the configuration processor with this command:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the Glance service reconfigure playbook which will update these
     settings:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-storage"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-storage">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_storage.xml</li><li><span class="ds-label">ID: </span>troubleshooting-storage</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for Swift services.
 </p><div class="sect2" id="troubleshooting-blockstorage"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-blockstorage">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>troubleshooting-blockstorage</li></ul></div></div></div></div><p>
  The block storage service utilizes <span class="productname">OpenStack</span> Cinder and can integrate with
  multiple back-ends including 3Par. Failures may exist at the Cinder API level,
  an operation may fail, or you may see an alarm trigger in the monitoring
  service. These may be caused by configuration problems, network issues, or
  issues with your servers or storage back-ends. The purpose of this page and
  section is to describe how the service works, where to find additional
  information, some of the common problems that come up, and how to address
  them.
 </p><div class="sect3" id="logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Where to find information</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#logs">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>logs</li></ul></div></div></div></div><p>
   When debugging block storage issues it is helpful to understand the
   deployment topology and know where to locate the logs with additional
   information.
  </p><p>
   The Cinder service consists of:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     An API service, typically deployed and active on the controller nodes.
    </p></li><li class="listitem "><p>
     A scheduler service, also typically deployed and active on the controller
     nodes.
    </p></li><li class="listitem "><p>
     A volume service, which is deployed on all of the controller nodes but
     only active on one of them.
    </p></li><li class="listitem "><p>
     A backup service, which is deployed on the same controller node as the
     volume service.
    </p></li></ul></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-troubleshooting-cinder_topology.png" target="_blank"><img src="images/media-hos.docs-troubleshooting-cinder_topology.png" width="" /></a></div></div><p>
   You can refer to your configuration files (usually located in
   <code class="literal">~/openstack/my_cloud/definition/</code> on the Cloud Lifecycle Manager) for
   specifics about where your services are located. They will usually be
   located on the controller nodes.
  </p><p>
   Cinder uses a MariaDB database and communicates between components by
   consuming messages from a RabbitMQ message service.
  </p><p>
   The Cinder API service is layered underneath a HAProxy service and accessed
   using a virtual IP address maintained using keepalived.
  </p><p>
   If any of the Cinder components is not running on its intended host then an
   alarm will be raised. Details on how to resolve these alarms can be found on
   our <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> page. You should check the logs for
   the service on the appropriate nodes. All Cinder logs are stored in
   <code class="literal">/var/log/cinder/</code> and all log entries above
   <code class="literal">INFO</code> level are also sent to the centralized logging
   service. For details on how to change the logging level of the Cinder
   service, see <a class="xref" href="topic-ttn-5fg-4v.html#central-log-configure-services" title="12.2.6. Configuring Settings for Other Services">Section 12.2.6, “Configuring Settings for Other Services”</a>.
  </p><p>
   In order to get the full context of an error you may need to examine the
   full log files on individual nodes. Note that if a component runs on more
   than one node you will need to review the logs on each of the nodes that
   component runs on. Also remember that as logs rotate that the time interval
   you are interested in may be in an older log file.
  </p><p>
   <span class="bold"><strong>Log locations:</strong></span>
  </p><p>
   <code class="literal">/var/log/cinder/cinder-api.log</code> - Check this log if you
   have endpoint or connectivity issues
  </p><p>
   <code class="literal">/var/log/cinder/cinder-scheduler.log</code> - Check this log if
   the system cannot assign your volume to a back-end
  </p><p>
   <code class="literal">/var/log/cinder/cinder-backup.log</code> - Check this log if you
   have backup or restore issues
  </p><p>
   <code class="literal">/var/log/cinder-cinder-volume.log</code> - Check here for
   failures during volume creation
  </p><p>
   <code class="literal">/var/log/nova/nova-compute.log</code> - Check here for failures
   with attaching volumes to compute instances
  </p><p>
   You can also check the logs for the database and/or the RabbitMQ service if
   your cloud exhibits database or messaging errors.
  </p><p>
   If the API servers are up and running but the API is not reachable then
   checking the HAProxy logs on the active keepalived node would be the place
   to look.
  </p><p>
   If you have errors attaching volumes to compute instances using the Nova API
   then the logs would be on the compute node associated with the instance. You
   can use the following command to determine which node is hosting the
   instance:
  </p><div class="verbatim-wrap"><pre class="screen">nova show &lt;instance_uuid&gt;</pre></div><p>
   Then you can check the logs located at
   <code class="literal">/var/log/nova/nova-compute.log</code> on that compute node.
  </p></div><div class="sect3" id="volume-states"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Cinder volume states</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#volume-states">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>volume-states</li></ul></div></div></div></div><p>
   Once the topology is understood, if the issue with the Cinder service
   relates to a specific volume then you should have a good understanding of
   what the various states a volume can be in are. The states are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     attaching
    </p></li><li class="listitem "><p>
     available
    </p></li><li class="listitem "><p>
     backing-up
    </p></li><li class="listitem "><p>
     creating
    </p></li><li class="listitem "><p>
     deleting
    </p></li><li class="listitem "><p>
     downloading
    </p></li><li class="listitem "><p>
     error
    </p></li><li class="listitem "><p>
     error attaching
    </p></li><li class="listitem "><p>
     error deleting
    </p></li><li class="listitem "><p>
     error detaching
    </p></li><li class="listitem "><p>
     error extending
    </p></li><li class="listitem "><p>
     error restoring
    </p></li><li class="listitem "><p>
     in-use
    </p></li><li class="listitem "><p>
     extending
    </p></li><li class="listitem "><p>
     restoring
    </p></li><li class="listitem "><p>
     restoring backup
    </p></li><li class="listitem "><p>
     retyping
    </p></li><li class="listitem "><p>
     uploading
    </p></li></ul></div><p>
   The common states are <code class="literal">in-use</code> which indicates a volume is
   currently attached to a compute instance and <code class="literal">available</code>
   means the volume is created on a back-end and is free to be attached to an
   instance. All <code class="literal">-ing</code> states are transient and represent a
   transition. If a volume stays in one of those states for too long indicating
   it is stuck, or if it fails and goes into an error state, you should check
   for failures in the logs.
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-blockstorage-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initial troubleshooting steps</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-ts-blockstorage-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-blockstorage-xml-7</li></ul></div></div></div></div><p>
   These should be the initial troubleshooting steps you go through.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If you have noticed an issue with the service, you should check your
     monitoring system for any alarms that may have triggered. See
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> for resolution steps for those alarms.
    </p></li><li class="step "><p>
     Check if the Cinder API service is active by listing the available volumes
     from the Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
openstack volume list</pre></div></li><li class="step "><p>
     Run a basic diagnostic from the Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts _cinder_post_check.yml</pre></div><p>
     This ansible playbook will list all volumes, create a 1 GB volume and then
     delete it using the v1 and v2 APIs, which will exercise basic Cinder
     capability.
    </p></li></ol></div></div></div><div class="sect3" id="common-issues"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Common failures</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#common-issues">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>common-issues</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Alerts from the Cinder service</strong></span>
  </p><p>
   Check for alerts associated with the block storage service, noting that
   these could include alerts related to the server nodes being down, alerts
   related to the messaging and database services, or the HAProxy and
   keepalived services, as well as alerts directly attributed to the block
   storage service.
  </p><p>
   The Operations Console provides a web UI method for checking
   alarms. See <span class="intraxref">Book “User Guide”, Chapter 1 “Using the Operations Console”, Section 1.1 “Operations Console Overview”</span> for details on how to connect to
   the Operations Console.
  </p><p>
   <span class="bold"><strong>Cinder volume service is down</strong></span>
  </p><p>
   The Cinder volume service could be down if the server hosting the
   volume service fails. (Running the command <code class="command">cinder
   service-list</code> will show the state of the volume service.) In this
   case you should follow the documented procedure linked below to start the
   volume service on another controller node. See <a class="xref" href="ops-managing-blockstorage.html#sec-operation-manage-block-storage" title="7.1.3. Managing Cinder Volume and Backup Services">Section 7.1.3, “Managing Cinder Volume and Backup Services”</a> for details.
  </p><p>
   <span class="bold"><strong>Creating a Cinder bootable volume fails</strong></span>
  </p><p>
   When creating a bootable volume from an image, your Cinder volume must be
   larger than the Virtual Size (raw size) of your image or creation will fail
   with an error.
  </p><p>
   An error like this error would appear in
   <code class="literal">cinder-volume.log</code> file:
  </p><div class="verbatim-wrap"><pre class="screen">'2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher ImageCopyFailure: Failed to copy image to volume: qemu-img: /dev/disk/by-path/ip-192.168.92.5:3260-iscsi-iqn.2003-10.com.lefthandnetworks:mg-ses:146:volume-c0e75c66-a20a-4368-b797-d70afedb45cc-lun-0: error while converting raw: Device is too small
2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher'</pre></div><p>
   In an example where creating a 1GB bootable volume fails, your image may
   look like this:
  </p><div class="verbatim-wrap"><pre class="screen">$ qemu-img info /tmp/image.qcow2
image: /tmp/image.qcow2
file format: qcow2
virtual size: 1.5G (1563295744 bytes)
disk size: 354M
cluster_size: 65536
...</pre></div><p>
   In this case, note that the image format is qcow2 and hte virtual size is
   1.5GB, which is greater than the size of the bootable volume. Even though
   the compressed image size is less than 1GB, this bootable volume creation
   will fail.
  </p><p>
   When creating your disk model for nodes that will have the cinder volume
   role make sure that there is sufficient disk space allocated for a temporary
   space for image conversion if you will be creating bootable volumes. You
   should allocate enough space to the filesystem as would be needed to cater
   for the raw size of images to be used for bootable volumes - for example
   Windows images can be quite large in raw format.
  </p><p>
   By default, Cinder uses <code class="literal">/var/lib/cinder</code> for image
   conversion and this will be on the root filesystem unless it is explicitly
   separated. You can ensure there is enough space by ensuring that the root
   file system is sufficiently large, or by creating a logical volume mounted
   at <code class="literal">/var/lib/cinder</code> in the disk model when installing the
   system.
  </p><p>
   If your system is already installed, use these steps to update this:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit the configuration item <code class="literal">image_conversion_dir</code> in
     <code class="literal">cinder.conf.j2</code> to point to another location with more
     disk space. Make sure that the new directory location has the same
     ownership and permissions as <code class="literal">/var/lib/cinder</code>
     (owner:cinder group:cinder. mode 0750).
    </p></li><li class="listitem "><p>
     Then run this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>API-level failures</strong></span>
  </p><p>
   If the API is inaccessible, determine if the API service is running on the
   target node. If it is not, check to see why the API service is not running in
   the log files. If it is running okay, check if the HAProxy service is
   functioning properly.
  </p><div id="id-1.6.17.9.3.6.20" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    After a controller node is rebooted, you must make sure to run the
    <code class="literal">ardana-start.yml</code> playbook to ensure all the services are
    up and running. For more information, see
    <a class="xref" href="system-maintenance.html#recover-downed-cluster" title="13.2.2.1. Restarting Controller Nodes After a Reboot">Section 13.2.2.1, “Restarting Controller Nodes After a Reboot”</a>.
   </p></div><p>
   If the API service is returning an error code, look for the error message in
   the API logs on all API nodes. Successful completions would be logged like
   this:
  </p><div class="verbatim-wrap"><pre class="screen">2016-04-25 10:09:51.107 30743 INFO eventlet.wsgi.server [<span class="bold"><strong>req-a14cd6f3-6c7c-4076-adc3-48f8c91448f6</strong></span>
dfb484eb00f94fb39b5d8f5a894cd163 7b61149483ba4eeb8a05efa92ef5b197 - - -] 192.168.186.105 - - [25/Apr/2016
10:09:51] "GET /v2/7b61149483ba4eeb8a05efa92ef5b197/volumes/detail HTTP/1.1" <span class="bold"><strong>200</strong></span> 13915 0.235921</pre></div><p>
   where <code class="literal">200</code> represents HTTP status 200 for a successful
   completion. Look for a line with your status code and then examine all
   entries associated with the request id. The request ID in the successful
   completion is highlighted in bold above.
  </p><p>
   The request may have failed at the scheduler or at the volume or backup
   service and you should also check those logs at the time interval of
   interest, noting that the log file of interest may be on a different node.
  </p><p>
   <span class="bold"><strong>Operations that do not complete</strong></span>
  </p><p>
   If you have started an operation, such as creating or deleting a volume,
   that does not complete, the Cinder volume may be stuck in a state. You
   should follow the procedures for detaling with stuck volumes.
  </p><p>
   There are six transitory states that a volume can get stuck in:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>State</th><th>Description</th></tr></thead><tbody><tr><td>creating</td><td>The Cinder volume manager has sent a request
                                                  to a back-end driver to create a volume, but has
                                                  not received confirmation that the volume is
                                                  available.</td></tr><tr><td>attaching</td><td>Cinder has received a request from Nova to
                                                  make a volume available for attaching to an
                                                  instance but has not received confirmation from
                                                  Nova that the attachment is complete.</td></tr><tr><td>detaching</td><td>Cinder has received notification from Nova
                                                  that it will detach a volume from an instance but
                                                  has not received notification that the detachment
                                                  is complete.</td></tr><tr><td>deleting</td><td>Cinder has received a request to delete a
                                                  volume but has not completed the
                                                  operation.</td></tr><tr><td>backing-up</td><td>Cinder backup manager has started to back a
                                                  volume up to Swift, or some other backup target,
                                                  but has not completed the operation.</td></tr><tr><td>restoring</td><td>Cinder backup manager has started to restore
                                                  a volume from Swift, or some other backup target,
                                                  but has not completed the operation.</td></tr></tbody></table></div><p>
   At a high level, the steps that you would take to address any of these
   states are similar:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Confirm that the volume is actually stuck, and not just temporarily
     blocked.
    </p></li><li class="listitem "><p>
     Where possible, remove any resources being held by the volume. For
     example, if a volume is stuck detaching it may be necessary to remove
     associated iSCSI or DM devices on the compute node.
    </p></li><li class="listitem "><p>
     Reset the state of the volume to an appropriate state, for example to
     <code class="literal">available</code> or <code class="literal">error</code>.
    </p></li><li class="listitem "><p>
     Do any final cleanup. For example, if you reset the state to
     <code class="literal">error</code> you can then delete the volume.
    </p></li></ol></div><p>
   The next sections will describe specific steps you can take for volumes
   stuck in each of the transitory states.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Creating</strong></span>
  </p><p>
   Broadly speaking, there are two possible scenarios where a volume would get
   stuck in <code class="literal">creating</code>. The <code class="literal">cinder-volume</code>
   service could have thrown an exception while it was attempting to create the
   volume, and failed to handle the exception correctly. Or the volume back-end
   could have failed, or gone offline, after it received the request from
   Cinder to create the volume.
  </p><p>
   These two cases are different in that for the second case you will need to
   determine the reason the back-end is offline and restart it. Often, when the
   back-end has been restarted, the volume will move from
   <code class="literal">creating</code> to <code class="literal">available</code> so your issue
   will be resolved.
  </p><p>
   If you can create volumes successfully on the same back-end as the volume
   stuck in <code class="literal">creating</code> then the back-end is not down. So you
   will need to reset the state for the volume and then delete it.
  </p><p>
   To reset the state of a volume you can use the <code class="literal">cinder
   reset-state</code> command. You can use either the UUID or the volume
   name of the stuck volume.
  </p><p>
   For example, here is a volume list where we have a stuck volume:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | creating  | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</pre></div><p>
   You can reset the state by using the <code class="literal">cinder reset-state</code>
   command, like this:
  </p><div class="verbatim-wrap"><pre class="screen">cinder reset-state --state error 14b76133-e076-4bd3-b335-fa67e09e51f6</pre></div><p>
   Confirm that with another listing:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | error     | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</pre></div><p>
   You can then delete the volume:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder delete 14b76133-e076-4bd3-b335-fa67e09e51f6
Request to delete volume 14b76133-e076-4bd3-b335-fa67e09e51f6 has been accepted.</pre></div><p>
   <span class="bold"><strong>Volumes stuck in Deleting</strong></span>
  </p><p>
   If a volume is stuck in the deleting state then the request to delete the
   volume may or may not have been sent to and actioned by the back-end. If you
   can identify volumes on the back-end then you can examine the back-end to
   determine whether the volume is still there or not. Then you can decide
   which of the following paths you can take. It may also be useful to
   determine whether the back-end is responding, either by checking for recent
   volume create attempts, or creating and deleting a test volume.
  </p><p>
   The first option is to reset the state of the volume to
   <code class="literal">available</code> and then attempt to delete the volume again.
  </p><p>
   The second option is to reset the state of the volume to
   <code class="literal">error</code> and then delete the volume.
  </p><p>
   If you have reset the volume state to <code class="literal">error</code> then the volume
   may still be consuming storage on the back-end. If that is the case then you
   will need to delete it from the back-end using your back-end's specific tool.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Attaching</strong></span>
  </p><p>
   The most complicated situation to deal with is where a volume is stuck
   either in attaching or detaching, because as well as dealing with the state
   of the volume in Cinder and the back-end, you have to deal with exports from
   the back-end, imports to the compute node, and attachments to the compute
   instance.
  </p><p>
   The two options you have here are to make sure that all exports and imports
   are deleted and to reset the state of the volume to
   <code class="literal">available</code> or to make sure all of the exports and imports
   are correct and to reset the state of the volume to
   <code class="literal">in-use</code>.
  </p><p>
   A volume that is in attaching state should never have been made available to
   a compute instance and therefore should not have any data written to it, or
   in any buffers between the compute instance and the volume back-end. In that
   situation, it is often safe to manually tear down the devices exported on
   the back-end and imported on the compute host and then reset the volume state
   to <code class="literal">available</code>.
  </p><p>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Detaching</strong></span>
  </p><p>
   The steps in dealing with a volume stuck in <code class="literal">detaching</code>
   state are very similar to those for a volume stuck in
   <code class="literal">attaching</code>. However, there is the added consideration that
   the volume was attached to, and probably servicing, I/O from a compute
   instance. So you must take care to ensure that all buffers are properly
   flushed before detaching the volume.
  </p><p>
   When a volume is stuck in <code class="literal">detaching</code>, the output from a
   <code class="literal">cinder list</code> command will include the UUID for the
   instance to which the volume was attached. From that you can identify the
   compute host that is running the instance using the <code class="literal">nova
   show</code> command.
  </p><p>
   For example, here are some snippets:
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder list
+--------------------------------------+-----------+-----------------------+-----------------+
|                  ID                  |   Status  |       Name            |   Attached to   |
+--------------------------------------+-----------+-----------------------+-----------------+
| 85384325-5505-419a-81bb-546c69064ec2 | detaching |        vol1           | 4bedaa76-78ca-… |
+--------------------------------------+-----------+-----------------------+-----------------+</pre></div><div class="verbatim-wrap"><pre class="screen">$ nova show 4bedaa76-78ca-4fe3-806a-3ba57a9af361|grep host
| OS-EXT-SRV-ATTR:host                 | mycloud-cp1-comp0005-mgmt
| OS-EXT-SRV-ATTR:hypervisor_hostname  | mycloud-cp1-comp0005-mgmt
| hostId                               | 61369a349bd6e17611a47adba60da317bd575be9a900ea590c1be816</pre></div><p>
   The first thing to check in this case is whether the instance is still
   importing the volume. Use <code class="literal">virsh list</code> and <code class="literal">virsh
   dumpxml</code> as described in the section above. If the XML for the
   instance has a reference to the device, then you should reset the volume
   state to <code class="literal">in-use</code> and attempt the <code class="literal">cinder
   detach</code> operation again.
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder reset-state --state in-use --attach-status attached 85384325-5505-419a-81bb-546c69064ec2</pre></div><p>
   If the volume gets stuck detaching again, there may be a more fundamental
   problem, which is outside the scope of this document and you should contact
   the Support team.
  </p><p>
   If the volume is not referenced in the XML for the instance then you should
   remove any devices on the compute node and back-end and then reset the state
   of the volume to <code class="literal">available</code>.
  </p><div class="verbatim-wrap"><pre class="screen">$ cinder reset-state --state available --attach-status detached 85384325-5505-419a-81bb-546c69064ec2</pre></div><p>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </p><p>
   <span class="bold"><strong>Volumes stuck in restoring</strong></span>
  </p><p>
   Restoring a Cinder volume from backup will be as slow as backing it up. So
   you must confirm that the volume is actually stuck by examining the
   <code class="literal">cinder-backup.log</code>. For example:
  </p><div class="verbatim-wrap"><pre class="screen"># tail -f cinder-backup.log |grep 162de6d5-ba92-4e36-aba4-e37cac41081b
2016-04-27 12:39:14.612 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - -
2016-04-27 12:39:15.533 6689 DEBUG cinder.backup.chunkeddriver [req-0c65ec42-8f9d-430a-b0d5-
2016-04-27 12:39:15.566 6689 DEBUG requests.packages.urllib3.connectionpool [req-0c65ec42-
2016-04-27 12:39:15.567 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - - -</pre></div><p>
   If you determine that the volume is genuinely stuck in
   <code class="literal">detaching</code> then you must follow the procedure described in
   the detaching section above to remove any volumes that remain exported from
   the back-end and imported on the controller node. Remember that in this case
   the volumes will be imported and mounted on the controller node running
   <code class="literal">cinder-backup</code>. So you do not have to search for the
   correct compute host. Also remember that no instances are involved so you do
   not need to confirm that the volume is not imported to any instances.
  </p></div><div class="sect3" id="debugging-attachment"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Debugging volume attachment</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#debugging-attachment">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>debugging-attachment</li></ul></div></div></div></div><p>
   In an error case, it is possible for a Cinder volume to fail to complete an
   operation and revert back to its initial state. For example, attaching a
   Cinder volume to a Nova instance, so you would follow the steps above to
   examine the Nova compute logs for the attach request.
  </p></div><div class="sect3" id="errors-creating"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Errors creating volumes</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#errors-creating">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>errors-creating</li></ul></div></div></div></div><p>
   If you are creating a volume and it goes into the <code class="literal">ERROR</code>
   state, a common error to see is <code class="literal">No valid host was found</code>.
   This means that the scheduler could not schedule your volume to a back-end.
   You should check that the volume service is up and running. You can use this
   command:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo cinder-manage service list
Binary           Host                                 Zone             Status     State Updated At
cinder-scheduler ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:30
cinder-volume    ha-volume-manager@ses1               nova             enabled    XXX   2016-04-25 11:27:26
cinder-backup    ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:28</pre></div><p>
   In this example, the state of <code class="literal">XXX</code> indicates that the
   service is down.
  </p><p>
   If the service is up, next check that the back-end has sufficient space. You
   can use this command to show the available and total space on each back-end:
  </p><div class="verbatim-wrap"><pre class="screen">cinder get-pools –detail</pre></div><p>
   If your deployment is using volume types, verify that the
   <code class="literal">volume_backend_name</code> in your
   <code class="literal">cinder.conf</code> file matches the
   <code class="literal">volume_backend_name</code> for the volume type you selected.
  </p><p>
   You can verify the back-end name on your volume type by using this command:
  </p><div class="verbatim-wrap"><pre class="screen">openstack volume type list</pre></div><p>
   Then list the details about your volume type. For example:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume type show dfa8ecbd-8b95-49eb-bde7-6520aebacde0
+---------------------------------+--------------------------------------+
| Field                           | Value                                |
+---------------------------------+--------------------------------------+
| description                     | None                                 |
| id                              | dfa8ecbd-8b95-49eb-bde7-6520aebacde0 |
| is_public                       | True                                 |
| name                            | my3par                               |
| os-volume-type-access:is_public | True                                 |
| properties                      | volume_backend_name='3par'           |
+---------------------------------+--------------------------------------+</pre></div></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-blockstorage-xml-12"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Diagnosing back-end issues</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-ts-blockstorage-xml-12">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-blockstorage-xml-12</li></ul></div></div></div></div><p>
   You can find further troubleshooting steps for specific back-end types by
   vising these pages:
  </p></div></div><div class="sect2" id="troubleshooting-objectstorage"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-objectstorage">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_swift.xml</li><li><span class="ds-label">ID: </span>troubleshooting-objectstorage</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Swift service. You can use
  these guides to help you identify and resolve basic problems you may
  experience while deploying or using the Object Storage service. It contains
  the following troubleshooting scenarios:
 </p><div class="sect3" id="topic-shs-j2b-kt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Fails With <span class="quote">“<span class="quote ">MSDOS Disks Labels Do Not Support Partition Names</span>”</span></span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-shs-j2b-kt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-deploy_fails_with_msdos_disks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-deploy_fails_with_msdos_disks.xml</li><li><span class="ds-label">ID: </span>topic-shs-j2b-kt</li></ul></div></div></div></div><p>
 Description
</p><p>
 If a disk drive allocated to Swift uses the MBR partition table type, the
 deploy process refuses to label and format the drive. This is to prevent
 potential data loss. (For more information, see <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.5 “Allocating Disk Drives for Object Storage”</span>. If you intend to use the disk drive for
 Swift, you must convert the MBR partition table to GPT on the drive using
 <code class="literal">/sbin/sgdisk</code>.
</p><div id="id-1.6.17.9.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   This process only applies to Swift drives. It does not apply to the
   operating system or boot drive.
  </p></div><p>
 Resolution
</p><p>
 You must install <code class="literal">gdisk</code>, before using
 <code class="literal">sgdisk</code>:
</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
   Run the following command to install <code class="literal">gdisk</code>:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper install gdisk</pre></div></li><li class="step "><p>
   Convert to the GPT partition type. Following is an example for
   converting <code class="literal">/dev/sdd</code> to the GPT partition type:
  </p><div class="verbatim-wrap"><pre class="screen">sudo sgdisk -g /dev/sdd</pre></div></li><li class="step "><p>
   Reboot the node to take effect. You may then resume the deployment
   (repeat the playbook that reported the error).
  </p></li></ol></div></div></div><div class="sect3" id="topic-ev4-q2b-kt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining Planned Ring Changes</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-ev4-q2b-kt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-examine_details_planned_ring_changes_prior_deploy.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-examine_details_planned_ring_changes_prior_deploy.xml</li><li><span class="ds-label">ID: </span>topic-ev4-q2b-kt</li></ul></div></div></div></div><p>
  Before making major changes to your rings, you can see the planned layout of
  Swift rings using the following steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Run the <code class="literal">swift-compare-model-rings.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div></li><li class="step "><p>
    Validate the following in the output:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Drives are being added to all rings in the ring specifications.
     </p></li><li class="listitem "><p>
      Servers are being used as expected (for example, you may have a different
      set of servers for the account/container rings than the object rings.)
     </p></li><li class="listitem "><p>
      The drive size is the expected size.
     </p></li></ul></div></li></ol></div></div></div><div class="sect3" id="sec-input-swift-error"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interpreting Swift Input Model Validation Errors</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-input-swift-error">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-interpreting_swift_validate_input_model.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-interpreting_swift_validate_input_model.xml</li><li><span class="ds-label">ID: </span>sec-input-swift-error</li></ul></div></div></div></div><p>
  The following examples provide an error message, description, and resolution.
 </p><div id="id-1.6.17.9.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
   To resolve an error, you must first modify the input model and re-run the
   configuration processor. (For instructions, see
   <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>.) Then, continue with the deployment.
  </p></div><div class="orderedlist " id="ol-vnh-g2b-kt"><ol class="orderedlist" type="1"><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Model Mismatch: Cannot find drive
    /dev/sdt on padawan-ccp-c1-m2 (192.168.245.3))</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The disk model used for node <span class="bold"><strong>padawan-ccp-c1-m2</strong></span> has drive
                                    <code class="literal">/dev/sdt</code> listed in the devices list of a
                                device-group where Swift is the consumer. However, the
                                    <code class="literal">dev/sdt</code> device does not exist on that
                                node.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>
        <p>
         If a drive or controller is failed on a node, the operating system
         does not see the drive and so the corresponding block device may not
         exist. Sometimes this is transitory and a reboot may resolve the
         problem. The problem may not be with <code class="literal">/dev/sdt</code>, but
         with another drive. For example, if <code class="literal">/dev/sds</code> is
         failed, when you boot the node, the drive that you expect to be called
         <code class="literal">/dev/sdt</code> is actually called
         <code class="literal">/dev/sds</code>.
        </p>
        <p>
         Alternatively, there may not be enough drives installed in the server.
         You can add drives. Another option is to remove
         <code class="literal">/dev/sdt</code> from the appropriate disk model. However,
         this removes the drive for all servers using the disk model.
        </p>
       </td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Model Mismatch: Cannot find drive
    /dev/sdd2 on padawan-ccp-c1-m2 (192.168.245.3)</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The disk model used for node<span class="bold"><strong>
                                    padawan-ccp-c1-m2</strong></span> has drive
                                    <code class="literal">/dev/sdt</code> listed in the devices list of a
                                device-group where Swift is the consumer. However, the partition
                                number (2) has been specified in the model. This is not supported -
                                only specify the block device name (for example
                                    <code class="literal">/dev/sdd</code>), not partition names in disk
                                models.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Remove the partition number from the disk model.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Cannot find IP address of
    padawan-ccp-c1-m3-swift for ring: account host:
    padawan-ccp-c1-m3-mgmt</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The service (in this example, swift-account) is running on the
                                node <span class="bold"><strong>padawan-ccp-c1-m3</strong></span>. However,
                                this node does not have a connection to the network designated for
                                the <code class="literal">swift-account</code> service (that is, the SWIFT
                                network).</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Check the input model for which networks are configured for each
                                node type.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Ring: object-2 has specified
    replication_policy and erasure_coding_policy. Only one may be specified.
    </strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>Only either <code class="literal">replication-policy</code> or
                                    <code class="literal">erasure-coding-policy</code> may be used in
                                    <code class="literal">ring-specifications</code>.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Remove one of the policy types.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Ring: object-3 is missing a policy
    type (replication-policy or erasure-coding-policy) </strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>There is no <code class="literal">replication-policy</code> or
                                    <code class="literal">erasure-coding-policy</code> section in
                                    <code class="literal">ring-specifications</code> for the object-0
                                ring.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Add a policy type to the input model file. </td></tr></tbody></table></div></li></ol></div></div><div class="sect3" id="topic-rtc-s3t-mt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying the Swift Ring Building Server</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-identify_ring_builder.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-identify_ring_builder.xml</li><li><span class="ds-label">ID: </span>topic-rtc-s3t-mt</li></ul></div></div></div></div><div class="sect4" id="idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identify the Swift Ring Building server</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-identify_ring_builder.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-identify_ring_builder.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6</li></ul></div></div></div></div><p>
   Perform the following steps to identify the Swift ring building server:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the following command:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml --limit SWF-ACC[0]</pre></div></li><li class="step "><p>
     Examine the output of this playbook. The last line underneath the play
     recap will give you the server name which is your Swift ring building
     server.
    </p><div class="verbatim-wrap"><pre class="screen">PLAY RECAP ********************************************************************
_SWF_CMN | status | Check systemd service running ----------------------- 1.61s
_SWF_CMN | status | Check systemd service running ----------------------- 1.16s
_SWF_CMN | status | Check systemd service running ----------------------- 1.09s
_SWF_CMN | status | Check systemd service running ----------------------- 0.32s
_SWF_CMN | status | Check systemd service running ----------------------- 0.31s
_SWF_CMN | status | Check systemd service running ----------------------- 0.26s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 7.88s
<span class="bold"><strong>ardana-cp1-c1-m1-mgmt</strong></span>      : ok=7    changed=0    unreachable=0    failed=0</pre></div><p>
     In the above example, the first swift proxy server is
     <code class="literal">ardana-cp1-c1-m1-mgmt</code>.
    </p></li></ol></div></div><div id="id-1.6.17.9.4.6.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg" /><h6>Important</h6><p>
    For the purposes of this document, any errors you see in the output of this
    playbook can be ignored if all you are looking for is the server name for
    your Swift ring builder server.
   </p></div></div></div><div class="sect3" id="verify-partition-label"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying a Swift Partition Label</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#verify-partition-label">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-label_on_partition.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-label_on_partition.xml</li><li><span class="ds-label">ID: </span>verify-partition-label</li></ul></div></div></div></div><div id="id-1.6.17.9.4.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   For a system upgrade do NOT clear the label before starting the upgrade.
  </p></div><p>
  This topic describes how to check whether a device has a label on a
  partition.
 </p><div class="sect4" id="label-partition"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Check Partition Label</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#label-partition">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-label_on_partition.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-label_on_partition.xml</li><li><span class="ds-label">ID: </span>label-partition</li></ul></div></div></div></div><p>
   To check whether a device has label on a partition, perform the following
   step:
  </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
     Log on to the node and use the <code class="literal">parted</code> command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo parted -l</pre></div><p>
     The output lists all of the block devices. Following is an example output
     for <code class="literal">/dev/sdc</code> with a single partition and a label of
     <span class="bold"><strong>c0a8f502h000</strong></span>.
     Because the partition has a label, if you are about to install and deploy
     the system, you must clear this label before starting the deployment. As
     part of the deployment process, the system will label the partition.
    </p><div class="verbatim-wrap"><pre class="screen">.
.
.
Model: QEMU QEMU HARDDISK (scsi)
Disk /dev/sdc: 20.0GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system  Name           Flags
1       1049kB  20.0GB  20.0GB  xfs          <span class="bold"><strong>c0a8f502h000</strong></span>

.
.
.</pre></div></li></ul></div></div></div></div><div class="sect3" id="verify-the-filesystem-label"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying a Swift File System Label</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#verify-the-filesystem-label">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-filesystem_label.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_label.xml</li><li><span class="ds-label">ID: </span>verify-the-filesystem-label</li></ul></div></div></div></div><div id="id-1.6.17.9.4.8.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
   For a system upgrade do NOT clear the label before starting the upgrade.
  </p></div><p>
  This topic describes how to check whether a file system in a partition has a
  label.
 </p><p>
  To check whether a file system in a partition has a label, perform the
  following step:
 </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
    Log on to the server and execute the <code class="literal">xfs_admin</code> command
    (where <code class="literal">/dev/sdc1</code> is the partition where the file system
    is located):
   </p><div class="verbatim-wrap"><pre class="screen">sudo xfs_admin -l /dev/sdc1</pre></div><p>
    The output shows if a file system has a label. For example, this shows a
    label of <span class="bold"><strong>c0a8f502h000</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">$ sudo xfs_admin -l /dev/sdc1
label = "<span class="bold"><strong>c0a8f502h000</strong></span>"</pre></div><p>
    If no file system exists, the result is as follows:
   </p><div class="verbatim-wrap"><pre class="screen">$ sudo xfs_admin -l /dev/sde1
xfs_admin: /dev/sde is not a valid XFS file system (unexpected SB magic number 0x00000000)</pre></div><p>
    If you are about to install and deploy the system, you must delete the
    label before starting the deployment. As part of the deployment process,
    the system will label the partition.
   </p></li></ul></div></div></div><div class="sect3" id="topic-gbz-13t-mt"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering Swift Builder Files</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-recovering_builder_file.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-recovering_builder_file.xml</li><li><span class="ds-label">ID: </span>topic-gbz-13t-mt</li></ul></div></div></div></div><p>
  When you execute the deploy process for a system, a copy of the builder files
  are stored on the following nodes and directories:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    On the Swift ring building node, the primary reference copy is stored in
    the
    <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
    directory.
   </p></li><li class="step "><p>
    On the next node after the Swift ring building node, a backup copy is
    stored in the
    <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
    directory.
   </p></li><li class="step "><p>
    In addition, in the deploy process, the builder files are also copied to
    the <code class="literal">/etc/swiftlm/deploy_dir/&lt;cloud-name&gt;</code> directory
    on every Swift node.
   </p></li></ol></div></div><p>
  If a copy of the builder files are found in the
  <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
  then no further recover action is needed. However, if all nodes running the
  Swift account (SWF-ACC) are lost, then you need to copy the files from the
  <code class="literal">/etc/swiftlm/deploy_dir/&lt;cloud-name&gt;</code> directory from
  an intact Swift node to the
  <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
  directory on the primary Swift ring building node.
 </p><p>
  If you have no intact <code class="literal">/etc/swiftlm</code> directory on any Swift
  node, you may be able to restore from Freezer. See
  <a class="xref" href="system-maintenance.html#recovering-controller-nodes" title="13.2.2.2. Recovering the Control Plane">Section 13.2.2.2, “Recovering the Control Plane”</a>.
 </p><p>
  To restore builder files from the <code class="literal">/etc/swiftlm/deploy_dir</code>
  directory, use the following process:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Swift ring building server (To identify the Swift ring
    building server, see <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt" title="15.6.2.4. Identifying the Swift Ring Building Server">Section 15.6.2.4, “Identifying the Swift Ring Building Server”</a>).
   </p></li><li class="step "><p>
    Create the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir</code> directory structure
    with these commands:
   </p><p>
    Replace <em class="replaceable ">CLOUD_NAME</em> with the name of your cloud
    and <em class="replaceable ">CONTROL_PLANE_NAME</em> with the name of your
    control plane.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo mkdir -p /etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/
<code class="prompt user">tux &gt; </code>sudo chown -R ardana.ardana /etc/swiftlm/</pre></div></li><li class="step "><p>
    Log in to a Swift node where an intact
    <code class="literal">/etc/swiftlm/deploy_dir</code> directory exists.
   </p></li><li class="step "><p>
    Copy the builder files to the Swift ring building node. In the example
    below we use scp to transfer the files, where
    <code class="literal">swpac-c1-m1-mgmt</code> is the ring building node,
    <code class="literal">cloud1</code> is the cloud, and <code class="literal">cp1</code> is the
    control plane name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp /etc/swiftlm//cloud1/cp1/* swpac-ccp-c1-m1-mgmt:/etc/swiftlm/cloud1/cp1/builder_dir/</pre></div></li><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Run the Swift reconfigure playbook to make sure every Swift node has the
    same rings:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="sec-trouble-restart-storeage-deployment"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting the Object Storage Deployment</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-trouble-restart-storeage-deployment">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>sec-trouble-restart-storeage-deployment</li></ul></div></div></div></div><p>
  This page describes the various operational procedures performed by Swift.
 </p><div class="sect4" id="restart-swift-depl"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Swift Object Storage Deployment</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#restart-swift-depl">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>restart-swift-depl</li></ul></div></div></div></div><p>
   The structure of ring is built in an incremental stages. When you modify a
   ring, the new ring uses the state of the old ring as a basis for the new
   ring. Rings are stored in the builder file. The
   <code class="literal">swiftlm-ring-supervisor</code> stores builder files in the
   <code class="literal">/etc/swiftlm/cloud1/cp1/builder_dir/</code>
   directory on the Ring-Builder node. The builder files are named
   &lt;ring-name&gt; builder. Prior versions of the builder files are stored in
   the
   <code class="literal">/etc/swiftlm/cloud1/cp1/builder_dir/backups</code>
   directory.
  </p><p>
   Generally, you use an existing builder file as the basis for changes to a
   ring. However, at initial deployment, when you create a ring there will be
   no builder file. Instead, the first step in the process is to build a
   builder file. The deploy playbook does this as a part of the deployment
   process. If you have successfully deployed some of the system, the ring
   builder files will exist.
  </p><p>
   If you change your input model (for example, by adding servers) now, the
   process assumes you are <span class="emphasis"><em>modifying</em></span> a ring and behaves
   differently than while creating a ring from scratch. In this case, the ring
   is not balanced. So, if the cloud model contains an error or you decide to
   make substantive changes, it is a best practice to start from scratch and
   build rings using the steps below.
  </p></div><div class="sect4" id="reset-builder-files"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reset Builder Files</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#reset-builder-files">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>reset-builder-files</li></ul></div></div></div></div><p>
   You must reset the builder files during the initial deployment process
   (only). This process should be used only when you want to restart a
   deployment from scratch. If you reset the builder files after completing
   your initial deployment, then you are at a risk of losing critical system
   data.
  </p><p>
   Delete the builder files in the
   <code class="filename">/etc/swiftlm/cloud1/cp1/builder-dir/</code>
   directory. For example, for the region0 Keystone region (the default single
   region designation), do the following:
  </p><div class="verbatim-wrap"><pre class="screen">sudo rm /etc/swiftlm/cloud1/cp1/builder_dir/*.builder</pre></div><div id="id-1.6.17.9.4.10.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you have successfully deployed a system and accidentally delete the
    builder files, you can recover to the correct state. For instructions, see
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="15.6.2.7. Recovering Swift Builder Files">Section 15.6.2.7, “Recovering Swift Builder Files”</a>.
   </p></div></div></div><div class="sect3" id="increase-timeout"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increasing the Swift Node Timeout Value</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#increase-timeout">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-increase_timeout.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-increase_timeout.xml</li><li><span class="ds-label">ID: </span>increase-timeout</li></ul></div></div></div></div><p>
  On a heavily loaded Object Storage system timeouts may occur when
  transferring data to or from Swift, particularly large objects.
 </p><p>
  The following is an example of a timeout message in the log
  (<code class="literal">/var/log/swift/swift.log</code>) on a Swift proxy server:
 </p><div class="verbatim-wrap"><pre class="screen">Jan 21 16:55:08 ardana-cp1-swpaco-m1-mgmt proxy-server: ERROR with Object server 10.243.66.202:6000/disk1 re: Trying to write to
/v1/AUTH_1234/testcontainer/largeobject: ChunkWriteTimeout (10s)</pre></div><p>
  If this occurs, it may be necessary to increase the
  <code class="literal">node_timeout</code> parameter in the
  <code class="literal">proxy-server.conf</code> configuration file.
 </p><p>
  The <code class="literal">node_timeout</code> parameter in the Swift
  <code class="literal">proxy-server.conf</code> file is the maximum amount of time the
  proxy server will wait for a response from the account, container, or object
  server. The default value is 10 seconds.
 </p><p>
  In order to modify the timeout you can use these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Edit the
    <code class="literal">~/openstack/my_cloud/config/swift/proxy-server.conf.j2</code> file
    and add a line specifying the <code class="literal">node_timeout</code> into the
    <code class="literal">[app:proxy-server]</code> section of the file.
   </p><p>
    Example, in bold, increasing the timeout to 30 seconds:
   </p><div class="verbatim-wrap"><pre class="screen">[app:proxy-server]
use = egg:swift#proxy
.
.
<span class="bold"><strong>node_timeout = 30</strong></span></pre></div></li><li class="step "><p>
    Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Installing with Cloud Lifecycle Manager</em>”, Chapter 10 “Using Git for Configuration Management”</span>, as follows:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Use the playbook below to create a deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Change to the deployment directory and run the Swift reconfigure playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="swift-filesystem-ts"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.6.2.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Swift File System Usage Issues</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#swift-filesystem-ts">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml</li><li><span class="ds-label">ID: </span>swift-filesystem-ts</li></ul></div></div></div></div><p>
  If you have recycled your environment to do a re-installation and you haven't
  run the <code class="literal">wipe_disks.yml</code> playbook in the process, you may
  experience an issue where your file system usage continues to grow
  exponentially even though you are not adding any files to your Swift system.
  This is likely occurring because the quarantined directory is getting filled
  up. You can find this directory at
  <code class="literal">/srv/node/disk0/quarantined</code>.
 </p><p>
  You can resolve this issue by following these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    SSH to each of your Swift nodes and stop the replication processes on each
    of them. The following commands must be executed on each of your Swift
    nodes. Make note of the time that you performed this action as you will
    reference it in step three.
   </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop swift-account-replicator
sudo systemctl stop swift-container-replicator
sudo systemctl stop swift-object-replicator</pre></div></li><li class="step "><p>
    Examine the <code class="literal">/var/log/swift/swift.log</code> file for events
    that indicate when the auditor processes have started and completed audit
    cycles. For more details, see <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#swift-filesystem-ts" title="15.6.2.10. Troubleshooting Swift File System Usage Issues">Section 15.6.2.10, “Troubleshooting Swift File System Usage Issues”</a>.
   </p></li><li class="step "><p>
    Wait until you see that the auditor processes have finished two complete
    cycles since the time you stopped the replication processes (from step
    one). You must check every Swift node, which on a lightly loaded system
    that was recently installed this should take less than two hours.
   </p></li><li class="step "><p>
    At this point you should notice that your quarantined directory has stopped
    growing. You may now delete the files in that directory on each of your
    nodes.
   </p></li><li class="step "><p>
    Restart the replication processes using the Swift start playbook:
   </p><ol type="a" class="substeps "><li class="step "><p>
      Log in to the Cloud Lifecycle Manager.
     </p></li><li class="step "><p>
      Run the Swift start playbook:
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-start.yml</pre></div></li></ol></li></ol></div></div><div class="sect4" id="swift-log"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.6.2.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining the Swift Log for Audit Event Cycles</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#swift-log">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml</li><li><span class="ds-label">ID: </span>swift-log</li></ul></div></div></div></div><p>
   Below is an example of the <code class="literal">object-server</code> start and end
   cycle details. They were taken by using the following command on a Swift
   node:
  </p><div class="verbatim-wrap"><pre class="screen">sudo grep object-auditor /var/log/swift/swift.log|grep ALL</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo grep object-auditor /var/log/swift/swift.log|grep ALL
...
Apr  1 13:31:18 padawan-ccp-c1-m1-mgmt object-auditor: Begin object audit "forever" mode (ALL)
Apr  1 13:31:18 padawan-ccp-c1-m1-mgmt object-auditor: Object audit (ALL). Since Fri Apr  1 13:31:18 2016: Locally: 0 passed, 0 quarantined, 0 errors files/sec: 0.00 , bytes/sec: 0.00, Total time: 0.00, Auditing time: 0.00, Rate: 0.00
Apr  1 13:51:32 padawan-ccp-c1-m1-mgmt object-auditor: Object audit (ALL) "forever" mode completed: 1213.78s. Total quarantined: 0, Total errors: 0, Total files/sec: 7.02, Total bytes/sec: 9999722.38, Auditing time: 1213.07, Rate: 1.00</pre></div><p>
   In this example, the auditor started at <code class="literal">13:31</code> and ended
   at <code class="literal">13:51</code>.
  </p><p>
   In this next example, the <code class="literal">account-auditor</code> and
   <code class="literal">container-auditor</code> use similar message structure, so we
   only show the container auditor. You can substitute
   <code class="literal">account</code> for <code class="literal">container</code> as well:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo grep container-auditor /var/log/swift/swift.log
...
Apr  1 14:07:00 padawan-ccp-c1-m1-mgmt container-auditor: Begin container audit pass.
Apr  1 14:07:00 padawan-ccp-c1-m1-mgmt container-auditor: Since Fri Apr  1 13:07:00 2016: Container audits: 42 passed audit, 0 failed audit
Apr  1 14:37:00 padawan-ccp-c1-m1-mgmt container-auditor: Container audit pass completed: 0.10s</pre></div><p>
   In the example, the container auditor started a cycle at
   <code class="literal">14:07</code> and the cycle finished at <code class="literal">14:37</code>.
  </p></div></div></div></div><div class="sect1" id="monitoring-logging-usage-reporting"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#monitoring-logging-usage-reporting">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_telemetry.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_telemetry.xml</li><li><span class="ds-label">ID: </span>monitoring-logging-usage-reporting</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Monitoring, Logging, and
  Usage Reporting services.
 </p><div class="sect2" id="sec-central-log-troubleshoot"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Centralized Logging</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-troubleshoot">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-troubleshoot</li></ul></div></div></div></div><p>
  This section contains the following scenarios:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-review-logs" title="15.7.1.1. Reviewing Log Files">Section 15.7.1.1, “Reviewing Log Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-monitor" title="15.7.1.2. Monitoring Centralized Logging">Section 15.7.1.2, “Monitoring Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-log-collection" title="15.7.1.3. Situations In Which Logs Might Not Be Collected">Section 15.7.1.3, “Situations In Which Logs Might Not Be Collected”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-error-kibana" title="15.7.1.4. Error When Creating a Kibana Visualization">Section 15.7.1.4, “Error When Creating a Kibana Visualization”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-store-log" title="15.7.1.5. After Deploying Logging-API, Logs Are Not Centrally Stored">Section 15.7.1.5, “After Deploying Logging-API, Logs Are Not Centrally Stored”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-slow-log" title="15.7.1.6. Re-enabling Slow Logging">Section 15.7.1.6, “Re-enabling Slow Logging”</a>
   </p></li></ul></div><div class="sect3" id="sec-central-log-review-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reviewing Log Files</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-review-logs">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-review-logs</li></ul></div></div></div></div><p>
   You can troubleshoot service-specific issues by reviewing the logs. After
   logging into Kibana, follow these steps to load the logs for viewing:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Settings</strong></span> menu to
     configure an index pattern to search for.
    </p></li><li class="listitem "><p>
     In the <span class="bold"><strong>Index name or pattern</strong></span> field, you
     can enter <code class="literal">logstash-*</code> to query all Elasticsearch
     indices.
    </p></li><li class="listitem "><p>
     Click the green <span class="bold"><strong>Create</strong></span> button to create
     and load the index.
    </p></li><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Discover</strong></span> menu to load the
     index and make it available to search.
    </p></li></ol></div><div id="id-1.6.17.10.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
    If you want to search specific Elasticsearch indices, you can run the
    following command from the control plane to get a full list of available
    indices:
   </p><div class="verbatim-wrap"><pre class="screen">curl localhost:9200/_cat/indices?v</pre></div></div><p>
   Once the logs load you can change the timeframe from the dropdown in the
   upper-righthand corner of the Kibana window. You have the following options
   to choose from:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Quick</strong></span> - a variety of time frame choices
     will be available here
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Relative</strong></span> - allows you to select a start
     time relative to the current time to show this range
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Absolute</strong></span> - allows you to select a date
     range to query
    </p></li></ul></div><p>
   When searching there are common fields you will want to use, such as:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>type</strong></span> - this will include the service
     name, such as <code class="literal">keystone</code> or <code class="literal">ceilometer</code>
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>host </strong></span>- you can specify a specific host to
     search for in the logs
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>file</strong></span> - you can specify a specific log
     file to search
    </p></li></ul></div><p>
   For more details on using Kibana and Elasticsearch to query logs, see
   <a class="link" href="https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html" target="_blank">https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html</a>
  </p></div><div class="sect3" id="sec-central-log-monitor"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Centralized Logging</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-monitor">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-monitor</li></ul></div></div></div></div><p>
   To help keep ahead of potential logging issues and resolve issues before
   they affect logging, you may want to monitor the Centralized Logging Alarms.
  </p><p>
   <span class="bold"><strong>To monitor logging alarms:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Operations Console.
    </p></li><li class="listitem "><p>
     From the menu button in the upper left corner, navigate to the
     <span class="bold"><strong>Alarm Definitions</strong></span> page.
    </p></li><li class="listitem "><p>
     Find the alarm definitions that are applied to the various hosts. See the
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a> for the Centralized Logging Alarm
     Definitions.
    </p></li><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Alarms</strong></span> page
    </p></li><li class="listitem "><p>
     Find the alarm definitions applied to the various hosts. These should
     match the alarm definitions in the <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a>.
    </p></li><li class="listitem "><p>
     See if the alarm is green (good) or is in a bad state. If any are in a bad
     state, see the possible actions to perform in the
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="15.1.1. Alarm Resolution Procedures">Section 15.1.1, “Alarm Resolution Procedures”</a>.
    </p></li></ol></div><p>
   You can use this filtering technique in the "Alarms" page to look for the
   following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To look for processes that may be down, filter for
     <span class="bold"><strong>"Process"</strong></span> then make sure the process are
     up:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Elasticsearch
      </p></li><li class="listitem "><p>
       Logstash
      </p></li><li class="listitem "><p>
       Beaver
      </p></li><li class="listitem "><p>
       Apache (Kafka)
      </p></li><li class="listitem "><p>
       Kibana
      </p></li><li class="listitem "><p>
       Monasca
      </p></li></ul></div></li><li class="listitem "><p>
     To look for sufficient disk space, filter for
     <span class="bold"><strong>"Disk"</strong></span>
    </p></li><li class="listitem "><p>
     To look for sufficient RAM memory, filter for
     <span class="bold"><strong>"Memory"</strong></span>
    </p></li></ol></div></div><div class="sect3" id="sec-central-log-log-collection"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Situations In Which Logs Might Not Be Collected</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-log-collection">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-log-collection</li></ul></div></div></div></div><p>
   Centralized logging might not collect log data under the following
   circumstances:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If the Beaver service is not running on one or more of the nodes
     (controller or compute), logs from these nodes will not be collected.
    </p></li></ul></div></div><div class="sect3" id="sec-central-log-error-kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error When Creating a Kibana Visualization</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-error-kibana">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-error-kibana</li></ul></div></div></div></div><p>
   When creating a visualization in Kibana you may get an error similiar to
   this:
  </p><div class="verbatim-wrap"><pre class="screen">"logstash-*" index pattern does not contain any of the following field types: number</pre></div><p>
   To resolve this issue:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Kibana.
    </p></li><li class="listitem "><p>
     Navigate to the <code class="literal">Settings</code> page.
    </p></li><li class="listitem "><p>
     In the left panel, select the <code class="literal">logstash-*</code> index.
    </p></li><li class="listitem "><p>
     Click the <span class="bold"><strong>Refresh</strong></span> button. You may see a
     mapping conflict warning after refreshing the index.
    </p></li><li class="listitem "><p>
     Re-create the visualization.
    </p></li></ol></div></div><div class="sect3" id="sec-central-log-store-log"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">After Deploying Logging-API, Logs Are Not Centrally Stored</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-store-log">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-store-log</li></ul></div></div></div></div><p>
   If you are using the Logging-API and logs are not being centrally stored,
   use the following checklist to troubleshoot Logging-API.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       <p>
        Ensure Monasca is running.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Check any alarms Monasca has triggered.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Check to see if the Logging-API (monasca-log-api) process alarm has
        triggered.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Run an Ansible playbook to get status of the Cloud Lifecycle Manager:
       </p>
<div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div>
      </td></tr><tr><td> </td><td>
       <p>
        Troubleshoot all specific tasks that have failed on the Cloud Lifecycle Manager.
       </p>
      </td></tr><tr><td> </td><td>Ensure that the Logging-API daemon is up.</td></tr><tr><td> </td><td>
       <p>
        Run an Ansible playbook to try and bring the Logging-API daemon up:
       </p>
<div class="verbatim-wrap"><pre class="screen">ansible-playbook –I hosts/verb_hosts logging-start.yml</pre></div>
      </td></tr><tr><td> </td><td>
       <p>
        If you get errors trying to bring up the daemon, resolve them.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Verify the Logging-API configuration settings are correct in the
        configuration file:
       </p>
<div class="verbatim-wrap"><pre class="screen">roles/kronos-api/templates/kronos-apache2.conf.j2</pre></div>
      </td></tr></tbody></table></div><p>
   The following is a sample Logging-API configuration file:
  </p><div class="verbatim-wrap"><pre class="screen">{#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
#}
Listen {{ kronos_api_host }}:{{ kronos_api_port }}
&lt;VirtualHost *:{{ kronos_api_port }}&gt;
    WSGIDaemonProcess log-api processes=4 threads=4 socket-timeout=300  user={{ kronos_user }} group={{ kronos_group }} python-path=/opt/stack/service/kronos/venv:/opt/stack/service/kronos/venv/bin/../lib/python2.7/site-packages/ display-name=monasca-log-api
    WSGIProcessGroup log-api
    WSGIApplicationGroup log-api
    WSGIScriptAlias / {{ kronos_wsgi_dir }}/app.wsgi
    ErrorLog /var/log/kronos/wsgi.log
    LogLevel info
    CustomLog /var/log/kronos/wsgi-access.log combined

    &lt;Directory /opt/stack/service/kronos/venv/bin/../lib/python2.7/site-packages/monasca_log_api&gt;
      Options Indexes FollowSymLinks MultiViews
      Require all granted
      AllowOverride None
      Order allow,deny
      allow from all
      LimitRequestBody 102400
    &lt;/Directory&gt;

    SetEnv no-gzip 1
&lt;/VirtualHost&gt;</pre></div></div><div class="sect3" id="sec-central-log-slow-log"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-enabling Slow Logging</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-slow-log">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-slow-log</li></ul></div></div></div></div><p>
   MariaDB slow logging was enabled by default in earlier versions. Slow
   logging logs slow MariaDB queries to
   <code class="filename">/var/log/mysql/mysql-slow.log</code> on
   FND-MDB hosts.
  </p><p>
   As it is possible for temporary tokens to be logged to the slow log, we have
   disabled slow log in this version for security reasons.
  </p><p>
   To re-enable slow logging follow the following procedure:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to the Cloud Lifecycle Manager and set a mariadb service configurable to
     enable slow logging.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud</pre></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Check slow_query_log is currently disabled with a value of 0:
      </p><div class="verbatim-wrap"><pre class="screen">grep slow ./config/percona/my.cfg.j2
slow_query_log          = 0
slow_query_log_file     = /var/log/mysql/mysql-slow.log</pre></div></li><li class="listitem "><p>
       Enable slow logging in the server configurable template file and confirm
       the new value:
      </p><div class="verbatim-wrap"><pre class="screen">sed -e 's/slow_query_log = 0/slow_query_log = 1/' -i ./config/percona/my.cfg.j2
grep slow ./config/percona/my.cfg.j2
slow_query_log          = 1
slow_query_log_file     = /var/log/mysql/mysql-slow.log</pre></div></li><li class="listitem "><p>
       Commit the changes:
      </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Enable Slow Logging"</pre></div></li></ol></div></li><li class="listitem "><p>
     Run the configuration procesor.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     You will be prompted for an encryption key, and also asked if you want to
     change the encryption key to a new value, and it must be a different key.
     You can turn off encryption by typing the following:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</pre></div></li><li class="listitem "><p>
     Create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Reconfigure Percona (note this will restart your mysqld server on your
     cluster hosts).
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts percona-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="topic1976"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage Reporting Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic1976">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>topic1976</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Ceilometer service.
 </p><p>
  This page describes troubleshooting scenarios for Ceilometer.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-ts-metering-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-ts-metering-xml-6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-metering-xml-6</li></ul></div></div></div></div><p>
   Logs for the various running components in the Overcloud Controllers can be
   found at <span class="emphasis"><em>/var/log/ceilometer.log</em></span>
  </p><p>
   The Upstart for the services also logs data at
   <span class="bold"><strong>/var/log/upstart</strong></span>
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-metering-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-ts-metering-xml-7">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-metering-xml-7</li></ul></div></div></div></div><p>
   Change the level of debugging in Ceilometer by editing the
   <span class="bold"><strong>ceilometer.conf</strong></span>
   file located at
   <span class="bold"><strong>/etc/ceilometer/ceilometer.conf</strong></span>.
   To log the maximum amount of information, change the
   <span class="bold"><strong>level</strong></span>
   entry to <span class="bold"><strong>DEBUG</strong></span>.
  </p><p>
   <span class="bold"><strong>Note</strong></span>: When the logging level for a service
   is changed, that service must be re-started before the change will take
   effect.
  </p><p>
   This is an excerpt of the <span class="bold"><strong>ceilometer.conf</strong></span>
   configuration file showing where to make changes:
  </p><div class="verbatim-wrap"><pre class="screen">[loggers]
 keys: root

[handlers]
 keys: watchedfile, logstash

[formatters]
 keys: context, logstash

[logger_root]
 qualname: root
 handlers: watchedfile, logstash
 level: NOTSET</pre></div></div><div class="sect3" id="qerrors"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.7.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Messaging/Queuing Errors</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#qerrors">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>qerrors</li></ul></div></div></div></div><p>
   Ceilometer relies on a message bus for passing data between the various
   components. In high-availability scenarios, RabbitMQ servers are used for
   this purpose. If these servers are not available, the Ceilometer log will
   record errors during "Connecting to AMQP" attempts.
  </p><p>
   These errors may indicate that the RabbitMQ messaging nodes are not running
   as expected and/or the RPC publishing pipeline is stale. When these errors
   occur, re-start the instances.
  </p><p>
   Example error:
  </p><div class="verbatim-wrap"><pre class="screen">Error: unable to connect to node 'rabbit@xxxx-rabbitmq0000': nodedown</pre></div><p>
   Use the RabbitMQ CLI to re-start the instances and then the host.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Restart the downed cluster node.
    </p><div class="verbatim-wrap"><pre class="screen">sudo invoke-rc.d rabbitmq-server start</pre></div></li><li class="listitem "><p>
     Restart the RabbitMQ host
    </p><div class="verbatim-wrap"><pre class="screen">sudo rabbitmqctl start_app</pre></div></li></ol></div></div></div></div><div class="sect1" id="topic-ly3-yyr-st"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup and Restore Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-ly3-yyr-st">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_bura.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_bura.xml</li><li><span class="ds-label">ID: </span>topic-ly3-yyr-st</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Backup and Restore
  service.
 </p><p>
  The following logs will help you troubleshoot Freezer functionality:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><tbody><tr><td>Component</td><td>Description</td></tr><tr><td>Freezer Client</td><td>
      <p>
       /var/log/freezer-agent/freezer-agent.log
      </p>
     </td></tr><tr><td>Freezer Scheduler</td><td>/var/log/freezer-agent/freezer-scheduler.log</td></tr><tr><td>Freezer API</td><td>/var/log/freezer-api/freezer-api-access.log/var/log/freezer-api/freezer-api-modwsgi.log
            /var/log/freezer-api/freezer-api.log</td></tr></tbody></table></div><p>
  The following issues apply to the Freezer UI and the backup and restore
  process:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The UI for backup and restore is supported only if you log in as
    "ardana_backup". All other users will see the UI panel but the UI will not
    work.
   </p></li><li class="listitem "><p>
    If a backup or restore action fails via the UI, you must check the Freezer
    logs for details of the failure.
   </p></li><li class="listitem "><p>
    Job Status and Job Result on the UI and backend (CLI) are not in sync.
   </p></li><li class="listitem "><p>
    For a given "Action" the following modes are not supported from the UI:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Microsoft SQL Server
     </p></li><li class="listitem "><p>
      Cinder
     </p></li><li class="listitem "><p>
      Nova
     </p></li></ul></div></li><li class="listitem "><p>
    Start and end dates and times available for job creation should not be used
    due to a known issue. Please refrain from using those fields.
   </p></li><li class="listitem "><p>
    Once a backup is created. A listing of the contents is needed to verify if
    the backup of any single item was done.
   </p></li></ul></div></div><div class="sect1" id="troubleshooting-orchestration"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Orchestration Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-orchestration">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_orchestration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_orchestration.xml</li><li><span class="ds-label">ID: </span>troubleshooting-orchestration</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Orchestration services.
  Troubleshooting scenarios with resolutions for the Orchestration services.
 </p><div class="sect2" id="troubleshootingHeat"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Heat Troubleshooting</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshootingHeat">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>troubleshootingHeat</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Heat service. This page
  describes troubleshooting scenarios for Heat.
 </p><div class="sect3" id="rpc-timeout-heat"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RPC timeout on Heat stack creation</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#rpc-timeout-heat">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>rpc-timeout-heat</li></ul></div></div></div></div><p>
   If you exerience a remote procedure call (RPC) timeout failure when
   attempting heat stack-create, you can work around the issue by increasing
   the timeout value and purging records of deleted stacks from the database.
   To do so, follow the steps below. An example of the error is:
  </p><div class="verbatim-wrap"><pre class="screen">MessagingTimeout: resources.XXX-LCP-Pair01.resources[0]: Timed out waiting for a reply to message ID e861c4e0d9d74f2ea77d3ec1984c5cb6</pre></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Increase the timeout value.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/config/heat</pre></div></li><li class="step "><p>
     Make changes to heat config files. In heat.conf.j2 add this timeout value:
    </p><div class="verbatim-wrap"><pre class="screen">rpc_response_timeout=300</pre></div><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "some message"</pre></div></li><li class="step "><p>
     Move to ansible directory and run the following playbooks:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Change to the scratch directory and run heat-reconfigure:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li><li class="step "><p>
     Purge records of deleted stacks from the database. First delete all stacks
     that are in failed state. Then execute the following
    </p><div class="verbatim-wrap"><pre class="screen">sudo /opt/stack/venv/heat-20151116T000451Z/bin/python2
/opt/stack/service/heat-engine/venv/bin/heat-manage
--config-file /opt/stack/service/heat-engine-20151116T000451Z/etc/heat/heat.conf
--config-file /opt/stack/service/heat-engine-20151116T000451Z/etc/heat/engine.conf purge_deleted 0</pre></div></li></ol></div></div></div><div class="sect3" id="hrat-stack-create-errors"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General Heat stack creation errors</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#hrat-stack-create-errors">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>hrat-stack-create-errors</li></ul></div></div></div></div><p>
   In Heat, in general when a timeout occurs it means that the underlying
   resource service such as Nova, Neutron, or Cinder, fails to complete the
   required action. No matter what error this underlying service reports, Heat
   simply reports it back. So in the case of time-out in Heat stack create, you
   should look at the logs of the underlying services, most importantly the
   Nova service, to understand the reason for the timeout.
  </p></div><div class="sect3" id="heat-stack-create-failure"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Heat stack create failure</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#heat-stack-create-failure">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>heat-stack-create-failure</li></ul></div></div></div></div><p>
   The Monasca AlarmDefinition resource,
   <code class="literal">OS::Monasca::AlarmDefinition</code> used for Heat autoscaling,
   consists of an optional property
   <span class="bold"><strong>name</strong></span> for
   defining the alarm name. In case this optional property being specified in
   the Heat template, this name must be unique in the same project of the
   system. Otherwise, multiple heat stack create using this heat template will
   fail with the following conflict:
  </p><div class="verbatim-wrap"><pre class="screen">| cpu_alarm_low  | 5fe0151b-5c6a-4a54-bd64-67405336a740 | HTTPConflict: resources.cpu_alarm_low: An alarm definition already exists for project / tenant: 835d6aeeb36249b88903b25ed3d2e55a named: CPU utilization less than 15 percent  | CREATE_FAILED  | 2016-07-29T10:28:47 |</pre></div><p>
   This is due to the fact that the Monasca registers the alarm definition name
   using this name property when it is defined in the Heat template. This name
   must be unique.
  </p><p>
   To avoid this problem, if you want to define an alarm name using this
   property in the template, you must be sure this name is unique within a
   project in the system. Otherwise, you can leave this optional property
   undefined in your template. In this case, the system will create an unique
   alarm name automatically during heat stack create.
  </p></div><div class="sect3" id="id-1.6.17.12.3.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unable to Retrieve QOS Policies</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#id-1.6.17.12.3.6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Launching the Orchestration Template Generator may trigger the message:
   <code class="literal">Unable to retrieve resources Qos Policies</code>. This is a
   known <a class="link" href="https://storyboard.openstack.org/#!/story/2003523" target="_blank">upstream
   bug</a>. This information message can be ignored.
  </p></div></div><div class="sect2" id="ts-magnum"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Magnum Service</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#ts-magnum">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_magnum.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_magnum.xml</li><li><span class="ds-label">ID: </span>ts-magnum</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Magnum service. Magnum
  Service provides container orchestration engines such as Docker Swarm,
  Kubernetes, and Apache Mesos available as first class resources. You can use
  this guide to help with known issues and troubleshooting of Magnum services.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-ts-magnum-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum cluster fails to create</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-ts-magnum-xml-6">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_magnum.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_magnum.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-magnum-xml-6</li></ul></div></div></div></div><p>
   Typically, small size clusters need about 3-5 minutes to stand up. If
   cluster stand up takes longer, you may proceed with troubleshooting, not
   waiting for status to turn to CREATE_FAILED after timing out.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use <code class="literal">heat resource-list -n2</code> to identify which Heat stack
     resource is stuck in <span class="bold"><strong>CREATE_IN_PROGRESS</strong></span>.
    </p><div id="id-1.6.17.12.4.3.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg" /><h6>Note</h6><p>
      The main Heat stack has nested stacks, one for kubemaster(s) and one
      for kubeminion(s). These stacks are visible as resources of type
      <span class="emphasis"><em>OS::Heat::ResourceGroup</em></span> (in parent stack) and
      <span class="emphasis"><em>file:///...</em></span> in nested stack. If any resource
      remains in <span class="emphasis"><em>CREATE_IN_PROGRESS</em></span> state within the
      nested stack, the overall state of the resource will be
      <span class="emphasis"><em>CREATE_IN_PROGRESS</em></span>.
     </p></div><div class="verbatim-wrap"><pre class="screen">$ heat resource-list -n2 22385a42-9e15-49d9-a382-f28acef36810
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| resource_name                 | physical_resource_id                 | resource_type                        | resource_status    | updated_time         | stack_name                                                       |
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| api_address_floating_switch   | 06b2cc0d-77f9-4633-8d96-f51e2db1faf3 | Magnum::FloatingIPAddressSwitcher    | CREATE_COMPLETE    | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
. . .

| fixed_subnet                  | d782bdf2-1324-49db-83a8-6a3e04f48bb9 | OS::Neutron::Subnet                  | CREATE_COMPLETE    | 2017-04-10T21:25:11Z | my-cluster-z4aquda2mgpv                                          |
| kube_masters                  | f0d000aa-d7b1-441a-a32b-17125552d3e0 | OS::Heat::ResourceGroup              | CREATE_IN_PROGRESS | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
| 0                             | b1ff8e2c-23dc-490e-ac7e-14e9f419cfb6 | file:///opt/s...ates/kubemaster.yaml | CREATE_IN_PROGRESS | 2017-04-10T21:25:41Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb                |
| kube_master                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5 | OS::Nova::Server                     | CREATE_IN_PROGRESS | 2017-04-10T21:25:48Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb-0-saafd5k7l7im |
. . .</pre></div></li><li class="listitem "><p>
     If stack creation failed on some native OpenStack resource, like
     <span class="bold"><strong>OS::Nova::Server</strong></span> or
     <span class="bold"><strong>OS::Neutron::Router</strong></span>, proceed with
     respective service troubleshooting. This type of error usually does not
     cause time out, and cluster turns into status
     <span class="bold"><strong>CREATE_FAILED</strong></span> quickly. The underlying
     reason of the failure, reported by Heat, can be checked via the
     <code class="literal">magnum cluster-show</code> command.
    </p></li><li class="listitem "><p>
     If stack creation stopped on resource of type OS::Heat::WaitCondition,
     Heat is not receiving notification from cluster VM about bootstrap
     sequence completion. Locate corresponding resource of type
     <span class="bold"><strong>OS::Nova::Server</strong></span> and use its
     <span class="bold"><strong>physical_resource_id</strong></span> to get information
     about the VM (which should be in status
     <span class="bold"><strong>CREATE_COMPLETE</strong></span>)
    </p><div class="verbatim-wrap"><pre class="screen">$ nova show 4d96510e-c202-4c62-8157-c0e3dddff6d5
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                                         |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                                        |
| OS-EXT-AZ:availability_zone          | nova                                                                                                          |
| OS-EXT-SRV-ATTR:host                 | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000025                                                                                             |
| OS-EXT-STS:power_state               | 1                                                                                                             |
| OS-EXT-STS:task_state                | -                                                                                                             |
| OS-EXT-STS:vm_state                  | active                                                                                                        |
| OS-SRV-USG:launched_at               | 2017-04-10T22:10:40.000000                                                                                    |
| OS-SRV-USG:terminated_at             | -                                                                                                             |
| accessIPv4                           |                                                                                                               |
| accessIPv6                           |                                                                                                               |
| config_drive                         |                                                                                                               |
| created                              | 2017-04-10T22:09:53Z                                                                                          |
| flavor                               | m1.small (2)                                                                                                  |
| hostId                               | eb101a0293a9c4c3a2d79cee4297ab6969e0f4ddd105f4d207df67d2                                                      |
| id                                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5                                                                          |
| image                                | fedora-atomic-26-20170723.0.x86_64 (4277115a-f254-46c0-9fb0-fffc45d2fd38)                                     |
| key_name                             | testkey                                                                                                       |
| metadata                             | {}                                                                                                            |
| name                                 | my-zaqshggwge-0-sqhpyez4dig7-kube_master-wc4vv7ta42r6                                                         |
| os-extended-volumes:volumes_attached | [{"id": "24012ce2-43dd-42b7-818f-12967cb4eb81"}]                                                              |
| private network                      | 10.0.0.14, 172.31.0.6                                                                                         |
| progress                             | 0                                                                                                             |
| security_groups                      | my-cluster-z7ttt2jvmyqf-secgroup_base-gzcpzsiqkhxx, my-cluster-z7ttt2jvmyqf-secgroup_kube_master-27mzhmkjiv5v |
| status                               | ACTIVE                                                                                                        |
| tenant_id                            | 2f5b83ab49d54aaea4b39f5082301d09                                                                              |
| updated                              | 2017-04-10T22:10:40Z                                                                                          |
| user_id                              | 7eba6d32db154d4790e1d3877f6056fb                                                                              |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     Use the floating IP of the master VM to log into first master node. Use
     the appropriate username below for your VM type. Passwords should not be
     required as the VMs should have public ssh key installed.
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th align="center">VM Type</th><th align="center">Username</th></tr></thead><tbody><tr><td>Kubernetes or Swarm on Fedora Atomic</td><td align="center">fedora</td></tr><tr><td>Kubernetes on CoreOS</td><td align="center">core</td></tr><tr><td>Mesos on Ubuntu</td><td align="center">ubuntu</td></tr></tbody></table></div></li><li class="listitem "><p>
     Useful dianostic commands
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Kubernetes cluster on Fedora Atomic
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u etcd.service
sudo journalctl -u docker.service
sudo journalctl -u kube-apiserver.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Kubernetes cluster on CoreOS
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u oem-cloudinit.service
sudo journalctl -u etcd2.service
sudo journalctl -u containerd.service
sudo journalctl -u flanneld.service
sudo journalctl -u docker.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Swarm cluster on Fedora Atomic
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u docker.service
sudo journalctl -u swarm-manager.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Mesos cluster on Ubuntu
      </p><div class="verbatim-wrap"><pre class="screen">sudo less /var/log/syslog
sudo less /var/log/cloud-init.log
sudo less /var/log/cloud-init-output.log
sudo less /var/log/os-collect-config.log
sudo less /var/log/marathon.log
sudo less /var/log/mesos-master.log</pre></div></li></ul></div></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-tools"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Tools</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#troubleshooting-tools">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-ts_tools.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_tools.xml</li><li><span class="ds-label">ID: </span>troubleshooting-tools</li></ul></div></div></div></div><p>
  Tools to assist with troubleshooting issues in your cloud. Additional
  troubleshooting information is available at <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#general-troubleshooting" title="15.1. General Troubleshooting">Section 15.1, “General Troubleshooting”</a>.
 </p><div class="sect2" id="idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the SOS Report</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-troubleshooting_sosreport.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_sosreport.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1</li></ul></div></div></div></div><p>
  The SOS report provides debug level information about your environment to
  assist in troubleshooting issues. When troubleshooting and debugging issues
  in your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment you can run an ansible playbook that will
  provide you with a full debug report, referred to as a SOS report. These
  reports can be sent to the support team when seeking assistance.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the SOS Report</span> <a title="Permalink" class="permalink" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4">#</a><a class="report-bug" rel="nofollow" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/operations-troubleshooting-troubleshooting_sosreport.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_sosreport.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the SOS report ansible playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts sosreport-run.yml</pre></div></li><li class="step "><p>
     Retrieve the SOS report tarballs, which will be in the following
     directories on your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">/tmp
/tmp/sosreport-report-archives/</pre></div></li><li class="step "><p>
     You can then use these reports to troubleshoot issues further or provide
     to the support team when you reach out to them.
    </p></li></ol></div></div><div id="id-1.6.17.13.3.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg" /><h6>Warning</h6><p>
    The SOS Report may contain sensitive information because service
    configuration file data is included in the report. Please remove any
    sensitive information before sending the SOSReport tarball externally.
   </p></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="bura-overview.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 14 </span>Backup and Restore</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>