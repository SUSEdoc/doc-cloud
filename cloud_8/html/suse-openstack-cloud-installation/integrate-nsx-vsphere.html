<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SUSE OpenStack Cloud 8 | Installing with Cloud Lifecycle Manager | Integrating NSX for vSphere</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Integrating NSX for vSphere | SUSE OpenStack Cloud 8"/>
<meta name="description" content="This section describes the installation and integration of NSX-v, a Software Defined Networking (SDN) network virtualization and security platform fo…"/>
<meta name="product-name" content="SUSE OpenStack Cloud"/>
<meta name="product-number" content="8"/>
<meta name="book-title" content="Installing with Cloud Lifecycle Manager"/>
<meta name="chapter-title" content="Chapter 16. Integrating NSX for vSphere"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8"/>
<meta property="og:title" content="Integrating NSX for vSphere | SUSE OpenStack Cloud 8"/>
<meta property="og:description" content="This section describes the installation and integration of NSX-v, a Software Defined Networking (SDN) network virtualization and security platform fo…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Integrating NSX for vSphere | SUSE OpenStack Cloud 8"/>
<meta name="twitter:description" content="This section describes the installation and integration of NSX-v, a Software Defined Networking (SDN) network virtualization and security platform fo…"/>
<link rel="prev" href="install-esx-ovsvapp.html" title="Chapter 15. Installing ESX Computes and OVSvAPP"/><link rel="next" href="install-ironic-overview.html" title="Chapter 17. Installing Baremetal (Ironic)"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/integrate-nsx-vsphere.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Installing with Cloud Lifecycle Manager</a><span> / </span><a class="crumb" href="cloudinstallation.html">Cloud Installation</a><span> / </span><a class="crumb" href="integrate-nsx-vsphere.html">Integrating NSX for vSphere</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title"><em class="citetitle">Installing with Cloud Lifecycle Manager</em></div><ol><li><a href="install-overview.html" class=" "><span class="title-number"> </span><span class="title-name">Installation Overview</span></a></li><li><a href="preinstall.html" class="has-children "><span class="title-number">I </span><span class="title-name">Pre-Installation</span></a><ol><li><a href="preinstall-overview.html" class=" "><span class="title-number">1 </span><span class="title-name">Overview</span></a></li><li><a href="preinstall-checklist.html" class=" "><span class="title-number">2 </span><span class="title-name">Pre-Installation Checklist</span></a></li><li><a href="cha-depl-dep-inst.html" class=" "><span class="title-number">3 </span><span class="title-name">Installing the Cloud Lifecycle Manager server</span></a></li><li><a href="app-deploy-smt-lcm.html" class=" "><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></li><li><a href="cha-depl-repo-conf-lcm.html" class=" "><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></a></li><li><a href="multipath-boot-from-san.html" class=" "><span class="title-number">6 </span><span class="title-name">Boot from SAN and Multipath Configuration</span></a></li></ol></li><li class="active"><a href="cloudinstallation.html" class="has-children you-are-here"><span class="title-number">II </span><span class="title-name">Cloud Installation</span></a><ol><li><a href="cloudinstallation-overview.html" class=" "><span class="title-number">7 </span><span class="title-name">Overview</span></a></li><li><a href="preparing-standalone.html" class=" "><span class="title-number">8 </span><span class="title-name">Preparing for Stand-Alone Deployment</span></a></li><li><a href="install-gui.html" class=" "><span class="title-number">9 </span><span class="title-name">Installing with the Install UI</span></a></li><li><a href="using-git.html" class=" "><span class="title-number">10 </span><span class="title-name">Using Git for Configuration Management</span></a></li><li><a href="install-standalone.html" class=" "><span class="title-number">11 </span><span class="title-name">Installing a Stand-Alone Cloud Lifecycle Manager</span></a></li><li><a href="install-kvm.html" class=" "><span class="title-number">12 </span><span class="title-name">Installing Mid-scale and Entry-scale KVM</span></a></li><li><a href="DesignateInstallOverview.html" class=" "><span class="title-number">13 </span><span class="title-name">DNS Service Installation Overview</span></a></li><li><a href="MagnumOverview.html" class=" "><span class="title-number">14 </span><span class="title-name">Magnum Overview</span></a></li><li><a href="install-esx-ovsvapp.html" class=" "><span class="title-number">15 </span><span class="title-name">Installing ESX Computes and OVSvAPP</span></a></li><li><a href="integrate-nsx-vsphere.html" class=" you-are-here"><span class="title-number">16 </span><span class="title-name">Integrating NSX for vSphere</span></a></li><li><a href="install-ironic-overview.html" class=" "><span class="title-number">17 </span><span class="title-name">Installing Baremetal (Ironic)</span></a></li><li><a href="install-swift.html" class=" "><span class="title-number">18 </span><span class="title-name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></li><li><a href="install-sles-compute.html" class=" "><span class="title-number">19 </span><span class="title-name">Installing SLES Compute</span></a></li><li><a href="install-ardana-manila.html" class=" "><span class="title-number">20 </span><span class="title-name">Installing Manila and Creating Manila Shares</span></a></li><li><a href="install-heat-templates.html" class=" "><span class="title-number">21 </span><span class="title-name">Installing SUSE CaaS Platform Heat Templates</span></a></li><li><a href="integrations.html" class=" "><span class="title-number">22 </span><span class="title-name">Integrations</span></a></li><li><a href="troubleshooting-installation.html" class=" "><span class="title-number">23 </span><span class="title-name">Troubleshooting the Installation</span></a></li><li><a href="esx-troubleshooting-installation.html" class=" "><span class="title-number">24 </span><span class="title-name">Troubleshooting the ESX</span></a></li></ol></li><li><a href="post-install.html" class="has-children "><span class="title-number">III </span><span class="title-name">Post-Installation</span></a><ol><li><a href="post-install-overview.html" class=" "><span class="title-number">25 </span><span class="title-name">Overview</span></a></li><li><a href="cloud-verification.html" class=" "><span class="title-number">26 </span><span class="title-name">Cloud Verification</span></a></li><li><a href="ui-verification.html" class=" "><span class="title-number">27 </span><span class="title-name">UI Verification</span></a></li><li><a href="install-openstack-clients.html" class=" "><span class="title-number">28 </span><span class="title-name">Installing OpenStack Clients</span></a></li><li><a href="tls30.html" class=" "><span class="title-number">29 </span><span class="title-name">Configuring Transport Layer Security (TLS)</span></a></li><li><a href="config-availability-zones.html" class=" "><span class="title-number">30 </span><span class="title-name">Configuring Availability Zones</span></a></li><li><a href="OctaviaInstall.html" class=" "><span class="title-number">31 </span><span class="title-name">Configuring Load Balancer as a Service</span></a></li><li><a href="postinstall-checklist.html" class=" "><span class="title-number">32 </span><span class="title-name">Other Common Post-Installation Tasks</span></a></li></ol></li><li><a href="cha-inst-trouble.html" class=" "><span class="title-number">33 </span><span class="title-name">Support</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="integrate-nsx-vsphere" data-id-title="Integrating NSX for vSphere"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber"><span class="phrase"><span class="phrase">8</span></span></span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">16 </span><span class="title-name">Integrating NSX for vSphere</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/integrate-nsx-vsphere.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes the installation and integration of NSX-v, a Software
  Defined Networking (SDN) network virtualization and security platform for
  VMware's vSphere.
 </p><p>
  VMware's NSX embeds networking and security functionality, normally handled
  by hardware, directly into the hypervisor. NSX can reproduce, in software, an
  entire networking environment, and provides a complete set of logical
  networking elements and services including logical switching, routing,
  firewalling, load balancing, VPN, QoS, and monitoring. Virtual networks are
  programmatically provisioned and managed independent of the underlying
  hardware.
 </p><p>
  VMware's Neutron plugin called NSX for vSphere (NSX-v) has been tested under
  the following scenarios:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Virtual <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment
   </p></li><li class="listitem"><p>
    Baremetal <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment
   </p></li></ul></div><p>
  Installation instructions are provided for both scenarios. This documentation
  is meant as an example of how to integrate VMware's NSX-v Neutron plugin
  with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The examples in this documentation are not suitable for
  all environments. To configure this for your specific environment, use the
  design guide <a class="link" href="https://communities.vmware.com/servlet/JiveServlet/downloadBody/27683-102-8-41631/NSX" target="_blank">Reference
  Design: VMware® NSX for vSphere (NSX) Network Virtualization Design
  Guide</a>.
 </p><p>
  This section includes instructions for:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Integrating with NSX for vSphere on Baremetal
   </p></li><li class="listitem"><p>
    Integrating with NSX for vSphere on virtual machines with changes necessary
    for Baremetal integration
   </p></li><li class="listitem"><p>
    Verifying NSX-v functionality
   </p></li></ul></div><section class="sect1" id="nsx-vsphere-vm" data-id-title="Integrating with NSX for vSphere"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.1 </span><span class="title-name">Integrating with NSX for vSphere</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-vsphere-vm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes the installation steps and requirements for
  integrating with NSX for vSphere on virtual machines and baremetal hardware.
 </p><section class="sect2" id="nsx-pre-integration" data-id-title="Pre-Integration Checklist"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.1.1 </span><span class="title-name">Pre-Integration Checklist</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-pre-integration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-pre_integration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following installation and integration instructions assumes an
    understanding of VMware's ESXI and vSphere products for setting up virtual
    environments.
   </p><p>
    Please review the following requirements for the VMware vSphere
    environment.
   </p><p>
    <span class="bold"><strong>Software Requirements</strong></span>
   </p><p>
    Before you install or upgrade NSX, verify your software versions. The
    following are the required versions.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Software</p></th><th style="border-bottom: 1px solid ; "><p>Version</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></p></td><td style="border-bottom: 1px solid ; "><p>8</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>VMware NSX-v Manager</p></td><td style="border-bottom: 1px solid ; "><p>6.3.4 or higher</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>VMWare NSX-v Neutron Plugin</p></td><td style="border-bottom: 1px solid ; "><p>Pike Release (TAG=11.0.0)</p></td></tr><tr><td style="border-right: 1px solid ; "><p>VMWare ESXi and vSphere Appliance (vSphere web Client)</p></td><td><p>6.0 or higher</p></td></tr></tbody></table></div><p>
    A vCenter server (appliance) is required to manage the vSphere
    environment. It is recommended that you install a vCenter appliance as an
    ESX virtual machine.
   </p><div id="id-1.4.5.11.9.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     Each ESXi compute cluster is required to have shared storage between the
     hosts in the cluster, otherwise attempts to create instances through
     nova-compute will fail.
    </p></div></section><section class="sect2" id="id-1.4.5.11.9.4" data-id-title="Installing OpenStack"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.1.2 </span><span class="title-name">Installing <span class="productname">OpenStack</span></span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.9.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="productname">OpenStack</span> can be deployed in two ways: on baremetal (physical hardware) or in
   an ESXi virtual environment on virtual machines. The following instructions
   describe how to install <span class="productname">OpenStack</span>.
 </p><div id="id-1.4.5.11.9.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   <span class="bold"><strong>Changes for installation on baremetal hardware are noted in each
   section.</strong></span>  
  </p></div><p>
   This deployment example will consist of two ESXi clusters at minimum: a
   <code class="literal">control-plane</code> cluster and a <code class="literal">compute</code>
   cluster. The control-plane cluster must have 3 ESXi hosts minimum (due to
   VMware's recommendation that each NSX controller virtual machine is on a
   separate host). The compute cluster must have 2 ESXi hosts minimum.  There
   can be multiple compute clusters. The following table outlines the virtual
   machine specifications to be built in the control-plane cluster:
  </p><div class="table" id="nsx-hw-reqs-vm" data-id-title="NSX Hardware Requirements for Virtual Machine Integration"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.1: </span><span class="title-name">NSX Hardware Requirements for Virtual Machine Integration </span></span><a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-hw-reqs-vm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/><col class="5"/><col class="6"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Virtual Machine Role</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Required Number</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Disk</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Memory</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Network</p></th><th style="border-bottom: 1px solid ; "><p>CPU</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Dedicated lifecycle manager
       </p>
       <p>
        <span class="bold"><strong>Baremetal - not needed</strong></span>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>100GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>8GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 VMXNET Virtual Network Adapters</p></td><td style="border-bottom: 1px solid ; "><p>4 vCPU</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Controller virtual machines
       </p>
       <p>
        <span class="bold"><strong>Baremetal - not needed</strong></span>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 x 300GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>32GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 VMXNET Virtual Network Adapters</p></td><td style="border-bottom: 1px solid ; "><p>8 vCPU</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Compute virtual machines</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>1 per compute cluster</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>80GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>4GB</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>3 VMXNET Virtual Network Adapters</p></td><td style="border-bottom: 1px solid ; "><p>2 vCPU</p></td></tr><tr><td style="border-right: 1px solid ; "><p>NSX Edge Gateway/DLR/Metadata-proxy appliances</p></td><td style="border-right: 1px solid ; "/><td style="border-right: 1px solid ; "><p>Autogenerated by NSXv</p></td><td style="border-right: 1px solid ; "><p>Autogenerated by NSXv</p></td><td style="border-right: 1px solid ; "><p>Autogenerated by NSXv</p></td><td><p>Autogenerated by NSXv</p></td></tr></tbody></table></div></div><p>
   <span class="bold"><strong>Baremetal: In addition to the ESXi hosts, it is
   recommended to have one physical host for the Cloud Lifecycle Manager node and three physical
   hosts for the controller nodes.</strong></span>
  </p><section class="sect3" id="nsx-ntwk-requirements" data-id-title="Network Requirements"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.1 </span><span class="title-name">Network Requirements</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-ntwk-requirements">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-ntwk-requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  NSX-v requires the following for networking:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The ESXi hosts, vCenter, and the NSX Manager appliance must resolve DNS lookup.
   </p></li><li class="listitem"><p>
    The ESXi host must have the NTP service configured and enabled.
   </p></li><li class="listitem"><p>
    Jumbo frames must be enabled on the switch ports that the ESXi hosts are connected to.
   </p></li><li class="listitem"><p>
    The ESXi hosts must have at least 2 physical network cards each.
   </p></li></ul></div></section><section class="sect3" id="nsx-ntwk-model" data-id-title="Network Model"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.2 </span><span class="title-name">Network Model</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-ntwk-model">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-ntwk-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The model in these instructions requires the following networks:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.11.9.4.8.3.1"><span class="term">ESXi Hosts and vCenter</span></dt><dd><p>
     This is the network that the ESXi hosts and vCenter use to route traffic with.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.2"><span class="term">NSX Management</span></dt><dd><p>
      The network which the NSX controllers and NSX Manager will use.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.3"><span class="term">NSX VTEP Pool</span></dt><dd><p>
     The network that NSX uses to create endpoints for VxLAN tunnels.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.4"><span class="term">Management</span></dt><dd><p>
     The network that <span class="productname">OpenStack</span> uses for deployment and maintenance of the cloud.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.5"><span class="term">Internal API (optional)</span></dt><dd><p>
     The network group that will be used for management (private API) traffic within the cloud.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.6"><span class="term">External API</span></dt><dd><p>
     This is the network that users will use to make requests to the cloud.
    </p></dd><dt id="id-1.4.5.11.9.4.8.3.7"><span class="term">External VM</span></dt><dd><p>
     VLAN-backed provider network for external access to guest VMs (floating IPs).
    </p></dd></dl></div></section><section class="sect3" id="id-1.4.5.11.9.4.9" data-id-title="vSphere port security settings"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.3 </span><span class="title-name">vSphere port security settings</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.9.4.9">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Baremetal: Even though the <span class="productname">OpenStack</span>
   deployment is on baremetal, it is still necessary to define each VLAN within
   a vSphere Distributed Switch for the Nova compute proxy virtual
   machine.</strong></span>
  </p><p>
   The vSphere port security settings for both VMs and baremetal are shown in the
   table below.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Network Group</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>VLAN Type</p></th><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Interface</p></th><th style="border-bottom: 1px solid ; "><p>vSphere Port Group Security Settings</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>ESXi Hosts and vCenter</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; "><p>Defaults</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>NSX Manager</p>
      <p>Must be able to reach ESXi Hosts and vCenter</p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; "><p>Defaults</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>NSX VTEP Pool</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; "><p>Defaults</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Management</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged or Untagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>eth0</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>:Reject
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Internal API (Optional, may be combined with the Management Network. If
        network segregation is required for security reasons, you can keep this
        as a separate network.)
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>eth2</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>External API (Public)</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>eth1</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>External VM</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Tagged</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>N/A</p></td><td style="border-bottom: 1px solid ; ">
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; "><p><span class="bold"><strong>Baremetal Only: IPMI</strong></span></p></td><td style="border-right: 1px solid ; "><p>Untagged</p></td><td style="border-right: 1px solid ; "><p>N/A</p></td><td><p>N/A</p></td></tr></tbody></table></div></section><section class="sect3" id="nsx-configure-vsphere-env" data-id-title="Configuring the vSphere Environment"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4 </span><span class="title-name">Configuring the vSphere Environment</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-configure-vsphere-env">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-vsphere_env.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before deploying <span class="productname">OpenStack</span> with NSX-v, the VMware vSphere environment must be
   properly configured, including setting up vSphere distributed switches and
   port groups. For detailed instructions, see <a class="xref" href="install-esx-ovsvapp.html" title="Chapter 15. Installing ESX Computes and OVSvAPP">Chapter 15, <em>Installing ESX Computes and OVSvAPP</em></a>.
  </p><p>
   Installing and configuring the VMware NSX Manager and creating the NSX
   network within the vSphere environment is covered below.
  </p><p>
   Before proceeding with the installation, ensure that the following are
   configured in the vSphere environment.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The vSphere datacenter is configured with at least two clusters, one
     <span class="bold"><strong>control-plane</strong></span> cluster and one <span class="bold"><strong>compute</strong></span> cluster.
    </p></li><li class="listitem"><p>
     Verify that all software, hardware, and networking requirements have been
     met.
    </p></li><li class="listitem"><p>
     Ensure the vSphere distributed virtual switches (DVS) are configured for each
     cluster.
    </p></li></ul></div><div id="id-1.4.5.11.9.4.10.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The MTU setting for each DVS should be set to 1600. NSX should
    automatically apply this setting to each DVS during the setup
    process. Alternatively, the setting can be manually applied to each DVS
    before setup if desired.
   </p></div><p>
   Make sure there is a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
   <code class="literal">ardana</code> home directory,
   <code class="filename">var/lib/ardana</code>, and that it is called
   <code class="filename">sles12sp3.iso</code>.
  </p><p>
   Install the <code class="literal">open-vm-tools</code> package.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install open-vm-tools</pre></div><section class="sect4" id="nsx-install-manager" data-id-title="Install NSX Manager"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.1 </span><span class="title-name">Install NSX Manager</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-install-manager">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-install-manager.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The NSX Manager is the centralized network management component of NSX. It
  provides a single point of configuration and REST API entry-points.
 </p><p>
   The NSX Manager is installed as a virtual appliance on one of the ESXi hosts
   within the vSphere environment. This guide will cover installing the
   appliance on one of the ESXi hosts within the control-plane cluster. For
   more detailed information, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html" target="_blank">VMware's
   NSX Installation Guide.</a>
 </p><p>
  To install the NSX Manager, download the virtual appliance from <a class="link" href="https://www.vmware.com/go/download-nsx-vsphere" target="_blank">VMware</a> and
  deploy the appliance within vCenter onto one of the ESXi hosts. For
  information on deploying appliances within vCenter, refer to VMware's
  documentation for ESXi <a class="link" href="https://pubs.vmware.com/vsphere-55/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">5.5</a>
  or <a class="link" href="https://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">6.0</a>.
 </p><p>
  During the deployment of the NSX Manager appliance, be aware of the
  following:
 </p><p>
  When prompted, select <span class="guimenu">Accept extra configuration options</span>.
  This will present options for configuring IPv4 and IPv6 addresses, the
  default gateway, DNS, NTP, and SSH properties during the installation, rather
  than configuring these settings manually after the installation.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Choose an ESXi host that resides within the control-plane cluster.
   </p></li><li class="listitem"><p>
    Ensure that the network mapped port group is the DVS port group that
    represents the VLAN the NSX Manager will use for its networking (in this
    example it is labeled as the <code class="literal">NSX Management</code> network).
   </p></li></ul></div><div id="id-1.4.5.11.9.4.10.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The IP address assigned to the NSX Manager must be able to resolve
   reverse DNS.
  </p></div><p>
  Power on the NSX Manager virtual machine after it finishes deploying and wait
  for the operating system to fully load. When ready, carry out the following
  steps to have the NSX Manager use single sign-on (SSO) and to
  register the NSX Manager with vCenter:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Open a web browser and enter the hostname or IP address that was assigned
    to the NSX Manager during setup.
   </p></li><li class="step"><p>
    Log in with the username <code class="literal">admin</code> and the
    password set during the deployment.
   </p></li><li class="step"><p>
    After logging in, click on <span class="guimenu">Manage vCenter Registration</span>.
   </p></li><li class="step"><p>
    Configure the NSX Manager to connect to the vCenter server.
   </p></li><li class="step"><p>
    Configure NSX manager for single sign on (SSO) under the <span class="guimenu">Lookup
    Server URL</span> section.
   </p></li></ol></div></div><div id="id-1.4.5.11.9.4.10.10.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   When configuring SSO, use <code class="literal">Lookup Service Port 443</code> for
   vCenter version 6.0. Use <code class="literal">Lookup Service Port 7444</code> for
   vCenter version 5.5.
  </p><p>
   SSO makes vSphere and NSX more secure by allowing the various components to
   communicate with each other through a secure token exchange mechanism,
   instead of requiring each component to authenticate a user separately. For
   more details, refer to VMware's documentation on <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html" target="_blank">Configure
   Single Sign-On</a>.
  </p></div><p>
  Both the <code class="literal">Lookup Service URL</code> and the <code class="literal">vCenter
  Server</code> sections should have a status of
  <code class="literal">connected</code> when configured properly.
 </p><p>
  Log into the vSphere Web Client (log out and and back in if already logged
  in). The NSX Manager will appear under the <span class="guimenu">Networking &amp;
  Security</span> section of the client.
 </p><div id="id-1.4.5.11.9.4.10.10.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The <span class="guimenu">Networking &amp; Security</span> section will not appear
   under the vSphere desktop client. Use of the web client is required for the
   rest of this process.
  </p></div></section><section class="sect4" id="nsx-add-controllers" data-id-title="Add NSX Controllers"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.2 </span><span class="title-name">Add NSX Controllers</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-add-controllers">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-add-controllers.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The NSX controllers serve as the central control point for all logical
  switches within the vSphere environment's network, and they maintain
  information about all hosts, logical switches (VXLANs), and distributed
  logical routers.
 </p><p>
  NSX controllers will each be deployed as a virtual appliance on the ESXi
  hosts within the control-plane cluster to form the NSX Controller
  cluster. For details about NSX controllers and the NSX control plane in
  general, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-4E0FEE83-CF2C-45E0-B0E6-177161C3D67C.html" target="_blank">VMware's
  NSX documentation</a>.
 </p><div id="id-1.4.5.11.9.4.10.11.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   Whatever the size of the NSX deployment, the following conditions must be
   met:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Each NSX Controller cluster must contain three controller nodes. Having a
     different number of controller nodes is not supported.
    </p></li><li class="listitem"><p>
     Before deploying NSX Controllers, you must deploy an NSX Manager appliance
     and register vCenter with NSX Manager.
    </p></li><li class="listitem"><p>
     Determine the IP pool settings for your controller cluster, including the
     gateway and IP address range. DNS settings are optional.
    </p></li><li class="listitem"><p>
     The NSX Controller IP network must have connectivity to the NSX Manager
     and to the management interfaces on the ESXi hosts.
    </p></li></ul></div></div><p>
   Log in to the vSphere web client and do the following steps to add the NSX
   controllers:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In vCenter, navigate to <span class="guimenu">Home</span>, select
     <span class="guimenu">Networking &amp;
     Security</span> › <span class="guimenu">Installation</span>, and then
     select the <span class="guimenu">Management</span> tab.
    </p></li><li class="step"><p>
     In the <span class="guimenu">NSX Controller nodes</span> section, click the
     <span class="guimenu">Add Node</span> icon represented by a green plus sign.
    </p></li><li class="step"><p>
     Enter the NSX Controller settings appropriate to your
     environment. If you are following this example, use the control-plane
     clustered ESXi hosts and control-plane DVS port group for the controller
     settings.
    </p></li><li class="step"><p>
     If it has not already been done, create an IP pool for the NSX Controller
     cluster with at least three IP addressess by clicking <span class="guimenu">New IP
     Pool</span>. Individual controllers can be in separate IP subnets, if
     necessary.
    </p></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to deploy the controller. After the first controller is
     completely deployed, deploy two additional controllers.
    </p></li></ol></div></div><div id="id-1.4.5.11.9.4.10.11.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Three NSX controllers is mandatory. VMware recommends configuring a DRS
    anti-affinity rule to prevent the controllers from residing on the same
    ESXi host. See more information about <a class="link" href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.resmgmt.doc%2FGUID-FF28F29C-8B67-4EFF-A2EF-63B3537E6934.html" target="_blank">DRS
    Affinity Rules</a>.
   </p></div></section><section class="sect4" id="nsx-prepare-clusters" data-id-title="Prepare Clusters for NSX Management"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.3 </span><span class="title-name">Prepare Clusters for NSX Management</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-prepare-clusters">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-prepare-clusters.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  During <span class="guimenu">Host Preparation</span>, the NSX Manager:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Installs the NSX kernel modules on ESXi hosts that are members of vSphere
    clusters
   </p></li><li class="listitem"><p>
    Builds the NSX control-plane and management-plane infrastructure
   </p></li></ul></div><p>
  The NSX kernel modules are packaged in <code class="filename">VIB</code>
  (vSphere Installation Bundle) files. They run within the hypervisor kernel and
  provide services such as distributed routing, distributed firewall, and VXLAN
  bridging capabilities. These files are installed on a per-cluster level, and
  the setup process deploys the required software on all ESXi hosts in the
  target cluster. When a new ESXi host is added to the cluster, the required
  software is automatically installed on the newly added host.
 </p><p>
  Before beginning the NSX host preparation process, make sure of the following
  in your environment:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Register vCenter with NSX Manager and deploy the NSX controllers.
   </p></li><li class="listitem"><p>
    Verify that DNS reverse lookup returns a fully qualified domain name when
    queried with the IP address of NSX Manager.
   </p></li><li class="listitem"><p>
    Verify that the ESXi hosts can resolve the DNS name of vCenter server.
   </p></li><li class="listitem"><p>
    Verify that the ESXi hosts can connect to vCenter Server on port 80.
   </p></li><li class="listitem"><p>
    Verify that the network time on vCenter Server and the ESXi hosts is
    synchronized.
   </p></li><li class="listitem"><p>
    For each vSphere cluster that will participate in NSX, verify that the ESXi
    hosts within each respective cluster are attached to a common VDS.
   </p><p>
    For example, given a deployment with two clusters named Host1 and
    Host2. Host1 is attached to VDS1 and VDS2. Host2 is attached to VDS1 and
    VDS3. When you prepare a cluster for NSX, you can only associate NSX with
    VDS1 on the cluster. If you add another host (Host3) to the cluster and
    Host3 is not attached to VDS1, it is an invalid configuration, and Host3
    will not be ready for NSX functionality.
   </p></li><li class="listitem"><p>
    If you have vSphere Update Manager (VUM) in your environment, you must
    disable it before preparing clusters for network virtualization. For
    information on how to check if VUM is enabled and how to disable it if
    necessary, see the <a class="link" href="http://kb.vmware.com/kb/2053782" target="_blank">VMware knowledge base</a>.
   </p></li><li class="listitem"><p>
    In the vSphere web client, ensure that the cluster is in the resolved state
    (listed under the <span class="guimenu">Host Preparation</span> tab). If the Resolve option does not
    appear in the cluster's Actions list, then it is in a resolved state.
   </p></li></ul></div><p>
  To prepare the vSphere clusters for NSX:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    In vCenter, select <span class="guimenu">Home</span> › <span class="guimenu">Networking
    &amp; Security</span> › <span class="guimenu">Installation</span>, and
    then select the <span class="guimenu">Host Preparation</span> tab.
   </p></li><li class="step"><p>
    Continuing with the example in these instructions, click on the
    <span class="guimenu">Actions</span> button (gear icon) and select
    <span class="guimenu">Install</span> for both the control-plane cluster and compute
    cluster (if you are using something other than this example, then only
    install on the clusters that require NSX logical switching, routing, and
    firewalls).
   </p></li><li class="step"><p>
    Monitor the installation until the <code class="literal">Installation Status</code>
    column displays a green check mark.
   </p><div id="id-1.4.5.11.9.4.10.12.8.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     While installation is in
     progress, do not deploy, upgrade, or uninstall any service or component.
    </p></div><div id="id-1.4.5.11.9.4.10.12.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     If the <code class="literal">Installation Status</code> column displays a red
     warning icon and says <code class="literal">Not Ready</code>, click
     <span class="guimenu">Resolve</span>. Clicking <span class="guimenu">Resolve</span> might
     result in a reboot of the host. If the installation is still not
     successful, click the warning icon. All errors will be displayed. Take the
     required action and click <span class="guimenu">Resolve</span> again.
    </p></div></li><li class="step"><p>
    To verify the VIBs (<code class="filename">esx-vsip</code> and
    <code class="filename">esx-vxlan</code>) are installed and registered, SSH into an
    ESXi host within the prepared cluster. List the names and versions of the
    VIBs installed by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>esxcli software vib list | grep esx</pre></div><div class="verbatim-wrap"><pre class="screen">...
esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
...</pre></div></li></ol></div></div><div id="id-1.4.5.11.9.4.10.12.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   After host preparation:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A host reboot is not required
    </p></li><li class="listitem"><p>
     If you add a host to a prepared cluster, the NSX VIBs are automatically
     installed on the host.
    </p></li><li class="listitem"><p>
     If you move a host to an unprepared cluster, the NSX VIBs are
     automatically uninstalled from the host. In this case, a host reboot
     is required to complete the uninstall process.
    </p></li></ul></div></div></section><section class="sect4" id="nsx-configure-vxlan-transport" data-id-title="Configure VXLAN Transport Parameters"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.4 </span><span class="title-name">Configure VXLAN Transport Parameters</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-configure-vxlan-transport">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-vxlan-transport.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  VXLAN is configured on a per-cluster basis, where each vSphere cluster that
  is to participate in NSX is mapped to a vSphere Distributed Virtual Switch
  (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in that
  cluster is enabled for logical switches. The settings chosen in this section
  will be used in creating the VMkernel interface.
 </p><p>
  Configuring transport parameters involves selecting a DVS, a VLAN ID, an MTU
  size, an IP addressing mechanism, and a NIC teaming policy. The MTU for each
  switch must be set to 1550 or higher. By default, it is set to 1600 by
  NSX. This is also the recommended setting for integration with <span class="productname">OpenStack</span>.
 </p><p>
  To configure the VXLAN transport parameters:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    In the vSphere web client, navigate to
    <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
    Security</span> › <span class="guimenu">Installation</span>.
   </p></li><li class="step"><p>
    Select the <span class="guimenu">Host Preparation</span> tab.
   </p></li><li class="step"><p>
    Click the <span class="guimenu">Configure</span> link in the VXLAN column.
   </p></li><li class="step"><p>
    Enter the required information.
   </p></li><li class="step"><p>
    If you have not already done so, create an IP pool for the VXLAN tunnel end
    points (VTEP) by clicking <span class="guimenu">New IP Pool</span>:
   </p></li><li class="step"><p>
    Click <span class="guimenu">OK</span> to create the VXLAN network.
   </p></li></ol></div></div><p>
  When configuring the VXLAN transport network, consider the following:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Use a NIC teaming policy that best suits the environment being
    built. <code class="literal">Load Balance - SRCID</code> as the VMKNic teaming policy
    is usually the most flexible out of all the available options. This allows
    each host to have a VTEP vmkernel interface for each dvuplink on the
    selected distributed switch (two dvuplinks gives two VTEP interfaces per
    ESXi host).
   </p></li><li class="listitem"><p>
    Do not mix different teaming policies for different portgroups on a VDS
    where some use Etherchannel or Link Aggregation Control Protocol (LACPv1 or
    LACPv2) and others use a different teaming policy. If uplinks are shared in
    these different teaming policies, traffic will be interrupted. If logical
    routers are present, there will be routing problems. Such a configuration
    is not supported and should be avoided.
   </p></li><li class="listitem"><p>
    For larger environments it may be better to use DHCP for the VMKNic IP
    Addressing.
   </p></li><li class="listitem"><p>
    For more information and further guidance, see the <a class="link" href="https://communities.vmware.com/docs/DOC-27683" target="_blank">VMware NSX for
    vSphere Network Virtualization Design Guide</a>.
   </p></li></ul></div></section><section class="sect4" id="nsx-assign-segment-id-pool" data-id-title="Assign Segment ID Pool"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.5 </span><span class="title-name">Assign Segment ID Pool</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-assign-segment-id-pool">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-assign-segment_id-pool.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Each VXLAN tunnel will need a segment ID to isolate its network
  traffic. Therefore, it is necessary to configure a segment ID pool for the
  NSX VXLAN network to use. If an NSX controller is not deployed within the
  vSphere environment, a multicast address range must be added to spread
  traffic across the network and avoid overloading a single multicast address.
 </p><p>
  For the purposes of the example in these instructions, do the following steps
  to assign a segment ID pool. Otherwise, follow best practices as outlined in
  <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
  documentation</a>.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    In the vSphere web client, navigate to
    <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
    Security</span> › <span class="guimenu">Installation</span>.
   </p></li><li class="step"><p>
    Select the <span class="guimenu">Logical Network Preparation</span> tab.
   </p></li><li class="step"><p>
    Click <span class="guimenu">Segment ID</span>, and then <span class="guimenu">Edit</span>.
   </p></li><li class="step"><p>
    Click <span class="guimenu">OK</span> to save your changes.
   </p></li></ol></div></div></section><section class="sect4" id="nsx-create-transport-zone" data-id-title="Create a Transport Zone"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.6 </span><span class="title-name">Create a Transport Zone</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-create-transport-zone">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-create-transport-zone.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  A transport zone controls which hosts a logical switch can reach and has the
  following characteristics.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    It can span one or more vSphere clusters.
   </p></li><li class="listitem"><p>
    Transport zones dictate which clusters can participate in the use of a
    particular network. Therefore they dictate which VMs can participate in the
    use of a particular network.
   </p></li><li class="listitem"><p>
    A vSphere NSX environment can contain one or more transport zones based on the
    environment's requirements.
   </p></li><li class="listitem"><p>
    A host cluster can belong to multiple transport
    zones.
   </p></li><li class="listitem"><p>
    A logical switch can belong to only one transport zone.
   </p></li></ul></div><div id="id-1.4.5.11.9.4.10.15.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   <span class="productname">OpenStack</span> has only been verified to work with a single transport zone within
   a vSphere NSX-v environment. Other configurations are currently not
   supported.
  </p></div><p>
   For more information on transport zones, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html" target="_blank">VMware's
   Add A Transport Zone</a>.
  </p><p>
   To create a transport zone:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In the vSphere web client, navigate to
     <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
     Security</span> › <span class="guimenu">Installation</span>.
    </p></li><li class="step"><p>
     Select the <span class="guimenu">Logical Network Preparation</span> tab.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Transport Zones</span>, and then click the <span class="guimenu">New
     Transport Zone</span> (New Logical Switch) icon.
    </p></li><li class="step"><p>
     In the <span class="guimenu">New Transport Zone</span> dialog box, type a name and
     an optional description for the transport zone.
    </p></li><li class="step"><p>
     For these example instructions, select the control plane mode as
     <code class="literal">Unicast</code>.
    </p><div id="id-1.4.5.11.9.4.10.15.7.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Whether there is a controller in the environment or if the environment is
      going to use multicast addresses will determine the control plane mode to
      select:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">Unicast</code> (what this set of instructions uses): The
        control plane is handled by an NSX controller. All unicast traffic
        leverages optimized headend replication. No multicast IP addresses or
        special network configuration is required.
       </p></li><li class="listitem"><p>
        <code class="literal">Multicast</code>: Multicast IP addresses in the physical
        network are used for the control plane. This mode is recommended only
        when upgrading from older VXLAN deployments. Requires PIM/IGMP in the
        physical network.
       </p></li><li class="listitem"><p>
        <code class="literal">Hybrid</code>: Offloads local traffic replication to the
        physical network (L2 multicast). This requires IGMP snooping on the
        first-hop switch and access to an IGMP querier in each VTEP subnet, but
        does not require PIM. The first-hop switch handles traffic replication
        for the subnet.
       </p></li></ul></div></div></li><li class="step"><p>
     Select the clusters to be added to the transport zone.
    </p></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to save your changes.
    </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.9.4.10.16" data-id-title="Deploying SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.4.7 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.9.4.10.16">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-vsphere_env.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    With vSphere environment setup completed, the <span class="productname">OpenStack</span> can be deployed. The
    following sections will cover creating virtual machines within the vSphere
    environment, configuring the cloud model and integrating NSX-v Neutron
    core plugin into the <span class="productname">OpenStack</span>:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the virtual machines
     </p></li><li class="step"><p>
      Deploy the Cloud Lifecycle Manager
     </p></li><li class="step"><p>
      Configure the Neutron environment with NSX-v
     </p></li><li class="step"><p>
      Modify the cloud input model
     </p></li><li class="step"><p>
      Set up the parameters
     </p></li><li class="step"><p>
      Deploy the Operating System with Cobbler
     </p></li><li class="step"><p>
      Deploy the cloud
     </p></li></ol></div></div></section></section><section class="sect3" id="id-1.4.5.11.9.4.11" data-id-title="Deploying SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.9.4.11">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Within the vSphere environment, create the <span class="productname">OpenStack</span> virtual machines. At
    minimum, there must be the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      One Cloud Lifecycle Manager deployer
     </p></li><li class="listitem"><p>
      Three <span class="productname">OpenStack</span> controllers
     </p></li><li class="listitem"><p>
      One <span class="productname">OpenStack</span> Neutron compute proxy
     </p></li></ul></div><p>
    For the minimum NSX hardware requirements, refer to <a class="xref" href="integrate-nsx-vsphere.html#nsx-hw-reqs-vm" title="NSX Hardware Requirements for Virtual Machine Integration">Table 16.1, “NSX Hardware Requirements for Virtual Machine Integration”</a>.
   </p><p>
    If ESX VMs are to be used as Nova compute proxy nodes, set up three LAN
    interfaces in each virtual machine as shown in the networking model table below.  There must
    be at least one Nova compute proxy node per cluster.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Network Group</p></th><th style="border-bottom: 1px solid ; "><p>Interface</p></th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>Management</p></td><td style="border-bottom: 1px solid ; "><p><code class="literal">eth0</code></p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><p>External API</p></td><td style="border-bottom: 1px solid ; "><p><code class="literal">eth1</code></p></td></tr><tr><td style="border-right: 1px solid ; "><p>Internal API</p></td><td><p><code class="literal">eth2</code></p></td></tr></tbody></table></div><section class="sect4" id="nsx-advanced-config" data-id-title="Advanced Configuration Option"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.1 </span><span class="title-name">Advanced Configuration Option</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-advanced-config">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-advanced-config.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.11.9.4.11.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   Within vSphere for each in the virtual machine:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     In the <span class="guimenu">Options</span> section, under <span class="guimenu">Advanced
     configuration parameters</span>, ensure that
     <code class="literal">disk.EnableUUIDoption</code> is set to
     <code class="literal">true</code>.
    </p></li><li class="listitem"><p>
     If the option does not exist, it must be added. This
     option is required for the <span class="productname">OpenStack</span> deployment.
    </p></li><li class="listitem"><p>
     If the option is not specified, then the deployment will fail when
     attempting to configure the disks of each virtual machine.
    </p></li></ul></div></div></section><section class="sect4" id="sec-nsx-setup-deployer" data-id-title="Setting Up the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.2 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#sec-nsx-setup-deployer">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect5" id="id-1.4.5.11.9.4.11.8.2" data-id-title="Installing the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.2.1 </span><span class="title-name">Installing the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.9.4.11.8.2">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-vm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the product from:
     </p><ol type="a" class="substeps"><li class="step"><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step"><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step"><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step"><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step"><p>
      Select the location.
     </p></li><li class="step"><p>
      Select the keyboard layout.
     </p></li><li class="step"><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step"><p>
      Create new account:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Enter a username.
       </p></li><li class="step"><p>
        Enter a password.
       </p></li><li class="step"><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step"><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.4.5.11.9.4.11.8.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></section></section><section class="sect4" id="nsx-configure-neutron-env-nsx" data-id-title="Configure the Neutron Environment with NSX-v"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3 </span><span class="title-name">Configure the Neutron Environment with NSX-v</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-configure-neutron-env-nsx">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-configure-neutron-env-nsx.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In summary, integrating NSX with vSphere has four major steps:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Modify the input model to define the server roles, servers, network roles
    and networks. <a class="xref" href="integrate-nsx-vsphere.html#nsx-modify-input-model" title="16.1.2.5.3.2. Modify the Input Model">Section 16.1.2.5.3.2, “Modify the Input Model”</a>
   </p></li><li class="step"><p>
    Set up the parameters needed for Neutron and Nova to communicate with the
    ESX and NSX Manager. <a class="xref" href="integrate-nsx-vsphere.html#nsx-deploy-os-cobbler" title="16.1.2.5.3.3. Deploying the Operating System with Cobbler">Section 16.1.2.5.3.3, “Deploying the Operating System with Cobbler”</a>
   </p></li><li class="step"><p>
    Do the steps to deploy the cloud. <a class="xref" href="integrate-nsx-vsphere.html#nsx-deploy-cloud" title="16.1.2.5.3.4. Deploying the Cloud">Section 16.1.2.5.3.4, “Deploying the Cloud”</a>
   </p></li></ol></div></div><section class="sect5" id="nsx-import-third-party" data-id-title="Third-Party Import of VMware NSX-v Into Neutron and Neutronclient"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.1 </span><span class="title-name">Third-Party Import of VMware NSX-v Into Neutron and
 Neutronclient</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-import-third-party">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-import-third_party.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To import the NSX-v Neutron core-plugin into Cloud Lifecycle Manager, run the third-party
  import playbook.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost third-party-import.yml</pre></div></section><section class="sect5" id="nsx-modify-input-model" data-id-title="Modify the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2 </span><span class="title-name">Modify the Input Model</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-modify-input-model">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-modify-input-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  After the third-party import has completed successfully, modify the input
  model:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Prepare for input model changes
   </p></li><li class="step"><p>
    Define the servers and server roles needed for a NSX-v cloud.
   </p></li><li class="step"><p>
    Define the necessary networks and network groups
   </p></li><li class="step"><p>
    Specify the services needed to be deployed on the Cloud Lifecycle Manager controllers and the
    Nova ESX compute proxy nodes.
   </p></li><li class="step"><p>
    Commit the changes and run the configuration processor.
   </p></li></ol></div></div><section class="sect6" id="nsx-prepare-input-model-changes" data-id-title="Prepare for Input Model Changes"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.1 </span><span class="title-name">Prepare for Input Model Changes</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-prepare-input-model-changes">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-prepare-input-model-changes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The previous steps created a modified <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tarball with the NSX-v
  core plugin in the Neutron and <code class="literal">neutronclient</code> venvs. The
  <code class="filename">tar</code> file can now be extracted and the
  <code class="filename">ardana-init.bash</code> script can be run to set up the
  deployment files and directories. If a modified <code class="filename">tar</code> file
  was not created, then extract the tar from the /media/cdrom/ardana location.
 </p><p>
  To run the <code class="filename">ardana-init.bash</code> script which is included in
  the build, use this commands:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/ardana/hos-init.bash</pre></div></section><section class="sect6" id="nsx-create-input-model" data-id-title="Create the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.2 </span><span class="title-name">Create the Input Model</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-create-input-model">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-create-input-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copy the example input model to
  <code class="filename">~/openstack/my_cloud/definition/</code> directory:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx
<code class="prompt user">ardana &gt; </code>cp -R entry-scale-nsx ~/openstack/my_cloud/definition</pre></div><p>
  Refer to the reference input model in
  <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
  for details about how these definitions should be made.  The main differences
  between this model and the standard Cloud Lifecycle Manager input models are:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Only the neutron-server is deployed.  No other neutron agents are deployed.
   </p></li><li class="listitem"><p>
    Additional parameters need to be set in
    <code class="filename">pass_through.yml</code> and
    <code class="filename">nsx/nsx_config.yml</code>.
   </p></li><li class="listitem"><p>
    Nova ESX compute proxy nodes may be ESX virtual machines.
   </p></li></ul></div><section class="sect6" id="id-1.4.5.11.9.4.11.9.5.5.6" data-id-title="Set up the Parameters"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.2.1 </span><span class="title-name">Set up the Parameters</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.9.4.11.9.5.5.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-create-input-model.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The special parameters needed for the NSX-v integrations are set in the
   files <code class="filename">pass_through.yml</code> and
   <code class="filename">nsx/nsx_config.yml</code>. They are in the
   <code class="filename">~/openstack/my_cloud/definition/data</code> directory.
  </p><p>
   Parameters in <code class="filename">pass_through.yml</code> are in the sample input
   model in the
   <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
   directory.  The comments in the sample input model file describe how to
   locate the values of the required parameters.
  </p><div class="verbatim-wrap"><pre class="screen">#
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <em class="replaceable">VCENTER_ADMIN_USERNAME</em>
        ip: <em class="replaceable">VCENTER_IP</em>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for the vCenter password. The string
        # generated is the encrypted password. Enter the string
        # enclosed by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_PASSWD_FROM_ARDANAENCRYPT</em>"

        # The id is is obtained by the URL
        # https://<em class="replaceable">VCENTER_IP</em>/mob/?moid=ServiceInstance&amp;doPath=content%2eabout,
        # field instanceUUID.
        id: <em class="replaceable">VCENTER_UUID</em>
  servers:
    -
      # Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml. There should be one esx-compute-proxy node per ESX
      # resource pool.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER1_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em>
    -
      id: esx-compute2
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER2_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em></pre></div><p>
   There are parameters in <code class="filename">nsx/nsx_config.yml</code>.  The
   comments describes how to retrieve the values.
  </p><div class="verbatim-wrap"><pre class="screen"># (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name: NSX-CONFIG-CP1
      services:
        - nsx
      data:
        # (Required) URL for NSXv manager (e.g - https://management_ip).
        manager_uri: 'https://<em class="replaceable">NSX_MGR_IP</em>

        # (Required) NSXv username.
        user: 'admin'

        # (Required) Encrypted NSX Manager password.
        # Password encryption is done by the script
        # ~/openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # NOTE: Make sure that the NSX Manager password is encrypted with the same key
        # used to encrypt the VCenter password.
        #
        # The script will prompt for the NSX Manager password. The string
        # generated is the encrypted password. Enter the string enclosed
        # by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_NSX_MGR_PASSWD_FROM_ARDANAENCRYPT</em>"
        # (Required) datacenter id for edge deployment.
        # Retrieved using
        #    http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob/?moid=ServiceInstance&amp;doPath=content
        # click on the value from the rootFolder property. The datacenter_moid is
        # the value of the childEntity property.
        # The vCenter-ip-address comes from the file pass_through.yml in the
        # input model under "pass-through.global.vmware.ip".
        datacenter_moid: 'datacenter-21'
        # (Required) id of logic switch for physical network connectivity.
        # How to retrieve
        # 1. Get to the same page where the datacenter_moid is found.
        # 2. Click on the value of the rootFolder property.
        # 3. Click on the value of the childEntity property
        # 4. Look at the network property. The external network is
        #    network associated with EXTERNAL VM in VCenter.
        external_network: 'dvportgroup-74'
        # (Required) clusters ids containing OpenStack hosts.
        # Retrieved using http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob, click on the value
        # from the rootFolder property. Then click on the value of the
        # hostFolder property. Cluster_moids are the values under childEntity
        # property of the compute clusters.
        cluster_moid: 'domain-c33,domain-c35'
        # (Required) resource-pool id for edge deployment.
        resource_pool_id: 'resgroup-67'
        # (Optional) datastore id for edge deployment. If not needed,
        # do not declare it.
        # datastore_id: 'datastore-117'

        # (Required) network scope id of the transport zone.
        # To get the vdn_scope_id, in the vSphere web client from the Home
        # menu:
        #   1. click on Networking &amp; Security
        #   2. click on installation
        #   3. click on the Logical Netowrk Preparation tab.
        #   4. click on the Transport Zones button.
        #   5. Double click on the transport zone being configure.
        #   6. Select Manage tab.
        #   7. The vdn_scope_id will appear at the end of the URL.
        vdn_scope_id: 'vdnscope-1'

        # (Optional) Dvs id for VLAN based networks. If not needed,
        # do not declare it.
        # dvs_id: 'dvs-68'

        # (Required) backup_edge_pool: backup edge pools management range,
        # - edge_type&gt;[edge_size]:<em class="replaceable">MINIMUM_POOLED_EDGES</em>:<em class="replaceable">MAXIMUM_POOLED_EDGES</em>
        # - edge_type: service (service edge) or  vdr (distributed edge)
        # - edge_size:  compact ,  large (by default),  xlarge  or  quadlarge
        backup_edge_pool: 'service:compact:4:10,vdr:compact:4:10'

        # (Optional) mgt_net_proxy_ips: management network IP address for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_ips: '10.142.14.251,10.142.14.252'

        # (Optional) mgt_net_proxy_netmask: management network netmask for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_netmask: '255.255.255.0'

        # (Optional) mgt_net_moid: Network ID for management network connectivity
        # Do not declare if not used.
        # mgt_net_moid: 'dvportgroup-73'

        # ca_file: Name of the certificate file. If insecure is set to True,
        # then this parameter is ignored. If insecure is set to False and this
        # parameter is not defined, then the system root CAs will be used
        # to verify the server certificate.
        ca_file: a/nsx/certificate/file

        # insecure:
        # If true (default), the NSXv server certificate is not verified.
        # If false, then the default CA truststore is used for verification.
        # This option is ignored if "ca_file" is set
        insecure: True
        # (Optional) edge_ha: if true, will duplicate any edge pool resources
        # Default to False if undeclared.
        # edge_ha: False
        # (Optional) spoofguard_enabled:
        # If True (default), indicates NSXV spoofguard component is used to
        # implement port-security feature.
        # spoofguard_enabled: True
        # (Optional) exclusive_router_appliance_size:
        # Edge appliance size to be used for creating exclusive router.
        # Valid values: 'compact', 'large', 'xlarge', 'quadlarge'
        # Defaults to 'compact' if not declared.  # exclusive_router_appliance_size:
        'compact'</pre></div></section></section><section class="sect6" id="commit-config-processor" data-id-title="Commit Changes and Run the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.2.3 </span><span class="title-name">Commit Changes and Run the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#commit-config-processor">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/commit-config_processor.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Commit your changes with the input model and the required configuration
  values added to the <code class="filename">pass_through.yml</code> and
  <code class="filename">nsx/nsx_config.yml</code> files.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git commit -A -m "Configuration changes for NSX deployment"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
 -e \encrypt="" -e rekey=""</pre></div><p>
  If the playbook <code class="filename">config-processor-run.yml</code> fails, there is
  an error in the input model. Fix the error and repeat the above steps.
 </p></section></section><section class="sect5" id="nsx-deploy-os-cobbler" data-id-title="Deploying the Operating System with Cobbler"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.3 </span><span class="title-name">Deploying the Operating System with Cobbler</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-deploy-os-cobbler">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-deploy-os-cobbler.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    From the Cloud Lifecycle Manager, run Cobbler to install the operating system on the nodes
    after it has to be deployed:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
    Verify the nodes that will have an operating system installed by Cobbler by
    running this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system find --netboot-enabled=1</pre></div></li><li class="step"><p>
    Reimage the nodes using Cobbler.  Do not use Cobbler to reimage the nodes
    running as ESX virtual machines. The command below is run on a setup where
    the Nova ESX compute proxies are VMs. Controllers 1, 2, and 3 are
    running on physical servers.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e \
   nodelist=controller1,controller2,controller3</pre></div></li><li class="step"><p>
    When the playbook has completed, each controller node should have an
    operating system installed with an IP address configured on
    <code class="literal">eth0</code>.
   </p></li><li class="step"><p>
    After your controller nodes have been completed, you should install the
    operating system on your Nova compute proxy virtual machines. Each
    configured virtual machine should be able to PXE boot into the operating
    system installer.
   </p></li><li class="step"><p>
    From within the vSphere environment, power on each Nova compute proxy
    virtual machine and watch for it to PXE boot into the OS installer via its
    console.
   </p><ol type="a" class="substeps"><li class="step"><p>
    If successful, the virtual machine will have the operating system
    automatically installed and will then automatically power off.
     </p></li><li class="step"><p>
      When the virtual machine has powered off, power it on and let it boot
      into the operating system.
     </p></li></ol></li><li class="step"><p>
    Verify network settings after deploying the operating system to each node.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Verify that the NIC bus mapping specified in the cloud model input file
      (<code class="filename">~/ardana/my_cloud/definition/data/nic_mappings.yml</code>)
      matches the NIC bus mapping on each <span class="productname">OpenStack</span> node.
     </p><p>
      Check the NIC bus mapping with this command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem"><p>
      After the playbook has completed, each controller node should have an
      operating system installed with an IP address configured on eth0.
     </p></li></ul></div></li><li class="step"><p>
    When the ESX compute proxy nodes are VMs, install the operating system if
    you have not already done so.
   </p></li></ol></div></div></section><section class="sect5" id="nsx-deploy-cloud" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.1.2.5.3.4 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-deploy-cloud">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-deploy-cloud.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  When the configuration processor has completed successfully, the cloud can be
  deployed. Set the ARDANA_USER_PASSWORD_ENCRYPT_KEY environment
  variable before running <code class="filename">site.yml</code>.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">PASSWORD_KEY</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><p>
<em class="replaceable">PASSWORD_KEY</em> in the <code class="literal">export</code>
command is the key used to encrypt the passwords for vCenter and NSX Manager.
</p></section></section></section></section></section><section class="sect1" id="nsx-vsphere-baremetal" data-id-title="Integrating with NSX for vSphere on Baremetal"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.2 </span><span class="title-name">Integrating with NSX for vSphere on Baremetal</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-vsphere-baremetal">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section describes the installation steps and requirements for
  integrating with NSX for vSphere on baremetal physical hardware.
 </p><section class="sect2" id="id-1.4.5.11.10.3" data-id-title="Pre-Integration Checklist"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.2.1 </span><span class="title-name">Pre-Integration Checklist</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.3">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following installation and integration instructions assumes an
   understanding of VMware's ESXI and vSphere products for setting up virtual
   environments.
  </p><p>
   Please review the following requirements for the VMware vSphere environment.
  </p><p>
   <span class="bold"><strong>Software Requirements</strong></span>
  </p><p>
   Before you install or upgrade NSX, verify your software versions. The
   following are the required versions.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Software
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Version
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        8
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        VMware NSX-v Manager
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        6.3.4 or higher
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        VMWare NSX-v Neutron Plugin
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Pike Release (TAG=11.0.0)
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        VMWare ESXi and vSphere Appliance (vSphere web Client)
       </p>
      </td><td>
       <p>
        6.0 or higher
       </p>
      </td></tr></tbody></table></div><p>
   A vCenter server (appliance) is required to manage the vSphere environment.
   It is recommended that you install a vCenter appliance as an ESX virtual
   machine.
  </p><div id="id-1.4.5.11.10.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Each ESXi compute cluster is required to have shared storage between the
    hosts in the cluster, otherwise attempts to create instances through
    nova-compute will fail.
   </p></div></section><section class="sect2" id="id-1.4.5.11.10.4" data-id-title="Installing on Baremetal"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">16.2.2 </span><span class="title-name">Installing on Baremetal</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="productname">OpenStack</span> can be deployed in two ways: on baremetal (physical hardware) or in
   an ESXi virtual environment on virtual machines. The following instructions
   describe how to install <span class="productname">OpenStack</span> on baremetal nodes with vCenter and NSX
   Manager running as virtual machines. For instructions on virtual machine
   installation, see <a class="xref" href="integrate-nsx-vsphere.html#nsx-vsphere-vm" title="16.1. Integrating with NSX for vSphere">Section 16.1, “Integrating with NSX for vSphere”</a>.
  </p><p>
   This deployment example will consist of two ESXi clusters at minimum: a
   <code class="literal">control-plane</code> cluster and a <code class="literal">compute</code>
   cluster. The control-plane cluster must have 3 ESXi hosts minimum (due to
   VMware's recommendation that each NSX controller virtual machine is on a
   separate host). The compute cluster must have 2 ESXi hosts minimum. There
   can be multiple compute clusters. The following table outlines the virtual
   machine specifications to be built in the control-plane cluster:
  </p><div class="table" id="nsx-hw-reqs-bm" data-id-title="NSX Hardware Requirements for Baremetal Integration"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.2: </span><span class="title-name">NSX Hardware Requirements for Baremetal Integration </span></span><a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-hw-reqs-bm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/><col class="5"/><col class="6"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Virtual Machine Role
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Required Number
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Disk
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Memory
       </p>
      </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Network
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        CPU
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Compute virtual machines
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        1 per compute cluster
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        80GB
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        4GB
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        3 VMXNET Virtual Network Adapters
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        2 vCPU
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        NSX Edge Gateway/DLR/Metadata-proxy appliances
       </p>
      </td><td style="border-right: 1px solid ; ">
       
      </td><td style="border-right: 1px solid ; ">
       <p>
        Autogenerated by NSXv
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        Autogenerated by NSXv
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        Autogenerated by NSXv
       </p>
      </td><td>
       <p>
        Autogenerated by NSXv
       </p>
      </td></tr></tbody></table></div></div><p>
   In addition to the ESXi hosts, it is recommended that there is one physical
   host for the Cloud Lifecycle Manager node and three physical hosts for the controller nodes.
  </p><section class="sect3" id="id-1.4.5.11.10.4.6" data-id-title="Network Requirements"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.1 </span><span class="title-name">Network Requirements</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.6">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    NSX-v requires the following for networking:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The ESXi hosts, vCenter, and the NSX Manager appliance must resolve DNS
      lookup.
     </p></li><li class="listitem"><p>
      The ESXi host must have the NTP service configured and enabled.
     </p></li><li class="listitem"><p>
      Jumbo frames must be enabled on the switch ports that the ESXi hosts are
      connected to.
     </p></li><li class="listitem"><p>
      The ESXi hosts must have at least 2 physical network cards each.
     </p></li></ul></div></section><section class="sect3" id="id-1.4.5.11.10.4.7" data-id-title="Network Model"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.2 </span><span class="title-name">Network Model</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.7">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The model in these instructions requires the following networks:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.11.10.4.7.3.1"><span class="term">ESXi Hosts and vCenter</span></dt><dd><p>
       This is the network that the ESXi hosts and vCenter use to route traffic
       with.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.2"><span class="term">NSX Management</span></dt><dd><p>
       The network which the NSX controllers and NSX Manager will use.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.3"><span class="term">NSX VTEP Pool</span></dt><dd><p>
       The network that NSX uses to create endpoints for VxLAN tunnels.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.4"><span class="term">Management</span></dt><dd><p>
       The network that <span class="productname">OpenStack</span> uses for deployment and maintenance of the
       cloud.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.5"><span class="term">Internal API (optional)</span></dt><dd><p>
       The network group that will be used for management (private API) traffic
       within the cloud.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.6"><span class="term">External API</span></dt><dd><p>
       This is the network that users will use to make requests to the cloud.
      </p></dd><dt id="id-1.4.5.11.10.4.7.3.7"><span class="term">External VM</span></dt><dd><p>
       VLAN-backed provider network for external access to guest VMs (floating
       IPs).
      </p></dd></dl></div></section><section class="sect3" id="id-1.4.5.11.10.4.8" data-id-title="vSphere port security settings"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.3 </span><span class="title-name">vSphere port security settings</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.8">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Even though the <span class="productname">OpenStack</span> deployment is on baremetal, it is still necessary
    to define each VLAN within a vSphere Distributed Switch for the Nova
    compute proxy virtual machine. Therefore, the vSphere port security
    settings are shown in the table below.
   </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Network Group
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         VLAN Type
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Interface
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         vSphere Port Group Security Settings
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         IPMI
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Untagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         ESXi Hosts and vCenter
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Defaults
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         NSX Manager
        </p>
        <p>
         Must be able to reach ESXi Hosts and vCenter
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Defaults
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         NSX VTEP Pool
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Defaults
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Management
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged or Untagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         bond0
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>:Reject
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Internal API (Optional, may be combined with the Management Network.
         If network segregation is required for security reasons, you can keep
         this as a separate network.)
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         bond0
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         External API (Public)
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         External VM
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         Tagged
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         N/A
        </p>
       </td><td>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem"><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr></tbody></table></div></section><section class="sect3" id="id-1.4.5.11.10.4.9" data-id-title="Configuring the vSphere Environment"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4 </span><span class="title-name">Configuring the vSphere Environment</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before deploying <span class="productname">OpenStack</span> with NSX-v, the VMware vSphere environment must
    be properly configured, including setting up vSphere distributed switches
    and port groups. For detailed instructions, see
    <a class="xref" href="install-esx-ovsvapp.html" title="Chapter 15. Installing ESX Computes and OVSvAPP">Chapter 15, <em>Installing ESX Computes and OVSvAPP</em></a>.
   </p><p>
    Installing and configuring the VMware NSX Manager and creating the NSX
    network within the vSphere environment is covered below.
   </p><p>
    Before proceeding with the installation, ensure that the following are
    configured in the vSphere environment.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The vSphere datacenter is configured with at least two clusters, one
      <span class="bold"><strong>control-plane</strong></span> cluster and one
      <span class="bold"><strong>compute</strong></span> cluster.
     </p></li><li class="listitem"><p>
      Verify that all software, hardware, and networking requirements have been
      met.
     </p></li><li class="listitem"><p>
      Ensure the vSphere distributed virtual switches (DVS) are configured for
      each cluster.
     </p></li></ul></div><div id="id-1.4.5.11.10.4.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     The MTU setting for each DVS should be set to 1600. NSX should
     automatically apply this setting to each DVS during the setup process.
     Alternatively, the setting can be manually applied to each DVS before
     setup if desired.
    </p></div><p>
    Make sure there is a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
    <code class="literal">ardana</code> home directory,
    <code class="filename">var/lib/ardana</code>, and that it is called
    <code class="filename">sles12sp3.iso</code>.
   </p><p>
    Install the <code class="literal">open-vm-tools</code> package.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install open-vm-tools</pre></div><section class="sect4" id="id-1.4.5.11.10.4.9.10" data-id-title="Install NSX Manager"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.1 </span><span class="title-name">Install NSX Manager</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.10">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The NSX Manager is the centralized network management component of NSX. It
     provides a single point of configuration and REST API entry-points.
    </p><p>
     The NSX Manager is installed as a virtual appliance on one of the ESXi
     hosts within the vSphere environment. This guide will cover installing the
     appliance on one of the ESXi hosts within the control-plane cluster. For
     more detailed information, refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html" target="_blank">VMware's
     NSX Installation Guide.</a>
    </p><p>
     To install the NSX Manager, download the virtual appliance from
     <a class="link" href="https://www.vmware.com/go/download-nsx-vsphere" target="_blank">VMware</a>
     and deploy the appliance within vCenter onto one of the ESXi hosts. For
     information on deploying appliances within vCenter, refer to VMware's
     documentation for ESXi
     <a class="link" href="https://pubs.vmware.com/vsphere-55/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">5.5</a>
     or
     <a class="link" href="https://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">6.0</a>.
    </p><p>
     During the deployment of the NSX Manager appliance, be aware of the
     following:
    </p><p>
     When prompted, select <span class="guimenu">Accept extra configuration
     options</span>. This will present options for configuring IPv4 and IPv6
     addresses, the default gateway, DNS, NTP, and SSH properties during the
     installation, rather than configuring these settings manually after the
     installation.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Choose an ESXi host that resides within the control-plane cluster.
      </p></li><li class="listitem"><p>
       Ensure that the network mapped port group is the DVS port group that
       represents the VLAN the NSX Manager will use for its networking (in this
       example it is labeled as the <code class="literal">NSX Management</code> network).
      </p></li></ul></div><div id="id-1.4.5.11.10.4.9.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The IP address assigned to the NSX Manager must be able to resolve
      reverse DNS.
     </p></div><p>
     Power on the NSX Manager virtual machine after it finishes deploying and
     wait for the operating system to fully load. When ready, carry out the
     following steps to have the NSX Manager use single sign-on (SSO) and to
     register the NSX Manager with vCenter:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Open a web browser and enter the hostname or IP address that was
       assigned to the NSX Manager during setup.
      </p></li><li class="step"><p>
       Log in with the username <code class="literal">admin</code> and the password set
       during the deployment.
      </p></li><li class="step"><p>
       After logging in, click on <span class="guimenu">Manage vCenter
       Registration</span>.
      </p></li><li class="step"><p>
       Configure the NSX Manager to connect to the vCenter server.
      </p></li><li class="step"><p>
       Configure NSX manager for single sign on (SSO) under the <span class="guimenu">Lookup
       Server URL</span> section.
      </p></li></ol></div></div><div id="id-1.4.5.11.10.4.9.10.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      When configuring SSO, use <code class="literal">Lookup Service Port 443</code> for
      vCenter version 6.0. Use <code class="literal">Lookup Service Port 7444</code> for
      vCenter version 5.5.
     </p><p>
      SSO makes vSphere and NSX more secure by allowing the various components
      to communicate with each other through a secure token exchange mechanism,
      instead of requiring each component to authenticate a user separately.
      For more details, refer to VMware's documentation on
      <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html" target="_blank">Configure
      Single Sign-On</a>.
     </p></div><p>
     Both the <code class="literal">Lookup Service URL</code> and the <code class="literal">vCenter
     Server</code> sections should have a status of
     <code class="literal">connected</code> when configured properly.
    </p><p>
     Log into the vSphere Web Client (log out and and back in if already logged
     in). The NSX Manager will appear under the <span class="guimenu">Networking &amp;
     Security</span> section of the client.
    </p><div id="id-1.4.5.11.10.4.9.10.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The <span class="guimenu">Networking &amp; Security</span> section will not appear
      under the vSphere desktop client. Use of the web client is required for
      the rest of this process.
     </p></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.11" data-id-title="Add NSX Controllers"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.2 </span><span class="title-name">Add NSX Controllers</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.11">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The NSX controllers serve as the central control point for all logical
     switches within the vSphere environment's network, and they maintain
     information about all hosts, logical switches (VXLANs), and distributed
     logical routers.
    </p><p>
     NSX controllers will each be deployed as a virtual appliance on the ESXi
     hosts within the control-plane cluster to form the NSX Controller cluster.
     For details about NSX controllers and the NSX control plane in general,
     refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-4E0FEE83-CF2C-45E0-B0E6-177161C3D67C.html" target="_blank">VMware's
     NSX documentation</a>.
    </p><div id="id-1.4.5.11.10.4.9.11.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      Whatever the size of the NSX deployment, the following conditions must be
      met:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Each NSX Controller cluster must contain three controller nodes. Having
        a different number of controller nodes is not supported.
       </p></li><li class="listitem"><p>
        Before deploying NSX Controllers, you must deploy an NSX Manager
        appliance and register vCenter with NSX Manager.
       </p></li><li class="listitem"><p>
        Determine the IP pool settings for your controller cluster, including
        the gateway and IP address range. DNS settings are optional.
       </p></li><li class="listitem"><p>
        The NSX Controller IP network must have connectivity to the NSX Manager
        and to the management interfaces on the ESXi hosts.
       </p></li></ul></div></div><p>
     Log in to the vSphere web client and do the following steps to add the NSX
     controllers:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In vCenter, navigate to <span class="guimenu">Home</span>, select
       <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>, and then
       select the <span class="guimenu">Management</span> tab.
      </p></li><li class="step"><p>
       In the <span class="guimenu">NSX Controller nodes</span> section, click the
       <span class="guimenu">Add Node</span> icon represented by a green plus sign.
      </p></li><li class="step"><p>
       Enter the NSX Controller settings appropriate to your environment. If
       you are following this example, use the control-plane clustered ESXi
       hosts and control-plane DVS port group for the controller settings.
      </p></li><li class="step"><p>
       If it has not already been done, create an IP pool for the NSX
       Controller cluster with at least three IP addressess by clicking
       <span class="guimenu">New IP Pool</span>. Individual controllers can be in
       separate IP subnets, if necessary.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to deploy the controller. After the first
       controller is completely deployed, deploy two additional controllers.
      </p></li></ol></div></div><div id="id-1.4.5.11.10.4.9.11.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      Three NSX controllers is mandatory. VMware recommends configuring a DRS
      anti-affinity rule to prevent the controllers from residing on the same
      ESXi host. See more information about
      <a class="link" href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.resmgmt.doc%2FGUID-FF28F29C-8B67-4EFF-A2EF-63B3537E6934.html" target="_blank">DRS
      Affinity Rules</a>.
     </p></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.12" data-id-title="Prepare Clusters for NSX Management"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.3 </span><span class="title-name">Prepare Clusters for NSX Management</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.12">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     During <span class="guimenu">Host Preparation</span>, the NSX Manager:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Installs the NSX kernel modules on ESXi hosts that are members of
       vSphere clusters
      </p></li><li class="listitem"><p>
       Builds the NSX control-plane and management-plane infrastructure
      </p></li></ul></div><p>
     The NSX kernel modules are packaged in <code class="filename">VIB</code> (vSphere
     Installation Bundle) files. They run within the hypervisor kernel and
     provide services such as distributed routing, distributed firewall, and
     VXLAN bridging capabilities. These files are installed on a per-cluster
     level, and the setup process deploys the required software on all ESXi
     hosts in the target cluster. When a new ESXi host is added to the cluster,
     the required software is automatically installed on the newly added host.
    </p><p>
     Before beginning the NSX host preparation process, make sure of the
     following in your environment:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Register vCenter with NSX Manager and deploy the NSX controllers.
      </p></li><li class="listitem"><p>
       Verify that DNS reverse lookup returns a fully qualified domain name
       when queried with the IP address of NSX Manager.
      </p></li><li class="listitem"><p>
       Verify that the ESXi hosts can resolve the DNS name of vCenter server.
      </p></li><li class="listitem"><p>
       Verify that the ESXi hosts can connect to vCenter Server on port 80.
      </p></li><li class="listitem"><p>
       Verify that the network time on vCenter Server and the ESXi hosts is
       synchronized.
      </p></li><li class="listitem"><p>
       For each vSphere cluster that will participate in NSX, verify that the
       ESXi hosts within each respective cluster are attached to a common VDS.
      </p><p>
       For example, given a deployment with two clusters named Host1 and Host2.
       Host1 is attached to VDS1 and VDS2. Host2 is attached to VDS1 and VDS3.
       When you prepare a cluster for NSX, you can only associate NSX with VDS1
       on the cluster. If you add another host (Host3) to the cluster and Host3
       is not attached to VDS1, it is an invalid configuration, and Host3 will
       not be ready for NSX functionality.
      </p></li><li class="listitem"><p>
       If you have vSphere Update Manager (VUM) in your environment, you must
       disable it before preparing clusters for network virtualization. For
       information on how to check if VUM is enabled and how to disable it if
       necessary, see the
       <a class="link" href="http://kb.vmware.com/kb/2053782" target="_blank">VMware knowledge
       base</a>.
      </p></li><li class="listitem"><p>
       In the vSphere web client, ensure that the cluster is in the resolved
       state (listed under the <span class="guimenu">Host Preparation</span> tab). If the
       Resolve option does not appear in the cluster's Actions list, then it is
       in a resolved state.
      </p></li></ul></div><p>
     To prepare the vSphere clusters for NSX:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In vCenter, select
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>, and then
       select the <span class="guimenu">Host Preparation</span> tab.
      </p></li><li class="step"><p>
       Continuing with the example in these instructions, click on the
       <span class="guimenu">Actions</span> button (gear icon) and select
       <span class="guimenu">Install</span> for both the control-plane cluster and
       compute cluster (if you are using something other than this example,
       then only install on the clusters that require NSX logical switching,
       routing, and firewalls).
      </p></li><li class="step"><p>
       Monitor the installation until the <code class="literal">Installation
       Status</code> column displays a green check mark.
      </p><div id="id-1.4.5.11.10.4.9.12.8.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        While installation is in progress, do not deploy, upgrade, or uninstall
        any service or component.
       </p></div><div id="id-1.4.5.11.10.4.9.12.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        If the <code class="literal">Installation Status</code> column displays a red
        warning icon and says <code class="literal">Not Ready</code>, click
        <span class="guimenu">Resolve</span>. Clicking <span class="guimenu">Resolve</span> might
        result in a reboot of the host. If the installation is still not
        successful, click the warning icon. All errors will be displayed. Take
        the required action and click <span class="guimenu">Resolve</span> again.
       </p></div></li><li class="step"><p>
       To verify the VIBs (<code class="filename">esx-vsip</code> and
       <code class="filename">esx-vxlan</code>) are installed and registered, SSH into
       an ESXi host within the prepared cluster. List the names and versions of
       the VIBs installed by running the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>esxcli software vib list | grep esx</pre></div><div class="verbatim-wrap"><pre class="screen">...
esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
...</pre></div></li></ol></div></div><div id="id-1.4.5.11.10.4.9.12.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      After host preparation:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        A host reboot is not required
       </p></li><li class="listitem"><p>
        If you add a host to a prepared cluster, the NSX VIBs are automatically
        installed on the host.
       </p></li><li class="listitem"><p>
        If you move a host to an unprepared cluster, the NSX VIBs are
        automatically uninstalled from the host. In this case, a host reboot is
        required to complete the uninstall process.
       </p></li></ul></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.13" data-id-title="Configure VXLAN Transport Parameters"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.4 </span><span class="title-name">Configure VXLAN Transport Parameters</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.13">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     VXLAN is configured on a per-cluster basis, where each vSphere cluster
     that is to participate in NSX is mapped to a vSphere Distributed Virtual
     Switch (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in
     that cluster is enabled for logical switches. The settings chosen in this
     section will be used in creating the VMkernel interface.
    </p><p>
     Configuring transport parameters involves selecting a DVS, a VLAN ID, an
     MTU size, an IP addressing mechanism, and a NIC teaming policy. The MTU
     for each switch must be set to 1550 or higher. By default, it is set to
     1600 by NSX. This is also the recommended setting for integration with
     <span class="productname">OpenStack</span>.
    </p><p>
     To configure the VXLAN transport parameters:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Host Preparation</span> tab.
      </p></li><li class="step"><p>
       Click the <span class="guimenu">Configure</span> link in the VXLAN column.
      </p></li><li class="step"><p>
       Enter the required information.
      </p></li><li class="step"><p>
       If you have not already done so, create an IP pool for the VXLAN tunnel
       end points (VTEP) by clicking <span class="guimenu">New IP Pool</span>:
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to create the VXLAN network.
      </p></li></ol></div></div><p>
     When configuring the VXLAN transport network, consider the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Use a NIC teaming policy that best suits the environment being built.
       <code class="literal">Load Balance - SRCID</code> as the VMKNic teaming policy is
       usually the most flexible out of all the available options. This allows
       each host to have a VTEP vmkernel interface for each dvuplink on the
       selected distributed switch (two dvuplinks gives two VTEP interfaces per
       ESXi host).
      </p></li><li class="listitem"><p>
       Do not mix different teaming policies for different portgroups on a VDS
       where some use Etherchannel or Link Aggregation Control Protocol (LACPv1
       or LACPv2) and others use a different teaming policy. If uplinks are
       shared in these different teaming policies, traffic will be interrupted.
       If logical routers are present, there will be routing problems. Such a
       configuration is not supported and should be avoided.
      </p></li><li class="listitem"><p>
       For larger environments it may be better to use DHCP for the VMKNic IP
       Addressing.
      </p></li><li class="listitem"><p>
       For more information and further guidance, see the
       <a class="link" href="https://communities.vmware.com/docs/DOC-27683" target="_blank">VMware
       NSX for vSphere Network Virtualization Design Guide</a>.
      </p></li></ul></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.14" data-id-title="Assign Segment ID Pool"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.5 </span><span class="title-name">Assign Segment ID Pool</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.14">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </p><p>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
     documentation</a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Logical Network Preparation</span> tab.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Segment ID</span>, and then <span class="guimenu">Edit</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save your changes.
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.15" data-id-title="Assign Segment ID Pool"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.6 </span><span class="title-name">Assign Segment ID Pool</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.15">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </p><p>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
     documentation</a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Logical Network Preparation</span> tab.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Segment ID</span>, and then <span class="guimenu">Edit</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save your changes.
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.16" data-id-title="Create a Transport Zone"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.7 </span><span class="title-name">Create a Transport Zone</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.16">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     A transport zone controls which hosts a logical switch can reach and has
     the following characteristics.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       It can span one or more vSphere clusters.
      </p></li><li class="listitem"><p>
       Transport zones dictate which clusters can participate in the use of a
       particular network. Therefore they dictate which VMs can participate in
       the use of a particular network.
      </p></li><li class="listitem"><p>
       A vSphere NSX environment can contain one or more transport zones based
       on the environment's requirements.
      </p></li><li class="listitem"><p>
       A host cluster can belong to multiple transport zones.
      </p></li><li class="listitem"><p>
       A logical switch can belong to only one transport zone.
      </p></li></ul></div><div id="id-1.4.5.11.10.4.9.16.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      <span class="productname">OpenStack</span> has only been verified to work with a single transport zone
      within a vSphere NSX-v environment. Other configurations are currently
      not supported.
     </p></div><p>
     For more information on transport zones, refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html" target="_blank">VMware's
     Add A Transport Zone</a>.
    </p><p>
     To create a transport zone:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       In the vSphere web client, navigate to
       <span class="guimenu">Home</span> › <span class="guimenu">Networking &amp;
       Security</span> › <span class="guimenu">Installation</span>.
      </p></li><li class="step"><p>
       Select the <span class="guimenu">Logical Network Preparation</span> tab.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Transport Zones</span>, and then click the
       <span class="guimenu">New Transport Zone</span> (New Logical Switch) icon.
      </p></li><li class="step"><p>
       In the <span class="guimenu">New Transport Zone</span> dialog box, type a name and
       an optional description for the transport zone.
      </p></li><li class="step"><p>
       For these example instructions, select the control plane mode as
       <code class="literal">Unicast</code>.
      </p><div id="id-1.4.5.11.10.4.9.16.7.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Whether there is a controller in the environment or if the environment
        is going to use multicast addresses will determine the control plane
        mode to select:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">Unicast</code> (what this set of instructions uses): The
          control plane is handled by an NSX controller. All unicast traffic
          leverages optimized headend replication. No multicast IP addresses or
          special network configuration is required.
         </p></li><li class="listitem"><p>
          <code class="literal">Multicast</code>: Multicast IP addresses in the physical
          network are used for the control plane. This mode is recommended only
          when upgrading from older VXLAN deployments. Requires PIM/IGMP in the
          physical network.
         </p></li><li class="listitem"><p>
          <code class="literal">Hybrid</code>: Offloads local traffic replication to the
          physical network (L2 multicast). This requires IGMP snooping on the
          first-hop switch and access to an IGMP querier in each VTEP subnet,
          but does not require PIM. The first-hop switch handles traffic
          replication for the subnet.
         </p></li></ul></div></div></li><li class="step"><p>
       Select the clusters to be added to the transport zone.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> to save your changes.
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.17" data-id-title="Deploying SUSE OpenStack Cloud"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.8 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.17">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     With vSphere environment setup completed, the <span class="productname">OpenStack</span> can be deployed.
     The following sections will cover creating virtual machines within the
     vSphere environment, configuring the cloud model and integrating NSX-v
     Neutron core plugin into the <span class="productname">OpenStack</span>:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create the virtual machines
      </p></li><li class="step"><p>
       Deploy the Cloud Lifecycle Manager
      </p></li><li class="step"><p>
       Configure the Neutron environment with NSX-v
      </p></li><li class="step"><p>
       Modify the cloud input model
      </p></li><li class="step"><p>
       Set up the parameters
      </p></li><li class="step"><p>
       Deploy the Operating System with Cobbler
      </p></li><li class="step"><p>
       Deploy the cloud
      </p></li></ol></div></div></section><section class="sect4" id="id-1.4.5.11.10.4.9.18" data-id-title="Deploying SUSE OpenStack Cloud on Baremetal"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9 </span><span class="title-name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> on Baremetal</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Within the vSphere environment, create the <span class="productname">OpenStack</span> compute proxy virtual
     machines. There needs to be one Neutron compute proxy virtual machine per
     ESXi compute cluster.
    </p><p>
     For the minimum NSX hardware requirements, refer to
     <a class="xref" href="integrate-nsx-vsphere.html#nsx-hw-reqs-bm" title="NSX Hardware Requirements for Baremetal Integration">Table 16.2, “NSX Hardware Requirements for Baremetal Integration”</a>. Also be aware of the networking
     model to use for the VM network interfaces, see
     <a class="xref" href="integrate-nsx-vsphere.html#nsx-interface-reqs" title="NSX Interface Requirements">Table 16.3, “NSX Interface Requirements”</a>:
    </p><p>
     If ESX VMs are to be used as Nova compute proxy nodes, set up three
     LAN interfaces in each virtual machine as shown in the table below. There
     is at least one Nova compute proxy node per cluster.
    </p><div class="table" id="nsx-interface-reqs" data-id-title="NSX Interface Requirements"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.3: </span><span class="title-name">NSX Interface Requirements </span></span><a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-interface-reqs">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Network Group
         </p>
        </th><th style="border-bottom: 1px solid ; ">
         <p>
          Interface
         </p>
        </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Management
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          <code class="literal">eth0</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          External API
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          <code class="literal">eth1</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Internal API
         </p>
        </td><td>
         <p>
          <code class="literal">eth2</code>
         </p>
        </td></tr></tbody></table></div></div><section class="sect5" id="id-1.4.5.11.10.4.9.18.6" data-id-title="Advanced Configuration Option"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.1 </span><span class="title-name">Advanced Configuration Option</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.11.10.4.9.18.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       Within vSphere for each in the virtual machine:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         In the <span class="guimenu">Options</span> section, under <span class="guimenu">Advanced
         configuration parameters</span>, ensure that
         <code class="literal">disk.EnableUUIDoption</code> is set to
         <code class="literal">true</code>.
        </p></li><li class="listitem"><p>
         If the option does not exist, it must be added. This option is
         required for the <span class="productname">OpenStack</span> deployment.
        </p></li><li class="listitem"><p>
         If the option is not specified, then the deployment will fail when
         attempting to configure the disks of each virtual machine.
        </p></li></ul></div></div></section><section class="sect5" id="id-1.4.5.11.10.4.9.18.7" data-id-title="Setting Up the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.2 </span><span class="title-name">Setting Up the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.7">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.7.2" data-id-title="Installing the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.2.1 </span><span class="title-name">Installing the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.7.2">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the product from:
     </p><ol type="a" class="substeps"><li class="step"><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step"><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step"><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step"><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step"><p>
      Select the location.
     </p></li><li class="step"><p>
      Select the keyboard layout.
     </p></li><li class="step"><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step"><p>
      Create new account:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Enter a username.
       </p></li><li class="step"><p>
        Enter a password.
       </p></li><li class="step"><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step"><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.4.5.11.10.4.9.18.7.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></section></section><section class="sect5" id="id-1.4.5.11.10.4.9.18.8" data-id-title="Configure the Neutron Environment with NSX-v"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3 </span><span class="title-name">Configure the Neutron Environment with NSX-v</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      In summary, integrating NSX with vSphere has four major steps:
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Modify the input model to define the server roles, servers, network
        roles and networks. <a class="xref" href="integrate-nsx-vsphere.html#nsx-modify-input-model" title="16.1.2.5.3.2. Modify the Input Model">Section 16.1.2.5.3.2, “Modify the Input Model”</a>
       </p></li><li class="step"><p>
        Set up the parameters needed for Neutron and Nova to communicate
        with the ESX and NSX Manager. <a class="xref" href="integrate-nsx-vsphere.html#nsx-deploy-os-cobbler" title="16.1.2.5.3.3. Deploying the Operating System with Cobbler">Section 16.1.2.5.3.3, “Deploying the Operating System with Cobbler”</a>
       </p></li><li class="step"><p>
        Do the steps to deploy the cloud. <a class="xref" href="integrate-nsx-vsphere.html#nsx-deploy-cloud" title="16.1.2.5.3.4. Deploying the Cloud">Section 16.1.2.5.3.4, “Deploying the Cloud”</a>
       </p></li></ol></div></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.4" data-id-title="Third-Party Import of VMware NSX-v Into Neutron and Neutronclient"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.1 </span><span class="title-name">Third-Party Import of VMware NSX-v Into Neutron and Neutronclient</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
       To import the NSX-v Neutron core-plugin into Cloud Lifecycle Manager, run the third-party
       import playbook.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost third-party-import.yml</pre></div></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5" data-id-title="Modify the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2 </span><span class="title-name">Modify the Input Model</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
       After the third-party import has completed successfully, modify the
       input model:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Prepare for input model changes
        </p></li><li class="step"><p>
         Define the servers and server roles needed for a NSX-v cloud.
        </p></li><li class="step"><p>
         Define the necessary networks and network groups
        </p></li><li class="step"><p>
         Specify the services needed to be deployed on the Cloud Lifecycle Manager controllers
         and the Nova ESX compute proxy nodes.
        </p></li><li class="step"><p>
         Commit the changes and run the configuration processor.
        </p></li></ol></div></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.4" data-id-title="Prepare for Input Model Changes"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.1 </span><span class="title-name">Prepare for Input Model Changes</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.5.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        The previous steps created a modified <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tarball with the
        NSX-v core plugin in the Neutron and <code class="literal">neutronclient</code>
        venvs. The <code class="filename">tar</code> file can now be extracted and the
        <code class="filename">ardana-init.bash</code> script can be run to set up the
        deployment files and directories. If a modified
        <code class="filename">tar</code> file was not created, then extract the tar
        from the /media/cdrom/ardana location.
       </p><p>
        To run the <code class="filename">ardana-init.bash</code> script which is
        included in the build, use this commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/ardana/hos-init.bash</pre></div></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.5" data-id-title="Create the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.2 </span><span class="title-name">Create the Input Model</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.5.5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Copy the example input model to
        <code class="filename">~/openstack/my_cloud/definition/</code> directory:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx
<code class="prompt user">ardana &gt; </code>cp -R entry-scale-nsx ~/openstack/my_cloud/definition</pre></div><p>
        Refer to the reference input model in
        <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
        for details about how these definitions should be made. The main
        differences between this model and the standard Cloud Lifecycle Manager input models are:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Only the neutron-server is deployed. No other neutron agents are
          deployed.
         </p></li><li class="listitem"><p>
          Additional parameters need to be set in
          <code class="filename">pass_through.yml</code> and
          <code class="filename">nsx/nsx_config.yml</code>.
         </p></li><li class="listitem"><p>
          Nova ESX compute proxy nodes may be ESX virtual machines.
         </p></li></ul></div><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.5.6" data-id-title="Set up the Parameters"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.2.1 </span><span class="title-name">Set up the Parameters</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.5.5.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         The special parameters needed for the NSX-v integrations are set in
         the files <code class="filename">pass_through.yml</code> and
         <code class="filename">nsx/nsx_config.yml</code>. They are in the
         <code class="filename">~/openstack/my_cloud/definition/data</code> directory.
        </p><p>
         Parameters in <code class="filename">pass_through.yml</code> are in the sample
         input model in the
         <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
         directory. The comments in the sample input model file describe how to
         locate the values of the required parameters.
        </p><div class="verbatim-wrap"><pre class="screen">#
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <em class="replaceable">VCENTER_ADMIN_USERNAME</em>
        ip: <em class="replaceable">VCENTER_IP</em>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for the vCenter password. The string
        # generated is the encrypted password. Enter the string
        # enclosed by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_PASSWD_FROM_ARDANAENCRYPT</em>"

        # The id is is obtained by the URL
        # https://<em class="replaceable">VCENTER_IP</em>/mob/?moid=ServiceInstance&amp;doPath=content%2eabout,
        # field instanceUUID.
        id: <em class="replaceable">VCENTER_UUID</em>
  servers:
    -
      # Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml. There should be one esx-compute-proxy node per ESX
      # resource pool.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER1_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em>
    -
      id: esx-compute2
      data:
        vmware:
          vcenter_cluster: <em class="replaceable">VMWARE_CLUSTER2_NAME</em>
          vcenter_id: <em class="replaceable">VCENTER_UUID</em></pre></div><p>
         There are parameters in <code class="filename">nsx/nsx_config.yml</code>. The
         comments describes how to retrieve the values.
        </p><div class="verbatim-wrap"><pre class="screen"># (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name: NSX-CONFIG-CP1
      services:
        - nsx
      data:
        # (Required) URL for NSXv manager (e.g - https://management_ip).
        manager_uri: 'https://<em class="replaceable">NSX_MGR_IP</em>

        # (Required) NSXv username.
        user: 'admin'

        # (Required) Encrypted NSX Manager password.
        # Password encryption is done by the script
        # ~/openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # NOTE: Make sure that the NSX Manager password is encrypted with the same key
        # used to encrypt the VCenter password.
        #
        # The script will prompt for the NSX Manager password. The string
        # generated is the encrypted password. Enter the string enclosed
        # by double-quotes below.
        password: "<em class="replaceable">ENCRYPTED_NSX_MGR_PASSWD_FROM_ARDANAENCRYPT</em>"
        # (Required) datacenter id for edge deployment.
        # Retrieved using
        #    http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob/?moid=ServiceInstance&amp;doPath=content
        # click on the value from the rootFolder property. The datacenter_moid is
        # the value of the childEntity property.
        # The vCenter-ip-address comes from the file pass_through.yml in the
        # input model under "pass-through.global.vmware.ip".
        datacenter_moid: 'datacenter-21'
        # (Required) id of logic switch for physical network connectivity.
        # How to retrieve
        # 1. Get to the same page where the datacenter_moid is found.
        # 2. Click on the value of the rootFolder property.
        # 3. Click on the value of the childEntity property
        # 4. Look at the network property. The external network is
        #    network associated with EXTERNAL VM in VCenter.
        external_network: 'dvportgroup-74'
        # (Required) clusters ids containing OpenStack hosts.
        # Retrieved using http://<em class="replaceable">VCENTER_IP_ADDR</em>/mob, click on the value
        # from the rootFolder property. Then click on the value of the
        # hostFolder property. Cluster_moids are the values under childEntity
        # property of the compute clusters.
        cluster_moid: 'domain-c33,domain-c35'
        # (Required) resource-pool id for edge deployment.
        resource_pool_id: 'resgroup-67'
        # (Optional) datastore id for edge deployment. If not needed,
        # do not declare it.
        # datastore_id: 'datastore-117'

        # (Required) network scope id of the transport zone.
        # To get the vdn_scope_id, in the vSphere web client from the Home
        # menu:
        #   1. click on Networking &amp; Security
        #   2. click on installation
        #   3. click on the Logical Netowrk Preparation tab.
        #   4. click on the Transport Zones button.
        #   5. Double click on the transport zone being configure.
        #   6. Select Manage tab.
        #   7. The vdn_scope_id will appear at the end of the URL.
        vdn_scope_id: 'vdnscope-1'

        # (Optional) Dvs id for VLAN based networks. If not needed,
        # do not declare it.
        # dvs_id: 'dvs-68'

        # (Required) backup_edge_pool: backup edge pools management range,
        # - edge_type&gt;[edge_size]:<em class="replaceable">MINIMUM_POOLED_EDGES</em>:<em class="replaceable">MAXIMUM_POOLED_EDGES</em>
        # - edge_type: service (service edge) or  vdr (distributed edge)
        # - edge_size:  compact ,  large (by default),  xlarge  or  quadlarge
        backup_edge_pool: 'service:compact:4:10,vdr:compact:4:10'

        # (Optional) mgt_net_proxy_ips: management network IP address for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_ips: '10.142.14.251,10.142.14.252'

        # (Optional) mgt_net_proxy_netmask: management network netmask for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_netmask: '255.255.255.0'

        # (Optional) mgt_net_moid: Network ID for management network connectivity
        # Do not declare if not used.
        # mgt_net_moid: 'dvportgroup-73'

        # ca_file: Name of the certificate file. If insecure is set to True,
        # then this parameter is ignored. If insecure is set to False and this
        # parameter is not defined, then the system root CAs will be used
        # to verify the server certificate.
        ca_file: a/nsx/certificate/file

        # insecure:
        # If true (default), the NSXv server certificate is not verified.
        # If false, then the default CA truststore is used for verification.
        # This option is ignored if "ca_file" is set
        insecure: True
        # (Optional) edge_ha: if true, will duplicate any edge pool resources
        # Default to False if undeclared.
        # edge_ha: False
        # (Optional) spoofguard_enabled:
        # If True (default), indicates NSXV spoofguard component is used to
        # implement port-security feature.
        # spoofguard_enabled: True
        # (Optional) exclusive_router_appliance_size:
        # Edge appliance size to be used for creating exclusive router.
        # Valid values: 'compact', 'large', 'xlarge', 'quadlarge'
        # Defaults to 'compact' if not declared.  # exclusive_router_appliance_size:
        'compact'</pre></div></section></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.5.6" data-id-title="Commit Changes and Run the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.2.3 </span><span class="title-name">Commit Changes and Run the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.5.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Commit your changes with the input model and the required configuration
        values added to the <code class="filename">pass_through.yml</code> and
        <code class="filename">nsx/nsx_config.yml</code> files.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git commit -A -m "Configuration changes for NSX deployment"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
 -e \encrypt="" -e rekey=""</pre></div><p>
        If the playbook <code class="filename">config-processor-run.yml</code> fails,
        there is an error in the input model. Fix the error and repeat the
        above steps.
       </p></section></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.6" data-id-title="Deploying the Operating System with Cobbler"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.3 </span><span class="title-name">Deploying the Operating System with Cobbler</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         From the Cloud Lifecycle Manager, run Cobbler to install the operating system on the
         nodes after it has to be deployed:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
         Verify the nodes that will have an operating system installed by
         Cobbler by running this command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system find --netboot-enabled=1</pre></div></li><li class="step"><p>
         Reimage the nodes using Cobbler. Do not use Cobbler to reimage the
         nodes running as ESX virtual machines. The command below is run on a
         setup where the Nova ESX compute proxies are VMs. Controllers 1,
         2, and 3 are running on physical servers.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e \
   nodelist=controller1,controller2,controller3</pre></div></li><li class="step"><p>
         When the playbook has completed, each controller node should have an
         operating system installed with an IP address configured on
         <code class="literal">eth0</code>.
        </p></li><li class="step"><p>
         After your controller nodes have been completed, you should install
         the operating system on your Nova compute proxy virtual machines.
         Each configured virtual machine should be able to PXE boot into the
         operating system installer.
        </p></li><li class="step"><p>
         From within the vSphere environment, power on each Nova compute
         proxy virtual machine and watch for it to PXE boot into the OS
         installer via its console.
        </p><ol type="a" class="substeps"><li class="step"><p>
           If successful, the virtual machine will have the operating system
           automatically installed and will then automatically power off.
          </p></li><li class="step"><p>
           When the virtual machine has powered off, power it on and let it
           boot into the operating system.
          </p></li></ol></li><li class="step"><p>
         Verify network settings after deploying the operating system to each
         node.
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Verify that the NIC bus mapping specified in the cloud model input
           file
           (<code class="filename">~/ardana/my_cloud/definition/data/nic_mappings.yml</code>)
           matches the NIC bus mapping on each <span class="productname">OpenStack</span> node.
          </p><p>
           Check the NIC bus mapping with this command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem"><p>
           After the playbook has completed, each controller node should have
           an operating system installed with an IP address configured on eth0.
          </p></li></ul></div></li><li class="step"><p>
         When the ESX compute proxy nodes are VMs, install the operating system
         if you have not already done so.
        </p></li></ol></div></div></section><section class="sect6" id="id-1.4.5.11.10.4.9.18.8.7" data-id-title="Deploying the Cloud"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">16.2.2.4.9.3.4 </span><span class="title-name">Deploying the Cloud</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#id-1.4.5.11.10.4.9.18.8.7">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-vsphere-baremetal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
       When the configuration processor has completed successfully, the cloud
       can be deployed. Set the ARDANA_USER_PASSWORD_ENCRYPT_KEY environment
       variable before running <code class="filename">site.yml</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">PASSWORD_KEY</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><p>
       <em class="replaceable">PASSWORD_KEY</em> in the <code class="literal">export</code>
       command is the key used to encrypt the passwords for vCenter and NSX
       Manager.
      </p></section></section></section></section></section></section><section class="sect1" id="nsx-verification" data-id-title="Verifying the NSX-v Functionality After Integration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.3 </span><span class="title-name">Verifying the NSX-v Functionality After Integration</span></span> <a title="Permalink" class="permalink" href="integrate-nsx-vsphere.html#nsx-verification">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_8/xml/nsx-verification.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  After you have completed your <span class="productname">OpenStack</span> deployment and integrated the NSX-v
  Neutron plugin, you can use these steps to verify that NSX-v is enabled and
  working in the environment.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Validating Neutron from the Cloud Lifecycle Manager. All of these commands require that you
    authenticate by <code class="filename">service.osrc</code> file.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="step"><p>
    List your Neutron networks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron network list
+--------------------------------------+----------------+-------------------------------------------------------+
| id                                   | name           | subnets                                               |
+--------------------------------------+----------------+-------------------------------------------------------+
| 574d5f6c-871e-47f8-86d2-4b7c33d91002 | inter-edge-net | c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512 169.254.128.0/17 |
+--------------------------------------+----------------+-------------------------------------------------------+</pre></div></li><li class="step"><p>
    List your Neutron subnets:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron subnet list
+--------------------------------------+-------------------+------------------+------------------------------------------------------+
| id                                   | name              | cidr             | allocation_pools                                     |
+--------------------------------------+-------------------+------------------+------------------------------------------------------+
| c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512 | inter-edge-subnet | 169.254.128.0/17 | {"start": "169.254.128.2", "end": "169.254.255.254"} |
+--------------------------------------+-------------------+------------------+------------------------------------------------------+</pre></div></li><li class="step"><p>
    List your Neutron routers:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron router list
+--------------------------------------+-----------------------+-----------------------+-------------+
| id                                   | name                  | external_gateway_info | distributed |
+--------------------------------------+-----------------------+-----------------------+-------------+
| 1c5bf781-5120-4b7e-938b-856e23e9f156 | metadata_proxy_router | null                  | False       |
| 8b5d03bf-6f77-4ea9-bb27-87dd2097eb5c | metadata_proxy_router | null                  | False       |
+--------------------------------------+-----------------------+-----------------------+-------------+</pre></div></li><li class="step"><p>
    List your Neutron ports:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron port list
+--------------------------------------+------+-------------------+------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                            |
+--------------------------------------+------+-------------------+------------------------------------------------------+
| 7f5f0461-0db4-4b9a-a0c6-faa0010b9be2 |      | fa:16:3e:e5:50:d4 | {"subnet_id":                                        |
|                                      |      |                   | "c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512",              |
|                                      |      |                   | "ip_address": "169.254.128.2"}                       |
| 89f27dff-f38d-4084-b9b0-ded495255dcb |      | fa:16:3e:96:a0:28 | {"subnet_id":                                        |
|                                      |      |                   | "c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512",              |
|                                      |      |                   | "ip_address": "169.254.128.3"}                       |
+--------------------------------------+------+-------------------+------------------------------------------------------+</pre></div></li><li class="step"><p>
    List your Neutron security group rules:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron security group rule list
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+
| id                                   | security_group | direction | ethertype | protocol/port | remote          |
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+
| 0385bd3a-1050-4bc2-a212-22ddab00c488 | default        | egress    | IPv6      | any           | any             |
| 19f6f841-1a9a-4b4b-bc45-7e8501953d8f | default        | ingress   | IPv6      | any           | default (group) |
| 1b3b5925-7aa6-4b74-9df0-f417ee6218f1 | default        | egress    | IPv4      | any           | any             |
| 256953cc-23d7-404d-b140-2600d55e44a2 | default        | ingress   | IPv4      | any           | default (group) |
| 314c4e25-5822-44b4-9d82-4658ae87d93f | default        | egress    | IPv6      | any           | any             |
| 59d4a71e-9f99-4b3b-b75b-7c9ad34081e0 | default        | ingress   | IPv6      | any           | default (group) |
| 887e25ef-64b7-4b69-b301-e053f88efa6c | default        | ingress   | IPv4      | any           | default (group) |
| 949e9744-75cd-4ae2-8cc6-6c0f578162d7 | default        | ingress   | IPv4      | any           | default (group) |
| 9a83027e-d6d6-4b6b-94fa-7c0ced2eba37 | default        | egress    | IPv4      | any           | any             |
| abf63b79-35ad-428a-8829-8e8d796a9917 | default        | egress    | IPv4      | any           | any             |
| be34b72b-66b6-4019-b782-7d91674ca01d | default        | ingress   | IPv6      | any           | default (group) |
| bf3d87ce-05c8-400d-88d9-a940e43760ca | default        | egress    | IPv6      | any           | any             |
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+</pre></div></li></ol></div></div><p>
  Verify metadata proxy functionality
 </p><p>
  To test that the metadata proxy virtual machines are working as intended,
  verify that there are at least two metadata proxy virtual machines from
  within vSphere (there will be four if edge high availability was set to
  true).
 </p><p>
  When that is verified, create a new compute instance either with the API,
  CLI, or within the cloud console GUI and log into the instance. From within
  the instance, using curl, grab the metadata instance-id from the
  metadata proxy address.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl http://169.254.169.254/latest/meta-data/instance-id
i-00000004</pre></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="install-esx-ovsvapp.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 15 </span>Installing ESX Computes and OVSvAPP</span></a> </div><div><a class="pagination-link next" href="install-ironic-overview.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 17 </span>Installing Baremetal (Ironic)</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="integrate-nsx-vsphere.html#nsx-vsphere-vm"><span class="title-number">16.1 </span><span class="title-name">Integrating with NSX for vSphere</span></a></span></li><li><span class="section"><a href="integrate-nsx-vsphere.html#nsx-vsphere-baremetal"><span class="title-number">16.2 </span><span class="title-name">Integrating with NSX for vSphere on Baremetal</span></a></span></li><li><span class="section"><a href="integrate-nsx-vsphere.html#nsx-verification"><span class="title-number">16.3 </span><span class="title-name">Verifying the NSX-v Functionality After Integration</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>