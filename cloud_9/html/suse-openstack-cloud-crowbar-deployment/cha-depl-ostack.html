<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Deploying the OpenStack Services | Deployment Guide using Crowbar | SUSE OpenStack Cloud Crowbar 9</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud Crowbar" /><meta name="product-number" content="9" /><meta name="book-title" content="Deployment Guide using Crowbar" /><meta name="chapter-title" content="Chapter 12. Deploying the OpenStack Services" /><meta name="description" content="After the nodes are installed and configured you can start deploying the OpenStack components to finalize the installation. The components need to be deployed in a given order, because they depend on one another. The Pacemaker component for an HA setup is the only exception from this rule—it can be …" /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-assignee" content="dpopov@suse.com" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9" /><link rel="home" href="index.html" title="SUSE OpenStack Cloud Crowbar 9 Documentation" /><link rel="up" href="part-depl-ostack.html" title="Part III. Setting Up OpenStack Nodes and Services" /><link rel="prev" href="cha-depl-inst-nodes.html" title="Chapter 11. Installing the OpenStack Nodes" /><link rel="next" href="sec-deploy-policy-json.html" title="Chapter 13. Limiting Users' Access Rights" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="SUSE OpenStack Cloud Crowbar 9 Documentation"><span class="book-icon">SUSE OpenStack Cloud Crowbar 9 Documentation</span></a><span> › </span><a class="crumb" href="book-crowbar-deployment.html">Deployment Guide using Crowbar</a><span> › </span><a class="crumb" href="part-depl-ostack.html">Setting Up OpenStack Nodes and Services</a><span> › </span><a class="crumb" href="cha-depl-ostack.html">Deploying the OpenStack Services</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Deployment Guide using Crowbar</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="pre-cloud-deploy.html"><span class="number"> </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="part-depl-intro.html"><span class="number">I </span><span class="name">Architecture and Requirements</span></a><ol><li class="inactive"><a href="cha-depl-arch.html"><span class="number">1 </span><span class="name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></a></li><li class="inactive"><a href="cha-depl-req.html"><span class="number">2 </span><span class="name">Considerations and Requirements</span></a></li></ol></li><li class="inactive"><a href="part-depl-admserv.html"><span class="number">II </span><span class="name">Setting Up the Administration Server</span></a><ol><li class="inactive"><a href="cha-depl-adm-inst.html"><span class="number">3 </span><span class="name">Installing the Administration Server</span></a></li><li class="inactive"><a href="app-deploy-smt.html"><span class="number">4 </span><span class="name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></a></li><li class="inactive"><a href="cha-depl-repo-conf.html"><span class="number">5 </span><span class="name">Software Repository Setup</span></a></li><li class="inactive"><a href="sec-depl-adm-inst-network.html"><span class="number">6 </span><span class="name">Service Configuration:  Administration Server Network Configuration</span></a></li><li class="inactive"><a href="sec-depl-adm-inst-crowbar.html"><span class="number">7 </span><span class="name">Crowbar Setup</span></a></li><li class="inactive"><a href="sec-depl-adm-start-crowbar.html"><span class="number">8 </span><span class="name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></a></li><li class="inactive"><a href="sec-depl-adm-crowbar-extra-features.html"><span class="number">9 </span><span class="name">Customizing Crowbar</span></a></li></ol></li><li class="inactive"><a href="part-depl-ostack.html"><span class="number">III </span><span class="name">Setting Up <span class="productname">OpenStack</span> Nodes and Services</span></a><ol><li class="inactive"><a href="cha-depl-crowbar.html"><span class="number">10 </span><span class="name">The Crowbar Web Interface</span></a></li><li class="inactive"><a href="cha-depl-inst-nodes.html"><span class="number">11 </span><span class="name">Installing the <span class="productname">OpenStack</span> Nodes</span></a></li><li class="inactive"><a href="cha-depl-ostack.html"><span class="number">12 </span><span class="name">Deploying the <span class="productname">OpenStack</span> Services</span></a></li><li class="inactive"><a href="sec-deploy-policy-json.html"><span class="number">13 </span><span class="name">Limiting Users' Access Rights</span></a></li><li class="inactive"><a href="cha-depl-ostack-configs.html"><span class="number">14 </span><span class="name">Configuration Files for <span class="productname">OpenStack</span> Services</span></a></li><li class="inactive"><a href="install-heat-templates.html"><span class="number">15 </span><span class="name">Installing SUSE CaaS Platform heat Templates</span></a></li><li class="inactive"><a href="install-caasp-terraform.html"><span class="number">16 </span><span class="name">Installing SUSE CaaS Platform v4 using terraform</span></a></li></ol></li><li class="inactive"><a href="part-depl-nostack.html"><span class="number">IV </span><span class="name">Setting Up Non-<span class="productname">OpenStack</span> Services</span></a><ol><li class="inactive"><a href="cha-depl-nostack.html"><span class="number">17 </span><span class="name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></a></li></ol></li><li class="inactive"><a href="part-depl-troubleshooting.html"><span class="number">V </span><span class="name">Troubleshooting and Support</span></a><ol><li class="inactive"><a href="cha-depl-trouble.html"><span class="number">18 </span><span class="name">Troubleshooting and Support</span></a></li></ol></li><li class="inactive"><a href="app-deploy-cisco.html"><span class="number">A </span><span class="name">Using Cisco Nexus Switches with neutron</span></a></li><li class="inactive"><a href="app-deploy-docupdates.html"><span class="number">B </span><span class="name">Documentation Updates</span></a></li><li class="inactive"><a href="gl-cloud.html"><span class="number"> </span><span class="name">Glossary of Terminology and Product Names</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 11. Installing the OpenStack Nodes" href="cha-depl-inst-nodes.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 13. Limiting Users' Access Rights" href="sec-deploy-policy-json.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="SUSE OpenStack Cloud Crowbar 9 Documentation"><span class="book-icon">SUSE OpenStack Cloud Crowbar 9 Documentation</span></a><span> › </span><a class="crumb" href="book-crowbar-deployment.html">Deployment Guide using Crowbar</a><span> › </span><a class="crumb" href="part-depl-ostack.html">Setting Up OpenStack Nodes and Services</a><span> › </span><a class="crumb" href="cha-depl-ostack.html">Deploying the OpenStack Services</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 11. Installing the OpenStack Nodes" href="cha-depl-inst-nodes.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 13. Limiting Users' Access Rights" href="sec-deploy-policy-json.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="cha-depl-ostack"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span> <span class="productnumber ">9</span></div><div><h2 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the <span class="productname">OpenStack</span> Services</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>cha-depl-ostack</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-designate"><span class="number">12.1 </span><span class="name">Deploying designate</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-pacemaker"><span class="number">12.2 </span><span class="name">Deploying Pacemaker (Optional, HA Setup Only)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-db"><span class="number">12.3 </span><span class="name">Deploying the Database</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-rabbit"><span class="number">12.4 </span><span class="name">Deploying RabbitMQ</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-keystone"><span class="number">12.5 </span><span class="name">Deploying keystone</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-swift"><span class="number">12.6 </span><span class="name">Deploying swift (optional)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-glance"><span class="number">12.7 </span><span class="name">Deploying glance</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-cinder"><span class="number">12.8 </span><span class="name">Deploying cinder</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-neutron"><span class="number">12.9 </span><span class="name">Deploying neutron</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-nova"><span class="number">12.10 </span><span class="name">Deploying nova</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-dash"><span class="number">12.11 </span><span class="name">Deploying horizon (<span class="productname">OpenStack</span> Dashboard)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-heat"><span class="number">12.12 </span><span class="name">Deploying heat (Optional)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-ceilometer"><span class="number">12.13 </span><span class="name">Deploying ceilometer (Optional)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-manila"><span class="number">12.14 </span><span class="name">Deploying manila</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-tempest"><span class="number">12.15 </span><span class="name">Deploying Tempest (Optional)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-magnum"><span class="number">12.16 </span><span class="name">Deploying Magnum (Optional)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-barbican"><span class="number">12.17 </span><span class="name">Deploying barbican (Optional)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-sahara"><span class="number">12.18 </span><span class="name">Deploying sahara</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-monasca"><span class="number">12.19 </span><span class="name">Deploying monasca</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-octavia"><span class="number">12.20 </span><span class="name">Deploying Octavia</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-ironic"><span class="number">12.21 </span><span class="name">Deploying ironic (optional)</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-ostack-final"><span class="number">12.22 </span><span class="name">How to Proceed</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#crow-ses-integration"><span class="number">12.23 </span><span class="name">SUSE Enterprise Storage integration</span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-depl-services"><span class="number">12.24 </span><span class="name">Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span></a></span></dt><dt><span class="sect1"><a href="cha-depl-ostack.html#sec-deploy-crowbatch-description"><span class="number">12.25 </span><span class="name">Crowbar Batch Command</span></a></span></dt></dl></div></div><p>
  After the nodes are installed and configured you can start deploying the
  <span class="productname">OpenStack</span> components to finalize the installation. The components need to be
  deployed in a given order, because they depend on one another. The
  <span class="guimenu ">Pacemaker</span> component for an HA setup is the only exception
  from this rule—it can be set up at any time. However, when deploying
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> from scratch, we recommend deploying the
  <span class="guimenu ">Pacemaker</span> proposal(s) first. Deployment for all components
  is done from the Crowbar Web interface through recipes, so-called
  <span class="quote">“<span class="quote ">barclamps</span>”</span>. (See <a class="xref" href="cha-depl-ostack.html#sec-depl-services" title="12.24. Roles and Services in SUSE OpenStack Cloud Crowbar">Section 12.24, “Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>”</a> for a table
  of all roles and services, and how to start and stop them.)
 </p><p>
  The components controlling the cloud, including storage management and
  control components, need to be installed on the Control Node(s) (refer to
  <a class="xref" href="cha-depl-arch.html#sec-depl-arch-components-control" title="1.2. The Control Node(s)">Section 1.2, “The Control Node(s)”</a> for more information).
  However, you may <span class="emphasis"><em>not</em></span> use your Control Node(s) as a
  compute node or storage host for swift. Do not install the components
  <span class="guimenu ">swift-storage</span> and <span class="guimenu ">nova-compute-*</span> on the
  Control Node(s). These components must be installed on dedicated Storage Nodes
  and Compute Nodes.
 </p><p>
  When deploying an HA setup, the  Control Nodes are replaced by one or more
  controller clusters consisting of at least two nodes, and three are
  recommended. We recommend setting up three separate clusters for data,
  services, and networking. See <a class="xref" href="cha-depl-req.html#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a> for more
  information on requirements and recommendations for an HA setup.
 </p><p>
  The <span class="productname">OpenStack</span> components need to be deployed in the following order. For
  general instructions on how to edit and deploy barclamps, refer to
  <a class="xref" href="cha-depl-crowbar.html#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>. Any optional components that you
  elect to use must be installed in their correct order.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-designate" title="12.1. Deploying designate">Deploying designate</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-pacemaker" title="12.2. Deploying Pacemaker (Optional, HA Setup Only)">Deploying Pacemaker (Optional, HA Setup Only)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-db" title="12.3. Deploying the Database">Deploying the Database</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-rabbit" title="12.4. Deploying RabbitMQ">Deploying RabbitMQ</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-keystone" title="12.5. Deploying keystone">Deploying keystone</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-swift" title="12.6. Deploying swift (optional)">Deploying swift (optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-glance" title="12.7. Deploying glance">Deploying glance</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-cinder" title="12.8. Deploying cinder">Deploying cinder</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-neutron" title="12.9. Deploying neutron">Deploying neutron</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-nova" title="12.10. Deploying nova">Deploying nova</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-dash" title="12.11. Deploying horizon (OpenStack Dashboard)">Deploying horizon (<span class="productname">OpenStack</span> Dashboard)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-heat" title="12.12. Deploying heat (Optional)">Deploying heat (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-ceilometer" title="12.13. Deploying ceilometer (Optional)">Deploying ceilometer (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-manila" title="12.14. Deploying manila">Deploying manila</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-tempest" title="12.15. Deploying Tempest (Optional)">Deploying Tempest (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-magnum" title="12.16. Deploying Magnum (Optional)">Deploying Magnum (Optional)</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-monasca" title="12.19. Deploying monasca">Deploying monasca</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-octavia" title="12.20. Deploying Octavia">Deploying Octavia</a>
   </p></li></ol></div><div class="sect1 " id="sec-depl-ostack-designate"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying designate</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-designate">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-designate</li></ul></div></div></div></div><p>
    designate provides <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> DNS as a Service (DNSaaS). It is used to
    create and propagate zones and records over the network using pools of DNS
    servers. Deployment defaults are in place, so not much is required to
    configure designate. neutron needs additional settings for integration with
    designate, which are also present in the <code class="literal">[designate]</code> section in neutron configuration.
  </p><p>
    The designate barclamp relies heavily on the DNS barclamp and expects
    it to be applied without any failures.
  </p><div id="id-1.4.5.4.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In order to deploy designate, at least one node is necessary in the DNS
    barclamp that is not the admin node. The admin node is not added to the
    public network. So another node is needed that can be attached to the public
    network and appear in the designate default pool.
   </p><p>
    We recommend that DNS services are running in a cluster in highly available
    deployments where Designate services are running in a cluster.
    For example, in a typical HA deployment where the controllers
    are deployed in a 3-node cluster, the DNS barclamp should be applied to all
    the controllers, in the same manner as Designate.
   </p></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.7.5.1"><span class="term ">designate-server role</span></dt><dd><p>
          Installs the designate server packages and configures the mini-dns (mdns)
          service required by designate.
        </p></dd><dt id="id-1.4.5.4.7.5.2"><span class="term ">designate-worker role</span></dt><dd><p>
        Configures a designate worker on the selected nodes. designate uses the
        workers to distribute its workload.
        </p></dd></dl></div><p>
    <code class="literal">designate Sink</code> is an optional service and is not configured as part
    of this barclamp.
  </p><p>
    designate uses pool(s) over which it can distribute zones and
    records. Pools can have varied configuration. Any misconfiguration can lead to
    information leakage.
  </p><p>
    The designate barclamp creates default Bind9 pool out of the box, which can be
    modified later as needed. The default Bind9 pool configuration is created by Crowbar
    on a node with <code class="literal">designate-server</code> role in
    <code class="filename">/etc/designate/pools.crowbar.yaml</code>. You can copy
    this file and edit it according to your requirements. Then provide this
    configuration to designate using the command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>designate-manage pool update --file /etc/designate/pools.crowbar.yaml</pre></div><p>
    The <code class="literal">dns_domain</code> specified in neutron configuration in <code class="literal">[designate]</code> section
    is the default Zone where DNS records for neutron resources are created via
    neutron-designate integration. If this is desired, you have to create this zone
    explicitly using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack zone create &lt; email &gt; &lt; dns_domain &gt;</pre></div><p>
   Editing the designate proposal:
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/designate-edit-proposal.png" target="_blank"><img src="images/designate-edit-proposal.png" width="" alt="Edit designate Proposal" /></a></div></div><div class="sect2 " id="sec-depl-ostack-designate-powerdns"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using PowerDNS Backend</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-designate-powerdns">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-designate-powerdns</li></ul></div></div></div></div><p>
    Designate uses Bind9 backend by default. It is also possible to
    use PowerDNS backend in addition to, or as an alternative, to Bind9 backend.
    To do so PowerDNS must be manually deployed as The designate barclamp
    currently does not provide any facility to automatically install and
    configure PowerDNS. This section outlines the steps to deploy PowerDNS
    backend.
   </p><div id="id-1.4.5.4.7.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     If PowerDNS is already deployed, you may skip the
     <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-install-powerdns" title="12.1.1.1. Install PowerDNS">Section 12.1.1.1, “Install PowerDNS”</a> section and jump to
     the <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-designate-use-powerdns-backend" title="12.1.1.2. Configure Designate To Use PowerDNS Backend">Section 12.1.1.2, “Configure Designate To Use PowerDNS Backend”</a>
     section.
    </p></div><div class="sect3 " id="sec-depl-ostack-install-powerdns"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install PowerDNS</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-install-powerdns">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-install-powerdns</li></ul></div></div></div></div><p>
     Follow these steps to install and configure PowerDNS on a Crowbar node.
     Keep in mind that PowerDNS must be deployed with MySQL backend.
    </p><div id="id-1.4.5.4.7.14.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      We recommend that PowerDNS are running in a cluster in highly
      availability deployments where Designate services are running in a
      cluster. For example, in a typical HA deployment
      where the controllers are deployed in a 3-node cluster, PowerDNS should
      be running on all the controllers, in the same manner as Designate.
     </p></div><div class="procedure " id="pro-deploy-powerdns"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Install PowerDNS packages.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install pdns pdns-backend-mysql</pre></div></li><li class="step "><p>
       Edit <code class="literal">/etc/pdns/pdns.conf</code> and provide these options:
       (See
       <a class="link" href="https://doc.powerdns.com/authoritative/settings.html" target="_blank">https://doc.powerdns.com/authoritative/settings.html</a> for a complete reference).
      </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.7.14.4.4.2.2.1"><span class="term ">api</span></dt><dd><p>
          Set it to <code class="literal">yes</code> to enable Web service Rest API.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.2"><span class="term ">api-key</span></dt><dd><p>
          Static Rest API access key. Use a secure random string here.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.3"><span class="term ">launch</span></dt><dd><p>
          Must set to <code class="literal">gmysql</code> to use MySQL backend.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.4"><span class="term ">gmysql-host</span></dt><dd><p>
          Hostname (i.e. FQDN) or IP address of the MySQL server.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.5"><span class="term ">gmysql-user</span></dt><dd><p>
          MySQL user which have full access to the PowerDNS database.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.6"><span class="term ">gmysql-password</span></dt><dd><p>
          Password for the MySQL user.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.7"><span class="term ">gmysql-dbname</span></dt><dd><p>
          MySQL database name for PowerDNS.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.8"><span class="term ">local-port</span></dt><dd><p>
          Port number where PowerDNS is listening for upcoming requests.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.9"><span class="term ">setgid</span></dt><dd><p>
          The group where the PowerDNS process is running under.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.10"><span class="term ">setuid</span></dt><dd><p>
          The user where the PowerDNS process is running under.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.11"><span class="term ">webserver</span></dt><dd><p>
          Must set to <code class="literal">yes</code> to enable web service RestAPI.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.12"><span class="term ">webserver-address</span></dt><dd><p>
          Hostname (FQDN) or IP address of the PowerDNS web service.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.13"><span class="term ">webserver-allow-from</span></dt><dd><p>
          List of IP addresses (IPv4 or IPv6) of the nodes that are permitted
          to talk to the PowerDNS web service. These must include the IP
          address of the Designate worker nodes.
         </p></dd></dl></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen">api=yes
api-key=Sfw234sDFw90z
launch=gmysql
gmysql-host=mysql.acme.com
gmysql-user=powerdns
gmysql-password=SuperSecured123
gmysql-dbname=powerdns
local-port=54
setgid=pdns
setuid=pdns
webserver=yes
webserver-address=192.168.124.83
webserver-allow-from=0.0.0.0/0,::/0</pre></div></li><li class="step "><p>
       Login to MySQL from a Crowbar MySQL node and create the PowerDNS database
       and the user which has full access to the PowerDNS database. Remember,
       the database name, username, and password must match
       <code class="literal">gmysql-dbname</code>, <code class="literal">gmysql-user</code>,
       and <code class="literal">gmysql-password</code> that were specified above
       respectively.
      </p><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 20075
Server version: 10.2.29-MariaDB-log SUSE package

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE powerdns;
Query OK, 1 row affected (0.01 sec)

MariaDB [(none)]&gt; GRANT ALL ON powerdns.* TO 'powerdns'@'localhost' IDENTIFIED BY 'SuperSecured123';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL ON powerdns.* TO 'powerdns'@'192.168.124.83' IDENTIFIED BY 'SuperSecured123';
Query OK, 0 rows affected, 1 warning (0.02 sec)

MariaDB [(none)]&gt; FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.01 sec)

MariaDB [(none)]&gt; exit
Bye</pre></div></li><li class="step "><p>
       Create a MySQL schema file, named <code class="literal">powerdns-schema.sql</code>,
       with the following content:
      </p><div class="verbatim-wrap"><pre class="screen">/*
 SQL statements to create tables in designate_pdns DB.
 Note: This file is taken as is from:
 https://raw.githubusercontent.com/openstack/designate/master/devstack/designate_plugins/backend-pdns4-mysql-db.sql
*/
CREATE TABLE domains (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255) NOT NULL,
  master                VARCHAR(128) DEFAULT NULL,
  last_check            INT DEFAULT NULL,
  type                  VARCHAR(6) NOT NULL,
  notified_serial       INT DEFAULT NULL,
  account               VARCHAR(40) DEFAULT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE UNIQUE INDEX name_index ON domains(name);


CREATE TABLE records (
  id                    INT AUTO_INCREMENT,
  domain_id             INT DEFAULT NULL,
  name                  VARCHAR(255) DEFAULT NULL,
  type                  VARCHAR(10) DEFAULT NULL,
  -- Changed to "TEXT", as VARCHAR(65000) is too big for most MySQL installs
  content               TEXT DEFAULT NULL,
  ttl                   INT DEFAULT NULL,
  prio                  INT DEFAULT NULL,
  change_date           INT DEFAULT NULL,
  disabled              TINYINT(1) DEFAULT 0,
  ordername             VARCHAR(255) BINARY DEFAULT NULL,
  auth                  TINYINT(1) DEFAULT 1,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX nametype_index ON records(name,type);
CREATE INDEX domain_id ON records(domain_id);
CREATE INDEX recordorder ON records (domain_id, ordername);


CREATE TABLE supermasters (
  ip                    VARCHAR(64) NOT NULL,
  nameserver            VARCHAR(255) NOT NULL,
  account               VARCHAR(40) NOT NULL,
  PRIMARY KEY (ip, nameserver)
) Engine=InnoDB;


CREATE TABLE comments (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  name                  VARCHAR(255) NOT NULL,
  type                  VARCHAR(10) NOT NULL,
  modified_at           INT NOT NULL,
  account               VARCHAR(40) NOT NULL,
  -- Changed to "TEXT", as VARCHAR(65000) is too big for most MySQL installs
  comment               TEXT NOT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX comments_domain_id_idx ON comments (domain_id);
CREATE INDEX comments_name_type_idx ON comments (name, type);
CREATE INDEX comments_order_idx ON comments (domain_id, modified_at);


CREATE TABLE domainmetadata (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  kind                  VARCHAR(32),
  content               TEXT,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX domainmetadata_idx ON domainmetadata (domain_id, kind);


CREATE TABLE cryptokeys (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  flags                 INT NOT NULL,
  active                BOOL,
  content               TEXT,
  PRIMARY KEY(id)
) Engine=InnoDB;

CREATE INDEX domainidindex ON cryptokeys(domain_id);


CREATE TABLE tsigkeys (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255),
  algorithm             VARCHAR(50),
  secret                VARCHAR(255),
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE UNIQUE INDEX namealgoindex ON tsigkeys(name, algorithm);</pre></div></li><li class="step "><p>
       Create the PowerDNS schema for the database using <code class="literal">mysql</code>
       CLI. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql powerdns &lt; powerdns-schema.sql</pre></div></li><li class="step "><p>
       Enable <code class="literal">pdns</code> systemd service.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable pdns
<code class="prompt user">root # </code>systemctl start pdns</pre></div><p>
       If <code class="literal">pdns</code> is successfully running, you should see the
       following logs by running <code class="literal">journalctl -u pdns</code> command.
      </p><div class="verbatim-wrap"><pre class="screen">Feb 07 01:44:12 d52-54-77-77-01-01 systemd[1]: Started PowerDNS Authoritative Server.
Feb 07 01:44:12 d52-54-77-77-01-01 pdns_server[21285]: Done launching threads, ready to distribute questions</pre></div></li></ol></div></div></div><div class="sect3 " id="sec-depl-ostack-designate-use-powerdns-backend"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Designate To Use PowerDNS Backend</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-designate-use-powerdns-backend">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-designate-use-powerdns-backend</li></ul></div></div></div></div><p>
     Configure Designate to use PowerDNS backend by appending the PowerDNS
     servers to <code class="literal">/etc/designate/pools.crowbar.yaml</code> file
     on a Designate worker node.
    </p><div id="id-1.4.5.4.7.14.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      If we are replacing Bind9 backend with PowerDNS backend, make sure to
      remove the <code class="literal">bind9</code> entries from
      <code class="literal">/etc/designate/pools.crowbar.yaml</code>.
     </p><p>
      In HA deployment, there should be multiple PowerDNS entries.
     </p><p>
      Also, make sure the <code class="literal">api_token</code> matches the
      <code class="literal">api-key</code> that was specified in the
      <code class="literal">/etc/pdns/pdns.conf</code> file earlier.
     </p></div><p>
     Append the PowerDNS entries to the end of
     <code class="literal">/etc/designate/pools.crowbar.yaml</code>. For example:
    </p><div class="verbatim-wrap"><pre class="screen">---
- name: default-bind
  description: Default BIND9 Pool
  id: 794ccc2c-d751-44fe-b57f-8894c9f5c842
  attributes: {}
  ns_records:
  - hostname: public-d52-54-77-77-01-01.virtual.cloud.suse.de.
    priority: 1
  - hostname: public-d52-54-77-77-01-02.virtual.cloud.suse.de.
    priority: 1
  nameservers:
  - host: 192.168.124.83
    port: 53
  - host: 192.168.124.81
    port: 53
  also_notifies: []
  targets:
  - type: bind9
    description: BIND9 Server
    masters:
    - host: 192.168.124.83
      port: 5354
    - host: 192.168.124.82
      port: 5354
    - host: 192.168.124.81
      port: 5354
    options:
      host: 192.168.124.83
      port: 53
      rndc_host: 192.168.124.83
      rndc_port: 953
      rndc_key_file: "/etc/designate/rndc.key"
  - type: bind9
    description: BIND9 Server
    masters:
    - host: 192.168.124.83
      port: 5354
    - host: 192.168.124.82
      port: 5354
    - host: 192.168.124.81
      port: 5354
    options:
      host: 192.168.124.81
      port: 53
      rndc_host: 192.168.124.81
      rndc_port: 953
      rndc_key_file: "/etc/designate/rndc.key"
  - type: pdns4
    description: PowerDNS4 DNS Server
    masters:
      - host: 192.168.124.83
        port: 5354
      - host: 192.168.124.82
        port: 5354
      - host: 192.168.124.81
        port: 5354
    options:
      host: 192.168.124.83
      port: 54
      api_endpoint: http://192.168.124.83:8081
      api_token: Sfw234sDFw90z</pre></div><p>
     Update the pools using <code class="literal">designate-manage</code> CLI.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>designate-manage pool update --file /etc/designate/pools.crowbar.yaml</pre></div><p>
     Once Designate sync up with PowerDNS, you should see the domains in the
     PowerDNS database which reflects the zones in Designate.
    </p><div id="id-1.4.5.4.7.14.5.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      It make take a few minutes for Designate to sync with PowerDNS.
     </p></div><p>
     We can verify that the domains are successfully sync up with Designate
     by inpsecting the <code class="literal">domains</code> table in the database.
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql powerdns
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 21131
Server version: 10.2.29-MariaDB-log SUSE package

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [powerdns]&gt; select * from domains;
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
| id | name    | master                                                       | last_check | type  | notified_serial | account |
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
|  1 | foo.bar | 192.168.124.81:5354 192.168.124.82:5354 192.168.124.83:5354  |       NULL | SLAVE |            NULL |         |
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
1 row in set (0.00 sec)</pre></div></div></div></div><div class="sect1 " id="sec-depl-ostack-pacemaker"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Pacemaker (Optional, HA Setup Only)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-pacemaker">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-pacemaker</li></ul></div></div></div></div><p>
   To make the SUSE <span class="productname">OpenStack</span> Cloud controller functions and the Compute Nodes highly
   available, set up one or more clusters by deploying Pacemaker (see
   <a class="xref" href="cha-depl-req.html#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a separate proposal needs to
   be created for each cluster.
  </p><p>
   Deploying Pacemaker is optional. In case you do not want to deploy it, skip
   this section and start the node deployment by deploying the database as
   described in <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-db" title="12.3. Deploying the Database">Section 12.3, “Deploying the Database”</a>.
  </p><div id="id-1.4.5.4.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Number of Cluster Nodes</h6><p>
    To set up a cluster, at least two nodes are required.  See <a class="xref" href="cha-depl-req.html#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a> for
    more information.
   </p></div><p>
   To create a proposal, go to <span class="guimenu ">Barclamps</span> › <span class="guimenu ">OpenStack</span> and click <span class="guimenu ">Edit</span>
   for the Pacemaker barclamp. A drop-down box where you can enter a name and a
   description for the proposal opens. Click <span class="guimenu ">Create</span> to open
   the configuration screen for the proposal.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_pacemaker_proposal.png" target="_blank"><img src="images/depl_barclamp_pacemaker_proposal.png" width="" alt="Create Pacemaker Proposal" /></a></div></div><div id="ann-depl-ostack-pacemaker-prop-name" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Proposal Name</h6><p>
    The name you enter for the proposal will be used to generate host names for
    the virtual IP addresses of HAProxy. By default, the names follow this
    scheme:
   </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">cluster-<em class="replaceable ">PROPOSAL_NAME</em>.<em class="replaceable ">FQDN</em></code>
    (for the internal name)</td></tr><tr><td><code class="literal">public-cluster-<em class="replaceable ">PROPOSAL_NAME</em>.<em class="replaceable ">FQDN</em></code>
    (for the public name)</td></tr></table><p>
    For example, when <em class="replaceable ">PROPOSAL_NAME</em> is set to
    <code class="literal">data</code>, this results in the following names:
   </p><table border="0" summary="Simple list" class="simplelist "><tr><td><code class="literal">cluster-data.example.com</code>
    </td></tr><tr><td><code class="literal">public-cluster-data.example.com</code>
    </td></tr></table><p>
    For requirements regarding SSL encryption and certificates, see
    <a class="xref" href="cha-depl-req.html#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a>.
   </p></div><p>
   The following options are configurable in the Pacemaker configuration
   screen:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.8.9.1"><span class="term ">Transport for Communication</span></dt><dd><p>
      Choose a technology used for cluster communication. You can choose
      between <span class="guimenu ">Multicast (UDP)</span>, sending a message to multiple
      destinations, or <span class="guimenu ">Unicast (UDPU)</span>, sending a message to
      a single destination. By default unicast is used.
     </p></dd><dt id="id-1.4.5.4.8.9.2"><span class="term "><span class="guimenu ">Policy when cluster does not have quorum</span>
    </span></dt><dd><p>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <span class="quote">“<span class="quote ">cluster partition</span>”</span> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes is
      defined to have <span class="quote">“<span class="quote ">quorum</span>”</span>.
     </p><p>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-conf-hawk2-cluster-config" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-conf-hawk2-cluster-config</a>,
      for details.
     </p><p>
      The recommended setting is to choose <span class="guimenu ">Stop</span>. However,
      <span class="guimenu ">Ignore</span> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the other
      node fails. For clusters using shared resources, choosing
      <span class="guimenu ">freeze</span> may be used to ensure that these resources
      continue to be available.
     </p></dd><dt id="vle-pacemaker-barcl-stonith"><span class="term ">STONITH: Configuration mode for STONITH
    </span></dt><dd><p>
      <span class="quote">“<span class="quote ">Misbehaving</span>”</span> nodes in a cluster are shut down to prevent
      them from causing trouble. This mechanism is called STONITH
      (<span class="quote">“<span class="quote ">Shoot the other node in the head</span>”</span>). STONITH can be
      configured in a variety of ways, refer to
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing</a>
      for details. The following configuration options exist:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.8.9.3.2.2.1"><span class="term "><span class="guimenu ">Configured manually</span>
       </span></dt><dd><p>
         STONITH will not be configured when deploying the barclamp. It needs
         to be configured manually as described in
         <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing</a>.
         For experts only.
        </p></dd><dt id="id-1.4.5.4.8.9.3.2.2.2"><span class="term "><span class="guimenu ">Configured with IPMI data from the IPMI barclamp</span>
       </span></dt><dd><p>
         Using this option automatically sets up STONITH with data received
         from the IPMI barclamp. Being able to use this option requires that
         IPMI is configured for all cluster nodes. This should be done by
         default. To check or change the IPMI deployment, go to <span class="guimenu ">Barclamps</span> › <span class="guimenu ">Crowbar</span> › <span class="guimenu ">IPMI</span> › <span class="guimenu ">Edit</span>. Also
         make sure the <span class="guimenu ">Enable BMC</span> option is set to
         <span class="guimenu ">true</span> on this barclamp.
        </p><div id="id-1.4.5.4.8.9.3.2.2.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: STONITH Devices Must Support IPMI</h6><p>
          To configure STONITH with the IPMI data, <span class="emphasis"><em>all</em></span>
          STONITH devices must support IPMI. Problems with this setup may
          occur with IPMI implementations that are not strictly standards
          compliant. In this case it is recommended to set up STONITH with
          STONITH block devices (SBD).
         </p></div></dd><dt id="id-1.4.5.4.8.9.3.2.2.3"><span class="term "><span class="guimenu ">Configured with STONITH Block Devices (SBD)</span>
       </span></dt><dd><p>
         This option requires manually setting up shared storage and a watchdog
         on the cluster nodes before applying the proposal. To do so, proceed
         as follows:
        </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
           Prepare the shared storage. The path to the shared storage device
           must be persistent and consistent across all nodes in the cluster.
           The SBD device must not use host-based RAID or cLVM2.
          </p></li><li class="listitem "><p>
           Install the package <code class="systemitem">sbd</code> on
           all cluster nodes.
          </p></li><li class="listitem "><p>
           Initialize the SBD device with by running the following command.
           Make sure to replace
           <code class="filename">/dev/<em class="replaceable ">SBD</em></code> with the
           path to the shared storage device.
          </p><div class="verbatim-wrap"><pre class="screen">sbd -d /dev/<em class="replaceable ">SBD</em> create</pre></div><p>
           Refer to
           <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-storage-protect-test" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-storage-protect-test</a>
           for details.
          </p></li></ol></div><p>
         In <span class="guimenu ">Kernel module for watchdog</span>, specify the
         respective kernel module to be used. Find the most commonly used
         watchdog drivers in the following table:
        </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><thead><tr><th>Hardware</th><th>Driver</th></tr></thead><tbody><tr><td>HP</td><td><code class="systemitem">hpwdt</code>
            </td></tr><tr><td>Dell, Fujitsu, Lenovo (Intel TCO)</td><td><code class="systemitem">iTCO_wdt</code>
            </td></tr><tr><td>Generic</td><td><code class="systemitem">softdog</code>
            </td></tr></tbody></table></div><p>
         If your hardware is not listed above, either ask your hardware vendor
         for the right name or check the following directory for a list of
         choices:
         <code class="filename">/lib/modules/<em class="replaceable ">KERNEL_VERSION</em>/kernel/drivers/watchdog</code>.
        </p><p>
         Alternatively, list the drivers that have been installed with your
         kernel version:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">rpm</code> -ql kernel-<em class="replaceable ">VERSION</em> | <code class="command">grep</code> watchdog</pre></div><p>
         If the nodes need different watchdog modules, leave the text box
         empty.
        </p><p>
         After the shared storage has been set up, specify the path using the
         <span class="quote">“<span class="quote ">by-id</span>”</span> notation
         (<code class="filename">/dev/disk/by-id/<em class="replaceable ">DEVICE</em></code>).
         It is possible to specify multiple paths as a comma-separated list.
        </p><p>
         Deploying the barclamp will automatically complete the SBD setup on the
         cluster nodes by starting the SBD daemon and configuring the fencing
         resource.
        </p></dd><dt id="id-1.4.5.4.8.9.3.2.2.4"><span class="term "><span class="guimenu ">
         Configured with one shared resource for the whole cluster
        </span>
       </span></dt><dd><p>
         All nodes will use the identical configuration. Specify the
         <span class="guimenu ">Fencing Agent</span> to use and enter
         <span class="guimenu ">Parameters</span> for the agent.
        </p><p>
         To get a list of STONITH devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <code class="command">stonith -L</code>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable ">agent</em> -n</pre></div></dd><dt id="id-1.4.5.4.8.9.3.2.2.5"><span class="term "><span class="guimenu ">Configured with one resource per node</span>
       </span></dt><dd><p>
         All nodes in the cluster use the same <span class="guimenu ">Fencing
         Agent</span>, but can be configured with different parameters. This
         setup is, for example, required when nodes are in different chassis
         and therefore need different IPMI parameters.
        </p><p>
         To get a list of STONITH devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <code class="command">stonith -L</code>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable ">agent</em> -n</pre></div></dd><dt id="id-1.4.5.4.8.9.3.2.2.6"><span class="term "><span class="guimenu ">Configured for nodes running in libvirt</span>
       </span></dt><dd><p>
         Use this setting for completely virtualized test installations. This
         option is not supported.
        </p></dd></dl></div></dd><dt id="var-depl-ostack-pacemaker-corosync-fencing"><span class="term ">STONITH: Do not start corosync on boot after fencing</span></dt><dd><p>
      With STONITH, Pacemaker clusters with two nodes may sometimes hit an
      issue known as STONITH deathmatch where each node kills the other one,
      resulting in both nodes rebooting all the time. Another similar issue in
      Pacemaker clusters is the fencing loop, where a reboot caused by
      STONITH will not be enough to fix a node and it will be fenced again
      and again.
     </p><p>
      This setting can be used to limit these issues. When set to
      <span class="guimenu ">true</span>, a node that has not been properly shut down or
      rebooted will not start the services for Pacemaker on boot. Instead, the
      node will wait for action from the SUSE <span class="productname">OpenStack</span> Cloud operator. When set to
      <span class="guimenu ">false</span>, the services for Pacemaker will always be
      started on boot. The <span class="guimenu ">Automatic</span> value is used to have
      the most appropriate value automatically picked: it will be
      <span class="guimenu ">true</span> for two-node clusters (to avoid STONITH
      deathmatches), and <span class="guimenu ">false</span> otherwise.
     </p><p>
      When a node boots but not starts corosync because of this setting, then
      the node's status is in the <span class="guimenu ">Node Dashboard</span> is set to
      "<code class="literal">Problem</code>" (red dot).
      
     </p></dd><dt id="id-1.4.5.4.8.9.5"><span class="term ">Mail Notifications: Enable Mail Notifications</span></dt><dd><p>
      Get notified of cluster node failures via e-mail. If set to
      <span class="guimenu ">true</span>, you need to specify which <span class="guimenu ">SMTP
      Server</span> to use, a prefix for the mails' subject and sender and
      recipient addresses. Note that the SMTP server must be accessible by the
      cluster nodes.
     </p></dd><dt id="id-1.4.5.4.8.9.6"><span class="term "><span class="guimenu ">HAProxy: Public name for public virtual IP</span>
    </span></dt><dd><p>
      The public name is the host name that will be used instead of the
      generated public name (see
      <a class="xref" href="cha-depl-ostack.html#ann-depl-ostack-pacemaker-prop-name" title="Important: Proposal Name">Important: Proposal Name</a>) for the public
      virtual IP address of HAProxy. (This is the case when registering
      public endpoints, for example). Any name specified here needs to be
      resolved by a name server placed outside of the SUSE <span class="productname">OpenStack</span> Cloud network.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.8.10"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_pacemaker.png" target="_blank"><img src="images/depl_barclamp_pacemaker.png" width="" alt="The Pacemaker Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.1: </span><span class="name">The Pacemaker Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.8.10">#</a></h6></div></div><p>
   The Pacemaker component consists of the following roles. Deploying the
   <span class="guimenu ">hawk-server</span> role is optional:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.8.12.1"><span class="term "><span class="guimenu ">pacemaker-cluster-member</span>
    </span></dt><dd><p>
      Deploy this role on all nodes that should become member of the cluster.
     </p></dd><dt id="id-1.4.5.4.8.12.2"><span class="term "><span class="guimenu ">hawk-server</span>
    </span></dt><dd><p>
      Deploying this role is optional. If deployed, sets up the Hawk Web
      interface which lets you monitor the status of the cluster. The Web
      interface can be accessed via
      <code class="literal">https://<em class="replaceable ">IP-ADDRESS</em>:7630</code>. The
      default hawk credentials are username <code class="literal">hacluster</code>, password
      <code class="literal">crowbar</code>.
      </p><p>
      The password is visible and editable in the <span class="guimenu ">Custom</span> view of the Pacemaker barclamp, and also in the <code class="literal">"corosync":</code> section of the
      <span class="guimenu ">Raw</span> view.
  </p><p>
      Note that the GUI on SUSE <span class="productname">OpenStack</span> Cloud can only be used to monitor the cluster
      status and not to change its configuration.
     </p><p>
      <span class="guimenu ">hawk-server</span> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </p></dd><dt id="id-1.4.5.4.8.12.3"><span class="term "><span class="guimenu ">pacemaker-remote</span>
    </span></dt><dd><p>
      Deploy this role on all nodes that should become members of the
      Compute Nodes cluster. They will run as Pacemaker remote nodes that are
      controlled by the cluster, but do not affect quorum. Instead of the
      complete cluster stack, only the <code class="literal">pacemaker-remote</code>
      component will be installed on this nodes.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.8.13"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_pacemaker_node_deployment.png" target="_blank"><img src="images/depl_barclamp_pacemaker_node_deployment.png" width="" alt="The Pacemaker Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.2: </span><span class="name">The Pacemaker Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.8.13">#</a></h6></div></div><p>
   After a cluster has been successfully deployed, it is listed under
   <span class="guimenu ">Available Clusters</span> in the <span class="guimenu ">Deployment</span>
   section and can be used for role deployment like a regular node.
  </p><div id="id-1.4.5.4.8.15" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Deploying Roles on Single Cluster Nodes</h6><p>
    When using clusters, roles from other barclamps must never be deployed to
    single nodes that are already part of a cluster. The only exceptions from
    this rule are the following roles:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      cinder-volume
     </p></li><li class="listitem "><p>
      swift-proxy + swift-dispersion
     </p></li><li class="listitem "><p>
      swift-ring-compute
     </p></li><li class="listitem "><p>
      swift-storage
     </p></li></ul></div></div><div id="id-1.4.5.4.8.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Service Management on the Cluster</h6><p>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <span class="emphasis"><em>never</em></span> manually start or stop
    an HA-managed service, nor configure it to start on boot. Services may only
    be started or stopped by using the cluster management tools Hawk or the crm
    shell. See
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-config-basics-resources" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-config-basics-resources</a>
    for more information.
   </p></div><div id="id-1.4.5.4.8.17" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Testing the Cluster Setup</h6><p>
    To check whether all cluster resources are running, either use the Hawk
    Web interface or run the command <code class="command">crm_mon</code>
    <code class="option">-1r</code>. If it is not the case, clean up the respective
    resource with <code class="command">crm</code> <code class="option">resource</code>
    <code class="option">cleanup</code> <em class="replaceable ">RESOURCE</em> , so it gets
    respawned.
   </p><p>
    Also make sure that STONITH correctly works before continuing with the
    SUSE <span class="productname">OpenStack</span> Cloud setup. This is especially important when having chosen a STONITH
    configuration requiring manual setup. To test if STONITH works, log in to
    a node on the cluster and run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">pkill -9 corosync</pre></div><p>
    In case STONITH is correctly configured, the node will reboot.
   </p><p>
    Before testing on a production cluster, plan a maintenance window in case
    issues should arise.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-db"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Database</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-db">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-db</li></ul></div></div></div></div><p>
   The very first service that needs to be deployed is the
   <span class="guimenu ">Database</span>. The database component is using MariaDB and
   is used by all other components. It must be installed on a Control Node. The
   Database can be made highly available by deploying it on a cluster.
  </p><p>
   The only attribute you may change is the maximum number of database
   connections (<span class="guimenu ">Global Connection Limit</span>). The default value
   should usually work—only change it for large deployments in case the
   log files show database connection failures.
  </p><div class="figure" id="id-1.4.5.4.9.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_database.png" target="_blank"><img src="images/depl_barclamp_database.png" width="" alt="The Database Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.3: </span><span class="name">The Database Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.9.5">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-db-mariadb"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying MariaDB</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-db-mariadb">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-db-mariadb</li></ul></div></div></div></div><p>Deploying the database requires the use of MariaDB</p><div id="id-1.4.5.4.9.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: MariaDB and HA</h6><p>
        MariaDB back end features full HA support based on the Galera
        clustering technology. The HA setup requires an odd number of
        nodes. The recommended number of nodes is 3.
      </p></div><div class="sect3 " id="sec-depl-ostack-db-mariadb-ssl"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SSL Configuration</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-db-mariadb-ssl">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-db-mariadb-ssl</li></ul></div></div></div></div><p>
        SSL can be enabled with either a stand-alone or cluster deployment.
        The replication traffic between database nodes is not encrypted,
        whilst traffic between the database server(s) and clients are, so
        a separate network for the database servers is recommended.
      </p><p>
        Certificates can be provided, or the barcamp can generate self-signed
        certificates. The certificate filenames are configurable in the
        barclamp, and the directories <code class="literal">/etc/mysql/ssl/certs</code>
        and <code class="literal">/etc/mysql/ssl/private</code> to use the defaults will
        need to be created before the barclamp is applied. The CA certificate
        and the certificate for MariaDB to use both go into
        <code class="literal">/etc/mysql/ssl/certs</code>. The appropriate private key
        for the certificate is placed into the
        <code class="literal">/etc/mysql/ssl/private</code> directory. As long as the
        files are readable when the barclamp is deployed, permissions can be
        tightened after a successful deployment once the appropriate UNIX
        groups exist.
      </p><p>
        The Common Name (CN) for the SSL certificate must be <code class="literal">fully
        qualified server name</code> for single host deployments, and
        cluster-<code class="literal">cluster name</code>.<code class="literal">full domain
        name</code> for cluster deployments.
      </p><div id="id-1.4.5.4.9.6.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Certificate validation errors</h6><p>
          If certificate validation errors are causing issues with deploying
          other barclamps (for example, when creating databases or users) you
          can check the configuration with
          <code class="command">mysql --ssl-verify-server-cert</code> which will perform
          the same verification that Crowbar does when connecting to the
          database server.
        </p></div><p>
        If certificates are supplied, the CA certificate and its full trust
        chain must be in the <code class="literal">ca.pem</code> file. The certificate
        must be trusted by the machine (or all cluster members in a cluster
        deployment), and it must be available on all client machines —
        IE, if the <span class="productname">OpenStack</span> services are deployed on separate machines or
        cluster members they will all require the CA certificate to be in
        <code class="literal">/etc/mysql/ssl/certs</code> as well as trusted by the
        machine.
      </p></div><div class="sect3 " id="id-1.4.5.4.9.6.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MariaDB Configuration Options</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.9.6.5">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="figure" id="id-1.4.5.4.9.6.5.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_database_mariadb.png" target="_blank"><img src="images/depl_barclamp_database_mariadb.png" width="" alt="MariaDB Configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.4: </span><span class="name">MariaDB Configuration </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.9.6.5.2">#</a></h6></div></div><p>
      The following configuration settings are available via the <span class="guimenu ">Database</span> barclamp
      graphical interface:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.9.6.5.4.1"><span class="term ">
          Datadir
        </span></dt><dd><p>
            Path to a directory for storing database data.
          </p></dd><dt id="id-1.4.5.4.9.6.5.4.2"><span class="term ">
          Maximum Number of Simultaneous Connections
        </span></dt><dd><p>
            The maximum number of simultaneous client connections.
          </p></dd><dt id="id-1.4.5.4.9.6.5.4.3"><span class="term ">
          Number of days after the binary logs can be automatically removed
        </span></dt><dd><p>
            A period after which the binary logs are removed.
          </p></dd><dt id="id-1.4.5.4.9.6.5.4.4"><span class="term ">
          Slow Query Logging
        </span></dt><dd><p>
            When enabled, all queries that take longer than usual to execute
            are logged to a separate log file (by default, it's
            <code class="filename">/var/log/mysql/mysql_slow.log</code>). This can be
            useful for debugging.
          </p></dd></dl></div><div id="id-1.4.5.4.9.6.5.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: MariaDB Deployment Restriction</h6><p>
        When MariaDB is used as the database back end, the <span class="guimenu ">monasca-server</span>
        role cannot be deployed to the node with the
        <span class="guimenu ">database-server</span> role. These two roles cannot
        coexist due to the fact that
        monasca uses its own MariaDB instance.
      </p></div></div></div></div><div class="sect1 " id="sec-depl-ostack-rabbit"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying RabbitMQ</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-rabbit">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbit</li></ul></div></div></div></div><p>
   The RabbitMQ messaging system enables services to communicate with the other
   nodes via Advanced Message Queue Protocol (AMQP). Deploying it is mandatory.
   RabbitMQ needs to be installed on a Control Node. RabbitMQ can be made highly
   available by deploying it on a cluster. We recommend not changing the
   default values of the proposal's attributes.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.10.3.1"><span class="term "><span class="guimenu ">Virtual Host</span>
    </span></dt><dd><p>
      Name of the default virtual host to be created and used by the RabbitMQ
      server (<code class="literal">default_vhost</code> configuration option in
      <code class="filename">rabbitmq.config</code>).
     </p></dd><dt id="id-1.4.5.4.10.3.2"><span class="term ">Port</span></dt><dd><p>
      Port the RabbitMQ server listens on (<code class="literal">tcp_listeners</code>
      configuration option in <code class="filename">rabbitmq.config</code>).
     </p></dd><dt id="id-1.4.5.4.10.3.3"><span class="term ">User</span></dt><dd><p>
      RabbitMQ default user (<code class="literal">default_user</code> configuration
      option in <code class="filename">rabbitmq.config</code>).
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.10.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_rabbitmq.png" target="_blank"><img src="images/depl_barclamp_rabbitmq.png" width="" alt="The RabbitMQ Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.5: </span><span class="name">The RabbitMQ Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.10.4">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-rabbit-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for RabbitMQ</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-rabbit-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbit-ha</li></ul></div></div></div></div><p>
    To make RabbitMQ highly available, deploy it on a cluster instead of a
    single Control Node. This also requires shared storage for the cluster that
    hosts the RabbitMQ data. We recommend using a dedicated cluster to deploy
    RabbitMQ together with the database,
    since both components require shared storage.
   </p><p>
    Deploying RabbitMQ on a cluster makes an additional <span class="guimenu ">High
    Availability</span> section available in the
    <span class="guimenu ">Attributes</span> section of the proposal. Configure the
    <span class="guimenu ">Storage Mode</span> in this section.
   </p></div><div class="sect2 " id="sec-depl-ostack-rabbitmq-ssl"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SSL Configuration for RabbitMQ</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-rabbitmq-ssl">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbitmq-ssl</li></ul></div></div></div></div><p>
    The RabbitMQ barclamp supports securing traffic via SSL. This is similar to
    the SSL support in other barclamps, but with these differences:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      RabbitMQ can listen on two ports at the same time, typically port 5672
      for unsecured and port 5671 for secured traffic.
     </p></li><li class="listitem "><p>
      The ceilometer pipeline for <span class="productname">OpenStack</span> swift cannot be passed
      SSL-related parameters. When SSL is enabled for RabbitMQ the ceilometer
      pipeline in swift is turned off, rather than sending it over an
      unsecured channel.
     </p></li></ul></div><p>
The following steps are the fastest way to set up and test a new SSL certificate authority (CA).
</p><div class="procedure " id="pro-rabbitmq-test"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
  In the RabbitMQ barclamp set <span class="guimenu ">Enable SSL</span> to <span class="guimenu ">true</span>, and <span class="guimenu ">Generate (self-signed) certificates (implies insecure)</span>
    to <code class="literal">true</code>, then apply the barclamp. The barclamp will create a new CA, enter the correct settings in <code class="filename">/etc/rabbitmq/rabbitmq.config</code>, and start RabbitMQ.
</p></li><li class="step "><p>
Test your new CA with OpenSSL, substituting the hostname of your control node:</p><div class="verbatim-wrap"><pre class="screen">openssl s_client -connect d52-54-00-59-e5-fd:5671
[...]
Verify return code: 18 (self signed certificate)</pre></div><p>
  This outputs a lot of information, including a copy of the server's public certificate, protocols, ciphers, and the chain of trust.
</p></li><li class="step "><p>
      The last step is to configure client services to use SSL to access the
      RabbitMQ service. (See
      <a class="link" href="https://www.rabbitmq.com/ssl.html" target="_blank">https://www.rabbitmq.com/ssl.html</a> for a complete reference).
     </p></li></ol></div></div><p>
    It is preferable to set up your own CA. The best practice is to use a commercial certificate authority. You may also deploy your own self-signed certificates, provided that your cloud is not publicly-accessible, and only for your internal use. Follow these steps to enable your own CA in RabbitMQ and deploy it to SUSE <span class="productname">OpenStack</span> Cloud:
   </p><div class="procedure " id="pro-rabbitmq-production"><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
      Configure the RabbitMQ barclamp to use the control node's
      certificate authority (CA), if it already has one, or create a CA specifically for RabbitMQ and configure the barclamp to use that. (See <a class="xref" href="cha-depl-req.html#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a>, and the RabbitMQ manual has a detailed howto on creating your CA at <a class="link" href="http://www.rabbitmq.com/ssl.html" target="_blank">http://www.rabbitmq.com/ssl.html</a>, with customizations for .NET and Java clients.)
    </p><div class="figure" id="id-1.4.5.4.10.6.7.1.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/rabbitmq-ssl-1.png" target="_blank"><img src="images/rabbitmq-ssl-1.png" width="" alt="Example RabbitMQ SSL barclamp configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.6: </span><span class="name">SSL Settings for RabbitMQ Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.10.6.7.1.2">#</a></h6></div></div></li></ul></div></div><p>
    The configuration options in the RabbitMQ barclamp allow tailoring the barclamp to your SSL setup.
</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.10.6.9.1"><span class="term "><span class="guimenu ">Enable SSL</span>
    </span></dt><dd><p>
          Set this to <span class="guimenu ">True</span> to expose all of your configuration options.
      </p></dd><dt id="id-1.4.5.4.10.6.9.2"><span class="term "><span class="guimenu ">SSL Port</span>
    </span></dt><dd><p>
      RabbitMQ's SSL listening port. The default is 5671.
      </p></dd><dt id="id-1.4.5.4.10.6.9.3"><span class="term "><span class="guimenu ">Generate (self-signed) certificates (implies insecure)</span>
    </span></dt><dd><p>
    When this is set to <code class="literal">true</code>, self-signed certificates are automatically generated and copied to the correct locations on the control node, and all other barclamp options are set automatically. This is the fastest way to apply and test the barclamp. Do not use this on production systems. When this is set to <code class="literal">false</code> the remaining options are exposed.
      </p></dd><dt id="id-1.4.5.4.10.6.9.4"><span class="term "><span class="guimenu ">SSL Certificate File</span>
    </span></dt><dd><p>
     The location of your public root CA certificate.
      </p></dd><dt id="id-1.4.5.4.10.6.9.5"><span class="term "><span class="guimenu ">SSL (Private) Key File</span>
    </span></dt><dd><p>
     The location of your private server key.
      </p></dd><dt id="id-1.4.5.4.10.6.9.6"><span class="term "><span class="guimenu ">Require Client Certificate</span>
    </span></dt><dd><p>
          This goes with <span class="guimenu ">SSL CA Certificates File</span>. Set to <span class="guimenu ">true</span> to require clients to present SSL certificates to RabbitMQ.
      </p></dd><dt id="id-1.4.5.4.10.6.9.7"><span class="term "><span class="guimenu ">SSL CA Certificates File</span>
    </span></dt><dd><p>
     Trust client certificates presented by the clients that are signed by other CAs. You'll need to store copies of the CA certificates; see "Trust the Client's Root CA" at <a class="link" href="http://www.rabbitmq.com/ssl.html" target="_blank">http://www.rabbitmq.com/ssl.html</a>.
      </p></dd><dt id="id-1.4.5.4.10.6.9.8"><span class="term "><span class="guimenu ">SSL Certificate is insecure (for instance, self-signed)</span>
    </span></dt><dd><p>
      When this is set to <span class="guimenu ">false</span>, clients validate the RabbitMQ server certificate with the <span class="guimenu ">SSL client CA</span> file.
      </p></dd><dt id="id-1.4.5.4.10.6.9.9"><span class="term "><span class="guimenu ">SSL client CA file (used to validate rabbitmq server certificate)</span>
    </span></dt><dd><p>
        Tells clients of RabbitMQ where to find the CA bundle that validates the certificate presented by the RabbitMQ server, when <span class="guimenu ">SSL Certificate is insecure (for instance, self-signed)</span> is set to <span class="guimenu ">false</span>.
      </p></dd></dl></div></div><div class="sect2 " id="sec-depl-ostack-rabbitmq-send-notifications"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Clients to Send Notifications</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-rabbitmq-send-notifications">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-rabbitmq-send-notifications</li></ul></div></div></div></div><p>
    RabbitMQ has an option called <code class="literal">Configure clients to send
    notifications</code>. It defaults to <code class="literal">false</code>, which
    means no events will be sent. It is required to be set to
    <code class="literal">true</code> for ceilometer, monasca, and any other services
    consuming notifications. When it is set to <code class="literal">true</code>,
    <span class="productname">OpenStack</span> services are configured to submit lifecycle audit events to the
    <code class="literal">notification</code> RabbitMQ queue.
   </p><p>
    This option should only be enabled if an active consumer is configured,
    otherwise events will accumulate on the RabbitMQ server, clogging up CPU,
    memory, and disk storage.
   </p><p>
    Any accumulation can be cleared by running:
   </p><div class="verbatim-wrap"><pre class="screen">$ rabbitmqctl -p /openstack purge_queue notifications.info
$ rabbitmqctl -p /openstack purge_queue notifications.error</pre></div></div></div><div class="sect1 " id="sec-depl-ostack-keystone"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying keystone</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-keystone">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone</li></ul></div></div></div></div><p>
   keystone is another core component that is used by all
   other <span class="productname">OpenStack</span> components. It provides authentication and authorization
   services. keystone needs to be installed on a
   Control Node. keystone can be made highly available by deploying it on a
   cluster. You can configure the following parameters of this barclamp:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.11.3.1"><span class="term "><span class="guimenu ">Algorithm for Token Generation</span>
    </span></dt><dd><p>
      Set the algorithm used by keystone to generate the tokens. You can
      choose between <code class="literal">Fernet</code> (the default) or
      <code class="literal">UUID</code>. Note that for performance and security reasons
      it is strongly recommended to use <code class="literal">Fernet</code>.
     </p></dd><dt id="id-1.4.5.4.11.3.2"><span class="term "><span class="guimenu ">Region Name</span>
    </span></dt><dd><p>
      Allows customizing the region name that crowbar is going to manage.
     </p></dd><dt id="id-1.4.5.4.11.3.3"><span class="term "><span class="guimenu ">Default Credentials: Default Tenant</span>
    </span></dt><dd><p>
      Tenant for the users. Do not change the default value of
      <code class="literal">openstack</code>.
     </p></dd><dt id="id-1.4.5.4.11.3.4"><span class="term "><span class="guimenu ">
      Default Credentials: Administrator User Name/Password
     </span>
    </span></dt><dd><p>
      User name and password for the administrator.
     </p></dd><dt id="id-1.4.5.4.11.3.5"><span class="term "><span class="guimenu ">
      Default Credentials: Create Regular User
     </span>
    </span></dt><dd><p>
      Specify whether a regular user should be created automatically. Not
      recommended in most scenarios, especially in an LDAP environment.
     </p></dd><dt id="id-1.4.5.4.11.3.6"><span class="term "><span class="guimenu ">
      Default Credentials: Regular User Username/Password
     </span>
    </span></dt><dd><p>
      User name and password for the regular user. Both the regular user and
      the administrator accounts can be used to log in to the SUSE <span class="productname">OpenStack</span> Cloud Dashboard.
      However, only the administrator can manage keystone users and access.
     </p><div class="figure" id="id-1.4.5.4.11.3.6.2.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_keystone.png" target="_blank"><img src="images/depl_barclamp_keystone.png" width="" alt="The keystone Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.7: </span><span class="name">The keystone Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.11.3.6.2.2">#</a></h6></div></div></dd><dt id="sec-depl-ostack-keystone-ssl"><span class="term ">SSL Support: Protocol
    </span></dt><dd><p>
      When you use the default value <span class="guimenu ">HTTP</span>, public
      communication will not be encrypted. Choose <span class="guimenu ">HTTPS</span> to
      use SSL for encryption. See <a class="xref" href="cha-depl-req.html#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for
      background information and <a class="xref" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ssl" title="11.4.6. Enabling SSL">Section 11.4.6, “Enabling SSL”</a>
      for installation instructions. The following additional configuration
      options will become available when choosing <span class="guimenu ">HTTPS</span>:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.11.3.7.2.2.1"><span class="term "><span class="guimenu ">Generate (self-signed) certificates</span>
       </span></dt><dd><p>
         When set to <code class="literal">true</code>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </p></dd><dt id="id-1.4.5.4.11.3.7.2.2.2"><span class="term "><span class="guimenu ">SSL Certificate File</span> / <span class="guimenu ">SSL (Private) Key
        File</span>
       </span></dt><dd><p>
         Location of the certificate key pair files.
        </p></dd><dt id="id-1.4.5.4.11.3.7.2.2.3"><span class="term "><span class="guimenu ">SSL Certificate is insecure</span>
       </span></dt><dd><p>
         Set this option to <code class="literal">true</code> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </p></dd><dt id="id-1.4.5.4.11.3.7.2.2.4"><span class="term "><span class="guimenu ">SSL CA Certificates File</span>
       </span></dt><dd><p>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the barclamp to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <code class="systemitem">apache2</code> service, and
         re-deploy the barclamp.
        </p><p>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </p><div class="figure" id="id-1.4.5.4.11.3.7.2.2.4.2.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_ssl.png" target="_blank"><img src="images/depl_barclamp_ssl.png" width="" alt="The SSL Dialog" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.8: </span><span class="name">The SSL Dialog </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.11.3.7.2.2.4.2.3">#</a></h6></div></div></dd></dl></div></dd></dl></div><div class="sect2 " id="sec-depl-ostack-keystone-ldap"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authenticating with LDAP</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-keystone-ldap">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone-ldap</li></ul></div></div></div></div><p>
  keystone has the ability to separate
  identity back-ends by domains. SUSE <span class="productname">OpenStack</span> Cloud 9 uses this method for
  authenticating users.
 </p><p>
  The keystone barclamp sets up a MariaDB database by default. Configuring
  an LDAP back-end is done in the <span class="guimenu ">Raw</span> view.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
             Set <span class="guimenu ">"domain_specific_drivers": true,</span>
         </p></li><li class="step "><p>
             Then in the <span class="guimenu ">"domain_specific_config":</span> section
             configure a map with domain names as keys, and configuration as
             values. In the default proposal the domain name key is
             <span class="guimenu ">"ldap_users"</span>, and the keys are the two required
             sections for an LDAP-based identity driver configuration, the <span class="guimenu ">[identity]</span> section which sets the driver, and the <span class="guimenu ">[ldap]</span> section which sets the LDAP connection options. You may configure multiple domains, each with its own configuration.
             </p></li></ol></div></div><p>
     You may make this available to horizon by setting <span class="guimenu ">multi_domain_support</span> to <span class="guimenu ">true</span> in the horizon barclamp.
 </p><p>
Users in the LDAP-backed domain have to know the name of the domain in order to authenticate, and must use the  keystone v3 API endpoint. (See the <span class="productname">OpenStack</span> manuals, <a class="link" href="https://docs.openstack.org/keystone/rocky/admin/identity-domain-specific-config.html" target="_blank">Domain-specific Configuration</a> and <a class="link" href="https://docs.openstack.org/keystone/rocky/admin/identity-integrate-with-ldap.html" target="_blank">Integrate Identity with LDAP</a>, for additional details.)
 </p></div><div class="sect2 " id="sec-depl-ostack-keystone-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for keystone</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-keystone-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone-ha</li></ul></div></div></div></div><p>
    Making keystone highly available requires no special
    configuration—it is sufficient to deploy it on a cluster.
   </p></div><div class="sect2 " id="sec-depl-ostack-keystone-openidc"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenID Connect Setup for keystone</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-keystone-openidc">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone-openidc</li></ul></div></div></div></div><p>
    keystone supports WebSSO by federating with an external identity
    provider (IdP) using  <a class="link" href="https://github.com/zmartzone/mod_auth_openidc" target="_blank">auth_openidc</a> module.
   </p><p>
    There are two steps to enable this feature:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       Configure the <code class="literal">"federation"</code> and <code class="literal">"openidc"</code> attributes for the Keystone Barclamp in Crowbar.
      </p></li><li class="listitem "><p>
       Create the Identity Provider, Protocol, and Mapping resource in
       keystone using <span class="guimenu ">OpenStack Command Line Tool (CLI)</span>.
      </p></li></ol></div><div class="sect3 " id="sec-depl-ostack-keystone-openidc-crowbar"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.5.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">keystone Barclamp Configuration</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-keystone-openidc-crowbar">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone-openidc-crowbar</li></ul></div></div></div></div><p>
    Configurating OpenID Connect is done in the <span class="guimenu ">Raw</span> view,
    under the <span class="guimenu ">federation</span> section. The global attributes,
    namely <span class="guimenu ">trusted_dashboards</span> and <span class="guimenu ">websso_keystone_url</span>
    are not specific to OpenID Connect. Rather, they are designed to help
    facilitate WebSSO browser redirects with external IdPs in a complex cloud
    deployment environment.
   </p><p>
    If the cloud deployment does not have any external proxies or load
    balancers, where the public keystone and horizon (Dashboard service)
    endpoints are directly managed by Crowbar, <span class="guimenu ">trusted_dashboards</span>
    and <span class="guimenu ">websso_keystone_url</span> does not need to be provided.
    However, in a complex cloud deployment where the public Keystone and
    Horizon endpoints are handled by external load balancers or proxies, and
    they are not directly managed by Crowbar, <span class="guimenu ">trusted_dashboards</span>
    and <span class="guimenu ">websso_keystone_url</span> must be provided and they must
    correctly reflect the external public endpoints.
   </p><p>
    To configure OpenID Connect, edit the attributes in the <span class="guimenu ">openidc</span> subsection.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Set <span class="guimenu ">"enabled":true</span>
     </p></li><li class="step "><p>
      Provide the name for the <span class="guimenu ">identity_provider</span>.
      This must be the same as the identity provider to be created in Keystone
      using the <span class="guimenu ">OpenStack Command Line Tool (CLI)</span>.
      For example, if the identity provider is <code class="literal">foo</code>,
      create the identity provider with the name via Openstack CLI (i.e.
      <span class="guimenu ">openstack identity provider create foo</span>).
     </p></li><li class="step "><p>
      <span class="guimenu ">response_type</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCResponseType</a>.
      In most cases, it should be <code class="literal">id_token</code>.
     </p></li><li class="step "><p>
      <span class="guimenu ">scope</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCScope</a>.
     </p></li><li class="step "><p>
      <span class="guimenu ">metadata_url</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCProviderMetadataURL</a>.
     </p></li><li class="step "><p>
      <span class="guimenu ">client_id</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCClientID</a>.
     </p></li><li class="step "><p>
      <span class="guimenu ">client_secret</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCClientSecret</a>.
     </p></li><li class="step "><p>
      <span class="guimenu ">redirect_uri</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCRedirectURI</a>.
      In a cloud deployment where all the external endpoints are directly
      managed by Crowbar, this attribute can be left blank as it will be
      auto-populated by Crowbar. However, in a complex cloud deployment where
      the public Keystone endpoint is handled by an external load balancer
      or proxy, this attribute must reflect the external Keystone auth endpoint
      for the OpenID Connect IdP. For example,
      <code class="literal">"https://keystone-public-endpoint.foo.com/v3/OS-FEDERATION/identity_providers/foo/protocols/openid/auth"</code>
     </p><div id="id-1.4.5.4.11.6.5.5.8.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
       Some OpenID Connect IdPs such as Google require the hostname in the
       <span class="guimenu ">redirect_uri</span> to be a public FQDN. In that case,
       the hostname in Keystone public endpoint must also be a public FQDN
       and must match the one specified in the <span class="guimenu ">redirect_uri</span>.
      </p></div></li></ol></div></div></div><div class="sect3 " id="sec-depl-ostack-keystone-openidc-cli"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.5.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Identity Provider, Protocol, and Mapping</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-keystone-openidc-cli">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-keystone-openidc-cli</li></ul></div></div></div></div><p>
    To fully enable OpenID Connect, the <code class="literal">Identity Provider</code>,
    <code class="literal">Protocol</code>, and <code class="literal">Mapping</code> for the given
    IdP must be created in Keystone. This is done by using the
    <span class="guimenu ">OpenStack Command Line Tool</span>, on a controller node, and
    using the Keystone <span class="bold"><strong>admin</strong></span> credential.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Login to a controller node as <code class="literal">root</code> user.
     </p></li><li class="step "><p>
      Use the Keystone <span class="bold"><strong>admin</strong></span> credential.
     </p><div class="verbatim-wrap"><pre class="screen">source ~/.openrc</pre></div></li><li class="step "><p>
      Create the <span class="bold"><strong>Identity Provider</strong></span>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack identity provider create foo</pre></div><div id="id-1.4.5.4.11.6.6.3.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
       The name of the Identity Provider must be exactly the same as the
        <span class="guimenu ">identity_provider</span> attribute given when configuring
        Keystone in the previous section.
      </p></div></li><li class="step "><p>
      Next, create the Mapping for the Identity Provider. Prior to creating the
      Mapping, one must fully grasp the intricacies
      of <a class="link" href="https://docs.openstack.org/keystone/latest/admin/federation/mapping_combinations.html" target="_blank">Mapping Combinations</a>
      as it may have profound security implications if done incorrectly.
      Here's an example of a mapping file.
     </p><div class="verbatim-wrap"><pre class="screen">[
    {
        "local": [
            {
                "user": {
                    "name": "{0}",
                    "email": "{1}",
                    "type": "ephemeral"
                 },
                 "group": {
                    "domain": {
                        "name": "Default"
                    },
                    "name": "openidc_demo"
                }
             }
         ],
         "remote": [
             {
                 "type": "REMOTE_USER"
             },
             {
                 "type": "HTTP_OIDC_EMAIL"
             }

        ]
    }
]</pre></div><p>
      Once the mapping file is created, now create the mapping resource in Keystone. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack mapping create --rule oidc_mapping.json oidc_mapping</pre></div></li><li class="step "><p>
      Lastly, create the Protocol for the Identity Provider and its mapping.
      For OpenID Connect, the protocol name must be <span class="bold"><strong>openid</strong></span>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack federation protocol create --identity-provider google --mapping oidc_mapping openid</pre></div></li></ol></div></div></div></div></div><div class="sect1 " id="sec-depl-ostack-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying swift (optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-swift">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-swift</li></ul></div></div></div></div><p>
   swift adds an object storage service to SUSE <span class="productname">OpenStack</span> Cloud for storing single
   files such as images or snapshots. It offers high data security by storing
   the data redundantly on a pool of Storage Nodes—therefore swift
   needs to be installed on at least two dedicated nodes.
  </p><p>
   To properly configure swift it is important to understand how it
   places the data. Data is always stored redundantly within the hierarchy. The
   swift hierarchy in SUSE <span class="productname">OpenStack</span> Cloud is formed out of zones, nodes, hard disks,
   and logical partitions. Zones are physically separated clusters, for example
   different server rooms each with its own power supply and network segment. A
   failure of one zone must not affect another zone. The next level in the
   hierarchy are the individual swift storage nodes (on which
   <span class="guimenu ">swift-storage</span> has been deployed), followed by the hard
   disks. Logical partitions come last.
  </p><p>
   swift automatically places three copies of each object on the highest
   hierarchy level possible. If three zones are available, then each copy of
   the object will be placed in a different zone. In a one zone setup with more
   than two nodes, the object copies will each be stored on a different node.
   In a one zone setup with two nodes, the copies will be distributed on
   different hard disks. If no other hierarchy element fits, logical partitions
   are used.
  </p><p>
   The following attributes can be set to configure swift:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.12.6.1"><span class="term "><span class="guimenu ">Allow Public Containers</span>
    </span></dt><dd><p>
      Set to <code class="literal">true</code> to enable public access to containers.
     </p></dd><dt id="id-1.4.5.4.12.6.2"><span class="term "><span class="guimenu ">Enable Object Versioning</span>
    </span></dt><dd><p>
      If set to true, a copy of the current version is archived each time an
      object is updated.
     </p></dd><dt id="id-1.4.5.4.12.6.3"><span class="term "><span class="guimenu ">Zones</span>
    </span></dt><dd><p>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <code class="literal">1</code>.
     </p></dd><dt id="id-1.4.5.4.12.6.4"><span class="term "><span class="guimenu ">Create 2^X Logical Partitions</span>
    </span></dt><dd><p>
      Partition power. The number entered here is used to compute the number of
      logical partitions to be created in the cluster. The number you enter is
      used as a power of 2 (2^X).
     </p><p>
      We recommend using a minimum of 100 partitions per disk. To measure the
      partition power for your setup, multiply the number of disks from all
      swift nodes by 100, and then round up to the nearest power of two.
      Keep in mind that the first disk of each node is not used by
      swift, but rather for the operating system.
     </p><p><span class="formalpara-title">Example: 10 swift nodes with 5 hard disks each. </span>
       Four hard disks on each node are used for swift, so there is a
       total of forty disks. 40 x 100 = 4000. The nearest power of two, 4096,
       equals 2^12. So the partition power that needs to be entered is
       <code class="literal">12</code>.
      </p><div id="id-1.4.5.4.12.6.4.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Value Cannot be Changed After the Proposal Has Been Deployed</h6><p>
       Changing the number of logical partition after swift has been
       deployed is not supported. Therefore the value for the partition power
       should be calculated from the maximum number of partitions this cloud
       installation is likely going to need at any point in time.
      </p></div></dd><dt id="id-1.4.5.4.12.6.5"><span class="term "><span class="guimenu ">Minimum Hours before Partition is reassigned</span>
    </span></dt><dd><p>
      This option sets the number of hours before a logical partition is
      considered for relocation. <code class="literal">24</code> is the recommended
      value.
     </p></dd><dt id="id-1.4.5.4.12.6.6"><span class="term "><span class="guimenu ">Replicas</span>
    </span></dt><dd><p>
      The number of copies generated for each object. The number of replicas
      depends on the number of disks and zones.
     </p></dd><dt id="id-1.4.5.4.12.6.7"><span class="term "><span class="guimenu ">Replication interval (in seconds)</span>
    </span></dt><dd><p>
      Time (in seconds) after which to start a new replication process.
     </p></dd><dt id="id-1.4.5.4.12.6.8"><span class="term "><span class="guimenu ">Debug</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <code class="literal">true</code>.
     </p></dd><dt id="id-1.4.5.4.12.6.9"><span class="term "><span class="guimenu ">SSL Support: Protocol</span>
    </span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If you choose <span class="guimenu ">HTTPS</span>,
      you have two options. You can either <span class="guimenu ">Generate (self-signed)
      certificates</span> or provide the locations for the certificate key
      pair files. Using self-signed certificates is for testing purposes only
      and should never be used in production environments!
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.12.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_swift.png" target="_blank"><img src="images/depl_barclamp_swift.png" width="" alt="The swift Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.9: </span><span class="name">The swift Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.12.7">#</a></h6></div></div><p>
   Apart from the general configuration described above, the swift
   barclamp lets you also activate and configure <span class="guimenu ">Additional
   Middlewares</span>. The features these middlewares provide can be used
   via the swift command line client only. The Ratelimit and S3
   middleware provide for the most interesting features, and we recommend
   enabling other middleware only for specific use-cases.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.12.9.1"><span class="term "><span class="guimenu ">S3 Middleware</span>
    </span></dt><dd><p>
      Provides an S3 compatible API on top of swift.
     </p></dd><dt id="id-1.4.5.4.12.9.2"><span class="term "><span class="guimenu ">StaticWeb</span>
    </span></dt><dd><p>
      Serve container data as a static Web site with an index file and optional
      file listings. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#staticweb" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#staticweb</a>
      for details.
     </p><p>
      This middleware requires setting <span class="guimenu ">Allow Public
      Containers</span> to <code class="literal">true</code>.
     </p></dd><dt id="id-1.4.5.4.12.9.3"><span class="term "><span class="guimenu ">TempURL</span>
    </span></dt><dd><p>
      Create URLs to provide time-limited access to objects. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#tempurl" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#tempurl</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.12.9.4"><span class="term "><span class="guimenu ">FormPOST</span>
    </span></dt><dd><p>
      Upload files to a container via Web form. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#formpost" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#formpost</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.12.9.5"><span class="term "><span class="guimenu ">Bulk</span>
    </span></dt><dd><p>
      Extract TAR archives into a swift account, and delete multiple objects or
      containers with a single request. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.12.9.6"><span class="term "><span class="guimenu ">Cross-domain</span>
    </span></dt><dd><p>
      Interact with the swift API via Flash, Java, and Silverlight from an
      external network. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.12.9.7"><span class="term "><span class="guimenu ">Domain Remap</span>
    </span></dt><dd><p>
      Translates container and account parts of a domain to path parameters
      that the swift proxy server understands. Can be used to create
      short URLs that are easy to remember, for example by rewriting
      <code class="literal">home.tux.example.com/$ROOT/tux/home/myfile</code>
      to <code class="literal">home.tux.example.com/myfile</code>.
      See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.12.9.8"><span class="term "><span class="guimenu ">Ratelimit</span>
    </span></dt><dd><p>
      Throttle resources such as requests per minute to provide denial of
      service protection. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit</a>
      for details.
     </p></dd></dl></div><p>
   The swift component consists of four different roles. Deploying
   <span class="guimenu ">swift-dispersion</span> is optional:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.12.11.1"><span class="term "><span class="guimenu ">swift-storage</span>
    </span></dt><dd><p>
      The virtual object storage service. Install this role on all dedicated
      swift Storage Nodes (at least two), but not on any other node.
     </p><div id="id-1.4.5.4.12.11.1.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: swift-storage Needs Dedicated Machines</h6><p>
       Never install the swift-storage service on a node that runs other
       <span class="productname">OpenStack</span> components.
      </p></div></dd><dt id="id-1.4.5.4.12.11.2"><span class="term "><span class="guimenu ">swift-ring-compute</span>
    </span></dt><dd><p>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index that is used by
      various <span class="productname">OpenStack</span> components to look up the physical location of objects.
      <span class="guimenu ">swift-ring-compute</span> must only be installed on a single
      node, preferably a Control Node.
     </p></dd><dt id="id-1.4.5.4.12.11.3"><span class="term "><span class="guimenu ">swift-proxy</span>
    </span></dt><dd><p>
      The swift proxy server takes care of routing requests to
      swift. Installing a single instance of
      <span class="guimenu ">swift-proxy</span> on a Control Node is recommended. The
      <span class="guimenu ">swift-proxy</span> role can be made highly available by
      deploying it on a cluster.
     </p></dd><dt id="id-1.4.5.4.12.11.4"><span class="term "><span class="guimenu ">swift-dispersion</span>
    </span></dt><dd><p>
      Deploying <span class="guimenu ">swift-dispersion</span> is optional. The
      swift dispersion tools can be used to test the health of the
      cluster. It creates a heap of dummy objects (using 1% of the total space
      available). The state of these objects can be queried using the
      swift-dispersion-report query. <span class="guimenu ">swift-dispersion</span> needs
      to be installed on a Control Node.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.12.12"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_swift_node_deployment.png" target="_blank"><img src="images/depl_barclamp_swift_node_deployment.png" width="" alt="The swift Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.10: </span><span class="name">The swift Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.12.12">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-swift-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for swift</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-swift-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-swift-ha</li></ul></div></div></div></div><p>
    swift replicates by design, so there is no need for a special HA setup.
    Make sure to fulfill the requirements listed in
    <a class="xref" href="cha-depl-req.html#sec-depl-reg-ha-storage-swift" title="2.6.4.1. swift—Avoiding Points of Failure">Section 2.6.4.1, “swift—Avoiding Points of Failure”</a>.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-glance"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying glance</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-glance">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-glance</li></ul></div></div></div></div><p>
   glance provides discovery, registration, and delivery services for virtual
   disk images. An image is needed to start an instance—it is its
   pre-installed root-partition. All images you want to use in your cloud to
   boot instances from, are provided by glance. glance must be deployed onto
   a Control Node. glance can be made highly available by deploying it on a
   cluster.
  </p><p>
   There are a lot of options to configure glance. The most important ones are
   explained below—for a complete reference refer to
   <a class="link" href="https://github.com/crowbar/crowbar-openstack/blob/master/glance.yml" target="_blank">https://github.com/crowbar/crowbar-openstack/blob/master/glance.yml</a>.
  </p><div id="note-glance-api-versions" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: glance API Versions</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7, the glance API v1 is no longer enabled by default.
    Instead, glance API v2 is used by default.
   </p><p>
    If you need to re-enable API v1 for compatibility reasons:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Switch to the <span class="guimenu ">Raw</span> view of the glance barclamp.
     </p></li><li class="listitem "><p>
      Search for the <code class="literal">enable_v1</code> entry and set it to
      <code class="literal">true</code>:
     </p><div class="verbatim-wrap"><pre class="screen">"enable_v1": true</pre></div><p>
      In new installations, this entry is set to <code class="literal">false</code> by
      default. When upgrading from an older version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> it is set
      to <code class="literal">true</code> by default.
     </p></li><li class="listitem "><p>
      Apply your changes.
     </p></li></ol></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.13.5.1"><span class="term "><span class="guimenu ">Image Storage: Default Storage Store</span>
    </span></dt><dd><p><span class="formalpara-title"><span class="guimenu ">File</span>. </span>
       Images are stored in an image file on the Control Node.
      </p><p><span class="formalpara-title"><span class="guimenu ">cinder</span>. </span>
       Provides volume block storage to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Use it to
       store images.
      </p><p><span class="formalpara-title"><span class="guimenu ">swift</span>. </span>
       Provides an object storage service to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">Rados</span>. </span>
       SUSE Enterprise Storage (based on Ceph) provides block storage service to
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">VMware</span>. </span>
       If you are using VMware as a hypervisor, it is recommended to use
       <span class="guimenu ">VMware</span> for storing images. This will make starting
       VMware instances much faster.
      </p><p><span class="formalpara-title"><span class="guimenu ">Expose Backend Store Location</span>. </span>
       If this is set to <span class="guimenu ">true</span>, the API will communicate the
       direct URl of the image's back-end location to HTTP clients. Set to
       <span class="guimenu ">false</span> by default.
      </p><p>
      Depending on the storage back-end, there are additional configuration
      options available:
     </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.13.5.1.2.8"><span class="name"><span class="guimenu ">File Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.13.5.1.2.8">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">File</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.13.5.1.2.10.1"><span class="term "><span class="guimenu ">Image Store Directory</span>
       </span></dt><dd><p>
         Specify the directory to host the image file. The directory specified
         here can also be an NFS share. See
         <a class="xref" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-nfs" title="11.4.3. Mounting NFS Shares on a Node">Section 11.4.3, “Mounting NFS Shares on a Node”</a> for more information.
        </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.13.5.1.2.11"><span class="name"><span class="guimenu ">swift Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.13.5.1.2.11">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">swift</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.13.5.1.2.13.1"><span class="term "><span class="guimenu ">swift Container</span>
       </span></dt><dd><p>
         Set the name of the container to use for the images in swift.
        </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.13.5.1.2.14"><span class="name"><span class="guimenu ">RADOS Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.13.5.1.2.14">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">Rados</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.13.5.1.2.16.1"><span class="term ">RADOS User for CephX Authentication</span></dt><dd><p>
         If you are using an external Ceph cluster, specify the user you have
         set up for glance (see <a class="xref" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for more information).
        </p></dd><dt id="id-1.4.5.4.13.5.1.2.16.2"><span class="term ">RADOS Pool for glance images</span></dt><dd><p>
         If you are using a SUSE <span class="productname">OpenStack</span> Cloud internal Ceph setup, the pool you specify
         here is created if it does not exist. If you are using an external
         Ceph cluster, specify the pool you have set up for glance (see
         <a class="xref" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for more
         information).
        </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.13.5.1.2.17"><span class="name"><span class="guimenu ">VMware Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.13.5.1.2.17">#</a></h5></div><p>
      Only required if <span class="guimenu ">Default Storage Store</span> is set to
      <span class="guimenu ">VMware</span>.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.13.5.1.2.19.1"><span class="term "><span class="guimenu ">vCenter Host/IP Address</span>
       </span></dt><dd><p>
         Name or IP address of the vCenter server.
        </p></dd><dt id="id-1.4.5.4.13.5.1.2.19.2"><span class="term "><span class="guimenu ">vCenter Username</span> / <span class="guimenu ">vCenter
        Password</span>
       </span></dt><dd><p>
         vCenter login credentials.
        </p></dd><dt id="id-1.4.5.4.13.5.1.2.19.3"><span class="term "><span class="guimenu ">Datastores for Storing Images</span>
       </span></dt><dd><p>
         A comma-separated list of datastores specified in the format:
         <em class="replaceable ">DATACENTER_NAME</em>:<em class="replaceable ">DATASTORE_NAME</em>
        </p></dd><dt id="id-1.4.5.4.13.5.1.2.19.4"><span class="term "><span class="guimenu ">
         Path on the datastore, where the glance images will be
         stored
        </span>
       </span></dt><dd><p>
         Specify an absolute path here.
        </p></dd></dl></div></dd><dt id="id-1.4.5.4.13.5.2"><span class="term "><span class="guimenu ">SSL Support: Protocol</span>
    </span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If you choose <span class="guimenu ">HTTPS</span>,
      refer to <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration
      details.
     </p></dd><dt id="id-1.4.5.4.13.5.3"><span class="term "><span class="guimenu ">Caching</span>
    </span></dt><dd><p>
      Enable and configure image caching in this section. By default, image
      caching is disabled. You can see this the Raw view of your nova barclamp:
     </p><div class="verbatim-wrap"><pre class="screen">image_cache_manager_interval = -1</pre></div><p>
      This option sets the number of seconds to wait between runs of the image
      cache manager. Disabling it means that the cache manager will not
      automatically remove the unused images from the cache, so if you have
      many glance images and are running out of storage you must manually
      remove the unused images from the cache. We recommend leaving this option
      disabled as it is known to cause issues, especially with shared storage.
      The cache manager may remove images still in use, e.g. when network
      outages cause synchronization problems with compute nodes.
     </p><p>
      If you wish to enable caching, re-enable it in a custom nova
      configuration file, for example
      <code class="filename">/etc/nova/nova.conf.d/500-nova.conf</code>. This sets the
      interval to four minutes:
     </p><div class="verbatim-wrap"><pre class="screen">image_cache_manager_interval = 2400</pre></div><p>
      See <a class="xref" href="cha-depl-ostack-configs.html" title="Chapter 14. Configuration Files for OpenStack Services">Chapter 14, <em>Configuration Files for <span class="productname">OpenStack</span> Services</em></a> for more information on
      custom configurations.
     </p><p>
      Learn more about glance's caching feature at
      <a class="link" href="http://docs.openstack.org/developer/glance/cache.html" target="_blank">http://docs.openstack.org/developer/glance/cache.html</a>.
     </p></dd><dt id="id-1.4.5.4.13.5.4"><span class="term "><span class="guimenu ">Logging: Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu ">true</span>.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.13.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_glance.png" target="_blank"><img src="images/depl_barclamp_glance.png" width="" alt="The glance Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.11: </span><span class="name">The glance Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.13.6">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-glance-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for glance</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-glance-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-glance-ha</li></ul></div></div></div></div><p>
    glance can be made highly available by deploying it on a cluster. We
    strongly recommended doing this for the image data as well. The recommended
    way is to use swift or an external Ceph cluster for the image
    repository. If you are using a directory on the node instead (file storage
    back-end), you should set up shared storage on the cluster for it.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-cinder"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying cinder</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-cinder">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-cinder</li></ul></div></div></div></div><p>
   cinder, the successor of <code class="literal">nova-volume</code>, provides
   volume block storage.
   It adds persistent storage to an instance that persists until deleted,
   contrary to ephemeral volumes that only persist while the instance is
   running.
  </p><p>
   cinder can provide volume storage by using different back-ends such
   as local file, one or more local disks, Ceph (RADOS), VMware, or network
   storage solutions from EMC, EqualLogic, Fujitsu, NetApp or Pure Storage.
   Since <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 5, cinder supports using several back-ends
   simultaneously. It is also possible to deploy the same network storage
   back-end multiple times and therefore use different installations at the
   same time.
  </p><p>
   The attributes that can be set to configure cinder depend on the
   back-end. The only general option is <span class="guimenu ">SSL Support:
   Protocol</span> (see <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for
   configuration details).
  </p><div id="id-1.4.5.4.14.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Adding or Changing a Back-End</h6><p>
    When first opening the cinder barclamp, the default
    proposal—<span class="guimenu ">Raw Devices</span>—is already available
    for configuration. To optionally add a back-end, go to the section
    <span class="guimenu ">Add New cinder Back-End</span> and choose a <span class="guimenu ">Type Of
    Volume</span> from the drop-down box. Optionally, specify the
    <span class="guimenu ">Name for the Backend</span>. This is recommended when deploying
    the same volume type more than once. Existing back-end configurations
    (including the default one) can be deleted by clicking the trashcan icon if
    no longer needed. Note that you must configure at least one back-end.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.14.6"><span class="name"><span class="guimenu ">Raw devices</span> (local disks)
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.6">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.7.1"><span class="term "><span class="guimenu ">Disk Selection Method</span>
    </span></dt><dd><p>
      Choose whether to use the <span class="guimenu ">First Available</span> disk or
      <span class="guimenu ">All Available</span> disks. <span class="quote">“<span class="quote ">Available disks</span>”</span>
      are all disks currently not used by the system. Note that one disk
      (usually <code class="filename">/dev/sda</code>) of every block storage node is
      already used for the operating system and is not available for
      cinder.
     </p></dd><dt id="id-1.4.5.4.14.7.2"><span class="term "><span class="guimenu ">Name of Volume</span>
    </span></dt><dd><p>
      Specify a name for the cinder volume.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.14.8"><span class="name"><span class="guimenu ">EMC</span> (EMC² Storage)
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.8">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.9.1"><span class="term "><span class="guimenu ">IP address of the ECOM server</span> / <span class="guimenu ">Port of the ECOM server</span>
    </span></dt><dd><p>
      IP address and Port of the ECOM server.
     </p></dd><dt id="id-1.4.5.4.14.9.2"><span class="term "><span class="guimenu ">Username for accessing the ECOM server</span> / <span class="guimenu ">Password for accessing the ECOM server</span>
    </span></dt><dd><p>
      Login credentials for the ECOM server.
     </p></dd><dt id="id-1.4.5.4.14.9.3"><span class="term "><span class="guimenu ">VMAX port groups to expose volumes managed by this backend</span>
    </span></dt><dd><p>
      VMAX port groups that expose volumes managed by this back-end.
     </p></dd><dt id="id-1.4.5.4.14.9.4"><span class="term "><span class="guimenu ">Serial number of the VMAX Array</span>
    </span></dt><dd><p>
      Unique VMAX array serial number.
     </p></dd><dt id="id-1.4.5.4.14.9.5"><span class="term "><span class="guimenu ">Pool name within a given array</span>
    </span></dt><dd><p>
      Unique pool name within a given array.
     </p></dd><dt id="id-1.4.5.4.14.9.6"><span class="term "><span class="guimenu ">FAST Policy name to be used</span>
    </span></dt><dd><p>
      Name of the FAST Policy to be used. When specified, volumes managed by
      this back-end are managed as under FAST control.
     </p></dd></dl></div><p>
   For more information on the EMC driver refer to the <span class="productname">OpenStack</span> documentation
   at
   <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html" target="_blank">http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html</a>.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.14.11"><span class="name"><span class="guimenu ">EqualLogic</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.11">#</a></h4></div><p>
   EqualLogic drivers are included as a technology preview and are not
   supported.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.14.13"><span class="name"><span class="guimenu ">Fujitsu ETERNUS DX</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.13">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.14.1"><span class="term "><span class="guimenu ">Connection Protocol</span>
    </span></dt><dd><p>
      Select the protocol used to connect, either
      <span class="guimenu ">FibreChannel</span> or <span class="guimenu ">iSCSI</span>.
     </p></dd><dt id="id-1.4.5.4.14.14.2"><span class="term "><span class="guimenu ">IP for SMI-S</span> / <span class="guimenu ">Port for SMI-S</span>
    </span></dt><dd><p>
      IP address and port of the ETERNUS SMI-S Server.
     </p></dd><dt id="id-1.4.5.4.14.14.3"><span class="term "><span class="guimenu ">Username for SMI-S</span> / <span class="guimenu ">Password for SMI-S</span>
    </span></dt><dd><p>
      Login credentials for the ETERNUS SMI-S Server.
     </p></dd><dt id="id-1.4.5.4.14.14.4"><span class="term "><span class="guimenu ">Snapshot (Thick/RAID Group) Pool Name</span>
    </span></dt><dd><p>
      Storage pool (RAID group) in which the volumes are created. Make sure
      that the RAID group on the server has already been created. If a RAID
      group that does not exist is specified, the RAID group is built from
      unused disk drives. The RAID level is automatically determined by the
      ETERNUS DX Disk storage system.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.14.15"><span class="name"><span class="guimenu ">Hitachi HUSVM</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.15">#</a></h3></div><p>
   For information on configuring the Hitachi HUSVM back-end, refer to
   <a class="link" href="http://docs.openstack.org/ocata/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html" target="_blank">http://docs.openstack.org/ocata/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html</a>.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.14.17"><span class="name"><span class="guimenu ">NetApp</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.17">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.18.1"><span class="term "><span class="guimenu ">Storage Family Type</span> / <span class="guimenu ">Storage Protocol</span>
    </span></dt><dd><p>
      SUSE <span class="productname">OpenStack</span> Cloud can use <span class="quote">“<span class="quote ">Data ONTAP</span>”</span> in <span class="guimenu ">7-Mode</span>,
      or in <span class="guimenu ">Clustered Mode</span>. In <span class="guimenu ">7-Mode</span>
      vFiler will be configured, in <span class="guimenu ">Clustered Mode</span> vServer
      will be configured. The <span class="guimenu ">Storage Protocol</span> can be set to
      either <span class="guimenu ">iSCSI</span> or <span class="guimenu ">NFS</span>. Choose the
      driver and the protocol your NetApp is licensed for.
     </p></dd><dt id="id-1.4.5.4.14.18.2"><span class="term "><span class="guimenu ">Server host name</span>
    </span></dt><dd><p>
      The management IP address for the 7-Mode storage controller, or the
      cluster management IP address for the clustered Data ONTAP.
     </p></dd><dt id="id-1.4.5.4.14.18.3"><span class="term "><span class="guimenu ">Transport Type</span>
    </span></dt><dd><p>
      Transport protocol for communicating with the storage controller or
      clustered Data ONTAP. Supported protocols are HTTP and HTTPS. Choose the
      protocol your NetApp is licensed for.
     </p></dd><dt id="id-1.4.5.4.14.18.4"><span class="term "><span class="guimenu ">Server port</span>
    </span></dt><dd><p>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </p></dd><dt id="id-1.4.5.4.14.18.5"><span class="term "><span class="guimenu ">Username for accessing NetApp</span> / <span class="guimenu ">Password for Accessing NetApp</span>
    </span></dt><dd><p>
      Login credentials.
     </p></dd><dt id="id-1.4.5.4.14.18.6"><span class="term "><span class="guimenu ">
      The vFiler Unit Name for provisioning <span class="productname">OpenStack</span> volumes (netapp_vfiler)
     </span>
    </span></dt><dd><p>
      The vFiler unit to be used for provisioning of <span class="productname">OpenStack</span> volumes. This
      setting is only available in <span class="guimenu ">7-Mode</span>.
     </p></dd><dt id="id-1.4.5.4.14.18.7"><span class="term "><span class="guimenu ">Restrict provisioning on iSCSI to these volumes (netapp_volume_list)</span>
    </span></dt><dd><p>
      Provide a list of comma-separated volume names to be used for
      provisioning. This setting is only available when using iSCSI as storage
      protocol.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.14.19"><span class="name"><span class="guimenu ">NFS</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.19">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.20.1"><span class="term "><span class="guimenu ">List of NFS Exports</span>
    </span></dt><dd><p>
      A list of available file systems on an NFS server. Enter your NFS mountpoints
      in the <span class="guimenu ">List of NFS Exports</span> form in this format: <em class="replaceable ">host:mountpoint -o options</em>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">host1:/srv/nfs/share1 /mnt/nfs/share1 -o rsize=8192,wsize=8192,timeo=14,intr</pre></div></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.14.21"><span class="name"><span class="guimenu ">Pure Storage (FlashArray)</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.21">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.22.1"><span class="term "><span class="guimenu ">IP address of the management VIP</span>
    </span></dt><dd><p>
      IP address of the FlashArray management VIP
     </p></dd><dt id="id-1.4.5.4.14.22.2"><span class="term "><span class="guimenu ">API token for the FlashArray</span>
    </span></dt><dd><p>
      API token for access to the FlashArray
     </p></dd><dt id="id-1.4.5.4.14.22.3"><span class="term "><span class="guimenu ">iSCSI CHAP authentication enabled</span>
    </span></dt><dd><p>
      Enable or disable iSCSI CHAP authentication
     </p></dd></dl></div><p>
   For more information on the Pure Storage FlashArray driver refer to the <span class="productname">OpenStack</span> documentation
   at
   <a class="link" href="https://docs.openstack.org/ocata/config-reference/block-storage/drivers/pure-storage-driver.html" target="_blank">https://docs.openstack.org/ocata/config-reference/block-storage/drivers/pure-storage-driver.html</a>.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.14.24"><span class="name"><span class="guimenu ">RADOS</span> (Ceph)
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.24">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.25.1"><span class="term "><span class="guimenu ">Use Ceph Deployed by Crowbar</span>
    </span></dt><dd><p>
      Select <span class="guimenu ">false</span>, if you are using an external Ceph cluster (see
      <a class="xref" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for setup
      instructions).
     </p></dd><dt id="id-1.4.5.4.14.25.2"><span class="term "><span class="guimenu ">RADOS pool for cinder volumes</span>
    </span></dt><dd><p>
      Name of the pool used to store the cinder volumes.
     </p></dd><dt id="id-1.4.5.4.14.25.3"><span class="term "><span class="guimenu ">
      RADOS user (Set Only if Using CephX authentication)
     </span>
    </span></dt><dd><p>
      Ceph user name.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.14.26"><span class="name"><span class="guimenu ">VMware Parameters</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.26">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.27.1"><span class="term "><span class="guimenu ">vCenter Host/IP Address</span>
    </span></dt><dd><p>
      Host name or IP address of the vCenter server.
     </p></dd><dt id="id-1.4.5.4.14.27.2"><span class="term "><span class="guimenu ">vCenter Username</span> / <span class="guimenu ">vCenter
     Password</span>
    </span></dt><dd><p>
      vCenter login credentials.
     </p></dd><dt id="id-1.4.5.4.14.27.3"><span class="term "><span class="guimenu ">vCenter Cluster Names for Volumes</span>
    </span></dt><dd><p>
      Provide a comma-separated list of cluster names.
     </p></dd><dt id="id-1.4.5.4.14.27.4"><span class="term "><span class="guimenu ">Folder for Volumes</span>
    </span></dt><dd><p>
      Path to the directory used to store the cinder volumes.
     </p></dd><dt id="id-1.4.5.4.14.27.5"><span class="term "><span class="guimenu ">CA file for verifying the vCenter certificate</span>
    </span></dt><dd><p>
      Absolute path to the vCenter CA certificate.
     </p></dd><dt id="id-1.4.5.4.14.27.6"><span class="term "><span class="guimenu ">
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </span>
    </span></dt><dd><p>
      Default value: <code class="literal">false</code> (the CA truststore is used for
      verification). Set this option to <code class="literal">true</code> when using
      self-signed certificates to disable certificate checks. This setting is
      for testing purposes only and must not be used in production
      environments!
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.14.28"><span class="name"><span class="guimenu ">Local file</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.28">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.29.1"><span class="term "><span class="guimenu ">Volume File Name</span>
    </span></dt><dd><p>
      Absolute path to the file to be used for block storage.
     </p></dd><dt id="id-1.4.5.4.14.29.2"><span class="term "><span class="guimenu ">Maximum File Size (GB)</span>
    </span></dt><dd><p>
      Maximum size of the volume file. Make sure not to overcommit the size,
      since it will result in data loss.
     </p></dd><dt id="id-1.4.5.4.14.29.3"><span class="term "><span class="guimenu ">Name of Volume</span>
    </span></dt><dd><p>
      Specify a name for the cinder volume.
     </p></dd></dl></div><div id="id-1.4.5.4.14.30" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Using <span class="guimenu ">Local File</span> for Block Storage</h6><p>
    Using a file for block storage is not recommended for production systems,
    because of performance and data security reasons.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.14.31"><span class="name"><span class="guimenu ">Other driver</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.31">#</a></h4></div><p>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, as it is not supported.
  </p><div class="figure" id="id-1.4.5.4.14.33"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_cinder.png" target="_blank"><img src="images/depl_barclamp_cinder.png" width="" alt="The cinder Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.12: </span><span class="name">The cinder Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.33">#</a></h6></div></div><p>
   The cinder component consists of two different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.14.35.1"><span class="term "><span class="guimenu ">cinder-controller</span>
    </span></dt><dd><p>
      The cinder controller provides the scheduler and the API.
      Installing <span class="guimenu ">cinder-controller</span> on a Control Node is
      recommended.
     </p></dd><dt id="id-1.4.5.4.14.35.2"><span class="term "><span class="guimenu ">cinder-volume</span>
    </span></dt><dd><p>
      The virtual block storage service. It can be installed on a Control Node.
      However, we recommend deploying it on one or more dedicated nodes
      supplied with sufficient networking capacity to handle the increase in
      network traffic.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.14.36"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_cinder_node_deployment.png" target="_blank"><img src="images/depl_barclamp_cinder_node_deployment.png" width="" alt="The cinder Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.13: </span><span class="name">The cinder Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.14.36">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-cinder-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for cinder</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-cinder-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-cinder-ha</li></ul></div></div></div></div><p>
    Both the <span class="guimenu ">cinder-controller</span> and the
    <span class="guimenu ">cinder-volume</span> role can be deployed on a cluster.
   </p><div id="id-1.4.5.4.14.37.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Moving <span class="guimenu ">cinder-volume</span> to a Cluster</h6><p>
     If you need to re-deploy <span class="guimenu ">cinder-volume</span> role from a
     single machine to a cluster environment, the following will happen:
     Volumes that are currently attached to instances will continue to work,
     but adding volumes to instances will not succeed.
    </p><p>
     To solve this issue, run the following script once on each node that
     belongs to the <span class="guimenu ">cinder-volume</span> cluster:
     <code class="filename">/usr/bin/cinder-migrate-volume-names-to-cluster</code>.
    </p><p>
     The script is automatically installed by Crowbar on every machine or
     cluster that has a <span class="guimenu ">cinder-volume</span> role applied to it.
    </p></div><p>
    In combination with Ceph or a network storage solution, deploying
    cinder in a cluster minimizes the potential downtime. For
    <span class="guimenu ">cinder-volume</span> to be applicable to a cluster, the role
    needs all cinder backends to be configured for non-local
    storage. If you are using local volumes or raw devices in any of your
    volume backends, you cannot apply <span class="guimenu ">cinder-volume</span> to a
    cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-neutron"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying neutron</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-neutron">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-neutron</li></ul></div></div></div></div><p>
   neutron provides network connectivity between interface devices managed by
   other <span class="productname">OpenStack</span> components (most likely nova). The service works by
   enabling users to create their own networks and then attach interfaces to
   them.
  </p><p>
   neutron must be deployed on a Control Node. You first need to choose a core
   plug-in—<span class="guimenu ">ml2</span> or <span class="guimenu ">vmware</span>. Depending
   on your choice, more configuration options will become available.
  </p><p>
   The <span class="guimenu ">vmware</span> option lets you use an existing VMware NSX
   installation. Using this plugin is not a prerequisite for the VMware vSphere
   hypervisor support. However, it is needed when wanting to have security
   groups supported on VMware compute nodes. For all other scenarios, choose
   <span class="guimenu ">ml2</span>.
  </p><p>
   The only global option that can be configured is <span class="guimenu ">SSL
   Support</span>. Choose whether to encrypt public communication
   (<span class="guimenu ">HTTPS</span>) or not (<span class="guimenu ">HTTP</span>). If choosing
   <span class="guimenu ">HTTPS</span>, refer to
   <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.6"><span class="name"><span class="guimenu ">ml2</span> (Modular Layer 2)
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.15.6">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.15.7.1"><span class="term "><span class="guimenu ">Modular Layer 2 Mechanism Drivers</span>
    </span></dt><dd><p>
      Select which mechanism driver(s) shall be enabled for the ml2 plugin. It
      is possible to select more than one driver by holding the
      <span class="keycap">Ctrl</span> key while clicking. Choices are:
     </p><p><span class="formalpara-title"><span class="guimenu ">openvswitch</span>. </span>
       Supports GRE, VLAN and VXLAN networks (to be configured via the
       <span class="guimenu ">Modular Layer 2 type drivers</span> setting). VXLAN is the
       default.
      </p><p><span class="formalpara-title"><span class="guimenu ">linuxbridge</span>. </span>
       Supports VLANs only. Requires to specify the <span class="guimenu ">Maximum Number of
       VLANs</span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">cisco_nexus</span>. </span>
       Enables neutron to dynamically adjust the VLAN settings of the ports of
       an existing Cisco Nexus switch when instances are launched. It also
       requires <span class="guimenu ">openvswitch</span> which will automatically be
       selected. With <span class="guimenu ">Modular Layer 2 type drivers</span>,
       <span class="guimenu ">vlan</span> must be added. This option also requires to
       specify the <span class="guimenu ">Cisco Switch Credentials</span>. See
       <a class="xref" href="app-deploy-cisco.html" title="Appendix A. Using Cisco Nexus Switches with neutron">Appendix A, <em>Using Cisco Nexus Switches with neutron</em></a> for details.
      </p><p><span class="formalpara-title"><span class="guimenu ">vmware_dvs</span>. </span>
       vmware_dvs driver makes it possible to use neutron for networking in a
       VMware-based environment. Choosing <span class="guimenu ">vmware_dvs</span>,
       automatically selects the required <span class="guimenu ">openswitch</span>, <span class="guimenu ">vxlan</span>, and
       <span class="guimenu ">vlan</span> drivers. In the <span class="guimenu ">Raw</span> view,
       it is also possible to configure two additional attributes:
       <span class="guimenu ">clean_on_start</span> (clean up the DVS portgroups on the
       target vCenter Servers when neutron-server is restarted) and
       <span class="guimenu ">precreate_networks</span> (create DVS portgroups
       corresponding to networks in advance, rather than when virtual machines are attached to these networks).
      </p></dd><dt id="id-1.4.5.4.15.7.2"><span class="term "><span class="guimenu ">Use Distributed Virtual Router Setup</span>
    </span></dt><dd><p>
      With the default setup, all intra-Compute Node traffic flows through the
      network Control Node. The same is true for all traffic from floating IPs.
      In large deployments the network Control Node can therefore quickly become
      a bottleneck. When this option is set to <span class="guimenu ">true</span>, network
      agents will be installed on all compute nodes. This will de-centralize
      the network traffic, since Compute Nodes will be able to directly
      <span class="quote">“<span class="quote ">talk</span>”</span> to each other. Distributed Virtual Routers (DVR)
      require the <span class="guimenu ">openvswitch</span> driver and will not work with
      the <span class="guimenu ">linuxbridge</span> driver. For details on DVR refer to
      <a class="link" href="https://wiki.openstack.org/wiki/Neutron/DVR" target="_blank">https://wiki.openstack.org/wiki/Neutron/DVR</a>.
     </p></dd><dt id="id-1.4.5.4.15.7.3"><span class="term "><span class="guimenu ">Modular Layer 2 Type Drivers</span>
    </span></dt><dd><p>
      This option is only available when having chosen the
      <span class="guimenu ">openvswitch</span> or the <span class="guimenu ">cisco_nexus</span>
      mechanism drivers. Options are <span class="guimenu ">vlan</span>,
      <span class="guimenu ">gre</span> and <span class="guimenu ">vxlan</span>. It is possible to
      select more than one driver by holding the <span class="keycap">Ctrl</span>
      key while clicking.
     </p><p>
      When multiple type drivers are enabled, you need to select the
      <span class="guimenu ">Default Type Driver for Provider Network</span>, that will be
      used for newly created provider networks. This also includes the
      <code class="literal">nova_fixed</code> network, that will be created when applying
      the neutron proposal. When manually creating provider networks with the
      <code class="command">neutron</code> command, the default can be overwritten with
      the <code class="option">--provider:network_type
      <em class="replaceable ">type</em></code> switch. You will also need to
      set a <span class="guimenu ">Default Type Driver for Tenant Network</span>. It is
      not possible to change this default when manually creating tenant
      networks with the <code class="command">neutron</code> command. The non-default
      type driver will only be used as a fallback.
     </p><p>
      Depending on your choice of the type driver, more configuration options
      become available.
     </p><p><span class="formalpara-title"><span class="guimenu ">gre</span>. </span>
       Having chosen <span class="guimenu ">gre</span>, you also need to specify the start
       and end of the tunnel ID range.
      </p><p><span class="formalpara-title"><span class="guimenu ">vlan</span>. </span>
       The option <span class="guimenu ">vlan</span> requires you to specify the
       <span class="guimenu ">Maximum number of VLANs</span>.
      </p><p><span class="formalpara-title"><span class="guimenu ">vxlan</span>. </span>
       Having chosen <span class="guimenu ">vxlan</span>, you also need to specify the
       start and end of the VNI range.
      </p></dd></dl></div><div id="id-1.4.5.4.15.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Drivers for the VMware Compute Node</h6><p>
    neutron must not be deployed with the <code class="literal">openvswitch with
    gre</code> plug-in.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.9"><span class="name"><span class="guimenu ">z/VM Configuration</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.15.9">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.15.10.1"><span class="term ">xCAT Host/IP Address</span></dt><dd><p>
      Host name or IP address of the xCAT Management Node.
      
     </p></dd><dt id="id-1.4.5.4.15.10.2"><span class="term ">xCAT Username/Password</span></dt><dd><p>
      xCAT login credentials.
     </p></dd><dt id="id-1.4.5.4.15.10.3"><span class="term ">rdev list for physnet1 vswitch uplink (if available)</span></dt><dd><p>
      List of rdev addresses that should be connected to this vswitch.
     </p></dd><dt id="id-1.4.5.4.15.10.4"><span class="term ">xCAT IP Address on Management Network</span></dt><dd><p>
      IP address of the xCAT management interface.
     </p></dd><dt id="id-1.4.5.4.15.10.5"><span class="term ">Net Mask of Management Network</span></dt><dd><p>
      Net mask of the xCAT management interface.
      
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.11"><span class="name"><span class="guimenu ">vmware</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.15.11">#</a></h3></div><p>
   This plug-in requires to configure access to the VMware NSX service.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.15.13.1"><span class="term "><span class="guimenu ">VMware NSX User Name/Password</span>
    </span></dt><dd><p>
      Login credentials for the VMware NSX server. The user needs to have
      administrator permissions on the NSX server.
     </p></dd><dt id="id-1.4.5.4.15.13.2"><span class="term "><span class="guimenu ">VMware NSX Controllers</span>
    </span></dt><dd><p>
      Enter the IP address and the port number
      (<em class="replaceable ">IP-ADDRESS</em>:<em class="replaceable ">PORT</em>)
      of the controller API endpoint. If the port number is omitted, port 443
      will be used. You may also enter multiple API endpoints
      (comma-separated), provided they all belong to the same controller
      cluster. When multiple API endpoints are specified, the plugin will load
      balance requests on the various API endpoints.
     </p></dd><dt id="id-1.4.5.4.15.13.3"><span class="term "><span class="guimenu ">UUID of the NSX Transport Zone/Gateway Service</span>
    </span></dt><dd><p>
      The UUIDs for the transport zone and the gateway service can be obtained
      from the NSX server. They will be used when networks are created.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.15.14"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_network.png" target="_blank"><img src="images/depl_barclamp_network.png" width="" alt="The neutron Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.14: </span><span class="name">The neutron Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.15.14">#</a></h6></div></div><p>
   The neutron component consists of two different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.15.16.1"><span class="term "><span class="guimenu ">neutron-server</span>
    </span></dt><dd><p>
      <span class="guimenu ">neutron-server</span> provides the scheduler and the API. It
      needs to be installed on a Control Node.
     </p></dd><dt id="id-1.4.5.4.15.16.2"><span class="term "><span class="guimenu ">neutron-network</span>
    </span></dt><dd><p>
      This service runs the various agents that manage the network traffic of
      all the cloud instances. It acts as the DHCP and DNS server and as a
      gateway for all cloud instances. It is recommend to deploy this role on a
      dedicated node supplied with sufficient network capacity.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.15.17"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_neutron_node_deployment.png" target="_blank"><img src="images/depl_barclamp_neutron_node_deployment.png" width="" alt="The neutron barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.15: </span><span class="name">The neutron barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.15.17">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-network-infoblox"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Infoblox IPAM Plug-in</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-network-infoblox">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-network-infoblox</li></ul></div></div></div></div><p>
    In the neutron barclamp, you can enable support for the infoblox IPAM
    plug-in and configure it. For configuration, the
    <code class="literal">infoblox</code> section contains the subsections
    <code class="literal">grids</code> and <code class="literal">grid_defaults</code>.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.15.18.3.1"><span class="term ">grids</span></dt><dd><p>
       This subsection must contain at least one entry. For each entry, the
       following parameters are required:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         admin_user_name
        </p></li><li class="listitem "><p>
         admin_password
        </p></li><li class="listitem "><p>
         grid_master_host
        </p></li><li class="listitem "><p>
         grid_master_name
        </p></li><li class="listitem "><p>
         data_center_name
        </p></li></ul></div><p>
       You can also add multiple entries to the <code class="literal">grids</code>
       section. However, the upstream infoblox agent only supports a single
       grid currently.
      </p></dd><dt id="id-1.4.5.4.15.18.3.2"><span class="term ">grid_defaults</span></dt><dd><p>
       This subsection contains the default settings that are used for each
       grid (unless you have configured specific settings within the
       <code class="literal">grids</code> section).
      </p></dd></dl></div><p>
    For detailed information on all infoblox-related configuration settings,
    see
    <a class="link" href="https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst" target="_blank">https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst</a>.
   </p><p>
    Currently, all configuration options for infoblox are only available in the
    <code class="literal">raw</code> mode of the neutron barclamp. To enable support for
    the infoblox IPAM plug-in and configure it, proceed as follows:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      <span class="guimenu ">Edit</span> the neutron barclamp proposal or create a new
      one.
     </p></li><li class="step "><p>
      Click <span class="guimenu ">Raw</span> and search for the following section:
     </p><div class="verbatim-wrap"><pre class="screen">"use_infoblox": false,</pre></div></li><li class="step "><p>
      To enable support for the infoblox IPAM plug-in, change this entry to:
     </p><div class="verbatim-wrap"><pre class="screen">"use_infoblox": true,</pre></div></li><li class="step "><p>
      In the <code class="literal">grids</code> section, configure at least one grid by
      replacing the example values for each parameter with real values.
     </p></li><li class="step "><p>
      If you need specific settings for a grid, add some of the parameters from
      the <code class="literal">grid_defaults</code> section to the respective grid entry
      and adjust their values.
     </p><p>
      Otherwise Crowbar applies the default setting to each grid when you save
      the barclamp proposal.
     </p></li><li class="step "><p>
      Save your changes and apply them.
     </p></li></ol></div></div></div><div class="sect2 " id="sec-depl-ostack-network-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for neutron</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-network-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-network-ha</li></ul></div></div></div></div><p>
    neutron can be made highly available by deploying
    <span class="guimenu ">neutron-server</span> and <span class="guimenu ">neutron-network</span> on
    a cluster. While <span class="guimenu ">neutron-server</span> may be deployed on a
    cluster shared with other services, it is strongly recommended to use a
    dedicated cluster solely for the <span class="guimenu ">neutron-network</span> role.
   </p></div><div class="sect2 " id="sec-setup-multi-ext-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up Multiple External Networks</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-setup-multi-ext-networks">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-setup-multi-ext-networks</li></ul></div></div></div></div><p>
   This section shows you how to create external networks on SUSE <span class="productname">OpenStack</span> Cloud.
  </p><div class="sect3 " id="sec-config-multi-ext-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Network Configurations</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-config-multi-ext-networks">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-config-multi-ext-networks</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      If you have not yet deployed Crowbar, add the following configuration to
      <code class="filename">/etc/crowbar/network.json</code>
      to set up an external network, using the name of your new network, VLAN
      ID, and network addresses. If you have already deployed Crowbar, then add
      this configuration to the <span class="guimenu ">Raw</span> view of the Network Barclamp.
     </p><div class="verbatim-wrap"><pre class="screen">"<em class="replaceable ">public2</em>": {
          "conduit": "intf1",
          "vlan": <em class="replaceable ">600</em>,
          "use_vlan": true,
          "add_bridge": false,
          "subnet": "<em class="replaceable ">192.168.135.128</em>",
          "netmask": "<em class="replaceable ">255.255.255.128</em>",
          "broadcast": "<em class="replaceable ">192.168.135.255</em>",
          "ranges": {
            "host": { "start": "<em class="replaceable ">192.168.135.129</em>",
               "end": "<em class="replaceable ">192.168.135.254</em>" }
          }
    },</pre></div></li><li class="step "><p>
      Modify the <em class="parameter ">additional_external_networks</em> in the
      <span class="guimenu ">Raw</span> view of the neutron Barclamp with the name of your
      new external network.
     </p></li><li class="step "><p>
       Apply both barclamps, and it may also be necessary to re-apply the nova
       Barclamp.
   </p></li><li class="step "><p>
      Then follow the steps in the next section to create the new external network.
    </p></li></ol></div></div></div><div class="sect3 " id="sec-confignet"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.9.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create the New External Network</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-confignet">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-confignet</li></ul></div></div></div></div><p>
    The following steps add the network settings, including IP address pools,
    gateway, routing, and virtual switches to your new network.
   </p><div class="procedure " id="pro-confignet"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Set up interface mapping using either Open vSwitch (OVS) or Linuxbridge.
      For Open vSwitch run the following command:
     </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable ">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable ">public2</em> --router:external=True</pre></div><p>
      For Linuxbridge run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"> openstack network create --router:external True --provider:physical_network physnet1 \
 --provider:network_type vlan --provider:segmentation_id <em class="replaceable ">600</em></pre></div></li><li class="step "><p>
      If a different network is used then Crowbar will create a new interface
      mapping. Then you can use a flat network:
     </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable ">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable ">public2</em> --router:external=True</pre></div></li><li class="step "><p>
      Create a subnet:
     </p><div class="verbatim-wrap"><pre class="screen">openstack subnet create --name <em class="replaceable ">public2</em> --allocation-pool \
 start=<em class="replaceable ">192.168.135.2</em>,end=<em class="replaceable ">192.168.135.127</em> --gateway <em class="replaceable ">192.168.135.1</em> <em class="replaceable ">public2</em> \
 <em class="replaceable ">192.168.135.0/24</em> --enable_dhcp False</pre></div></li><li class="step "><p>
      Create a router, <em class="replaceable ">router2</em>:
     </p><div class="verbatim-wrap"><pre class="screen">openstack router create <em class="replaceable ">router2</em></pre></div></li><li class="step "><p>
      Connect <em class="replaceable ">router2</em> to the new external network:
     </p><div class="verbatim-wrap"><pre class="screen">openstack router set <em class="replaceable ">router2</em>  <em class="replaceable ">public2</em></pre></div></li><li class="step "><p>
      Create a new private network and connect it to
      <em class="replaceable ">router2</em>
     </p><div class="verbatim-wrap"><pre class="screen">openstack network create priv-net
openstack subnet create priv-net --gateway <em class="replaceable ">10.10.10.1 10.10.10.0/24</em> \
 --name priv-net-sub
openstack router add subnet <em class="replaceable ">router2</em> priv-net-sub</pre></div></li><li class="step "><p>
      Boot a VM on priv-net-sub and set a security group that allows SSH.
     </p></li><li class="step "><p>
      Assign a floating IP address to the VM, this time from network
      <em class="replaceable ">public2</em>.
     </p></li><li class="step "><p>
      From the node verify that SSH is working by opening an SSH session to the
      VM.
     </p></li></ol></div></div></div><div class="sect3 " id="sec-howbridges"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.9.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How the Network Bridges are Created</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-howbridges">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-howbridges</li></ul></div></div></div></div><p>
    For OVS, a new bridge will be created by Crowbar, in this case
    <code class="literal">br-public2</code>. In the bridge mapping the new network will
    be assigned to the bridge. The interface specified in
    <code class="filename">/etc/crowbar/network.json</code> (in this case eth0.600) will
    be plugged into <code class="literal">br-public2</code>. The new public network can
    be created in neutron using the new public network name as
    <em class="parameter ">provider:physical_network</em>.
   </p><p>
    For Linuxbridge, Crowbar will check the interface associated with
    <em class="replaceable ">public2</em>. If this is the same as physnet1 no
    interface mapping will be created. The new public network can be created in
    neutron using physnet1 as physical network and specifying the correct VLAN
    ID:
   </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable ">public2</em> --router:external True \
 --provider:physical_network physnet1 --provider:network_type vlan \
 --provider:segmentation_id <em class="replaceable ">600</em></pre></div><p>
    A bridge named <code class="varname">brq-NET_ID</code> will be created and the
    interface specified in <code class="filename">/etc/crowbar/network.json</code> will
    be plugged into it. If a new interface is associated in
    <code class="filename">/etc/crowbar/network.json</code> with
    <em class="replaceable ">public2</em> then Crowbar will add a new interface
    mapping and the second public network can be created using
    <em class="replaceable ">public2</em> as the physical network:
   </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable ">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable ">public2</em> --router:external=True</pre></div></div></div></div><div class="sect1 " id="sec-depl-ostack-nova"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying nova</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-nova">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-nova</li></ul></div></div></div></div><p>
   nova provides key services for managing the SUSE <span class="productname">OpenStack</span> Cloud, sets up the
   Compute Nodes. SUSE <span class="productname">OpenStack</span> Cloud currently supports KVM and VMware vSphere. The
   unsupported QEMU option is included to enable test setups with virtualized
   nodes. The following attributes can be configured for nova:
   
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.16.3.1"><span class="term "><span class="guimenu ">
      Scheduler Options: Virtual RAM to Physical RAM allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote ">overcommit ratio</span>”</span> for RAM for instances on the
      Compute Nodes. A ratio of <code class="literal">1.0</code> means no overcommitment.
      Changing this value is not recommended.
     </p></dd><dt id="id-1.4.5.4.16.3.2"><span class="term "><span class="guimenu ">
      Scheduler Options: Virtual CPU to Physical CPU allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote ">overcommit ratio</span>”</span> for CPUs for instances on the
      Compute Nodes. A ratio of <code class="literal">1.0</code> means no overcommitment.
     </p></dd><dt id="id-1.4.5.4.16.3.3"><span class="term "><span class="guimenu ">
      Scheduler Options: Virtual Disk to Physical Disk allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote ">overcommit ratio</span>”</span> for virtual disks for instances
      on the Compute Nodes. A ratio of <code class="literal">1.0</code> means no
      overcommitment.
     </p></dd><dt id="id-1.4.5.4.16.3.4"><span class="term "><span class="guimenu ">
      Scheduler Options: Reserved Memory for nova-compute hosts (MB)
     </span>
    </span></dt><dd><p>
      Amount of reserved host memory that is not used for allocating VMs by
      <code class="literal">nova-compute</code>.
     </p></dd><dt id="id-1.4.5.4.16.3.5"><span class="term "><span class="guimenu ">Live Migration Support: Enable Libvirt Migration</span>
    </span></dt><dd><p>
      Allows to move KVM instances to a different Compute Node
      running the same hypervisor (cross hypervisor migrations are not
      supported). Useful when a Compute Node needs to be shut down or rebooted
      for maintenance or when the load of the Compute Node is very high.
      Instances can be moved while running (Live Migration).
     </p><div id="id-1.4.5.4.16.3.5.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Libvirt Migration and Security</h6><p>
       Enabling the libvirt migration option will open a TCP port on the
       Compute Nodes that allows access to all instances from all machines in
       the admin network. Ensure that only authorized machines have access to
       the admin network when enabling this option.
      </p></div><div id="id-1.4.5.4.16.3.5.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Specifying Network for Live Migration</h6><p>
        It is possible to change a network to live migrate images. This is done
        in the raw view of the nova barclamp. In the
        <code class="literal">migration</code> section, change the
        <code class="varname">network</code> attribute to the appropriate value (for
        example, <code class="literal">storage</code> for Ceph).
      </p></div></dd><dt id="id-1.4.5.4.16.3.6"><span class="term "><span class="guimenu ">KVM Options: Enable Kernel Samepage Merging</span>
    </span></dt><dd><p>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the Compute Nodes when using
      the KVM hypervisor at the cost of slightly increasing CPU usage.
     </p></dd><dt id="id-1.4.5.4.16.3.7"><span class="term ">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If choosing
      <span class="guimenu ">HTTPS</span>,refer to
      <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
     </p></dd><dt id="id-1.4.5.4.16.3.8"><span class="term ">VNC Settings: NoVNC Protocol</span></dt><dd><p>
      After having started an instance you can display its VNC console in the
      <span class="productname">OpenStack</span> Dashboard (horizon) via the browser using the noVNC
      implementation. By default this connection is not encrypted and can
      potentially be eavesdropped.
     </p><p>
      Enable encrypted communication for noVNC by choosing
      <span class="guimenu ">HTTPS</span> and providing the locations for the certificate
      key pair files.
     </p></dd><dt id="id-1.4.5.4.16.3.9"><span class="term "><span class="guimenu ">Logging: Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu ">true</span>.
     </p></dd></dl></div><div id="note-custom-vendor" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Custom Vendor Data for Instances</h6><p>
    You can pass custom vendor data to all VMs via nova's metadata server.
    For example, information about a custom SMT server can be used by the
    SUSE guest images to automatically configure the repositories for the
    guest.
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To pass custom vendor data, switch to the <span class="guimenu ">Raw</span> view of
      the nova barclamp.
     </p></li><li class="listitem "><p>
      Search for the following section:
     </p><div class="verbatim-wrap"><pre class="screen">"metadata": {
  "vendordata": {
    "json": "{}"
  }
}</pre></div></li><li class="listitem "><p>
      As value of the <code class="literal">json</code> entry, enter valid JSON data. For
      example:
     </p><div class="verbatim-wrap"><pre class="screen">"metadata": {
  "vendordata": {
    "json": "{\"<em class="replaceable ">CUSTOM_KEY</em>\": \"<em class="replaceable ">CUSTOM_VALUE</em>\"}"
  }
}</pre></div><p>
      The string needs to be escaped because the barclamp file is in JSON
      format, too.
     </p></li></ol></div><p>
    Use the following command to access the custom vendor data from inside a
    VM:
   </p><div class="verbatim-wrap"><pre class="screen">curl -s http://<em class="replaceable ">METADATA_SERVER</em>/openstack/latest/vendor_data.json</pre></div><p>
    The IP address of the metadata server is always the same from within a VM.
    For more details, see
    <a class="link" href="https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/" target="_blank">https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/</a>.
   </p></div><div class="figure" id="id-1.4.5.4.16.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nova.png" target="_blank"><img src="images/depl_barclamp_nova.png" width="" alt="The nova Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.16: </span><span class="name">The nova Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.16.5">#</a></h6></div></div><p>
   The nova component consists of eight different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.16.7.1"><span class="term "><span class="guimenu ">nova-controller</span>
    </span></dt><dd><p>
      Distributing and scheduling the instances is managed by the
      <span class="guimenu ">nova-controller</span>. It also provides networking and
      messaging services. <span class="guimenu ">nova-controller</span> needs to be
      installed on a Control Node.
     </p></dd><dt id="id-1.4.5.4.16.7.2"><span class="term "><span class="guimenu ">nova-compute-kvm</span> /
    <span class="guimenu ">nova-compute-qemu</span> /
    <span class="guimenu ">nova-compute-vmware</span> /
    </span></dt><dd><p>
      Provides the hypervisors (KVM, QEMU, VMware vSphere, and z/VM)
      and tools needed to manage the instances. Only one hypervisor can be
      deployed on a single compute node. To use different hypervisors in your
      cloud, deploy different hypervisors to different Compute Nodes. A
      <code class="literal">nova-compute-*</code> role needs to be installed on every
      Compute Node. However, not all hypervisors need to be deployed.
     </p><p>
      Each image that will be made available in SUSE <span class="productname">OpenStack</span> Cloud to start an instance
      is bound to a hypervisor. Each hypervisor can be deployed on multiple
      Compute Nodes (except for the VMware vSphere role, see below). In a
      multi-hypervisor deployment you should make sure to deploy the
      <code class="literal">nova-compute-*</code> roles in a way, that enough compute
      power is available for each hypervisor.
     </p><div id="id-1.4.5.4.16.7.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Re-assigning Hypervisors</h6><p>
       Existing <code class="literal">nova-compute-*</code> nodes can be changed in a
       production SUSE <span class="productname">OpenStack</span> Cloud without service interruption. You need to
       <span class="quote">“<span class="quote ">evacuate</span>”</span>
       the node, re-assign a new <code class="literal">nova-compute</code> role via the
       nova barclamp and <span class="guimenu ">Apply</span> the change.
       <span class="guimenu ">nova-compute-vmware</span> can only be deployed on a single
       node.
      </p></div></dd></dl></div><div class="figure" id="id-1.4.5.4.16.8"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nova_node_deployment.png" target="_blank"><img src="images/depl_barclamp_nova_node_deployment.png" width="" alt="The nova Barclamp: Node Deployment Example with Two KVM Nodes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.17: </span><span class="name">The nova Barclamp: Node Deployment Example with Two KVM Nodes </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.16.8">#</a></h6></div></div><p>
    When deploying a <span class="guimenu ">nova-compute-vmware</span> node with the
    <span class="guimenu ">vmware_dvs</span> ML2 driver enabled in the neutron barclamp, the following
    new attributes are also available in the <span class="guimenu ">vcenter</span> section of the
    <span class="guimenu ">Raw</span> mode:<span class="guimenu ">dvs_name</span> (the name of the
    DVS switch configured on the target vCenter cluster) and
    <span class="guimenu ">dvs_security_groups</span> (enable or disable implementing
    security groups through DVS traffic rules).
  </p><p>
    It is important to specify the correct <span class="guimenu ">dvs_name</span>
    value, as the barclamp expects the  DVS switch to be preconfigured on the
    target VMware vCenter cluster.
  </p><div id="id-1.4.5.4.16.11" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: vmware_dvs must be enabled</h6><p>
      Deploying <span class="guimenu ">nova-compute-vmware</span> nodes will not result in
      a functional cloud setup if the <span class="guimenu ">vmware_dvs</span> ML2 plugin
      is not enabled in the neutron barclamp.
     </p></div><div class="sect2 " id="sec-depl-ostack-nova-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for nova</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-nova-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-nova-ha</li></ul></div></div></div></div><p>
    Making <span class="guimenu ">nova-controller</span> highly available requires no
    special configuration—it is sufficient to deploy it on a cluster.
   </p><p>
    To enable High Availability for Compute Nodes, deploy the following roles to one or more
    clusters with remote nodes:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      nova-compute-kvm
     </p></li><li class="listitem "><p>
      nova-compute-qemu
     </p></li><li class="listitem "><p>
      ec2-api
     </p></li></ul></div><p>
    The cluster to which you deploy the roles above can be completely
    independent of the one to which the role <code class="literal">nova-controller</code>
    is deployed.
   </p><p>
    However, the <code class="literal">nova-controller</code> and
    <code class="literal">ec2-api</code> roles must be deployed the same way (either
    <span class="emphasis"><em>both</em></span> to a cluster or <span class="emphasis"><em>both</em></span> to
    individual nodes. This is due to Crowbar design limitations.
   </p><div id="id-1.4.5.4.16.12.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Shared Storage</h6><p>
     It is recommended to use shared storage for the
     <code class="filename">/var/lib/nova/instances</code> directory, to ensure that
     ephemeral disks will be preserved during recovery of VMs from failed
     compute nodes. Without shared storage, any ephemeral disks will be lost,
     and recovery will rebuild the VM from its original image.
    </p><p>
     If an external NFS server is used, enable the following option in the
     nova barclamp proposal: <span class="guimenu ">Shared Storage for nova instances has
     been manually configured</span>.
    </p></div></div></div><div class="sect1 " id="sec-depl-ostack-dash"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying horizon (<span class="productname">OpenStack</span> Dashboard)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-dash">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-dash</li></ul></div></div></div></div><p>
   The last component that needs to be deployed is horizon, the <span class="productname">OpenStack</span>
   Dashboard. It provides a Web interface for users to start and stop instances
   and for administrators to manage users, groups, roles, etc. horizon should
   be installed on a Control Node. To make horizon highly available, deploy it
   on a cluster.
  </p><p>
   The following attributes can be configured:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.17.4.1"><span class="term ">Session Timeout</span></dt><dd><p>
      Timeout (in minutes) after which a user is been logged out automatically.
      The default value is set to four hours (240 minutes).
     </p><div id="id-1.4.5.4.17.4.1.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Timeouts Larger than Four Hours</h6><p>
       Every horizon session requires a valid keystone token. These tokens
       also have a lifetime of four hours (14400 seconds). Setting the horizon
       session timeout to a value larger than 240 will therefore have no
       effect, and you will receive a warning when applying the barclamp.
      </p><p>
       To successfully apply a timeout larger than four hours, you first need
       to adjust the keystone token expiration accordingly. To do so, open the
       keystone barclamp in <span class="guimenu ">Raw</span> mode and adjust the value of
       the key <code class="literal">token_expiration</code>. Note that the value has to
       be provided in <span class="emphasis"><em>seconds</em></span>. When the change is
       successfully applied, you can adjust the horizon session timeout (in
       <span class="emphasis"><em>minutes</em></span>). Note that extending the keystone token
       expiration may cause scalability issues in large and very busy SUSE <span class="productname">OpenStack</span> Cloud
       installations.
      </p></div></dd><dt id="id-1.4.5.4.17.4.2"><span class="term "><span class="guimenu ">
      User Password Validation: Regular expression used for password
      validation
     </span>
    </span></dt><dd><p>
      Specify a regular expression with which to check the password. The
      default expression (<code class="literal">.{8,}</code>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see
      <a class="link" href="http://docs.python.org/2.7/library/re.html#module-re" target="_blank">http://docs.python.org/2.7/library/re.html#module-re</a>
      for a reference).
     </p></dd><dt id="id-1.4.5.4.17.4.3"><span class="term "><span class="guimenu ">
      User Password Validation: Text to display if the password does not pass
      validation
     </span>
    </span></dt><dd><p>
      Error message that will be displayed in case the password validation
      fails.
     </p></dd><dt id="id-1.4.5.4.17.4.4"><span class="term ">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If choosing <span class="guimenu ">HTTPS</span>,
      you have two choices. You can either <span class="guimenu ">Generate (self-signed)
      certificates</span> or provide the locations for the certificate key
      pair files and,—optionally— the certificate chain file. Using
      self-signed certificates is for testing purposes only and should never be
      used in production environments!
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.17.5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_nova_dashboard.png" target="_blank"><img src="images/depl_barclamp_nova_dashboard.png" width="" alt="The horizon Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.18: </span><span class="name">The horizon Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.17.5">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-dash-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for horizon</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-dash-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-dash-ha</li></ul></div></div></div></div><p>
    Making horizon highly available requires no special configuration—it
    is sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-heat"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying heat (Optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-heat">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-heat</li></ul></div></div></div></div><p>
   heat is a template-based orchestration engine that enables you to, for
   example, start workloads requiring multiple servers or to automatically
   restart instances if needed. It also brings auto-scaling to SUSE <span class="productname">OpenStack</span> Cloud by
   automatically starting additional instances if certain criteria are met.
   For more information about heat refer to the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/heat/" target="_blank">http://docs.openstack.org/developer/heat/</a>.
  </p><p>
   heat should be deployed on a Control Node. To make heat highly
   available, deploy it on a cluster.
  </p><p>
   The following attributes can be configured for heat:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.18.5.1"><span class="term "><span class="guimenu ">Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu ">true</span>.
     </p></dd><dt id="id-1.4.5.4.18.5.2"><span class="term ">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu ">HTTPS</span>)
      or not (<span class="guimenu ">HTTP</span>). If choosing
      <span class="guimenu ">HTTPS</span>, refer to
      <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.18.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_heat.png" target="_blank"><img src="images/depl_barclamp_heat.png" width="" alt="The heat Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.19: </span><span class="name">The heat Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.18.6">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-heat-delegated-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Identity Trusts Authorization (Optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-heat-delegated-roles">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-heat-delegated-roles</li></ul></div></div></div></div><p>
    heat uses keystone Trusts to delegate a subset of user roles to the
    heat engine for deferred operations (see
    <a class="link" href="http://hardysteven.blogspot.de/2014/04/heat-auth-model-updates-part-1-trusts.html" target="_blank">Steve
    Hardy's blog</a> for details). It can either delegate all user roles or
    only those specified in the <code class="literal">trusts_delegated_roles</code>
    setting. Consequently, all roles listed in
    <code class="literal">trusts_delegated_roles</code> need to be assigned to a user,
    otherwise the user will not be able to use heat.
   </p><p>
    The recommended setting for <code class="literal">trusts_delegated_roles</code> is
    <code class="literal">member</code>, since this is the default role most users are
    likely to have. This is also the default setting when installing SUSE <span class="productname">OpenStack</span> Cloud
    from scratch.
   </p><p>
    On installations where this setting is introduced through an upgrade,
    <code class="literal">trusts_delegated_roles</code> will be set to
    <code class="literal">heat_stack_owner</code>. This is a conservative choice to
    prevent breakage in situations where unprivileged users may already have
    been assigned the <code class="literal">heat_stack_owner</code> role to enable them
    to use heat but lack the <code class="literal">member</code> role. As long as you can
    ensure that all users who have the <code class="literal">heat_stack_owner</code> role
    also have the <code class="literal">member</code> role, it is both safe and
    recommended to change trusts_delegated_roles to <code class="literal">member</code>.
    
   </p><div id="id-1.4.5.4.18.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       If the Octavia barclamp is deployed, the <code class="literal">trusts_delegated_roles</code>
       configuration option either needs to be set to an empty value, or the
       <code class="literal">load-balancer_member</code> role needs to be included, otherwise
       it won't be possible to create Octavia load balancers via heat stacks.
       Refer to the <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-octavia-migrate-users" title="12.20.3. Migrating Users to Octavia">Section 12.20.3, “Migrating Users to Octavia”</a> section for more
       details on the list of specialized roles employed by Octavia.
       Also note that adding the <code class="literal">load-balancer_member</code> role
       to the <code class="literal">trusts_delegated_roles</code> list has the undesired
       side effect that only users that have this role assigned to them will be
       allowed to access the Heat API, as covered previously in this section.
     </p></div><p>
    To view or change the trusts_delegated_role setting you need to open the
    heat barclamp and click <span class="guimenu ">Raw</span> in the
    <span class="guimenu ">Attributes</span> section. Search for the
    <code class="literal">trusts_delegated_roles</code> setting and modify the list of
    roles as desired.
   </p><div class="figure" id="id-1.4.5.4.18.7.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_heat_raw.png" target="_blank"><img src="images/depl_barclamp_heat_raw.png" width="" alt="the heat barclamp: Raw Mode" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.20: </span><span class="name">the heat barclamp: Raw Mode </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.18.7.7">#</a></h6></div></div><div id="id-1.4.5.4.18.7.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning: Empty Value</h6><p>
     An empty value for <code class="literal">trusts_delegated_roles</code> will delegate
     <span class="emphasis"><em>all</em></span> of user roles to heat. This may create a security
     risk for users who are assigned privileged roles, such as
     <code class="literal">admin</code>, because these privileged roles will also be
     delegated to the heat engine when these users create heat stacks.
    </p></div></div><div class="sect2 " id="sec-depl-ostack-heat-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for heat</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-heat-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-heat-ha</li></ul></div></div></div></div><p>
    Making heat highly available requires no special configuration—it
    is sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-ceilometer"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying ceilometer (Optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-ceilometer">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ceilometer</li></ul></div></div></div></div><p>
   ceilometer collects CPU and networking data from SUSE <span class="productname">OpenStack</span> Cloud. This data can be
   used by a billing system to enable customer billing. Deploying ceilometer is
   optional. ceilometer agents use monasca database to store collected data.
  </p><p>
   For more information about ceilometer refer to the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/" target="_blank">http://docs.openstack.org/developer/ceilometer/</a>.
  </p><div id="id-1.4.5.4.19.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: ceilometer Restrictions</h6><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 data measuring is only supported for
    KVM and Windows instances. Other hypervisors and SUSE <span class="productname">OpenStack</span> Cloud features
    such as object or block storage will not be measured.
   </p></div><p>
   The following attributes can be configured for ceilometer:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.19.6.1"><span class="term ">
       Intervals used for <span class="productname">OpenStack</span> Compute, Image, or Block Storage meter updates (in seconds)
     </span></dt><dd><p>
      Specify intervals in seconds after which ceilometer performs updates of
      specified meters.
     </p></dd><dt id="id-1.4.5.4.19.6.2"><span class="term ">How long are metering samples kept in the database (in days)
    </span></dt><dd><p>
      Specify how long to keep the metering data. <code class="literal">-1</code> means that samples are
      kept in the database forever.
     </p></dd><dt id="id-1.4.5.4.19.6.3"><span class="term ">How long are event samples kept in the database (in days)
    </span></dt><dd><p>
      Specify how long to keep the event data. <code class="literal">-1</code> means that samples are
      kept in the database forever.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.19.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_ceilometer.png" target="_blank"><img src="images/depl_barclamp_ceilometer.png" width="" alt="The ceilometer Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.21: </span><span class="name">The ceilometer Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.19.7">#</a></h6></div></div><p>
   The ceilometer component consists of four different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.19.9.1"><span class="term "><span class="guimenu ">ceilometer-server</span>
    </span></dt><dd><p>
      The notification agent.
     </p></dd><dt id="id-1.4.5.4.19.9.2"><span class="term "><span class="guimenu ">ceilometer-central</span>
    </span></dt><dd><p>
      The polling agent listens to the message bus to collect data. It needs to
      be deployed on a Control Node. It can be deployed on the same node as
      <span class="guimenu ">ceilometer-server</span>.
     </p></dd><dt id="id-1.4.5.4.19.9.3"><span class="term "><span class="guimenu ">ceilometer-agent</span>
    </span></dt><dd><p>
      The compute agents collect data from the compute nodes. They need to be
      deployed on all KVM compute nodes in your cloud (other
      hypervisors are currently not supported).
     </p></dd><dt id="id-1.4.5.4.19.9.4"><span class="term "><span class="guimenu ">ceilometer-swift-proxy-middleware</span>
    </span></dt><dd><p>
      An agent collecting data from the swift nodes. This role needs to be
      deployed on the same node as swift-proxy.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.19.10"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_ceilometer_deployment.png" target="_blank"><img src="images/depl_barclamp_ceilometer_deployment.png" width="" alt="The ceilometer Barclamp: Node Deployment" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.22: </span><span class="name">The ceilometer Barclamp: Node Deployment </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.19.10">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-ceilometer-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for ceilometer</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-ceilometer-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ceilometer-ha</li></ul></div></div></div></div><p>
    Making ceilometer highly available requires no special
    configuration—it is sufficient to deploy the roles
    <span class="guimenu ">ceilometer-server</span> and
    <span class="guimenu ">ceilometer-central</span> on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-manila"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying manila</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-manila">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-manila</li></ul></div></div></div></div><p>
   manila provides coordinated access to shared or distributed file
   systems, similar to what cinder does for block storage. These file
   systems can be shared between instances in SUSE <span class="productname">OpenStack</span> Cloud.
  </p><p>
   manila uses different back-ends. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 currently
   supported back-ends include <span class="guimenu ">Hitachi HNAS</span>, <span class="guimenu ">NetApp
   Driver</span>, and <span class="guimenu ">CephFS</span>. Two more back-end options,
   <span class="guimenu ">Generic Driver</span> and <span class="guimenu ">Other Driver</span> are
   available for testing purposes and are not supported.
  </p><div id="note-limit-cephfs" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Limitations for CephFS Back-end</h6><p>
    manila uses some CephFS features that are currently
    <span class="emphasis"><em>not</em></span> supported by the SUSE Linux Enterprise Server 12 SP4 CephFS kernel
    client:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      RADOS namespaces
     </p></li><li class="listitem "><p>
      MDS path restrictions
     </p></li><li class="listitem "><p>
      Quotas
     </p></li></ul></div><p>
    As a result, to access CephFS shares provisioned by manila, you must
    use ceph-fuse. For details, see
    <a class="link" href="http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html" target="_blank">http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html</a>.
   </p></div><p>
   When first opening the manila barclamp, the default proposal
   <span class="guimenu ">Generic Driver</span> is already available for configuration. To
   replace it, first delete it by clicking the trashcan icon and then choose a
   different back-end in the section <span class="guimenu ">Add new manila Backend</span>.
   Select a <span class="guimenu ">Type of Share</span> and—optionally—provide
   a <span class="guimenu ">Name for Backend</span>. Activate the back-end with
   <span class="guimenu ">Add Backend</span>. Note that at least one back-end must be
   configured.
  </p><p>
   The attributes that can be set to configure cinder depend on the back-end:
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.20.7"><span class="name"><span class="guimenu ">Back-end: Generic</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.20.7">#</a></h3></div><p>
   The generic driver is included as a technology preview and is not supported.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.20.9"><span class="name"><span class="guimenu ">Hitachi HNAS</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.20.9">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.20.10.1"><span class="term "><span class="guimenu ">Specify which EVS this backend is assigned to</span>
    </span></dt><dd><p>
      Provide the name of the Enterprise Virtual Server that the selected
      back-end is assigned to.
      
     </p></dd><dt id="id-1.4.5.4.20.10.2"><span class="term "><span class="guimenu ">Specify IP for mounting shares</span>
    </span></dt><dd><p>
      IP address for mounting shares.
      
     </p></dd><dt id="id-1.4.5.4.20.10.3"><span class="term "><span class="guimenu ">Specify file-system name for creating shares</span>
    </span></dt><dd><p>
      Provide a file-system name for creating shares.
      
     </p></dd><dt id="id-1.4.5.4.20.10.4"><span class="term "><span class="guimenu ">HNAS management interface IP</span>
    </span></dt><dd><p>
      IP address of the HNAS management interface for communication between
      manila controller and HNAS.
     </p></dd><dt id="id-1.4.5.4.20.10.5"><span class="term "><span class="guimenu ">HNAS username Base64 String</span>
    </span></dt><dd><p>
      HNAS username Base64 String required to perform tasks like creating
      file-systems and network interfaces.
     </p></dd><dt id="id-1.4.5.4.20.10.6"><span class="term "><span class="guimenu ">HNAS user password</span>
    </span></dt><dd><p>
      HNAS user password. Required only if private key is not provided.
      
     </p></dd><dt id="id-1.4.5.4.20.10.7"><span class="term "><span class="guimenu ">RSA/DSA private key</span>
    </span></dt><dd><p>
      RSA/DSA private key necessary for connecting to HNAS. Required only if
      password is not provided.
      
     </p></dd><dt id="id-1.4.5.4.20.10.8"><span class="term "><span class="guimenu ">The time to wait for stalled HNAS jobs before aborting</span>
    </span></dt><dd><p>
      Time in seconds to wait before aborting stalled HNAS jobs.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.20.11"><span class="name"><span class="guimenu ">Back-end: Netapp</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.20.11">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.20.12.1"><span class="term "><span class="guimenu ">Name of the Virtual Storage Server (vserver)</span>
    </span></dt><dd><p>
      Host name of the Virtual Storage Server.
     </p></dd><dt id="id-1.4.5.4.20.12.2"><span class="term "><span class="guimenu ">Server Host Name</span>
    </span></dt><dd><p>
      The name or IP address for the storage controller or the cluster.
     </p></dd><dt id="id-1.4.5.4.20.12.3"><span class="term "><span class="guimenu ">Server Port</span>
    </span></dt><dd><p>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </p></dd><dt id="id-1.4.5.4.20.12.4"><span class="term "><span class="guimenu ">User name/Password for Accessing NetApp</span>
    </span></dt><dd><p>
      Login credentials.
     </p></dd><dt id="id-1.4.5.4.20.12.5"><span class="term "><span class="guimenu ">Transport Type</span>
    </span></dt><dd><p>
      Transport protocol for communicating with the storage controller or
      cluster. Supported protocols are HTTP and HTTPS. Choose the protocol your
      NetApp is licensed for.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.20.13"><span class="name"><span class="guimenu ">Back-end: CephFS</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.20.13">#</a></h3></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.20.14.1"><span class="term ">Use Ceph deployed by Crowbar</span></dt><dd><p>
      Set to <code class="systemitem">true</code> to use Ceph deployed with Crowbar.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.20.15"><span class="name"><span class="guimenu ">Back-end: Manual</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.20.15">#</a></h3></div><p>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </p><div class="figure" id="id-1.4.5.4.20.17"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_manila.png" target="_blank"><img src="images/depl_barclamp_manila.png" width="" alt="The manila Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.23: </span><span class="name">The manila Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.20.17">#</a></h6></div></div><p>
   The manila component consists of two different roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.20.19.1"><span class="term "><span class="guimenu ">manila-server</span>
    </span></dt><dd><p>
      The manila server provides the scheduler and the API. Installing it
      on a Control Node is recommended.
     </p></dd><dt id="id-1.4.5.4.20.19.2"><span class="term "><span class="guimenu ">manila-share</span>
    </span></dt><dd><p>
      The shared storage service. It can be installed on a Control Node, but it
      is recommended to deploy it on one or more dedicated nodes supplied with
      sufficient disk space and networking capacity, since it will generate a
      lot of network traffic.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.20.20"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_manila_node_deployment.png" target="_blank"><img src="images/depl_barclamp_manila_node_deployment.png" width="" alt="The manila Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.24: </span><span class="name">The manila Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.20.20">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-manila-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for manila</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-manila-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-manila-ha</li></ul></div></div></div></div><p>
    While the <span class="guimenu ">manila-server</span> role can be deployed on a
    cluster, deploying <span class="guimenu ">manila-share</span> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <span class="guimenu ">manila-share</span> on several nodes—this ensures the
    service continues to be available even when a node fails.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-tempest"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Tempest (Optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-tempest">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-tempest</li></ul></div></div></div></div><p>
   Tempest is an integration test suite for SUSE <span class="productname">OpenStack</span> Cloud written in Python. It
   contains multiple integration tests for validating your SUSE <span class="productname">OpenStack</span> Cloud deployment.
   For more information about Tempest refer to the <span class="productname">OpenStack</span> documentation
   at <a class="link" href="http://docs.openstack.org/developer/tempest/" target="_blank">http://docs.openstack.org/developer/tempest/</a>.
  </p><div id="id-1.4.5.4.21.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Technology Preview</h6><p>
    Tempest is only included as a technology preview and not supported.
   </p><p>
    Tempest may be used for testing whether the intended setup will run
    without problems. It should not be used in a production environment.
   </p></div><p>
   Tempest should be deployed on a Control Node.
  </p><p>
   The following attributes can be configured for Tempest:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.21.6.1"><span class="term "><span class="guimenu ">Choose User name / Password</span>
    </span></dt><dd><p>
      Credentials for a regular user. If the user does not exist, it will be
      created.
     </p></dd><dt id="id-1.4.5.4.21.6.2"><span class="term "><span class="guimenu ">Choose Tenant</span>
    </span></dt><dd><p>
      Tenant to be used by Tempest. If it does not exist, it will be
      created. It is safe to stick with the default value.
     </p></dd><dt id="id-1.4.5.4.21.6.3"><span class="term "><span class="guimenu ">Choose Tempest Admin User name/Password</span>
    </span></dt><dd><p>
      Credentials for an admin user. If the user does not exist, it will be
      created.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.21.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_tempest.png" target="_blank"><img src="images/depl_barclamp_tempest.png" width="" alt="The Tempest Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.25: </span><span class="name">The Tempest Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.21.7">#</a></h6></div></div><div id="id-1.4.5.4.21.8" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Running Tests</h6><p>
    To run tests with Tempest, log in to the Control Node on which
    Tempest was deployed. Change into the directory
    <code class="filename">/var/lib/openstack-tempest-test</code>. To get an overview of
    available commands, run:
   </p><div class="verbatim-wrap"><pre class="screen">./tempest --help</pre></div><p>
    To serially invoke a subset of all tests (<span class="quote">“<span class="quote ">the gating
    smoketests</span>”</span>) to help validate the working functionality of your
    local cloud instance, run the following command. It will save the output to
    a log file
    <code class="filename">tempest_<em class="replaceable ">CURRENT_DATE</em>.log</code>.
   </p><div class="verbatim-wrap"><pre class="screen">./tempest run --smoke --serial 2&gt;&amp;1 \
| tee "tempest_$(date +%Y-%m-%d_%H%M%S).log"</pre></div></div><div class="sect2 " id="sec-depl-ostack-tempest-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Tempest</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-tempest-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-tempest-ha</li></ul></div></div></div></div><p>
    Tempest cannot be made highly available.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-magnum"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Magnum (Optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-magnum">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-magnum</li></ul></div></div></div></div><p>
   Magnum is an <span class="productname">OpenStack</span> project which offers container orchestration
   engines for deploying and managing containers as first class resources in
   <span class="productname">OpenStack</span>.
  </p><p>
   For more information about Magnum, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/magnum/" target="_blank">http://docs.openstack.org/developer/magnum/</a>.
  </p><p>
   For information on how to deploy a Kubernetes cluster (either from command
   line or from the horizon Dashboard), see the <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">User Guide</em></em>. It is available
   from <a class="link" href="https://documentation.suse.com/soc/9/" target="_blank">https://documentation.suse.com/soc/9/</a>.
  </p><p>
   The following <span class="guimenu ">Attributes</span> can be configured for
   Magnum:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.22.6.1"><span class="term "><span class="guimenu ">Trustee Domain</span>: <span class="guimenu ">Delegate trust to
    cluster users if required</span></span></dt><dd><p>
      Deploying Kubernetes clusters in a cloud without an Internet connection
      requires the <code class="literal">registry_enabled</code> option in its cluster
      template set to <code class="literal">true</code>. To make this offline scenario
      work, you also need to set the <span class="guimenu ">Delegate trust to cluster users if
      required</span> option to <code class="literal">true</code>. This restores the old,
      insecure behavior for clusters with the
      <code class="literal">registry-enabled</code> or <code class="literal">volume_driver=Rexray</code> options enabled.
     </p></dd><dt id="id-1.4.5.4.22.6.2"><span class="term "><span class="guimenu ">Trustee Domain</span>: <span class="guimenu ">Domain Name</span></span></dt><dd><p>
      Domain name to use for creating trustee for bays.
     </p></dd><dt id="id-1.4.5.4.22.6.3"><span class="term "><span class="guimenu ">Logging</span>: <span class="guimenu ">Verbose</span></span></dt><dd><p>
      Increases the amount of information that is written to the log files when
      set to <span class="guimenu ">true</span>.
     </p></dd><dt id="id-1.4.5.4.22.6.4"><span class="term "><span class="guimenu ">Logging</span>: <span class="guimenu ">Debug</span></span></dt><dd><p>
      Shows debugging output in the log files when set to <span class="guimenu ">true</span>.
     </p></dd><dt id="id-1.4.5.4.22.6.5"><span class="term "><span class="guimenu ">Certificate Manager</span>: <span class="guimenu ">Plugin</span>
    </span></dt><dd><p>
      To store certificates, either use the <span class="guimenu ">barbican</span>
      <span class="productname">OpenStack</span> service, a local directory (<span class="guimenu ">Local</span>), or the
      <span class="guimenu ">Magnum Database (x590keypair)</span>.
     </p><div id="id-1.4.5.4.22.6.5.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: barbican As Certificate Manager</h6><p>
       If you choose to use barbican for managing certificates, make sure
       that the barbican barclamp is enabled.
      </p></div></dd></dl></div><div class="figure" id="id-1.4.5.4.22.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_magnum_attributes.png" target="_blank"><img src="images/depl_barclamp_magnum_attributes.png" width="" alt="The Magnum Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.26: </span><span class="name">The Magnum Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.22.7">#</a></h6></div></div><p>
   The Magnum barclamp consists of the following roles:
   <span class="guimenu ">magnum-server</span>. It can either be deployed on a Control Node
   or on a cluster—see <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-magnum-ha" title="12.16.1. HA Setup for Magnum">Section 12.16.1, “HA Setup for Magnum”</a>. When
   deploying the role onto a Control Node, additional RAM is required for the
   Magnum server. It is recommended to only deploy the role to a
   Control Node that has 16 GB RAM.
  </p><div class="sect2 " id="sec-depl-ostack-magnum-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for Magnum</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-magnum-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-magnum-ha</li></ul></div></div></div></div><p>
    Making Magnum highly available requires no special configuration. It
    is sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-barbican"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying barbican (Optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-barbican">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barbican</li></ul></div></div></div></div><p>
   barbican is a component designed for storing secrets in a secure and
   standardized manner protected by keystone authentication. Secrets include
   SSL certificates and passwords used by various <span class="productname">OpenStack</span> components.
  </p><p>
   barbican settings can be configured in <code class="literal">Raw</code> mode
   only. To do this, open the barbican barclamp <span class="guimenu ">Attribute
   </span>configuration in <span class="guimenu ">Raw</span> mode.
  </p><div class="figure" id="id-1.4.5.4.23.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_barbican_raw.png" target="_blank"><img src="images/depl_barclamp_barbican_raw.png" width="" alt="The barbican Barclamp: Raw Mode" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.27: </span><span class="name">The barbican Barclamp: Raw Mode </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.23.4">#</a></h6></div></div><p>
   When configuring barbican, pay particular attention to the following
   settings:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="literal">bind_host</code> Bind host for the barbican API service
    </p></li><li class="listitem "><p>
     <code class="literal">bind_port</code> Bind port for the barbican API service
    </p></li><li class="listitem "><p>
     <code class="literal">processes</code> Number of API processes to run in Apache
    </p></li><li class="listitem "><p>
     <code class="literal">ssl</code> Enable or disable SSL
    </p></li><li class="listitem "><p>
     <code class="literal">threads</code> Number of API worker threads
    </p></li><li class="listitem "><p>
     <code class="literal">debug</code> Enable or disable debug logging
    </p></li><li class="listitem "><p>
     <code class="literal">enable_keystone_listener</code> Enable or disable the
     keystone listener services
    </p></li><li class="listitem "><p>
     <code class="literal">kek</code> An encryption key (fixed-length 32-byte
     Base64-encoded value) for barbican's
     <code class="systemitem">simple_crypto</code> plugin. If left unspecified, the
     key will be generated automatically.
    </p><div id="id-1.4.5.4.23.6.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Existing Encryption Key</h6><p>
      If you plan to restore and use the existing barbican database after
      a full reinstall (including a complete wipe of the Crowbar node), make
      sure to save the specified encryption key beforehand. You will need to
      provide it after the full reinstall in order to access the data in the
      restored barbican database.
     </p></div></li></ul></div><div class="variablelist "><dl class="variablelist"><dt id="sec-depl-ostack-barbican-ssl"><span class="term ">SSL Support: Protocol
    </span></dt><dd><p>
      With the default value <span class="guimenu ">HTTP</span>, public communication will
      not be encrypted. Choose <span class="guimenu ">HTTPS</span> to use SSL for
      encryption. See <a class="xref" href="cha-depl-req.html#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for background
      information and <a class="xref" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ssl" title="11.4.6. Enabling SSL">Section 11.4.6, “Enabling SSL”</a> for
      installation instructions. The following additional configuration options
      will become available when choosing <span class="guimenu ">HTTPS</span>:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.23.7.1.2.2.1"><span class="term "><span class="guimenu ">Generate (self-signed) certificates</span>
       </span></dt><dd><p>
         When set to <code class="literal">true</code>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </p></dd><dt id="id-1.4.5.4.23.7.1.2.2.2"><span class="term "><span class="guimenu ">SSL Certificate File</span> / <span class="guimenu ">SSL (Private) Key
        File</span>
       </span></dt><dd><p>
         Location of the certificate key pair files.
        </p></dd><dt id="id-1.4.5.4.23.7.1.2.2.3"><span class="term "><span class="guimenu ">SSL Certificate is insecure</span>
       </span></dt><dd><p>
         Set this option to <code class="literal">true</code> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </p></dd><dt id="id-1.4.5.4.23.7.1.2.2.4"><span class="term "><span class="guimenu ">SSL CA Certificates File</span>
       </span></dt><dd><p>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the barclamp to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <code class="systemitem">apache2</code> service, and
         re-deploy the barclamp.
        </p><p>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </p><div class="figure" id="id-1.4.5.4.23.7.1.2.2.4.2.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barbican_ssl.png" target="_blank"><img src="images/depl_barbican_ssl.png" width="" alt="The SSL Dialog" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.28: </span><span class="name">The SSL Dialog </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.23.7.1.2.2.4.2.3">#</a></h6></div></div></dd></dl></div></dd></dl></div><div class="sect2 " id="sec-depl-ostack-barbican-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for barbican</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-barbican-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-barbican-ha</li></ul></div></div></div></div><p>
    To make barbican highly available, assign the
    <span class="guimenu ">barbican-controller</span> role to the Controller Cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-sahara"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying sahara</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-sahara">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-sahara</li></ul></div></div></div></div><p>
   sahara provides users with simple means to provision data processing
   frameworks (such as Hadoop, Spark, and Storm) on <span class="productname">OpenStack</span>. This is
   accomplished by specifying configuration parameters such as the framework
   version, cluster topology, node hardware details, etc.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.24.3.1"><span class="term ">Logging: Verbose</span></dt><dd><p>
      Set to <code class="systemitem">true</code> to increase the amount of
      information written to the log files.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.24.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_sahara.png" target="_blank"><img src="images/depl_barclamp_sahara.png" width="" alt="The sahara Barclamp" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.29: </span><span class="name">The sahara Barclamp </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.24.4">#</a></h6></div></div><div class="sect2 " id="sec-depl-ostack-sahara-ha"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.18.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HA Setup for sahara</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-sahara-ha">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-sahara-ha</li></ul></div></div></div></div><p>
    Making sahara highly available requires no special configuration. It is
    sufficient to deploy it on a cluster.
   </p></div></div><div class="sect1 " id="sec-depl-ostack-monasca"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying monasca</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-monasca">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-monasca</li></ul></div></div></div></div><p>
   monasca is an open-source monitoring-as-a-service solution that
   integrates with <span class="productname">OpenStack</span>. monasca is designed for scalability, high
   performance, and fault tolerance.
  </p><p>
   Accessing the <span class="guimenu ">Raw</span> interface is not required for
   day-to-day operation. But as not all monasca settings are exposed in the
   barclamp graphical interface (for example, various performance tuneables), it
   is recommended to configure monasca in the <span class="guimenu ">Raw</span>
   mode. Below are the options that can be configured via the
   <span class="guimenu ">Raw</span> interface of the monasca barclamp.
  </p><div class="figure" id="id-1.4.5.4.25.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_monasca_raw.png" target="_blank"><img src="images/depl_barclamp_monasca_raw.png" width="" alt="The monasca barclamp Raw Mode" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.30: </span><span class="name">The monasca barclamp Raw Mode </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.4">#</a></h6></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.5"><span class="name"><span class="guimenu ">agent: settings for openstack-monasca-agent</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.5">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.6.1"><span class="term ">keystone</span></dt><dd><p>
      Contains keystone credentials that the agents use to send metrics. Do
      not change these options, as they are configured by Crowbar.
     </p></dd><dt id="id-1.4.5.4.25.6.2"><span class="term ">insecure</span></dt><dd><p>
      Specifies whether SSL certificates are verified when communicating with
      keystone. If set to <code class="literal">false</code>, the
      <code class="literal">ca_file</code> option must be specified.
     </p></dd><dt id="id-1.4.5.4.25.6.3"><span class="term ">ca_file</span></dt><dd><p>
      Specifies the location of a CA certificate that is used for verifying
      keystone's SSL certificate.
     </p></dd><dt id="id-1.4.5.4.25.6.4"><span class="term ">log_dir</span></dt><dd><p>
      Path for storing log files. The specified path must exist. Do not change
      the default <code class="filename">/var/log/monasca-agent</code> path.
     </p></dd><dt id="id-1.4.5.4.25.6.5"><span class="term ">log_level</span></dt><dd><p>
      Agent's log level. Limits log messages to the specified level and above.
      The following levels are available: Error, Warning, Info (default), and
      Debug.
     </p></dd><dt id="id-1.4.5.4.25.6.6"><span class="term ">check_frequency</span></dt><dd><p>
      Interval in seconds between running agents' checks.
     </p></dd><dt id="id-1.4.5.4.25.6.7"><span class="term ">num_collector_threads</span></dt><dd><p>
      Number of simultaneous collector threads to run. This refers to the
      maximum number of different collector plug-ins (for example,
      <code class="literal">http_check</code>) that are allowed to run simultaneously. The
      default value <code class="literal">1</code> means that plug-ins are run
      sequentially.
     </p></dd><dt id="id-1.4.5.4.25.6.8"><span class="term ">pool_full_max_retries</span></dt><dd><p>
      If a problem with the results from multiple plug-ins results blocks the
      entire thread pool (as specified by the
      <code class="systemitem">num_collector_threads</code> parameter), the collector
      exits, so it can be restarted by the
      <code class="systemitem">supervisord</code>. The parameter
      <code class="systemitem">pool_full_max_retries</code> specifies when this event
      occurs. The collector exits when the defined number of consecutive
      collection cycles have ended with the thread pool completely full.
     </p></dd><dt id="id-1.4.5.4.25.6.9"><span class="term ">plugin_collect_time_warn</span></dt><dd><p>
      Upper limit in seconds for any collection plug-in's run time. A warning
      is logged if a plug-in runs longer than the specified limit.
     </p></dd><dt id="id-1.4.5.4.25.6.10"><span class="term ">max_measurement_buffer_size</span></dt><dd><p>
      Maximum number of measurements to buffer locally if the monasca API
      is unreachable. Measurements will be dropped in batches, if the API is
      still unreachable after the specified number of messages are buffered.
      The default <code class="literal">-1</code> value indicates unlimited buffering.
      Note that a large buffer increases the agent's memory usage.
     </p></dd><dt id="id-1.4.5.4.25.6.11"><span class="term ">backlog_send_rate</span></dt><dd><p>
      Maximum number of measurements to send when the local measurement buffer
      is flushed.
     </p></dd><dt id="id-1.4.5.4.25.6.12"><span class="term ">amplifier</span></dt><dd><p>
      Number of extra dimensions to add to metrics sent to the monasca API.
      This option is intended for load testing purposes only. Do not enable the
      option in production! The default <code class="literal">0</code> value disables the
      addition of dimensions.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.7"><span class="name"><span class="guimenu ">log_agent: settings for openstack-monasca-log-agent</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.7">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.8.1"><span class="term ">max_data_size_kb</span></dt><dd><p>
      Maximum payload size in kilobytes for a request sent to the monasca
      log API.
     </p></dd><dt id="id-1.4.5.4.25.8.2"><span class="term ">num_of_logs</span></dt><dd><p>
      Maximum number of log entries the log agent sends to the monasca log
      API in a single request. Reducing the number increases performance.
     </p></dd><dt id="id-1.4.5.4.25.8.3"><span class="term ">elapsed_time_sec</span></dt><dd><p>
      Time interval in seconds between sending logs to the monasca log API.
     </p></dd><dt id="id-1.4.5.4.25.8.4"><span class="term ">delay</span></dt><dd><p>
      Interval in seconds for checking whether
      <code class="literal">elapsed_time_sec</code> has been reached.
     </p></dd><dt id="id-1.4.5.4.25.8.5"><span class="term ">keystone</span></dt><dd><p>
      keystone credentials the log agents use to send logs to the monasca
      log API. Do not change this option manually, as it is configured by
      Crowbar.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.9"><span class="name"><span class="guimenu ">api: Settings for openstack-monasca-api</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.9">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.10.1"><span class="term ">bind_host</span></dt><dd><p>
      Interfaces <code class="literal">monasca-api</code> listens on. Do not change this
      option, as it is configured by Crowbar.
     </p></dd><dt id="id-1.4.5.4.25.10.2"><span class="term ">processes</span></dt><dd><p>
      Number of processes to spawn.
     </p></dd><dt id="id-1.4.5.4.25.10.3"><span class="term ">threads</span></dt><dd><p>
      Number of WSGI worker threads to spawn.
     </p></dd><dt id="id-1.4.5.4.25.10.4"><span class="term ">log_level</span></dt><dd><p>
      Log level for <code class="systemitem">openstack-monasca-api</code>. Limits log
      messages to the specified level and above. The following levels are
      available: Critical, Error, Warning, Info (default), Debug, and Trace.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.11"><span class="name"><span class="guimenu ">elasticsearch: server-side settings for elasticsearch</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.11">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.12.1"><span class="term ">repo_dir</span></dt><dd><p>
      List of directories for storing Elasticsearch snapshots. Must be created
      manually and be writeable by the
      <code class="systemitem">elasticsearch</code> user.
      Must contain at least one entry in order for the snapshot functionality
      to work.
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.12.1.2.2.1"><span class="term ">heap_size</span></dt><dd><p>
         Sets the heap size. We recommend setting heap size at 50% of the
         available memory, but not more than 31 GB. The default of 4 GB is
         likely too small and should be increased if possible.
        </p></dd><dt id="id-1.4.5.4.25.12.1.2.2.2"><span class="term ">limit_memlock</span></dt><dd><p>
         The maximum size that may be locked into memory in bytes
        </p></dd><dt id="id-1.4.5.4.25.12.1.2.2.3"><span class="term ">limit_nofile</span></dt><dd><p>
         The maximum number of open file descriptors
        </p></dd><dt id="id-1.4.5.4.25.12.1.2.2.4"><span class="term ">limit_nproc</span></dt><dd><p>
         The maximum number of processes
        </p></dd><dt id="id-1.4.5.4.25.12.1.2.2.5"><span class="term ">vm_max_map_count</span></dt><dd><p>
         The maximum number of memory map areas a process may have.
        </p></dd></dl></div></dd></dl></div><p>
   For instructions on creating an Elasticsearch snapshot, see
   <span class="intraxref">Book “<em class="citetitle ">Operations Guide Crowbar</em>”, Chapter 4 “SUSE <span class="productname">OpenStack</span> Cloud Monitoring”, Section 4.7 “Operation and Maintenance”, Section 4.7.4 “Backup and Recovery”</span>.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.14"><span class="name"><span class="guimenu ">elasticsearch_curator: settings for
    elastisearch-curator</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.14">#</a></h4></div><p>
   <code class="systemitem">elasticsearch-curator</code> removes old and large
   elasticsearch indices. The settings below determine its behavior.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.16.1"><span class="term ">delete_after_days</span></dt><dd><p>
      Time threshold for deleting indices. Indices older the specified number
      of days are deleted. This parameter is unset by default, so indices are
      kept indefinitely.
     </p></dd><dt id="id-1.4.5.4.25.16.2"><span class="term ">delete_after_size</span></dt><dd><p>
      Maximum size in megabytes of indices. Indices larger than the specified
      size are deleted. This parameter is unset by default, so indices are kept
      irrespective of their size.
     </p></dd><dt id="id-1.4.5.4.25.16.3"><span class="term ">delete_exclude_index</span></dt><dd><p>
      List of indices to exclude from
      <code class="systemitem">elasticsearch-curator</code> runs. By default, only the
      <code class="filename">.kibana</code> files are excluded.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.17"><span class="name"><span class="guimenu ">kafka: tunables for
Kafka</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.17">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.18.1"><span class="term ">log_retention_hours</span></dt><dd><p>
      Number of hours for retaining log segments in Kafka's on-disk log.
      Messages older than the specified value are dropped.
     </p></dd><dt id="id-1.4.5.4.25.18.2"><span class="term ">log_retention_bytes</span></dt><dd><p>
      Maximum size for Kafka's on-disk log in bytes. If the log grows beyond
      this size, the oldest log segments are dropped.
     </p></dd><dt id="id-1.4.5.4.25.18.3"><span class="term ">topics</span></dt><dd><p>
      list of topics
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        metrics
       </p></li><li class="listitem "><p>
        events
       </p></li><li class="listitem "><p>
        alarm-state-transitions
       </p></li><li class="listitem "><p>
        alarm-notifications
       </p></li><li class="listitem "><p>
        retry-notifications
       </p></li><li class="listitem "><p>
        60-seconds-notifications
       </p></li><li class="listitem "><p>
        log
       </p></li><li class="listitem "><p>
        transformed-log
       </p></li></ul></div><p>
      The following are options of every topic:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.18.3.2.4.1"><span class="term ">replicas</span></dt><dd><p>
         Controls how many servers replicate each message that is written
        </p></dd><dt id="id-1.4.5.4.25.18.3.2.4.2"><span class="term ">partitions</span></dt><dd><p>
         Controls how many logs the topic is sharded into
        </p></dd><dt id="id-1.4.5.4.25.18.3.2.4.3"><span class="term ">config_options</span></dt><dd><p>
         Map of configuration options is described in the <a class="link" href="https://kafka.apache.org/documentation/#topicconfigs" target="_blank">Apache
         Kafka documentation</a>
        </p></dd></dl></div></dd></dl></div><p>
   These parameters only affect first time installations. Parameters may be
   changed after installation with scripts available from <a class="link" href="https://kafka.apache.org/documentation/#basic_ops" target="_blank">Apache Kafka</a>.
  </p><p>
   Kafka does not support reducing the number of partitions for a topic.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.21"><span class="name"><span class="guimenu ">notification:</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.21">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.22.1"><span class="term ">email_enabled</span></dt><dd><p>
      Enable or disable email alarm notifications.
     </p></dd><dt id="id-1.4.5.4.25.22.2"><span class="term ">email_smtp_host</span></dt><dd><p>
      SMTP smarthost for sending alarm notifications.
     </p></dd><dt id="id-1.4.5.4.25.22.3"><span class="term ">email_smtp_port</span></dt><dd><p>
      Port for the SMTP smarthost.
     </p></dd><dt id="id-1.4.5.4.25.22.4"><span class="term ">email_smtp_user</span></dt><dd><p>
      User name for authenticating against the smarthost.
     </p></dd><dt id="id-1.4.5.4.25.22.5"><span class="term ">email_smtp_password</span></dt><dd><p>
      Password for authenticating against the smarthost.
     </p></dd><dt id="id-1.4.5.4.25.22.6"><span class="term ">email_smtp_from_address</span></dt><dd><p>
      Sender address for alarm notifications.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.23"><span class="name"><span class="guimenu ">master: configuration for
monasca-installer on the Crowbar node</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.23">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.24.1"><span class="term ">influxdb_retention_policy</span></dt><dd><p>
      Number of days to keep metrics records in influxdb.
     </p><p>
      For an overview of all supported values, see
      <a class="link" href="https://docs.influxdata.com/influxdb/v1.1/query_language/database_management/#create-retention-policies-with-create-retention-policy" target="_blank">https://docs.influxdata.com/influxdb/v1.1/query_language/database_management/#create-retention-policies-with-create-retention-policy</a>.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.25"><span class="name"><span class="guimenu ">monasca: settings for libvirt and Ceph
monitoring</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.25">#</a></h4></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.26.1"><span class="term ">
     monitor_libvirt
    </span></dt><dd><p>
      The global switch for toggling libvirt monitoring. If set to
      <em class="parameter ">true</em>, libvirt metrics will be gathered on all
      libvirt based Compute Nodes. This setting is available in the Crowbar UI.
     </p></dd><dt id="id-1.4.5.4.25.26.2"><span class="term ">
     monitor_ceph
    </span></dt><dd><p>
      The global switch for toggling Ceph monitoring. If set to
      <em class="parameter ">true</em>, Ceph metrics will be gathered on all
      Ceph-based Compute Nodes. This setting is available in Crowbar UI. If the
      Ceph cluster has been set up independently, Crowbar ignores this setting.
     </p></dd><dt id="id-1.4.5.4.25.26.3"><span class="term ">
     cache_dir
    </span></dt><dd><p>
      The directory where monasca-agent will locally cache various metadata
      about locally running VMs on each Compute Node.
     </p></dd><dt id="id-1.4.5.4.25.26.4"><span class="term ">
     customer_metadata
    </span></dt><dd><p>
      Specifies the list of instance metadata keys to be included as dimensions
      with customer metrics. This is useful for providing more information
      about an instance.
     </p></dd><dt id="id-1.4.5.4.25.26.5"><span class="term ">
     disk_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<code class="option">check_frequency</code>), it
      will be ignored in favor of the global collection period.
     </p></dd><dt id="id-1.4.5.4.25.26.6"><span class="term ">
     max_ping_concurrency
    </span></dt><dd><p>
      Specifies the number of ping command processes to run concurrently when
      determining whether the VM is reachable. This should be set to a value
      that allows the plugin to finish within the agent's collection period,
      even if there is a networking issue. For example, if the expected number
      of VMs per Compute Node is 40 and each VM has one IP address, then the
      plugin will take at least 40 seconds to do the ping checks in the
      worst-case scenario where all pings fail (assuming the default timeout of
      1 second). Increasing <code class="option">max_ping_concurrency</code> allows the
      plugin to finish faster.
     </p></dd><dt id="id-1.4.5.4.25.26.7"><span class="term ">
     metadata
    </span></dt><dd><p>
      Specifies the list of nova side instance metadata keys to be included
      as dimensions with the cross-tenant metrics for the
      <span class="guimenu ">monasca</span> project. This is useful for providing more
      information about an instance.
     </p></dd><dt id="id-1.4.5.4.25.26.8"><span class="term ">
     nova_refresh
    </span></dt><dd><p>
      Specifies the number of seconds between calls to the nova API to
      refresh the instance cache. This is helpful for updating VM hostname and
      pruning deleted instances from the cache. By default, it is set to 14,400
      seconds (four hours). Set to 0 to refresh every time the Collector runs,
      or to <em class="parameter ">None</em> to disable regular refreshes entirely.
      In this case, the instance cache will only be refreshed when a new
      instance is detected.
     </p></dd><dt id="id-1.4.5.4.25.26.9"><span class="term ">
     ping_check
    </span></dt><dd><p>
      Includes the entire ping command (without the IP address, which is
      automatically appended) to perform a ping check against instances. The
      <code class="literal">NAMESPACE</code> keyword is automatically replaced with the
      appropriate network namespace for the VM being monitored. Set to
      <em class="parameter ">False</em> to disable ping checks.
     </p></dd><dt id="id-1.4.5.4.25.26.10"><span class="term ">
     vnic_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<code class="option">check_frequency</code>), it
      will be ignored in favor of the global collection period.
     </p></dd><dt id="id-1.4.5.4.25.26.11"><span class="term ">
     vm_cpu_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM CPU metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.25.26.12"><span class="term ">
     vm_disks_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM disk metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.25.26.13"><span class="term ">
     vm_extended_disks_check_enable
    </span></dt><dd><p>
      Toggles the collection of extended disk metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.25.26.14"><span class="term ">
     vm_network_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM network metrics. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.25.26.15"><span class="term ">
     vm_ping_check_enable
    </span></dt><dd><p>
      Toggles ping checks for checking whether a host is alive. Set to
      <em class="parameter ">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.25.26.16"><span class="term ">
     vm_probation
    </span></dt><dd><p>
      Specifies a period of time (in seconds) in which to suspend metrics from
      a newly-created VM. This is to prevent quickly-obsolete metrics in an
      environment with a high amount of instance churn (VMs created and
      destroyed in rapid succession). The default probation length is 300
      seconds (5 minutes). Set to 0 to disable VM probation. In this case,
      metrics are recorded immediately after a VM is created.
     </p></dd><dt id="id-1.4.5.4.25.26.17"><span class="term ">
     vnic_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting VM network
      metrics. Increase this value to reduce I/O load. If the value is lower
      than the global agent collection period
      (<em class="parameter ">check_frequency</em>), it will be ignored in favor of
      the global collection period.
     </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.25.27"><span class="name"><span class="guimenu ">Deployment</span>
  </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.27">#</a></h4></div><p>
   The monasca component consists of following roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.25.29.1"><span class="term ">monasca-server</span></dt><dd><p>
      monasca server-side components that are deployed by Chef.
      Currently, this only creates keystone resources required by monasca,
      such as users, roles, endpoints, etc. The rest is left to the
      Ansible-based <code class="systemitem">monasca-installer</code> run by the
      <code class="systemitem">monasca-master</code> role.
     </p></dd><dt id="id-1.4.5.4.25.29.2"><span class="term ">monasca-master</span></dt><dd><p>
      Runs the Ansible-based <code class="systemitem">monasca-installer</code> from
      the Crowbar node. The installer deploys the monasca server-side
      components to the node that has the
      <code class="systemitem">monasca-server</code> role assigned to it. These
      components are <code class="systemitem">openstack-monasca-api</code>, and
      <code class="systemitem">openstack-monasca-log-api</code>, as well as all the
      back-end services they use.
     </p></dd><dt id="id-1.4.5.4.25.29.3"><span class="term ">monasca-agent</span></dt><dd><p>
      Deploys <code class="systemitem">openstack-monasca-agent</code> that is
      responsible for sending metrics to <code class="systemitem">monasca-api</code>
      on nodes it is assigned to.
     </p></dd><dt id="id-1.4.5.4.25.29.4"><span class="term ">monasca-log-agent</span></dt><dd><p>
      Deploys <code class="systemitem">openstack-monasca-log-agent</code> responsible
      for sending logs to <code class="systemitem">monasca-log-api</code> on nodes it
      is assigned to.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.25.30"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/depl_barclamp_monasca_node_deployment.png" target="_blank"><img src="images/depl_barclamp_monasca_node_deployment.png" width="" alt="The monasca Barclamp: Node Deployment Example" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.31: </span><span class="name">The monasca Barclamp: Node Deployment Example </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.25.30">#</a></h6></div></div></div><div class="sect1 " id="sec-depl-ostack-octavia"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Octavia</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-octavia">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-octavia</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 provides Octavia Load Balancing as a Service
    (LBaaS). It is used to manage a fleet of virtual machines, containers, or
    bare metal servers—collectively known as amphorae — which it spins up on
    demand.
  </p><div id="id-1.4.5.4.26.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Starting with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 release, we recommend
      running Octavia as a standalone load balancing solution. Neutron LBaaS
      is deprecated in the OpenStack Queens release, and Octavia is its
      replacement. Whenever possible, operators are strongly advised to
      migrate to Octavia.
      For further information on OpenStack Neutron LBaaS deprecation, refer
      to https://wiki.openstack.org/wiki/Neutron/LBaaS/Deprecation.
    </p></div><div id="id-1.4.5.4.26.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Deploying the Octavia barclamp does not automatically run all tasks
      required to complete the migration from Neutron LBaaS.
    </p><p>
      Please refer to <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-octavia-migrate-users" title="12.20.3. Migrating Users to Octavia">Section 12.20.3, “Migrating Users to Octavia”</a>
      for instructions on migrating existing users to allow them to access
      the Octavia load balancer API after the Octavia barclamp is deployed.
    </p><p>
      Please refer to <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-octavia-migrate" title="12.20.4. Migrating Neutron LBaaS Instances to Octavia">Section 12.20.4, “Migrating Neutron LBaaS Instances to Octavia”</a> for
      instructions on migrating existing Neutron LBaaS load balancer instances
      to Octavia and on disabling the deprecated Neutron LBaaS provider
      after the Octavia barclamp is deployed.
    </p></div><p>
    Octavia consists of the following major components:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.26.6.1"><span class="term ">amphorae</span></dt><dd><p>
       Amphorae are the individual virtual machines, containers, or bare metal
       servers that accomplish the delivery of load balancing services to
       tenant application environments.
     </p></dd><dt id="id-1.4.5.4.26.6.2"><span class="term ">controller</span></dt><dd><p>
       The controller is the brains of Octavia. It consists of five
       sub-components as individual daemons. They can be run on
       separate back-end infrastructure.
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           The API Controller is a subcomponent that
           runs Octavia’s API. It takes API requests, performs simple
           sanitizing on them, and ships them off to the controller worker
           over the Oslo messaging bus.
         </p></li><li class="listitem "><p>
           The controller worker subcomponent takes sanitized API commands
           from the API controller and performs the actions necessary to
           fulfill the API request.
         </p></li><li class="listitem "><p>
           The health manager subcomponent monitors individual amphorae
           to ensure they are up and running, and healthy. It
           also handles failover events if amphorae fail unexpectedly.
         </p></li><li class="listitem "><p>
           The housekeeping manager subcomponent cleans up stale (deleted)
           database records, manages the spares pool, and manages amphora
           certificate rotation.
         </p></li><li class="listitem "><p>
           The driver agent subcomponent receives status and statistics
           updates from provider drivers.
         </p></li></ul></div></dd><dt id="id-1.4.5.4.26.6.3"><span class="term ">network</span></dt><dd><p>
       Octavia cannot accomplish what it does without manipulating the
       network environment. Amphorae are spun up with a network
       interface on the <code class="literal">load balancer network</code>. They
       can also plug directly into tenant networks to reach back-end
       pool members, depending on how any given load balancing service
       is deployed by the tenant.
     </p></dd></dl></div><p>The <span class="productname">OpenStack</span> Octavia team has created a glossary of terms used within
   the context of the Octavia project and Neutron LBaaS version 2.
   This glossary is available here: <a class="link" href="https://docs.openstack.org/octavia/rocky/reference/glossary.html" target="_blank">Octavia Glossary</a>.</p><p>In accomplishing its role, Octavia requires OpenStack services
      managed by other barclamps to be already deployed:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Nova - For managing amphora lifecycle and spinning up compute
          resources on demand.
        </p></li><li class="listitem "><p>
          Neutron - For network connectivity between amphorae, tenant
          environments, and external networks.
        </p></li><li class="listitem "><p>
          Barbican - For managing TLS certificates and credentials, when TLS
          session termination is configured on the amphorae.
        </p></li><li class="listitem "><p>
          Keystone - For authentication against the Octavia API, and for
          Octavia to authenticate with other <span class="productname">OpenStack</span> projects.
        </p></li><li class="listitem "><p>
          Glance - For storing the amphora virtual machine image.
        </p></li></ul></div><p>
    The Octavia barclamp component consists of following roles:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.26.11.1"><span class="term ">octavia-api</span></dt><dd><p>
          The Octavia API.
        </p></dd><dt id="id-1.4.5.4.26.11.2"><span class="term ">octavia-backend</span></dt><dd><p>
        Octavia worker, health-manager and house-keeping.
        </p></dd></dl></div><div class="sect2 " id="sec-depl-ostack-octavia-prereqs"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.20.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-octavia-prereqs">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-octavia-prereqs</li></ul></div></div></div></div><p>
      Before configuring and applying the Octavia barclamp, there are a couple
      of prerequisites that have to be prepared: the Neutron management network
      used by the Octavia control plane services to communicate with Amphorae
      and the certificates needed to secure this communication.
    </p><div class="sect3 " id="sec-depl-ostack-octavia-mgmtnet"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.20.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Management network</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-octavia-mgmtnet">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-octavia-mgmtnet</li></ul></div></div></div></div><p>
        Octavia needs a neutron provider network as a management network that the
        controller uses to communicate with the amphorae. The amphorae
        that Octavia deploys have interfaces and IP addresses on this
        network. It’s important that the subnet deployed on this network
        be sufficiently large to allow for the maximum number of
        amphorae and controllers likely to be deployed throughout the
        lifespan of the cloud installation.
      </p><p>
        To configure the Octavia management network, the network configuration
        must be initialized or updated to include an <code class="literal">octavia</code>
        network entry. The Octavia barclamp uses this information to automatically
        create the neutron provider network used for management traffic.
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          If you have not yet deployed Crowbar, add the following configuration to
          <code class="filename">/etc/crowbar/network.json</code>
          to set up the Octavia management network, using the applicable VLAN
          ID, and network address values. If you have already deployed Crowbar, then add
          this configuration to the <span class="guimenu ">Raw</span> view of the Network Barclamp.
         </p><div class="verbatim-wrap"><pre class="screen">"octavia": {
              "conduit": "intf1",
              "vlan": <em class="replaceable ">450</em>,
              "use_vlan": true,
              "add_bridge": false,
              "subnet": "<em class="replaceable ">172.31.0.0</em>",
              "netmask": "<em class="replaceable ">255.255.0.0</em>",
              "broadcast": "<em class="replaceable ">172.31.255.255</em>",
              "ranges": {
                "host": { "start": "<em class="replaceable ">172.31.0.1</em>",
                   "end": "<em class="replaceable ">172.31.0.255</em>" },
                "dhcp": { "start": "<em class="replaceable ">172.31.1.1</em>",
                   "end": "<em class="replaceable ">172.31.255.254</em>" }
              }
        },</pre></div><div id="id-1.4.5.4.26.12.3.4.1.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
             Care should be taken to ensure the IP subnet doesn't overlap with any of those
             configured for the other networks. The chosen VLAN ID must not be used within the SUSE <span class="productname">OpenStack</span> Cloud
             network and not used by neutron (i.e. if deploying neutron with VLAN support - using the plugins
             linuxbridge or openvswitch plus VLAN - ensure that the VLAN ID doesn't overlap with the range of
             VLAN IDs allocated for the <code class="literal">nova-fixed</code> neutron network).
           </p></div><p>
          The <code class="literal">host</code> range will be used to allocate IP addresses to
          the controller nodes where Octavia services are running, so it needs to
          accommodate the maximum number of controller nodes likely to be deployed
          throughout the lifespan of the cloud installation.
         </p><p>
          The <code class="literal">dhcp</code> range will be reflected in the configuration of the
          actual neutron provider network used for Octavia management traffic and its
          size will determine the maximum number of amphorae and therefore the maximum
          number of load balancer instances that can be running at the same time.
         </p><p>
          See <a class="xref" href="sec-depl-adm-inst-crowbar.html#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for detailed instructions
          on how to customize the network configuration.
         </p></li><li class="step "><p>
           If Crowbar is already deployed, it is also necessary to re-apply both the neutron
           Barclamp and the nova Barclamp for the configuration to take effect before applying
           the Octavia Barclamp.
       </p></li></ol></div></div><p>
        Aside from configuring the physical switches to allow VLAN traffic to be correctly forwarded,
        no additional external network configuration is required.
      </p></div><div class="sect3 " id="sec-depl-ostack-octavia-certificates"><div class="titlepage"><div><div><h4 class="title"><span class="number">12.20.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Certificates</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-octavia-certificates">#</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-octavia-certificates</li></ul></div></div></div></div><div id="id-1.4.5.4.26.12.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
          Crowbar will automatically change the filesystem ownership settings for
          these files to match the username and group used by the Octavia
          services, but it is otherwise the responsibility of the cloud
          administrator to ensure that access to these files on the controller
          nodes is properly restricted.
        </p></div><p>
        Octavia administrators set up certificate authorities for the
        two-way TLS authentication used in Octavia for command and
	control of amphorae. For more information, see the 
	<code class="literal">Creating the Certificate Authorities</code> section
	of the <a class="link" href="https://docs.openstack.org/octavia/stein/admin/guides/certificates.html" target="_blank">Octavia
        Certificate Configuration Guide</a> . Note that the <code class="literal">Configuring
	Octavia</code> section of that guide does not apply as the
        barclamp will configure Octavia.
      </p><p>
        The following certificates need to be generated and stored on all
        controller nodes where Octavia is deployed under
        <code class="literal">/etc/octavia/certs</code>, in a relative path matching the
        certificate location attribute values configured in the Octavia barclamp:
      </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.26.12.4.5.1"><span class="term ">Server CA certificate</span></dt><dd><p>
           The Certificate Authority (CA) certificate that is used by
           the Octavia controller(s) to sign the generated Amphora server
           certificates. The Octavia control plane services also validate
           the server certificates presented by Amphorae during the TLS handshake
           against this CA certificate.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.2"><span class="term ">Server CA key</span></dt><dd><p>
           The private key associated with the server CA certificate. This key
           must be encrypted with a non-empty passphrase that also needs to
           be provided as a separate barclamp attribute. The private key is
           required alongside the server CA certificate on the Octavia
           controller(s), to sign the generated Amphora server certificates.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.3"><span class="term ">Passphrase</span></dt><dd><p>
           The passphrase used to encrypt the server CA key.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.4"><span class="term ">Client CA certificate</span></dt><dd><p>
           The CA certificate used to sign the client certificates installed on
           the Octavia controller nodes and presented by Octavia control plane
           services during the TLS handshake. This CA certificate is stored on the
           Amphorae, which use it to validate the client certificate presented by
           the Octavia control plane services during the TLS handshake.
           The same CA certificate may be used for both client and server roles,
           but this is perceived as a security weakness and recommended against,
           as a server certificate from an amphora could be used to impersonate a
           controller.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.5"><span class="term ">Client certificate concat key</span></dt><dd><p>
           The client certificate, signed with the client CA certificate, bundled
           together with the client certificate key, that is presented by
           the Octavia control plane services during the TLS handshake.
         </p></dd></dl></div><p>
        All Octavia barclamp attributes listed above, with the exception of the
        pasphrase are paths relative to <code class="literal">/etc/octavia/certs</code>.
        The required certificates must be present in their corresponding locations
        on all controller nodes where the Octavia barclamp will be deployed.
      </p></div></div><div class="sect2 " id="sec-depl-ostack-octavia-raw-mode"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.20.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Barclmap raw mode</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-octavia-raw-mode">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-octavia-raw-mode</li></ul></div></div></div></div><p>
      If a user wants to be able to debug or get access to an amphora,
      you can provide an SSH keyname to the barclamp via the
      <code class="literal">raw mode</code>. This is a keyname to a key that has
      been uploaded to openstack. For example:
    </p><div class="verbatim-wrap"><pre class="screen">      openstack keypair create --public-key /etc/octavia/.ssh/id_rsa_amphora.pub octavia_key</pre></div><p>
      Note that the keypair has to be owned by the octavia user.
    </p></div><div class="sect2 " id="sec-depl-ostack-octavia-migrate-users"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.20.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating Users to Octavia</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-octavia-migrate-users">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-octavia-migrate-users</li></ul></div></div></div></div><div id="id-1.4.5.4.26.14.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        This behaviour is not backwards compatible with the legacy Neutron LBaaS
        API policy, as non-admin OpenStack users will not be allowed to run
        <code class="literal">openstack loadbalancer</code> CLI commands or use
        the load balancer horizon dashboard unless their accounts are explicitly
        reconfigured to be associated with one or more of these roles.
      </p></div><div id="id-1.4.5.4.26.14.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        Please follow the instructions documented under <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-heat-delegated-roles" title="12.12.1. Enabling Identity Trusts Authorization (Optional)">Section 12.12.1, “Enabling Identity Trusts Authorization (Optional)”</a>
        on updating the trusts roles in the heat barclamp configuration. This is
        required to configure heat to use the correct roles when communicating with
        the Octavia API and manage load balancers.
      </p></div><p>
      Octavia employs a set of specialized roles to control access to the
      load balancer API:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.26.14.5.1"><span class="term ">load-balancer_observer</span></dt><dd><p>
         User has access to load-balancer read-only APIs.
       </p></dd><dt id="id-1.4.5.4.26.14.5.2"><span class="term ">load-balancer_global_observer</span></dt><dd><p>
         User has access to load-balancer read-only APIs including resources
         owned by others.
       </p></dd><dt id="id-1.4.5.4.26.14.5.3"><span class="term ">load-balancer_member</span></dt><dd><p>
         User has access to load-balancer read and write APIs.
       </p></dd><dt id="id-1.4.5.4.26.14.5.4"><span class="term ">load-balancer_quota_admin</span></dt><dd><p>
         User is considered an admin for quota APIs only.
       </p></dd><dt id="id-1.4.5.4.26.14.5.5"><span class="term ">load-balancer_admin</span></dt><dd><p>
         User is considered an admin for all load-balancer APIs including
         resources owned by others.
       </p></dd></dl></div></div><div class="sect2 " id="sec-depl-ostack-octavia-migrate"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.20.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating Neutron LBaaS Instances to Octavia</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-octavia-migrate">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-octavia-migrate</li></ul></div></div></div></div><div id="id-1.4.5.4.26.15.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        Disabling LBaaS or switching the LBaaS provider in the Neutron barclamp
        to Octavia is not possible while there are load balancers still running
        under the previous Neutron LBaaS provider and will result in a Neutron
        barclamp redeployment failure. To avoid this, ensure that load balancer
        instances that are running under the old provider are either migrated
        or deleted.
      </p></div><p>
      The migration procedure documented in this section is only relevant if
      LBaaS was already enabled in the Neutron barclamp, with either the HAProxy
      or H5 provider configured, before Octavia was deployed.
      The procedure should be followed by operators to migrate and/or delete all
      load balancer instances using the Neutron LBaaS provider that are still
      active, and concluded the switch to Octavia by reconfiguring or disabling
      the deprecated Neutron LBaaS feature.
    </p><p>
      Octavia is a replacement for the Neutron LBaaS feature, that is deprecated
      in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 release. However, deploying the
      Octavia barclamp does not automatically disable the legacy Neutron LBaaS
      provider, if one is already configured in the Neutron barclamp.
    </p><p>
      Both Octavia and Neutron LBaaS need to be enabled at the same time,
      to facilitate the load balancer migration process. This way, operators
      have a migration path they can use to gradually decommission Neutron LBaaS
      load balancers that use the HAProxy or F5 provider and replace them with
      Octavia load balancers.
    </p><p>
      With Octavia deployed and Neutron LBaaS enabled, both load balancer
      providers can be used simultaneously:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          The (deprecated) <code class="literal">neutron lbaas-...</code> CLI
          commands can be used to manage load balancer instances using the
          legacy Neutron LBaaS provider configured in the Neutron barclamp.
          Note that the legacy Neutron LBaaS instances will not be visible in
          the load balancer horizon dashboard.
        </p></li><li class="listitem "><p>
          The <code class="literal">openstack loadbalancer</code> CLI commands as well
          as the load balancer horizon dashboard can be used to manage Octavia
          load balancers. Also note that OpenStack users are required to have
          special roles associated with their projects to be able to access
          the Octavia API, as covered in
          <a class="xref" href="cha-depl-ostack.html#sec-depl-ostack-octavia-migrate-users" title="12.20.3. Migrating Users to Octavia">Section 12.20.3, “Migrating Users to Octavia”</a>.
        </p></li></ul></div><div id="id-1.4.5.4.26.15.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        (Optional): to prevent regular users from creating or changing the
        configuration of currently running legacy Neutron LBaaS load balancer
        instances during the migration process, the neutron API policy should be
        temporarily changed to prevent these operations. For this purpose, a
        <code class="literal">neutron-lbaas.json</code> file can be created in the
        <code class="literal">/etc/neutron/policy.d</code> folder on all neutron-server
        nodes (no service restart required):
      </p><div class="verbatim-wrap"><pre class="screen">mkdir /etc/neutron/policy.d
cat &gt; /etc/neutron/policy.d/neutron-lbaas.json &lt;&lt;EOF
{
  "context_is_admin": "role:admin",
  "context_is_advsvc": "role:advsvc",
  "default": "rule:admin_or_owner",
  "create_loadbalancer": "rule:admin_only",
  "update_loadbalancer": "rule:admin_only",
  "get_loadbalancer": "!",
  "delete_loadbalancer": "rule:admin_only",
  "create_listener": "rule:admin_only",
  "get_listener": "",
  "delete_listener": "rule:admin_only",
  "update_listener": "rule:admin_only",
  "create_pool": "rule:admin_only",
  "get_pool": "",
  "delete_pool": "rule:admin_only",
  "update_pool": "rule:admin_only",
  "create_healthmonitor": "rule:admin_only",
  "get_healthmonitor": "",
  "update_healthmonitor": "rule:admin_only",
  "delete_healthmonitor": "rule:admin_only",
  "create_pool_member": "rule:admin_only",
  "get_pool_member": "",
  "update_pool_member": "rule:admin_only",
  "delete_pool_member": "rule:admin_only"
}
EOF
chown -R root:neutron /etc/neutron/policy.d
chmod 640 /etc/neutron/policy.d/neutron-lbaas.json</pre></div><p>
	If users need to create or change the configuration of currently running
	legacy Neutron LBaaS load balancer instances during the migration process,
	Create a <code class="filename">neutron-lbaas.json</code> file in the
        <code class="filename">/etc/neutron/policy.d</code> folder on all neutron-server nodes.
	The <code class="filename">neutron-lbaas.json</code> file should be empty, then
	restart the neutron service via <code class="command">systemctl restart openstack-neutron.service</code>
	on all neutron-server nodes.
      </p></div><p>
      With all of the above in check, the actual migration process consists of
      replacing Neutron LBaaS instances with Octavia instances. There are many
      different ways to accomplish this, depending on the size and purpose of
      the cloud deployment, the number of load balancers that need to be
      migrated, the project and user configuration etc. This section only gives
      a few pointers and recommendations on how to approach this tasks, but
      the actual execution needs to be attuned to each particular situation.
    </p><p>
      Migrating a single load balancer instance is generally comprised of these
      steps:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Use the <code class="literal">neutron lbaas-...</code> CLI to
          retrieve information about the load balancer configuration,
          including the complete set of related listener, pool, member
          and health monitor instances
        </p></li><li class="listitem "><p>
          Use the <code class="literal">openstack loadbalancer</code> CLI or the
          load balancer horizon dashboard to create an Octavia load
          balancer and its associated listener, pool, member and health
          monitor instances to accurately match the project and Neutron LBaaS
          load balancer configuration extracted during the previous step.
          Note that the Octavia load balancer instance and the Neutron
          LBaaS instance cannot share the same VIP address value if
          both instances are running at the same time. This could be a
          problem, if the load balancer VIP address is accessed directly
          (i.e. as opposed to being accessed via a floating IP).
          In this case, the legacy load balancer instance needs to be
          deleted first, which incurs a longer interruption in service
          availability.
        </p></li><li class="listitem "><p>
          Once the Octavia instance is up and running, if a floating IP
          is associated with the Neutron LBaaS load balancer VIP
          address, re-associate the floating IP with the Octavia load
          balancer VIP address. Using a floating IP has the advantage
          that the migration can be performed with minimal downtime.
          If the load balancer VIP address needs to be accessed directly
          (e.g. from another VM attached to the same Neutron network or
          router), then all the remote affected services need to be
          reconfigured to use the new VIP address.
        </p></li><li class="listitem "><p>
          The two load balancer instances can continue to run in parallel,
          while the operator or owner verifies the Octavia load balancer
          operation. If any problems occur, the change can be reverted by
          undoing the actions performed during the previous step. If a
          floating IP is involved, this could be as simple as switching it
          back to the Neutron LBaaS load balancer instance.
        </p></li><li class="listitem "><p>
          When it's safe, delete the Neutron LBaaS load balancer instance,
          along with all its related listener, pool, member and health
          monitor instances.
        </p></li></ul></div><p>
      Depending on the number of load balancer instances that need to
      be migrated and the complexity of the overall setup that they are
      integrated into, the migration may be performed by the cloud
      operators, the owners themselves, or a combination of both. It is
      generally recommended that the load balancer owners have some involvement
      in this process or at least be notified of this migration procedure,
      because the load balancer migration is not an entirely seamless operation.
      One or more of the load balancer configuration attributes listed below may
      change during the migration and there may be other operational components,
      managed by OpenStack or otherwise (e.g. OpenStack heat stacks,
      configuration management scripts, database entries or non-persistent
      application states, etc.), that only the owner(s) may be aware
      of:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          The load balancer UUID value, along with the UUID values of every
          other related object (listeners, pools, members etc.). Even though the
          name values may be preserved by the migration, the UUID values will be
          different.
        </p></li><li class="listitem "><p>
          The load balancer VIP address will change during a non-disruptive
          migration. This is especially relevant if there is no floating IP
          associated with the previous VIP address.
        </p></li></ul></div><p>
      When the load balancer migration is complete, the Neutron LBaaS provider
      can either be switched to Octavia or turned off entirely in the Neutron
      barclamp, to finalize the migration process.
    </p><p>
      The only advantage of having Octavia configured as the Neutron LBaaS
      provider is that it continues to allow users to manage Octavia load
      balancers via the deprecated <code class="literal">neutron lbaas-...</code> CLI, but
      it is otherwise recommended to disable LBaaS in the Neutron barclamp.
    </p></div></div><div class="sect1 " id="sec-depl-ostack-ironic"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying ironic (optional)</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-ironic">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ironic</li></ul></div></div></div></div><p>
  Ironic is the <span class="productname">OpenStack</span> bare metal service for provisioning physical
  machines. Refer to the <span class="productname">OpenStack</span> <a class="link" href="https://docs.openstack.org/ironic/latest/" target="_blank">developer and admin
  manual</a> for information on drivers, and administering ironic.
 </p><p>
  Deploying the ironic barclamp is done in five steps:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
Set options in the Custom view of the barclamp.
</p></li><li class="listitem "><p>
List the <code class="literal">enabled_drivers</code> in the Raw view.
</p></li><li class="listitem "><p>
Configure the ironic network in <code class="filename">network.json</code>.
</p></li><li class="listitem "><p>
Apply the barclamp to a Control Node.
</p></li><li class="listitem "><p>
          Apply the <span class="guimenu ">nova-compute-ironic</span> role to the same node
          you applied the ironic barclamp to, in place of the other
          <span class="guimenu ">nova-compute-*</span> roles.
</p></li></ul></div><div class="sect2 " id="sec-depl-ostack-ironic-custom-view"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.21.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Custom View Options</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-ironic-custom-view">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ironic-custom-view</li></ul></div></div></div></div><p>
        Currently, there are two options in the Custom view of the barclamp.
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.27.5.3.1"><span class="term "><span class="guimenu ">Enable automated node cleaning</span>
    </span></dt><dd><p>
         Node cleaning prepares the node to accept a new workload. When you set this to <span class="guimenu ">true</span>,
         ironic collects a list of cleaning steps from the Power, Deploy, Management, and RAID
         interfaces of the driver assigned to the node. ironic automatically prioritizes and
         executes the cleaning steps, and changes the state of the node to "cleaning". When cleaning
         is complete the state becomes "available". After a new workload is assigned to the machine
         its state changes to "active".
     </p><p>
        <span class="guimenu ">false</span> disables automatic cleaning, and you must configure and apply
        node cleaning manually. This requires the admin to create and prioritize the cleaning steps,
        and to set up a cleaning network. Apply manual cleaning when you have long-running or
        destructive tasks that you wish to monitor and control more closely.
        (See <a class="link" href="https://docs.openstack.org/ironic/latest/admin/cleaning.html" target="_blank">Node Cleaning</a>.)
        </p></dd><dt id="id-1.4.5.4.27.5.3.2"><span class="term "><span class="guimenu ">SSL Support: Protocol</span>
    </span></dt><dd><p>
         SSL support is not yet enabled, so the only option is <span class="guimenu ">HTTP</span>.
        </p></dd></dl></div><div class="figure" id="id-1.4.5.4.27.5.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/ironic-1.png" target="_blank"><img src="images/ironic-1.png" width="" alt="The ironic barclamp Custom view" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 12.32: </span><span class="name">The ironic barclamp Custom view </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.27.5.4">#</a></h6></div></div></div><div class="sect2 " id="sec-depl-ostack-ironic-drivers"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.21.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ironic Drivers</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-ironic-drivers">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-ironic-drivers</li></ul></div></div></div></div><p>
    You must enter the Raw view of barclamp and specify a list of drivers to load during service initialization.
    <code class="literal">pxe_ipmitool</code> is the recommended default ironic driver. It uses the
    Intelligent Platform Management Interface (IPMI) to control the power state
    of your bare metal machines, creates the appropriate PXE configurations
    to start them, and then performs the steps to provision and configure the machines.</p><div class="verbatim-wrap"><pre class="screen">"enabled_drivers": ["pxe_ipmitool"],</pre></div><p>
     See <a class="link" href="https://docs.openstack.org/ironic/latest/admin/drivers.html" target="_blank">ironic
     Drivers</a> for more information.
    </p></div><div class="sect2 " id="id-1.4.5.4.27.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.21.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example ironic Network Configuration</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.27.7">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
          This is a complete ironic <code class="filename">network.json</code> example, using
          the default <code class="filename">network.json</code>, followed by a diff that shows
          the ironic-specific configurations.</p><div class="example" id="ex-ironic-network-json"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 12.1: </span><span class="name">Example network.json </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#ex-ironic-network-json">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
  "start_up_delay": 30,
  "enable_rx_offloading": true,
  "enable_tx_offloading": true,
  "mode": "single",
  "teaming": {
    "mode": 1
  },
  "interface_map": [
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R610"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01.1/0000:01:00.0",
        "0000:00/0000:00:01.1/0000.01:00.1",
        "0000:00/0000:00:01.0/0000:02:00.0",
        "0000:00/0000:00:01.0/0000:02:00.1"
      ],
      "pattern": "PowerEdge R620"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R710"
    },
    {
      "bus_order": [
        "0000:00/0000:00:04",
        "0000:00/0000:00:02"
      ],
      "pattern": "PowerEdge C6145"
    },
    {
      "bus_order": [
        "0000:00/0000:00:03.0/0000:01:00.0",
        "0000:00/0000:00:03.0/0000:01:00.1",
        "0000:00/0000:00:1c.4/0000:06:00.0",
        "0000:00/0000:00:1c.4/0000:06:00.1"
      ],
      "pattern": "PowerEdge R730xd"
    },
    {
      "bus_order": [
        "0000:00/0000:00:1c",
        "0000:00/0000:00:07",
        "0000:00/0000:00:09",
        "0000:00/0000:00:01"
      ],
      "pattern": "PowerEdge C2100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03",
        "0000:00/0000:00:07"
      ],
      "pattern": "C6100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:02"
      ],
      "pattern": "product"
    }
  ],
  "conduit_map": [
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        }
      },
      "pattern": "team/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "dual/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "single/.*/.*ironic.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "single/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1"
          ]
        }
      },
      "pattern": ".*/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "mode/1g_adpt_count/role"
    }
  ],
  "networks": {
    "ironic": {
      "conduit": "intf3",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-ironic",
      "subnet": "192.168.128.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.128.255",
      "router": "192.168.128.1",
      "router_pref": 50,
      "ranges": {
        "admin": {
          "start": "192.168.128.10",
          "end": "192.168.128.11"
        },
        "dhcp": {
          "start": "192.168.128.21",
          "end": "192.168.128.254"
        }
      },
      "mtu": 1500
    },
    "storage": {
      "conduit": "intf1",
      "vlan": 200,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.125.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.125.255",
      "ranges": {
        "host": {
          "start": "192.168.125.10",
          "end": "192.168.125.239"
        }
      }
    },
    "public": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.122.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.122.255",
      "router": "192.168.122.1",
      "router_pref": 5,
      "ranges": {
        "host": {
          "start": "192.168.122.2",
          "end": "192.168.122.127"
        }
      },
      "mtu": 1500
    },
    "nova_fixed": {
      "conduit": "intf1",
      "vlan": 500,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-fixed",
      "subnet": "192.168.123.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.123.255",
      "router": "192.168.123.1",
      "router_pref": 20,
      "ranges": {
        "dhcp": {
          "start": "192.168.123.1",
          "end": "192.168.123.254"
        }
      },
      "mtu": 1500
    },
    "nova_floating": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-public",
      "subnet": "192.168.122.128",
      "netmask": "255.255.255.128",
      "broadcast": "192.168.122.255",
      "ranges": {
        "host": {
          "start": "192.168.122.129",
          "end": "192.168.122.254"
        }
      },
      "mtu": 1500
    },
    "bmc": {
      "conduit": "bmc",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.162",
          "end": "192.168.124.240"
        }
      },
      "router": "192.168.124.1"
    },
    "bmc_vlan": {
      "conduit": "intf2",
      "vlan": 100,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.161",
          "end": "192.168.124.161"
        }
      }
    },
    "os_sdn": {
      "conduit": "intf1",
      "vlan": 400,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.130.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.130.255",
      "ranges": {
        "host": {
          "start": "192.168.130.10",
          "end": "192.168.130.254"
        }
      }
    },
    "admin": {
      "conduit": "intf0",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "router": "192.168.124.1",
      "router_pref": 10,
      "ranges": {
        "admin": {
          "start": "192.168.124.10",
          "end": "192.168.124.11"
        },
        "dhcp": {
          "start": "192.168.124.21",
          "end": "192.168.124.80"
        },
        "host": {
          "start": "192.168.124.81",
          "end": "192.168.124.160"
        },
        "switch": {
          "start": "192.168.124.241",
          "end": "192.168.124.250"
        }
      }
    }
  }
}</pre></div></div></div><div class="complex-example"><div class="example" id="ex-ironic-network-json-diff"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 12.2: </span><span class="name">Diff of ironic Configuration </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#ex-ironic-network-json-diff">#</a></h6></div><div class="example-contents"><p>
      This diff should help you separate the ironic items from the default
      <code class="filename">network.json</code>.
</p><div class="verbatim-wrap"><pre class="screen">--- network.json        2017-06-07 09:22:38.614557114 +0200
+++ ironic_network.json 2017-06-05 12:01:15.927028019 +0200
@@ -91,6 +91,12 @@
             "1g1",
             "1g2"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1",
+            "1g2"
+          ]
         }
       },
       "pattern": "team/.*/.*"
@@ -111,6 +117,11 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
         }
       },
       "pattern": "dual/.*/.*"
@@ -131,6 +142,36 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
+        }
+      },
+      "pattern": "single/.*/.*ironic.*"
+    },
+    {
+      "conduit_list": {
+        "intf0": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf1": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf2": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "single/.*/.*"
@@ -151,6 +192,11 @@
           "if_list": [
             "1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1"
+          ]
         }
       },
       "pattern": ".*/.*/.*"
@@ -171,12 +217,41 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "mode/1g_adpt_count/role"
     }
   ],
   "networks": {
+    "ironic": {
+      "conduit": "intf3",
+      "vlan": 100,
+      "use_vlan": false,
+      "add_bridge": false,
+      "add_ovs_bridge": false,
+      "bridge_name": "br-ironic",
+      "subnet": "192.168.128.0",
+      "netmask": "255.255.255.0",
+      "broadcast": "192.168.128.255",
+      "router": "192.168.128.1",
+      "router_pref": 50,
+      "ranges": {
+        "admin": {
+          "start": "192.168.128.10",
+          "end": "192.168.128.11"
+        },
+        "dhcp": {
+          "start": "192.168.128.21",
+          "end": "192.168.128.254"
+        }
+      },
+      "mtu": 1500
+    },
     "storage": {
       "conduit": "intf1",
       "vlan": 200,</pre></div></div></div></div></div></div><div class="sect1 " id="sec-depl-ostack-final"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.22 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to Proceed</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-ostack-final">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-ostack-final</li></ul></div></div></div></div><p>
   With a successful deployment of the <span class="productname">OpenStack</span> Dashboard, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   installation is finished. To be able to test your setup by starting an
   instance one last step remains to be done—uploading an image to the
   glance component. Refer to the <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">User Guide</em></em>, chapter <em class="citetitle ">Manage
   images</em>

   for instructions. Images for SUSE <span class="productname">OpenStack</span> Cloud can be built in SUSE Studio. Refer to
   the <em class="citetitle ">Supplement to <em class="citetitle ">Administrator Guide</em> and <em class="citetitle ">User Guide</em></em>, section <em class="citetitle ">Building Images with
   SUSE Studio</em>.
  </p><p>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.—refer to the <em class="citetitle ">Administrator Guide</em> for details. The default
   credentials for the <span class="productname">OpenStack</span> Dashboard are user name <code class="literal">admin</code>
   and password <code class="literal">crowbar</code>.
  </p></div><div class="sect1 " id="crow-ses-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.23 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage integration</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#crow-ses-integration">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>crow-ses-integration</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports integration with SUSE Enterprise Storage (SES), enabling Ceph block
    storage as well as image storage services in SUSE <span class="productname">OpenStack</span> Cloud.
   </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.3"><span class="name">Enabling SES Integration
   </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.29.3">#</a></h3></div><p>
    To enable SES integration on Crowbar, an SES configuration file must be
    uploaded to Crowbar. SES integration functionality is included in the
    <code class="literal">crowbar-core</code> package and can be used with the Crowbar UI
    or CLI (<code class="literal">crowbarctl</code>). The SES configuration file
    describes various aspects of the Ceph environment, and keyrings for each
    user and pool created in the Ceph environment for SUSE <span class="productname">OpenStack</span> Cloud Crowbar services.
   </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.5"><span class="name">SES Configuration
   </span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.29.5">#</a></h3></div><p>
    For SES deployments that are version 5.5 and higher, a Salt runner is used
    to create all the users and pools. It also generates a YAML
    configuration that is needed to integrate with SUSE <span class="productname">OpenStack</span> Cloud. The integration
    runner creates separate users for cinder, cinder backup
    (not used by Crowbar currently) and glance. Both the cinder and
    nova services have the same user, because cinder needs
    access to create objects that nova uses.
   </p><p>
    Configure SES with the following steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Login as <code class="literal">root</code> and run the SES 5.5 Salt runner on the
      Salt admin host.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run --out=yaml openstack.integrate prefix=mycloud</pre></div><p>
      The prefix parameter allows pools to be created with the specified
      prefix. By using different prefix parameters, multiple cloud deployments
      can support different users and pools on the same SES deployment.
     </p></li><li class="step "><p>
      YAML output is created with content similar to the following
      example, and can be redirected to a file using the redirect
      operator <code class="literal">&gt;</code> or using the additional
      parameter <code class="literal">--out-file=&lt;filename&gt;</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ceph_conf:
     cluster_network: 10.84.56.0/21
     fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
     mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
     mon_initial_members: ses-osd1, ses-osd2, ses-osd3
     public_network: 10.84.56.0/21
cinder:
     key: ABCDEFGaxefEMxAAW4zp2My/5HjoST2Y87654321==
     rbd_store_pool: mycloud-cinder
     rbd_store_user: cinder
cinder-backup:
     key: AQBb8hdbrY2bNRAAqJC2ZzR5Q4yrionh7V5PkQ==
     rbd_store_pool: mycloud-backups
     rbd_store_user: cinder-backup
glance:
     key: AQD9eYRachg1NxAAiT6Hw/xYDA1vwSWLItLpgA==
     rbd_store_pool: mycloud-glance
     rbd_store_user: glance
nova:
     rbd_store_pool: mycloud-nova
radosgw_urls:
     - http://10.84.56.7:80/swift/v1
     - http://10.84.56.8:80/swift/v1</pre></div></li><li class="step "><p>
      Upload the generated YAML file to Crowbar using the UI or
      <code class="literal">crowbarctl</code> CLI.
     </p></li><li class="step "><p>
      If the Salt runner is not available, you must manually create pools and
      users to allow SUSE <span class="productname">OpenStack</span> Cloud services to use the SES/Ceph cluster. Pools and
      users must be created for cinder, nova, and
      glance. Instructions for creating and managing pools, users and keyrings
      can be found in the <a class="link" href="https://documentation.suse.com/ses/5.5/" target="_blank">
      SUSE Enterprise Storage</a> Administration Guide in the Key Management
      section.
     </p><p>
      After the required pools and users are set up on the SUSE Enterprise Storage/Ceph
      cluster, create an SES configuration file in YAML format (using the
      example template above). Upload this file to Crowbar using the UI or
      <code class="literal">crowbarctl</code> CLI.
     </p></li><li class="step "><p>
      As indicated above, the SES configuration file can be uploaded to Crowbar
      using the UI or <code class="literal">crowbarctl</code> CLI.
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        From the main Crowbar UI, the upload page is under
        <span class="guimenu ">Utilities</span> › <span class="guimenu ">SUSE Enterprise
        Storage</span>.
       </p><p>
        If a configuration is already stored in Crowbar, it will be visible in
        the upload page. A newly uploaded configuration will replace existing
        one. The new configuration will be applied to the cloud on the next
        <code class="literal">chef-client</code> run. There is no need to reapply
        proposals.
       </p><p>
        Configurations can also be deleted from Crowbar. After deleting a
        configuration, you must manually update and reapply all proposals that
        used SES integration.
       </p></li><li class="listitem "><p>
        With the <code class="literal">crowbarctl</code> CLI, the command <code class="command">crowbarctl ses
        upload <em class="replaceable ">FILE</em></code> accepts a path to the
        SES configuration file.
       </p></li></ul></div></li></ol></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.9"><span class="name">Cloud Service Configuration</span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.29.9">#</a></h3></div><p>
    SES integration with SUSE <span class="productname">OpenStack</span> Cloud services is implemented with relevant Barclamps
    and installed with the <code class="literal">crowbar-openstack</code> package.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.29.11.1"><span class="term ">glance</span></dt><dd><p>
       Set <code class="literal">Use SES Configuration</code> to <code class="literal">true</code>
       under <code class="literal">RADOS Store Parameters</code>. The glance barclamp
       pulls the uploaded SES configuration from Crowbar when applying the
       glance proposal and on <code class="literal">chef-client</code> runs. If the SES
       configuration is uploaded before the glance proposal is created,
       <code class="literal">Use SES Configuration</code> is enabled automatically
       upon proposal creation.
      </p></dd><dt id="id-1.4.5.4.29.11.2"><span class="term ">cinder</span></dt><dd><p>
       Create a new RADOS backend and set <code class="literal">Use SES
       Configuration</code> to <code class="literal">true</code>. The cinder
       barclamp pulls the uploaded SES configuration from Crowbar when applying the
       cinder proposal and on <code class="literal">chef-client</code> runs. If
       the SES configuration was uploaded before the cinder proposal
       was created, a <code class="literal">ses-ceph</code> RADOS backend is created
       automatically on proposal creation with <code class="literal">Use SES
       Configuration</code> already enabled.
      </p></dd><dt id="id-1.4.5.4.29.11.3"><span class="term ">nova</span></dt><dd><p>
       To connect with volumes stores in SES, nova uses the configuration
       from the cinder barclamp.
       For ephemeral storage, nova re-uses the <code class="literal">rbd_store_user</code>
       and <code class="literal">key</code> from cinder but has a separate <code class="literal">rbd_store_pool</code>
       defined in the SES configuration. Ephemeral storage on SES can be
       enabled or disabled by setting <code class="option">Use Ceph RBD Ephemeral Backend</code>
       in nova proposal. In new deployments it is enabled by default.
       In existing ones it is disabled for compatibility reasons.
      </p></dd></dl></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.12"><span class="name"><span class="productname">RADOS Gateway</span> Integration</span><a title="Permalink" class="permalink" href="cha-depl-ostack.html#id-1.4.5.4.29.12">#</a></h3></div><p>
      Besides block storage, the SES cluster can also be used as a swift
      replacement for object storage. If <code class="literal">radosgw_urls</code> section is present
      in uploaded SES configuration, first of the URLs is registered
      in the keystone catalog as the "swift"/"object-store" service. Some
      configuration is needed on SES side to fully integrate with keystone
      auth.
      If SES integration is enabled on a cloud with swift deployed,
      SES object storage service will get higher priority by default. To
      override this and use swift for object storage instead, remove
      <code class="literal">radosgw_urls</code> section from the SES configuration file and re-upload it
      to Crowbar. Re-apply swift proposal or wait for next periodic
      chef-client run to make changes effective.
    </p></div><div class="sect1 " id="sec-depl-services"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.24 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-depl-services">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-depl-services</li></ul></div></div></div></div><p>
   The following table lists all roles (as defined in the barclamps), and their
   associated services. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, this list is work in
   progress. Services can be manually started and stopped with the commands
   <code class="command">systemctl start <em class="replaceable ">SERVICE</em></code> and
   <code class="command">systemctl stop <em class="replaceable ">SERVICE</em></code>.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
       <p>
        Role
       </p>
      </th><th>
       <p>
        Service
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        ceilometer-agent
       </p>
      </td><td><code class="systemitem">
       openstack-ceilometer-agent-compute
      </code>
      </td></tr><tr><td rowspan="2">
       <p>
        ceilometer-central
       </p>
       <p>
        ceilometer-server
       </p>
       <p>
        ceilometer-swift-proxy-middleware
       </p>
      </td><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-agent-notification
        </code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-ceilometer-agent-central </code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        cinder-controller
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-cinder-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-cinder-scheduler</code>
       </p>
      </td></tr><tr><td>
       <p>
        cinder-volume
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-cinder-volume</code>
       </p>
      </td></tr><tr><td>
       <p>
        database-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">postgresql</code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        glance-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-glance-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-glance-registry</code>
       </p>
      </td></tr><tr><td rowspan="4">
       <p>
        heat-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-heat-api-cfn</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-heat-api-cloudwatch</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-heat-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-heat-engine</code>
       </p>
      </td></tr><tr><td>
       <p>
        horizon
       </p>
      </td><td>
       <p>
        <code class="systemitem">apache2</code>
       </p>
      </td></tr><tr><td>
       <p>
        keystone-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-keystone</code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        manila-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-manila-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-manila-scheduler</code>
       </p>
      </td></tr><tr><td>
       <p>
        manila-share
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-manila-share</code>
       </p>
      </td></tr><tr><td>
       <p>
        neutron-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-neutron</code>
       </p>
      </td></tr><tr><td rowspan="2">
       <p>
        nova-compute-*
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-nova-compute</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem"> openstack-neutron-openvswitch-agent
        </code> (when neutron is deployed with openvswitch)
       </p>
      </td></tr><tr><td rowspan="6">
       <p>
        nova-controller
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-nova-api</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-cert</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-conductor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-novncproxy</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-objectstore</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-nova-scheduler</code>
       </p>
      </td></tr><tr><td>
       <p>
        rabbitmq-server
       </p>
      </td><td>
       <p>
        <code class="systemitem">rabbitmq-server</code>
       </p>
      </td></tr><tr><td>
       <p>
        swift-dispersion
       </p>
      </td><td>
       <p>
        none
       </p>
      </td></tr><tr><td>
       <p>
        swift-proxy
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-swift-proxy</code>
       </p>
      </td></tr><tr><td>
       <p>
        swift-ring-compute
       </p>
      </td><td>
       <p>
        none
       </p>
      </td></tr><tr><td rowspan="14">
       <p>
        swift-storage
       </p>
      </td><td>
       <p>
        <code class="systemitem">openstack-swift-account-auditor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-account-reaper</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-account-replicator</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-account</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-auditor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-replicator</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-sync</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container-updater</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-container</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-auditor</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-expirer</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-replicator</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object-updater</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object</code>
       </p>
      </td></tr></tbody></table></div></div><div class="sect1 " id="sec-deploy-crowbatch-description"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.25 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Crowbar Batch Command</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-deploy-crowbatch-description">#</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-description</li></ul></div></div></div></div><p>
   This is the documentation for the <code class="command">crowbar batch</code>
   subcommand.
  </p><p>
   <code class="command">crowbar batch</code> provides a quick way of creating, updating,
   and applying Crowbar proposals. It can be used to:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Accurately capture the configuration of an existing Crowbar environment.
    </p></li><li class="listitem "><p>
     Drive Crowbar to build a complete new environment from scratch.
    </p></li><li class="listitem "><p>
     Capture one SUSE <span class="productname">OpenStack</span> Cloud environment and then reproduce it on another set of
     hardware (provided hardware and network configuration match to an
     appropriate extent).
    </p></li><li class="listitem "><p>
     Automatically update existing proposals.
    </p></li></ul></div><p>
   As the name suggests, <code class="command">crowbar batch</code> is intended to be run
   in <span class="quote">“<span class="quote ">batch mode</span>”</span> that is mostly unattended. It has two modes of
   operation:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.31.6.1"><span class="term ">crowbar batch export
    </span></dt><dd><p>
      Exports a YAML file which describes existing proposals and how their
      parameters deviate from the default proposal values for that barclamp.
     </p></dd><dt id="id-1.4.5.4.31.6.2"><span class="term ">crowbar batch build
    </span></dt><dd><p>
      Imports a YAML file in the same format as above. Uses it to build new
      proposals if they do not yet exist. Updates the existing proposals so
      that their parameters match those given in the YAML file.
     </p></dd></dl></div><div class="sect2 " id="sec-deploy-crowbatch-yaml"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.25.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">YAML file format</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-deploy-crowbatch-yaml">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-yaml</li></ul></div></div></div></div><p>
    Here is an example YAML file. At the top-level there is a proposals array,
    each entry of which is a hash representing a proposal:
   </p><div class="verbatim-wrap"><pre class="screen">proposals:
- barclamp: provisioner
  # Proposal name defaults to 'default'.
  attributes:
    shell_prompt: USER@ALIAS:CWD SUFFIX
- barclamp: database
  # Default attributes are good enough, so we just need to assign
  # nodes to roles:
  deployment:
    elements:
      database-server:
        - "@@controller1@@"
- barclamp: rabbitmq
  deployment:
    elements:
      rabbitmq-server:
        - "@@controller1@@"</pre></div><div id="id-1.4.5.4.31.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Reserved Indicators in YAML</h6><p>
     Note that the characters <code class="literal">@</code> and <code class="literal">`</code> are
     reserved indicators in YAML. They can appear anywhere in a string
     <span class="emphasis"><em>except at the beginning</em></span>. Therefore a string such as
     <code class="literal">@@controller1@@</code> needs to be quoted using double quotes.
    </p></div></div><div class="sect2 " id="sec-deploy-crowbatch-yaml-attributes"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.25.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Top-level proposal attributes</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-deploy-crowbatch-yaml-attributes">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-yaml-attributes</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.31.8.2.1"><span class="term ">barclamp
     </span></dt><dd><p>
       Name of the barclamp for this proposal (required).
      </p></dd><dt id="id-1.4.5.4.31.8.2.2"><span class="term ">name
     </span></dt><dd><p>
       Name of this proposal (optional; default is <code class="literal">default</code>).
       In <code class="command">build</code> mode, if the proposal does not already
       exist, it will be created.
      </p></dd><dt id="id-1.4.5.4.31.8.2.3"><span class="term ">attributes
     </span></dt><dd><p>
       An optional nested hash containing any attributes for this proposal
       which deviate from the defaults for the barclamp.
      </p><p>
       In <code class="command">export</code> mode, any attributes set to the default
       values are excluded to keep the YAML as short and readable as possible.
      </p><p>
       In <code class="command">build</code> mode, these attributes are deep-merged with
       the current values for the proposal. If the proposal did not already
       exist, batch build will create it first. The attributes are merged with
       the default values for the barclamp's proposal.
      </p></dd><dt id="id-1.4.5.4.31.8.2.4"><span class="term ">wipe_attributes
     </span></dt><dd><p>
       An optional array of paths to nested attributes which should be removed
       from the proposal.
      </p><p>
       Each path is a period-delimited sequence of attributes; for example
       <code class="literal">pacemaker.stonith.sbd.nodes</code> would remove all SBD
       nodes from the proposal if it already exists. If a path segment contains
       a period, it should be escaped with a backslash, for example
       <code class="literal">segment-one.segment\.two.segment_three</code>.
      </p><p>
       This removal occurs before the deep merge described above.

       For example, think of a YAML file which includes a Pacemaker barclamp
       proposal where the <code class="literal">wipe_attributes</code> entry contains
       <code class="literal">pacemaker.stonith.sbd.nodes</code>. A batch build with this
       YAML file ensures that only SBD nodes listed in the <code class="literal">attributes
       sibling</code> hash are used at the end of the run. In contrast,
       without the <code class="literal">wipe_attributes</code> entry, the given SBD
       nodes would be appended to any SBD nodes already defined in the
       proposal.
      </p></dd><dt id="id-1.4.5.4.31.8.2.5"><span class="term ">deployment
     </span></dt><dd><p>
       A nested hash defining how and where this proposal should be deployed.
      </p><p>
       In <code class="command">build</code> mode, this hash is deep-merged in the same
       way as the attributes hash, except that the array of elements for each
       Chef role is reset to the empty list before the deep merge. This
       behavior may change in the future.
      </p></dd></dl></div></div><div class="sect2 " id="sec-deploy-crowbatch-yaml-subst"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.25.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Alias Substitutions</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-deploy-crowbatch-yaml-subst">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-yaml-subst</li></ul></div></div></div></div><p>
    A string like <code class="literal">@@<em class="replaceable ">node</em>@@</code> (where
    <em class="replaceable ">node</em> is a node alias) will be substituted for
    the name of that node, no matter where the string appears in the YAML file.
    For example, if <code class="literal">controller1</code> is a Crowbar alias for node
    <code class="literal">d52-54-02-77-77-02.mycloud.com</code>, then
    <code class="literal">@@controller1@@</code> will be substituted for that host name.
    This allows YAML files to be reused across environments.
   </p></div><div class="sect2 " id="sec-deploy-crowbatch-options"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.25.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Options</span> <a title="Permalink" class="permalink" href="cha-depl-ostack.html#sec-deploy-crowbatch-options">#</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>depl_nodes.xml</li><li><span class="ds-label">ID: </span>sec-deploy-crowbatch-options</li></ul></div></div></div></div><p>
    In addition to the standard options available to every
    <code class="command">crowbar</code> subcommand (run <code class="command">crowbar batch
    --help</code> for a full list), there are some extra options
    specifically for <code class="command">crowbar batch</code>:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.4.5.4.31.10.3.1"><span class="term ">--include &lt;barclamp[.proposal]&gt;
    </span></dt><dd><p>
       Only include the barclamp / proposals given.
      </p><p>
       This option can be repeated multiple times. The inclusion value can
       either be the name of a barclamp (for example,
       <code class="literal">pacemaker</code>) or a specifically named proposal within
       the barclamp (for example, <code class="literal">pacemaker.network_cluster</code>).
      </p><p>
       If it is specified, then only the barclamp / proposals specified are
       included in the build or export operation, and all others are ignored.
      </p></dd><dt id="id-1.4.5.4.31.10.3.2"><span class="term ">--exclude &lt;barclamp[.proposal]&gt;
    </span></dt><dd><p>
       This option can be repeated multiple times. The exclusion value is the
       same format as for <code class="option">--include</code>. The barclamps / proposals
       specified are excluded from the build or export operation.
      </p></dd><dt id="id-1.4.5.4.31.10.3.3"><span class="term ">--timeout &lt;seconds&gt;
    </span></dt><dd><p>
       Change the timeout for Crowbar API calls.
      </p><p>
       As Chef's run lists grow, some of the later <span class="productname">OpenStack</span> barclamp proposals
       (for example nova, horizon, or heat) can take over 5 or even 10
       minutes to apply. Therefore you may need to increase this timeout to 900
       seconds in some circumstances.
      </p></dd></dl></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="sec-deploy-policy-json.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 13 </span>Limiting Users' Access Rights</span></a><a class="nav-link" href="cha-depl-inst-nodes.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 11 </span>Installing the <span class="productname">OpenStack</span> Nodes</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>