<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SUSE OpenStack Cloud Crowbar 9 | Deployment Guide using Crowbar | Installing the OpenStack Nodes</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Installing the OpenStack Nodes | SUSE OpenStack Cloud …"/>
<meta name="description" content="The OpenStack nodes represent the actual cloud infrastructure. Node installation and service deployment is done automatically from the Administration…"/>
<meta name="product-name" content="SUSE OpenStack Cloud Crowbar"/>
<meta name="product-number" content="9"/>
<meta name="book-title" content="Deployment Guide using Crowbar"/>
<meta name="chapter-title" content="Chapter 11. Installing the OpenStack Nodes"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="dpopov@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9"/>
<meta property="og:title" content="Installing the OpenStack Nodes | SUSE OpenStack Cloud …"/>
<meta property="og:description" content="The OpenStack nodes represent the actual cloud infrastructure. Node installation and service deployment is done automatically from the Administration…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Installing the OpenStack Nodes | SUSE OpenStack Cloud …"/>
<meta name="twitter:description" content="The OpenStack nodes represent the actual cloud infrastructure. Node installation and service deployment is done automatically from the Administration…"/>
<link rel="prev" href="cha-depl-crowbar.html" title="Chapter 10. The Crowbar Web Interface"/><link rel="next" href="cha-depl-ostack.html" title="Chapter 12. Deploying the OpenStack Services"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Deployment Guide using Crowbar</a><span> / </span><a class="crumb" href="part-depl-ostack.html">Setting Up OpenStack Nodes and Services</a><span> / </span><a class="crumb" href="cha-depl-inst-nodes.html">Installing the OpenStack Nodes</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title"><em class="citetitle">Deployment Guide using Crowbar</em></div><ol><li><a href="pre-cloud-deploy.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-depl-intro.html" class="has-children "><span class="title-number">I </span><span class="title-name">Architecture and Requirements</span></a><ol><li><a href="cha-depl-arch.html" class=" "><span class="title-number">1 </span><span class="title-name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></a></li><li><a href="cha-depl-req.html" class=" "><span class="title-number">2 </span><span class="title-name">Considerations and Requirements</span></a></li></ol></li><li><a href="part-depl-admserv.html" class="has-children "><span class="title-number">II </span><span class="title-name">Setting Up the Administration Server</span></a><ol><li><a href="cha-depl-adm-inst.html" class=" "><span class="title-number">3 </span><span class="title-name">Installing the Administration Server</span></a></li><li><a href="app-deploy-smt.html" class=" "><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></a></li><li><a href="cha-depl-repo-conf.html" class=" "><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></a></li><li><a href="sec-depl-adm-inst-network.html" class=" "><span class="title-number">6 </span><span class="title-name">Service Configuration:  Administration Server Network Configuration</span></a></li><li><a href="sec-depl-adm-inst-crowbar.html" class=" "><span class="title-number">7 </span><span class="title-name">Crowbar Setup</span></a></li><li><a href="sec-depl-adm-start-crowbar.html" class=" "><span class="title-number">8 </span><span class="title-name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></a></li><li><a href="sec-depl-adm-crowbar-extra-features.html" class=" "><span class="title-number">9 </span><span class="title-name">Customizing Crowbar</span></a></li></ol></li><li class="active"><a href="part-depl-ostack.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Setting Up <span class="productname">OpenStack</span> Nodes and Services</span></a><ol><li><a href="cha-depl-crowbar.html" class=" "><span class="title-number">10 </span><span class="title-name">The Crowbar Web Interface</span></a></li><li><a href="cha-depl-inst-nodes.html" class=" you-are-here"><span class="title-number">11 </span><span class="title-name">Installing the <span class="productname">OpenStack</span> Nodes</span></a></li><li><a href="cha-depl-ostack.html" class=" "><span class="title-number">12 </span><span class="title-name">Deploying the <span class="productname">OpenStack</span> Services</span></a></li><li><a href="sec-deploy-policy-json.html" class=" "><span class="title-number">13 </span><span class="title-name">Limiting Users' Access Rights</span></a></li><li><a href="cha-depl-ostack-configs.html" class=" "><span class="title-number">14 </span><span class="title-name">Configuration Files for <span class="productname">OpenStack</span> Services</span></a></li><li><a href="install-heat-templates.html" class=" "><span class="title-number">15 </span><span class="title-name">Installing SUSE CaaS Platform heat Templates</span></a></li><li><a href="install-caasp-terraform.html" class=" "><span class="title-number">16 </span><span class="title-name">Installing SUSE CaaS Platform v4 using terraform</span></a></li></ol></li><li><a href="part-depl-nostack.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Setting Up Non-<span class="productname">OpenStack</span> Services</span></a><ol><li><a href="cha-depl-nostack.html" class=" "><span class="title-number">17 </span><span class="title-name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></a></li></ol></li><li><a href="part-depl-troubleshooting.html" class="has-children "><span class="title-number">V </span><span class="title-name">Troubleshooting and Support</span></a><ol><li><a href="cha-depl-trouble.html" class=" "><span class="title-number">18 </span><span class="title-name">Troubleshooting and Support</span></a></li></ol></li><li><a href="app-deploy-cisco.html" class=" "><span class="title-number">A </span><span class="title-name">Using Cisco Nexus Switches with neutron</span></a></li><li><a href="app-deploy-docupdates.html" class=" "><span class="title-number">B </span><span class="title-name">Documentation Updates</span></a></li><li><a href="gl-cloud.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary of Terminology and Product Names</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-depl-inst-nodes" data-id-title="Installing the OpenStack Nodes"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span> <span class="productnumber">9</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">11 </span><span class="title-name">Installing the <span class="productname">OpenStack</span> Nodes</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  The <span class="productname">OpenStack</span> nodes represent the actual cloud infrastructure. Node
  installation and service deployment is done automatically from the
  Administration Server. Before deploying the <span class="productname">OpenStack</span> services, SUSE Linux Enterprise Server 12 SP4 will be installed on all Control Nodes and Storage Nodes.
 </p><p>
  To prepare the installation, each node needs to be booted using PXE, which
  is provided by the <code class="systemitem">tftp</code> server
  from the Administration Server. Afterward you can allocate the nodes and trigger
  the operating system installation.
 </p><section class="sect1" id="sec-depl-inst-nodes-prep" data-id-title="Preparations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.1 </span><span class="title-name">Preparations</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-prep">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.5.2.1"><span class="term">Meaningful Node Names</span></dt><dd><p>
      Make a note of the MAC address and the purpose of each node (for
      example, controller, block storage, object storage, compute).
      This will make deploying the <span class="productname">OpenStack</span> components a lot easier and
      less error-prone. It also enables you to assign meaningful names
      (aliases) to the nodes, which are otherwise listed with the MAC
      address by default.
     </p></dd><dt id="id-1.4.5.3.5.2.2"><span class="term">BIOS Boot Settings</span></dt><dd><p>
      Make sure booting using PXE (booting from the network) is enabled and
      configured as the <span class="emphasis"><em>primary</em></span> boot-option for each
      node. The nodes will boot twice from the network during the allocation
      and installation phase. Booting from the first hard disk needs to be
      configured as the second boot option.
     </p></dd><dt id="id-1.4.5.3.5.2.3"><span class="term">Custom Node Configuration</span></dt><dd><p>
      All nodes are installed using AutoYaST with the same configuration
      located at
      <code class="filename">/opt/dell/chef/cookbooks/provisioner/templates/default/autoyast.xml.erb</code>.
      If this configuration does not match your needs (for example if you
      need special third party drivers) you need to make adjustments to this
      file. See the <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast</a> for details.
      If you change the AutoYaST configuration file, you need to re-upload
      it to Chef using the following command:
     </p><div class="verbatim-wrap"><pre class="screen">knife cookbook upload -o /opt/dell/chef/cookbooks/ provisioner</pre></div></dd><dt id="var-depl-inst-nodes-prep-root-login"><span class="term">Direct <code class="systemitem">root</code> Login</span></dt><dd><p>
      By default, the <code class="systemitem">root</code> account on the nodes has no password
      assigned, so a direct <code class="systemitem">root</code> login is not possible. Logging in
      on the nodes as <code class="systemitem">root</code> is only possible via SSH public keys
      (for example, from the Administration Server).
     </p><p>
      If you want to allow direct <code class="systemitem">root</code> login, you can set a
      password via the Crowbar Provisioner barclamp before deploying the
      nodes. That password will be used for the <code class="systemitem">root</code> account on all
      <span class="productname">OpenStack</span> nodes. Using this method after the nodes are deployed is
      not possible. In that case you would need to log in to each node via
      SSH from the Administration Server and change the password manually with
      <code class="command">passwd</code>.
     </p><div class="orderedlist"><div class="title-container"><div class="orderedlist-title-wrap"><div class="orderedlist-title"><span class="title-number-name"><span class="title-name">Setting a <code class="systemitem">root</code> Password for the <span class="productname">OpenStack</span> Nodes </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.5.2.4.2.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ol class="orderedlist" type="1"><li class="listitem"><p>
        Create an md5-hashed <code class="systemitem">root</code>-password, for example by using
        <code class="command">openssl passwd</code> <code class="option">-1</code>.
       </p></li><li class="listitem"><p>
        Open a browser and point it to the Crowbar Web interface on the
        Administration Server, for example <code class="literal">http://192.168.124.10</code>. Log
        in as user <code class="systemitem">crowbar</code>. The
        password is <code class="literal">crowbar</code> by default, if you have not
        changed it during the installation.
       </p></li><li class="listitem"><p>
        Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>. Click the <span class="guimenu">Provisioner</span> barclamp
        entry and <span class="guimenu">Edit</span> the <span class="guimenu">Default</span>
        proposal.
       </p></li><li class="listitem"><p>
        Click <span class="guimenu">Raw</span> in the <span class="guimenu">Attributes</span>
        section to edit the configuration file.
       </p></li><li class="listitem"><p>
        Add the following line to the end of the file before the last
        closing curly bracket:
       </p><div class="verbatim-wrap"><pre class="screen">, "root_password_hash": "<em class="replaceable">HASHED_PASSWORD</em>"</pre></div><p>
        replacing "<em class="replaceable">HASHED_PASSWORD</em>" with the
        password you generated in the first step.
       </p></li><li class="listitem"><p>
        Click <span class="guimenu">Apply</span>.
       </p></li></ol></div></dd></dl></div></section><section class="sect1" id="sec-depl-inst-nodes-install" data-id-title="Node Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.2 </span><span class="title-name">Node Installation</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   To install a node, you need to boot it first using PXE. It will be booted
   with an image that enables the Administration Server to discover the node and make
   it available for installation. When you have allocated the node, it will
   boot using PXE again and the automatic installation will start.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Boot all nodes that you want to deploy using PXE. The nodes will boot
     into the SLEShammer image, which performs the initial
     hardware discovery.
    </p><div id="id-1.4.5.3.6.3.1.2" data-id-title="Limit the Number of Concurrent Boots using PXE" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Limit the Number of Concurrent Boots using PXE</div><p>
      Booting many nodes at the same time using PXE will cause heavy load on
      the TFTP server, because all nodes will request the boot image at the
      same time. We recommend booting the nodes at different intervals.
     </p></div></li><li class="step"><p>
     Open a browser and point it to the Crowbar Web interface on the Administration Server,
     for example <code class="literal">http://192.168.124.10/</code>. Log in as user
     <code class="systemitem">crowbar</code>. The password is
     <code class="literal">crowbar</code> by default, if you have not changed it.
    </p><p>
     Click <span class="guimenu">Nodes</span> › <span class="guimenu">Dashboard</span> to open the <span class="guimenu">Node
     Dashboard</span>.
    </p></li><li class="step"><p>
     Each node that has successfully booted will be listed as being in state
     <code class="literal">Discovered</code>, indicated by a yellow bullet. The nodes
     will be listed with their MAC address as a name. Wait until all nodes
     are listed as <code class="literal">Discovered</code> before proceeding. If a node does not report as <code class="literal">Discovered</code>, it
     may need to be rebooted manually.
    </p><div class="figure" id="id-1.4.5.3.6.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_initial_nodes.png"><img src="images/depl_node_dashboard_initial_nodes.png" width="75%" alt="Discovered Nodes" title="Discovered Nodes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.1: </span><span class="title-name">Discovered Nodes </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.6.3.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p>
     Although this step is optional, we recommend properly grouping
     your nodes at this stage, since it lets you clearly arrange all nodes.
     Grouping the nodes by role would be one option, for example control,
     compute and object storage (swift).
    </p><ol type="a" class="substeps"><li class="step"><p>
       Enter the name of a new group into the <span class="guimenu">New Group</span>
       text box and click <span class="guimenu">Add Group</span>.
      </p></li><li class="step"><p>
       Drag and drop a node onto the title of the newly created group.
       Repeat this step for each node you want to put into the group.
       
      </p><div class="figure" id="id-1.4.5.3.6.3.4.2.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_groups_initial.png"><img src="images/depl_node_dashboard_groups_initial.png" width="75%" alt="Grouping Nodes" title="Grouping Nodes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.2: </span><span class="title-name">Grouping Nodes </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.6.3.4.2.2.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></li><li class="step"><p>
     To allocate all nodes, click <span class="guimenu">Nodes</span> › <span class="guimenu">Bulk Edit</span>. To allocate a single node,
     click the name of a node, then click <span class="guimenu">Edit</span>.
    </p><div class="figure" id="id-1.4.5.3.6.3.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_edit.png"><img src="images/depl_node_edit.png" width="75%" alt="Editing a Single Node" title="Editing a Single Node"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.3: </span><span class="title-name">Editing a Single Node </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.6.3.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.5.3.6.3.5.3" data-id-title="Limit the Number of Concurrent Node Deployments" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Limit the Number of Concurrent Node Deployments</div><p>
      Deploying many nodes in bulk mode will cause heavy load
      on the Administration Server. The subsequent concurrent Chef client runs
      triggered by the nodes will require a lot of RAM on the Administration Server.
     </p><p>
      Therefore it is recommended to limit the number of concurrent
      <span class="quote">“<span class="quote">Allocations</span>”</span> in bulk mode. The maximum number depends on
      the amount of RAM on the Administration Server—limiting concurrent
      deployments to five up to ten is recommended.
     </p></div></li><li class="step"><p> In single node editing mode, you can also specify the
      <span class="guimenu">Filesystem Type</span> for the node. By default, it is set to
      <code class="literal">ext4</code> for all nodes. We recommended using the default.</p></li><li class="step"><p>
     Provide a meaningful <span class="guimenu">Alias</span>, <span class="guimenu">Public
     Name</span>, and a <span class="guimenu">Description</span> for each node, and then
     check the <span class="guimenu">Allocate</span> box. You can also specify the
     <span class="guimenu">Intended Role</span> for the node. This optional setting is
     used to make reasonable proposals for the barclamps.
    </p><p>
     By default the <span class="guimenu">Target Platform</span> is set to <span class="guimenu">SLES 12
     SP2</span>.
    </p><div id="id-1.4.5.3.6.3.7.3" data-id-title="Alias Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Alias Names</div><p>
      Providing an alias name will change the default node names (MAC
      address) to the name you provided, making it easier to identify the
      node. Furthermore, this alias will also be used as a DNS
      <code class="literal">CNAME</code> for the node in the admin network. As a
      result, you can access the node via this alias when, for example,
      logging in via SSH.
     </p></div><div id="id-1.4.5.3.6.3.7.4" data-id-title="Public Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Public Names</div><p>
      A node's <span class="guimenu">Alias Name</span> is resolved by the DNS server
      installed on the Administration Server and therefore only available within the
      cloud network. The <span class="productname">OpenStack</span> Dashboard or some APIs
      (<code class="systemitem">keystone-server</code>,
      <code class="systemitem">glance-server</code>,
      <code class="systemitem">cinder-controller</code>,
      <code class="systemitem">neutron-server</code>,
      <code class="systemitem">nova-controller</code>, and
      <code class="systemitem">swift-proxy</code>) can be accessed
      from outside the SUSE <span class="productname">OpenStack</span> Cloud network. To be able to access them by
      name, these names need to be resolved by a name server placed outside
      of the SUSE <span class="productname">OpenStack</span> Cloud network. If you have created DNS entries for nodes,
      specify the name in the <span class="guimenu">Public Name</span> field.
     </p><p>
      The <span class="guimenu">Public Name</span> is never used within the SUSE <span class="productname">OpenStack</span> Cloud
      network. However, if you create an SSL certificate for a node that has
      a public name, this name must be added as an
      <code class="literal">AlternativeName</code> to the certificate. See <a class="xref" href="cha-depl-req.html#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for more information.
     </p></div><div class="figure" id="id-1.4.5.3.6.3.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_bulk_edit_allocate.png"><img src="images/depl_node_bulk_edit_allocate.png" width="75%" alt="Bulk Editing Nodes" title="Bulk Editing Nodes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.4: </span><span class="title-name">Bulk Editing Nodes </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.6.3.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p> When you have filled in the data for all nodes, click
      <span class="guimenu">Save</span>. The nodes will reboot and commence the
     AutoYaST-based SUSE Linux Enterprise Server installation (or installation of other target platforms,
     if selected) via a second boot using PXE. Click <span class="guimenu">Nodes</span> › <span class="guimenu">Dashboard</span> to return to the <span class="guimenu">Node Dashboard</span>. </p></li><li class="step"><p>
     Nodes that are being installed are listed with the status
     <code class="literal">Installing</code> (yellow/green bullet). When the
     installation of a node has finished, it is listed as being
     <code class="literal">Ready</code>, indicated by a green bullet. Wait until all
     nodes are listed as <code class="literal">Ready</code> before proceeding.
    </p><div class="figure" id="id-1.4.5.3.6.3.9.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_groups_installed.png"><img src="images/depl_node_dashboard_groups_installed.png" width="75%" alt="All Nodes Have Been Installed" title="All Nodes Have Been Installed"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.5: </span><span class="title-name">All Nodes Have Been Installed </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.6.3.9.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div></section><section class="sect1" id="sec-depl-inst-nodes-install-external" data-id-title="Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE OpenStack Cloud Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.3 </span><span class="title-name">Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-install-external">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   SUSE <span class="productname">OpenStack</span> Cloud allows adding existing machines installed with SUSE Linux Enterprise Server 12 SP4 to
   the pool of nodes. This enables you to use spare machines for
   SUSE <span class="productname">OpenStack</span> Cloud, and offers an alternative way of provisioning and installing
   nodes (via SUSE Manager for example). The
   machine must run SUSE Linux Enterprise Server 12 SP4.
  </p><p>
   The machine also needs to be on the same network as the
   Administration Server, because it needs to communicate with this server. Since the
   Administration Server provides a DHCP server, we recommend configuring this machine to get its network assignments from DHCP. If it has a static IP address, make
   sure it is not already used in the admin network. Check the list of used
   IP addresses with the YaST Crowbar module as described in
   <a class="xref" href="sec-depl-adm-inst-crowbar.html#sec-depl-adm-inst-crowbar-network" title="7.2. Networks">Section 7.2, “<span class="guimenu">Networks</span>”</a>.
  </p><p>
   Proceed as follows to convert an existing SUSE Linux Enterprise Server 12 SP4 machine into a
   SUSE <span class="productname">OpenStack</span> Cloud node:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Download the <code class="filename">crowbar_register</code> script from the
     Administration Server at
     <code class="literal">http://<em class="replaceable">192.168.124.10</em>:8091/suse-12.4/x86_64/crowbar_register</code>.
     Replace the IP address with the IP address of your Administration Server using
     <code class="command">curl</code> or <code class="command">wget</code>. Note that the
     download only works from within the admin network.
    </p></li><li class="step"><p>
     Make the <code class="filename">crowbar_register</code> script executable
     (<code class="command">chmod</code> <code class="option">a+x</code> crowbar_register).
    </p></li><li class="step"><p>
     Run the <code class="filename">crowbar_register</code> script. If you have
     multiple network interfaces, the script tries to automatically detect
     the one that is connected to the admin network. You may also explicitly
     specify which network interface to use by using the
     <code class="option">--interface</code> switch, for example
     <code class="command">crowbar_register</code> <code class="option">--interface eth1</code>.
    </p></li><li class="step"><p>
     After the script has successfully run, the machine has been added to
     the pool of nodes in the SUSE <span class="productname">OpenStack</span> Cloud and can be used as any other node
     from the pool.
    </p></li></ol></div></div></section><section class="sect1" id="sec-depl-inst-nodes-post" data-id-title="Post-Installation Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.4 </span><span class="title-name">Post-Installation Configuration</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The following lists some <span class="emphasis"><em>optional</em></span> configuration
   steps like configuring node updates, monitoring, access, and
   enabling SSL. You may entirely skip the following steps or perform any
   of them at a later stage.
  </p><section class="sect2" id="sec-depl-inst-nodes-post-updater" data-id-title="Deploying Node Updates with the Updater Barclamp"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.1 </span><span class="title-name">Deploying Node Updates with the Updater Barclamp</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-updater">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To keep the operating system and the SUSE <span class="productname">OpenStack</span> Cloud software itself
    up-to-date on the nodes, you can deploy either the Updater barclamp or
    the SUSE Manager barclamp. The latter requires access to a
    SUSE Manager server. The Updater barclamp uses Zypper to install
    updates and patches from repositories made available on the
    Administration Server.
   </p><p>
    The easiest way to provide the required repositories on the Administration Server
    is to set up an SMT server as described in
    <a class="xref" href="app-deploy-smt.html" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>. Alternatives to setting up an SMT
    server are described in <a class="xref" href="cha-depl-repo-conf.html" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
   </p><p>
    The Updater barclamp lets you deploy updates that are available on the
    update repositories at the moment of deployment. Each time you deploy
    updates with this barclamp you can choose a different set of nodes to
    which the updates are deployed. This lets you exactly control where and
    when updates are deployed.
   </p><p>
    To deploy the Updater barclamp, proceed as follows. For general
    instructions on how to edit barclamp proposals refer to
    <a class="xref" href="cha-depl-crowbar.html#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>.
      Click the <span class="guimenu">Updater</span> barclamp entry and
      <span class="guimenu">Create</span> to open the proposal.
     </p></li><li class="step"><p>
      Configure the barclamp by the following attributes. This
      configuration always applies to all nodes on which the barclamp is
      deployed. Individual configurations for certain nodes are only supported
      by creating a separate proposal.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.8.3.6.3.2.1"><span class="term"><span class="guimenu">Use zypper</span>
       </span></dt><dd><p>
         Define which Zypper subcommand to use for updating.
         <span class="guimenu">patch</span> will install all patches applying to the
         system from the configured update repositories that are available.
         <span class="guimenu">update</span> will update packages from all configured
         repositories (not just the update repositories) that have a higher
         version number than the installed packages.
         <span class="guimenu">dist-upgrade</span> replaces each package installed
         with the version from the repository and deletes packages not
         available in the repositories.
        </p><p>
         We recommend using <span class="guimenu">patch</span>.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.2"><span class="term"><span class="guimenu">Enable GPG Checks</span>
       </span></dt><dd><p>
         If set to true (recommended), checks if packages are correctly
         signed.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.3"><span class="term"><span class="guimenu">Automatically Agree With Licenses</span>
       </span></dt><dd><p>
         If set to true (recommended), Zypper automatically accepts third
         party licenses.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.4"><span class="term"><span class="guimenu">Include Patches that need Reboots (Kernel)</span>
       </span></dt><dd><p>
         Installs patches that require a reboot (for example Kernel or glibc
         updates). Only set this option to <code class="literal">true</code> when you
         can safely reboot the affected nodes. Refer to
         <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 1 “Maintenance”, Section 1.1 “Keeping the Nodes Up-To-Date”</span> for more information.
         Installing a new Kernel and not rebooting may result in an unstable
         system.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.5"><span class="term"><span class="guimenu">Reboot Nodes if Needed</span>
       </span></dt><dd><p>
         Automatically reboots the system in case a patch requiring a reboot
         has been installed. Only set this option to <code class="literal">true</code>
         when you can safely reboot the affected nodes. Refer to
         <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 1 “Maintenance”, Section 1.1 “Keeping the Nodes Up-To-Date”</span> for more information.
        </p></dd></dl></div><div class="figure" id="id-1.4.5.3.8.3.6.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_updater_attributes.png"><img src="images/depl_barclamp_updater_attributes.png" width="75%" alt="SUSE Updater barclamp: Configuration" title="SUSE Updater barclamp: Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.6: </span><span class="title-name">SUSE Updater barclamp: Configuration </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.8.3.6.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p>
      Choose the nodes on which the Updater barclamp should be deployed in
      the <span class="guimenu">Node Deployment</span> section by dragging them to the
      <span class="guimenu">Updater</span> column.
     </p><div class="figure" id="id-1.4.5.3.8.3.6.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_updater_nodes.png"><img src="images/depl_barclamp_updater_nodes.png" width="75%" alt="SUSE Updater barclamp: Node Deployment" title="SUSE Updater barclamp: Node Deployment"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.7: </span><span class="title-name">SUSE Updater barclamp: Node Deployment </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.8.3.6.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div><p>
    <code class="command">zypper</code> keeps track of the packages and patches it
    installs in <code class="filename">/var/log/zypp/history</code>. Review that log
    file on a node to find out which updates have been installed. A second
    log file recording debug information on the <code class="command">zypper</code>
    runs can be found at <code class="filename">/var/log/zypper.log</code> on each
    node.
   </p><div id="id-1.4.5.3.8.3.8" data-id-title="Updating Software Packages on Cluster Nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Updating Software Packages on Cluster Nodes</div><p>
     Before starting an update for a cluster node, either stop the cluster
     stack on that node or put the cluster into maintenance mode. If the
     cluster resource manager on a node is active during the software update,
     this can lead to unpredictable results like fencing of active nodes. For
     detailed instructions refer to <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate</a>.
    </p></div></section><section class="sect2" id="sec-depl-inst-nodes-post-manager" data-id-title="Configuring Node Updates with the SUSE Manager Client Barclamp"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.2 </span><span class="title-name">Configuring Node Updates with the <span class="guimenu">SUSE Manager Client</span>
    Barclamp</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-manager">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To keep the operating system and the SUSE <span class="productname">OpenStack</span> Cloud software itself
    up-to-date on the nodes, you can deploy either <span class="guimenu">SUSE Manager
    Client</span> barclamp or the Updater barclamp. The latter uses
    Zypper to install updates and patches from repositories made available
    on the Administration Server.
   </p><p>
    To enable the SUSE Manager server to manage the SUSE <span class="productname">OpenStack</span> Cloud nodes, you must make the
    respective <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 channels, the SUSE Linux Enterprise Server 12 SP4 channels,
    and the channels for extensions used with your deployment (High Availability Extension,
    SUSE Enterprise Storage) available via an activation key.
   </p><p>
    The <span class="guimenu">SUSE Manager Client</span> barclamp requires access to
    the SUSE Manager server from every node it is deployed to.
   </p><p>
    To deploy the <span class="guimenu">SUSE Manager Client</span> barclamp, proceed
    as follows. For general instructions on how to edit barclamp proposals
    refer to <a class="xref" href="cha-depl-crowbar.html#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the package
      <code class="literal">rhn-org-trusted-ssl-cert-<em class="replaceable">VERSION</em>-<em class="replaceable">RELEASE</em>.noarch.rpm</code>
      from
      https://<em class="replaceable">susemanager.example.com</em>/pub/.
      <em class="replaceable">VERSION</em> and
      <em class="replaceable">RELEASE</em> may vary, ask the administrator of
      the SUSE Manager for the correct values.
      <em class="replaceable">susemanager.example.com</em> needs to be
      replaced by the address of your SUSE Manager server. Copy the file you
      downloaded to
      <code class="filename">/opt/dell/chef/cookbooks/suse-manager-client/files/default/ssl-cert.rpm</code>
      on the Administration Server. The package contains the SUSE Manager's CA SSL
      Public Certificate. The certificate installation has not been
      automated on purpose, because downloading the certificate manually
      enables you to check it before copying it.
     </p></li><li class="step"><p>
      Re-install the barclamp by running the following command:
     </p><div class="verbatim-wrap"><pre class="screen">/opt/dell/bin/barclamp_install.rb --rpm core</pre></div></li><li class="step"><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>.
      Click the <span class="guimenu">SUSE Manager Client</span> barclamp entry and
      <span class="guimenu">Create</span> to open the proposal.
     </p></li><li class="step"><p>
      Specify the URL of the script for activation of the clients in the <span class="guimenu">URL of the bootstrap script</span> field.
     </p></li><li class="step"><p>
      Choose the nodes on which the SUSE Manager barclamp should be
      deployed in the <span class="guimenu">Deployment</span> section by dragging
      them to the <span class="guimenu">suse-manager-client</span> column. We
      recommend deploying it on all nodes in the SUSE <span class="productname">OpenStack</span> Cloud.
     </p><div class="figure" id="id-1.4.5.3.8.4.6.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_susemgr.png"><img src="images/depl_barclamp_susemgr.png" width="75%" alt="SUSE Manager barclamp" title="SUSE Manager barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.8: </span><span class="title-name">SUSE Manager barclamp </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.8.4.6.6.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div><div id="id-1.4.5.3.8.4.7" data-id-title="Updating Software Packages on Cluster Nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Updating Software Packages on Cluster Nodes</div><p>
     Before starting an update for a cluster node, either stop the cluster
     stack on that node or put the cluster into maintenance mode. If the
     cluster resource manager on a node is active during the software update,
     this can lead to unpredictable results like fencing of active nodes. For
     detailed instructions refer to <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate</a>.
    </p></div></section><section class="sect2" id="sec-depl-inst-nodes-post-nfs" data-id-title="Mounting NFS Shares on a Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.3 </span><span class="title-name">Mounting NFS Shares on a Node</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-nfs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The NFS barclamp allows you to mount NFS share from a remote host on
    nodes in the cloud. This feature can, for example, be used to provide an
    image repository for glance. Note that all nodes which are to mount
    an NFS share must be able to reach the NFS server. This requires manually adjusting the network configuration.
   </p><p>
    To deploy the NFS barclamp, proceed as follows. For general
    instructions on how to edit barclamp proposals refer to
    <a class="xref" href="cha-depl-crowbar.html#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>.
      Click the <span class="guimenu">NFS Client</span> barclamp entry and
      <span class="guimenu">Create</span> to open the proposal.
     </p></li><li class="step"><p>
      Configure the barclamp by the following attributes. Each set of
      attributes is used to mount a single NFS share.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.8.5.4.3.2.1"><span class="term"><span class="guimenu">Name</span>
       </span></dt><dd><p>
         Unique name for the current configuration. This name is used in the
         Web interface only to distinguish between different shares.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.2"><span class="term"><span class="guimenu">NFS Server</span>
       </span></dt><dd><p>
         Fully qualified host name or IP address of the NFS server.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.3"><span class="term"><span class="guimenu">Export</span>
       </span></dt><dd><p>
         Export name for the share on the NFS server.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.4"><span class="term"><span class="guimenu">Path</span>
       </span></dt><dd><p>
         Mount point on the target machine.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.5"><span class="term"><span class="guimenu">Mount Options</span>
       </span></dt><dd><p>
         Mount options that will be used on the node. See <code class="command">man 8
         mount </code> for general mount options and <code class="command">man 5
         nfs</code> for a list of NFS-specific options. Note that the
         general option <code class="option">nofail</code> (do not report errors if
         device does not exist) is automatically set.
        </p></dd></dl></div></li><li class="step"><p>
      After having filled in all attributes, click <span class="guimenu">Add</span>. If
      you want to mount more than one share, fill in the data for another
      NFS mount. Otherwise click <span class="guimenu">Save</span> to save the data,
      or <span class="guimenu">Apply</span> to deploy the proposal. Note that you must
      always click <span class="guimenu">Add</span> before saving or applying the
      barclamp, otherwise the data that was entered will be lost.
     </p><div class="figure" id="id-1.4.5.3.8.5.4.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_nfs.png"><img src="images/depl_barclamp_nfs.png" width="75%" alt="NFS barclamp" title="NFS barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.9: </span><span class="title-name">NFS barclamp </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.8.5.4.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p>
      Go to the <span class="guimenu">Node Deployment</span> section and drag and drop
      all nodes, on which the NFS shares defined above should be mounted, to
      the <span class="guimenu">nfs-client</span> column. Click
      <span class="guimenu">Apply</span> to deploy the proposal.
     </p><p>
      The NFS barclamp is the only barclamp that lets you create
      different proposals, enabling you to mount different NFS
      shares on different nodes. When you have created an NFS proposal, a
      special <span class="guimenu">Edit</span> is shown in the barclamp overview of the
      Crowbar Web interface. Click it to either
      <span class="guimenu">Edit</span> an existing proposal or
      <span class="guimenu">Create</span> a new one. New proposals must have unique names.
     </p><div class="figure" id="id-1.4.5.3.8.5.4.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_nfs_edit.png"><img src="images/depl_barclamp_nfs_edit.png" width="75%" alt="Editing an NFS barclamp Proposal" title="Editing an NFS barclamp Proposal"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.10: </span><span class="title-name">Editing an NFS barclamp Proposal </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.8.5.4.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div></section><section class="sect2" id="sec-depl-inst-nodes-post-ceph-ext" data-id-title="Using an Externally Managed Ceph Cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.4 </span><span class="title-name">Using an Externally Managed Ceph Cluster</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ceph-ext">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The following chapter provides instructions on using an external Ceph
    cluster in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p><section class="sect3" id="sec-depl-inst-nodes-post-ceph-ext-requirements" data-id-title="Requirements"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">11.4.4.1 </span><span class="title-name">Requirements</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ceph-ext-requirements">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.8.6.3.2.1"><span class="term">Ceph Release</span></dt><dd><p>
        External Ceph cluster are supported with SUSE Enterprise Storage 5 or higher. The
        version of Ceph should be compatible with the version of the Ceph
        client supplied with SUSE Linux Enterprise Server 12 SP4.
       </p></dd><dt id="id-1.4.5.3.8.6.3.2.2"><span class="term">Network Configuration</span></dt><dd><p>
        The external Ceph cluster needs to be connected to a separate
        VLAN, which is mapped to the SUSE <span class="productname">OpenStack</span> Cloud storage VLAN. See
        <a class="xref" href="cha-depl-req.html#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for more information.
       </p></dd></dl></div></section><section class="sect3" id="sec-depl-inst-nodes-post-ceph-ext-install" data-id-title="Making Ceph Available on the SUSE OpenStack Cloud Nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">11.4.4.2 </span><span class="title-name">Making Ceph Available on the SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ceph-ext-install">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Ceph can be used from the KVM Compute Nodes, with
     cinder, and with glance. The following installation
     steps need to be executed on each node accessing Ceph:
    </p><div id="id-1.4.5.3.8.6.4.3" data-id-title="Installation Workflow" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Installation Workflow</div><p>
      The following steps need to be executed before the barclamps get
      deployed.
     </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in as user <code class="systemitem">root</code> to a machine in the Ceph cluster
       and generate keyring files for cinder users. Optionally, you can
       generate keyring files for the glance users (only needed
       when using glance with Ceph/Rados). The keyring file that will be
       generated for cinder will also be used on the Compute Nodes.
       To do so, you need to specify pool names and user names for both services. The default
       names are:
      </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           
          </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           <p>
            glance
           </p>
          </th><th style="border-bottom: 1px solid ; ">
           <p>
            cinder
           </p>
          </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           <p>
            <span class="bold"><strong>User</strong></span>
           </p>
          </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           <p>
            glance
           </p>
          </td><td style="border-bottom: 1px solid ; ">
           <p>
            cinder
           </p>
          </td></tr><tr><td style="border-right: 1px solid ; ">
           <p>
            <span class="bold"><strong>Pool</strong></span>
           </p>
          </td><td style="border-right: 1px solid ; ">
           <p>
            images
           </p>
          </td><td>
           <p>
            volumes
           </p>
          </td></tr></tbody></table></div><p>
       Make a note of user and pool names in case you do not use the default
       values. You will need this information later, when deploying
       glance and cinder.
      </p></li><li class="step"><div id="id-1.4.5.3.8.6.4.4.2.1" data-id-title="Automatic Changes to the Cluster" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Automatic Changes to the Cluster</div><p>
        If you decide to use the admin keyring file to connect the external
        Ceph cluster, be aware that after Crowbar discovers this admin keyring,
        it will create client keyring files, pools, and capabilities needed to run
        glance, cinder, or nova integration.
       </p></div><p>
       If you have access to the admin keyring file and agree that automatic
       changes will be done to the cluster as described above, copy it together
       with the Ceph configuration file to the Administration Server. If you cannot
       access this file, create a keyring:
      </p><ol type="a" class="substeps"><li class="step"><p>
         When you can access the admin keyring file
         <code class="filename">ceph.client.admin.keyring</code>, copy it together with
         <code class="filename">ceph.conf</code> (both files are usually located in
         <code class="filename">/etc/ceph</code>) to a temporary location on the
         Administration Server, for example <code class="filename">/root/tmp/</code>.
        </p></li><li class="step"><p>
         If you cannot access the admin keyring file create a new keyring file
         with the following commands. Re-run the commands for glance, too, if
         needed. First create a key:
        </p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.<em class="replaceable">USERNAME</em> mon "allow r" \
osd 'allow class-read object_prefix rbd_children, allow rwx \
pool=<em class="replaceable">POOLNAME</em>'</pre></div><p>
         Replace <em class="replaceable">USERNAME</em> and
         <em class="replaceable">POOLNAME</em> with the respective values.
        </p><p>
         Now use the key to generate the keyring file
         <code class="filename">/etc/ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</code>:
        </p><div class="verbatim-wrap"><pre class="screen">ceph-authtool \
/etc/ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring \
--create-keyring --name=client.<em class="replaceable">USERNAME</em>&gt; \
--add-key=<em class="replaceable">KEY</em></pre></div><p>
         Replace <em class="replaceable">USERNAME</em> with the respective
         value.
        </p><p>
         Copy the Ceph configuration file <code class="filename">ceph.conf</code>
         (usually located in <code class="filename">/etc/ceph</code>) and the keyring
         file(s) generated above to a temporary location on the Administration Server, for
         example <code class="filename">/root/tmp/</code>.
        </p></li></ol></li><li class="step"><p>
       Log in to the Crowbar Web interface and check whether the nodes
       which should have access to the Ceph cluster already have an IP
       address from the storage network. Do so by going to the
       <span class="guimenu">Dashboard</span> and clicking the node name. An
       <span class="guimenu">IP address</span> should be listed for
       <span class="guimenu">storage</span>. Make a note of the <span class="guimenu">Full
       name</span> of each node that has <span class="emphasis"><em>no</em></span> storage
       network IP address.
      </p></li><li class="step"><p>
       Log in to the Administration Server as user <code class="systemitem">root</code> and run the
       following command for all nodes you noted down in the previous step:
      </p><div class="verbatim-wrap"><pre class="screen">crowbar network allocate_ip "default" <em class="replaceable">NODE</em> "storage" "host"
chef-client</pre></div><p>
       <em class="replaceable">NODE</em> needs to be replaced by the node's
       name.
      </p></li><li class="step"><p>
       After executing the command in the previous step for all
       affected nodes, run the command <code class="command">chef-client</code> on the
       Administration Server.
      </p></li><li class="step"><p>
       Log in to each affected node as user <code class="systemitem">root</code>. See
       <a class="xref" href="cha-depl-trouble.html#var-depl-trouble-faq-ostack-login" title="Q:"><em>
       How can I log in to a node as root?
      </em></a> for instructions.
       On each node, do the following:
      </p><ol type="a" class="substeps"><li class="step"><p>
         Manually install nova, cinder (if using cinder) and/or glance
         (if using glance) packages with the following commands:
        </p><div class="verbatim-wrap"><pre class="screen">zypper in openstack-glance
zypper in openstack-cinder
zypper in openstack-nova</pre></div></li><li class="step"><p>
         Copy the ceph.conf file from the Administration Server to
         <code class="filename">/etc/ceph</code>:
        </p><div class="verbatim-wrap"><pre class="screen">mkdir -p /etc/ceph
scp root@admin:/root/tmp/ceph.conf /etc/ceph
chmod 664 /etc/ceph/ceph.conf</pre></div></li><li class="step"><p>
         Copy the keyring file(s) to <code class="filename">/etc/ceph</code>. The
         exact process depends on whether you have copied the admin keyring
         file or whether you have created your own keyrings:
        </p><ol type="i" class="substeps"><li class="step"><p>
           If you have copied the admin keyring file, run the following
           command on the Control Node(s) on which cinder and glance
           will be deployed, and on all KVM Compute Nodes:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.admin.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.admin.keyring</pre></div></li><li class="step"><p>
           If you have created you own keyrings, run the following command on
           the Control Node on which cinder will be deployed, and on all
           KVM Compute Nodes to copy the cinder keyring:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.cinder.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
	    On Control Node on which cinder will be deployed run the
	    following command to update file ownership:
          </p><div class="verbatim-wrap"><pre class="screen">chown root.cinder /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
	    On KVM Compute Nodes run the following command to update file ownership:
          </p><div class="verbatim-wrap"><pre class="screen">chown root.nova /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
           Now copy the glance keyring to the Control Node on which glance
           will be deployed:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.glance.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.glance.keyring
chown root.glance /etc/ceph/ceph.client.glance.keyring</pre></div></li></ol></li></ol></li></ol></div></div></section></section><section class="sect2" id="sec-depl-inst-nodes-post-access" data-id-title="Accessing the Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.5 </span><span class="title-name">Accessing the Nodes</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-access">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The nodes can only be accessed via SSH from the Administration Server—it
    is not possible to connect to them from any other host in the network.
   </p><p>
    The <code class="systemitem">root</code> account <span class="emphasis"><em>on the nodes</em></span> has no
    password assigned, therefore logging in to a node as
    <code class="systemitem">root</code>@<em class="replaceable">node</em> is only possible via SSH
    with key authentication. By default, you can only log in with the key of
    the <code class="systemitem">root</code> of the Administration Server
    (root@<em class="replaceable">admin</em>) via SSH only.
   </p><p>
    If you have added users to the Administration Server and want to
    give them permission to log in to the nodes as well, you need to add
    these users' public SSH keys to <code class="systemitem">root</code>'s
    <code class="filename">authorized_keys</code> file on all nodes. Proceed as
    follows:
   </p><div class="procedure" id="id-1.4.5.3.8.7.5" data-id-title="Copying SSH Keys to All Nodes"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.1: </span><span class="title-name">Copying SSH Keys to All Nodes </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.8.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If they do not already exist, generate an SSH key pair with
      <code class="command">ssh-keygen</code>. This key pair belongs to the user that you use to log in to the nodes. Alternatively, copy an existing public
      key with <code class="command">ssh-copy-id</code>. Refer to the respective man
      pages for more information.
     </p></li><li class="step"><p>
      Log in to the Crowbar Web interface on the Administration Server, for
      example <code class="literal">http://192.168.124.10/</code> (user name and default
      password: <code class="literal">crowbar</code>).
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>. Click the <span class="guimenu">Provisioner</span> barclamp
      entry and <span class="guimenu">Edit</span> the <span class="guimenu">Default</span>
      proposal.
     </p></li><li class="step"><p>
      Copy and paste the <span class="emphasis"><em>public</em></span> SSH key of the user
      into the <span class="guimenu">Additional SSH Keys</span> text box. If adding
      keys for multiple users, note that each key needs to be placed on a
      new line.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Apply</span> to deploy the keys and save your
      changes to the proposal.
     </p></li></ol></div></div></section><section class="sect2" id="sec-depl-inst-nodes-post-ssl" data-id-title="Enabling SSL"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.6 </span><span class="title-name">Enabling SSL</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post-ssl">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To enable SSL to encrypt communication within the cloud (see
    <a class="xref" href="cha-depl-req.html#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for details), all nodes running encrypted services need SSL certificates. An SSL certificate is, at a minimum, required on the Control Node.
   </p><p>
    Each certificate consists
    of a pair of files: the certificate file (for example,
    <code class="filename">signing_cert.pem</code>) and the key file (for example,
    <code class="filename">signing_key.pem</code>). If you use your own certificate
    authority (CA) for signing, you will also need a certificate file for
    the CA (for example, <code class="filename">ca.pem</code>). We recommend copying the files to the <code class="filename">/etc</code> directory using the
    directory structure outlined below. If you use a dedicated certificate
    for each service, create directories named after the services (for
    example, <code class="filename">/etc/keystone</code>). If you are using shared
    certificates, use a directory such as <code class="filename">/etc/cloud</code>.
   </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Recommended Locations for Shared Certificates </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.8.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><dl class="variablelist"><dt id="id-1.4.5.3.8.8.4.2"><span class="term">SSL Certificate File</span></dt><dd><p>
       <code class="filename">/etc/cloud/ssl/certs/signing_cert.pem</code>
      </p></dd><dt id="id-1.4.5.3.8.8.4.3"><span class="term">SSL Key File</span></dt><dd><p>
       <code class="filename">/etc/cloud/private/signing_key.pem</code>
      </p></dd><dt id="id-1.4.5.3.8.8.4.4"><span class="term">CA Certificates File</span></dt><dd><p>
       <code class="filename">/etc/cloud/ssl/certs/ca.pem</code>
      </p></dd></dl></div></section></section><section class="sect1" id="sec-depl-inst-nodes-edit" data-id-title="Editing Allocated Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.5 </span><span class="title-name">Editing Allocated Nodes</span></span> <a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-edit">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   All nodes that have been allocated can be decommissioned or re-installed.
   Click a node's name in the <span class="guimenu">Node Dashboard</span> to open a
   screen with the node details. The following options are available:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.9.3.1"><span class="term"><span class="guimenu">Forget</span>
    </span></dt><dd><p>
      Deletes a node from the pool. If you want to re-use this node again,
      it needs to be reallocated and re-installed from scratch.
     </p></dd><dt id="id-1.4.5.3.9.3.2"><span class="term"><span class="guimenu">Reinstall</span>
    </span></dt><dd><p>
      Triggers a reinstallation. The machine stays allocated. Any barclamps that were deployed on the machine will be re-applied after the installation.
     </p></dd><dt id="id-1.4.5.3.9.3.3"><span class="term"><span class="guimenu">Deallocate</span>
    </span></dt><dd><p>
      Temporarily removes the node from the pool of nodes. After you
      reallocate the node it will take its former role. Useful for adding
      additional machines in times of high load or for decommissioning
      machines in times of low load.
     </p></dd><dt id="id-1.4.5.3.9.3.4"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Reboot</span>
    </span></dt><dd><p>
      Reboots the node.
     </p></dd><dt id="id-1.4.5.3.9.3.5"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Shutdown</span>
    </span></dt><dd><p>
      Shuts the node down.
     </p></dd><dt id="id-1.4.5.3.9.3.6"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Power Cycle</span>
    </span></dt><dd><p>
      Forces a (non-clean) shuts down and a restart afterward. Only use if a
      reboot does not work.
     </p></dd><dt id="id-1.4.5.3.9.3.7"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Power Off</span>
    </span></dt><dd><p>
      Forces a (non-clean) node shut down. Only use if a clean shut down does
      not work.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.3.9.4"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_nodeinfo.png"><img src="images/depl_nodeinfo.png" width="75%" alt="Node Information" title="Node Information"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.11: </span><span class="title-name">Node Information </span></span><a title="Permalink" class="permalink" href="cha-depl-inst-nodes.html#id-1.4.5.3.9.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.5.3.9.5" data-id-title="Editing Nodes in a Production System" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Editing Nodes in a Production System</div><p>
    When de-allocating nodes that provide essential services, the complete
    cloud will become unusable. If you have not disabled redundancy, you can disable single storage nodes or single
    compute nodes. However, disabling Control Node(s) will cause major problems. It
    will either <span class="quote">“<span class="quote">kill</span>”</span> certain services (for example
    swift) or, at worst the complete cloud (when deallocating the Control Node
    hosting neutron). You should also not disable the nodes providing
    swift ring and proxy services.
   </p></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-depl-crowbar.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 10 </span>The Crowbar Web Interface</span></a> </div><div><a class="pagination-link next" href="cha-depl-ostack.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 12 </span>Deploying the <span class="productname">OpenStack</span> Services</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-prep"><span class="title-number">11.1 </span><span class="title-name">Preparations</span></a></span></li><li><span class="sect1"><a href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-install"><span class="title-number">11.2 </span><span class="title-name">Node Installation</span></a></span></li><li><span class="sect1"><a href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-install-external"><span class="title-number">11.3 </span><span class="title-name">Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></a></span></li><li><span class="sect1"><a href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-post"><span class="title-number">11.4 </span><span class="title-name">Post-Installation Configuration</span></a></span></li><li><span class="sect1"><a href="cha-depl-inst-nodes.html#sec-depl-inst-nodes-edit"><span class="title-number">11.5 </span><span class="title-name">Editing Allocated Nodes</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>