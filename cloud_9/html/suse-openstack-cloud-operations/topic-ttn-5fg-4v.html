<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Managing Monitoring, Logging, and Usage Reporting | Operations Guide CLM | SUSE OpenStack Cloud 9</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.2.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.81.0 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="9" /><meta name="book-title" content="Operations Guide CLM" /><meta name="chapter-title" content="Chapter 13. Managing Monitoring, Logging, and Usage Reporting" /><meta name="description" content="Information about the monitoring, logging, and metering services included with your SUSE OpenStack Cloud." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9" /><link rel="home" href="index.html" title="Documentation" /><link rel="up" href="book-operations.html" title="Operations Guide CLM" /><link rel="prev" href="ops-managing-orchestration.html" title="Chapter 12. Managing Orchestration" /><link rel="next" href="using-container-as-a-service-overview.html" title="Chapter 14. Managing Container as a Service (Magnum)" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #E11;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-operations.html">Operations Guide CLM</a><span> › </span><a class="crumb" href="topic-ttn-5fg-4v.html">Managing Monitoring, Logging, and Usage Reporting</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Operations Guide CLM</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="gettingstarted-ops.html"><span class="number">1 </span><span class="name">Operations Overview</span></a></li><li class="inactive"><a href="tutorials.html"><span class="number">2 </span><span class="name">Tutorials</span></a></li><li class="inactive"><a href="clm-admin-ui.html"><span class="number">3 </span><span class="name">Cloud Lifecycle Manager Admin UI User Guide</span></a></li><li class="inactive"><a href="third-party-integrations.html"><span class="number">4 </span><span class="name">Third-Party Integrations</span></a></li><li class="inactive"><a href="ops-managing-identity.html"><span class="number">5 </span><span class="name">Managing Identity</span></a></li><li class="inactive"><a href="ops-managing-compute.html"><span class="number">6 </span><span class="name">Managing Compute</span></a></li><li class="inactive"><a href="ops-managing-esx.html"><span class="number">7 </span><span class="name">Managing ESX</span></a></li><li class="inactive"><a href="ops-managing-blockstorage.html"><span class="number">8 </span><span class="name">Managing Block Storage</span></a></li><li class="inactive"><a href="ops-managing-objectstorage.html"><span class="number">9 </span><span class="name">Managing Object Storage</span></a></li><li class="inactive"><a href="ops-managing-networking.html"><span class="number">10 </span><span class="name">Managing Networking</span></a></li><li class="inactive"><a href="ops-managing-dashboards.html"><span class="number">11 </span><span class="name">Managing the Dashboard</span></a></li><li class="inactive"><a href="ops-managing-orchestration.html"><span class="number">12 </span><span class="name">Managing Orchestration</span></a></li><li class="inactive"><a href="topic-ttn-5fg-4v.html"><span class="number">13 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li class="inactive"><a href="using-container-as-a-service-overview.html"><span class="number">14 </span><span class="name">Managing Container as a Service (Magnum)</span></a></li><li class="inactive"><a href="system-maintenance.html"><span class="number">15 </span><span class="name">System Maintenance</span></a></li><li class="inactive"><a href="manage-ops-console.html"><span class="number">16 </span><span class="name">Operations Console</span></a></li><li class="inactive"><a href="bura-overview.html"><span class="number">17 </span><span class="name">Backup and Restore</span></a></li><li class="inactive"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html"><span class="number">18 </span><span class="name">Troubleshooting Issues</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 12. Managing Orchestration" href="ops-managing-orchestration.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 14. Managing Container as a Service (Magnum)" href="using-container-as-a-service-overview.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #E11;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-operations.html">Operations Guide CLM</a><span> › </span><a class="crumb" href="topic-ttn-5fg-4v.html">Managing Monitoring, Logging, and Usage Reporting</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 12. Managing Orchestration" href="ops-managing-orchestration.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 14. Managing Container as a Service (Magnum)" href="using-container-as-a-service-overview.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="topic-ttn-5fg-4v"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber ">9</span></div><div><h1 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Monitoring, Logging, and Usage Reporting</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_telemetry.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_telemetry.xml</li><li><span class="ds-label">ID: </span>topic-ttn-5fg-4v</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="topic-ttn-5fg-4v.html#mon"><span class="number">13.1 </span><span class="name">Monitoring</span></a></span></dt><dt><span class="section"><a href="topic-ttn-5fg-4v.html#centralized-logging"><span class="number">13.2 </span><span class="name">Centralized Logging Service</span></a></span></dt><dt><span class="section"><a href="topic-ttn-5fg-4v.html#ceilo-metering-overview"><span class="number">13.3 </span><span class="name">Metering Service (ceilometer) Overview</span></a></span></dt></dl></div></div><p>
  Information about the monitoring, logging, and metering services included
  with your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect1" id="mon"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#mon">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring.xml</li><li><span class="ds-label">ID: </span>mon</li></ul></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring service leverages OpenStack monasca, which is a
  multi-tenant, scalable, fault tolerant monitoring service.
 </p><div class="sect2" id="monitoring-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started with Monitoring</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#monitoring-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring_service.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring_service.xml</li><li><span class="ds-label">ID: </span>monitoring-service</li></ul></div></div></div></div><p>
  You can use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring service to monitor the health of your
  cloud and, if necessary, to troubleshoot issues.
 </p><p>
  monasca data can be extracted and used for a variety of legitimate purposes,
  and different purposes require different forms of data sanitization or
  encoding to protect against invalid or malicious data. Any data pulled from
  monasca should be considered untrusted data, so users are advised to apply
  appropriate encoding and/or sanitization techniques to ensure safe and
  correct usage and display of data in a web browser, database scan, or any
  other use of the data.
 </p><div class="sect3" id="idg-all-operations-monitoring-monitoring-overview-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Service Overview</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-monitoring-overview-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-1</li></ul></div></div></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-2"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-monitoring-overview-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-2</li></ul></div></div></div></div><p>
   The monitoring service is automatically installed as part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation.
  </p><p>
   No specific configuration is required to use monasca. However, you can
   configure the database for storing metrics as explained in
   <a class="xref" href="topic-ttn-5fg-4v.html#configure-monitoring" title="13.1.2. Configuring the Monitoring Service">Section 13.1.2, “Configuring the Monitoring Service”</a>.
  </p></div><div class="sect4" id="differences"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Differences Between Upstream and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Implementations</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#differences">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>differences</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the OpenStack monitoring service, monasca,
   is included as the monitoring solution, except for the following which are
   not included:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Transform Engine
    </p></li><li class="listitem "><p>
     Events Engine
    </p></li><li class="listitem "><p>
     Anomaly and Prediction Engine
    </p></li></ul></div><div id="id-1.5.15.3.3.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Icinga was supported in previous <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> versions but it has been
    deprecated in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
   </p></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Diagram of monasca Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-monitoring-overview-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-4</li></ul></div></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-monasca_diagram.png" target="_blank"><img src="images/media-monasca_diagram.png" width="" /></a></div></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-monitoring-overview-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-5</li></ul></div></div></div></div><p>
   For more details on OpenStack monasca, see
   <a class="link" href="http://monasca.io/" target="_blank">monasca.io</a>
  </p></div><div class="sect4" id="id-1.5.15.3.3.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Back-end Database</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.3.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The monitoring service default metrics database is Cassandra, which is a
   highly-scalable analytics database and the recommended database for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   You can learn more about Cassandra at <a class="link" href="http://cassandra.apache.org/" target="_blank">Apache Cassandra</a>.
  </p></div></div><div class="sect3" id="working-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Working with Monasca</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#working-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-working_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-working_monitoring.xml</li><li><span class="ds-label">ID: </span>working-monitoring</li></ul></div></div></div></div><p>
  <span class="bold"><strong>monasca-Agent</strong></span>
 </p><p>
  The <span class="bold"><strong>monasca-agent</strong></span> is a Python program that
  runs on the control plane nodes. It runs the defined checks and then sends
  data onto the API. The checks that the agent runs include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    System Metrics: CPU utilization, memory usage, disk I/O, network I/O, and
    filesystem utilization on the control plane and resource nodes.
   </p></li><li class="listitem "><p>
    Service Metrics: the agent supports plugins such as MySQL, RabbitMQ, Kafka,
    and many others.
   </p></li><li class="listitem "><p>
    VM Metrics: CPU utilization, disk I/O, network I/O, and memory usage of
    hosted virtual machines on compute nodes. Full details of these can be
    found
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#per-instance-metrics" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#per-instance-metrics</a>.
   </p></li></ul></div><p>
  For a full list of packaged plugins that are included <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, see
  <a class="link" href="https://github.com/stackforge/monasca-agent/blob/master/docs/Plugins.md" target="_blank">monasca
  Plugins</a>
 </p><p>
  You can further customize the monasca-agent to suit your needs, see
  <a class="link" href="https://github.com/stackforge/monasca-agent/blob/master/docs/Customizations.md" target="_blank">Customizing
  the Agent</a>
 </p></div><div class="sect3" id="accessing-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing the Monitoring Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#accessing-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>accessing-monitoring</li></ul></div></div></div></div><p>
  Access to the Monitoring service is available through a number of different
  interfaces.
 </p><div class="sect4" id="id-1.5.15.3.3.6.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Command-Line Interface</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.3.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For users who prefer using the command line, there is the
   python-monascaclient, which is part of the default installation on your
   Cloud Lifecycle Manager node.
  </p><p>
   For details on the CLI, including installation instructions, see
   <a class="link" href="https://github.com/stackforge/python-monascaclient/blob/master/README.rst" target="_blank">Python-monasca
   Client</a>
  </p><p>
   <span class="bold"><strong>monasca API</strong></span>
  </p><p>
   If low-level access is desired, there is the monasca REST API.
  </p><p>
   Full details of the monasca API can be found
   <a class="link" href="https://github.com/stackforge/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">on
   GitHub</a>.
  </p></div><div class="sect4" id="idg-all-operations-monitoring-accessing-monitoring-xml-2"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console GUI</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-accessing-monitoring-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-accessing-monitoring-xml-2</li></ul></div></div></div></div><p>
   You can use the Operations Console (Ops Console) for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to view
   data about your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud infrastructure in a web-based graphical user
   interface (GUI) and ensure your cloud is operating correctly. By logging on
   to the console, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> administrators can manage data in the following
   ways: <span class="bold"><strong>Triage alarm notifications.</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Alarm Definitions and notifications now have their own screens and are
     collected under the <span class="bold"><strong>Alarm Explorer</strong></span> menu
     item which can be accessed from the Central Dashboard. Central Dashboard
     now allows you to customize the view in the following ways:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Rename or re-configure existing alarm cards to include services
       different from the defaults
      </p></li><li class="listitem "><p>
       Create a new alarm card with the services you want to select
      </p></li><li class="listitem "><p>
       Reorder alarm cards using drag and drop
      </p></li><li class="listitem "><p>
       View all alarms that have no service dimension now grouped in an
       <span class="bold"><strong>Uncategorized Alarms</strong></span> card
      </p></li><li class="listitem "><p>
       View all alarms that have a service dimension that does not match any of
       the other cards -now grouped in an <span class="bold"><strong>Other
       Alarms</strong></span> card
      </p></li></ul></div></li><li class="listitem "><p>
     You can also easily access alarm data for a specific component. On the
     Summary page for the following components, a link is provided to an alarms
     screen specifically for that component.
    </p></li></ul></div></div><div class="sect4" id="idg-all-operations-monitoring-accessing-monitoring-xml-3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Connecting to the Operations Console</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-accessing-monitoring-xml-3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-accessing-monitoring-xml-3</li></ul></div></div></div></div><p>
   To connect to Operations Console, perform the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Ensure your login has the required access credentials.
    </p></li><li class="listitem "><p>
     Connect through a browser.
    </p></li><li class="listitem "><p>
     Optionally use a Host name OR virtual IP address to access Operations Console.
    </p></li></ul></div><p>
   Operations Console will always be accessed over port 9095.
  </p></div></div><div class="sect3" id="alarms-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service Alarm Definitions</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#alarms-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-alarms_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-alarms_monitoring.xml</li><li><span class="ds-label">ID: </span>alarms-monitoring</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with some predefined monitoring alarms for the services
  installed.
 </p><p>
  Full details of all service alarms can be found here:
  <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a>.
 </p><p>
  Each alarm will have one of the following statuses:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="guimenu ">Critical</span> - Open alarms, identified by red indicator.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Warning</span> - Open alarms, identified by yellow indicator.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Unknown</span> - Open alarms, identified by gray indicator.
    Unknown will be the status of an alarm that has stopped receiving a metric.
    This can be caused by the following conditions:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      An alarm exists for a service or component that is not installed in the
      environment.
     </p></li><li class="listitem "><p>
      An alarm exists for a virtual machine or node that previously existed but
      has been removed without the corresponding alarms being removed.
     </p></li><li class="listitem "><p>
      There is a gap between the last reported metric and the next metric.
     </p></li></ul></div></li><li class="listitem "><p>
    <span class="guimenu ">Open</span> - Complete list of open alarms.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Total</span> - Complete list of alarms, may include
    Acknowledged and Resolved alarms.
   </p></li></ul></div><p>
  When alarms are triggered it is helpful to review the service logs.
 </p></div></div><div class="sect2" id="configure-monitoring"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Monitoring Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#configure-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-configure_monitoring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-configure_monitoring.xml</li><li><span class="ds-label">ID: </span>configure-monitoring</li></ul></div></div></div></div><p>
  The monitoring service, based on monasca, allows you to configure an external
  SMTP server for email notifications when alarms trigger. You also have
  options for your alarm metrics database should you choose not to use the
  default option provided with the product.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> you have the option to specify a SMTP server for email
  notifications and a database platform you want to use for the metrics
  database. These steps will assist in this process.
 </p><div class="sect3" id="config-mon-notemail"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Monitoring Email Notification Settings</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#config-mon-notemail">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>config-mon-notemail</li></ul></div></div></div></div><p>
  The monitoring service, based on monasca, allows you to configure an external
  SMTP server for email notifications when alarms trigger. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you
  have the option to specify a SMTP server for email notifications. These steps
  will assist in this process.
 </p><p>
  If you are going to use the email notifiication feature of the monitoring
  service, you must set the configuration options with valid email settings
  including an SMTP server and valid email addresses. The email server is not
  provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, but must be specified in the configuration file
  described below. The email server must support SMTP.
 </p><div class="sect4" id="email"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring monitoring notification settings during initial installation</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#email">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>email</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To change the SMTP server configuration settings edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/cloudConfig.yml</pre></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Enter your email server settings. Here is an example snippet showing the
       configuration file contents, uncomment these lines before entering your
       environment details.
      </p><div class="verbatim-wrap"><pre class="screen">    smtp-settings:
    #  server: mailserver.examplecloud.com
    #  port: 25
    #  timeout: 15
    # These are only needed if your server requires authentication
    #  user:
    #  password:</pre></div><p>
       This table explains each of these values:
      </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>Server (required)</td><td>
           <p>
            The server entry must be uncommented and set to a valid hostname or
            IP Address.
           </p>
          </td></tr><tr><td>Port (optional)</td><td>
           <p>
            If your SMTP server is running on a port other than the standard
            25, then uncomment the port line and set it your port.
           </p>
          </td></tr><tr><td>Timeout (optional)</td><td>
           <p>
            If your email server is heavily loaded, the timeout parameter can
            be uncommented and set to a larger value. 15 seconds is the
            default.
           </p>
          </td></tr><tr><td>User / Password (optional)</td><td>
           <p>
            If your SMTP server requires authentication, then you can configure
            user and password. Use double quotes around the password to avoid
            issues with special characters.
           </p>
          </td></tr></tbody></table></div></li></ol></div></li><li class="listitem "><p>
     To configure the sending email addresses, edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</pre></div><p>
     Modify the following value to add your sending email address:
    </p><div class="verbatim-wrap"><pre class="screen">email_from_addr</pre></div><div id="id-1.5.15.3.4.4.4.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The default value in the file is <code class="literal">email_from_address:
      notification@exampleCloud.com</code> which you should edit.
     </p></div></li><li class="listitem "><p>
     [optional] To configure the receiving email addresses, edit the following
     file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-default-alarms/defaults/main.yml</pre></div><p>
     Modify the following value to configure a receiving email address:
    </p><div class="verbatim-wrap"><pre class="screen">notification_address</pre></div><div id="id-1.5.15.3.4.4.4.2.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can also set the receiving email address via the Operations Console.
      Instructions for this are in the last section.
     </p></div></li><li class="listitem "><p>
     If your environment requires a proxy address then you can add that in as
     well:
    </p><div class="verbatim-wrap"><pre class="screen"># notification_environment can be used to configure proxies if needed.
# Below is an example configuration. Note that all of the quotes are required.
# notification_environment: '"http_proxy=http://&lt;your_proxy&gt;:&lt;port&gt;" "https_proxy=http://&lt;your_proxy&gt;:&lt;port&gt;"'
<span class="bold"><strong>notification_environment: ''</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Updated monitoring service email notification settings"</pre></div></li><li class="listitem "><p>
     Continue with your installation.
    </p></li></ol></div></div><div class="sect4" id="apache-commons-validate"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca and Apache Commons Validator</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#apache-commons-validate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>apache-commons-validate</li></ul></div></div></div></div><p>
   The monasca notification uses a standard Apache Commons validator to
   validate the configured <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> domain names before sending the
   notification over webhook. monasca notification supports some non-standard
   domain names, but not all. See the Domain Validator documentation for more
   information:
   <a class="link" href="https://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/DomainValidator.html" target="_blank">https://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/DomainValidator.html</a>
  </p><p>
   You should ensure that any domains that you use are supported by IETF and
   IANA. As an example, <span class="bold"><strong>.local</strong></span> is not listed
   by IANA and is invalid but <span class="bold"><strong>.gov</strong></span> and
   <span class="bold"><strong>.edu</strong></span> are valid.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Internet Assigned Numbers Authority (IANA):
     <a class="link" href="https://www.iana.org/domains/root/db" target="_blank">https://www.iana.org/domains/root/db</a>
    </p></li></ul></div><p>
   Failure to use supported domains will generate an unprocessable exception in
   monasca notification create:
  </p><div class="verbatim-wrap"><pre class="screen">HTTPException code=422 message={"unprocessable_entity":
{"code":422,"message":"Address https://myopenstack.sample:8000/v1/signal/test is not of correct format","details":"","internal_code":"c6cf9d9eb79c3fc4"}</pre></div></div><div class="sect4" id="id-1.5.15.3.4.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring monitoring notification settings after the initial installation</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.4.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you need to make changes to the email notification settings after your
   initial deployment, you can change the "From" address using the
   configuration files but the "To" address will need to be changed in the
   Operations Console. The following section will describe both of these
   processes.
  </p><p>
   <span class="bold"><strong>To change the sending email address:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To configure the sending email addresses, edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</pre></div><p>
     Modify the following value to add your sending email address:
    </p><div class="verbatim-wrap"><pre class="screen">email_from_addr</pre></div><div id="id-1.5.15.3.4.4.6.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The default value in the file is <code class="literal">email_from_address:
      notification@exampleCloud.com</code> which you should edit.
     </p></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Updated monitoring service email notification settings"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the monasca reconfigure playbook to deploy the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</pre></div><div id="id-1.5.15.3.4.4.6.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You may need to use the <code class="literal">--ask-vault-pass</code> switch if you
      opted for encryption during the initial deployment.
     </p></div></li></ol></div><p>
   <span class="bold"><strong>To change the receiving email address via the
   Operations Console:</strong></span>
  </p><p>
   To configure the "To" email address, after installation,
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Connect to and log in to the Operations Console.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left side, click
     <span class="bold"><strong>Home</strong></span>, and then
     <span class="bold"><strong>Alarm Explorer</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Alarm Explorer</strong></span> page, at the top,
     click the <span class="bold"><strong>Notification Methods</strong></span> text.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Notification Methods</strong></span> page, find
     the row with the <span class="bold"><strong>Default Email</strong></span>
     notification.
    </p></li><li class="listitem "><p>
     In the <span class="bold"><strong>Default Email</strong></span> row, click the
     details icon (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-DetailDots.png" width="" alt="Ellipsis Icon" /></span>), then click
     <span class="bold"><strong>Edit</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Edit Notification Method: Default
     Email</strong></span> page, in <span class="bold"><strong>Name</strong></span>,
     <span class="bold"><strong>Type</strong></span>, and
     <span class="bold"><strong>Address/Key</strong></span>, type in the values you want
     to use.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Edit Notification Method: Default
     Email</strong></span> page, click <span class="bold"><strong>Update
     Notification</strong></span>.
    </p></li></ol></div><div id="id-1.5.15.3.4.4.6.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Once the notification has been added, using the procedures using the
    Ansible playbooks will not change it.
   </p></div></div></div><div class="sect3" id="manage-note-methods"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Notification Methods for Alarms</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#manage-note-methods">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>manage-note-methods</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#proxy" title="13.1.2.2.1. Enabling a Proxy for Webhook or Pager Duty Notifications">Section 13.1.2.2.1, “Enabling a Proxy for Webhook or Pager Duty Notifications”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#create" title="13.1.2.2.2. Creating a New Notification Method">Section 13.1.2.2.2, “Creating a New Notification Method”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#apply" title="13.1.2.2.3. Applying a Notification Method to an Alarm Definition">Section 13.1.2.2.3, “Applying a Notification Method to an Alarm Definition”</a>
   </p></li></ul></div><div class="sect4" id="proxy"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling a Proxy for Webhook or Pager Duty Notifications</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#proxy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>proxy</li></ul></div></div></div></div><p>
   If your environment requires a proxy in order for communications to function
   then these steps will show you how you can enable one. These steps will only
   be needed if you are utilizing the webhook or pager duty notification
   methods.
  </p><p>
   These steps will require access to the Cloud Lifecycle Manager in your cloud
   deployment so you may need to contact your Administrator. You can make these
   changes during the initial configuration phase prior to the first
   installation or you can modify your existing environment, the only
   difference being the last step.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the
     <code class="literal">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</code>
     file and edit the line below with your proxy address values:
    </p><div class="verbatim-wrap"><pre class="screen">notification_environment: '"http_proxy=http://&lt;proxy_address&gt;:&lt;port&gt;" "https_proxy=&lt;http://proxy_address&gt;:&lt;port&gt;"'</pre></div><div id="id-1.5.15.3.4.5.3.4.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      There are single quotation marks around the entire value of this entry and
      then double quotation marks around the individual proxy entries. This
      formatting must exist when you enter these values into your configuration
      file.
     </p></div></li><li class="step "><p>
     If you are making these changes prior to your initial installation then
     you are done and can continue on with the installation. However, if you
     are modifying an existing environment, you will need to continue on with
     the remaining steps below.
    </p></li><li class="step "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Generate an updated deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the monasca reconfigure playbook to enable these changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</pre></div></li></ol></div></div></div><div class="sect4" id="create"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a New Notification Method</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#create">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>create</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Operations Console.
    </p></li><li class="step "><p>
     Use the navigation menu to go to the <span class="bold"><strong>Alarm
     Explorer</strong></span> page:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod.png" width="" /></a></div></div></li><li class="step "><p>
     Select the <span class="bold"><strong>Notification Methods</strong></span> menu and
     then click the <span class="bold"><strong>Create Notification Method</strong></span>
     button:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod1.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod1.png" width="" /></a></div></div></li><li class="step "><p>
     On the <span class="bold"><strong>Create Notification Method</strong></span> window
     you will select your options and then click the
     <span class="bold"><strong>Create Notification</strong></span> button.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod3.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod3.png" width="" /></a></div></div><p>
     A description of each of the fields you use for each notification method:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>Name</td><td><p>Enter a unique name value for the notification method you are creating.</p></td></tr><tr><td>Type</td><td><p>Choose a type. Available values are <span class="bold"><strong>Webhook</strong></span>, <span class="bold"><strong>Email</strong></span>, or <span class="bold"><strong>Pager Duty</strong></span>.</p></td></tr><tr><td>Address/Key</td><td>Enter the value corresponding to the type you chose.</td></tr></tbody></table></div></li></ol></div></div></div><div class="sect4" id="apply"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Applying a Notification Method to an Alarm Definition</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#apply">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>apply</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Operations Console.
    </p></li><li class="step "><p>
     Use the navigation menu to go to the <span class="bold"><strong>Alarm
     Explorer</strong></span> page:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod.png" width="" /></a></div></div></li><li class="step "><p>
     Select the <span class="bold"><strong>Alarm Definition</strong></span> menu which
     will give you a list of each of the alarm definitions in your environment.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod4.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod4.png" width="" /></a></div></div></li><li class="step "><p>
     Locate the alarm you want to change the notification method for and click
     on its name to bring up the edit menu. You can use the sorting methods for
     assistance.
    </p></li><li class="step "><p>
     In the edit menu, scroll down to the <span class="bold"><strong>Notifications
     and Severity</strong></span> section where you will select one or more
     <span class="bold"><strong>Notification Methods</strong></span> before selecting the
     <span class="bold"><strong>Update Alarm Definition</strong></span> button:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod6.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod6.png" width="" /></a></div></div></li><li class="step "><p>
     Repeat as needed until all of your alarms have the notification methods
     you desire.
    </p></li></ol></div></div></div></div><div class="sect3" id="enable-rabbitmq-admin-console"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the RabbitMQ Admin Console</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#enable-rabbitmq-admin-console">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-enable_mon_console.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-enable_mon_console.xml</li><li><span class="ds-label">ID: </span>enable-rabbitmq-admin-console</li></ul></div></div></div></div><p>
  The RabbitMQ Admin Console is off by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. You can turn on the
  console by following these steps:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Edit the <code class="filename">~/openstack/my_cloud/config/rabbitmq/main.yml</code>
    file. Under the <code class="literal">rabbit_plugins:</code>line, uncomment
   </p><div class="verbatim-wrap"><pre class="screen">- rabbitmq_management</pre></div></li><li class="listitem "><p>
    Commit your configuration to the local Git repository (see
    <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enabled RabbitMQ Admin Console"</pre></div></li><li class="listitem "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run the RabbitMQ reconfigure playbook to deploy the changes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-reconfigure.yml</pre></div></li></ol></div><p>
  To turn the RabbitMQ Admin Console off again, add the comment back and repeat
  steps 3 through 6.
  
 </p></div><div class="sect3" id="capacity-reporting-transform"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Capacity Reporting and Monasca Transform</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#capacity-reporting-transform">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-capacity_reporting_transform.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-capacity_reporting_transform.xml</li><li><span class="ds-label">ID: </span>capacity-reporting-transform</li></ul></div></div></div></div><p>
  Capacity reporting is a new feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which will provide cloud
  operators overall capacity (available, used, and remaining) information via
  the Operations Console so that the cloud operator can ensure that cloud resource pools
  have sufficient capacity to meet the demands of users.  The cloud operator is
  also able to set thresholds and set alarms to be notified when the thresholds
  are reached.
 </p><p>
  <span class="bold"><strong>For Compute</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Host Capacity - CPU/Disk/Memory: Used, Available and Remaining Capacity -
    for the entire cloud installation or by host
   </p></li><li class="listitem "><p>
    VM Capacity - CPU/Disk/Memory: Allocated, Available and Remaining - for
    the entire cloud installation, by host or by project
   </p></li></ul></div><p>
  <span class="bold"><strong>For Object Storage</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Disk Capacity - Used, Available and Remaining Capacity - for the entire
    cloud installation or by project
   </p></li></ul></div><p>
  In addition to overall capacity, roll up views with appropriate slices provide
  views by a particular project, or compute node. Graphs also show trends and
  the change in capacity over time.
 </p><div class="sect4" id="crt-features"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">monasca Transform Features</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-features">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_features.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_features.xml</li><li><span class="ds-label">ID: </span>crt-features</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    monasca Transform is a new component in monasca which transforms and
    aggregates metrics using Apache Spark
   </p></li><li class="listitem "><p>
    Aggregated metrics are published to Kafka and are available for other
    monasca components like monasca-threshold and are stored in monasca
    datastore
   </p></li><li class="listitem "><p>
    Cloud operators can set thresholds and set alarms to receive notifications
    when thresholds are met.
   </p></li><li class="listitem "><p>
    These aggregated metrics are made available to the cloud operators via
    Operations Console's new Capacity Summary (reporting) UI
   </p></li><li class="listitem "><p>
    Capacity reporting is a new feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which will provides
    cloud operators an overall capacity (available, used and remaining) for
    Compute and Object Storage
   </p></li><li class="listitem "><p>
    Cloud operators can look at Capacity reporting via Operations Console's Compute
    Capacity Summary and Object Storage Capacity Summary UI
   </p></li><li class="listitem "><p>
    Capacity reporting allows the cloud operators the ability to ensure that
    cloud resource pools have sufficient capacity to meet demands of users. See
    table below for Service and Capacity Types.
   </p></li><li class="listitem "><p>
    A list of aggregated metrics is provided in
    <a class="xref" href="topic-ttn-5fg-4v.html#crt-aggregated-metrics" title="13.1.2.4.4. New Aggregated Metrics">Section 13.1.2.4.4, “New Aggregated Metrics”</a>.
   </p></li><li class="listitem "><p>
    Capacity reporting aggregated metrics are aggregated and published every
    hour
   </p></li><li class="listitem "><p>
    In addition to the overall capacity, there are graphs which show the
    capacity trends over time range (for 1 day, for 7 days, for 30 days or for
    45 days)
   </p></li><li class="listitem "><p>
    Graphs showing the capacity trends by a particular project or compute host
    are also provided.
   </p></li><li class="listitem "><p>
    monasca Transform is integrated with centralized monitoring (monasca) and
    centralized logging
   </p></li><li class="listitem "><p>
    Flexible Deployment
   </p></li><li class="listitem "><p>
    Upgrade &amp; Patch Support
   </p></li></ul></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Type of Capacity</th><th>Description</th></tr></thead><tbody><tr><td>Compute</td><td>Host Capacity</td><td>
      <p>
       CPU/Disk/Memory: Used, Available and Remaining Capacity - for entire
       cloud installation or by compute host
      </p>
     </td></tr><tr><td> </td><td>VM Capacity</td><td>
      <p>
       CPU/Disk/Memory: Allocated, Available and Remaining - for entire cloud
       installation, by host or by project
      </p>
     </td></tr><tr><td>Object Storage</td><td>Disk Capacity</td><td>
      <p>
       Used, Available and Remaining Disk Capacity - for entire cloud
       installation or by project
      </p>
     </td></tr><tr><td> </td><td>Storage Capacity</td><td>
      <p>
       Utilized Storage Capacity - for entire cloud installation or by project
      </p>
     </td></tr></tbody></table></div></div><div class="sect4" id="crt-arch-transform-spark"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Architecture for Monasca Transform and Spark</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-arch-transform-spark">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_arch_transform_spark.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_arch_transform_spark.xml</li><li><span class="ds-label">ID: </span>crt-arch-transform-spark</li></ul></div></div></div></div><p>
  monasca Transform is a new component in monasca. monasca Transform uses
  Spark for data aggregation. Both monasca Transform and Spark are depicted in
  the example diagram below.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-monasca-Monasca_Service_Arch_Diagram.png" target="_blank"><img src="images/media-monasca-Monasca_Service_Arch_Diagram.png" width="" /></a></div></div><p>
  You can see that the monasca components run on the Cloud Controller nodes,
  and the monasca agents run on all nodes in the Mid-scale Example
  configuration.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png" target="_blank"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div></div><div class="sect4" id="crt-components"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Components for Capacity Reporting</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-components">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span>crt-components</li></ul></div></div></div></div><div class="sect5" id="id-1.5.15.3.4.7.10.2"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">monasca Transform: Data Aggregation Reporting</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.4.7.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   monasca-transform is a new component which provides mechanism to aggregate
   or transform metrics and publish new aggregated metrics to monasca.
  </p><p>
   monasca Transform is a data driven Apache Spark based data aggregation
   engine which collects, groups and aggregates existing individual monasca
   metrics according to business requirements and publishes new transformed
   (derived) metrics to the monasca Kafka queue.
  </p><p>
   Since the new transformed metrics are published as any other metric in
   monasca, alarms can be set and triggered on the transformed metric, just
   like any other metric.
  </p></div><div class="sect5" id="id-1.5.15.3.4.7.10.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage and Compute Capacity Summary Operations Console UI</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.4.7.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A new "Capacity Summary" tab for Compute and Object Storage will displays
   all the aggregated metrics under the "Compute" and "Object Storage"
   sections.
  </p><p>
   Operations Console UI makes calls to monasca API to retrieve and display various
   tiles and graphs on Capacity Summary tab in Compute and Object Storage
   Summary UI pages.
  </p></div><div class="sect5" id="id-1.5.15.3.4.7.10.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.4.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persist new metrics and Trigger Alarms</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.4.7.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   New aggregated metrics will be published to monasca's Kafka queue and will
   be ingested by monasca-persister. If thresholds and alarms have been set on
   the aggregated metrics, monasca will generate and trigger alarms as it
   currently does with any other metric. No new/additional change is expected
   with persisting of new aggregated metrics or setting threshold/alarms.
  </p></div></div><div class="sect4" id="crt-aggregated-metrics"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Aggregated Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-aggregated-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_aggregated_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_aggregated_metrics.xml</li><li><span class="ds-label">ID: </span>crt-aggregated-metrics</li></ul></div></div></div></div><p>
  Following is the list of aggregated metrics produced by monasca transform in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
 </p><div class="table" id="table-ztc-yn5-3y"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.1: </span><span class="name">Aggregated Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#table-ztc-yn5-3y">#</a></h6></div><div class="table-contents"><table class="table" summary="Aggregated Metrics" border="1"><colgroup><col align="left" class="c1" /><col align="left" class="c2" /><col align="left" class="c3" /><col align="left" class="c4" /><col align="left" class="c5" /><col align="left" class="c6" /></colgroup><thead><tr><th align="left"> </th><th align="left">Metric Name</th><th align="left">For</th><th align="left">Description</th><th align="left">Dimensions</th><th align="left">Notes</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">
      <p>
       cpu.utilized_logical_cores_agg
      </p>
     </td><td align="left">compute summary </td><td align="left">
      <p>
       utilized physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">2</td><td align="left">cpu.total_logical_cores_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">3</td><td align="left">mem.total_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">4</td><td align="left">mem.usable_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       usable physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">5</td><td align="left">disk.total_used_space_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       utilized physical host disk capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">6</td><td align="left">disk.total_space_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host disk capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">7</td><td align="left">nova.vm.cpu.total_allocated_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       cpus allocated across all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">8</td><td align="left">vcpus_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       virtual cpus allocated capacity for VMs of one or all projects by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all or &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">9</td><td align="left">nova.vm.mem.total_allocated_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory allocated to all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">10</td><td align="left">vm.mem.used_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory utilized by VMs of one or all projects by time interval (defaults
       to an hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">11</td><td align="left">vm.mem.total_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory allocated to VMs of one or all projects by time interval
       (defaults to an hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">12</td><td align="left">vm.cpu.utilization_perc_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       cpu utilized by all VMs by project by time interval (defaults to an
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">13</td><td align="left">nova.vm.disk.total_allocated_gb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       disk space allocated to all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">14</td><td align="left">vm.disk.allocation_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       disk allocation for VMs of one or all projects by time interval
       (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all or &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">15</td><td align="left">swiftlm.diskusage.val.size_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       total available object storage capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">16</td><td align="left">swiftlm.diskusage.val.avail_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       remaining object storage capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">17</td><td align="left">swiftlm.diskusage.rate_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       rate of change of object storage usage by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">18</td><td align="left">storage.objects.size_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       used object storage capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr></tbody></table></div></div></div><div class="sect4" id="crt-deployment"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-deployment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_deployment.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_deployment.xml</li><li><span class="ds-label">ID: </span>crt-deployment</li></ul></div></div></div></div><p>
  monasca Transform and Spark will be deployed on the same control plane nodes
  along with Logging and Monitoring Service (monasca).
 </p><p>
  <span class="bold"><strong>Security Consideration during deployment of monasca
  Transform and Spark</strong></span>
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring system connects internally to the Kafka and Spark
  technologies without authentication. If you choose to deploy Monitoring,
  configure it to use only trusted networks such as the Management network, as
  illustrated on the network diagrams below for Entry Scale Deployment and Mid
  Scale Deployment.
 </p><p>
  <span class="bold"><strong>Entry Scale Deployment</strong></span>
 </p><p>
  In Entry Scale Deployment monasca Transform and Spark will be deployed on
  Shared Control Plane along with other Openstack Services along with
  Monitoring and Logging
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-entryScale-Entry-ScaleAllNetworks.png" target="_blank"><img src="images/media-entryScale-Entry-ScaleAllNetworks.png" width="" /></a></div></div><p>
  <span class="bold"><strong>Mid scale Deployment</strong></span>
 </p><p>
  In a Mid Scale Deployment monasca Transform and Spark will be deployed on
  dedicated Metering Monitoring and Logging (MML) control plane along with
  other data processing intensive services like Metering, Monitoring and
  Logging
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png" target="_blank"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div><p>
  <span class="bold"><strong>Multi Control Plane Deployment</strong></span>
 </p><p>
  In a Multi Control Plane Deployment, monasca Transform and Spark will be
  deployed on the Shared Control plane along with rest of monasca Components.
 </p><p>
  <span class="bold"><strong>Start, Stop and Status for monasca Transform and Spark
  processes</strong></span>
 </p><p>
  The service management methods for monasca-transform and spark follow the
  convention for services in the <span class="productname">OpenStack</span> platform. When executing from the
  deployer node, the commands are as follows:
 </p><p>
  <span class="bold"><strong>Status</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-status.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-status.yml</pre></div><p>
  <span class="bold"><strong>Start</strong></span>
 </p><p>
  As monasca-transform depends on spark for the processing of the metrics spark
  will need to be started before monasca-transform.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-start.yml</pre></div><p>
  <span class="bold"><strong>Stop</strong></span>
 </p><p>
  As a precaution, stop the monasca-transform service before
  taking spark down. Interruption to the spark service altogether while
  monasca-transform is still running can result in a monasca-transform process
  that is unresponsive and needing to be tidied up.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-stop.yml</pre></div></div><div class="sect4" id="crt-reconfigure"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfigure</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-reconfigure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_reconfigure.xml</li><li><span class="ds-label">ID: </span>crt-reconfigure</li></ul></div></div></div></div><p>
  The reconfigure process can be triggered again from the deployer. Presuming
  that changes have been made to the variables in the appropriate places
  execution of the respective ansible scripts will be enough to update the
  configuration. The spark reconfigure process alters the nodes serially
  meaning that spark is never down altogether, each node is stopped in turn
  and zookeeper manages the leaders accordingly. This means that
  monasca-transform may be left running even while spark is upgraded.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></div><div class="sect4" id="crt-adding-transform-spark"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding monasca Transform and Spark to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Deployment</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-adding-transform-spark">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_adding_transform_spark.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_adding_transform_spark.xml</li><li><span class="ds-label">ID: </span>crt-adding-transform-spark</li></ul></div></div></div></div><p>
  Since monasca Transform and Spark are optional components, the users might
  elect to not install these two components during their initial <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  install. The following instructions provide a way the users can add monasca
  Transform and Spark to their existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Add monasca Transform and Spark to the input model. monasca Transform and
    Spark on a entry level cloud would be installed on the common control
    plane, for mid scale cloud which has a MML (Metering, Monitoring and
    Logging) cluster, monasca Transform and Spark will should be added to
    MML cluster.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div><p>
    Add spark and monasca-transform to input model, control_plane.yml
   </p><div class="verbatim-wrap"><pre class="screen">clusters
       - name: core
         cluster-prefix: c1
         server-role: CONTROLLER-ROLE
         member-count: 3
         allocation-policy: strict
         service-components:

           [...]

           - zookeeper
           - kafka
           - cassandra
           - storm
           - spark
           - monasca-api
           - monasca-persister
           - monasca-notifier
           - monasca-threshold
           - monasca-client
           - monasca-transform

           [...]</pre></div></li><li class="listitem "><p>
    Run the Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Adding monasca Transform and Spark"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Run Ready Deployment
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run Cloud Lifecycle Manager Deploy
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li></ol></div><p>
  <span class="bold"><strong>Verify Deployment</strong></span>
 </p><p>
  Login to each controller node and run
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service monasca-transform status
<code class="prompt user">tux &gt; </code>sudo service spark-master status
<code class="prompt user">tux &gt; </code>sudo service spark-worker status</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service monasca-transform status
● monasca-transform.service - monasca Transform Daemon
  Loaded: loaded (/etc/systemd/system/monasca-transform.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:47:56 UTC; 2 days ago
Main PID: 7351 (bash)
  CGroup: /system.slice/monasca-transform.service
          ├─ 7351 bash /etc/monasca/transform/init/start-monasca-transform.sh
          ├─ 7352 /opt/stack/service/monasca-transform/venv//bin/python /opt/monasca/monasca-transform/lib/service_runner.py
          ├─27904 /bin/sh -c export SPARK_HOME=/opt/stack/service/spark/venv/bin/../current &amp;&amp; spark-submit --supervise --master spark://omega-cp1-c1-m1-mgmt:7077,omega-cp1-c1-m2-mgmt:7077,omega-cp1-c1...
          ├─27905 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -cp /opt/stack/service/spark/venv/lib/drizzle-jdbc-1.3.jar:/opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/v...
          └─28355 python /opt/monasca/monasca-transform/lib/driver.py
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.


<code class="prompt user">tux &gt; </code>sudo service spark-worker status
● spark-worker.service - Spark Worker Daemon
  Loaded: loaded (/etc/systemd/system/spark-worker.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:46:05 UTC; 2 days ago
Main PID: 63513 (bash)
  CGroup: /system.slice/spark-worker.service
          ├─ 7671 python -m pyspark.daemon
          ├─28948 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0...
          ├─63513 bash /etc/spark/init/start-spark-worker.sh &amp;
          └─63514 /usr/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0.jar:/opt/stack/service/spark/ven...
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.



<code class="prompt user">tux &gt; </code>sudo service spark-master status
● spark-master.service - Spark Master Daemon
  Loaded: loaded (/etc/systemd/system/spark-master.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:44:24 UTC; 2 days ago
Main PID: 55572 (bash)
  CGroup: /system.slice/spark-master.service
          ├─55572 bash /etc/spark/init/start-spark-master.sh &amp;
          └─55573 /usr/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0.jar:/opt/stack/service/spark/ven...
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</pre></div></div><div class="sect4" id="crt-increasing-scale"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increase monasca Transform Scale</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-increasing-scale">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_increasing_scale.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_increasing_scale.xml</li><li><span class="ds-label">ID: </span>crt-increasing-scale</li></ul></div></div></div></div><p>
  monasca Transform in the default configuration can scale up to estimated
  data for 100 node cloud deployment. Estimated maximum rate of metrics from a
  100 node cloud deployment is 120M/hour.
 </p><p>
  You can further increase the processing rate to 180M/hour. Making
  the Spark configuration change will increase the CPU's being used by Spark
  and monasca Transform from average of around 3.5 to 5.5 CPU's per control
  node over a 10 minute batch processing interval.
 </p><p>
  To increase the processing rate to 180M/hour the customer will have to make
  following spark configuration change.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Edit /var/lib/ardana/openstack/my_cloud/config/spark/spark-defaults.conf.j2 and
    set spark.cores.max to 6 and spark.executor.cores 2
   </p><p>
    <span class="bold"><strong>Set spark.cores.max to 6</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">spark.cores.max {{ spark_cores_max }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">spark.cores.max 6</pre></div><p>
    <span class="bold"><strong>Set spark.executor.cores to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">spark.executor.cores {{ spark_executor_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">spark.executor.cores 2</pre></div></li><li class="listitem "><p>
    Edit ~/openstack/my_cloud/config/spark/spark-env.sh.j2
   </p><p>
    <span class="bold"><strong>Set SPARK_WORKER_CORES to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES={{ spark_worker_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES=2</pre></div></li><li class="listitem "><p>
    Edit ~/openstack/my_cloud/config/spark/spark-worker-env.sh.j2
   </p><p>
    <span class="bold"><strong>Set SPARK_WORKER_CORES to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES={{ spark_worker_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES=2</pre></div></li><li class="listitem "><p>
    Run Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Changing Spark Config increase scale"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Run Ready Deployment
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run spark-reconfigure.yml and monasca-transform-reconfigure.yml
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></li></ol></div></div><div class="sect4" id="crt-change-compute-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change Compute Host Pattern Filter in Monasca Transform</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crt-change-compute-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_change_compute_host.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_change_compute_host.xml</li><li><span class="ds-label">ID: </span>crt-change-compute-host</li></ul></div></div></div></div><p>
  monasca Transform identifies compute host metrics by pattern matching on
  hostname dimension in the incoming monasca metrics. The default pattern is of
  the form <code class="literal">comp<em class="replaceable ">NNN</em></code>. For example,
  <code class="literal">comp001</code>, <code class="literal">comp002</code>, etc. To filter for it
  in the transformation specs, use the expression
  <code class="literal">-comp[0-9]+-</code>. In case the compute
  host names follow a different pattern other than the standard pattern above,
  the filter by expression when aggregating metrics will have to be changed.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the deployer: Edit
     <code class="filename">~/openstack/my_cloud/config/monasca-transform/transform_specs.json.j2</code>
   </p></li><li class="step " id="st-monasca-check-comp-reference"><p>
    Look for all references of <code class="literal">-comp[0-9]+-</code> and change the
    regular expression to the desired pattern say for example
    <code class="literal">-compute[0-9]+-</code>.
   </p><div class="verbatim-wrap"><pre class="screen">{"aggregation_params_map":{"aggregation_pipeline":{"source":"streaming", "usage":"fetch_quantity", "setters":["rollup_quantity", "set_aggregated_metric_name", "set_aggregated_period"], "insert":["prepare_data","insert_data_pre_hourly"]}, "aggregated_metric_name":"mem.total_mb_agg", "aggregation_period":"hourly", "aggregation_group_by_list": ["host", "metric_id", "tenant_id"], "usage_fetch_operation": "avg", "filter_by_list": [{"field_to_filter": "host", "filter_expression": "-comp[0-9]+", "filter_operation": "include"}], "setter_rollup_group_by_list":[], "setter_rollup_operation": "sum", "dimension_list":["aggregation_period", "host", "project_id"], "pre_hourly_operation":"avg", "pre_hourly_group_by_list":["default"]}, "metric_group":"mem_total_all", "metric_id":"mem_total_all"}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">{"aggregation_params_map":{"aggregation_pipeline":{"source":"streaming", "usage":"fetch_quantity", "setters":["rollup_quantity", "set_aggregated_metric_name", "set_aggregated_period"], "insert":["prepare_data", "insert_data_pre_hourly"]}, "aggregated_metric_name":"mem.total_mb_agg", "aggregation_period":"hourly", "aggregation_group_by_list": ["host", "metric_id", "tenant_id"],"usage_fetch_operation": "avg","filter_by_list": [{"field_to_filter": "host","filter_expression": "-compute[0-9]+", "filter_operation": "include"}], "setter_rollup_group_by_list":[], "setter_rollup_operation": "sum", "dimension_list":["aggregation_period", "host", "project_id"], "pre_hourly_operation":"avg", "pre_hourly_group_by_list":["default"]}, "metric_group":"mem_total_all", "metric_id":"mem_total_all"}</pre></div><div id="id-1.5.15.3.4.7.16.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     The filter_expression has been changed to the <span class="emphasis"><em>new</em></span>
     pattern.
    </p></div></li><li class="step "><p>
    To change all host metric transformation specs in the same
    JSON file, repeat <a class="xref" href="topic-ttn-5fg-4v.html#st-monasca-check-comp-reference" title="Step 2">Step 2</a>.
   </p><p>
    Transformation specs will have to be changed for following metric_ids
    namely "mem_total_all", "mem_usable_all", "disk_total_all",
    "disk_usable_all", "cpu_total_all", "cpu_total_host", "cpu_util_all",
    "cpu_util_host"
   </p></li><li class="step "><p>
     Run the Configuration Processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Changing monasca Transform specs"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run Ready Deployment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run monasca Transform Reconfigure:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect3" id="config-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Availability of Alarm Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#config-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_metrics.xml</li><li><span class="ds-label">ID: </span>config-metrics</li></ul></div></div></div></div><p>
  Using the monasca agent tuning knobs, you can choose which alarm metrics are
  available in your environment.
 </p><p>
  The addition of the libvirt and OVS plugins to the monasca agent provides a
  number of additional metrics that can be used. Most of these metrics are
  included by default, but others are not. You have the ability to use tuning
  knobs to add or remove these metrics to your environment based on your
  individual needs in your cloud.
 </p><p>
  We will list these metrics along with the tuning knob name and instructions
  for how to adjust these.
 </p><div class="sect4" id="libvirt-tuningknobs"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Libvirt plugin metric tuning knobs</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#libvirt-tuningknobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-libvirt_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_tuningknobs.xml</li><li><span class="ds-label">ID: </span>libvirt-tuningknobs</li></ul></div></div></div></div><p>
  The following metrics are added as part of the libvirt plugin:
 </p><div id="id-1.5.15.3.4.8.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For a description of each of these metrics, see
   <a class="xref" href="topic-ttn-5fg-4v.html#libvirt-metrics" title="13.1.4.11. Libvirt Metrics">Section 13.1.4.11, “Libvirt Metrics”</a>.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="newCol2" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Tuning Knob</th><th>Default Setting</th><th>Admin Metric Name</th><th>Project Metric Name</th></tr></thead><tbody><tr><td rowspan="3">vm_cpu_check_enable</td><td rowspan="3">True</td><td>vm.cpu.time_ns</td><td>cpu.time_ns</td></tr><tr><td>vm.cpu.utilization_norm_perc</td><td>cpu.utilization_norm_perc</td></tr><tr><td>vm.cpu.utilization_perc</td><td>cpu.utilization_perc</td></tr><tr><td rowspan="10">vm_disks_check_enable</td><td rowspan="10">
      <p>
       True
      </p>
      <p>
       Creates 20 disk metrics per disk device per virtual machine.
      </p>
     </td><td>vm.io.errors</td><td>io.errors</td></tr><tr><td>vm.io.errors_sec</td><td>io.errors_sec</td></tr><tr><td>vm.io.read_bytes</td><td>io.read_bytes</td></tr><tr><td>vm.io.read_bytes_sec</td><td>io.read_bytes_sec</td></tr><tr><td>vm.io.read_ops</td><td>io.read_ops</td></tr><tr><td>vm.io.read_ops_sec</td><td>io.read_ops_sec</td></tr><tr><td>vm.io.write_bytes</td><td>io.write_bytes</td></tr><tr><td>vm.io.write_bytes_sec</td><td>io.write_bytes_sec</td></tr><tr><td>vm.io.write_ops</td><td>io.write_ops</td></tr><tr><td>vm.io.write_ops_sec</td><td> io.write_ops_sec</td></tr><tr><td rowspan="8">vm_network_check_enable</td><td rowspan="8">
      <p>
       True
      </p>
      <p>
       Creates 16 network metrics per NIC per virtual machine.
      </p>
     </td><td>vm.net.in_bytes</td><td>net.in_bytes</td></tr><tr><td>vm.net.in_bytes_sec</td><td>net.in_bytes_sec</td></tr><tr><td>vm.net.in_packets</td><td>net.in_packets</td></tr><tr><td>vm.net.in_packets_sec</td><td>net.in_packets_sec</td></tr><tr><td>vm.net.out_bytes</td><td>net.out_bytes</td></tr><tr><td>vm.net.out_bytes_sec</td><td>net.out_bytes_sec</td></tr><tr><td>vm.net.out_packets</td><td>net.out_packets</td></tr><tr><td>vm.net.out_packets_sec</td><td>net.out_packets_sec</td></tr><tr><td>vm_ping_check_enable</td><td>True</td><td>vm.ping_status</td><td>ping_status</td></tr><tr><td rowspan="6">vm_extended_disks_check_enable</td><td rowspan="3">
      <p>
       True
      </p>
      <p>
       Creates 6 metrics per device per virtual machine.
      </p>
     </td><td>vm.disk.allocation</td><td>disk.allocation</td></tr><tr><td>vm.disk.capacity</td><td>disk.capacity</td></tr><tr><td>vm.disk.physical</td><td>disk.physical</td></tr><tr><td rowspan="3">
      <p>
       True
      </p>
      <p>
       Creates 6 aggregate metrics per virtual machine.
      </p>
     </td><td>vm.disk.allocation_total</td><td>disk.allocation_total</td></tr><tr><td>vm.disk.capacity_total</td><td>disk.capacity.total</td></tr><tr><td>vm.disk.physical_total</td><td>disk.physical_total</td></tr><tr><td rowspan="10">vm_disks_check_enable vm_extended_disks_check_enable</td><td rowspan="10">
      <p>
       True
      </p>
      <p>
       Creates 20 aggregate metrics per virtual machine.
      </p>
     </td><td>vm.io.errors_total</td><td>io.errors_total</td></tr><tr><td>vm.io.errors_total_sec</td><td>io.errors_total_sec</td></tr><tr><td>vm.io.read_bytes_total</td><td>io.read_bytes_total</td></tr><tr><td>vm.io.read_bytes_total_sec</td><td>io.read_bytes_total_sec</td></tr><tr><td>vm.io.read_ops_total</td><td>io.read_ops_total</td></tr><tr><td>vm.io.read_ops_total_sec</td><td>io.read_ops_total_sec</td></tr><tr><td>vm.io.write_bytes_total</td><td>io.write_bytes_total</td></tr><tr><td>vm.io.write_bytes_total_sec</td><td>io.write_bytes_total_sec</td></tr><tr><td>vm.io.write_ops_total</td><td>io.write_ops_total</td></tr><tr><td>vm.io.write_ops_total_sec</td><td>io.write_ops_total_sec</td></tr></tbody></table></div><div class="sect5" id="configuring-libvirt-tuning-knobs"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the libvirt metrics using the tuning knobs</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#configuring-libvirt-tuning-knobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-libvirt_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_tuningknobs.xml</li><li><span class="ds-label">ID: </span>configuring-libvirt-tuning-knobs</li></ul></div></div></div></div><p>
   Use the following steps to configure the tuning knobs for the libvirt plugin
   metrics.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml</pre></div></li><li class="listitem "><p>
     Change the value for each tuning knob to the desired setting,
     <code class="literal">True</code> if you want the metrics created and
     <code class="literal">False</code> if you want them removed. Refer to the table
     above for which metrics are controlled by each tuning knob.
    </p><div class="verbatim-wrap"><pre class="screen">vm_cpu_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_disks_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_extended_disks_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_network_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_ping_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "configuring libvirt plugin tuning knobs"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the nova reconfigure playbook to implement the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div><div id="id-1.5.15.3.4.8.5.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   If you modify either of the following files, then the monasca tuning
   parameters should be adjusted to handle a higher load on the system.
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml
~/openstack/my_cloud/config/neutron/monasca_ovs_plugin.yaml.j2</pre></div><p>
   Tuning parameters are located in
   <code class="filename">~/openstack/my_cloud/config/monasca/configuration.yml</code>.
   The parameter <code class="literal">monasca_tuning_selector_override</code> should be
   changed to the <code class="literal">extra-large</code> setting.
  </p></div></div></div><div class="sect4" id="ovs-tuningknobs"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVS plugin metric tuning knobs</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ovs-tuningknobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ovs_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_tuningknobs.xml</li><li><span class="ds-label">ID: </span>ovs-tuningknobs</li></ul></div></div></div></div><p>
  The following metrics are added as part of the OVS plugin:
 </p><div id="id-1.5.15.3.4.8.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For a description of each of these metrics, see
   <a class="xref" href="topic-ttn-5fg-4v.html#sec-metric-ovs" title="13.1.4.16. Open vSwitch (OVS) Metrics">Section 13.1.4.16, “Open vSwitch (OVS) Metrics”</a>.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="newCol2" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Tuning Knob</th><th>Default Setting</th><th>Admin Metric Name</th><th>Project Metric Name</th></tr></thead><tbody><tr><td rowspan="4">use_rate_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_bytes_sec</td><td>vrouter.in_bytes_sec</td></tr><tr><td>ovs.vrouter.in_packets_sec</td><td>vrouter.in_packets_sec</td></tr><tr><td>ovs.vrouter.out_bytes_sec</td><td>vrouter.out_bytes_sec</td></tr><tr><td>ovs.vrouter.out_packets_sec</td><td>vrouter.out_packets_sec</td></tr><tr><td rowspan="4">use_absolute_metrics</td><td rowspan="4">True</td><td>ovs.vrouter.in_bytes</td><td>vrouter.in_bytes</td></tr><tr><td>ovs.vrouter.in_packets</td><td>vrouter.in_packets</td></tr><tr><td>ovs.vrouter.out_bytes</td><td>vrouter.out_bytes</td></tr><tr><td>ovs.vrouter.out_packets</td><td>vrouter.out_packets</td></tr><tr><td rowspan="4">use_health_metrics with use_rate_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_dropped_sec</td><td>vrouter.in_dropped_sec</td></tr><tr><td>ovs.vrouter.in_errors_sec</td><td>vrouter.in_errors_sec</td></tr><tr><td>ovs.vrouter.out_dropped_sec</td><td>vrouter.out_dropped_sec</td></tr><tr><td>ovs.vrouter.out_errors_sec</td><td>vrouter.out_errors_sec</td></tr><tr><td rowspan="4">use_health_metrics with use_absolute_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_dropped</td><td>vrouter.in_dropped</td></tr><tr><td>ovs.vrouter.in_errors</td><td>vrouter.in_errors</td></tr><tr><td>ovs.vrouter.out_dropped</td><td>vrouter.out_dropped</td></tr><tr><td>ovs.vrouter.out_errors</td><td>vrouter.out_errors</td></tr></tbody></table></div><div class="sect5" id="id-1.5.15.3.4.8.6.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the OVS metrics using the tuning knobs</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.4.8.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ovs_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_tuningknobs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Use the following steps to configure the tuning knobs for the libvirt plugin
   metrics.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/neutron/monasca_ovs_plugin.yaml.j2</pre></div></li><li class="listitem "><p>
     Change the value for each tuning knob to the desired setting,
     <code class="literal">True</code> if you want the metrics created and
     <code class="literal">False</code> if you want them removed. Refer to the table
     above for which metrics are controlled by each tuning knob.
    </p><div class="verbatim-wrap"><pre class="screen">init_config:
   use_absolute_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
   use_rate_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
   use_health_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "configuring OVS plugin tuning knobs"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the neutron reconfigure playbook to implement the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="monasca-notification-plugins"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating HipChat, Slack, and JIRA</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#monasca-notification-plugins">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_plugins_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_plugins_overview.xml</li><li><span class="ds-label">ID: </span>monasca-notification-plugins</li></ul></div></div></div></div><p>
  monasca, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> monitoring and notification service, includes
  three default notification methods, <span class="bold"><strong>email</strong></span>,
  <span class="bold"><strong>PagerDuty</strong></span>, and
  <span class="bold"><strong>webhook</strong></span>. monasca also supports three other
  notification plugins which allow you to send notifications to
  <span class="bold"><strong>HipChat</strong></span>,
  <span class="bold"><strong>Slack</strong></span>, and
  <span class="bold"><strong>JIRA</strong></span>. Unlike the default notification
  methods, the additional notification plugins must be manually configured.
 </p><p>
  This guide details the steps to configure each of the three non-default
  notification plugins. This guide also assumes that your cloud
  is fully deployed and functional.
 </p><div class="sect3" id="hipchat-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the HipChat Plugin</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#hipchat-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_hipchat_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_hipchat_plugin.xml</li><li><span class="ds-label">ID: </span>hipchat-plugin</li></ul></div></div></div></div><p>
  To configure the HipChat plugin you will need the following four pieces of
  information from your HipChat system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The URL of your HipChat system.
   </p></li><li class="listitem "><p>
    A token providing permission to send notifications to your HipChat system.
   </p></li><li class="listitem "><p>
    The ID of the HipChat room you wish to send notifications to.
   </p></li><li class="listitem "><p>
    A HipChat user account. This account will be used to authenticate any
    incoming notifications from your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
   </p></li></ul></div><p>
  <span class="bold"><strong>Obtain a token</strong></span>
 </p><p>
  Use the following instructions to obtain a token from your Hipchat system.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to HipChat as the user account that will be used to authenticate the
    notifications.
   </p></li><li class="listitem "><p>
    Navigate to the following URL:
    <code class="literal">https://&lt;your_hipchat_system&gt;/account/api</code>. Replace
    <code class="literal">&lt;your_hipchat_system&gt;</code> with the
    fully-qualified-domain-name of your HipChat system.
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Create token</strong></span> option. Ensure
    that the token has the "SendNotification" attribute.
   </p></li></ol></div><p>
  <span class="bold"><strong>Obtain a room ID</strong></span>
 </p><p>
  Use the following instructions to obtain the ID of a HipChat room.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to HipChat as the user account that will be used to authenticate the
    notifications.
   </p></li><li class="listitem "><p>
    Select <span class="bold"><strong>My account</strong></span> from the application
    menu.
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Rooms</strong></span> tab.
   </p></li><li class="listitem "><p>
    Select the room that you want your notifications sent to.
   </p></li><li class="listitem "><p>
    Look for the API ID field in the room information. This is the room ID.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create HipChat notification type</strong></span>
 </p><p>
  Use the following instructions to create a HipChat notification type.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Begin by obtaining the API URL for the HipChat room that you wish to send
    notifications to. The format for a URL used to send notifications to a room
    is as follows:
   </p><p>
    <code class="literal">/v2/room/{room_id_or_name}/notification</code>
   </p></li><li class="listitem "><p>
    Use the monasca API to create a new notification method. The following
    example demonstrates how to create a HipChat notification type named
    <span class="bold"><strong>MyHipChatNotification</strong></span>, for room
    <span class="bold"><strong>ID 13</strong></span>, using an example API URL and auth
    token.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  <em class="replaceable ">NAME</em> <em class="replaceable ">TYPE</em> <em class="replaceable ">ADDRESS</em>
<code class="prompt user">ardana &gt; </code>monasca notification-create  MyHipChatNotification HIPCHAT https://hipchat.hpe.net/v2/room/13/notification?auth_token=1234567890</pre></div><p>
    The preceding example creates a notification type with the following
    characteristics
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      NAME: MyHipChatNotification
     </p></li><li class="listitem "><p>
      TYPE: HIPCHAT
     </p></li><li class="listitem "><p>
      ADDRESS: https://hipchat.hpe.net/v2/room/13/notification
     </p></li><li class="listitem "><p>
      auth_token: 1234567890
     </p></li></ul></div></li></ol></div><div id="id-1.5.15.3.5.4.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The horizon dashboard can also be used to create a HipChat notification
   type.
  </p></div></div><div class="sect3" id="monasca-slack-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Slack Plugin</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#monasca-slack-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_slack_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_slack_plugin.xml</li><li><span class="ds-label">ID: </span>monasca-slack-plugin</li></ul></div></div></div></div><p>
  Configuring a Slack notification type requires four pieces of information
  from your Slack system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Slack server URL
   </p></li><li class="listitem "><p>
    Authentication token
   </p></li><li class="listitem "><p>
    Slack channel
   </p></li><li class="listitem "><p>
    A Slack user account. This account will be used to authenticate incoming
    notifications to Slack.
   </p></li></ul></div><p>
  <span class="bold"><strong>Identify a Slack channel</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to your Slack system as the user account that will be used to
    authenticate the notifications to Slack.
   </p></li><li class="listitem "><p>
    In the left navigation panel, under the
    <span class="bold"><strong>CHANNELS</strong></span> section locate the channel that
    you wish to receive the notifications. The instructions that follow will
    use the example channel <span class="bold"><strong>#general</strong></span>.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create a Slack token</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to your Slack system as the user account that will be used to
    authenticate the notifications to Slack
   </p></li><li class="listitem "><p>
    Navigate to the following URL:
    <a class="link" href="https://api.slack.com/docs/oauth-test-tokens" target="_blank">https://api.slack.com/docs/oauth-test-tokens</a>
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Create token</strong></span> button.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create a Slack notification type</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Begin by identifying the structure of the API call to be used by your
    notification method. The format for a call to the Slack Web API is as
    follows:
   </p><p>
    <code class="literal">https://slack.com/api/METHOD</code>
   </p><p>
    You can authenticate a Web API request by using the token that you created
    in the previous <span class="bold"><strong>Create a Slack
    Token</strong></span>section. Doing so will result in an API call that looks
    like the following.
   </p><p>
    <code class="literal">https://slack.com/api/METHOD?token=auth_token</code>
   </p><p>
    You can further refine your call by specifying the channel that the message
    will be posted to. Doing so will result in an API call that looks like the
    following.
   </p><p>
    <code class="literal">https://slack.com/api/<em class="replaceable ">METHOD</em>?token=<em class="replaceable ">AUTH_TOKEN</em>&amp;channel=<em class="replaceable ">#channel</em></code>
   </p><p>
    The following example uses the <code class="literal">chat.postMessage</code> method,
    the token <code class="literal">1234567890</code>, and the channel
    <code class="literal">#general</code>.
   </p><div class="verbatim-wrap"><pre class="screen">https://slack.com/api/chat.postMessage?token=1234567890&amp;channel=#general</pre></div><p>
    Find more information on the Slack Web API here:
    <a class="link" href="https://api.slack.com/web" target="_blank">https://api.slack.com/web</a>
   </p></li><li class="listitem "><p>
    Use the CLI on your Cloud Lifecycle Manager to create a new Slack notification
    type, using the API call that you created in the preceding step. The
    following example creates a notification type named
    <span class="bold"><strong>MySlackNotification</strong></span>, using token
    <span class="bold"><strong>1234567890</strong></span>, and posting to channel
    <span class="bold"><strong>#general</strong></span>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  MySlackNotification SLACK https://slack.com/api/chat.postMessage?token=1234567890&amp;channel=#general</pre></div></li></ol></div><div id="id-1.5.15.3.5.5.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   Notification types can also be created in the horizon dashboard.
  </p></div></div><div class="sect3" id="monasca-jira-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the JIRA Plugin</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#monasca-jira-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_jira_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_jira_plugin.xml</li><li><span class="ds-label">ID: </span>monasca-jira-plugin</li></ul></div></div></div></div><p>
  Configuring the JIRA plugin requires three pieces of information from your
  JIRA system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The URL of your JIRA system.
   </p></li><li class="listitem "><p>
    Username and password of a JIRA account that will be used to authenticate
    the notifications.
   </p></li><li class="listitem "><p>
    The name of the JIRA project that the notifications will be sent to.
   </p></li></ul></div><p>
  <span class="bold"><strong>Create JIRA notification type</strong></span>
 </p><p>
  You will configure the monasca service to send notifications to a particular
  JIRA project. You must also configure JIRA to create new issues for each
  notification it receives to this project, however, that configuration is
  outside the scope of this document.
 </p><p>
  The monasca JIRA notification plugin supports only the following two JIRA
  issue fields.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <em class="replaceable ">PROJECT</em>. This is the only supported
    <span class="quote">“<span class="quote ">mandatory</span>”</span> JIRA issue field.
   </p></li><li class="listitem "><p>
    <em class="replaceable ">COMPONENT</em>. This is the only supported
    <span class="quote">“<span class="quote ">optional</span>”</span> JIRA issue field.
   </p></li></ul></div><p>
  The JIRA issue type that your notifications will create may only be
  configured with the "Project" field as mandatory. If your JIRA issue type has
  any other mandatory fields, the monasca plugin will not function correctly.
  Currently, the monasca plugin only supports the single optional "component"
  field.
 </p><p>
  Creating the JIRA notification type requires a few more steps than other
  notification types covered in this guide. Because the Python and YAML files
  for this notification type are not yet included in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, you must
  perform the following steps to manually retrieve and place them on your
  Cloud Lifecycle Manager.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Configure the JIRA plugin by adding the following block to the
    <code class="filename">/etc/monasca/notification.yaml</code> file, under the
    <code class="literal">notification_types</code> section, and adding the username and
    password of the JIRA account used for the notifications to the respective
    sections.
   </p><div class="verbatim-wrap"><pre class="screen">    plugins:

     - monasca_notification.plugins.jira_notifier:JiraNotifier

    jira:
        user:

        password:

        timeout: 60</pre></div><p>
    After adding the necessary block, the <code class="literal">notification_types</code>
    section should look like the following example. Note that you must also add
    the username and password for the JIRA user related to the notification
    type.
   </p><div class="verbatim-wrap"><pre class="screen">notification_types:
    plugins:

     - monasca_notification.plugins.jira_notifier:JiraNotifier

    jira:
        user:

        password:

        timeout: 60

    webhook:
        timeout: 5

    pagerduty:
        timeout: 5

        url: "https://events.pagerduty.com/generic/2010-04-15/create_event.json"</pre></div></li><li class="listitem "><p>
    Create the JIRA notification type. The following command example creates a
    JIRA notification type named
    <code class="literal">MyJiraNotification</code>, in the JIRA project
    <code class="literal">HISO</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  MyJiraNotification JIRA https://jira.hpcloud.net/?project=HISO</pre></div><p>
    The following command example creates a JIRA notification type named
    <code class="literal">MyJiraNotification</code>, in the JIRA project
    <code class="literal">HISO</code>, and adds the optional
    <em class="replaceable ">component</em> field with a value of
    <code class="literal">keystone</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create MyJiraNotification JIRA https://jira.hpcloud.net/?project=HISO&amp;component=keystone</pre></div><div id="id-1.5.15.3.5.6.10.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     There is a slash (<code class="literal">/</code>) separating the URL path and the
     query string. The
     slash is required if you have a query parameter without a path parameter.
    </p></div><div id="id-1.5.15.3.5.6.10.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Notification types may also be created in the horizon dashboard.
    </p></div></li></ol></div></div></div><div class="sect2" id="alarm-metrics"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#alarm-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-alarm_metrics.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-alarm_metrics.xml</li><li><span class="ds-label">ID: </span>alarm-metrics</li></ul></div></div></div></div><p>
  You can use the available metrics to create custom alarms to further monitor
  your cloud infrastructure and facilitate autoscaling features.
 </p><p>
  For details on how to create customer alarms using the Operations Console,
  see <a class="xref" href="manage-ops-console.html#opsconsole-alarm-definitions" title="16.2. Alarm Definition">Section 16.2, “Alarm Definition”</a>.
 </p><div class="sect3" id="apache-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Apache Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#apache-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-apache_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-apache_metrics.xml</li><li><span class="ds-label">ID: </span>apache-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Apache service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>apache.net.hits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total accesses</td></tr><tr><td>apache.net.kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total Kbytes per second</td></tr><tr><td>apache.net.requests_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total accesses per second</td></tr><tr><td>apache.net.total_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total Kbytes</td></tr><tr><td>apache.performance.busy_worker_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>The number of workers serving requests</td></tr><tr><td>apache.performance.cpu_load_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>
      <p>
       The current percentage of CPU used by each worker and in total by all
       workers combined
      </p>
     </td></tr><tr><td>apache.performance.idle_worker_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>The number of idle workers</td></tr><tr><td>apache.status</td><td>
<div class="verbatim-wrap"><pre class="screen">apache_port
hostname
service=apache
component=apache</pre></div>
     </td><td>Status of Apache port</td></tr></tbody></table></div></div><div class="sect3" id="ceilometer-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ceilometer Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilometer-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ceilometer_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ceilometer_metrics.xml</li><li><span class="ds-label">ID: </span>ceilometer-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the ceilometer service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>disk.total_space_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total space of disk</td></tr><tr><td>disk.total_used_space_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total used space of disk</td></tr><tr><td>swiftlm.diskusage.rate_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>swiftlm.diskusage.val.avail_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>swiftlm.diskusage.val.size_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>image</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Existence of the image</td></tr><tr><td>image.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Delete operation on this image</td></tr><tr><td>image.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=B,
source=openstack</pre></div>
     </td><td>Size of the uploaded image</td></tr><tr><td>image.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Update operation on this image</td></tr><tr><td>image.upload</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Upload operation on this image</td></tr><tr><td>instance</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=instance,
source=openstack</pre></div>
     </td><td>Existence of instance</td></tr><tr><td>disk.ephemeral.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of ephemeral disk on this instance</td></tr><tr><td>disk.root.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of root disk on this instance</td></tr><tr><td>memory</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=MB,
source=openstack</pre></div>
     </td><td>Size of memory on this instance</td></tr><tr><td>ip.floating</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=ip,
source=openstack</pre></div>
     </td><td>Existence of IP</td></tr><tr><td>ip.floating.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=ip,
source=openstack</pre></div>
     </td><td>Create operation on this fip</td></tr><tr><td>ip.floating.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=ip,
source=openstack</pre></div>
     </td><td>Update operation on this fip</td></tr><tr><td>mem.total_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total space of memory</td></tr><tr><td>mem.usable_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Available space of memory</td></tr><tr><td>network</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=network,
source=openstack</pre></div>
     </td><td>Existence of network</td></tr><tr><td>network.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Create operation on this network</td></tr><tr><td>network.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Update operation on this network</td></tr><tr><td>network.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Delete operation on this network</td></tr><tr><td>port</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=port,
source=openstack</pre></div>
     </td><td>Existence of port</td></tr><tr><td>port.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Create operation on this port</td></tr><tr><td>port.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Delete operation on this port</td></tr><tr><td>port.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Update operation on this port</td></tr><tr><td>router</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=router,
source=openstack</pre></div>
     </td><td>Existence of router</td></tr><tr><td>router.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Create operation on this router</td></tr><tr><td>router.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Delete operation on this router</td></tr><tr><td>router.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Update operation on this router</td></tr><tr><td>snapshot</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Existence of the snapshot</td></tr><tr><td>snapshot.create.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Create operation on this snapshot</td></tr><tr><td>snapshot.delete.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Delete operation on this snapshot</td></tr><tr><td>snapshot.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of this snapshot</td></tr><tr><td>subnet</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=subnet,
source=openstack</pre></div>
     </td><td>Existence of the subnet</td></tr><tr><td>subnet.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Create operation on this subnet</td></tr><tr><td>subnet.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Delete operation on this subnet</td></tr><tr><td>subnet.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Update operation on this subnet</td></tr><tr><td>vcpus</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=vcpus,
source=openstack</pre></div>
     </td><td>Number of virtual CPUs allocated to the instance</td></tr><tr><td>vcpus_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id</pre></div>
     </td><td>Number of vcpus used by a project</td></tr><tr><td>volume</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=volume,
source=openstack</pre></div>
     </td><td>Existence of the volume</td></tr><tr><td>volume.create.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Create operation on this volume</td></tr><tr><td>volume.delete.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Delete operation on this volume</td></tr><tr><td>volume.resize.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Resize operation on this volume</td></tr><tr><td>volume.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of this volume</td></tr><tr><td>volume.update.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Update operation on this volume</td></tr><tr><td>storage.objects</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=object,
source=openstack</pre></div>
     </td><td>Number of objects</td></tr><tr><td>storage.objects.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=B,
source=openstack</pre></div>
     </td><td>Total size of stored objects</td></tr><tr><td>storage.objects.containers</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=container,
source=openstack</pre></div>
     </td><td>Number of containers</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-cinder-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">cinder Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-cinder-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-cinder_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-cinder_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-cinder-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the cinder service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cinderlm.cinder.backend.physical.list</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backends
      </p>
     </td><td> List of physical backends</td></tr><tr><td>cinderlm.cinder.backend.total.avail</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backendname
      </p>
     </td><td>Total available capacity metric per backend</td></tr><tr><td>cinderlm.cinder.backend.total.size</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backendname
      </p>
     </td><td>Total capacity metric per backend</td></tr><tr><td>cinderlm.cinder.cinder_services</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component
      </p>
     </td><td>Status of a cinder-volume service</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.logical_drive</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, sub_component, logical_drive, controller_slot, array
      </p>
      <p>
       The HPE Smart Storage Administrator (HPE SSA) CLI component will have to be
       installed for SSACLI status to be reported. To download and install the
       SSACLI utility to enable management of disk controllers, please refer
       to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
      </p>
     </td><td>Status of a logical drive</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.physical_drive</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, box, bay, controller_slot
      </p>
     </td><td>Status of a logical drive</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.smart_array</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, sub_component, model
      </p>
     </td><td>Status of smart array</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.smart_array.firmware</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, model
      </p>
     </td><td>Checks firmware version</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-compute-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-compute-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-compute_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-compute_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-compute-metrics-xml-1</li></ul></div></div></div></div><div id="id-1.5.15.3.6.7.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   Compute instance metrics are listed in <a class="xref" href="topic-ttn-5fg-4v.html#libvirt-metrics" title="13.1.4.11. Libvirt Metrics">Section 13.1.4.11, “Libvirt Metrics”</a>.
  </p></div><p>
  A list of metrics associated with the Compute service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>nova.heartbeat</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
cloud_name
hostname
component
control_plane
cluster</pre></div>
     </td><td>
      <p>
       Checks that all services are running heartbeats (uses nova user and to
       list services then sets up checks for each. For example, nova-scheduler,
       nova-conductor, nova-compute)
      </p>
     </td></tr><tr><td>nova.vm.cpu.total_allocated</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total CPUs allocated across all VMs</td></tr><tr><td>nova.vm.disk.total_allocated_gb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total Gbytes of disk space allocated to all VMs</td></tr><tr><td>nova.vm.mem.total_allocated_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total Mbytes of memory allocated to all VMs</td></tr></tbody></table></div></div><div class="sect3" id="crash-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Crash Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#crash-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crash_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crash_metrics.xml</li><li><span class="ds-label">ID: </span>crash-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Crash service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>crash.dump_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>Number of crash dumps found</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-directory-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Directory Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-directory-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-directory_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-directory_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-directory-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Directory service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>directory.files_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service
hostname
path</pre></div>
     </td><td>Total number of files under a specific directory path</td></tr><tr><td>directory.size_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service
hostname
path</pre></div>
     </td><td>Total size of a specific directory path</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-elasticsearch-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Elasticsearch Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-elasticsearch-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-elasticsearch_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-elasticsearch_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-elasticsearch-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Elasticsearch service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>elasticsearch.active_primary_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Indicates the number of primary shards in your cluster. This is an
       aggregate total across all indices.
      </p>
     </td></tr><tr><td>elasticsearch.active_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Aggregate total of all shards across all indices, which includes replica
       shards.
      </p>
     </td></tr><tr><td>elasticsearch.cluster_status</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Cluster health status.
      </p>
     </td></tr><tr><td>elasticsearch.initializing_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       The count of shards that are being freshly created.
      </p>
     </td></tr><tr><td>elasticsearch.number_of_data_nodes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Number of data nodes.
      </p>
     </td></tr><tr><td>elasticsearch.number_of_nodes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Number of nodes.
      </p>
     </td></tr><tr><td>elasticsearch.relocating_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Shows the number of shards that are currently moving from one node to
       another node.
      </p>
     </td></tr><tr><td>elasticsearch.unassigned_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       The number of unassigned shards from the master node.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-haproxy-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HAProxy Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-haproxy-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-haproxy_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-haproxy_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-haproxy-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the HAProxy service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>haproxy.backend.bytes.in_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.bytes.out_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.denied.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.denied.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.errors.con_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.errors.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.queue.current</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.1xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.2xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.3xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.4xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.5xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.other</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.current</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.limit</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.pct</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.warnings.redis_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.warnings.retr_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.bytes.in_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.bytes.out_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.denied.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.denied.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.errors.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.requests.rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.1xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.2xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.3xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.4xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.5xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.other</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.current</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.limit</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.pct</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.rate</td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect3" id="httpcheck-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HTTP Check Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#httpcheck-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-httpcheck_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-httpcheck_metrics.xml</li><li><span class="ds-label">ID: </span>httpcheck-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the HTTP Check service:
 </p><div class="table" id="id-1.5.15.3.6.12.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.2: </span><span class="name">HTTP Check Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.12.3">#</a></h6></div><div class="table-contents"><table class="table" summary="HTTP Check Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>http_response_time</td><td>
<div class="verbatim-wrap"><pre class="screen">url
hostname
service
component</pre></div>
     </td><td>The response time in seconds of the http endpoint call.</td></tr><tr><td>http_status</td><td>
<div class="verbatim-wrap"><pre class="screen">url
hostname
service</pre></div>
     </td><td>The status of the http endpoint call (0 = success, 1 = failure).</td></tr></tbody></table></div></div><p>
  For each component and HTTP metric name there are two separate metrics
  reported, one for the local URL and another for the virtual IP (VIP) URL:
 </p><div class="table" id="id-1.5.15.3.6.12.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.3: </span><span class="name">HTTP Metric Components </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.12.5">#</a></h6></div><div class="table-contents"><table class="table" summary="HTTP Metric Components" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>account-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=account-server
url</pre></div>
     </td><td>swift account-server http endpoint status and response time</td></tr><tr><td>barbican-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=key-manager
component=barbican-api
url</pre></div>
     </td><td>barbican-api http endpoint status and response time</td></tr><tr><td>cinder-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
component=cinder-api
url</pre></div>
     </td><td>cinder-api http endpoint status and response time</td></tr><tr><td>container-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=container-server
url</pre></div>
     </td><td>swift container-server http endpoint status and response time</td></tr><tr><td>designate-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
component=designate-api
url</pre></div>
     </td><td>designate-api http endpoint status and response time</td></tr><tr><td>glance-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=image-service
component=glance-api
url</pre></div>
     </td><td>glance-api http endpoint status and response time</td></tr><tr><td>glance-registry</td><td>
<div class="verbatim-wrap"><pre class="screen">service=image-service
component=glance-registry
url</pre></div>
     </td><td>glance-registry http endpoint status and response time</td></tr><tr><td>heat-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api
url</pre></div>
     </td><td>heat-api http endpoint status and response time</td></tr><tr><td>heat-api-cfn</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api-cfn
url</pre></div>
     </td><td>heat-api-cfn http endpoint status and response time</td></tr><tr><td>heat-api-cloudwatch</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api-cloudwatch
url</pre></div>
     </td><td>heat-api-cloudwatch http endpoint status and response time</td></tr><tr><td>ardana-ux-services</td><td>
<div class="verbatim-wrap"><pre class="screen">service=ardana-ux-services
component=ardana-ux-services
url</pre></div>
     </td><td>ardana-ux-services http endpoint status and response time</td></tr><tr><td>horizon</td><td>
<div class="verbatim-wrap"><pre class="screen">service=web-ui
component=horizon
url</pre></div>
     </td><td>horizon http endpoint status and response time</td></tr><tr><td>keystone-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
component=keystone-api
url</pre></div>
     </td><td>keystone-api http endpoint status and response time</td></tr><tr><td>monasca-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
component=monasca-api
url</pre></div>
     </td><td>monasca-api http endpoint status</td></tr><tr><td>monasca-persister</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
component=monasca-persister
url</pre></div>
     </td><td>monasca-persister http endpoint status</td></tr><tr><td>neutron-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
component=neutron-server
url</pre></div>
     </td><td>neutron-server http endpoint status and response time</td></tr><tr><td>neutron-server-vip</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
component=neutron-server-vip
url</pre></div>
     </td><td>neutron-server-vip http endpoint status and response time</td></tr><tr><td>nova-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
component=nova-api
url</pre></div>
     </td><td>nova-api http endpoint status and response time</td></tr><tr><td>nova-vnc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
component=nova-vnc
url</pre></div>
     </td><td>nova-vnc http endpoint status and response time</td></tr><tr><td>object-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=object-server
url</pre></div>
     </td><td>object-server http endpoint status and response time</td></tr><tr><td>object-storage-vip</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=object-storage-vip
url</pre></div>
     </td><td>object-storage-vip http endpoint status and response time</td></tr><tr><td>octavia-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
component=octavia-api
url</pre></div>
     </td><td>octavia-api http endpoint status and response time</td></tr><tr><td>ops-console-web</td><td>
<div class="verbatim-wrap"><pre class="screen">service=ops-console
component=ops-console-web
url</pre></div>
     </td><td>ops-console-web http endpoint status and response time</td></tr><tr><td>proxy-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=proxy-server
url</pre></div>
     </td><td>proxy-server http endpoint status and response time</td></tr></tbody></table></div></div></div><div class="sect3" id="kafka-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Kafka Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#kafka-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-kafka_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-kafka_metrics.xml</li><li><span class="ds-label">ID: </span>kafka-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Kafka service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>kafka.consumer_lag</td><td>
<div class="verbatim-wrap"><pre class="screen">topic
service
component=kafka
consumer_group
hostname</pre></div>
     </td><td>Hostname consumer offset lag from broker offset</td></tr></tbody></table></div></div><div class="sect3" id="libvirt-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Libvirt Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#libvirt-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-libvirt_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_metrics.xml</li><li><span class="ds-label">ID: </span>libvirt-metrics</li></ul></div></div></div></div><div id="id-1.5.15.3.6.14.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For information on how to turn these metrics on and off using the tuning
   knobs, see <a class="xref" href="topic-ttn-5fg-4v.html#libvirt-tuningknobs" title="13.1.2.5.1. Libvirt plugin metric tuning knobs">Section 13.1.2.5.1, “Libvirt plugin metric tuning knobs”</a>.
  </p></div><p>
  A list of metrics associated with the Libvirt service.
 </p><div class="table" id="id-1.5.15.3.6.14.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.4: </span><span class="name">Tunable Libvirt Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.14.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Tunable Libvirt Metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>vm.cpu.time_ns</td><td>cpu.time_ns</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Cumulative CPU time (in ns)</td></tr><tr><td>vm.cpu.utilization_norm_perc</td><td>cpu.utilization_norm_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Normalized CPU utilization (percentage)</td></tr><tr><td>vm.cpu.utilization_perc</td><td>cpu.utilization_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Overall CPU utilization (percentage)</td></tr><tr><td>vm.io.errors</td><td>io.errors</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Overall disk I/O errors</td></tr><tr><td>vm.io.errors_sec</td><td>io.errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O errors per second</td></tr><tr><td>vm.io.read_bytes</td><td>io.read_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read bytes value</td></tr><tr><td>vm.io.read_bytes_sec</td><td>io.read_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read bytes per second</td></tr><tr><td>vm.io.read_ops</td><td>io.read_ops</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read operations value</td></tr><tr><td>vm.io.read_ops_sec</td><td>io.read_ops_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations per second</td></tr><tr><td>vm.io.write_bytes</td><td>io.write_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write bytes value</td></tr><tr><td>vm.io.write_bytes_sec</td><td>io.write_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write bytes per second</td></tr><tr><td>vm.io.write_ops</td><td>io.write_ops</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations value</td></tr><tr><td>vm.io.write_ops_sec</td><td> io.write_ops_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations per second</td></tr><tr><td>vm.net.in_bytes</td><td>net.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received total bytes</td></tr><tr><td>vm.net.in_bytes_sec</td><td>net.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received bytes per second</td></tr><tr><td>vm.net.in_packets</td><td>net.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received total packets</td></tr><tr><td>vm.net.in_packets_sec</td><td>net.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received packets per second</td></tr><tr><td>vm.net.out_bytes</td><td>net.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted total bytes</td></tr><tr><td>vm.net.out_bytes_sec</td><td>net.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted bytes per second</td></tr><tr><td>vm.net.out_packets</td><td>net.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted total packets</td></tr><tr><td>vm.net.out_packets_sec</td><td>net.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted packets per second</td></tr><tr><td>vm.ping_status</td><td>ping_status</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>0 for ping success, 1 for ping failure</td></tr><tr><td>vm.disk.allocation</td><td>disk.allocation</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk allocation for a device</td></tr><tr><td>vm.disk.allocation_total</td><td>disk.allocation_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk allocation across devices for instances</td></tr><tr><td>vm.disk.capacity</td><td>disk.capacity</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk capacity for a device</td></tr><tr><td>vm.disk.capacity_total</td><td>disk.capacity_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk capacity across devices for instances</td></tr><tr><td>vm.disk.physical</td><td>disk.physical</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk usage for a device</td></tr><tr><td>vm.disk.physical_total</td><td>disk.physical_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk usage across devices for instances</td></tr><tr><td>vm.io.errors_total</td><td>io.errors_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O errors across all devices</td></tr><tr><td>vm.io.errors_total_sec</td><td>io.errors_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O errors per second across all devices</td></tr><tr><td>vm.io.read_bytes_total</td><td>io.read_bytes_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read bytes across all devices</td></tr><tr><td>vm.io.read_bytes_total_sec</td><td>io.read_bytes_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read bytes per second across devices</td></tr><tr><td>vm.io.read_ops_total</td><td>io.read_ops_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read operations across all devices</td></tr><tr><td>vm.io.read_ops_total_sec</td><td>io.read_ops_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read operations across all devices per sec</td></tr><tr><td>vm.io.write_bytes_total</td><td>io.write_bytes_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write bytes across all devices</td></tr><tr><td>vm.io.write_bytes_total_sec</td><td>io.write_bytes_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O Write bytes per second across devices</td></tr><tr><td>vm.io.write_ops_total</td><td>io.write_ops_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write operations across all devices</td></tr><tr><td>vm.io.write_ops_total_sec</td><td>io.write_ops_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write operations across all devices per sec</td></tr></tbody></table></div></div><p>
  These metrics in libvirt are always enabled and cannot be disabled using the
  tuning knobs.
 </p><div class="table" id="id-1.5.15.3.6.14.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.5: </span><span class="name">Untunable Libvirt Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.14.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Untunable Libvirt Metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>vm.host_alive_status</td><td>host_alive_status</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>
      <p>
       -1 for no status, 0 for Running / OK, 1 for Idle / blocked, 2 for
       Paused,
      </p>
      <p>
       3 for Shutting down, 4 for Shut off or nova suspend 5 for Crashed,
      </p>
      <p>
       6 for Power management suspend (S3 state)
      </p>
     </td></tr><tr><td>vm.mem.free_mb</td><td>mem.free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Free memory in Mbytes</td></tr><tr><td>vm.mem.free_perc</td><td>mem.free_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Percent of memory free</td></tr><tr><td>vm.mem.resident_mb</td><td> </td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Total memory used on host, an Operations-only metric</td></tr><tr><td>vm.mem.swap_used_mb</td><td>mem.swap_used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Used swap space in Mbytes</td></tr><tr><td>vm.mem.total_mb</td><td>mem.total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Total memory in Mbytes</td></tr><tr><td>vm.mem.used_mb</td><td>mem.used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Used memory in Mbytes</td></tr></tbody></table></div></div></div><div class="sect3" id="idg-all-operations-monitoring-monasca-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-monasca-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monasca-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Monitoring service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>alarm-state-transitions-added-to-batch-counter</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-persister</pre></div>
     </td><td> </td></tr><tr><td>jvm.memory.total.max</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Maximum JVM overall memory</td></tr><tr><td>jvm.memory.total.used</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Used JVM overall memory</td></tr><tr><td>metrics-added-to-batch-counter</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-persister</pre></div>
     </td><td> </td></tr><tr><td>metrics.published</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-api</pre></div>
     </td><td>Total number of published metrics</td></tr><tr><td>monasca.alarms_finished_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Total number of alarms received</td></tr><tr><td>monasca.checks_running_too_long</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-agent
service=monitoring
cluster</pre></div>
     </td><td>Only emitted when collection time for a check is too long</td></tr><tr><td>monasca.collection_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-agent
service=monitoring
cluster</pre></div>
     </td><td>Collection time in monasca-agent</td></tr><tr><td>monasca.config_db_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.created_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Number of notifications created</td></tr><tr><td>monasca.invalid_type_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Number of notifications with invalid type</td></tr><tr><td>monasca.log.in_bulks_rejected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs_rejected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs_lost</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs_truncated_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.processing_time_ms</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.publish_time_ms</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.thread_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name
hostname
component</pre></div>
     </td><td>Number of threads monasca is using</td></tr><tr><td>raw-sql.time.avg</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Average raw sql query time</td></tr><tr><td>raw-sql.time.max</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Max raw sql query time</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-monasca-agg-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Aggregated Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-monasca-agg-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_agg_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_agg_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monasca-agg-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of the aggregated metrics associated with the monasca Transform
  feature.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="newCol2" /><col class="newCol3" /><col class="c3" /><col class="c2" /></colgroup><thead><tr><th>Metric Name</th><th>For</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cpu.utilized_logical_cores_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Utilized physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour).
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>cpu.total_logical_cores_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Total physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>mem.total_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Total physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>mem.usable_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>Usable physical host memory capacity by time interval (defaults to a
                hour)</td></tr><tr><td>disk.total_used_space_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Utilized physical host disk capacity by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>disk.total_space_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>Total physical host disk capacity by time interval (defaults to a hour)</td></tr><tr><td>nova.vm.cpu.total_allocated_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       CPUs allocated across all virtual machines by time interval (defaults to
       a hour)
      </p>
     </td></tr><tr><td>vcpus_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Virtual CPUs allocated capacity for virtual machines of one or all
       projects by time interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>nova.vm.mem.total_allocated_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Memory allocated to all virtual machines by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>vm.mem.used_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Memory utilized by virtual machines of one or all projects by time
       interval (defaults to an hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>vm.mem.total_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Memory allocated to virtual machines of one or all projects by time
       interval (defaults to an hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>vm.cpu.utilization_perc_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       CPU utilized by all virtual machines by project by time interval
       (defaults to an hour)
      </p>
     </td></tr><tr><td>nova.vm.disk.total_allocated_gb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Disk space allocated to all virtual machines by time interval (defaults
       to an hour)
      </p>
     </td></tr><tr><td>vm.disk.allocation_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Disk allocation for virtual machines of one or all projects by time
       interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.val.size_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Total available object storage capacity by time interval (defaults to a
       hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.val.avail_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Remaining object storage capacity by time interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.rate_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Rate of change of object storage usage by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>storage.objects.size_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Used object storage capacity by time interval (defaults to a hour)
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="mysql-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MySQL Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#mysql-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-mysql_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-mysql_metrics.xml</li><li><span class="ds-label">ID: </span>mysql-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the MySQL service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>mysql.innodb.buffer_pool_free</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The number of free pages, in bytes. This value is calculated by
       multiplying <code class="literal">Innodb_buffer_pool_pages_free</code> and
       <code class="literal">Innodb_page_size</code> of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.buffer_pool_total</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The total size of buffer pool, in bytes. This value is calculated by
       multiplying <code class="literal">Innodb_buffer_pool_pages_total</code> and
       <code class="literal">Innodb_page_size</code> of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.buffer_pool_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The number of used pages, in bytes. This value is calculated by
       subtracting <code class="literal">Innodb_buffer_pool_pages_total</code> away from
       <code class="literal">Innodb_buffer_pool_pages_free</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.innodb.current_row_locks</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to current row locks of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.data_reads</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_data_reads</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.data_writes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_data_writes</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_os_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to the OS waits of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_spin_rounds</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to spinlock rounds of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_spin_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to the spin waits of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.os_log_fsyncs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_os_log_fsyncs</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.row_lock_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_row_lock_time</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.row_lock_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_row_lock_waits</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.net.connections</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Connections</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.net.max_connections</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Max_used_connections</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_delete</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_delete</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_delete_multi</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_delete_multi</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_insert</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_insert</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_insert_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_insert_select</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_replace_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_replace_select</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_select</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_update</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_update</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_update_multi</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_update_multi</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_disk_tables</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_disk_tables</code> of the
       server status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_files</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_files</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_tables</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_tables</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.kernel_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The kernel time for the databases performance, in seconds.
      </p>
     </td></tr><tr><td>mysql.performance.open_files</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Open_files</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.qcache_hits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Qcache_hits</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.queries</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Queries</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.questions</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Question</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.slow_queries</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Slow_queries</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.table_locks_waited</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Table_locks_waited</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.threads_connected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Threads_connected</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.user_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The CPU user time for the databases performance, in seconds.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-ntp-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NTP Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-ntp-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ntp_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ntp_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-ntp-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the NTP service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ntp.connection_status</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
ntp_server</pre></div>
     </td><td>Value of ntp server connection status (0=Healthy)</td></tr><tr><td>ntp.offset</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
ntp_server</pre></div>
     </td><td>Time offset in seconds</td></tr></tbody></table></div></div><div class="sect3" id="sec-metric-ovs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Open vSwitch (OVS) Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#sec-metric-ovs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ovs_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_metrics.xml</li><li><span class="ds-label">ID: </span>sec-metric-ovs</li></ul></div></div></div></div><p>
  A list of metrics associated with the OVS service.
 </p><div id="id-1.5.15.3.6.19.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For information on how to turn these metrics on and off using the tuning
   knobs, see <a class="xref" href="topic-ttn-5fg-4v.html#ovs-tuningknobs" title="13.1.2.5.2. OVS plugin metric tuning knobs">Section 13.1.2.5.2, “OVS plugin metric tuning knobs”</a>.
  </p></div><div class="table" id="id-1.5.15.3.6.19.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.6: </span><span class="name">Per-router metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.19.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Per-router metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ovs.vrouter.in_bytes_sec</td><td>vrouter.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes per second for the router (if
       <code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vrouter.in_packets_sec</td><td>vrouter.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_bytes_sec</td><td>vrouter.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes per second for the router (if
       <code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vrouter.out_packets_sec</td><td>vrouter.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_bytes</td><td>vrouter.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes for the router (if <code class="literal">network_use_bits</code> is
       false)
      </p>
     </td></tr><tr><td>ovs.vrouter.in_packets</td><td>vrouter.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_bytes</td><td>vrouter.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes for the router (if <code class="literal">network_use_bits</code> is
       false)
      </p>
     </td></tr><tr><td>ovs.vrouter.out_packets</td><td>vrouter.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_dropped_sec</td><td>vrouter.in_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_errors_sec</td><td>vrouter.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of incoming errors per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_dropped_sec</td><td>vrouter.out_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_errors_sec</td><td>vrouter.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of outgoing errors per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_dropped</td><td>vrouter.in_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_errors</td><td>vrouter.in_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of incoming errors for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_dropped</td><td>vrouter.out_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_errors</td><td>vrouter.out_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of outgoing errors for the router
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.19.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.7: </span><span class="name">Per-DHCP port and rate metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.19.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Per-DHCP port and rate metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Tenant Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ovs.vswitch.in_bytes_sec</td><td>vswitch.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming Bytes per second on DHCP
       port(if<code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.in_packets_sec</td><td>vswitch.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_bytes_sec</td><td>vswitch.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing Bytes per second on DHCP
       port(if<code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.out_packets_sec</td><td>vswitch.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_bytes</td><td>vswitch.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes for the DHCP port (if <code class="literal">network_use_bits</code>
       is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.in_packets</td><td>vswitch.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_bytes</td><td>vswitch.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes for the DHCP port (if <code class="literal">network_use_bits</code>
       is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.out_packets</td><td>vswitch.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_dropped_sec</td><td>vswitch.in_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_errors_sec</td><td>vswitch.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming errors per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_dropped_sec</td><td>vswitch.out_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_errors_sec</td><td>vswitch.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing errors per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_dropped</td><td>vswitch.in_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_errors</td><td>vswitch.in_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Errors received for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_dropped</td><td>vswitch.out_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_errors</td><td>vswitch.out_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Errors transmitted for the DHCP port
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect3" id="process-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Process Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#process-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span>process-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with processes.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>process.cpu_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Percentage of cpu being consumed by a process</td></tr><tr><td>process.io.read_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of reads by a process</td></tr><tr><td>process.io.read_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Kbytes read by a process</td></tr><tr><td>process.io.write_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of writes by a process</td></tr><tr><td>process.io.write_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Kbytes written by a process</td></tr><tr><td>process.mem.rss_mbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Amount of physical memory allocated to a process, including memory from shared
                libraries in Mbytes</td></tr><tr><td>process.open_file_descriptors</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of files being used by a process</td></tr><tr><td>process.pid_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of processes that exist with this process name</td></tr><tr><td>process.thread_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of threads a process is using</td></tr></tbody></table></div><div class="sect4" id="id-1.5.15.3.6.20.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">process.cpu_perc, process.mem.rss_mbytes, process.pid_count and process.thread_count metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>apache-storm</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-thresh
process_user=storm</pre></div>
      </td><td>apache-storm process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>barbican-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=key-manager
process_name=barbican-api</pre></div>
      </td><td>barbican-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>ceilometer-agent-notification</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
process_name=ceilometer-agent-notification</pre></div>
      </td><td>ceilometer-agent-notification process info: cpu percent, momory, pid count
                  and thread count</td></tr><tr><td>ceilometer-polling</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
process_name=ceilometer-polling</pre></div>
      </td><td>ceilometer-polling process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cinder-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
process_name=cinder-api</pre></div>
      </td><td>cinder-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cinder-scheduler</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
process_name=cinder-scheduler</pre></div>
      </td><td>cinder-scheduler process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-api</pre></div>
      </td><td>designate-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-central</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-central</pre></div>
      </td><td>designate-central process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-mdns</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-mdns</pre></div>
      </td><td>designate-mdns process cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-pool-manager</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-pool-manager</pre></div>
      </td><td>designate-pool-manager process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>heat-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api</pre></div>
      </td><td>heat-api process cpu percent, momory, pid count and thread count</td></tr><tr><td>heat-api-cfn</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api-cfn</pre></div>
      </td><td>heat-api-cfn process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>heat-api-cloudwatch</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api-cloudwatch</pre></div>
      </td><td>heat-api-cloudwatch process cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>heat-engine</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-engine</pre></div>
      </td><td>heat-engine process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>ipsec/charon</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=ipsec/charon</pre></div>
      </td><td>ipsec/charon process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>keystone-admin</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
process_name=keystone-admin</pre></div>
      </td><td>keystone-admin process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>keystone-main</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
process_name=keystone-main</pre></div>
      </td><td>keystone-main process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-agent</pre></div>
      </td><td>monasca-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-api</pre></div>
      </td><td>monasca-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-notification</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-notification</pre></div>
      </td><td>monasca-notification process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-persister</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-persister</pre></div>
      </td><td>monasca-persister process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-transform</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=monasca-transform</pre></div>
      </td><td>monasca-transform process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-dhcp-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-dhcp-agent</pre></div>
      </td><td>neutron-dhcp-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-l3-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-l3-agent</pre></div>
      </td><td>neutron-l3-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-metadata-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-metadata-agent</pre></div>
      </td><td>neutron-metadata-agent process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>neutron-openvswitch-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-openvswitch-agent</pre></div>
      </td><td>neutron-openvswitch-agent process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>neutron-rootwrap</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-rootwrap</pre></div>
      </td><td>neutron-rootwrap process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-server</pre></div>
      </td><td>neutron-server process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-vpn-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-vpn-agent</pre></div>
      </td><td>neutron-vpn-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-api</pre></div>
      </td><td>nova-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-compute</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-compute</pre></div>
      </td><td>nova-compute process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-conductor</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-conductor</pre></div>
      </td><td>nova-conductor process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-novncproxy</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-novncproxy</pre></div>
      </td><td>nova-novncproxy process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-scheduler</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-scheduler</pre></div>
      </td><td>nova-scheduler process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-api</pre></div>
      </td><td>octavia-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-health-manager</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-health-manager</pre></div>
      </td><td>octavia-health-manager process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>octavia-housekeeping</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-housekeeping</pre></div>
      </td><td>octavia-housekeeping process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-worker</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-worker</pre></div>
      </td><td>octavia-worker process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>org.apache.spark.deploy.master.Master</td><td>
<div class="verbatim-wrap"><pre class="screen">service=spark
process_name=org.apache.spark.deploy.master.Master</pre></div>
      </td><td>org.apache.spark.deploy.master.Master process info: cpu percent, momory, pid
                  count and thread count</td></tr><tr><td>org.apache.spark.executor.CoarseGrainedExecutorBackend</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=org.apache.spark.executor.CoarseGrainedExecutorBackend</pre></div>
      </td><td>org.apache.spark.executor.CoarseGrainedExecutorBackend process info: cpu
                  percent, momory, pid count and thread count</td></tr><tr><td> pyspark</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=pyspark</pre></div>
      </td><td>pyspark process info: cpu percent, momory, pid count and thread count</td></tr><tr><td>transform/lib/driver</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=transform/lib/driver</pre></div>
      </td><td>transform/lib/driver process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cassandra</td><td>
<div class="verbatim-wrap"><pre class="screen">service=cassandra
process_name=cassandra</pre></div>
      </td><td>cassandra process info: cpu percent, momory, pid count and thread count</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.15.3.6.20.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.17.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">process.io.*, process.open_file_descriptors metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>monasca-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-agent
process_user=mon-agent</pre></div>
      </td><td>monasca-agent process info: number of reads, number of writes,number of files
                  being used</td></tr></tbody></table></div></div></div><div class="sect3" id="rabbitmq-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RabbitMQ Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#rabbitmq-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-rabbitmq_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-rabbitmq_metrics.xml</li><li><span class="ds-label">ID: </span>rabbitmq-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the RabbitMQ service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>rabbitmq.exchange.messages.published_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "publish_out" field of "message_stats" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.published_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_out_details" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.received_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "publish_in" field of "message_stats" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.received_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_in_details" object
      </p>
     </td></tr><tr><td>rabbitmq.node.fd_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "fd_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.mem_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "mem_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.run_queue</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "run_queue" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.sockets_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "sockets_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Sum of ready and unacknowledged messages (queue depth)
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.deliver_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/deliver_details" object
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.publish_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_details" object
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.redeliver_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/redeliver_details" object
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="swift-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#swift-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-swift_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-swift_metrics.xml</li><li><span class="ds-label">ID: </span>swift-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the swift service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>swiftlm.access.host.operation.get.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes read from objects in GET requests
       processed by this host during the last minute. Only successful GET
       requests to objects are counted. GET requests to the account or
       container is not included.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.ops</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is a count of the all the API requests made to swift that
       were processed by this host during the last minute.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.project.get.bytes</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.project.ops</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.project.put.bytes</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.put.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes written to objects in PUT or POST
       requests processed by this host during the last minute. Only successful
       requests to objects are counted. Requests to the account or container is
       not included.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.status</td><td> </td><td> </td></tr><tr><td>swiftlm.access.project.operation.status</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports whether the swiftlm-access-log-tailer program is
       running normally.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.ops</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is a count of the all the API requests made to swift that
       were processed by this host during the last minute to a given project
       id.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.get.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes read from objects in GET requests
       processed by this host for a given project during the last minute. Only
       successful GET requests to objects are counted. GET requests to the
       account or container is not included.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.put.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes written to objects in PUT or POST
       requests processed by this host for a given project during the last
       minute. Only successful requests to objects are counted. Requests to the
       account or container is not included.
      </p>
     </td></tr><tr><td>swiftlm.async_pending.cp.total.queue_length</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the total length of all async pending queues in the
       system.
      </p>
      <p>
       When a container update fails, the update is placed on the async pending
       queue. An update may fail becuase the container server is too busy or
       because the server is down or failed. Later the system will “replay”
       updates from the queue – so eventually, the container listings will
       show all objects known to the system.
      </p>
      <p>
       If you know that container servers are down, it is normal to see the
       value of async pending increase. Once the server is restored, the value
       should return to zero.
      </p>
      <p>
       A non-zero value may also indicate that containers are too large. Look
       for “lock timeout” messages in /var/log/swift/swift.log. If you find
       such messages consider reducing the container size or enable rate
       limiting.
      </p>
     </td></tr><tr><td>swiftlm.check.failure</td><td>
<div class="verbatim-wrap"><pre class="screen">check
error
component
service=object-storage</pre></div>
     </td><td>
      <p>
       The total exception string is truncated if longer than 1919 characters
       and an ellipsis is prepended in the first three characters of the
       message. If there is more than one error reported, the list of errors is
       paired to the last reported error and the operator is expected to
       resolve failures until no more are reported. Where there are no further
       reported errors, the Value Class is emitted as ‘Ok’.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.avg.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the average utilization of all drives in the system. The value is a
       percentage (example: 30.0 means 30% of the total space is used).
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.max.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the highest utilization of all drives in the system. The value is a
       percentage (example: 80.0 means at least one drive is 80% utilized). The
       value is just as important as swiftlm.diskusage.usage.avg. For example,
       if swiftlm.diskusage.usage.avg is 70% you might think that there is
       plenty of space available. However, if swiftlm.diskusage.usage.max is
       100%, this means that some objects cannot be stored on that drive. swift
       will store replicas on other drives. However, this will create extra
       overhead.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.min.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the lowest utilization of all drives in the system. The value is a
       percentage (example: 10.0 means at least one drive is 10% utilized)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.avail</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of available (unused) space of all drives in the
       system. Only drives used by swift are included.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.size</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of raw size of all drives in the system.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.used</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of used space of all drives in the system. Only
       drives used by swift are included.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.avg.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the average percent usage of all swift filesystems
       on a host.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.max.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a swift filesystem that is most
       used (full) on a host. The value is the max of the percentage used of
       all swift filesystems.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.min.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a swift filesystem that is
       least used (has free space) on a host. The value is the min of the
       percentage used of all swift filesystems.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.avail</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the number of bytes available (free) in a swift
       filesystem. The value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.size</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the size in bytes of a swift filesystem. The
       value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a swift filesystem. The value
       is a floating point number in range 0.0 to 100.0
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the number of used bytes in a swift filesystem.
       The value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.load.cp.avg.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the averaged value of the five minutes system load average of
       all nodes in the swift system.
      </p>
     </td></tr><tr><td>swiftlm.load.cp.max.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the five minute load average of the busiest host in the swift
       system.
      </p>
     </td></tr><tr><td>swiftlm.load.cp.min.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the five minute load average of the least loaded host in the
       swift system.
      </p>
     </td></tr><tr><td>swiftlm.load.host.val.five</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the 5 minute load average of a host. The value is
       derived from <code class="literal">/proc/loadavg</code>.
      </p>
     </td></tr><tr><td>swiftlm.md5sum.cp.check.ring_checksums</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       If you are in the middle of deploying new rings, it is normal for this
       to be in the failed state.
      </p>
      <p>
       However, if you are not in the middle of a deployment, you need to
       investigate the cause. Use “swift-recon –md5 -v” to identify the
       problem hosts.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.account_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the account replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.container_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the container replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.object_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the object replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.account_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the account replicator last
       completed a scan on the host that has the oldest completion time.
       Normally the replicators runs periodically and hence this value will
       decrease whenever a replicator completes. However, if a replicator is
       not completing a cycle, this value increases (by one second for each
       second that the replicator is not completing). If the value remains high
       and increasing for a long period of time, it indicates that one of the
       hosts is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.container_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the container replicator last
       completed a scan on the host that has the oldest completion time.
       Normally the replicators runs periodically and hence this value will
       decrease whenever a replicator completes. However, if a replicator is
       not completing a cycle, this value increases (by one second for each
       second that the replicator is not completing). If the value remains high
       and increasing for a long period of time, it indicates that one of the
       hosts is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.object_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the object replicator last completed
       a scan on the host that has the oldest completion time. Normally the
       replicators runs periodically and hence this value will decrease
       whenever a replicator completes. However, if a replicator is not
       completing a cycle, this value increases (by one second for each second
       that the replicator is not completing). If the value remains high and
       increasing for a long period of time, it indicates that one of the hosts
       is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.swift.drive_audit</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount_point
kernel_device</pre></div>
     </td><td>
      <p>
       If an unrecoverable read error (URE) occurs on a filesystem, the error
       is logged in the kernel log. The swift-drive-audit program scans the
       kernel log looking for patterns indicating possible UREs.
      </p>
      <p>
       To get more information, log onto the node in question and run:
      </p>
<div class="verbatim-wrap"><pre class="screen">sudoswift-drive-audit/etc/swift/drive-audit.conf</pre></div>
      <p>
       UREs are common on large disk drives. They do not necessarily indicate
       that the drive is failed. You can use the xfs_repair command to attempt
       to repair the filesystem. Failing this, you may need to wipe the
       filesystem.
      </p>
      <p>
       If UREs occur very often on a specific drive, this may indicate that the
       drive is about to fail and should be replaced.
      </p>
     </td></tr><tr><td>swiftlm.swift.file_ownership.config</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service</pre></div>
     </td><td>
      <p>
       This metric reports if a directory or file has the appropriate owner.
       The check looks at swift configuration directories and files. It also
       looks at the top-level directories of mounted file systems (for example,
       /srv/node/disk0 and /srv/node/disk0/objects).
      </p>
     </td></tr><tr><td>swiftlm.swift.file_ownership.data</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service</pre></div>
     </td><td>
      <p>
       This metric reports if a directory or file has the appropriate owner.
       The check looks at swift configuration directories and files. It also
       looks at the top-level directories of mounted file systems (for example,
       /srv/node/disk0 and /srv/node/disk0/objects).
      </p>
     </td></tr><tr><td>swiftlm.swiftlm_check</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This indicates of the swiftlm <code class="literal">monasca-agent</code> Plug-in is running normally.
       If the status is failed, it probable that some or all metrics are no
       longer being reported.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.account.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.container.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.object.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.swift_services</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports of the process as named in the component dimension
       and the msg value_meta is running or not.
      </p>
      <p>
       Use the <code class="literal">swift-start.yml</code> playbook to attempt to
       restart the stopped process (it will start any process that has stopped
       – you do not need to specifically name the process).
      </p>
     </td></tr><tr><td>swiftlm.swift.swift_services.check_ip_port</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
component</pre></div>
     </td><td>Reports if a service is listening to the correct ip and port.</td></tr><tr><td>swiftlm.systems.check_mounts</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the mount state of each drive that should be mounted
       on this node.
      </p>
     </td></tr><tr><td>swiftlm.systems.connectivity.connect_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
url
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if a server can connect to a VIPs. Currently the
       following VIPs are checked:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         The keystone VIP used to validate tokens (normally port 5000)
        </p></li></ul></div>
     </td></tr><tr><td>swiftlm.systems.connectivity.memcache_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
hostname
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if memcached on the host as specified by the
       hostname dimension is accepting connections from the host running the
       check. The following value_meta.msg are used:
      </p>
      <p>
       We successfully connected to &lt;hostname&gt; on port
       &lt;target_port&gt;
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "11211"
  },
  "metric": "swiftlm.systems.connectivity.memcache_check",
  "timestamp": 1449084058,
  "value": 0,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:11211 ok"
  }
}</pre></div>
      <p>
       We failed to connect to &lt;hostname&gt; on port &lt;target_port&gt;
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "fail_message": "[Errno 111] Connection refused",
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "11211"
  },
  "metric": "swiftlm.systems.connectivity.memcache_check",
  "timestamp": 1449084150,
  "value": 2,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:11211 [Errno 111] Connection refused"
  }
}</pre></div>
     </td></tr><tr><td>swiftlm.systems.connectivity.rsync_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
hostname
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if rsyncd on the host as specified by the hostname
       dimension is accepting connections from the host running the check. The
       following value_meta.msg are used:
      </p>
      <p>
       We successfully connected to &lt;hostname&gt; on port
       &lt;target_port&gt;:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "873"
  },
  "metric": "swiftlm.systems.connectivity.rsync_check",
  "timestamp": 1449082663,
  "value": 0,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:873 ok"
  }
}</pre></div>
      <p>
       We failed to connect to &lt;hostname&gt; on port &lt;target_port&gt;:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "fail_message": "[Errno 111] Connection refused",
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "873"
  },
  "metric": "swiftlm.systems.connectivity.rsync_check",
  "timestamp": 1449082860,
  "value": 2,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:873 [Errno 111] Connection refused"
  }
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.avg.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       Reports the average value of N-iterations of the latency values recorded
       for a component.
      </p>
     </td></tr><tr><td>swiftlm.umon.target.check.state</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the state of each component after N-iterations of
       checks. If the initial check succeeds, the checks move onto the next
       component until all components are queried, then the checks sleep for
       ‘main_loop_interval’ seconds. If a check fails, it is retried every
       second for ‘retries’ number of times per component. If the check
       fails ‘retries’ times, it is reported as a fail instance.
      </p>
      <p>
       A successful state will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.check.state",
    "timestamp": 1453111805,
    "value": 0
},</pre></div>
      <p>
       A failed state will report a “fail” value and the value_meta will
       provide the http response error.
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.check.state",
    "timestamp": 1453112841,
    "value": 2,
    "value_meta": {
        "msg": "HTTPConnectionPool(host='192.168.245.9', port=8080): Max retries exceeded with url: /v1/AUTH_76538ce683654a35983b62e333001b47 (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7fd857d7f550&gt;: Failed to establish a new connection: [Errno 110] Connection timed out',))"
    }
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.max.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the maximum response time in seconds of a REST call
       from the observer to the component REST API listening on the reported
       host
      </p>
      <p>
       A response time query will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.max.latency_sec",
    "timestamp": 1453111805,
    "value": 0.2772650718688965
}</pre></div>
      <p>
       A failed query will have a much longer time value:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.max.latency_sec",
    "timestamp": 1453112841,
    "value": 127.288015127182
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.min.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the minimum response time in seconds of a REST call
       from the observer to the component REST API listening on the reported
       host
      </p>
      <p>
       A response time query will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.min.latency_sec",
    "timestamp": 1453111805,
    "value": 0.10025882720947266
}</pre></div>
      <p>
       A failed query will have a much longer time value:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.min.latency_sec",
    "timestamp": 1453112841,
    "value": 127.25378203392029
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.val.avail_day</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the average of all the collected records in the
       swiftlm.umon.target.val.avail_minute metric data. This is a walking
       average data set of these approximately per-minute states of the swift
       Object Store. The most basic case is a whole day of successful
       per-minute records, which will average to 100% availability. If there is
       any downtime throughout the day resulting in gaps of data which are two
       minutes or longer, the per-minute availability data will be “back
       filled” with an assumption of a down state for all the per-minute
       records which did not exist during the non-reported time. Because this
       is a walking average of approximately 24 hours worth of data, any
       outtage will take 24 hours to be purged from the dataset.
      </p>
      <p>
       A 24-hour average availability report:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_day",
    "timestamp": 1453645405,
    "value": 7.894736842105263
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.val.avail_minute</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       A value of 100 indicates that swift-uptime-monitor was able to get a
       token from keystone and was able to perform operations against the swift
       API during the reported minute. A value of zero indicates that either
       keystone or swift failed to respond successfully. A metric is produced
       every minute that swift-uptime-monitor is running.
      </p>
      <p>
       An “up” minute report value will report 100 [percent]:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_minute",
    "timestamp": 1453645405,
    "value": 100.0
}</pre></div>
      <p>
       A “down” minute report value will report 0 [percent]:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_minute",
    "timestamp": 1453649139,
    "value": 0.0
}</pre></div>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.smart_array.firmware</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
model
controller_slot</pre></div>
     </td><td>
      <p>
       This metric reports the firmware version of a component of a Smart Array
       controller.
      </p>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.smart_array</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
sub_component
model
controller_slot</pre></div>
     </td><td>
      <p>
       This reports the status of various sub-components of a Smart Array
       Controller.
      </p>
      <p>
       A failure is considered to have occured if:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Controller is failed
        </p></li><li class="listitem "><p>
         Cache is not enabled or has failed
        </p></li><li class="listitem "><p>
         Battery or capacitor is not installed
        </p></li><li class="listitem "><p>
         Battery or capacitor has failed
        </p></li></ul></div>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.physical_drive</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
controller_slot
box
bay</pre></div>
     </td><td>
      <p>
       This reports the status of a disk drive attached to a Smart Array
       controller.
      </p>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.logical_drive</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
controller_slot
array
logical_drive
sub_component</pre></div>
     </td><td>
      <p>
       This reports the status of a LUN presented by a Smart Array controller.
      </p>
      <p>
       A LUN is considered failed if the LUN has failed or if the LUN cache is
       not enabled and working.
      </p>
     </td></tr></tbody></table></div><div id="id-1.5.15.3.6.22.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     HPE Smart Storage Administrator (HPE SSA) CLI component will have to be
     installed on all control nodes that are swift nodes, in order to generate
     the following swift metrics:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.smart_array
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.logical_drive
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.smart_array.firmware
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.physical_drive
      </p></li></ul></div></li><li class="listitem "><p>
     HPE-specific binaries that are not based on open source are distributed
     directly from and supported by HPE. To download and install the SSACLI
     utility, please refer to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
    </p></li><li class="listitem "><p>
     After the HPE SSA CLI component is installed on the swift nodes, the
     metrics will be generated automatically during the next agent polling
     cycle. Manual reboot of the node is not required.
    </p></li></ul></div></div></div><div class="sect3" id="system-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#system-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-system_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-system_metrics.xml</li><li><span class="ds-label">ID: </span>system-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the System.
 </p><div class="table" id="id-1.5.15.3.6.23.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.8: </span><span class="name">CPU Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.23.3">#</a></h6></div><div class="table-contents"><table class="table" summary="CPU Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cpu.frequency_mhz</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Maximum MHz value for the cpu frequency.
      </p>
      <div id="id-1.5.15.3.6.23.3.2.5.1.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This value is dynamic, and driven by CPU governor depending on current
        resource need.
       </p></div>
     </td></tr><tr><td>cpu.idle_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is idle when no I/O requests are in progress
      </p>
     </td></tr><tr><td>cpu.idle_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is idle when no I/O requests are in progress
      </p>
     </td></tr><tr><td>cpu.percent</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used in total
      </p>
     </td></tr><tr><td>cpu.stolen_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of stolen CPU time, that is, the time spent in other OS
       contexts when running in a virtualized environment
      </p>
     </td></tr><tr><td>cpu.system_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used at the system level
      </p>
     </td></tr><tr><td>cpu.system_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the system level
      </p>
     </td></tr><tr><td>cpu.time_ns</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the host level
      </p>
     </td></tr><tr><td>cpu.total_logical_cores</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Total number of logical cores available for an entire node (Includes
       hyper threading).
      </p>
      <div id="id-1.5.15.3.6.23.3.2.5.9.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: </h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>cpu.user_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used at the user level
      </p>
     </td></tr><tr><td>cpu.user_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the user level
      </p>
     </td></tr><tr><td>cpu.wait_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is idle AND there is at least one I/O request
       in progress
      </p>
     </td></tr><tr><td>cpu.wait_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is idle AND there is at least one I/O request in progress
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.9: </span><span class="name">Disk Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.23.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Disk Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>disk.inode_used_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The percentage of inodes that are used on a device
      </p>
     </td></tr><tr><td>disk.space_used_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The percentage of disk space that is being used on a device
      </p>
     </td></tr><tr><td>disk.total_space_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The total amount of disk space in Mbytes aggregated across all the disks
       on a particular node.
      </p>
      <div id="id-1.5.15.3.6.23.4.2.5.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>disk.total_used_space_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The total amount of used disk space in Mbytes aggregated across all the
       disks on a particular node.
      </p>
      <div id="id-1.5.15.3.6.23.4.2.5.4.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>io.read_kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Kbytes/sec read by an io device
      </p>
     </td></tr><tr><td>io.read_req_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Number of read requests/sec to an io device
      </p>
     </td></tr><tr><td>io.read_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Amount of read time in seconds to an io device
      </p>
     </td></tr><tr><td>io.write_kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Kbytes/sec written by an io device
      </p>
     </td></tr><tr><td>io.write_req_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Number of write requests/sec to an io device
      </p>
     </td></tr><tr><td>io.write_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Amount of write time in seconds to an io device
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.10: </span><span class="name">Load Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.23.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Load Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>load.avg_15_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a
       15 minute period
      </p>
     </td></tr><tr><td>load.avg_1_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a 1
       minute period
      </p>
     </td></tr><tr><td>load.avg_5_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a 5
       minute period
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.11: </span><span class="name">Memory Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.23.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Memory Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>mem.free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of free memory
      </p>
     </td></tr><tr><td>mem.swap_free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Percentage of free swap memory that is free
      </p>
     </td></tr><tr><td>mem.swap_free_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of free swap memory that is free
      </p>
     </td></tr><tr><td>mem.swap_total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of total physical swap memory
      </p>
     </td></tr><tr><td>mem.swap_used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of total swap memory used
      </p>
     </td></tr><tr><td>mem.total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of memory
      </p>
     </td></tr><tr><td>mem.usable_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of usable memory
      </p>
     </td></tr><tr><td>mem.usable_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Percentage of total memory that is usable
      </p>
     </td></tr><tr><td>mem.used_buffers</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Number of buffers in Mbytes being used by the kernel for block io
      </p>
     </td></tr><tr><td>mem.used_cache</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of memory used for the page cache
      </p>
     </td></tr><tr><td>mem.used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of used memory
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.7"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.12: </span><span class="name">Network Metrics </span><a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.3.6.23.7">#</a></h6></div><div class="table-contents"><table class="table" summary="Network Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>net.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network bytes received per second
      </p>
     </td></tr><tr><td>net.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network errors on incoming network traffic per second
      </p>
     </td></tr><tr><td>net.in_packets_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of inbound network packets dropped per second
      </p>
     </td></tr><tr><td>net.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network packets received per second
      </p>
     </td></tr><tr><td>net.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network bytes sent per second
      </p>
     </td></tr><tr><td>net.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network errors on outgoing network traffic per second
      </p>
     </td></tr><tr><td>net.out_packets_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of outbound network packets dropped per second
      </p>
     </td></tr><tr><td>net.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network packets sent per second
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect3" id="idg-all-operations-monitoring-zookeeper-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Zookeeper Metrics</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-monitoring-zookeeper-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-zookeeper_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-zookeeper_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-zookeeper-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Zookeeper service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>zookeeper.avg_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Average latency in second</td></tr><tr><td>zookeeper.connections_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Number of connections</td></tr><tr><td>zookeeper.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Received bytes</td></tr><tr><td>zookeeper.max_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Maximum latency in second</td></tr><tr><td>zookeeper.min_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Minimum latency in second</td></tr><tr><td>zookeeper.node_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Number of nodes</td></tr><tr><td>zookeeper.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Sent bytes</td></tr><tr><td>zookeeper.outstanding_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Outstanding bytes</td></tr><tr><td>zookeeper.zxid_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Count number</td></tr><tr><td>zookeeper.zxid_epoch</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Epoch number</td></tr></tbody></table></div></div></div></div><div class="sect1" id="centralized-logging"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Centralized Logging Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#centralized-logging">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-centralized_logging.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-centralized_logging.xml</li><li><span class="ds-label">ID: </span>centralized-logging</li></ul></div></div></div></div><p>
  You can use the Centralized Logging Service to evaluate and troubleshoot your
  distributed cloud environment from a single location.
 </p><div class="sect2" id="central-log-GS"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started with Centralized Logging Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#central-log-GS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_GS.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_GS.xml</li><li><span class="ds-label">ID: </span>central-log-GS</li></ul></div></div></div></div><p>
  A typical cloud consists of multiple servers which makes locating a specific
  log from a single server difficult. The Centralized Logging feature helps the
  administrator evaluate and troubleshoot the distributed cloud deployment from
  a single location.
 </p><p>
  The Logging API is a component in the centralized logging architecture. It
  works between log producers and log storage. In most cases it works by
  default after installation with no additional configuration. To use Logging
  API with logging-as-a-service, you must
  configure an end-point. This component adds flexibility and supportability
  for features in the future.
 </p><p>
  <span class="bold"><strong>Do I need to Configure monasca-log-api?</strong></span> If
  you are only using Cloud Lifecycle Manager , then the default
  configuration is ready to use.
 </p><div id="id-1.5.15.4.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   If you are using logging in any of the following deployments, then you will
   need to query keystone to get an end-point to use.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Logging as a Service
    </p></li><li class="listitem "><p>
     Platform as a Service
    </p></li></ul></div></div><p>
  The Logging API is protected by keystone’s role-based access control. To
  ensure that logging is allowed and monasca alarms can be triggered, the user
  must have the monasca-user role. <span class="bold"><strong>To get an end-point
  from keystone:</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log on to Cloud Lifecycle Manager (deployer node).
   </p></li><li class="listitem "><p>
    To list the Identity service catalog, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ./service.osrc
<code class="prompt user">ardana &gt; </code>openstack catalog list</pre></div></li><li class="listitem "><p>
    In the output, find Kronos. For example:
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Name</th><th>Type</th><th>Endpoints</th></tr></thead><tbody><tr><td>kronos</td><td>region0</td><td>
        <p>
         public: http://myardana.test:5607/v3.0, admin:
         http://192.168.245.5:5607/v3.0, internal:
         http://192.168.245.5:5607/v3.0
        </p>
       </td></tr></tbody></table></div></li><li class="listitem "><p>
    Use the same port number as found in the output. In the example, you would
    use port 5607.
   </p></li></ol></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the logging-ansible restart playbook has been updated to manage
  the start,stop, and restart of the Centralized Logging Service in a specific
  way. This change was made to ensure the proper stop, start, and restart of
  Elasticsearch.
 </p><div id="id-1.5.15.4.3.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   It is recommended that you only use the logging playbooks to perform the
   start, stop, and restart of the Centralized Logging Service. Manually mixing
   the start, stop, and restart operations with the logging playbooks will
   result in complex failures.
  </p></div><p>
  For more information, see <a class="xref" href="topic-ttn-5fg-4v.html#central-log-manage" title="13.2.4. Managing the Centralized Logging Feature">Section 13.2.4, “Managing the Centralized Logging Feature”</a>.
 </p><div class="sect3" id="idg-all-operations-central-log-GS-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-central-log-GS-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_GS.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_GS.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-central-log-GS-xml-4</li></ul></div></div></div></div><p>
   For more information about the centralized logging components, see the
   following sites:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="link" href="https://www.elastic.co/guide/en/logstash/current/introduction.html" target="_blank">Logstash</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/guide" target="_blank">Elasticsearch Guide</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/blog/scripting-security" target="_blank">Elasticsearch
     Scripting and Security</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://python-beaver.readthedocs.io/en/latest/" target="_blank">Beaver</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/guide/en/kibana/current/index.html" target="_blank">Kibana
     Dashboard</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://kafka.apache.org/" target="_blank">Apache Kafka</a>
    </p></li></ul></div></div></div><div class="sect2" id="central-log-concepts"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Centralized Logging Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#central-log-concepts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>central-log-concepts</li></ul></div></div></div></div><p>
  The Centralized Logging feature collects logs on a central system, rather
  than leaving the logs scattered across the network. The administrator can use
  a single Kibana interface to view log information in charts, graphs, tables,
  histograms, and other forms.
 </p><div class="sect3" id="idg-all-operations-central-log-understanding-xml-2"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What Components are Part of Centralized Logging?</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-central-log-understanding-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-central-log-understanding-xml-2</li></ul></div></div></div></div><p>
   Centralized logging consists of several components, detailed below:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Administrator's Browser: </span>
      Operations Console can be used to access logging alarms or to access Kibana's
      dashboards to review logging data.
     </p></li><li class="listitem "><p><span class="formalpara-title">Apache Website for Kibana: </span>
      A standard Apache website that proxies web/REST requests to the Kibana
      NodeJS server.
     </p></li><li class="listitem "><p><span class="formalpara-title">Beaver: </span>
      A Python daemon that collects information in log files and sends it to
      the Logging API (monasca-log API) over a secure connection.
     </p></li><li class="listitem "><p><span class="formalpara-title">Cloud Auditing Data Federation (CADF): </span>
      Defines a standard, full-event model anyone can use to fill in the
      essential data needed to certify, self-manage and self-audit
      application security in cloud environments.
     </p></li><li class="listitem "><p><span class="formalpara-title">Centralized Logging and Monitoring (CLM): </span>
      Used to evaluate and troubleshoot your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> distributed cloud
      environment from a single location.
     </p></li><li class="listitem "><p>
     <span class="bold"><strong>Curator:</strong></span> a tool provided by
     Elasticsearch to manage indices.
    </p></li><li class="listitem "><p><span class="formalpara-title">Elasticsearch: </span>
      A data store offering fast indexing and querying.
     </p></li><li class="listitem "><p><span class="formalpara-title"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>: </span>
      Provides public, private, and managed cloud solutions to get you moving
      on your cloud journey.
     </p></li><li class="listitem "><p><span class="formalpara-title">JavaScript Object Notation (JSON) log file: </span>
      A file stored in the JSON format and used to exchange data. JSON uses
      JavaScript syntax, but the JSON format is text only. Text can be read
      and used as a data format by any programming language. This format is
      used by the Beaver and Logstash components.
     </p></li><li class="listitem "><p><span class="formalpara-title">Kafka: </span>
      A messaging broker used for collection of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> centralized
      logging data across nodes. It is highly available, scalable and
      performant. Kafka stores logs in disk instead of memory and is
      therefore more tolerant to consumer down times.
     </p><div id="id-1.5.15.4.4.3.3.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Make sure not to undersize your Kafka partition or the data retention
      period may be lower than expected. If the Kafka partition capacity is
      lower than 85%, the retention period will increase to 30 minutes. Over
      time Kafka will also eject old data.
     </p></div></li><li class="listitem "><p><span class="formalpara-title">Kibana: </span>
      A client/server application with rich dashboards to visualize the data
      in Elasticsearch through a web browser. Kibana enables you to create
      charts and graphs using the log data.
     </p></li><li class="listitem "><p>
     <span class="bold"><strong>Logging API (monasca-log-api):</strong></span> <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
     API provides a standard REST interface to store logs. It uses keystone
     authentication and role-based access control support.
    </p></li><li class="listitem "><p><span class="formalpara-title">Logstash: </span>
      A log processing system for receiving, processing and outputting logs.
      Logstash retrieves logs from Kafka, processes and enriches the data,
      then stores the data in Elasticsearch.
     </p></li><li class="listitem "><p><span class="formalpara-title">MML Service Node: </span>
      Metering, Monitoring, and Logging (MML) service node. All services
      associated with metering, monitoring, and logging run on a dedicated
      three-node cluster. Three nodes are required for high availability with
      quorum.
     </p></li><li class="listitem "><p><span class="formalpara-title">Monasca: </span>
      <span class="productname">OpenStack</span> monitoring at scale infrastructure for the cloud that supports
      alarms and reporting.
     </p></li><li class="listitem "><p><span class="formalpara-title"><span class="productname">OpenStack</span> Service. </span>
      An <span class="productname">OpenStack</span> service process that requires logging services.
     </p></li><li class="listitem "><p><span class="formalpara-title">Oslo.log. </span>
      An <span class="productname">OpenStack</span> library for log handling. The library functions automate
      configuration, deployment and scaling of complete, ready-for-work
      application platforms. Some PaaS solutions, such as Cloud Foundry,
      combine operating systems, containers, and orchestrators with developer
      tools, operations utilities, metrics, and security to create a
      developer-rich solution.
     </p></li><li class="listitem "><p><span class="formalpara-title">Text log: </span>
      A type of file used in the logging process that contains human-readable
      records.
     </p></li></ul></div><p>
   These components are configured to work out-of-the-box and the admin should
   be able to view log data using the default configurations.
  </p><p>
   In addition to each of the services, Centralized Logging also processes logs
   for the following features:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     HAProxy
    </p></li><li class="listitem "><p>
     Syslog
    </p></li><li class="listitem "><p>
     keepalived
    </p></li></ul></div><p>
   The purpose of the logging service is to provide a common logging
   infrastructure with centralized user access. Since there are numerous
   services and applications running in each node of a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud, and
   there could be hundreds of nodes, all of these services and applications can
   generate enough log files to make it very difficult to search for specific
   events in log files across all of the nodes. Centralized Logging addresses
   this issue by sending log messages in real time to a central Elasticsearch,
   Logstash, and Kibana cluster. In this cluster they are indexed and organized
   for easier and visual searches. The following illustration describes the
   architecture used to collect operational logs.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-logservice_arch.png" target="_blank"><img src="images/media-logservice_arch.png" width="" /></a></div></div><div id="id-1.5.15.4.4.3.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The arrows come from the active (requesting) side to the passive
    (listening) side. The active side is always the one providing
    credentials, so the arrows may also be seen as coming from the credential
    holder to the application requiring authentication.
   </p></div></div><div class="sect3" id="log-arch-st1to2"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 1- 2</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#log-arch-st1to2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st1to2</li></ul></div></div></div></div><p>
   Services configured to generate log files record the data. Beaver listens
   for changes to the files and sends the log files to the Logging Service. The
   first step the Logging service takes is to re-format the original log file
   to a new log file with text only and to remove all network operations. In
   Step 1a, the Logging service uses the Oslo.log library to re-format the file
   to text-only. In Step 1b, the Logging service uses the Python-Logstash
   library to format the original audit log file to a JSON file.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.4.3.1"><span class="term ">Step 1a</span></dt><dd><p>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </p></dd><dt id="id-1.5.15.4.4.4.3.2"><span class="term ">Step 1b</span></dt><dd><p>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </p></dd><dt id="id-1.5.15.4.4.4.3.3"><span class="term ">Step 2a</span></dt><dd><p>
      The monascalog transport of Beaver makes a token request call to keystone
      passing in credentials. The token returned is cached to avoid multiple
      network round-trips.
     </p></dd><dt id="id-1.5.15.4.4.4.3.4"><span class="term ">Step 2b</span></dt><dd><p>
      The monascalog transport of Beaver batches multiple logs (operational or
      audit) and posts them to the monasca-log-api VIP over a secure
      connection. Failure logs are written to the local Beaver log.
     </p></dd><dt id="id-1.5.15.4.4.4.3.5"><span class="term ">Step 2c</span></dt><dd><p>
      The REST API client for monasca-log-api makes a token-request call to
      keystone passing in credentials. The token returned is cached to avoid
      multiple network round-trips.
     </p></dd><dt id="id-1.5.15.4.4.4.3.6"><span class="term ">Step 2d</span></dt><dd><p>
      The REST API client for monasca-log-api batches multiple logs
      (operational or audit) and posts them to the monasca-log-api VIP over a
      secure connection.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st3ab"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 3a- 3b</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#log-arch-st3ab">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st3ab</li></ul></div></div></div></div><p>
   The Logging API (monasca-log API) communicates with keystone to validate the
   incoming request, and then sends the logs to Kafka.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.5.3.1"><span class="term ">Step 3a</span></dt><dd><p>
      The monasca-log-api WSGI pipeline is configured to validate incoming
      request tokens with keystone. The keystone middleware used for this
      purpose is configured to use the monasca-log-api admin user, password and
      project that have the required keystone role to validate a token.
     </p></dd><dt id="id-1.5.15.4.4.5.3.2"><span class="term ">Step 3b</span></dt><dd><p>
      monasca-log-api sends log messages to Kafka using a language-agnostic TCP
      protocol.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st4to8"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 4- 8</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#log-arch-st4to8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st4to8</li></ul></div></div></div></div><p>
   Logstash pulls messages from Kafka, identifies the log type, and transforms
   the messages into either the audit log format or operational format. Then
   Logstash sends the messages to Elasticsearch, using either an audit or
   operational indices.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.6.3.1"><span class="term ">Step 4</span></dt><dd><p>
      Logstash input workers pull log messages from the Kafka-Logstash topic
      using TCP.
     </p></dd><dt id="id-1.5.15.4.4.6.3.2"><span class="term ">Step 5</span></dt><dd><p>
      This Logstash filter processes the log message in-memory in the request
      pipeline. Logstash identifies the log type from this field.
     </p></dd><dt id="id-1.5.15.4.4.6.3.3"><span class="term ">Step 6</span></dt><dd><p>
      This Logstash filter processes the log message in-memory in the request
      pipeline. If the message is of audit-log type, Logstash transforms it
      from the monasca-log-api envelope format to the original CADF format.
     </p></dd><dt id="id-1.5.15.4.4.6.3.4"><span class="term ">Step 7</span></dt><dd><p>
      This Logstash filter determines which index should receive the log
      message. There are separate indices in Elasticsearch for operational
      versus audit logs.
     </p></dd><dt id="id-1.5.15.4.4.6.3.5"><span class="term ">Step 8</span></dt><dd><p>
      Logstash output workers write the messages read from Kafka to the daily
      index in the local Elasticsearch instance.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st9to12"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 9- 12</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#log-arch-st9to12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st9to12</li></ul></div></div></div></div><p>
   When an administrator who has access to the guest network accesses the
   Kibana client and makes a request, Apache forwards the request to the Kibana
   NodeJS server. Then the server uses the Elasticsearch REST API to service
   the client requests.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.7.3.1"><span class="term ">Step 9</span></dt><dd><p>
      An administrator who has access to the guest network accesses the Kibana
      client to view and search log data. The request can originate from the
      external network in the cloud through a tenant that has a pre-defined
      access route to the guest network.
     </p></dd><dt id="id-1.5.15.4.4.7.3.2"><span class="term ">Step 10</span></dt><dd><p>
      An administrator who has access to the guest network uses a web browser
      and points to the Kibana URL. This allows the user to search logs and
      view Dashboard reports.
     </p></dd><dt id="id-1.5.15.4.4.7.3.3"><span class="term ">Step 11</span></dt><dd><p>
      The authenticated request is forwarded to the Kibana NodeJS server to
      render the required dashboard, visualization, or search page.
     </p></dd><dt id="id-1.5.15.4.4.7.3.4"><span class="term ">Step 12</span></dt><dd><p>
      The Kibana NodeJS web server uses the Elasticsearch REST API in localhost
      to service the UI requests.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st13to15"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 13- 15</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#log-arch-st13to15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st13to15</li></ul></div></div></div></div><p>
   Log data is backed-up and deleted in the final steps.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.8.3.1"><span class="term ">Step 13</span></dt><dd><p>
      A daily cron job running in the ELK node runs curator to prune old
      Elasticsearch log indices.
     </p></dd><dt id="id-1.5.15.4.4.8.3.2"><span class="term ">Step 14</span></dt><dd><p>
      The curator configuration is done at the deployer node through the
      Ansible role logging-common. Curator is scripted to then prune or clone
      old indices based on this configuration.
     </p></dd><dt id="id-1.5.15.4.4.8.3.3"><span class="term ">Step 15</span></dt><dd><p>
      The audit logs must be backed up manually. For more information about
      Backup and Recovery, see <a class="xref" href="bura-overview.html" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
     </p></dd></dl></div></div><div class="sect3" id="retaining-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Long are Log Files Retained?</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#retaining-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>retaining-logs</li></ul></div></div></div></div><p>
   The logs that are centrally stored are saved to persistent storage as
   Elasticsearch indices. These indices are stored in the partition
   <code class="literal">/var/lib/elasticsearch</code> on each of the Elasticsearch
   cluster nodes. Out of the box, logs are stored in one Elasticsearch index
   per service. As more days go by, the number of indices stored in this disk
   partition grows. Eventually the partition fills up. If they are
   <span class="bold"><strong>open</strong></span>, each of these indices takes up CPU
   and memory. If these indices are left unattended they will continue to
   consume system resources and eventually deplete them.
  </p><p>
   Elasticsearch, by itself, does not prevent this from happening.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a tool called curator that is developed by the Elasticsearch
   community to handle these situations. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installs and uses a curator
   in conjunction with several configurable settings. This curator is called by
   cron and performs the following checks:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>First Check.</strong></span> The hourly cron job checks
     to see if the currently used Elasticsearch partition size is over the
     value set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_low_watermark_percent</pre></div><p>
     If it is higher than this value, the curator deletes old indices according
     to the value set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_num_of_indices_to_keep</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>Second Check.</strong></span> Another check is made to
     verify if the partition size is below the high watermark percent. If it is
     still too high, curator will delete all indices except the current one
     that is over the size as set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_max_index_size_in_gb</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>Third Check.</strong></span> A third check verifies if
     the partition size is still too high. If it is, curator will delete all
     indices except the current one.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Final Check.</strong></span> A final check verifies if
     the partition size is still high. If it is, an error message is written to
     the log file but the current index is NOT deleted.
    </p></li></ul></div><p>
   In the case of an extreme network issue, log files can run out of disk space
   in under an hour. To avoid this <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a shell script called
   <code class="literal">logrotate_if_needed.sh</code>. The cron process runs this script
   every 5 minutes to see if the size of <code class="literal">/var/log</code> has
   exceeded the high_watermark_percent (95% of the disk, by default). If it is
   at or above this level, <code class="literal">logrotate_if_needed.sh</code> runs the
   <code class="literal">logrotate</code> script to rotate logs and to free up extra
   space. This script helps to minimize the chance of running out of disk space
   on <code class="literal">/var/log</code>.
  </p></div><div class="sect3" id="rotating-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Are Logs Rotated?</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#rotating-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>rotating-logs</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the cron process which in turn calls Logrotate to provide
   rotation, compression, and removal of log files. Each log file can be
   rotated hourly, daily, weekly, or monthly. If no rotation period is set then
   the log file will only be rotated when it grows too large.
  </p><p>
   Rotating a file means that the Logrotate process creates a copy of the log
   file with a new extension, for example, the .1 extension, then empties the
   contents of the original file. If a .1 file already exists, then that file
   is first renamed with a .2 extension. If a .2 file already exists, it is
   renamed to .3, etc., up to the maximum number of rotated files specified in
   the settings file. When Logrotate reaches the last possible file extension,
   it will delete the last file first on the next rotation. By the time the
   Logrotate process needs to delete a file, the results will have been copied
   to Elasticsearch, the central logging database.
  </p><p>
   The log rotation setting files can be found in the following directory
  </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/logging-common/vars</pre></div><p>
   These files allow you to set the following options:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.10.7.1"><span class="term ">Service</span></dt><dd><p>
      The name of the service that creates the log entries.
     </p></dd><dt id="id-1.5.15.4.4.10.7.2"><span class="term ">Rotated Log Files</span></dt><dd><p>
      List of log files to be rotated. These files are kept locally on the
      server and will continue to be rotated. If the file is also listed as
      Centrally Logged, it will also be copied to Elasticsearch.
     </p></dd><dt id="id-1.5.15.4.4.10.7.3"><span class="term ">Frequency</span></dt><dd><p>
      The timing of when the logs are rotated. Options include:hourly, daily,
      weekly, or monthly.
     </p></dd><dt id="id-1.5.15.4.4.10.7.4"><span class="term ">Max Size</span></dt><dd><p>
      The maximum file size the log can be before it is rotated out.
     </p></dd><dt id="id-1.5.15.4.4.10.7.5"><span class="term ">Rotation</span></dt><dd><p>
      The number of log files that are rotated.
     </p></dd><dt id="id-1.5.15.4.4.10.7.6"><span class="term ">Centrally Logged Files</span></dt><dd><p>
      These files will be indexed by Elasticsearch and will be available for
      searching in the Kibana user interface.
     </p></dd></dl></div><p>
   Only files that are listed in the <span class="bold"><strong>Centrally Logged
   Files</strong></span> section are copied to Elasticsearch.
  </p><p>
   All of the variables for the Logrotate process are found in the following
   file:
  </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/logging-ansible/logging-common/defaults/main.yml</pre></div><p>
   Cron runs Logrotate hourly. Every 5 minutes another process is run called
   <span class="bold"><strong>"logrotate_if_needed"</strong></span> which uses a
   watermark value to determine if the Logrotate process needs to be run. If
   the <span class="bold"><strong>"high watermark"</strong></span> has been reached, and
   the /var/log partition is more than 95% full (by default - this can be
   adjusted), then Logrotate will be run within 5 minutes.
  </p></div><div class="sect3" id="BUElasticsearch"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Are Log Files Backed-Up To Elasticsearch?</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#BUElasticsearch">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>BUElasticsearch</li></ul></div></div></div></div><p>
   While centralized logging is enabled out of the box, the backup of these
   logs is not. The reason is because Centralized Logging relies on the
   Elasticsearch FileSystem Repository plugin, which in turn requires shared
   disk partitions to be configured and accessible from each of the
   Elasticsearch nodes. Since there are multiple ways to setup a shared disk
   partition, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> allows you to choose an approach that works best for
   your deployment before enabling the back-up of log files to Elasticsearch.
  </p><p>
   If you enable automatic back-up of centralized log files, then all the logs
   collected from the cloud nodes will be backed-up to Elasticsearch. Every
   hour, in the management controller nodes where Elasticsearch is setup, a
   cron job runs to check if Elasticsearch is running low on disk space. If the
   check succeeds, it further checks if the backup feature is enabled. If
   enabled, the cron job saves a snapshot of the Elasticsearch indices to the
   configured shared disk partition using curator. Next, the script starts
   deleting the oldest index and moves down from there checking each time if
   there is enough space for Elasticsearch. A check is also made to ensure that
   the backup runs only once a day.
  </p><p>
   For steps on how to enable automatic back-up, see
   <a class="xref" href="topic-ttn-5fg-4v.html#central-log-configure-settings" title="13.2.5. Configuring Centralized Logging">Section 13.2.5, “Configuring Centralized Logging”</a>.
  </p></div></div><div class="sect2" id="central-log-access-data"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing Log Data</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#central-log-access-data">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>central-log-access-data</li></ul></div></div></div></div><p>
  All logging data in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is managed by the Centralized Logging Service
  and can be viewed or analyzed by Kibana. Kibana is the only graphical
  interface provided with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to search or create a report from log data.
  Operations Console provides only a link to the Kibana Logging dashboard.
 </p><p>
  The following two methods allow you to access the Kibana Logging dashboard to
  search log data:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-access-OpsConsole" title="13.2.3.1. Use the Operations Console Link">Section 13.2.3.1, “Use the Operations Console Link”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-access-Kibana" title="13.2.3.2. Using Kibana to Access Log Data">Section 13.2.3.2, “Using Kibana to Access Log Data”</a>
   </p></li></ul></div><p>
  To learn more about Kibana, read the
  <a class="link" href="https://www.elastic.co/guide/en/kibana/current/getting-started.html" target="_blank">Getting
  Started with Kibana</a> guide.
 </p><div class="sect3" id="CL-access-OpsConsole"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use the Operations Console Link</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-access-OpsConsole">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>CL-access-OpsConsole</li></ul></div></div></div></div><p>
   Operations Console allows you to access Kibana in the same tool that you use
   to manage the other <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> resources in your deployment. To use Operations Console,
   you must have the correct permissions.
  </p><p>
   To use Operations Console:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In a browser, open the Operations Console.
    </p></li><li class="listitem "><p>
     On the login page, enter the user name, and the
     <span class="bold"><strong>Password</strong></span>, and then click
     <span class="bold"><strong>LOG IN</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home/Central Dashboard</strong></span> page, click
     the menu represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left, select
     <span class="bold"><strong>Home</strong></span>, and then select
     <span class="bold"><strong>Logging</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home/Logging</strong></span> page, click
     <span class="bold"><strong>View Logging Dashboard</strong></span>.
    </p></li></ol></div><div id="id-1.5.15.4.5.6.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, Kibana usually runs on a different network than Operations Console.
    Due to this configuration, it is possible that using Operations Console
    to access Kibana will result in an “404 not found” error. This
    error only occurs if the user has access only to the public facing network.
   </p></div></div><div class="sect3" id="CL-access-Kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Kibana to Access Log Data</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-access-Kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>CL-access-Kibana</li></ul></div></div></div></div><p>
   Kibana is an open-source, data-visualization plugin for Elasticsearch.
   Kibana provides visualization capabilities using the log content indexed on
   an Elasticsearch cluster. Users can create bar and pie charts, line and
   scatter plots, and maps using the data collected by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in the cloud
   log files.
  </p><p>
   While creating Kibana dashboards is beyond the scope of this document, it is
   important to know that the dashboards you create are JSON files that you can
   modify or create new dashboards based on existing dashboards.
  </p><div id="id-1.5.15.4.5.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Kibana is client-server software. To operate properly, the browser must be
    able to access port 5601 on the control plane.
   </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Field</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>user</td><td>kibana</td><td>
       <p>
        Username that will be required for logging into the Kibana UI.
       </p>
      </td></tr><tr><td>password</td><td>random password is generated</td><td>
       <p>
        Password generated during installation that is used to login to the
        Kibana UI.
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="Login-creds-Kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging into Kibana</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Login-creds-Kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>Login-creds-Kibana</li></ul></div></div></div></div><p>
   To log into Kibana to view data, you must make sure you have the required
   login configuration.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Verify login credentials: <a class="xref" href="topic-ttn-5fg-4v.html#KLogin-Creds" title="13.2.3.3.1. Verify Login Credentials">Section 13.2.3.3.1, “Verify Login Credentials”</a>
    </p></li><li class="listitem "><p>
     Find the randomized password: <a class="xref" href="topic-ttn-5fg-4v.html#KLogin-Psswd" title="13.2.3.3.2. Find the Randomized Password">Section 13.2.3.3.2, “Find the Randomized Password”</a>
    </p></li><li class="listitem "><p>
     Access Kibana using a direct link: <a class="xref" href="topic-ttn-5fg-4v.html#KLogin-DLink" title="13.2.3.3.3. Access Kibana Using a Direct Link:">Section 13.2.3.3.3, “Access Kibana Using a Direct Link:”</a>
    </p></li></ol></div><div class="sect4" id="KLogin-Creds"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verify Login Credentials</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#KLogin-Creds">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-Creds</li></ul></div></div></div></div><p>
    During the installation of Kibana, a password is automatically set and it
    is randomized. Therefore, unless an administrator has already changed it,
    you need to retrieve the default password from a file on the control plane
    node.
   </p></div><div class="sect4" id="KLogin-Psswd"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Find the Randomized Password</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#KLogin-Psswd">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-Psswd</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To find the Kibana password, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep kibana ~/scratch/ansible/next/my_cloud/stage/internal/CloudModel.yaml</pre></div></li></ol></div></div><div class="sect4" id="KLogin-DLink"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Kibana Using a Direct Link:</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#KLogin-DLink">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-DLink</li></ul></div></div></div></div><p>
    This section helps you verify the horizon virtual IP (VIP) address that you
    should use. To provide enhanced security, access to Kibana is not available on the
    External network.
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To determine which IP address to use to access Kibana, from your Cloud Lifecycle Manager, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep HZN-WEB /etc/hosts</pre></div><p>
      The output of the grep command should show you the virtual IP address for
      Kibana that you should use.
     </p><div id="id-1.5.15.4.5.8.6.3.1.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       If nothing is returned by the grep command, you can open the following
       file to look for the IP address manually:
      </p><div class="verbatim-wrap"><pre class="screen">/etc/hosts</pre></div></div><p>
      Access to Kibana will be over port 5601 of that virtual IP address.
      Example:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable ">VIP</em>:5601</pre></div></li></ol></div></div></div></div><div class="sect2" id="central-log-manage"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing the Centralized Logging Feature</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#central-log-manage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>central-log-manage</li></ul></div></div></div></div><p>
  No specific configuration tasks are required to use Centralized Logging, as
  it is enabled by default after installation. However, you can configure the
  individual components as needed for your environment.
 </p><div class="sect3" id="CL-stop-start"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Do I Stop and Start the Logging Service?</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-stop-start">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>CL-stop-start</li></ul></div></div></div></div><p>
   Although you might not need to stop and start the logging service very
   often, you may need to if, for example, one of the logging services is not
   behaving as expected or not working.
  </p><p>
   You cannot enable or disable centralized logging across all services unless
   you stop all centralized logging. Instead, it is recommended that you enable
   or disable individual log files in the &lt;service&gt;-clr.yml files and
   then reconfigure logging. You would enable centralized logging for a file
   when you want to make sure you are able to monitor those logs in Kibana.
  </p><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the logging-ansible restart playbook has been updated to manage
   the start,stop, and restart of the Centralized Logging Service in a specific
   way. This change was made to ensure the proper stop, start, and restart of
   Elasticsearch.
  </p><div id="id-1.5.15.4.6.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    It is recommended that you only use the logging playbooks to perform the
    start, stop, and restart of the Centralized Logging Service. Manually
    mixing the start, stop, and restart operations with the logging playbooks
    will result in complex failures.
   </p></div><p>
   The steps in this section only impact centralized logging. Logrotate is an
   essential feature that keeps the service log files from filling the disk and
   will not be affected.
  </p><div id="id-1.5.15.4.6.3.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    These playbooks must be run from the Cloud Lifecycle Manager.
   </p></div><p>
   <span class="bold"><strong>To stop the Logging service:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the ansible playbook, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the ansible playbook that will stop the logging service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-stop.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>To start the Logging service:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the ansible playbook, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the ansible playbook that will stop the logging service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-start.yml</pre></div></li></ol></div></div><div class="sect3" id="CL-disable"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Do I Enable or Disable Centralized Logging For a Service?</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-disable">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>CL-disable</li></ul></div></div></div></div><p>
   To enable or disable Centralized Logging for a service you need to modify
   the configuration for the service, set the
   <span class="bold"><strong>enabled</strong></span> flag to
   <span class="bold"><strong>true</strong></span> or
   <span class="bold"><strong>false</strong></span>, and then reconfigure logging.
  </p><div id="id-1.5.15.4.6.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    There are consequences if you enable too many logging files for a service.
    If there is not enough storage to support the increased logging, the
    retention period of logs in Elasticsearch is decreased. Alternatively, if
    you wanted to increase the retention period of log files or if you did not
    want those logs to show up in Kibana, you would disable centralized logging
    for a file.
   </p></div><p>
   To enable Centralized Logging for a service:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use the documentation provided with the service to ensure it is not
     configured for logging.
    </p></li><li class="listitem "><p>
     To find the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> file to edit, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>find ~/openstack/my_cloud/config/logging/vars/ -name "*<em class="replaceable ">service-name</em>*"</pre></div></li><li class="listitem "><p>
     Edit the file for the service for which you want to enable logging.
    </p></li><li class="listitem "><p>
     To enable Centralized Logging, find the following code and change the
     enabled flag to <span class="bold"><strong>true</strong></span>, to disable, change
     the enabled flag to <span class="bold"><strong>false</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">logging_options:
 - centralized_logging:
        enabled: true
        format: json</pre></div></li><li class="listitem "><p>
     Save the changes to the file.
    </p></li><li class="listitem "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     To reconfigure logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div></div><div class="sect2" id="central-log-configure-settings"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Centralized Logging</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#central-log-configure-settings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>central-log-configure-settings</li></ul></div></div></div></div><p>
  You can adjust the settings for centralized logging when you are
  troubleshooting problems with a service or to decrease log size and retention
  to save on disk space. For steps on how to configure logging settings, refer
  to the following tasks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-config-files" title="13.2.5.1. Configuration Files">Section 13.2.5.1, “Configuration Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-general-config" title="13.2.5.2. Planning Resource Requirements">Section 13.2.5.2, “Planning Resource Requirements”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-BU-Elasticsearch" title="13.2.5.3. Backing Up Elasticsearch Log Indices">Section 13.2.5.3, “Backing Up Elasticsearch Log Indices”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#restore-elastic-logs" title="13.2.5.4. Restoring Logs From an Elasticsearch Backup">Section 13.2.5.4, “Restoring Logs From an Elasticsearch Backup”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#tuning-logging-parameters" title="13.2.5.5. Tuning Logging Parameters">Section 13.2.5.5, “Tuning Logging Parameters”</a>
   </p></li></ul></div><div class="sect3" id="CL-config-files"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Files</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-config-files">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-config-files</li></ul></div></div></div></div><p>
   Centralized Logging settings are stored in the configuration files in the
   following directory on the Cloud Lifecycle Manager:
   <code class="literal">~/openstack/my_cloud/config/logging/</code>
  </p><p>
   The configuration files and their use are described below:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>File</th><th>Description</th></tr></thead><tbody><tr><td>main.yml</td><td>Main configuration file for all centralized logging components.</td></tr><tr><td>elasticsearch.yml.j2</td><td>Main configuration file for Elasticsearch.</td></tr><tr><td>elasticsearch-default.j2</td><td>Default overrides for the Elasticsearch init script.</td></tr><tr><td>kibana.yml.j2</td><td>Main configuration file for Kibana.</td></tr><tr><td>kibana-apache2.conf.j2</td><td>Apache configuration file for Kibana.</td></tr><tr><td>logstash.conf.j2</td><td>Logstash inputs/outputs configuration.</td></tr><tr><td>logstash-default.j2</td><td>Default overrides for the Logstash init script.</td></tr><tr><td>beaver.conf.j2</td><td>Main configuration file for Beaver.</td></tr><tr><td>vars</td><td>Path to logrotate configuration files.</td></tr></tbody></table></div></div><div class="sect3" id="CL-general-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planning Resource Requirements</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-general-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-general-config</li></ul></div></div></div></div><p>
   The Centralized Logging service needs to have enough resources available to
   it to perform adequately for different scale environments. The base logging
   levels are tuned during installation according to the amount of RAM
   allocated to your control plane nodes to ensure optimum performance.
  </p><p>
   These values can be viewed and changed in the
   <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code> file, but you
   will need to run a reconfigure of the Centralized Logging service if changes
   are made.
  </p><div id="id-1.5.15.4.7.5.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    The total process memory consumption for Elasticsearch will be the above
    allocated heap value (in
    <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code>) plus any Java
    Virtual Machine (JVM) overhead.
   </p></div><p>
   <span class="bold"><strong>Setting Disk Size Requirements</strong></span>
  </p><p>
   In the entry-scale models, the disk partition sizes on your controller nodes
   for the logging and Elasticsearch data are set as a percentage of your total
   disk size. You can see these in the following file on the Cloud Lifecycle Manager
   (deployer):
   <code class="literal">~/openstack/my_cloud/definition/data/&lt;controller_disk_files_used&gt;</code>
  </p><p>
   Sample file settings:
  </p><div class="verbatim-wrap"><pre class="screen"># Local Log files.
- name: log
  size: 13%
  mount: /var/log
  fstype: ext4
  mkfs-opts: -O large_file

# Data storage for centralized logging. This holds log entries from all
# servers in the cloud and hence can require a lot of disk space.
- name: elasticsearch
  size: 30%
  mount: /var/lib/elasticsearch
  fstype: ext4</pre></div><div id="id-1.5.15.4.7.5.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The disk size is set automatically based on the hardware configuration. If
    you need to adjust it, you can set it manually with the following steps.
   </p></div><p>
   To set disk sizes:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="listitem "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks.yml</pre></div></li><li class="listitem "><p>
     Make any desired changes.
    </p></li><li class="listitem "><p>
     Save the changes to the file.
    </p></li><li class="listitem "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A git
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To run the logging reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div><div class="sect3" id="CL-BU-Elasticsearch"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backing Up Elasticsearch Log Indices</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-BU-Elasticsearch">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-BU-Elasticsearch</li></ul></div></div></div></div><p>
   The log files that are centrally collected in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are stored by
   Elasticsearch on disk in the <code class="literal">/var/lib/elasticsearch</code>
   partition. However, this is distributed across each of the Elasticsearch
   cluster nodes as shards. A cron job runs periodically to see if the disk
   partition runs low on space, and, if so, it runs curator to delete the old
   log indices to make room for new logs. This deletion is permanent and the
   logs are lost forever. If you want to backup old logs, for example to comply
   with certain regulations, you can configure automatic backup of
   Elasticsearch indices.
  </p><div id="id-1.5.15.4.7.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you need to restore data that was archived prior to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 and
    used the older versions of Elasticsearch, then this data will need to be
    restored to a separate deployment of Elasticsearch.
   </p><p>
    This can be accomplished using the following steps:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Deploy a separate distinct Elasticsearch instance version matching the
      version in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li><li class="listitem "><p>
      Configure the backed-up data using NFS or some other share mechanism to
      be available to the Elasticsearch instance matching the version in
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li></ol></div></div><p>
   Before enabling automatic back-ups, make sure you understand how much disk
   space you will need, and configure the disks that will store the data. Use
   the following checklist to prepare your deployment for enabling automatic
   backups:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td>☐</td><td>
       <p>
        Add a shared disk partition to each of the Elasticsearch controller
        nodes.
       </p>
       <p>
        The default partition name used for backup is
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/lib/esbackup</pre></div>
       <p>
        You can change this by:
       </p>
       <div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
          Open the following file:
          <code class="literal">my_cloud/config/logging/main.yml</code>
         </p></li><li class="listitem "><p>
          Edit the following variable <code class="literal">curator_es_backup_partition
          </code>
         </p></li></ol></div>
      </td></tr><tr><td>☐</td><td>
       <p>
        Ensure the shared disk has enough storage to retain backups for the
        desired retention period.
       </p>
      </td></tr></tbody></table></div><p>
   To enable automatic back-up of centralized logs to Elasticsearch:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager (deployer node).
    </p></li><li class="listitem "><p>
     Open the following file in a text editor:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/main.yml</pre></div></li><li class="listitem "><p>
     Find the following variables:
    </p><div class="verbatim-wrap"><pre class="screen">curator_backup_repo_name: "es_{{host.my_dimensions.cloud_name}}"
curator_es_backup_partition: /var/lib/esbackup</pre></div></li><li class="listitem "><p>
     To enable backup, change the
     <span class="bold"><strong>curator_enable_backup</strong></span> value to
     <span class="bold"><strong>true</strong></span> in the curator section:
    </p><div class="verbatim-wrap"><pre class="screen">curator_enable_backup: true</pre></div></li><li class="listitem "><p>
     Save your changes and re-run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
# Verify the added files
<code class="prompt user">ardana &gt; </code>git status
<code class="prompt user">ardana &gt; </code>git commit -m "Enabling Elasticsearch Backup"

$ cd ~/openstack/ardana/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml
$ ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To re-configure logging:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li><li class="listitem "><p>
     To verify that the indices are backed up, check the contents of the
     partition:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls /var/lib/esbackup</pre></div></li></ol></div></div><div class="sect3" id="restore-elastic-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restoring Logs From an Elasticsearch Backup</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#restore-elastic-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>restore-elastic-logs</li></ul></div></div></div></div><p>
   To restore logs from an Elasticsearch backup, see
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-snapshots.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-snapshots.html</a>.
  </p><div id="id-1.5.15.4.7.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    We do not recommend restoring to the original <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Centralized Logging
    cluster as it may cause storage/capacity issues. We rather recommend setting
    up a separate ELK cluster of the same version and restoring the logs there.
   </p></div></div><div class="sect3" id="tuning-logging-parameters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning Logging Parameters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#tuning-logging-parameters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>tuning-logging-parameters</li></ul></div></div></div></div><p>
   When centralized logging is installed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, parameters for
   Elasticsearch heap size and logstash heap size are automatically configured
   based on the amount of RAM on the system. These values are typically the
   required values, but they may need to be adjusted if performance issues
   arise, or disk space issues are encountered. These values may also need to
   be adjusted if hardware changes are made after an installation.
  </p><p>
   These values are defined at the top of the following file
   <code class="literal">.../logging-common/defaults/main.yml</code>. An example of the
   contents of the file is below:
  </p><div class="verbatim-wrap"><pre class="screen">1. Select heap tunings based on system RAM
#-------------------------------------------------------------------------------
threshold_small_mb: 31000
threshold_medium_mb: 63000
threshold_large_mb: 127000
tuning_selector: " {% if ansible_memtotal_mb &lt; threshold_small_mb|int %}
demo
{% elif ansible_memtotal_mb &lt; threshold_medium_mb|int %}
small
{% elif ansible_memtotal_mb &lt; threshold_large_mb|int %}
medium
{% else %}
large
{%endif %}
"

logging_possible_tunings:
2. RAM &lt; 32GB
demo:
elasticsearch_heap_size: 512m
logstash_heap_size: 512m
3. RAM &lt; 64GB
small:
elasticsearch_heap_size: 8g
logstash_heap_size: 2g
4. RAM &lt; 128GB
medium:
elasticsearch_heap_size: 16g
logstash_heap_size: 4g
5. RAM &gt;= 128GB
large:
elasticsearch_heap_size: 31g
logstash_heap_size: 8g
logging_tunings: "{{ logging_possible_tunings[tuning_selector] }}"</pre></div><p>
   This specifies thresholds for what a <span class="bold"><strong>small</strong></span>,
   <span class="bold"><strong>medium</strong></span>, or
   <span class="bold"><strong>large</strong></span> system would look like, in terms of
   memory. To see what values will be used, see what RAM your system uses, and
   see where it fits in with the thresholds to see what values you will be
   installed with. To modify the values, you can either adjust the threshold
   values so that your system will change from a
   <span class="bold"><strong>small</strong></span> configuration to a
   <span class="bold"><strong>medium</strong></span> configuration, for example, or keep
   the threshold values the same, and modify the heap_size variables directly
   for the selector that your system is set for. For example, if your
   configuration is a <span class="bold"><strong>medium</strong></span> configuration,
   which sets heap_sizes to 16 GB for Elasticsearch and 4 GB for logstash, and
   you want twice as much set aside for logstash, then you could increase the
   4 GB for logstash to 8 GB.
  </p></div></div><div class="sect2" id="central-log-configure-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Settings for Other Services</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#central-log-configure-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>central-log-configure-services</li></ul></div></div></div></div><p>
  When you configure settings for the Centralized Logging Service, those
  changes impact all services that are enabled for centralized logging.
  However, if you only need to change the logging configuration for one
  specific service, you will want to modify the service's files instead of
  changing the settings for the entire Centralized Logging service. This topic
  helps you complete the following tasks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-log-level-srv" title="13.2.6.1. Setting Logging Levels for Services">Section 13.2.6.1, “Setting Logging Levels for Services”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-select-central-logging" title="13.2.6.19. Selecting Files for Centralized Logging">Section 13.2.6.19, “Selecting Files for Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-space-allocation" title="13.2.6.20. Controlling Disk Space Allocation and Retention of Log Files">Section 13.2.6.20, “Controlling Disk Space Allocation and Retention of Log Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-elasticsearch-config" title="13.2.6.21. Configuring Elasticsearch for Centralized Logging">Section 13.2.6.21, “Configuring Elasticsearch for Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="topic-ttn-5fg-4v.html#CL-safeguards" title="13.2.6.22. Safeguards for the Log Partitions Disk Capacity">Section 13.2.6.22, “Safeguards for the Log Partitions Disk Capacity”</a>
   </p></li></ul></div><div class="sect3" id="CL-log-level-srv"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Logging Levels for Services</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-log-level-srv">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-log-level-srv</li></ul></div></div></div></div><p>
   When it is necessary to increase the logging level for a specific service to
   troubleshoot an issue, or to decrease logging levels to save disk space, you
   can edit the service's config file and then reconfigure logging. All changes
   will be made to the service's files and not to the Centralized Logging
   service files.
  </p><p>
   Messages only appear in the log files if they are the same as or more severe
   than the log level you set. The DEBUG level logs everything. Most services
   default to the INFO logging level, which lists informational events, plus
   warnings, errors, and critical errors. Some services provide other logging
   options which will narrow the focus to help you debug an issue, receive a
   warning if an operation fails, or if there is a serious issue with the
   cloud.
  </p><p>
   For more information on logging levels, see the
   <a class="link" href="http://specs.openstack.org/openstack/openstack-specs/specs/log-guidelines.html" target="_blank">OpenStack
   Logging Guidelines</a> documentation.
  </p></div><div class="sect3" id="Loglvl-intro"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Logging Level for a Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-intro">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-intro</li></ul></div></div></div></div><p>
   If you want to increase or decrease the amount of details that are logged by
   a service, you can change the current logging level in the configuration
   files. Most services support, at a minimum, the DEBUG and INFO logging
   levels. For more information about what levels are supported by a service,
   check the documentation or Website for the specific service.
  </p></div><div class="sect3" id="Loglvl-barb"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Barbican</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-barb">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-barb</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>barbican</td><td>
       <p>
        barbican-api
       </p>
       <p>
        barbican-worker
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the barbican logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>vi my_cloud/config/barbican/barbican_deploy_config.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">barbican_loglevel: "{{ ardana_loglevel | default('INFO') }}"
barbican_logstash_loglevel: "{{ ardana_loglevel | default('INFO') }}"</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-cinder"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block Storage (cinder)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-cinder">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-cinder</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>cinder</td><td>
       <p>
        cinder-api
       </p>
       <p>
        cinder-scheduler
       </p>
       <p>
        cinder-backup
       </p>
       <p>
        cinder-volume
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To manage cinder logging:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi roles/_CND-CMN/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">cinder_loglevel: {{ ardana_loglevel | default('INFO') }}
cinder_logstash_loglevel: {{ ardana_loglevel | default('INFO') }}</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-ceilo"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-ceilo">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-ceilo</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>ceilometer</td><td>
       <p>
        ceilometer-collector
       </p>
       <p>
        ceilometer-agent-notification
       </p>
       <p>
        ceilometer-polling
       </p>
       <p>
        ceilometer-expirer
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the ceilometer logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi roles/_CEI-CMN/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">ceilometer_loglevel:  INFO
ceilometer_logstash_loglevel:  INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-nova"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (nova)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-nova">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-nova</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>nova</td><td> </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the nova logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The nova service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/novncproxy-logging.conf.j2
~/openstack/my_cloud/config/nova/api-logging.conf.j2
~/openstack/my_cloud/config/nova/compute-logging.conf.j2
~/openstack/my_cloud/config/nova/conductor-logging.conf.j2
~/openstack/my_cloud/config/nova/scheduler-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-designate"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designate (DNS)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-designate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-designate</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>designate</td><td>
       <p>
        designate-api
       </p>
       <p>
        designate-central
       </p>
       <p>
        designate-mdns
       </p>
       <p>
        designate-producer
       </p>
       <p>
        designate-worker
       </p>
       <p>
        designate-pool-manager
       </p>
       <p>
        designate-zone-manager
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the designate logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>vi my_cloud/config/designate/designate.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, set the value of the following line:
    </p><div class="verbatim-wrap"><pre class="screen">debug = False</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-keystone"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity (keystone)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-keystone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-keystone</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>keystone</td><td>keystone</td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
       <p>
        WARN
       </p>
       <p>
        ERROR
       </p>
      </td></tr></tbody></table></div><p>
   To change the keystone logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_loglevel: INFO
keystone_logstash_loglevel: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-glance"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image (glance)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-glance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-glance</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>glance</td><td>
       <p>
        glance-api
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the glance logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/glance/glance-api-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-ironic"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bare Metal (ironic)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-ironic</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>ironic</td><td>
       <p>
        ironic-api-logging.conf.j2
       </p>
       <p>
        ironic-conductor-logging.conf.j2
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the ironic logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi roles/ironic-common/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">ironic_api_loglevel: "{{ ardana_loglevel | default('INFO') }}"
ironic_api_logstash_loglevel: "{{ ardana_loglevel | default('INFO') }}"
ironic_conductor_loglevel: "{{ ardana_loglevel | default('INFO') }}"
ironic_conductor_logstash_loglevel: "{{ ardana_loglevel | default('INFO') }}"</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-monasca"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring (monasca)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-monasca</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>monasca</td><td>
       <p>
        monasca-persister
       </p>
       <p>
        zookeeper
       </p>
       <p>
        storm
       </p>
       <p>
        monasca-notification
       </p>
       <p>
        monasca-api
       </p>
       <p>
        kafka
       </p>
       <p>
        monasca-agent
       </p>
      </td><td>
       <p>
        WARN (default)
       </p>
       <p>
        INFO
       </p>
      </td></tr></tbody></table></div><p>
   To change the monasca logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Monitoring service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-persister/defaults/main.yml
~/openstack/ardana/ansible/roles/storm/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-api/defaults/main.yml
~/openstack/ardana/ansible/roles/kafka/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-agent/defaults/main.yml (For this file, you will need to add the variable)</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line:
    </p><div class="verbatim-wrap"><pre class="screen">monasca_log_level: WARN</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-neutron"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking (neutron)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-neutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-neutron</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>neutron</td><td>
       <p>
        neutron-server
       </p>
       <p>
        dhcp-agent
       </p>
       <p>
        l3-agent
       </p>
       <p>
        metadata-agent
       </p>
       <p>
        openvswitch-agent
       </p>
       <p>
        ovsvapp-agent
       </p>
       <p>
        sriov-agent
       </p>
       <p>
        infoblox-ipam-agent
       </p>
       <p>
        l2gateway-agent
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the neutron logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The neutron service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/neutron-common/templates/dhcp-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/infoblox-ipam-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/l2gateway-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/l3-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/metadata-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/openvswitch-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/ovsvapp-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/sriov-agent-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-swift"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage (swift)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-swift</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>swift</td><td> </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><div id="id-1.5.15.4.8.16.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Currently it is not recommended to log at any level other than INFO.
   </p></div></div><div class="sect3" id="Loglvl-octavia"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Octavia</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-octavia">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-octavia</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>octavia</td><td>
       <p>
        octavia-api
       </p>
       <p>
        octavia-worker
       </p>
       <p>
        octavia-hk
       </p>
       <p>
        octavia-hm
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Octavia logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The Octavia service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/octavia/octavia-api-logging.conf.j2
~/openstack/my_cloud/config/octavia/octavia-worker-logging.conf.j2
~/openstack/my_cloud/config/octavia/octavia-hk-logging.conf.j2
~/openstack/my_cloud/config/octavia/octavia-hm-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-opsconsole"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-opsconsole">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-opsconsole</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>opsconsole</td><td>
       <p>
        ops-web
       </p>
       <p>
        ops-mon
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Operations Console logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/OPS-WEB/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line:
    </p><div class="verbatim-wrap"><pre class="screen">ops_console_loglevel: "{{ ardana_loglevel | default('INFO') }}"</pre></div></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-heat"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Orchestration (heat)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-heat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-heat</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>heat</td><td>
       <p>
        api-cfn
       </p>
       <p>
        api
       </p>
       <p>
        engine
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the heat logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/heat/*-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-magnum"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-magnum">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-magnum</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>magnum</td><td>
       <p>
        api
       </p>
       <p>
        conductor
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Magnum logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/magnum/api-logging.conf.j2
~/openstack/my_cloud/config/magnum/conductor-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts magnum-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-manila"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">File Storage (manila)</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Loglvl-manila">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-manila</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>manila</td><td>
       <p>
        api
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the manila logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/manila/manila-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">manila_loglevel: INFO
manila_logstash_loglevel: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="CL-select-central-logging"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting Files for Centralized Logging</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-select-central-logging">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-select-central-logging</li></ul></div></div></div></div><p>
   As you use <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you might find a need to redefine which log files are
   rotated on disk or transferred to centralized logging. These changes are all
   made in the centralized logging definition files.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the logrotate service to provide rotation, compression, and
   removal of log files. All of the tunable variables for the logrotate process
   itself can be controlled in the following file:
   <code class="literal">~/openstack/ardana/ansible/roles/logging-common/defaults/main.yml</code>
  </p><p>
   You can find the centralized logging definition files for each service in
   the following directory:
   <code class="literal">~/openstack/ardana/ansible/roles/logging-common/vars</code>
  </p><p>
   You can change log settings for a service by following these steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p><p>
     Open the *.yml file for the service or sub-component that you want to
     modify.
    </p><p>
     Using keystone, the Identity service as an example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/ardana/ansible/roles/logging-common/vars/keystone-clr.yml</pre></div><p>
     Consider the opening clause of the file:
    </p><div class="verbatim-wrap"><pre class="screen">sub_service:
  hosts: KEY-API
  name: keystone
  service: keystone</pre></div><p>
     The <span class="bold"><strong>hosts</strong></span> setting defines the role which
     will trigger this logrotate definition being applied to a particular host.
     It can use regular expressions for pattern matching, that is,
     <span class="bold"><strong>NEU-.*</strong></span>.
    </p><p>
     The <span class="bold"><strong>service</strong></span> setting identifies the
     high-level service name associated with this content, which will be used
     for determining log files' collective quotas for storage on disk.
    </p></li><li class="step "><p>
     Verify logging is enabled by locating the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">centralized_logging:
  enabled: true
  format: rawjson</pre></div><div id="id-1.5.15.4.8.22.6.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When possible, centralized logging is most effective on log files
      generated using logstash-formatted JSON. These files should specify
      <span class="emphasis"><em>format: rawjson</em></span>. When only plaintext log files are
      available, <span class="emphasis"><em>format: json</em></span> is appropriate. (This will
      cause their plaintext log lines to be wrapped in a json envelope before
      being sent to centralized logging storage.)
     </p></div></li><li class="step "><p>
     Observe log files selected for rotation:
    </p><div class="verbatim-wrap"><pre class="screen">- files:
  - /var/log/keystone/keystone.log
  log_rotate:
  - daily
  - maxsize 300M
  - rotate 7
  - compress
  - missingok
  - notifempty
  - copytruncate
  - create 640 keystone adm</pre></div><div id="id-1.5.15.4.8.22.6.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      With the introduction of dynamic log rotation, the frequency (that is,
      <span class="emphasis"><em>daily</em></span>) and file size threshold (that is,
      <span class="emphasis"><em>maxsize</em></span>) settings no longer have any effect. The
      <span class="emphasis"><em>rotate</em></span> setting may be easily overridden on a
      service-by-service basis.
     </p></div></li><li class="step "><p>
     Commit any changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the logging reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="CL-space-allocation"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Controlling Disk Space Allocation and Retention of Log Files</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-space-allocation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-space-allocation</li></ul></div></div></div></div><p>
   Each service is assigned a weighted allocation of the
   <code class="literal">/var/log</code> filesystem's capacity. When all its log files'
   cumulative sizes exceed this allocation, a rotation is triggered for that
   service's log files according to the behavior specified in the
   <code class="literal">/etc/logrotate.d/*</code> specification.
  </p><p>
   These specification files are auto-generated based on YML sources delivered
   with the Cloud Lifecycle Manager codebase. The source files can be edited and
   reapplied to control the allocation of disk space across services or the
   behavior during a rotation.
  </p><p>
   Disk capacity is allocated as a percentage of the total weighted value of
   all services running on a particular node. For example, if 20 services run
   on the same node, all with a default weight of
   <span class="bold"><strong>100</strong></span>, they will each be granted 1/20th of
   the log filesystem's capacity. If the configuration is updated to change one
   service's weight to <span class="bold"><strong>150</strong></span>, all the services'
   allocations will be adjusted to make it possible for that one service to
   consume 150% of the space available to other individual services.
  </p><p>
   These policies are enforced by the script
   <code class="literal">/opt/kronos/rotate_if_exceeded_quota.py</code>, which will be
   executed every 5 minutes via a cron job and will rotate the log files of any
   services which have exceeded their respective quotas. When log rotation
   takes place for a service, logs are generated to describe the activity in
   <code class="literal">/var/log/kronos/check_if_exceeded_quota.log</code>.
  </p><p>
   When logrotate is performed on a service, its existing log files are
   compressed and archived to make space available for fresh log entries. Once
   the number of archived log files exceeds that service's retention
   thresholds, the oldest files are deleted. Thus, longer retention thresholds
   (that is, 10 to 15) will result in more space in the service's allocated log
   capacity being used for historic logs, while shorter retention thresholds
   (that is, 1 to 5) will keep more space available for its active plaintext log
   files.
  </p><p>
   Use the following process to make adjustments to services' log capacity
   allocations or retention thresholds:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Navigate to the following directory on your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">~/stack/scratch/ansible/next/ardana/ansible</pre></div></li><li class="step "><p>
     Open and edit the service weights file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi roles/kronos-logrotation/vars/rotation_config.yml</pre></div></li><li class="step "><p>
     Edit the service parameters to set the desired parameters. Example:
    </p><div class="verbatim-wrap"><pre class="screen">cinder:
  weight: 300
  retention: 2</pre></div><div id="id-1.5.15.4.8.23.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The retention setting of <span class="emphasis"><em>default</em></span> will use recommend
      defaults for each services' log files.
     </p></div></li><li class="step "><p>
     Run the kronos-logrotation-deploy playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-logrotation-deploy.yml</pre></div></li><li class="step "><p>
     Verify the changes to the quotas have been changed:
    </p><p>
     Login to a node and check the contents of the file
     /opt/kronos/service_info.yml to see the active quotas for that node, and
     the specifications in /etc/logrotate.d/* for rotation thresholds.
    </p></li></ol></div></div></div><div class="sect3" id="CL-elasticsearch-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Elasticsearch for Centralized Logging</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-elasticsearch-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-elasticsearch-config</li></ul></div></div></div></div><p>
   Elasticsearch includes some tunable options exposed in its configuration.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses these options in Elasticsearch to prioritize indexing speed
   over search speed. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> also configures Elasticsearch for optimal
   performance in low RAM environments. The options that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> modifies are
   listed below along with an explanation about why they were modified.
  </p><p>
   These configurations are defined in the
   <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code> file and are
   implemented in the Elasticsearch configuration file
   <code class="literal">~/openstack/my_cloud/config/logging/elasticsearch.yml.j2</code>.
  </p></div><div class="sect3" id="CL-safeguards"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.22 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Safeguards for the Log Partitions Disk Capacity</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#CL-safeguards">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-safeguards</li></ul></div></div></div></div><p>
   Because the logging partitions are at a high risk of filling up over time, a
   condition which can cause many negative side effects on services running, it
   is important to safeguard against log files consuming 100 % of available
   capacity.
  </p><p>
   This protection is implemented by pairs of low/high
   <span class="bold"><strong>watermark</strong></span> thresholds, with values
   established in
   <code class="literal">~/stack/scratch/ansible/next/ardana/ansible/roles/logging-common/defaults/main.yml</code>
   and applied by the <code class="literal">kronos-logrotation-deploy</code> playbook.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>var_log_low_watermark_percent</strong></span> (default:
     80) sets a capacity level for the contents of the
     <code class="literal">/var/log</code> partition beyond which alarms will be
     triggered (visible to administrators in monasca).
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_log_high_watermark_percent</strong></span> (default:
     95) defines how much capacity of the <code class="literal">/var/log</code> partition
     to make available for log rotation (in calculating weighted service
     allocations).
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_audit_low_watermark_percent</strong></span> (default:
     80) sets a capacity level for the contents of the
     <code class="literal">/var/audit</code> partition beyond which alarm notifications
     will be triggered.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_audit_high_watermark_percent</strong></span>
     (default: 95) sets a capacity level for the contents of the
     <code class="literal">/var/audit</code> partition which will cause log rotation to
     be forced according to the specification in
     <code class="literal">/etc/auditlogrotate.conf</code>.
    </p></li></ul></div></div></div><div class="sect2" id="topic-overview-audit-logs"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Logging Overview</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#topic-overview-audit-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_overview.xml</li><li><span class="ds-label">ID: </span>topic-overview-audit-logs</li></ul></div></div></div></div><p>
  Existing OpenStack service logging varies widely across services. Generally,
  log messages do not have enough detail about who is requesting the
  application program interface (API), or enough context-specific details about
  an action performed. Often details are not even consistently logged across
  various services, leading to inconsistent data formats being used across
  services. These issues make it difficult to integrate logging with existing
  audit tools and processes.
 </p><p>
  To help you monitor your workload and data in compliance with your corporate,
  industry or regional policies, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides auditing support as a basic
  security feature. The audit logging can be integrated with customer Security
  Information and Event Management (SIEM) tools and support your efforts to
  correlate threat forensics.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> audit logging feature uses Audit Middleware for Python services.
  This middleware service is based on OpenStack services which use the Paste
  Deploy system. Most OpenStack services use the paste deploy mechanism to find
  and configure WSGI servers and applications. Utilizing the paste deploy
  system provides auditing support in services with minimal changes.
 </p><p>
  By default, audit logging as a post-installation feature is disabled in the
  cloudConfig file on the Cloud Lifecycle Manager and it can only be enabled after
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation or upgrade.
 </p><p>
  The tasks in this section explain how to enable services for audit logging in
  your environment. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides audit logging for the following services:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    nova
   </p></li><li class="listitem "><p>
    barbican
   </p></li><li class="listitem "><p>
    keystone
   </p></li><li class="listitem "><p>
    cinder
   </p></li><li class="listitem "><p>
    ceilometer
   </p></li><li class="listitem "><p>
    neutron
   </p></li><li class="listitem "><p>
    glance
   </p></li><li class="listitem "><p>
    heat
   </p></li></ul></div><p>
  For audit log backup information see <a class="xref" href="bura-overview.html#manual-audit-log-bur" title="17.3.4. Audit Log Backup and Restore">Section 17.3.4, “Audit Log Backup and Restore”</a>
 </p><div class="sect3" id="idg-all-operations-audit-logs-checklist-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Logging Checklist</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-operations-audit-logs-checklist-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-audit-logs-checklist-xml-1</li></ul></div></div></div></div><p>
  Before enabling audit logging, make sure you understand how much disk space
  you will need, and configure the disks that will store the logging data. Use
  the following table to complete these tasks:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
      <p>
       <a class="xref" href="topic-ttn-5fg-4v.html#audit-FAQ" title="13.2.7.1.1. Frequently Asked Questions">Section 13.2.7.1.1, “Frequently Asked Questions”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="topic-ttn-5fg-4v.html#audit-log-est" title="13.2.7.1.2. Estimate Disk Size">Section 13.2.7.1.2, “Estimate Disk Size”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="topic-ttn-5fg-4v.html#audit-add-disks" title="13.2.7.1.3. Add disks to the controller nodes">Section 13.2.7.1.3, “Add disks to the controller nodes”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="topic-ttn-5fg-4v.html#audit-update-disks" title="13.2.7.1.4. Update the disk template for the controller nodes">Section 13.2.7.1.4, “Update the disk template for the controller nodes”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="topic-ttn-5fg-4v.html#audit-save-update-disks" title="13.2.7.1.5. Save your changes">Section 13.2.7.1.5, “Save your changes”</a>
      </p>
     </td></tr></tbody></table></div><div class="sect4" id="audit-FAQ"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Frequently Asked Questions</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-FAQ">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-FAQ</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.9.9.4.2.1"><span class="term ">How are audit logs generated?</span></dt><dd><p>
      The audit logs are created by services running in the cloud management
      controller nodes. The events that create auditing entries are formatted
      using a structure that is compliant with Cloud Auditing Data Federation
      (CADF) policies. The formatted audit entries are then saved to disk
      files. For more information, see the
      <a class="link" href="http://www.dmtf.org/standards/cadf" target="_blank">Cloud Auditing Data
      Federation Website.</a>
     </p></dd><dt id="id-1.5.15.4.9.9.4.2.2"><span class="term ">Where are audit logs stored?</span></dt><dd><p>
      We strongly recommend adding a dedicated disk volume for
      <code class="literal">/var/audit</code>.
     </p><p>
      If the disk templates for the controllers are not updated to create a
      separate volume for <code class="filename">/var/audit</code>,
      the audit logs will still be created in
      the root partition under the folder <code class="filename">/var/audit</code>. This
      could be problematic if the root partition does not have adequate space to
      hold the audit logs.
     </p><div id="id-1.5.15.4.9.9.4.2.2.2.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
       We recommend that you do <span class="bold"><strong>not</strong></span> store
       audit logs in the <code class="filename">/var/log</code> volume. The
       <code class="filename">/var/log</code> volume is used for storing operational logs
       and logrotation/alarms have been preconfigured for various services
       based on the size of this volume. Adding audit logs here may impact
       these causing undesired alarms. This would also impact the retention
       times for the operational logs.
      </p></div></dd><dt id="id-1.5.15.4.9.9.4.2.3"><span class="term ">Are audit logs centrally stored?</span></dt><dd><p>
      Yes. The existing operational log profiles have been configured to
      centrally log audit logs as well, once their generation has been enabled.
      The audit logs will be stored in separate Elasticsearch indices separate
      from the operational logs.
     </p></dd><dt id="id-1.5.15.4.9.9.4.2.4"><span class="term ">How long are audit log files retained?</span></dt><dd><p>
      By default, audit logs are configured to be retained for 7 days on disk.
      The audit logs are rotated each day and the rotated files are stored in a
      compressed format and retained up to 7 days (configurable). The backup
      service has been configured to back up the audit logs to a location
      outside of the controller nodes for much longer retention periods.
     </p></dd><dt id="id-1.5.15.4.9.9.4.2.5"><span class="term ">Do I lose audit data if a management controller node goes down?</span></dt><dd><p>
      Yes. For this reason, it is strongly recommended that you back up the
      audit partition in each of the management controller nodes for protection
      against any data loss.
     </p></dd></dl></div></div><div class="sect4" id="audit-log-est"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Estimate Disk Size</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-log-est">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-log-est</li></ul></div></div></div></div><p>
   The table below provides estimates from each service of audit log size
   generated per day. The estimates are provided for environments with 100
   nodes, 300 nodes, and 500 nodes.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Service</th><th>
       <p>
        Log File Size: 100 nodes
       </p>
      </th><th>
       <p>
        Log File Size: 300 nodes
       </p>
      </th><th>
       <p>
        Log File Size: 500 nodes
       </p>
      </th></tr></thead><tbody><tr><td>barbican</td><td>2.6 MB</td><td>4.2 MB</td><td>5.6 MB</td></tr><tr><td>keystone</td><td>96 - 131 MB</td><td>288 - 394 MB</td><td>480 - 657 MB</td></tr><tr><td>nova</td><td>186 (with a margin of 46) MB</td><td>557 (with a margin of 139) MB</td><td>928 (with a margin of 232) MB</td></tr><tr><td>ceilometer</td><td>12 MB</td><td>12 MB</td><td>12 MB</td></tr><tr><td>cinder</td><td>2 - 250 MB</td><td>2 - 250 MB</td><td>2 - 250 MB</td></tr><tr><td>neutron</td><td>145 MB</td><td>433 MB</td><td>722 MB</td></tr><tr><td>glance</td><td>20 (with a margin of 8) MB</td><td>60 (with a margin of 22) MB</td><td>100 (with a margin of 36) MB</td></tr><tr><td>heat</td><td>432 MB (1 transaction per second)</td><td>432 MB (1 transaction per second)</td><td>432 MB (1 transaction per second)</td></tr><tr><td>swift</td><td>33 GB (700 transactions per second)</td><td>102 GB (2100 transactions per second)</td><td>172 GB (3500 transactions per second)</td></tr></tbody></table></div></div><div class="sect4" id="audit-add-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add disks to the controller nodes</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-add-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-add-disks</li></ul></div></div></div></div><p>
   You need to add disks for the audit log partition to store the data in a
   secure manner. The steps to complete this task will vary depending on the
   type of server you are running. Please refer to the manufacturer’s
   instructions on how to add disks for the type of server node used by the
   management controller cluster. If you already have extra disks in the
   controller node, you can identify any unused one and use it for the audit
   log partition.
  </p></div><div class="sect4" id="audit-update-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the disk template for the controller nodes</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-update-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-update-disks</li></ul></div></div></div></div><p>
   Since audit logging is disabled by default, the audit volume groups in the
   disk templates are commented out. If you want to turn on audit logging, the
   template needs to be updated first. If it is not updated, there will be no
   back-up volume group. To update the disk template, you will need to copy
   templates from the examples folder to the definition folder and then edit
   the disk controller settings. Changes to the disk template used for
   provisioning cloud nodes must be made prior to deploying the nodes.
  </p><p>
   To update the disk controller template:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To copy the example templates folder, run the following command:
    </p><div id="id-1.5.15.4.9.9.7.4.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      If you already have the required templates in the definition folder, you
      can skip this step.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/entry-scale-esx/* ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     To change to the data folder, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     To edit the disks controller settings, open the file that matches your
     server model and disk model in a text editor:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Model</th><th>File</th></tr></thead><tbody><tr><td>entry-scale-kvm</td><td>
<div class="verbatim-wrap"><pre class="screen">disks_controller_1TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_controller_600GB.yml</pre></div>
        </td></tr><tr><td>mid-scale</td><td>
<div class="verbatim-wrap"><pre class="screen">disks_compute.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_control_common_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_dbmq_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_2TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_4.5TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_swobj.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_swpac.yml</pre></div>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     To update the settings and enable an audit log volume group, edit the
     appropriate file(s) listed above and remove the '#' comments from these
     lines, confirming that they are appropriate for your environment.
    </p><div class="verbatim-wrap"><pre class="screen">- name: audit-vg
  physical-volumes:
    - /dev/sdz
  logical-volumes:
    - name: audit
      size: 95%
      mount: /var/audit
      fstype: ext4
      mkfs-opts: -O large_file</pre></div></li></ol></div></div><div class="sect4" id="audit-save-update-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Save your changes</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-save-update-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-save-update-disks</li></ul></div></div></div></div><p>
   To save your changes you will use the GIT repository to add the setup disk
   files.
  </p><p>
   To save your changes:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the openstack directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack</pre></div></li><li class="listitem "><p>
     To add the new and updated files, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A</pre></div></li><li class="listitem "><p>
     To verify the files are added, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div></li><li class="listitem "><p>
     To commit your changes, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Setup disks for audit logging"</pre></div></li></ol></div></div></div><div class="sect3" id="topic-enable-audit-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Audit Logging</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#topic-enable-audit-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>topic-enable-audit-logs</li></ul></div></div></div></div><p>
  To enable audit logging you must edit your cloud configuration settings, save
  your changes and re-run the configuration processor. Then you can run the
  playbooks to create the volume groups and configure them.
 </p><p>
  In the <code class="literal">~/openstack/my_cloud/definition/cloudConfig.yml</code> file,
  service names defined under enabled-services or disabled-services override
  the default setting.
 </p><p>
  The following is an example of your audit-settings section:
 </p><div class="verbatim-wrap"><pre class="screen"># Disc space needs to be allocated to the audit directory before enabling
# auditing.
# Default can be either "disabled" or "enabled". Services listed in
# "enabled-services" and "disabled-services" override the default setting.
audit-settings:
   default: disabled
   #enabled-services:
   #  - keystone
   #  - barbican
   disabled-services:
     - nova
     - barbican
     - keystone
     - cinder
     - ceilometer
     - neutron</pre></div><p>
  In this example, although the default setting for all services is set to
  <span class="bold"><strong>disabled</strong></span>, keystone and barbican may be
  explicitly enabled by removing the comments from these lines and this setting
  overrides the default.
 </p><div class="sect4" id="audit-edit-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To edit the configuration file:</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-edit-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-edit-config</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To change to the cloud definition folder, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition</pre></div></li><li class="listitem "><p>
     To edit the auditing settings, in a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">cloudConfig.yml</pre></div></li><li class="listitem "><p>
     To enable audit logging, begin by uncommenting the "enabled-services:"
     block.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       enabled-service:
      </p></li><li class="listitem "><p>
       any service you want to enable for audit logging.
      </p></li></ul></div><p>
     For example, keystone has been enabled in the following text:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Default cloudConfig.yml file</th><th>Enabling keystone audit logging</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
#  - keystone</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
  - keystone</pre></div>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     To move the services you want to enable, comment out the service in the
     disabled section and add it to the enabled section. For example, barbican
     has been enabled in the following text:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>cloudConfig.yml file</th><th>Enabling barbican audit logging</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
  - keystone
disabled-services:
   - nova
   # - keystone
   - barbican
   - cinder</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
 - keystone
 - barbican
disabled-services:
 - nova
 # - barbican
 # - keystone
 - cinder</pre></div>
        </td></tr></tbody></table></div></li></ol></div></div><div class="sect4" id="audit-save-config2"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To save your changes and run the configuration processor:</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-save-config2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-save-config2</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the openstack directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack</pre></div></li><li class="listitem "><p>
     To add the new and updated files, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A</pre></div></li><li class="listitem "><p>
     To verify the files are added, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div></li><li class="listitem "><p>
     To commit your changes, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Enable audit logging"</pre></div></li><li class="listitem "><p>
     To change to the directory with the ansible playbooks, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="listitem "><p>
     To rerun the configuration processor, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div><div class="sect4" id="audit-create-vgroup"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To create the volume group:</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-create-vgroup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-create-vgroup</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the osconfig playbook, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To remove the stub file that osconfig uses to decide if the disks are
     already configured, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts KEY-API -a 'sudo rm -f /etc/openstack/osconfig-ran'</pre></div><div id="id-1.5.15.4.9.10.9.2.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The osconfig playbook uses the stub file to mark already configured disks
      as "idempotent." To stop osconfig from identifying your new disk as
      already configured, you must remove the stub file /etc/hos/osconfig-ran
      before re-running the osconfig playbook.
     </p></div></li><li class="listitem "><p>
     To run the playbook that enables auditing for a service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API</pre></div><div id="id-1.5.15.4.9.10.9.2.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The variable KEY-API is used as an example to cover the management
      controller cluster. To enable auditing for a service that is not run on
      the same cluster, add the service to the –limit flag in the above
      command. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API:NEU-SVR</pre></div></div></li></ol></div></div><div class="sect4" id="audit-reconfig-services"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Reconfigure services for audit logging:</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#audit-reconfig-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-reconfig-services</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the service playbooks, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the playbook that reconfigures a service for audit logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts <em class="replaceable ">SERVICE_NAME</em>-reconfigure.yml</pre></div><p>
     For example, to reconfigure keystone for audit logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Repeat steps 1 and 2 for each service you need to reconfigure.
    </p><div id="id-1.5.15.4.9.10.10.2.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      You must reconfigure each service that you changed to be enabled or
      disabled in the cloudConfig.yml file.
     </p></div></li></ol></div></div></div></div><div class="sect2" id="id-1.5.15.4.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#id-1.5.15.4.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-centralized_logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-centralized_logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For information on troubleshooting Central Logging, see
   <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-central-log-troubleshoot" title="18.7.1. Troubleshooting Centralized Logging">Section 18.7.1, “Troubleshooting Centralized Logging”</a>.
  </p></div></div><div class="sect1" id="ceilo-metering-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service (ceilometer) Overview</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-metering-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_overview.xml</li><li><span class="ds-label">ID: </span>ceilo-metering-overview</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> metering service collects and provides access to OpenStack
   usage data that can be used for billing reporting such as showback and
   chargeback. The metering service can also provide general usage reporting.
   ceilometer acts as the central collection and data access service to the
   meters provided by all the OpenStack services. The data collected is
   available through the monasca API. ceilometer V2 API was deprecated in the
   Pike release upstream.
   </p><div class="sect2" id="Metering-NewFunctions"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service New Functionality</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Metering-NewFunctions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>Metering-NewFunctions</li></ul></div></div></div></div><div class="sect3" id="newfunct"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Metering Functionality in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#newfunct">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>newfunct</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     ceilometer is now integrated with monasca, using it as the datastore.
    </p></li><li class="listitem "><p>
     The default meters and other items configured for ceilometer can now be
     modified and additional meters can be added. We recommend that users test
     overall <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> performance prior to deploying any ceilometer
     modifications to ensure the addition of new notifications or polling
     events does not negatively affect overall system performance.
    </p></li><li class="listitem "><p>
     ceilometer Central Agent (pollster) is now called Polling Agent and is
     configured to support HA (Active-Active).
    </p></li><li class="listitem "><p>
     Notification Agent has built-in HA (Active-Active) with support for
     pipeline transformers, but workload partitioning has been disabled in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    </p></li><li class="listitem "><p>
     SWIFT Poll-based account level meters will be enabled by default with an
     hourly collection cycle.
    </p></li><li class="listitem "><p>
     Integration with centralized monitoring (monasca) and centralized logging
    </p></li><li class="listitem "><p>
     Support for upgrade and reconfigure operations
    </p></li></ul></div></div><div class="sect3" id="idg-all-metering-metering-newfunctions-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-metering-metering-newfunctions-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-newfunctions-xml-7</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Number of metadata attributes that can be extracted from
     resource_metadata has a maximum of 16. This is the number of fields in the
     metadata section of the
     <span class="bold"><strong>monasca_field_definitions.yaml</strong></span> file for
     any service. It is also the number that is equal to fields in
     metadata.common and fields in metadata.&lt;service.meters&gt; sections.
     The total number of these fields cannot be more than 16.
    </p></li><li class="listitem "><p>
     Several network-related attributes are accessible using a colon ":" but
     are returned as a period ".". For example, you can access a sample list
     using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>ceilometer --debug sample-list network -q "resource_id=421d50a5-156e-4cb9-b404-
d2ce5f32f18b;resource_metadata.provider.network_type=flat"</pre></div><p>
     However, in response you will see the following:
    </p><div class="verbatim-wrap"><pre class="screen">provider.network_type</pre></div><p>
     instead of
    </p><div class="verbatim-wrap"><pre class="screen">provider:network_type</pre></div><p>
     This limitation is known for the following attributes:
    </p><div class="verbatim-wrap"><pre class="screen">provider:network_type
provider:physical_network
provider:segmentation_id</pre></div></li><li class="listitem "><p>
     ceilometer Expirer is not supported. Data retention expiration is handled
     by monasca with a default retention period of 45 days.
    </p></li><li class="listitem "><p>
     ceilometer Collector is not supported.
    </p></li></ul></div></div></div><div class="sect2" id="ceilo-metering-concepts-overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Metering Service Concepts</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-metering-concepts-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-metering-concepts-overview</li></ul></div></div></div></div><div class="sect3" id="ceilo-concept-intro"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Introduction</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-concept-intro">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-concept-intro</li></ul></div></div></div></div><p>
   Before configuring the ceilometer Metering Service, it is important to
   understand how it works.
  </p><div class="sect4" id="ceilo-architecture"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.3.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Architecture</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-architecture">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-architecture</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> automatically configures ceilometer to use Logging and
   Monitoring Service (monasca) as its backend. ceilometer is deployed on the
   same control plane nodes as monasca.
  </p><p>
   The installation of Celiometer creates several management nodes running
   different metering components.
  </p><p>
   <span class="bold"><strong>ceilometer Components on Controller nodes</strong></span>
  </p><p>
   This controller node is the first of the High Available (HA) cluster.
  </p><p>
   <span class="bold"><strong>ceilometer Sample Polling</strong></span>
  </p><p>
   Sample Polling is part of the Polling Agent. Messages are posted by the
   Notification Agent directly to monasca API.
  </p><p>
   <span class="bold"><strong>ceilometer Polling Agent</strong></span>
  </p><p>
   The Polling Agent is responsible for coordinating the polling activity. It
   parses the <code class="filename">pipeline.yml</code> configuration file and
   identifies all the sources that need to be polled. The sources are then
   evaluated using a discovery mechanism and all the sources are translated to
   resources where a dedicated pollster can retrieve and publish data. At each
   identified interval the discovery mechanism is triggered, the resource list
   is composed, and the data is polled and sent to the queue.
  </p><p>
   <span class="bold"><strong>ceilometer Collector No Longer Required</strong></span>
  </p><p>
   In previous versions, the collector was responsible for getting the
   samples/events from the RabbitMQ service and storing it in the main
   database. The ceilometer Collector is no longer enabled. Now that
   Notification Agent posts the data directly to monasca API, the collector is
   no longer required
  </p></div><div class="sect4" id="ceilo-about-meters"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.3.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Meter Reference</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-about-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-about-meters</li></ul></div></div></div></div><p>
   ceilometer collects basic information grouped into categories known as
   <code class="literal">meters</code>. A meter is the unique resource-usage measurement
   of a particular OpenStack service. Each OpenStack service defines what type
   of data is exposed for metering.
  </p><p>
   Each meter has the following characteristics:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Attribute</th><th>Description</th></tr></thead><tbody><tr><td>Name</td><td>Description of the meter</td></tr><tr><td>Unit of Measurement</td><td>The method by which the data is measured. For example: storage meters are
                defined in Gigabytes (GB) and network bandwidth is measured in Gigabits
                (Gb).</td></tr><tr><td>Type</td><td><p>The origin of the meter's data. OpenStack defines the following origins: </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Cumulative - Increasing over time (instance hours)
         </p></li><li class="listitem "><p>
          Gauge - a discrete value. For example: the number of floating IP
          addresses or image uploads.
         </p></li><li class="listitem "><p>
          Delta - Changing over time (bandwidth)
         </p></li></ul></div>
      </td></tr></tbody></table></div><p>
   A meter is defined for every measurable resource. A meter can exist beyond
   the actual existence of a particular resource, such as an active instance,
   to provision long-cycle use cases such as billing.
  </p><div id="id-1.5.15.5.4.2.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    For a list of meter types and default meters installed with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, see
    <a class="xref" href="topic-ttn-5fg-4v.html#topic3051" title="13.3.3. Ceilometer Metering Available Meter Types">Section 13.3.3, “Ceilometer Metering Available Meter Types”</a>
   </p></div><p>
   The most common meter submission method is notifications. With this method,
   each service sends the data from their respective meters on a periodic basis
   to a common notifications bus.
  </p><p>
   ceilometer, in turn, pulls all of the events from the bus and saves the
   notifications in a ceilometer-specific database. The period of time that the
   data is collected and saved is known as the ceilometer expiry and is
   configured during ceilometer installation. Each meter is collected from one
   or more samples, gathered from the messaging queue or polled by agents. The
   samples are represented by counter objects. Each counter has the following
   fields:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Attribute</th><th>Description</th></tr></thead><tbody><tr><td>counter_name</td><td>Description of the counter</td></tr><tr><td>counter_unit</td><td>The method by which the data is measured. For example: data can be
                defined in Gigabytes (GB) or for network bandwidth, measured in Gigabits
                (Gb).</td></tr><tr><td>counter_typee</td><td>
       <p>The origin of the counter's data. OpenStack defines the following origins:</p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Cumulative - Increasing over time (instance hours)
         </p></li><li class="listitem "><p>
          Gauge - a discrete value. For example: the number of floating IP
          addresses or image uploads.
         </p></li><li class="listitem "><p>
          Delta - Changing over time (bandwidth)
         </p></li></ul></div>
      </td></tr><tr><td>counter_volume</td><td>The volume of data measured (CPU ticks, bytes transmitted, etc.). Not used for gauge
                counters. Set to a default value such as 1.</td></tr><tr><td>resource_id</td><td>The identifier of the resource measured (UUID)</td></tr><tr><td>project_id</td><td>The project (tenant) ID to which the resource belongs.</td></tr><tr><td>user_id</td><td>The ID of the user who owns the resource.</td></tr><tr><td>resource_metadata</td><td>Other data transmitted in the metering notification payload.</td></tr></tbody></table></div></div></div></div><div class="sect2" id="topic3051"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Available Meter Types</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#topic3051">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_metertypes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_metertypes.xml</li><li><span class="ds-label">ID: </span>topic3051</li></ul></div></div></div></div><p>
  The Metering service contains three types of meters:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.5.5.3.1"><span class="term ">Cumulative</span></dt><dd><p>
     A cumulative meter measures data over time (for example, instance hours).
    </p></dd><dt id="id-1.5.15.5.5.3.2"><span class="term ">Gauge</span></dt><dd><p>
     A gauge measures discrete items (for example, floating IPs or image
     uploads) or fluctuating values (such as disk input or output).
    </p></dd><dt id="id-1.5.15.5.5.3.3"><span class="term ">Delta</span></dt><dd><p>
     A delta measures change over time, for example, monitoring bandwidth.
    </p></dd></dl></div><p>
  Each meter is populated from one or more <span class="emphasis"><em>samples</em></span>, which
  are gathered from the messaging queue (listening agent), polling agents, or
  push agents. Samples are populated by <span class="emphasis"><em>counter</em></span> objects.
 </p><p>
  Each counter contains the following <span class="emphasis"><em>fields</em></span>:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.5.5.6.1"><span class="term ">name</span></dt><dd><p>
     the name of the meter
    </p></dd><dt id="id-1.5.15.5.5.6.2"><span class="term ">type</span></dt><dd><p>
     the type of meter (cumulative, gauge, or delta)
    </p></dd><dt id="id-1.5.15.5.5.6.3"><span class="term ">amount</span></dt><dd><p>
     the amount of data measured
    </p></dd><dt id="id-1.5.15.5.5.6.4"><span class="term ">unit</span></dt><dd><p>
     the unit of measure
    </p></dd><dt id="id-1.5.15.5.5.6.5"><span class="term ">resource</span></dt><dd><p>
     the resource being measured
    </p></dd><dt id="id-1.5.15.5.5.6.6"><span class="term ">project ID</span></dt><dd><p>
     the project the resource is assigned to
    </p></dd><dt id="id-1.5.15.5.5.6.7"><span class="term ">user</span></dt><dd><p>
     the user the resource is assigned to.
    </p></dd></dl></div><p>
  <span class="bold"><strong>Note</strong></span>: The metering service shares the same
  High-availability proxy, messaging, and database clusters with the other
  Information services. To avoid unnecessarily high loads,
  <a class="xref" href="topic-ttn-5fg-4v.html#Ceilo-optimize" title="13.3.8. Optimizing the Ceilometer Metering Service">Section 13.3.8, “Optimizing the Ceilometer Metering Service”</a>.
 </p><div class="sect3" id="openstack-default-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Default Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#openstack-default-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_metertypes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_metertypes.xml</li><li><span class="ds-label">ID: </span>openstack-default-meters</li></ul></div></div></div></div><p>
   These meters are installed and enabled by default during an <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation. More information about ceilometer can be found at <a class="link" href="https://docs.openstack.org/ceilometer/latest/" target="_blank">OpenStack
   ceilometer</a>.
  </p></div><div class="sect3" id="nova-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (nova) Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#nova-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-nova_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-nova_meters.xml</li><li><span class="ds-label">ID: </span>nova-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>vcpus</td><td>Gauge</td><td>vcpu</td><td>Instance ID</td><td>Notification</td><td>Number of virtual CPUs allocated to the instance</td></tr><tr><td>memory</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Notification</td><td>Volume of RAM allocated to the instance</td></tr><tr><td>memory.resident</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Pollster</td><td>Volume of RAM used by the instance on the physical machine</td></tr><tr><td>memory.usage</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Pollster</td><td>Volume of RAM used by the instance from the amount of its allocated
                memory</td></tr><tr><td>cpu</td><td>Cumulative</td><td>ns</td><td>Instance ID</td><td>Pollster</td><td>CPU time used</td></tr><tr><td>cpu_util</td><td>Gauge</td><td>%</td><td>Instance ID</td><td>Pollster</td><td>Average CPU utilization</td></tr><tr><td>disk.read.requests</td><td>Cumulative</td><td>request</td><td>Instance ID</td><td>Pollster</td><td>Number of read requests</td></tr><tr><td>disk.read.requests.rate</td><td>Gauge</td><td>request/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of read requests</td></tr><tr><td>disk.write.requests</td><td>Cumulative</td><td>request</td><td>Instance ID</td><td>Pollster</td><td>Number of write requests</td></tr><tr><td>disk.write.requests.rate</td><td>Gauge</td><td>request/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of write requests</td></tr><tr><td>disk.read.bytes</td><td>Cumulative</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>Volume of reads</td></tr><tr><td>disk.read.bytes.rate</td><td>Gauge</td><td>B/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of reads</td></tr><tr><td>disk.write.bytes</td><td>Cumulative</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>Volume of writes</td></tr><tr><td>disk.write.bytes.rate</td><td>Gauge</td><td>B/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of writes</td></tr><tr><td>disk.root.size</td><td>Gauge</td><td>GB</td><td>Instance ID</td><td>Notification</td><td>Size of root disk</td></tr><tr><td>disk.ephemeral.size</td><td>Gauge</td><td>GB</td><td>Instance ID</td><td>Notification</td><td>Size of ephemeral disk</td></tr><tr><td>disk.device.read.requests</td><td>Cumulative</td><td>request</td><td>Disk ID</td><td>Pollster</td><td>Number of read requests</td></tr><tr><td>disk.device.read.requests.rate</td><td>Gauge</td><td>request/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of read requests</td></tr><tr><td>disk.device.write.requests</td><td>Cumulative</td><td>request</td><td>Disk ID</td><td>Pollster</td><td>Number of write requests</td></tr><tr><td>disk.device.write.requests.rate</td><td>Gauge</td><td>request/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of write requests</td></tr><tr><td>disk.device.read.bytes</td><td>Cumulative</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>Volume of reads</td></tr><tr><td>disk.device.read.bytes .rate</td><td>Gauge</td><td>B/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of reads</td></tr><tr><td>disk.device.write.bytes</td><td>Cumulative</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>Volume of writes</td></tr><tr><td>disk.device.write.bytes .rate</td><td>Gauge</td><td>B/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of writes</td></tr><tr><td>disk.capacity</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The amount of disk that the instance can see</td></tr><tr><td>disk.allocation</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The amount of disk occupied by the instance on the host machine</td></tr><tr><td>disk.usage</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The physical size in bytes of the image container on the host</td></tr><tr><td>disk.device.capacity</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The amount of disk per device that the instance can see</td></tr><tr><td>disk.device.allocation</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The amount of disk per device occupied by the instance on the host
                machine</td></tr><tr><td>disk.device.usage</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The physical size in bytes of the image container on the host per
                device</td></tr><tr><td>network.incoming.bytes</td><td>Cumulative</td><td>B</td><td>Interface ID</td><td>Pollster</td><td>Number of incoming bytes</td></tr><tr><td>network.outgoing.bytes</td><td>Cumulative</td><td>B</td><td>Interface ID</td><td>Pollster</td><td>Number of outgoing bytes</td></tr><tr><td>network.incoming.packets</td><td>Cumulative</td><td>packet</td><td>Interface ID</td><td>Pollster</td><td>Number of incoming packets</td></tr><tr><td>network.outgoing.packets</td><td>Cumulative</td><td>packet</td><td>Interface ID</td><td>Pollster</td><td>Number of outgoing packets</td></tr></tbody></table></div></div><div class="sect3" id="computehost-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Host Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#computehost-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-computehost_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-computehost_meters.xml</li><li><span class="ds-label">ID: </span>computehost-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>compute.node.cpu.frequency</td><td>Gauge</td><td>MHz</td><td>Host ID</td><td>Notification</td><td>CPU frequency</td></tr><tr><td>compute.node.cpu.kernel.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU kernel time</td></tr><tr><td>compute.node.cpu.idle.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU idle time</td></tr><tr><td>compute.node.cpu.user.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU user mode time</td></tr><tr><td>compute.node.cpu.iowait.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU I/O wait time</td></tr><tr><td>compute.node.cpu.kernel.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU kernel percentage</td></tr><tr><td>compute.node.cpu.idle.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU idle percentage</td></tr><tr><td>compute.node.cpu.user.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU user mode percentage</td></tr><tr><td>compute.node.cpu.iowait.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU I/O wait percentage</td></tr><tr><td>compute.node.cpu.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU utilization</td></tr></tbody></table></div></div><div class="sect3" id="glance-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image (glance) Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#glance-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-glance_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-glance_meters.xml</li><li><span class="ds-label">ID: </span>glance-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>image.size</td><td>Gauge</td><td>B</td><td>Image ID</td><td>Notification</td><td>Uploaded image size</td></tr><tr><td>image.update</td><td>Delta</td><td>Image</td><td>Image ID</td><td>Notification</td><td>Number of uploads of the image</td></tr><tr><td>image.upload</td><td>Delta</td><td>Image</td><td>image ID</td><td>notification</td><td>Number of uploads of the image</td></tr><tr><td>image.delete</td><td>Delta</td><td>Image</td><td>Image ID</td><td>Notification</td><td>Number of deletes on the image</td></tr></tbody></table></div></div><div class="sect3" id="cinder-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Volume (cinder) Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#cinder-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-cinder_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-cinder_meters.xml</li><li><span class="ds-label">ID: </span>cinder-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>volume.size</td><td>Gauge</td><td>GB</td><td>Vol ID</td><td>Notification</td><td>Size of volume</td></tr><tr><td>snapshot.size</td><td>Gauge</td><td>GB</td><td>Snap ID</td><td>Notification</td><td>Size of snapshot's volume</td></tr></tbody></table></div></div><div class="sect3" id="swift-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage (swift) Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#swift-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-swift_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-swift_meters.xml</li><li><span class="ds-label">ID: </span>swift-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>storage.objects</td><td>Gauge</td><td>Object</td><td>Storage ID</td><td>Pollster</td><td>Number of objects</td></tr><tr><td>storage.objects.size</td><td>Gauge</td><td>B</td><td>Storage ID</td><td>Pollster</td><td>Total size of stored objects</td></tr><tr><td>storage.objects.containers</td><td>Gauge</td><td>Container</td><td>Storage ID</td><td>Pollster</td><td>Number of containers</td></tr></tbody></table></div><p>
  The <code class="literal">resource_id</code> for any ceilometer query is the
  <code class="literal">tenant_id</code> for the swift object because swift usage is
  rolled up at the tenant level.
 </p></div></div><div class="sect2" id="reconfig-metering"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the Ceilometer Metering Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#reconfig-metering">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>reconfig-metering</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 automatically deploys ceilometer to use the monasca database.
  ceilometer is deployed on the same control plane nodes along with other
  OpenStack services such as keystone, nova, neutron, glance, and swift.
 </p><p>
  The Metering Service can be configured using one of the procedures described
  below.
 </p><div class="sect3" id="idg-all-metering-metering-reconfig-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Run the Upgrade Playbook</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-metering-metering-reconfig-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-reconfig-xml-7</li></ul></div></div></div></div><p>
   Follow Standard Service upgrade mechanism available in the Cloud Lifecycle Manager
   distribution. For ceilometer, the playbook included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is
   <span class="bold"><strong>ceilometer-upgrade.yml</strong></span>
  </p></div><div class="sect3" id="metering-services"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Services for Messaging Notifications</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#metering-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>metering-services</li></ul></div></div></div></div><p>
   After installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the following services are enabled
   by default to send notifications:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     nova
    </p></li><li class="listitem "><p>
     cinder
    </p></li><li class="listitem "><p>
     glance
    </p></li><li class="listitem "><p>
     neutron
    </p></li><li class="listitem "><p>
     swift
    </p></li></ul></div><p>
   The list of meters for these services are specified in the Notification
   Agent or Polling Agent's pipeline configuration file.
  </p><p>
   For steps on how to edit the pipeline configuration files, see:
   <a class="xref" href="topic-ttn-5fg-4v.html#notifications" title="13.3.5. Ceilometer Metering Service Notifications">Section 13.3.5, “Ceilometer Metering Service Notifications”</a>
  </p></div><div class="sect3" id="Ceilo-StopStart"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Polling Agent</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Ceilo-StopStart">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>Ceilo-StopStart</li></ul></div></div></div></div><p>
   The Polling Agent is responsible for coordinating the polling activity. It
   parses the <span class="bold"><strong>pipeline.yml</strong></span> configuration file
   and identifies all the sources where data is collected. The sources are then
   evaluated and are translated to resources that a dedicated pollster can
   retrieve. The Polling Agent follows this process:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     At each identified interval, the
     <span class="bold"><strong>pipeline.yml</strong></span> configuration file is
     parsed.
    </p></li><li class="listitem "><p>
     The resource list is composed.
    </p></li><li class="listitem "><p>
     The pollster collects the data.
    </p></li><li class="listitem "><p>
     The pollster sends data to the queue.
    </p></li></ol></div><p>
   Metering processes should normally be operating at all times. This need is
   addressed by the Upstart event engine which is designed to run on any Linux
   system. Upstart creates events, handles the consequences of those events,
   and starts and stops processes as required. Upstart will continually attempt
   to restart stopped processes even if the process was stopped manually. To
   stop or start the Polling Agent and avoid the conflict with Upstart, using
   the following steps.
  </p><p>
   <span class="bold"><strong>To restart the Polling Agent:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To determine whether the process is running, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl status ceilometer-agent-notification
#SAMPLE OUTPUT:
ceilometer-agent-notification.service - ceilometer-agent-notification Service
   Loaded: loaded (/etc/systemd/system/ceilometer-agent-notification.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-06-12 05:07:14 UTC; 2 days ago
 Main PID: 31529 (ceilometer-agen)
    Tasks: 69
   CGroup: /system.slice/ceilometer-agent-notification.service
           ├─31529 ceilometer-agent-notification: master process [/opt/stack/service/ceilometer-agent-notification/venv/bin/ceilometer-agent-notification --config-file /opt/stack/service/ceilometer-agent-noti...
           └─31621 ceilometer-agent-notification: NotificationService worker(0)

Jun 12 05:07:14 ardana-qe201-cp1-c1-m2-mgmt systemd[1]: Started ceilometer-agent-notification Service.</pre></div></li><li class="step "><p>
     To stop the process, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop ceilometer-agent-notification</pre></div></li><li class="step "><p>
     To start the process, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl start ceilometer-agent-notification</pre></div></li></ol></div></div></div><div class="sect3" id="ceilo-replace-cntrler"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replace a Logging, Monitoring, and Metering Controller</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-replace-cntrler">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>ceilo-replace-cntrler</li></ul></div></div></div></div><p>
   In a medium-scale environment, if a metering controller has to be replaced
   or rebuilt, use the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     <a class="xref" href="system-maintenance.html#replacing-controller" title="15.1.2.1. Replacing a Controller Node">Section 15.1.2.1, “Replacing a Controller Node”</a>.
    </p></li><li class="step "><p>
     If the ceilometer nodes are not on the shared control plane, to implement
     the changes and replace the controller, you must reconfigure ceilometer.
     To do this, run the ceilometer-reconfigure.yml ansible playbook
     <span class="bold"><strong>without</strong></span> the limit option
    </p></li></ol></div></div></div><div class="sect3" id="ceilo-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Monitoring</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>ceilo-monitoring</li></ul></div></div></div></div><p>
   The monasca HTTP Process monitors ceilometer's notification and polling
   agents are monitored. If these agents are down, monasca monitoring alarms
   are triggered. You can use the notification alarms to debug the issue and
   restart the notifications agent. However, for
   <code class="literal">Central-Agent</code> (polling) and <code class="literal">Collector</code>
   the alarms need to be deleted. These two processes are not started after an
   upgrade so when the monitoring process checks the alarms for these
   components, they will be in <code class="literal">UNDETERMINED</code>
   state. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not monitor these processes anymore. To resolve
   this issue, manually delete alarms that are no longer used but are
   installed.
  </p><p>
   To resolve notification alarms, first check the
   <span class="bold"><strong>ceilometer-agent-notification</strong></span> logs for
   errors in the <span class="bold"><strong>/var/log/ceilometer</strong></span>
   directory. You can also use the Operations Console to access Kibana and
   check the logs. This will help you understand and debug the error.
  </p><p>
   To restart the service, run the
   <span class="bold"><strong>ceilometer-start.yml</strong></span>. This playbook starts
   the ceilometer processes that has stopped and only restarts during install,
   upgrade or reconfigure which is what is needed in this case. Restarting the
   process that has stopped will resolve this alarm because this monasca alarm
   means that ceilometer-agent-notification is no longer running on certain
   nodes.
  </p><p>
   You can access ceilometer data through monasca. ceilometer publishes samples
   to monasca with credentials of the following accounts:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>ceilometer</strong></span> user
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>services</strong></span>
    </p></li></ul></div><p>
   Data collected by ceilometer can also be retrieved by the monasca REST API.
   Make sure you use the following guidelines when requesting data from the
   monasca REST API:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Verify you have the monasca-admin role. This role is configured in the
     monasca-api configuration file.
    </p></li><li class="listitem "><p>
     Specify the <code class="literal">tenant id</code> of the
     <span class="bold"><strong>services</strong></span> project.
    </p></li></ul></div><p>
   For more details, read the
   <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">monasca
   API Specification</a>.
  </p><p>
   To run monasca commands at the command line, you must be have the
   <span class="bold"><strong>admin</strong></span> role. This allows you to use the
   ceilometer account credentials to replace the default admin account
   credentials defined in the <span class="bold"><strong>service.osrc</strong></span>
   file. When you use the ceilometer account credentials, monasca commands will
   only return data collected by ceilometer. At this time, monasca command line
   interface (CLI) does not support the data retrieval of other tenants or
   projects.
  </p></div></div><div class="sect2" id="notifications"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Service Notifications</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#notifications">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>notifications</li></ul></div></div></div></div><p>
  ceilometer uses the notification agent to listen to the message queue,
  convert notifications to Events and Samples, and apply pipeline actions.
 </p><div class="sect3" id="whitelist"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manage Whitelisting and Polling</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#whitelist">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>whitelist</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is designed to reduce the amount of data that is stored.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>'s use of a SQL-based cluster, which is not recommended for big data,
   means you must control the data that ceilometer collects. You can do this by
   filtering (whitelisting) the data or by using the configuration files for
   the ceilometer Polling Agent and the ceilometer Notificfoation Agent.
  </p><p>
   Whitelisting is used in a rule specification as a positive filtering
   parameter. Whitelist is only included in rules that can be used in direct
   mappings, for identity service issues such as service discovery,
   provisioning users, groups, roles, projects, domains as well as user
   authentication and authorization.
  </p><p>
   You can run tests against specific scenarios to see if filtering reduces the
   amount of data stored. You can create a test by editing or creating a run
   filter file (whitelist). For steps on how to do this, see:
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 38 “Post Installation Tasks”, Section 38.1 “API Verification”</span>.
  </p><p>
   ceilometer Polling Agent (polling agent) and ceilometer Notification Agent
   (notification agent) use different pipeline.yaml files to configure meters
   that are collected. This prevents accidentally polling for meters which can
   be retrieved by the polling agent as well as the notification agent. For
   example, glance image and image.size are meters which can be retrieved both
   by polling and notifications.
  </p><p>
   In both of the separate configuration files, there is a setting for
   <code class="literal">interval</code>. The interval attribute determines the
   frequency, in seconds, of how often data is collected. You can use this
   setting to control the amount of resources that are used for notifications
   and for polling. For example, you want to use more resources for
   notifications and less for polling. To accomplish this you would set the
   <code class="literal">interval</code> in the polling configuration file to a large
   amount of time, such as 604800 seconds, which polls only once a week. Then
   in the notifications configuration file, you can set the
   <code class="literal">interval</code> to a higher amount, such as collecting data
   every 30 seconds.
  </p><div id="id-1.5.15.5.7.3.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    swift account data will be collected using the polling mechanism in an
    hourly interval.
   </p></div><p>
   Setting this interval to manage both notifications and polling is the
   recommended procedure when using a SQL cluster back-end.
  </p><p>
   <span class="bold"><strong>Sample ceilometer Polling Agent file:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">#File: ~/opt/stack/service/ceilometer-polling/etc/pipeline-polling.yaml
---
sources:
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
   <span class="bold"><strong>Sample ceilometer Notification Agent(notification
   agent) file:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">#File:    ~/opt/stack/service/ceilometer-agent-notification/etc/pipeline-agent-notification.yaml
---
sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"
resources:
discovery:
sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
   Both of the pipeline files have two major sections:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.5.7.3.14.1"><span class="term ">Sources</span></dt><dd><p>
      represents the data that is collected either from notifications posted by
      services or through polling. In the Sources section there is a list of
      meters. These meters define what kind of data is collected. For a full
      list refer to the ceilometer documentation available at:
      <a class="link" href="http://docs.openstack.org/admin-guide/telemetry-measurements.html" target="_blank">Telemetry
      Measurements</a>
     </p></dd><dt id="id-1.5.15.5.7.3.14.2"><span class="term ">Sinks</span></dt><dd><p>
      represents how the data is modified before it is published to the
      internal queue for collection and storage.
     </p></dd></dl></div><p>
   You will only need to change a setting in the Sources section to control the
   data collection interval.
  </p><p>
   For more information, see
   <a class="link" href="http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html" target="_blank">Telemetry
   Measurements</a>
  </p><p>
   <span class="bold"><strong>To change the ceilometer Polling Agent interval
   setting:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To find the polling agent configuration file, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/opt/stack/service/ceilometer-polling/etc</pre></div></li><li class="step "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">pipeline-polling.yaml</pre></div></li><li class="step "><p>
     In the following section, change the value of <code class="literal">interval</code>
     to the desired amount of time:
    </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
     In the sample code above, the polling agent will collect data every 600
     seconds, or 10 minutes.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>To change the ceilometer Notification Agent
   (notification agent) interval setting:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To find the notification agent configuration file, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd /opt/stack/service/ceilometer-agent-notification</pre></div></li><li class="step "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">pipeline-agent-notification.yaml</pre></div></li><li class="step "><p>
     In the following section, change the value of <code class="literal">interval</code>
     to the desired amount of time:
    </p><div class="verbatim-wrap"><pre class="screen">sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"</pre></div><p>
     In the sample code above, the notification agent will collect data every
     30 seconds.
    </p></li></ol></div></div><div id="id-1.5.15.5.7.3.21" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The <code class="literal">pipeline-agent-notification.yaml</code> file needs to be changed on all
    controller nodes to change the white-listing and polling strategy.
   </p></div></div><div class="sect3" id="idg-all-metering-metering-notifications-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Edit the List of Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#idg-all-metering-metering-notifications-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-notifications-xml-7</li></ul></div></div></div></div><p>
   The number of enabled meters can be reduced or increased by editing the
   pipeline configuration of the notification and polling agents. To deploy
   these changes you must then restart the agent. If pollsters and
   notifications are both modified, then you will have to restart both the
   Polling Agent and the Notification Agent. ceilometer Collector will also
   need to be restarted. The following code is an example of a compute-only
   ceilometer Notification Agent (notification agent)
   <span class="bold"><strong>pipeline-agent-notification.yaml </strong></span>file:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><div id="id-1.5.15.5.7.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you enable meters at the container level in this file, every time the
    polling interval triggers a collection, at least 5 messages per existing
    container in swift are collected.
   </p></div><p>
   The following table illustrates the amount of data produced hourly in
   different scenarios:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td>swift Containers</td><td>swift Objects per container</td><td>Samples per Hour</td><td>Samples stored per 24 hours</td></tr><tr><td>10</td><td>10</td><td>500</td><td>12000</td></tr><tr><td>10</td><td>100</td><td>5000</td><td>120000</td></tr><tr><td>100</td><td>100</td><td>50000</td><td>1200000</td></tr><tr><td>100</td><td>1000</td><td>500000</td><td>12000000</td></tr></tbody></table></div><p>
   The data in the table shows that even a very small swift storage with 10
   containers and 100 files will store 120,000 samples in 24 hours, generating
   a total of 3.6 million samples.
  </p><div id="id-1.5.15.5.7.4.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The size of each file does not have any impact on the number of samples
    collected. As shown in the table above, the smallest number of samples
    results from polling when there are a small number of files and a small
    number of containers. When there are a lot of small files and containers,
    the number of samples is the highest.
   </p></div></div><div class="sect3" id="meters-add"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add Resource Fields to Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#meters-add">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>meters-add</li></ul></div></div></div></div><p>
   By default, not all the resource metadata fields for an event are recorded
   and stored in ceilometer. If you want to collect metadata fields for a
   consumer application, for example, it is easier to add a field to an
   existing meter rather than creating a new meter. If you create a new meter,
   you must also reconfigure ceilometer.
  </p><div id="id-1.5.15.5.7.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Consider the following information before you add or edit a meter:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      You can add a maximum of 12 new fields.
     </p></li><li class="listitem "><p>
      Adding or editing a meter causes all non-default meters to STOP receiving
      notifications. You will need to restart ceilometer.
     </p></li><li class="listitem "><p>
      New meters added to the <code class="literal">pipeline-polling.yaml.j2</code> file
      must also be added to the
      <code class="literal">pipeline-agent-notification.yaml.j2</code> file. This is due
      to the fact that polling meters are drained by the notification agent and
      not by the collector.
     </p></li><li class="listitem "><p>
      After <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is installed, services like compute, cinder, glance, and
      neutron are configured to publish ceilometer meters by default. Other
      meters can also be enabled after the services are configured to start
      publishing the meter. The only requirement for publishing a meter is that
      the <code class="literal">origin</code> must have a value of
      <code class="literal">notification</code>. For a complete list of meters, see the
      OpenStack documentation on
      <a class="link" href="http://docs.openstack.org/admin-guide/telemetry-measurements.html" target="_blank">Measurements</a>.
     </p></li><li class="listitem "><p>
      Not all meters are supported. Meters collected by ceilometer Compute
      Agent or any agent other than ceilometer Polling are not supported or
      tested with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li><li class="listitem "><p>
      Identity meters are disabled by keystone.
     </p></li><li class="listitem "><p>
      To enable ceilometer to start collecting meters, some services require
      you enable the meters you need in the service first before enabling them
      in ceilometer. Refer to the documentation for the specific service before
      you add new meters or resource fields.
     </p></li></ul></div></div><p>
   <span class="bold"><strong>To add Resource Metadata fields:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log on to the Cloud Lifecycle Manager (deployer node).
    </p></li><li class="step "><p>
     To change to the ceilometer directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/ceilometer</pre></div></li><li class="step "><p>
     In a text editor, open the target configuration file (for example,
     monasca-field-definitions.yaml.j2).
    </p></li><li class="step "><p>
     In the metadata section, either add a new meter or edit an existing one
     provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="step "><p>
     Include the metadata fields you need. You can use the <code class="literal">instance
     meter</code> in the file as an example.
    </p></li><li class="step "><p>
     Save and close the configuration file.
    </p></li><li class="step "><p>
     To save your changes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config"</pre></div></li><li class="step "><p>
     If you added a new meter, reconfigure ceilometer:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
# To run the config-processor playbook:
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
#To run the ready-deployment playbook:
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="update-pollSwift"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the Polling Strategy and Swift Considerations</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#update-pollSwift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>update-pollSwift</li></ul></div></div></div></div><p>
   Polling can be very taxing on the system due to the sheer volume of data
   that the system may have to process. It also has a severe impact on
   queries since the database will now have a very large amount of data to scan
   to respond to the query. This consumes a great amount of cpu and memory.
   This can result in long wait times for query responses, and in extreme cases
   can result in timeouts.
  </p><p>
   There are 3 polling meters in swift:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     storage.objects
    </p></li><li class="listitem "><p>
     storage.objects.size
    </p></li><li class="listitem "><p>
     storage.objects.containers
    </p></li></ul></div><p>
   Here is an example of <code class="filename">pipeline.yml</code> in which
   swift polling is set to occur hourly.
  </p><div class="verbatim-wrap"><pre class="screen">---
      sources:
      - name: swift_source
      interval: 3600
      meters:
      - "storage.objects"
      - "storage.objects.size"
      - "storage.objects.containers"
      resources:
      discovery:
      sinks:
      - meter_sink
      sinks:
      - name: meter_sink
      transformers:
      publishers:
      - notifier://</pre></div><p>
   With this configuration above, we did not enable polling of container based
   meters and we only collect 3 messages for any given tenant, one for each
   meter listed in the configuration files. Since we have 3 messages only per
   tenant, it does not create a heavy load on the MySQL database as it would
   have if container-based meters were enabled. Hence, other APIs are not
   hit because of this data collection configuration.
  </p></div></div><div class="sect2" id="topic15050"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Setting Role-based Access Control</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#topic15050">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>topic15050</li></ul></div></div></div></div><p>
  Role Base Access Control (RBAC) is a technique that limits access to
  resources based on a specific set of roles associated with each user's
  credentials.
 </p><p>
  keystone has a set of users that are associated with each project. Each user
  has at least one role. After a user has authenticated with keystone using a
  valid set of credentials, keystone will augment that request with the Roles
  that are associated with that user. These roles are added to the Request
  Header under the X-Roles attribute and are presented as a comma-separated
  list.
 </p><div class="sect3" id="display-users"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Displaying All Users</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#display-users">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>display-users</li></ul></div></div></div></div><p>
   To discover the list of users available in the system, an administrator can
   run the following command using the keystone command-line interface:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack user list</pre></div><p>
   The output should resemble this response, which is a list of all the users
   currently available in this system.
  </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-----------------------------------------+----+
|                id                |    name      | enabled |       email        |
+----------------------------------+-----------------------------------------+----+
| 1c20d327c92a4ea8bb513894ce26f1f1 |   admin      |   True  | admin.example.com  |
| 0f48f3cc093c44b4ad969898713a0d65 | ceilometer   |   True  | nobody@example.com |
| 85ba98d27b1c4c8f97993e34fcd14f48 |   cinder     |   True  | nobody@example.com |
| d2ff982a0b6547d0921b94957db714d6 |    demo      |   True  |  demo@example.com  |
| b2d597e83664489ebd1d3c4742a04b7c |    ec2       |   True  | nobody@example.com |
| 2bd85070ceec4b608d9f1b06c6be22cb |   glance     |   True  | nobody@example.com |
| 0e9e2daebbd3464097557b87af4afa4c |    heat      |   True  | nobody@example.com |
| 0b466ddc2c0f478aa139d2a0be314467 |  neutron     |   True  | nobody@example.com |
| 5cda1a541dee4555aab88f36e5759268 |    nova      |   True  | nobody@example.com ||
| 5cda1a541dee4555aab88f36e5759268 |    nova      |   True  | nobody@example.com |
| 1cefd1361be8437d9684eb2add8bdbfa |   swift      |   True  | nobody@example.com |
| f05bac3532c44414a26c0086797dab23 | user20141203213957|True| nobody@example.com |
| 3db0588e140d4f88b0d4cc8b5ca86a0b | user20141205232231|True| nobody@example.com |
+----------------------------------+-----------------------------------------+----+</pre></div></div><div class="sect3" id="display-roles"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Displaying All Roles</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#display-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>display-roles</li></ul></div></div></div></div><p>
   To see all the roles that are currently available in the deployment, an
   administrator (someone with the admin role) can run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack role list</pre></div><p>
   The output should resemble the following response:
  </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-------------------------------------+
|                id                |                 name                |
+----------------------------------+-------------------------------------+
| 507bface531e4ac2b7019a1684df3370 |            ResellerAdmin            |
| 9fe2ff9ee4384b1894a90878d3e92bab |               member                |
| e00e9406b536470dbde2689ce1edb683 |                admin                |
| aa60501f1e664ddab72b0a9f27f96d2c |           heat_stack_user           |
| a082d27b033b4fdea37ebb2a5dc1a07b |               service               |
| 8f11f6761534407585feecb5e896922f |            swiftoperator            |
+----------------------------------+-------------------------------------+</pre></div></div><div class="sect3" id="assign-role"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assigning a Role to a User</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#assign-role">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>assign-role</li></ul></div></div></div></div><p>
   In this example, we want to add the role
   <span class="bold"><strong>ResellerAdmin</strong></span> to the demo user who has the
   ID <span class="bold"><strong>d2ff982a0b6547d0921b94957db714d6</strong></span>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Determine which Project/Tenant the user belongs to.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack user show d2ff982a0b6547d0921b94957db714d6</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
|    id               | d2ff982a0b6547d0921b94957db714d6 |
| name                | admin                            |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+</pre></div></li><li class="step "><p>
     We need to link the ResellerAdmin Role to a Project/Tenant. To start,
     determine which tenants are available on this deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-------------------------------+--+
|                id                |        name       | enabled |
+----------------------------------+-------------------------------+--+
| 4a8f4207a13444089a18dc524f41b2cf |       admin       |   True  |
| 00cbaf647bf24627b01b1a314e796138 |        demo       |   True  |
| 8374761f28df43b09b20fcd3148c4a08 |        gf1        |   True  |
| 0f8a9eef727f4011a7c709e3fbe435fa |        gf2        |   True  |
| 6eff7b888f8e470a89a113acfcca87db |        gf3        |   True  |
| f0b5d86c7769478da82cdeb180aba1b0 |        jaq1       |   True  |
| a46f1127e78744e88d6bba20d2fc6e23 |        jaq2       |   True  |
| 977b9b7f9a6b4f59aaa70e5a1f4ebf0b |        jaq3       |   True  |
| 4055962ba9e44561ab495e8d4fafa41d |        jaq4       |   True  |
| 33ec7f15476545d1980cf90b05e1b5a8 |        jaq5       |   True  |
| 9550570f8bf147b3b9451a635a1024a1 |      service      |   True  |
+----------------------------------+-------------------------------+--+</pre></div></li><li class="step "><p>
     Now that we have all the pieces, we can assign the ResellerAdmin role to
     this User on the Demo project.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138 507bface531e4ac2b7019a1684df3370</pre></div><p>
     This will produce no response if everything is correct.
    </p></li><li class="step "><p>
     Validate that the role has been assigned correctly. Pass in the user and
     tenant ID and request a list of roles assigned.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role list --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138</pre></div><p>
     Note that all members have the <span class="emphasis"><em>member</em></span> role as a
     default role in addition to any other roles that have been assigned.
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------------+----------------------------------+----------------------------------+
|                id                |      name     |             user_id              | tenant_id             |
+----------------------------------+---------------+----------------------------------+----------------------------------+
| 507bface531e4ac2b7019a1684df3370 | ResellerAdmin | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 9fe2ff9ee4384b1894a90878d3e92bab |    member     | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
+----------------------------------+---------------+----------------------------------+----------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="create-role"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a New Role</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#create-role">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>create-role</li></ul></div></div></div></div><p>
   In this example, we will create a Level 3 Support role called
   <span class="bold"><strong>L3Support</strong></span>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add the new role to the list of roles.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role create L3Support</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|    id    | 7e77946db05645c4ba56c6c82bf3f8d2 |
|   name   |            L3Support             |
+----------+----------------------------------+</pre></div></li><li class="step "><p>
     Now that we have the new role's ID, we can add that role to the Demo user
     from the previous example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add --user d2ff982a0b6547d0921b94957db714d6  --project 00cbaf647bf24627b01b1a314e796138 7e77946db05645c4ba56c6c82bf3f8d2</pre></div><p>
     This will produce no response if everything is correct.
    </p></li><li class="step "><p>
     Verify that the user Demo has both the ResellerAdmin and L3Support roles.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role list --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138</pre></div></li><li class="step "><p>
     The response should resemble the following output. Note that this user has
     the L3Support role, the ResellerAdmin role, and the default member role.
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------------+----------------------------------+----------------------------------+
|                id                |      name     |             user_id              |            tenant_id             |
+----------------------------------+---------------+----------------------------------+----------------------------------+
| 7e77946db05645c4ba56c6c82bf3f8d2 |   L3Support   | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 507bface531e4ac2b7019a1684df3370 | ResellerAdmin | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 9fe2ff9ee4384b1894a90878d3e92bab |    member     | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
+----------------------------------+---------------+----------------------------------+----------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="access-policies"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Policies</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#access-policies">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>access-policies</li></ul></div></div></div></div><p>
   Before introducing RBAC, ceilometer had very simple access control. There
   were two types of user: admins and users. Admins will be able to access any
   API and perform any operation. Users will only be able to access non-admin
   APIs and perform operations only on the Project/Tenant where they belonged.
  </p></div></div><div class="sect2" id="topic-zx2-mmd-5t"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Failover HA Support</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#topic-zx2-mmd-5t">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_failover_ha.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_failover_ha.xml</li><li><span class="ds-label">ID: </span>topic-zx2-mmd-5t</li></ul></div></div></div></div><p>
  In the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment, the ceilometer metering service supports native
  Active-Active high-availability (HA) for the notification and polling agents.
  Implementing HA support includes workload-balancing, workload-distribution
  and failover.
 </p><p>
  Tooz is the coordination engine that is used to coordinate workload among
  multiple active agent instances. It also maintains the knowledge of
  active-instance-to-handle failover and group membership using hearbeats
  (pings).
 </p><p>
  Zookeeper is used as the coordination backend. Zookeeper uses Tooz to expose
  the APIs that manage group membership and retrieve workload specific to each
  agent.
 </p><p>
  The following section in the configuration file is used to implement
  high-availability (HA):
 </p><div class="verbatim-wrap"><pre class="screen">[coordination]
backend_url = &lt;IP address of Zookeeper host: port&gt; (port is usually 2181 as a zookeeper default)
heartbeat = 1.0
check_watchers = 10.0</pre></div><p>
  For the notification agent to be configured in HA mode, additional
  configuration is needed in the configuration file:
 </p><div class="verbatim-wrap"><pre class="screen">[notification]
workload_partitioning = true</pre></div><p>
  The HA notification agent distributes workload among multiple queues that are
  created based on the number of unique source:sink combinations. The
  combinations are configured in the notification agent pipeline configuration
  file. If there are additional services to be metered using notifications,
  then the recommendation is to use a separate source for those events. This is
  recommended especially if the expected load of data from that source is
  considered high. Implementing HA support should lead to better workload
  balancing among multiple active notification agents.
 </p><p>
  ceilometer-expirer is also an Active-Active HA. Tooz is used to pick an
  expirer process that acquires a lock when there are multiple contenders and
  the winning process runs. There is no failover support, as expirer is not a
  daemon and is scheduled to run at pre-determined intervals.
 </p><div id="id-1.5.15.5.9.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   You must ensure that a single expirer process runs when multiple processes
   are scheduled to run at the same time. This must be done using cron-based
   scheduling. on multiple controller nodes
  </p></div><p>
  The following configuration is needed to enable expirer HA:
 </p><div class="verbatim-wrap"><pre class="screen">[coordination]
backend_url = &lt;IP address of Zookeeper host: port&gt; (port is usually 2181 as a zookeeper default)
heartbeat = 1.0
check_watchers = 10.0</pre></div><p>
  The notification agent HA support is mainly designed to coordinate among
  notification agents so that correlated samples can be handled by the same
  agent. This happens when samples get transformed from other samples. The
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ceilometer pipeline has no transformers, so this task of coordination
  and workload partitioning does not need to be enabled. The notification agent
  is deployed on multiple controller nodes and they distribute workload among
  themselves by randomly fetching the data from the queue.
 </p><p>
  To disable coordination and workload partitioning by OpenStack, set the
  following value in the configuration file:
 </p><div class="verbatim-wrap"><pre class="screen">        [notification]
        workload_partitioning = False</pre></div><div id="id-1.5.15.5.9.17" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   When a configuration change is made to an API running under the HA Proxy,
   that change needs to be replicated in <span class="bold"><strong>all</strong></span>
   controllers.
  </p></div></div><div class="sect2" id="Ceilo-optimize"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optimizing the Ceilometer Metering Service</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#Ceilo-optimize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>Ceilo-optimize</li></ul></div></div></div></div><p>
  You can improve ceilometer responsiveness by configuring metering to store
  only the data you are require. This topic provides strategies for getting the
  most out of metering while not overloading your resources.
 </p><div class="sect3" id="changing-meter-list"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change the List of Meters</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#changing-meter-list">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>changing-meter-list</li></ul></div></div></div></div><p>
   The list of meters can be easily reduced or increased by editing the
   pipeline.yaml file and restarting the polling agent.
  </p><p>
   Sample compute-only pipeline.yaml file with the daily poll interval:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><div id="id-1.5.15.5.10.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    This change will cause all non-default meters to stop receiving
    notifications.
   </p></div></div><div class="sect3" id="ceilometer-nova"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Nova Notifications</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilometer-nova">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>ceilometer-nova</li></ul></div></div></div></div><p>
   You can configure nova to send notifications by enabling the setting in the
   configuration file. When enabled, nova will send information to ceilometer
   related to its usage and VM status. You must restart nova for these changes
   to take effect.
  </p><p>
   The Openstack notification daemon, also known as a polling agent, monitors
   the message bus for data being provided by other OpenStack components such
   as nova. The notification daemon loads one or more listener plugins, using
   the <code class="literal">ceilometer.notification</code> namespace. Each plugin can
   listen to any topic, but by default it will listen to the
   <code class="literal">notifications.info</code> topic. The listeners grab messages off
   the defined topics and redistribute them to the appropriate plugins
   (endpoints) to be processed into Events and Samples. After the nova service
   is restarted, you should verify that the notification daemons are receiving
   traffic.
  </p><p>
   For a more in-depth look at how information is sent over
   <span class="emphasis"><em>openstack.common.rpc</em></span>, refer to the
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/measurements.html" target="_blank">OpenStack
   ceilometer documentation</a>.
  </p><p>
   nova can be configured to send following data to ceilometer:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><tbody><tr><td><span class="bold"><strong>Name</strong></span>
      </td><td><span class="bold"><strong>Unit</strong></span>
      </td><td><span class="bold"><strong>Type</strong></span>
      </td><td><span class="bold"><strong>Resource</strong></span>
      </td><td><span class="bold"><strong>Note</strong></span>
      </td></tr><tr><td>instance</td><td>g</td><td>instance</td><td> inst ID</td><td>Existence of instance</td></tr><tr><td>instance: <code class="varname">type</code>
      </td><td>g</td><td>instance</td><td> inst ID</td><td>Existence of instance of <code class="varname">type</code> (Where
                                    <code class="varname">type</code> is a valid OpenStack type.) </td></tr><tr><td>memory</td><td>g</td><td>MB</td><td> inst ID</td><td>Amount of allocated RAM. Measured in MB.</td></tr><tr><td>vcpus</td><td>g</td><td>vcpu</td><td> inst ID</td><td>Number of VCPUs</td></tr><tr><td>disk.root.size</td><td>g</td><td>GB</td><td> inst ID</td><td>Size of root disk. Measured in GB.</td></tr><tr><td>disk.ephemeral.size</td><td>g</td><td>GB</td><td> inst ID</td><td>Size of ephemeral disk. Measured in GB.</td></tr></tbody></table></div><p>
   To enable nova to publish notifications:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">nova.conf</pre></div></li><li class="listitem "><p>
     Compare the example of a working configuration file with the necessary
     changes to your configuration file. If there is anything missing in your
     file, add it, and then save the file.
    </p><div class="verbatim-wrap"><pre class="screen">notification_driver=messaging
notification_topics=notifications
notify_on_state_change=vm_and_task_state
instance_usage_audit=True
instance_usage_audit_period=hour</pre></div><div id="id-1.5.15.5.10.4.8.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The <code class="literal">instance_usage_audit_period</code> interval can be set to
      check the instance's status every hour, once a day, once a week or once a
      month. Every time the audit period elapses, nova sends a notification to
      ceilometer to record whether or not the instance is alive and running.
      Metering this statistic is critical if billing depends on usage.
     </p></div></li><li class="listitem "><p>
     To restart nova service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl restart nova-api.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-conductor.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-scheduler.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-novncproxy.service</pre></div><div id="id-1.5.15.5.10.4.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Different platforms may use their own unique command to restart
      nova-compute services. If the above command does not work, please refer
      to the documentation for your specific platform.
     </p></div></li><li class="listitem "><p>
     To verify successful launch of each process, list the service components:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host       | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | controller | internal | enabled | up    | 2014-09-16T23:54:02.000000 | -               |
| 3  | nova-scheduler   | controller | internal | enabled | up    | 2014-09-16T23:54:07.000000 | -               |
| 4  | nova-cert        | controller | internal | enabled | up    | 2014-09-16T23:54:00.000000 | -               |
| 5  | nova-compute     | compute1   | nova     | enabled | up    | 2014-09-16T23:54:06.000000 | -               |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+</pre></div></li></ol></div></div><div class="sect3" id="webserverapi"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve Reporting API Responsiveness</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#webserverapi">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>webserverapi</li></ul></div></div></div></div><p>
   Reporting APIs are the main access to the metering data stored in
   ceilometer. These APIs are accessed by horizon to provide basic usage data
   and information.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses Apache2 Web Server to provide the API access. This topic
   provides some strategies to help you optimize the front-end and back-end
   databases.
  </p><p>
   To improve the responsiveness you can increase the number of threads and
   processes in the ceilometer configuration file. Each process can have a
   certain amount of threads managing the filters and applications, which can
   comprise the processing pipeline.
  </p><p>
   <span class="bold"><strong>To configure Apache2 to use increase the number of
   threads</strong></span>, use the steps in <a class="xref" href="topic-ttn-5fg-4v.html#reconfig-metering" title="13.3.4. Configure the Ceilometer Metering Service">Section 13.3.4, “Configure the Ceilometer Metering Service”</a>
  </p><div id="id-1.5.15.5.10.5.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    The resource usage panel could take some time to load depending on the
    number of metrics selected.
   </p></div></div><div class="sect3" id="update-polling-strategy"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the Polling Strategy and Swift Considerations</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#update-polling-strategy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>update-polling-strategy</li></ul></div></div></div></div><p>
   Polling can put an excessive amount of strain on the system due to the
   amount of data the system may have to process. Polling also has a severe
   impact on queries since the database can have very large amount of data to
   scan before responding to the query. This process usually consumes a large
   amount of CPU and memory to complete the requests. Clients can also
   experience long waits for queries to come back and, in extreme cases, even
   timeout.
  </p><p>
   There are 3 polling meters in swift:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     storage.objects
    </p></li><li class="listitem "><p>
     storage.objects.size
    </p></li><li class="listitem "><p>
     storage.objects.containers
    </p></li></ul></div><p>
   Sample section of the pipeline.yaml configuration file with swift polling on
   an hourly interval:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: swift_source
      interval: 3600
      sources:
            meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><p>
   Every time the polling interval occurs, at least 3 messages per existing
   object/container in swift are collected. The following table illustrates the
   amount of data produced hourly in different scenarios:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td>swift Containers</td><td>swift Objects per container</td><td>Samples per Hour</td><td>Samples stored per 24 hours</td></tr><tr><td>10</td><td>10</td><td>500</td><td>12000</td></tr><tr><td>10</td><td>100</td><td>5000</td><td>120000</td></tr><tr><td>100</td><td>100</td><td>50000</td><td>1200000</td></tr><tr><td>100</td><td>1000</td><td>500000</td><td>12000000</td></tr></tbody></table></div><p>
   Looking at the data we can see that even a very small swift storage with 10
   containers and 100 files will store 120K samples in 24 hours, bringing it to
   a total of 3.6 million samples.
  </p><div id="id-1.5.15.5.10.6.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The file size of each file does not have any impact on the number of samples
    collected. In fact the smaller the number of containers or files, the
    smaller the sample size. In the scenario where there a large number of small
    files and containers, the sample size is also large and the performance is
    at its worst.
   </p></div></div></div><div class="sect2" id="ceilo-samples"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service Samples</span> <a title="Permalink" class="permalink" href="topic-ttn-5fg-4v.html#ceilo-samples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_samples.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_samples.xml</li><li><span class="ds-label">ID: </span>ceilo-samples</li></ul></div></div></div></div><p>
  Samples are discrete collections of a particular meter or the actual usage
  data defined by a meter description. Each sample is time-stamped and includes
  a variety of data that varies per meter but usually includes the project ID
  and UserID of the entity that consumed the resource represented by the meter
  and sample.
 </p><p>
  In a typical deployment, the number of samples can be in the tens of
  thousands if not higher for a specific collection period depending on overall
  activity.
 </p><p>
  Sample collection and data storage expiry settings are configured in
  ceilometer. Use cases that include collecting data for monthly billing cycles
  are usually stored over a period of 45 days and require a large, scalable,
  back-end database to support the large volume of samples generated by
  production OpenStack deployments.
 </p><p>
  <span class="bold"><strong>Example configuration:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">[database]
metering_time_to_live=-1</pre></div><p>
  In our example use case, to construct a complete billing record, an external
  billing application must collect all pertinent samples. Then the results must
  be sorted, summarized, and combine with the results of other types of metered
  samples that are required. This function is known as aggregation and is
  external to the ceilometer service.
 </p><p>
  Meter data, or samples, can also be collected directly from the service APIs
  by individual ceilometer polling agents. These polling agents directly access
  service usage by calling the API of each service.
 </p><p>
  OpenStack services such as swift currently only provide metered data through
  this function and some of the other OpenStack services provide specific
  metrics only through a polling action.
 </p></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="using-container-as-a-service-overview.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 14 </span>Managing Container as a Service (Magnum)</span></a><a class="nav-link" href="ops-managing-orchestration.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 12 </span>Managing Orchestration</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>