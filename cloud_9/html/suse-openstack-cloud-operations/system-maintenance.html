<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SUSE OpenStack Cloud 9 | Operations Guide CLM | System Maintenance</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="System Maintenance | SUSE OpenStack Cloud 9"/>
<meta name="description" content="This section contains the following subsections to help you manage, configure, and maintain your SUSE OpenStack Cloud cloud as well as procedures for…"/>
<meta name="product-name" content="SUSE OpenStack Cloud"/>
<meta name="product-number" content="9"/>
<meta name="book-title" content="Operations Guide CLM"/>
<meta name="chapter-title" content="Chapter 15. System Maintenance"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9"/>
<meta property="og:title" content="System Maintenance | SUSE OpenStack Cloud 9"/>
<meta property="og:description" content="This section contains the following subsections to help you manage, configure, and maintain your SUSE OpenStack Cloud cloud as well as procedures for…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="System Maintenance | SUSE OpenStack Cloud 9"/>
<meta name="twitter:description" content="This section contains the following subsections to help you manage, configure, and maintain your SUSE OpenStack Cloud cloud as well as procedures for…"/>
<link rel="prev" href="using-container-as-a-service-overview.html" title="Chapter 14. Managing Container as a Service (Magnum)"/><link rel="next" href="manage-ops-console.html" title="Chapter 16. Operations Console"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-system_maintenance.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Operations Guide CLM</a><span> / </span><a class="crumb" href="system-maintenance.html">System Maintenance</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Operations Guide CLM</div><ol><li><a href="gettingstarted-ops.html" class=" "><span class="title-number">1 </span><span class="title-name">Operations Overview</span></a></li><li><a href="tutorials.html" class=" "><span class="title-number">2 </span><span class="title-name">Tutorials</span></a></li><li><a href="clm-admin-ui.html" class=" "><span class="title-number">3 </span><span class="title-name">Cloud Lifecycle Manager Admin UI User Guide</span></a></li><li><a href="third-party-integrations.html" class=" "><span class="title-number">4 </span><span class="title-name">Third-Party Integrations</span></a></li><li><a href="ops-managing-identity.html" class=" "><span class="title-number">5 </span><span class="title-name">Managing Identity</span></a></li><li><a href="ops-managing-compute.html" class=" "><span class="title-number">6 </span><span class="title-name">Managing Compute</span></a></li><li><a href="ops-managing-esx.html" class=" "><span class="title-number">7 </span><span class="title-name">Managing ESX</span></a></li><li><a href="ops-managing-blockstorage.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Block Storage</span></a></li><li><a href="ops-managing-objectstorage.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing Object Storage</span></a></li><li><a href="ops-managing-networking.html" class=" "><span class="title-number">10 </span><span class="title-name">Managing Networking</span></a></li><li><a href="ops-managing-dashboards.html" class=" "><span class="title-number">11 </span><span class="title-name">Managing the Dashboard</span></a></li><li><a href="ops-managing-orchestration.html" class=" "><span class="title-number">12 </span><span class="title-name">Managing Orchestration</span></a></li><li><a href="topic-ttn-5fg-4v.html" class=" "><span class="title-number">13 </span><span class="title-name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li><a href="using-container-as-a-service-overview.html" class=" "><span class="title-number">14 </span><span class="title-name">Managing Container as a Service (Magnum)</span></a></li><li><a href="system-maintenance.html" class=" you-are-here"><span class="title-number">15 </span><span class="title-name">System Maintenance</span></a></li><li><a href="manage-ops-console.html" class=" "><span class="title-number">16 </span><span class="title-name">Operations Console</span></a></li><li><a href="bura-overview.html" class=" "><span class="title-number">17 </span><span class="title-name">Backup and Restore</span></a></li><li><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html" class=" "><span class="title-number">18 </span><span class="title-name">Troubleshooting Issues</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="system-maintenance" data-id-title="System Maintenance"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber">9</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">15 </span><span class="title-name">System Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-system_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section contains the following subsections to help you manage, configure,
  and maintain your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud as well as procedures
  for performing node maintenance.
 </p><section class="sect1" id="planned-maintenance" data-id-title="Planned System Maintenance"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.1 </span><span class="title-name">Planned System Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-maintenance">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-planned_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Planned maintenance tasks for your cloud. See sections below for:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="system-maintenance.html#cont-planned" title="15.1.2. Planned Control Plane Maintenance">Section 15.1.2, “Planned Control Plane Maintenance”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="system-maintenance.html#comp-planned" title="15.1.3. Planned Compute Maintenance">Section 15.1.3, “Planned Compute Maintenance”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="system-maintenance.html#planned-maintenance-task-for-networking-nodes" title="15.1.4. Planned Network Maintenance">Section 15.1.4, “Planned Network Maintenance”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="system-maintenance.html#storage-maintenance" title="15.1.5. Planned Storage Maintenance">Section 15.1.5, “Planned Storage Maintenance”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="system-maintenance.html#mariadb-manual-update" title="15.1.6. Updating MariaDB with Galera">Section 15.1.6, “Updating MariaDB with Galera”</a>
   </p></li></ul></div><section class="sect2" id="sysmn-gen" data-id-title="Whole Cloud Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.1.1 </span><span class="title-name">Whole Cloud Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#sysmn-gen">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-general_procedures.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Planned maintenance procedures for your whole cloud.
 </p><section class="sect3" id="stop-restart" data-id-title="Bringing Down Your Cloud: Services Down Method"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.1.1 </span><span class="title-name">Bringing Down Your Cloud: Services Down Method</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#stop-restart">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.5.17.3.4.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   If you have a planned maintenance and need to bring down your entire cloud,
   update and reboot all nodes in the cloud one by one. Start with the deployer
   node, then follow the order recommended in <a class="xref" href="system-maintenance.html#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>. This method will bring down all of your services.
  </p></div><p>
  If you wish to use a method utilizing rolling reboots where your cloud
  services will continue running then see <a class="xref" href="system-maintenance.html#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>.
 </p><p>
  To perform backups prior to these steps, visit the backup and
  restore pages first at <a class="xref" href="bura-overview.html" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
 </p><section class="sect4" id="idg-all-operations-maintenance-reboot-cloud-down-xml-7" data-id-title="Gracefully Bringing Down and Restarting Your Cloud Environment"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.1.1.1 </span><span class="title-name">Gracefully Bringing Down and Restarting Your Cloud Environment</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-down-xml-7">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You will do the following steps from your Cloud Lifecycle Manager.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Gracefully shut down your cloud by running the
     <code class="literal">ardana-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml</pre></div></li><li class="step"><p>
     Shut down and restart your nodes. There are multiple ways you can do this:
    </p><ol type="a" class="substeps"><li class="step"><p>
       You can SSH to each node and use <code class="literal">sudo reboot -f</code> to
       reboot the node. Reboot the control plane nodes first so that they
       become functional as early as possible.
      </p></li><li class="step"><p>
       You can shut down the nodes and then physically restart them either via a
       power button or the IPMI. If your cloud data model
       <code class="filename">servers.yml</code> specifies iLO connectivity for all
       nodes, then you can use the <code class="literal">bm-power-down.yml</code> and
       <code class="literal">bm-power-up.yml</code> playbooks on the Cloud Lifecycle Manager.
      </p><p>
       Power down the control plane nodes last so that they remain online as
       long as possible, and power them back up before other nodes to restore
       their services quickly.
      </p></li></ol></li><li class="step"><p>
     Perform the necessary maintenance.
    </p></li><li class="step"><p>
     After the maintenance is complete, power your Cloud Lifecycle Manager back up
     and then SSH to it.
    </p></li><li class="step"><p>
     Determine the current power status of the nodes in your environment:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-status.yml</pre></div></li><li class="step"><p>
     If necessary, power up any nodes that are not already powered up, ensuring
     that you power up your controller nodes first. You can target specific
     nodes with the <code class="literal">-e nodelist=&lt;node_name&gt;</code> switch.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-up.yml [-e nodelist=&lt;node_name&gt;]</pre></div><div id="id-1.5.17.3.4.3.5.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     Obtain the <code class="literal">&lt;node_name&gt;</code> by using the
     <code class="command">sudo cobbler system list</code> command from the Cloud Lifecycle Manager.
    </p></div></li><li class="step"><p>
     Bring the databases back up:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step"><p>
     Gracefully bring up your cloud services by running the
     <code class="literal">ardana-start.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml</pre></div></li><li class="step"><p>
     Pause for a few minutes and give the cloud environment time to come up
     completely and then verify the status of the individual services using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li><li class="step"><p>
     If any services did not start properly, you can run playbooks for the
     specific services having issues.
    </p><p>
     For example:
    </p><p>
     If RabbitMQ fails, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-start.yml</pre></div><p>
     You can check the status of RabbitMQ afterwards with this:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div><p>
     If the recovery had failed, you can run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</pre></div><p>
     Each of the other services have playbooks in the
     <code class="literal">~/scratch/ansible/next/ardana/ansible</code> directory in the
     format of <code class="literal">&lt;service&gt;-start.yml</code> that you can run.
     One example, for the compute service, is
     <code class="literal">nova-start.yml</code>.
    </p></li><li class="step"><p>
     Continue checking the status of your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 cloud services until
     there are no more failed or unreachable nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></section></section><section class="sect3" id="rebootNodes" data-id-title="Rolling Reboot of the Cloud"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.1.2 </span><span class="title-name">Rolling Reboot of the Cloud</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#rebootNodes">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If you have a planned maintenance and need to bring down your entire cloud
  and restart services while minimizing downtime, follow the steps here to
  safely restart your cloud. If you do not mind your services being down, then
  another option for planned maintenance can be found at
  <a class="xref" href="system-maintenance.html#stop-restart" title="15.1.1.1. Bringing Down Your Cloud: Services Down Method">Section 15.1.1.1, “Bringing Down Your Cloud: Services Down Method”</a>.
 </p><section class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-5" data-id-title="Recommended node reboot order"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.1.2.1 </span><span class="title-name">Recommended node reboot order</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-5">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To ensure that rebooted nodes reintegrate into the cluster, the key is
   having enough time between controller reboots.
  </p><p>
   The recommended way to achieve this is as follows:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Reboot controller nodes one-by-one with a suitable interval in between. If
     you alternate between controllers and compute nodes you will gain more
     time between the controller reboots.
    </p></li><li class="step"><p>
     Reboot of compute nodes (if present in your cloud).
    </p></li><li class="step"><p>
     Reboot of swift nodes (if present in your cloud).
    </p></li><li class="step"><p>
     Reboot of ESX nodes (if present in your cloud).
    </p></li></ol></div></div></section><section class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-7" data-id-title="Rebooting controller nodes"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.1.2.2 </span><span class="title-name">Rebooting controller nodes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-7">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Turn off the keystone Fernet Token-Signing Key
   Rotation</strong></span>
  </p><p>
   Before rebooting any controller node, you need to ensure that the keystone
   Fernet token-signing key rotation is turned off. Run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-stop-fernet-auto-rotation.yml</pre></div><p>
   <span class="bold"><strong>Migrate singleton services first</strong></span>
  </p><div id="id-1.5.17.3.4.4.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    If you have previously rebooted your Cloud Lifecycle Manager for any reason, ensure that
    the <code class="systemitem">apache2</code> service is running before
    continuing. To start the <code class="systemitem">apache2</code> service, use
    this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl start apache2</pre></div></div><p>
   The first consideration before rebooting any controller nodes is that there
   are a few services that run as singletons (non-HA), thus they will be
   unavailable while the controller they run on is down. Typically this is a
   very small window, but if you want to retain the service during the reboot
   of that server you should take special action to maintain service, such as
   migrating the service.
  </p><p>
   For these steps, if your singleton services are running on controller1 and
   you move them to controller2, then ensure you move them back to controller1
   before proceeding to reboot controller2.
  </p><p>
   <span class="bold"><strong>For the <code class="literal">cinder-volume</code> singleton
   service:</strong></span>
  </p><p>
   Execute the following command on each controller node to determine which
   node is hosting the cinder-volume singleton. It should be running on only
   one node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ps auxww | grep cinder-volume | grep -v grep</pre></div><p>
   Run the <code class="literal">cinder-migrate-volume.yml</code> playbook - details
   about the cinder volume and backup migration instructions can be found in
   <a class="xref" href="ops-managing-blockstorage.html#sec-operation-manage-block-storage" title="8.1.3. Managing cinder Volume and Backup Services">Section 8.1.3, “Managing cinder Volume and Backup Services”</a>.
  </p><p>
   <span class="bold"><strong>For the SNAT namespace singleton service:</strong></span>
  </p><p>
   If you reboot the controller node hosting the SNAT namespace service on it,
   Compute instances without floating IPs will lose network connectivity when
   that controller is rebooted. To prevent this from happening, you can use
   these steps to determine which controller node is hosting the SNAT namespace
   service and migrate it to one of the other controller nodes while that node
   is rebooted.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Locate the SNAT node where the router is providing the active
     <code class="literal">snat_service</code>:
    </p><ol type="a" class="substeps"><li class="step"><p>
       From the Cloud Lifecycle Manager, list out your ports to determine which port
       is serving as the router gateway:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack port list --device_owner network:router_gateway</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack port list --device_owner network:router_gateway
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                           |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| 287746e6-7d82-4b2c-914c-191954eba342 |      | fa:16:3e:2e:26:ac | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"} |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+</pre></div></li><li class="step"><p>
       Look at the details of this port to determine what the
       <code class="literal">binding:host_id</code> value is, which will point to the
       host in which the port is bound to:
      </p><div class="verbatim-wrap"><pre class="screen">openstack port show &lt;port_id&gt;</pre></div><p>
       Example, with the value you need in bold:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m2-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div><p>
       In this example, the <code class="literal">ardana-cp1-c1-m2-mgmt</code> is the
       node hosting the SNAT namespace service.
      </p></li></ol></li><li class="step"><p>
     SSH to the node hosting the SNAT namespace service and check the SNAT
     namespace, specifying the router_id that has the interface with the subnet
     that you are interested in:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh &lt;IP_of_SNAT_namespace_host&gt;
<code class="prompt user">ardana &gt; </code>sudo ip netns exec snat-&lt;router_ID&gt; bash</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip netns exec snat-e122ea3f-90c5-4662-bf4a-3889f677aacf bash</pre></div></li><li class="step"><p>
     Obtain the ID for the L3 Agent for the node hosting your SNAT namespace:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack network agent list</pre></div><p>
     Example, with the entry you need given the examples above:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 0126bbbf-5758-4fd0-84a8-7af4d93614b8 | DHCP agent           | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| 33dec174-3602-41d5-b7f8-a25fd8ff6341 | Metadata agent       | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-metadata-agent    |
| 3bc28451-c895-437b-999d-fdcff259b016 | L3 agent             | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 4af1a941-61c1-4e74-9ec1-961cebd6097b | L3 agent             | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-l3-agent          |
| 65bcb3a0-4039-4d9d-911c-5bb790953297 | Open vSwitch agent   | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| 6981c0e5-5314-4ccd-bbad-98ace7db7784 | L3 agent             | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 7df9fa0b-5f41-411f-a532-591e6db04ff1 | Metadata agent       | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-metadata-agent    |
| 92880ab4-b47c-436c-976a-a605daa8779a | Metadata agent       | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-metadata-agent    |
<span class="bold"><strong>| a209c67d-c00f-4a00-b31c-0db30e9ec661 | L3 agent             | ardana-cp1-c1-m2-mgmt</strong></span>    | :-)   | True           | neutron-vpn-agent         |
| a9467f7e-ec62-4134-826f-366292c1f2d0 | DHCP agent           | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| b13350df-f61d-40ec-b0a3-c7c647e60f75 | Open vSwitch agent   | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| d4c07683-e8b0-4a2b-9d31-b5b0107b0b31 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-openvswitch-agent |
| e91d7f3f-147f-4ad2-8751-837b936801e3 | Open vSwitch agent   | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| f33015c8-f4e4-4505-b19b-5a1915b6e22a | DHCP agent           | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| fe43c0e9-f1db-4b67-a474-77936f7acebf | Metadata agent       | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-metadata-agent    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+</pre></div></li><li class="step"><p>
     Also obtain the ID for the L3 Agent of the node you are going to move the
     SNAT namespace service to using the same commands as the previous step.
    </p></li><li class="step"><p>
     Use these commands to move the SNAT namespace service, with the
     <code class="literal">router_id</code> being the same value as the ID for router:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Remove the L3 Agent for the old host:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent remove router –agent-type l3
&lt;agent_id_of_snat_namespace_host&gt; \
&lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent remove router –agent-type l3
a209c67d-c00f-4a00-b31c-0db30e9ec661 \
e122ea3f-90c5-4662-bf4a-3889f677aacf
Removed router e122ea3f-90c5-4662-bf4a-3889f677aacf from L3 agent</pre></div></li><li class="step"><p>
       Remove the SNAT namespace:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip netns delete snat-&lt;router_id&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip netns delete snat-e122ea3f-90c5-4662-bf4a-3889f677aacf</pre></div></li><li class="step"><p>
       Create a new L3 Agent for the new host:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent add router –agent-type l3
&lt;agent_id_of_new_snat_namespace_host&gt; \
&lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent add router –agent-type l3
3bc28451-c895-437b-999d-fdcff259b016 \
e122ea3f-90c5-4662-bf4a-3889f677aacf
Added router e122ea3f-90c5-4662-bf4a-3889f677aacf to L3 agent</pre></div></li></ol><p>
     Confirm that it has been moved by listing the details of your port from step
     1b above, noting the value of <code class="literal">binding:host_id</code> which
     should be updated to the host you moved your SNAT namespace to:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show &lt;port_ID&gt;</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m1-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Reboot the controllers</strong></span>
  </p><p>
   In order to reboot the controller nodes, you must first retrieve a list of
   nodes in your cloud running control plane services.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>for i in $(grep -w cluster-prefix
~/openstack/my_cloud/definition/data/control_plane.yml \
| awk '{print $2}'); do grep $i
~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts \
| grep ansible_ssh_host | awk '{print $1}'; done</pre></div><p>
   Then perform the following steps from your Cloud Lifecycle Manager for each of
   your controller nodes:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     If any singleton services are active on this node, they will be
     unavailable while the node is down. If you want to retain the service
     during the reboot, you should take special action to maintain the service,
     such as migrating the service as appropriate as noted above.
    </p></li><li class="step"><p>
     Stop all services on the controller node that you are rebooting first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit \
&lt;controller node&gt;</pre></div></li><li class="step"><p>
     Reboot the controller node, e.g. run the following command on the
     controller itself:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo reboot</pre></div><p>
     Note that the current node being rebooted could be hosting the lifecycle
     manager.
    </p></li><li class="step"><p>
     Wait for the controller node to become ssh-able and allow an additional
     minimum of five minutes for the controller node to settle. Start all
     services on the controller node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml \
--limit &lt;controller node&gt;</pre></div></li><li class="step"><p>
     Verify that the status of all services on that is OK on the controller
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml \
--limit &lt;controller node&gt;</pre></div></li><li class="step"><p>
     When above start operation has completed successfully, you may proceed to
     the next controller node. Ensure that you migrate your singleton services
     off the node first.
    </p></li></ol></div></div><div id="id-1.5.17.3.4.4.4.21" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    It is important that you not begin the reboot procedure for a new
    controller node until the reboot of the previous controller node has been
    completed successfully (that is, the ardana-status playbook has completed
    without error).
   </p></div><p>
   <span class="bold"><strong>
    Reenable the keystone Fernet Token-Signing Key Rotation
   </strong></span>
  </p><p>
   After all the controller nodes are successfully updated and back online, you
   need to re-enable the keystone Fernet token-signing key rotation job by
   running the <code class="filename">keystone-reconfigure.yml</code> playbook. On the
   deployer, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></section><section class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-9" data-id-title="Rebooting compute nodes"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.1.2.3 </span><span class="title-name">Rebooting compute nodes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-9">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To reboot a compute node the following operations will need to be performed:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Disable provisioning of the node to take the node offline to prevent
     further instances being scheduled to the node during the reboot.
    </p></li><li class="listitem"><p>
     Identify instances that exist on the compute node, and then either:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Live migrate the instances off the node before actioning the reboot. OR
      </p></li><li class="listitem"><p>
       Stop the instances
      </p></li></ul></div></li><li class="listitem"><p>
     Reboot the node
    </p></li><li class="listitem"><p>
     Restart the nova services
    </p></li></ul></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Disable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set --disable --disable-reason "<em class="replaceable">DESCRIBE REASON</em>" compute nova-compute</pre></div><p>
     If the node has existing instances running on it these instances will need
     to be migrated or stopped prior to re-booting the node.
    </p></li><li class="step"><p>
     Live migrate existing instances. Identify the instances on the compute
     node. Note: The following command must be run with nova admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="step"><p>
     Migrate or Stop the instances on the compute node.
    </p><p>
     Migrate the instances off the node by running one of the following
     commands for each of the instances:
    </p><p>
     If your instance is booted from a volume and has any number of cinder
     volume attached, use the nova live-migration command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only, you can use the
     --block-migrate option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     Note: The [&lt;target compute host&gt;] option is optional. If you do not
     specify a target host then the nova scheduler will choose a node for you.
    </p><p>
     OR
    </p><p>
     Stop the instances on the node by running the following command for each
     of the instances:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server stop &lt;instance-uuid&gt;</pre></div></li><li class="step"><p>
     Stop all services on the Compute node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;compute node&gt;</pre></div></li><li class="step"><p>
     SSH to your Compute nodes and reboot them:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo reboot</pre></div><p>
     The operating system cleanly shuts down services and then automatically
     reboots. If you want to be very thorough, run your backup jobs just before
     you reboot.
    </p></li><li class="step"><p>
     Run the ardana-start.yml playbook from the Cloud Lifecycle Manager. If needed, use
     the bm-power-up.yml playbook to restart the node. Specify just the node(s)
     you want to start in the 'nodelist' parameter arguments, that is,
     nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node&gt;</pre></div></li><li class="step"><p>
     Execute the <span class="bold"><strong>ardana-start.yml </strong></span>playbook.
     Specifying the node(s) you want to start in the 'limit' parameter
     arguments. This parameter accepts wildcard arguments and also
     '@&lt;filename&gt;' to process all hosts listed in the file.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;compute node&gt;</pre></div></li><li class="step"><p>
     Re-enable provisioning on the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set --enable compute nova-compute</pre></div></li><li class="step"><p>
     Restart any instances you stopped.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server start &lt;instance-uuid&gt;</pre></div></li></ol></div></div></section><section class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-10" data-id-title="Rebooting swift nodes"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.1.2.4 </span><span class="title-name">Rebooting swift nodes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-10">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If your swift services are on controller node, please follow the controller
   node reboot instructions above.
  </p><p>
   For a dedicated swift PAC cluster or swift Object resource node:
  </p><p>
   For each swift host
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Stop all services on the swift node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;swift node&gt;</pre></div></li><li class="step"><p>
     Reboot the swift node by running the following command on the swift node
     itself:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo reboot</pre></div></li><li class="step"><p>
     Wait for the node to become ssh-able and then start all services on the
     swift node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;swift node&gt;</pre></div></li></ol></div></div></section><section class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-14" data-id-title="Get list of status playbooks"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.1.2.5 </span><span class="title-name">Get list of status playbooks</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-reboot-cloud-rolling-xml-14">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following command will display a list of status playbooks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ls *status*</pre></div></section></section></section><section class="sect2" id="cont-planned" data-id-title="Planned Control Plane Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.1.2 </span><span class="title-name">Planned Control Plane Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#cont-planned">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-cont_planned.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Planned maintenance tasks for controller nodes such as full cloud reboots and
  replacing controller nodes.
 </p><section class="sect3" id="replacing-controller" data-id-title="Replacing a Controller Node"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.2.1 </span><span class="title-name">Replacing a Controller Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#replacing-controller">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-replace_controller.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This section outlines steps for replacing a controller node in your
  environment.
 </p><p>
  For <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you must have three controller nodes.
  Therefore, adding or removing nodes is not an option. However, if you need to
  repair or replace a controller node, you may do so by following the steps
  outlined here. Note that to run any playbooks whatsoever for cloud
  maintenance, you will always run the steps from the Cloud Lifecycle Manager.
 </p><p>
  These steps will depend on whether you need to replace a shared lifecycle
  manager/controller node or whether this is a standalone controller node.
 </p><p>Keep in mind while performing the following tasks:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Do not add entries for a new server. Instead, update the entries for
    the broken one.
   </p></li><li class="listitem"><p>
    Be aware that all management commands are run on the node where the
    Cloud Lifecycle Manager is running.
   </p></li></ul></div><section class="sect4" id="replace-shared-lm" data-id-title="Replacing a Shared Cloud Lifecycle Manager/Controller Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.2.1.1 </span><span class="title-name">Replacing a Shared Cloud Lifecycle Manager/Controller Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace-shared-lm">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-replace_shared_lm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If the controller node you need to replace was also being used as your
  Cloud Lifecycle Manager then use these steps below. If this is not a shared
  controller, skip to the next section.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that you
    previously had loaded on your Cloud Lifecycle Manager, you will need to download and install
    the lifecycle management software using the instructions from the
    installation guide:
   </p><p>
    <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span>
   </p></li><li class="step"><p>
    Initialize the Cloud Lifecycle Manager platform by running <code class="command">ardana-init</code>.
   </p></li><li class="step"><p>
    To restore your data, see <a class="xref" href="system-maintenance.html#pit-lifecyclemanager-recovery" title="15.2.3.2.3. Point-in-time Cloud Lifecycle Manager Recovery">Section 15.2.3.2.3, “Point-in-time Cloud Lifecycle Manager Recovery”</a>.
    At this time, restore only the backup of <code class="systemitem">ardana</code>
    files on the system into <code class="filename">/var/lib/ardana</code> (the user's
    home directory.)
   </p></li><li class="step"><p>
    On the new node, update your cloud model with the new
    <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields to
    reflect the attributes of the node. Do not change the
    <code class="literal">id</code>, <code class="literal">ip-addr</code>, <code class="literal">role</code>,
    or <code class="literal">server-group</code> settings.
   </p><div id="id-1.5.17.3.5.3.7.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div></li><li class="step"><p>
    Open the <code class="filename">servers.yml</code> file describing your cloud nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git -C ~/openstack checkout site
<code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>vi servers.yml</pre></div><p>
    Change, as necessary, the <code class="literal">mac-addr</code>,
    <code class="literal">ilo-ip</code>, <code class="literal">ilo-password</code>, and
    <code class="literal">ilo-user</code> fields of the existing controller node. Save
    and commit the change:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "repaired node X"</pre></div></li><li class="step"><p>
    Run the configuration processor and ready-deployment playbooks as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
    Run the <code class="filename">wipe_disks.yml</code> playbook to ensure
    all non-OS partitions on the new node are completely wiped prior to
    continuing with the installation. (The value to be used for
    <code class="literal">hostname</code> is the host's identifier from
    <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit <em class="replaceable">deployer_node_name</em></pre></div><p>
    The value for <em class="replaceable">deployer_node_name</em> should be the
    name identifying the deployer/controller being initialized as it is
    represented in the <code class="filename">hosts/verb_hosts</code> file.
   </p></li><li class="step"><p>
    Deploy Cobbler:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
    Refer again to <a class="xref" href="system-maintenance.html#pit-lifecyclemanager-recovery" title="15.2.3.2.3. Point-in-time Cloud Lifecycle Manager Recovery">Section 15.2.3.2.3, “Point-in-time Cloud Lifecycle Manager Recovery”</a> and proceed
    to restore all remaining backups, with the exclusion of
    <code class="systemitem">/var/lib/ardana</code> (which was done earlier) and the
    cobbler content in <code class="systemitem">/var/lib/cobbler</code> and
    <code class="systemitem">/srv/www/cobbler</code>.
   </p></li><li class="step"><p>
    Install the software on your new Cloud Lifecycle Manager/controller node with these playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml \
-e rebuild=True --limit <em class="replaceable">deployer_node_name</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml \
-e rebuild=True --limit <em class="replaceable">deployer_node_name</em>,localhost
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tempest-deploy.yml</pre></div><div id="id-1.5.17.3.5.3.7.3.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     If you receive the message <code class="literal">stderr: Error:
     mnesia_not_running</code> when running the
     <code class="filename">ardana-deploy.yml</code> playbook, it is likely due to one
     of the following conditions:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       RabbitMQ was not running on the clustered node
      </p></li><li class="listitem"><p>
       The old node was not removed from the cluster
      </p></li></ul></div><p>
     Correct this problem with the following steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Of the remaining clustered nodes (<em class="replaceable">M2</em> and
       <em class="replaceable">M3</em>), <em class="replaceable">M2</em> is the new
       master. Make sure the application has started and M1 is no longer a
       member. On the <em class="replaceable">M2</em> node, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl start_app; \
<code class="prompt user">ardana &gt; </code>sudo rabbitmqctl forget_cluster_node rabbit@<em class="replaceable">M1</em></pre></div><p>
       Check that <em class="replaceable">M1</em> is no longer a member of the cluster.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl cluster_status</pre></div></li><li class="step"><p>
       On the newly installed node, <em class="replaceable">M1</em>, make sure
       RabbitMQ has stopped. On <em class="replaceable">M1</em>, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl stop_app</pre></div></li><li class="step"><p>
       Re-run the <code class="filename">ardana-deploy.yml</code> playbook as before.
      </p></li></ol></div></div></div></li><li class="step"><p>
    During the replacement of the node, alarms will show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></section><section class="sect4" id="replace-dedicated-lm" data-id-title="Replacing a Standalone Controller Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.2.1.2 </span><span class="title-name">Replacing a Standalone Controller Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace-dedicated-lm">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-replace_dedicated_lm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If the controller node you need to replace is not also being used as the
  Cloud Lifecycle Manager, follow the steps below.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step"><p>
    Update your cloud model, specifically the <code class="literal">servers.yml</code>
    file, with the new <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields
    where these have changed. Do not change the <code class="literal">id</code>,
    <code class="literal">ip-addr</code>, <code class="literal">role</code>, or
    <code class="literal">server-group</code> settings.
   </p></li><li class="step"><p>
    Commit your configuration to the <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>, as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step"><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
    Remove the old controller node(s) from Cobbler. You can list out the
    systems in Cobbler currently with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div><p>
    and then remove the old controller nodes with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name &lt;node&gt;</pre></div></li><li class="step"><p>
    Remove the SSH key of the old controller node from the known hosts file.
    You will specify the <code class="literal">ip-addr</code> value:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh-keygen -f "~/.ssh/known_hosts" -R &lt;ip_addr&gt;</pre></div><p>
    You should see a response similar to this one:
   </p><div class="verbatim-wrap"><pre class="screen">ardana@ardana-cp1-c1-m1-mgmt:~/openstack/ardana/ansible$ ssh-keygen -f "~/.ssh/known_hosts" -R 10.13.111.135
# Host 10.13.111.135 found: line 6 type ECDSA
~/.ssh/known_hosts updated.
Original contents retained as ~/.ssh/known_hosts.old</pre></div></li><li class="step"><p>
    Run the cobbler-deploy playbook to add the new controller node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
    Image the new node(s) by using the bm-reimage playbook. You will specify
    the name for the node in Cobbler in the command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node-name&gt;</pre></div><div id="id-1.5.17.3.5.3.8.3.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     You must ensure that the old controller node is powered off before
     completing this step. This is because the new controller node will re-use
     the original IP address.
    </p></div></li><li class="step"><p>
    Run the <code class="filename">wipe_disks.yml</code> playbook to ensure
    all non-OS partitions on the new node are completely wiped prior to
    continuing with the installation. (The value to be used for
    <code class="literal">hostname</code> is the host's identifier from
    <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="step"><p>
    Run osconfig on the replacement controller node. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;</pre></div></li><li class="step"><p>
    If the controller being replaced is the swift ring builder (see
    <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a>) you need to restore the swift ring
    builder files to the <code class="literal">/etc/swiftlm/<em class="replaceable">CLOUD-NAME</em>/<em class="replaceable">CONTROL-PLANE-NAME</em>/builder_dir</code> directory.
    See <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a> for details.
   </p></li><li class="step"><p>
    Run the ardana-deploy playbook on the replacement controller.
   </p><p>
    If the node being replaced is the swift ring builder server then you only
    need to use the <code class="literal">--limit</code> switch for that node, otherwise
    you need to specify the hostname of your swift ringer builder server and
    the hostname of the node being replaced.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True
--limit=&lt;controller-hostname&gt;,&lt;swift-ring-builder-hostname&gt;</pre></div><div id="id-1.5.17.3.5.3.8.3.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     If you receive a keystone failure when running this playbook, it is
     likely due to Fernet keys being out of sync. This problem can be corrected
     by running the <code class="filename">keystone-reconfigure.yml</code> playbook to
     re-sync the Fernet keys.
    </p><p>
     In this situation, do not use the <code class="literal">--limit</code> option when
     running <code class="filename">keystone-reconfigure.yml</code>. In order to re-sync
     Fernet keys, all the controller nodes must be in the play.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></div><div id="id-1.5.17.3.5.3.8.3.13.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
     If you receive a RabbitMQ failure when running this playbook, review
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#recoverrabbit" title="18.2.1. Understanding and Recovering RabbitMQ after Failure">Section 18.2.1, “Understanding and Recovering RabbitMQ after Failure”</a> for how to resolve the issue and then
     re-run the ardana-deploy playbook.
    </p></div></li><li class="step"><p>
    During the replacement of the node there will be alarms that show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></section></section></section><section class="sect2" id="comp-planned" data-id-title="Planned Compute Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.1.3 </span><span class="title-name">Planned Compute Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#comp-planned">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-comp_planned.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Planned maintenance tasks for compute nodes.
 </p><section class="sect3" id="planned-computenode" data-id-title="Planned Maintenance for a Compute Node"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.3.1 </span><span class="title-name">Planned Maintenance for a Compute Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-computenode">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If one or more of your compute nodes needs hardware maintenance and you can
  schedule a planned maintenance then this procedure should be followed.
 </p><section class="sect4" id="id-1.5.17.3.6.3.3" data-id-title="Performing planned maintenance on a compute node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.1.1 </span><span class="title-name">Performing planned maintenance on a compute node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.6.3.3">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you have planned maintenance to perform on a compute node, you have to
   take it offline, repair it, and restart it. To do so, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step"><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen">openstack host list | grep compute</pre></div><p>
     The following example shows two compute nodes:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack host list | grep compute
| ardana-cp1-comp0001-mgmt | compute     | AZ1      |
| ardana-cp1-comp0002-mgmt | compute     | AZ2      |</pre></div></li><li class="step"><p>
     Disable provisioning on the compute node, which will prevent additional
     instances from being spawned on it:
    </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set –disable --reason "Maintenance mode" &lt;hostname&gt;</pre></div><div id="id-1.5.17.3.6.3.3.3.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Make sure you re-enable provisioning after the maintenance is complete
      if you want to continue to be able to spawn instances on the node. You
      can do this with the command:
     </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set –enable &lt;hostname&gt;</pre></div></div></li><li class="step"><p>
     At this point you have two choices:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       <span class="bold"><strong>Live migration</strong></span>: This option enables you
       to migrate the instances off the compute node with minimal downtime so
       you can perform the maintenance without risk of losing data.
      </p></li><li class="listitem"><p>
       <span class="bold"><strong>Stop/start the instances</strong></span>: Issuing
       <code class="literal">openstack server stop</code> commands to each of the
       instances will halt them. This option lets you do maintenance and then
       start the instances back up, as long as no disk failures occur on the
       compute node data disks. This method involves downtime for the length of
       the maintenance.
      </p></li></ol></div><p>
     If you choose the live migration route, See
     <a class="xref" href="system-maintenance.html#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a> for more details. Skip to step #6
     after you finish live migration.
    </p><p>
     If you choose the stop start method, continue on.
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       List all of the instances on the node so you can issue stop commands to
       them:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem"><p>
       Issue the <code class="literal">openstack server stop</code> command against each of the
       instances:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server stop &lt;instance uuid&gt;</pre></div></li><li class="listitem"><p>
       Confirm that the instances are stopped. If stoppage was successful you
       should see the instances in a <code class="literal">SHUTOFF</code> state, as shown
       here:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack server list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status  | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>SHUTOFF</strong></span> | -          | Shutdown    | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+</pre></div></li><li class="listitem"><p>
       Do your required maintenance. If this maintenance does not take down the
       disks completely then you should be able to list the instances again
       after the repair and confirm that they are still in their
       <code class="literal">SHUTOFF</code> state:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem"><p>
       Start the instances back up using this command:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server start &lt;instance uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack server start ef31c453-f046-4355-9bd3-11e774b1772f
Request to start server ef31c453-f046-4355-9bd3-11e774b1772f has been accepted.</pre></div></li><li class="listitem"><p>
       Confirm that the instances started back up. If restarting is successful
       you should see the instances in an <code class="literal">ACTIVE</code> state, as
       shown here:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack server list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>ACTIVE</strong></span> | -          | Running     | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="listitem"><p>
       If the <code class="literal">openstack server start</code> fails, you can try doing a hard
       reboot:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server reboot --hard &lt;instance uuid&gt;</pre></div><p>
       If this does not resolve the issue you may want to contact support.
      </p></li></ol></div></li><li class="step"><p>
     Re-enable provisioning when the node is fixed:
    </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set –enable &lt;hostname&gt;</pre></div></li></ol></div></div></section></section><section class="sect3" id="reboot-computenode" data-id-title="Rebooting a Compute Node"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.3.2 </span><span class="title-name">Rebooting a Compute Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#reboot-computenode">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-reboot_computenode.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If all you need to do is reboot a Compute node, the following steps can be
  used.
 </p><p>
  You can choose to live migrate all Compute instances off the node prior to
  the reboot. Any instances that remain will be restarted when the node is
  rebooted. This playbook will ensure that all services on the Compute node are
  restarted properly.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step"><p>
    Reboot the Compute node(s) with the following playbook.
   </p><p>
    You can specify either single or multiple Compute nodes using the
    <code class="literal">--limit</code> switch.
   </p><p>
    An optional reboot wait time can also be specified. If no reboot wait time
    is specified it will default to 300 seconds.
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-compute-reboot.yml --limit [compute_node_or_list] [-e nova_reboot_wait_timeout=(seconds)]</pre></div><div id="id-1.5.17.3.6.4.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     If the Compute node fails to reboot, you should troubleshoot this issue
     separately as this playbook will not attempt to recover after a failed
     reboot.
    </p></div></li></ol></div></div></section><section class="sect3" id="liveInstMigration" data-id-title="Live Migration of Instances"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.3.3 </span><span class="title-name">Live Migration of Instances</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#liveInstMigration">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Live migration allows you to move active compute instances between compute
  nodes, allowing for less downtime during maintenance.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nova offers a set of commands that allow you to move compute
  instances between compute hosts. Which command you use will depend on the
  state of the host, what operating system is on the host, what type of storage
  the instances are using, and whether you want to migrate a single instance or
  all of the instances off of the host. We will describe these options on this
  page as well as give you step-by-step instructions for performing them.
 </p><section class="sect4" id="options" data-id-title="Migration Options"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.3.1 </span><span class="title-name">Migration Options</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#options">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>If your compute node has failed</strong></span>
  </p><p>
   A compute host failure could be caused by hardware failure, such as the data
   disk needing to be replaced, power has been lost, or any other type of
   failure which requires that you replace the baremetal host. In this
   scenario, the instances on the compute node are unrecoverable and any data
   on the local ephemeral storage is lost. If you are utilizing block storage
   volumes, either as a boot device or as additional storage, they should be
   unaffected.
  </p><p>
   In these cases you will want to use one of the nova evacuate commands, which
   will cause nova to rebuild the instances on other hosts.
  </p><p>
   This table describes each of the evacuate options for failed compute nodes:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Command</th><th style="border-bottom: 1px solid ; ">Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">nova evacuate &lt;instance&gt; &lt;hostname&gt;</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        This command is used to evacuate a single instance from a failed host.
        You specify the compute instance UUID and the target host you want to
        evacuate it to. If no host is specified then the nova scheduler will
        choose one for you.
       </p>
       <p>
        See <code class="literal">nova help evacuate</code> for more information and
        syntax. Further details can also be seen in the OpenStack documentation
        at
        <a class="link" href="http://docs.openstack.org/admin-guide/cli_nova_evacuate.html" target="_blank">http://docs.openstack.org/admin-guide/cli_nova_evacuate.html</a>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        <code class="literal">nova host-evacuate &lt;hostname&gt; --target_host
        &lt;target_hostname&gt;</code>
       </p>
      </td><td>
       <p>
        This command is used to evacuate all instances from a failed host. You
        specify the hostname of the compute host you want to evacuate.
        Optionally you can specify a target host. If no target host is
        specified then the nova scheduler will choose a target host for each
        instance.
       </p>
       <p>
        See <code class="literal">nova help host-evacuate</code> for more information and
        syntax.
       </p>
      </td></tr></tbody></table></div><p>
   If your compute host is active, powered on and the data disks are in working order
   you can utilize the migration commands to migrate your compute instances.
   There are two migration features, "cold" migration (also referred to simply
   as "migration") and live migration. Migration and live migration are two
   different functions.
  </p><p>
   <span class="bold"><strong>Cold migration</strong></span> is used to copy an instances
   data in a <code class="literal">SHUTOFF</code> status from one compute host to
   another. It does this using passwordless SSH access which has security
   concerns associated with it. For this reason, the <code class="literal">openstack server
   migrate</code> function has been disabled by default but you have the
   ability to enable this feature if you would like. Details on how to do this
   can be found in <a class="xref" href="ops-managing-compute.html#enabling-the-nova-resize" title="6.4. Enabling the Nova Resize and Migrate Features">Section 6.4, “Enabling the Nova Resize and Migrate Features”</a>.
  </p><p>
   <span class="bold"><strong>Live migration</strong></span> can be performed on
   instances in either an <code class="literal">ACTIVE</code> or
   <code class="literal">PAUSED</code> state and uses the QEMU hypervisor to manage the
   copy of the running processes and associated resources to the destination
   compute host using the hypervisors own protocol and thus is a more secure
   method and allows for less downtime. There may be a short network outage,
   usually a few milliseconds but could be up to a few seconds if your compute
   instances are busy, during a live migration. Also there may be some
   performance degredation during the process.
  </p><p>
   The compute host must remain powered on during the migration process.
  </p><p>
   Both the cold migration and live migration options will honor nova group
   policies, which includes affinity settings. There is a limitation to keep in
   mind if you use group policies and that is discussed in the
   <a class="xref" href="system-maintenance.html#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a> section.
  </p><p>
   This table describes each of the migration options for active compute nodes:
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/><col class="c3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Command</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Description</th><th style="border-bottom: 1px solid ; ">SLES</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">openstack server migrate &lt;instance_uuid&gt;</code>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Used to cold migrate a single instance from a compute host. The
        <code class="literal">nova-scheduler</code> will choose the new host.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">nova host-servers-migrate &lt;hostname&gt;</code>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Used to cold migrate all instances off a specified host to other
        available hosts, chosen by the <code class="literal">nova-scheduler</code>.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td style="border-bottom: 1px solid ; "> </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">nova live-migration &lt;instance_uuid&gt; [&lt;target
        host&gt;]</code>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td style="border-bottom: 1px solid ; ">X</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">nova live-migration --block-migrate &lt;instance_uuid&gt;
        [&lt;target host&gt;]</code>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td style="border-bottom: 1px solid ; ">X</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">nova host-evacuate-live &lt;hostname&gt; [--target-host
        &lt;target_hostname&gt;]</code>
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td style="border-bottom: 1px solid ; ">X</td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        <code class="literal">nova host-evacuate-live --block-migrate &lt;hostname&gt;
        [--target-host &lt;target_hostname&gt;]</code>
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr></tbody></table></div></section><section class="sect4" id="idg-all-operations-maintenance-live-migration-xml-10" data-id-title="Limitations of these Features"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.3.2 </span><span class="title-name">Limitations of these Features</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-live-migration-xml-10">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   There are limitations that may impact your use of this feature:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     To use live migration, your compute instances must be in either an
     <code class="literal">ACTIVE</code> or <code class="literal">PAUSED</code> state on the
     compute host. If you have instances in a <code class="literal">SHUTOFF</code> state
     then cold migration should be used.
    </p></li><li class="listitem"><p>
     Instances in a <code class="literal">Paused</code> state cannot be live migrated
     using the horizon dashboard. You will need to utilize the <code class="literal">python-novaclient</code>
     CLI to perform these.
    </p></li><li class="listitem"><p>
     Both cold migration and live migration honor an instance's group policies.
     If you are utilizing an affinity policy and are migrating multiple
     instances you may run into an error stating no hosts are available to
     migrate to. To work around this issue you should specify a target host
     when migrating these instances, which will bypass the
     <code class="literal">nova-scheduler</code>. You should ensure that the target host
     you choose has the resources available to host the instances.
    </p></li><li class="listitem"><p>
     The <code class="literal">nova host-evacuate-live</code> command will produce an
     error if you have a compute host that has a mix of instances that use
     local ephemeral storage and instances that are booted from a block storage
     volume or have any number of block storage volumes attached. If you have a
     mix of these instance types, you may need to run the command twice,
     utilizing the <code class="literal">--block-migrate</code> option. This is described
     in further detail in <a class="xref" href="system-maintenance.html#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a>.
    </p></li><li class="listitem"><p>
     Instances on KVM hosts can only be live migrated to other KVM hosts.
    </p></li><li class="listitem"><p>
     The migration options described in this document are not available on ESX
     compute hosts.
    </p></li><li class="listitem"><p>
     Ensure that you read and take into account any other limitations that
     exist in the release notes. See the release notes for
     more details.
    </p></li></ul></div></section><section class="sect4" id="liveMigrate" data-id-title="Performing a Live Migration"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.3.3 </span><span class="title-name">Performing a Live Migration</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#liveMigrate">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Cloud administrators can perform a migration on an instance using either the
   horizon dashboard, API, or CLI. Instances in a <code class="literal">Paused</code>
   state cannot be live migrated using the horizon GUI. You will need to
   utilize the CLI to perform these.
  </p><p>
   We have documented different scenarios:
  </p></section><section class="sect4" id="failed-host" data-id-title="Migrating instances off of a failed compute host"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.3.4 </span><span class="title-name">Migrating instances off of a failed compute host</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#failed-host">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     If the compute node is not already powered off, do so with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.5.17.3.6.5.7.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The value for <code class="literal">&lt;node_name&gt;</code> will be the name that
      Cobbler has when you run <code class="command">sudo cobbler system list</code> from
      the Cloud Lifecycle Manager.
     </p></div></li><li class="step"><p>
     Source the admin credentials necessary to run administrative commands
     against the nova API:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step"><p>
     Force the <code class="literal">nova-compute</code> service to go down on the
     compute node:
    </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set --down <em class="replaceable">HOSTNAME</em> nova-compute</pre></div><div id="id-1.5.17.3.6.5.7.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The value for <em class="replaceable">HOSTNAME</em> can be obtained by
      using <code class="command">openstack host list</code> from the Cloud Lifecycle Manager.
     </p></div></li><li class="step"><p>
     Evacuate the instances off of the failed compute node. This will cause the
     nova-scheduler to rebuild the instances on other valid hosts. Any local
     ephemeral data on the instances is lost.
    </p><p>
     For single instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova evacuate &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
     For all instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate &lt;hostname&gt; [--target_host &lt;target_hostname&gt;]</pre></div></li><li class="step"><p>
     When you have repaired the failed node and start it back up again, when
     the <code class="command">nova-compute</code> process starts again, it will clean
     up the evacuated instances.
    </p></li></ol></div></div></section><section class="sect4" id="active-host" data-id-title="Migrating instances off of an active compute host"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.3.5 </span><span class="title-name">Migrating instances off of an active compute host</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#active-host">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Migrating instances using the horizon
   dashboard</strong></span>
  </p><p>
   The horizon dashboard offers a GUI method for performing live migrations.
   Instances in a <code class="literal">Paused</code> state will not provide you the live
   migration option in horizon so you will need to use the CLI instructions in
   the next section to perform these.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log into the horizon dashboard with admin credentials.
    </p></li><li class="step"><p>
     Navigate to the menu <span class="guimenu">Admin</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Instances</span>.
    </p></li><li class="step"><p>
     Next to the instance you want to migrate, select the drop down menu and
     choose the <span class="guimenu">Live Migrate Instance</span> option.
    </p></li><li class="step"><p>
     In the Live Migrate wizard you will see the compute host the instance
     currently resides on and then a drop down menu that allows you to choose
     the compute host you want to migrate the instance to. Select a destination
     host from that menu. You also have two checkboxes for additional options,
     which are described below:
    </p><p>
     <span class="guimenu">Disk Over Commit</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will allow you to override the check that occurs to ensure the
     destination host has the available disk space to host the instance.
    </p><p>
     <span class="guimenu">Block Migration</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will migrate the local disks by using block migration. Use this
     option if you are only using ephemeral storage on your instances. If you
     are using block storage for your instance then ensure this box is not
     checked.
    </p></li><li class="step"><p>
     To begin the live migration, click <span class="guimenu">Submit</span>.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Migrating instances using the python-novaclient
   CLI</strong></span>
  </p><p>
   To perform migrations from the command-line, use the <code class="literal">python-novaclient</code>.
   The Cloud Lifecycle Manager node in your cloud environment should have
   the <code class="literal">python-novaclient</code> already installed. If you will be accessing your environment
   through a different method, ensure that the <code class="literal">python-novaclient</code> is
   installed. You can do so using Python's <code class="command">pip</code> package
   manager.
  </p><p>
   To run the commands in the steps below, you need administrator
   credentials. From the Cloud Lifecycle Manager, you can source the
   <code class="filename">service.osrc</code> file which is provided that has the
   necessary credentials:
  </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div><p>
   Here are the steps to perform:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Identify the instances on the compute node you wish to migrate:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants --host &lt;hostname&gt;</pre></div><p>
     Example showing a host with a single compute instance on it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack server list --host ardana-cp1-comp0001-mgmt --all-tenants
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| 553ba508-2d75-4513-b69a-f6a2a08d04e3 | test | 193548a949c146dfa1f051088e141f0b | ACTIVE | -          | Running     | adminnetwork=10.0.0.5 |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="step"><p>
     When using live migration you can either specify a target host that the
     instance will be migrated to or you can omit the target to allow the
     nova-scheduler to choose a node for you. If you want to get a list of
     available hosts you can use this command:
    </p><div class="verbatim-wrap"><pre class="screen">openstack host list</pre></div></li><li class="step"><p>
     Migrate the instance(s) on the compute node using the notes below.
    </p><p>
     If your instance is booted from a block storage volume or has any number
     of block storage volumes attached, use the <code class="literal">nova
     live-migration</code> command with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only or if your instance
     has a mix of ephemeral disk(s) and block storage volume(s), you should use
     the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><div id="id-1.5.17.3.6.5.8.10.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The <code class="literal">[&lt;target compute host&gt;]</code> option is
      optional. If you do not specify a target host then the nova scheduler
      will choose a node for you.
     </p></div><p>
     <span class="bold"><strong>Multiple instances</strong></span>
    </p><p>
     If you want to live migrate all of the instances off a single compute host
     you can utilize the <code class="literal">nova host-evacuate-live</code> command.
    </p><p>
     Issue the host-evacuate-live command, which will begin the live migration
     process.
    </p><p>
     If all of the instances on the host are using at least one local
     (ephemeral) disk, you should use this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live --block-migrate &lt;hostname&gt;</pre></div><p>
     Alternatively, if all of the instances are only using block storage
     volumes then omit the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt;</pre></div><div id="id-1.5.17.3.6.5.8.10.4.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      You can either let the nova-scheduler choose a suitable target host or
      you can specify one using the
      <code class="literal">--target-host &lt;hostname&gt;</code> switch. See
      <code class="command">nova help host-evacuate-live</code> for details.
     </p></div></li></ol></div></div></section><section class="sect4" id="idg-all-operations-maintenance-live-migration-xml-14" data-id-title="Troubleshooting migration or host evacuate issues"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.3.6 </span><span class="title-name">Troubleshooting migration or host evacuate issues</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-live-migration-xml-14">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                                                                                        |
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 95a7ded8-ebfc-4848-9090-2df378c88a4c | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-9fd79670-a780-40ed-a515-c14e28e0a0a7)     |
| 13ab4ef7-0623-4d00-bb5a-5bb2f1214be4 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration cannot be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-26834267-c3ec-4f8b-83cc-5193d6a394d6)     |
+--------------------------------------+-------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from local storage and
   you are not specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live <span class="bold"><strong>--block-migrate</strong></span> &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live --block-migrate ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| e9874122-c5dc-406f-9039-217d9258c020 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-60b1196e-84a0-4b71-9e49-96d6f1358e1a)     |
| 84a02b42-9527-47ac-bed9-8fde1f98e3fe | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-0cdf1198-5dbd-40f4-9e0c-e94aa1065112)     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from a block storage
   volume and you are specifying <code class="literal">--block-migrate</code> in your
   command. Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration 2a13ffe6-e269-4d75-8e46-624fec7a5da0 ardana-cp1-comp0002-mgmt
ERROR (BadRequest): ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-158dd415-0bb7-4613-8529-6689265387e7)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from local storage and you are not
   specifying <code class="literal">--block-migrate</code> in your command. Re-attempt
   the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration <span class="bold"><strong>--block-migrate</strong></span> &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration --block-migrate 84a02b42-9527-47ac-bed9-8fde1f98e3fe ardana-cp1-comp0001-mgmt
ERROR (BadRequest): ardana-cp1-comp0002-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-51fee8d6-6561-4afc-b0c9-7afa7dc43a5b)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from a block storage volume and you
   are specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div></section></section><section class="sect3" id="adding-compute-nodes" data-id-title="Adding Compute Node"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.3.4 </span><span class="title-name">Adding Compute Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#adding-compute-nodes">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-adding_compute_nodes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Adding a Compute Node allows you to add capacity.
 </p><section class="sect4" id="add-sles-compute" data-id-title="Adding a SLES Compute Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.4.1 </span><span class="title-name">Adding a SLES Compute Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#add-sles-compute">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Adding a SLES compute node allows you to add additional capacity for more
  virtual machines.
 </p><p>
  You may have a need to add additional SLES compute hosts for more virtual
  machine capacity or another purpose and these steps will help you achieve
  this.
 </p><p>
  There are two methods you can use to add SLES compute hosts to your
  environment:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    Adding SLES pre-installed compute hosts. This method does not require the
    SLES ISO be on the Cloud Lifecycle Manager to complete.
   </p></li><li class="listitem"><p>
    Using the provided Ansible playbooks and Cobbler, SLES will be installed on
    your new compute hosts. This method requires that you provided a SUSE Linux Enterprise Server 12 SP4
    ISO during the initial installation of your cloud, following the
    instructions at <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”, Section 31.1 “SLES Compute Node Installation Overview”</span>.
   </p><p>
    If you want to use the provided Ansible playbooks and Cobbler to setup and
    configure your SLES hosts and you did not have the SUSE Linux Enterprise Server 12 SP4 ISO on your
    Cloud Lifecycle Manager during your initial installation then ensure you look at
    the note at the top of that section before proceeding.
   </p></li></ol></div><section class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-5" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.3.4.1.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-add-sles-compute-xml-5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You need to ensure your input model files are properly setup for SLES
   compute host clusters. This must be done during the installation process of
   your cloud and is discussed further at <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”, Section 31.3 “Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes”</span> and
   <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
  </p></section><section class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-6" data-id-title="Adding a SLES compute node"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.3.4.1.2 </span><span class="title-name">Adding a SLES compute node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-add-sles-compute-xml-6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Adding pre-installed SLES compute hosts</strong></span>
  </p><p>
   This method requires that you have SUSE Linux Enterprise Server 12 SP4 pre-installed on the
   baremetal host prior to beginning these steps.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Ensure you have SUSE Linux Enterprise Server 12 SP4 pre-installed on your baremetal host.
    </p></li><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in the
     format. Note that we left out the IPMI details because they will not be
     needed since you pre-installed the SLES OS on your host(s).
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.5.17.3.6.6.3.7.4.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem"><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See for <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> more details.
    </p></li><li class="listitem"><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="listitem"><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="listitem"><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation.
    </p><div id="id-1.5.17.3.6.6.3.7.4.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The <code class="filename">wipe_disks.yml</code> playbook is only meant to be run
      on systems immediately after running
      <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
      not wipe all of the expected partitions.
     </p></div><p>
     The value to be used for <code class="literal">hostname</code> is host's identifier from
     <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem"><p>
     Complete the compute host deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Adding SLES compute hosts with Ansible playbooks and
   Cobbler</strong></span>
  </p><p>
   These steps will show you how to add the new SLES compute host to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><p>
   If you did not have the SUSE Linux Enterprise Server 12 SP4 ISO available on your Cloud Lifecycle Manager
   during your initial installation, it must be installed before proceeding
   further. Instructions can be found in <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”</span>.
  </p><p>
   When you are prepared to continue, use these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="step"><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in this
     format:
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1
  mac-addr: e8:39:35:21:32:4e
  ilo-ip: 10.1.192.36
  ilo-password: password
  ilo-user: admin
  distro-id: sles12sp4-x86_64</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.5.17.3.6.6.3.7.9.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="step"><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step"><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="step"><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
     The following playbook confirms that your servers are accessible over their IPMI ports.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml -e nodelist=compute4</pre></div></li><li class="step"><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
     Run the following playbook, ensuring that you specify only your UEFI
     SLES nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step"><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div><div id="id-1.5.17.3.6.6.3.7.9.10.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       If you do not know the <code class="literal">&lt;node name&gt;</code>, you can
       get it by using <code class="command">sudo cobbler system list</code>.
      </p></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="step"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your hosts are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div><div id="id-1.5.17.3.6.6.3.7.9.12.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      You can obtain the <code class="literal">&lt;hostname&gt;</code> from the file
      <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.
     </p></div></li><li class="step"><p>
     You should verify that the netmask, bootproto, and other necessary
     settings are correct and if they are not then re-do them. See
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”</span> for details.
    </p></li><li class="step"><p>
     Complete the compute host deployment with these playbooks. For the last
     one, ensure you specify the compute hosts you are added with the
     <code class="literal">--limit</code> switch:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div></section><section class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-8" data-id-title="Adding a new SLES compute node to monitoring"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.3.4.1.3 </span><span class="title-name">Adding a new SLES compute node to monitoring</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-add-sles-compute-xml-8">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you want to add a new Compute node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></section></section></section><section class="sect3" id="remove-compute-node" data-id-title="Removing a Compute Node"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5 </span><span class="title-name">Removing a Compute Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-compute-node">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Removing a Compute node allows you to remove capacity.
 </p><p>
  You may have a need to remove a Compute node and these steps will help you
  achieve this.
 </p><section class="sect4" id="disable-provisioning" data-id-title="Disable Provisioning on the Compute Host"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.1 </span><span class="title-name">Disable Provisioning on the Compute Host</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#disable-provisioning">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Get a list of the nova services running which will provide us with the
     details we need to disable the provisioning on the Compute host you are
     wanting to remove:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -               |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -               |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -               |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | -               |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</pre></div></li><li class="step"><p>
     Disable the nova service on the Compute node you are wanting to remove
     which will ensure it is taken out of the scheduling rotation:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>compute service set --disable --reason "<em class="replaceable">enter reason here</em>" <em class="replaceable">node hostname</em></pre></div><p>
     Here is an example if I wanted to remove the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> in the output above:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>compute service set –disable --reason "hardware reallocation" ardana-cp1-comp0002-mgmt
+--------------------------+--------------+----------+-----------------------+
| Host                     | Binary       | Status   | Disabled Reason       |
+--------------------------+--------------+----------+-----------------------+
| ardana-cp1-comp0002-mgmt | nova-compute | disabled | hardware reallocation |
+--------------------------+--------------+----------+-----------------------+</pre></div></li></ol></div></div></section><section class="sect4" id="remove-az" data-id-title="Remove the Compute Host from its Availability Zone"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.2 </span><span class="title-name">Remove the Compute Host from its Availability Zone</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-az">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you configured the Compute host to be part of an availability zone, these
   steps will show you how to remove it.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Get a list of the nova services running which will provide us with the
     details we need to remove a Compute node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+</pre></div></li><li class="step"><p>
     If the <code class="literal">Zone</code> reported for this host is simply "nova",
     then it is not a member of a particular availability zone, and this step
     will not be necessary. Otherwise, you must remove the Compute host from
     its availability zone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate remove host <em class="replaceable">availability zone</em> <em class="replaceable">nova hostname</em></pre></div><p>
     So for the same example in the previous step, the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> host was in the
     <code class="literal">AZ2</code> availability zone so you would use this command to
     remove it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate remove host AZ2 ardana-cp1-comp0002-mgmt
Host ardana-cp1-comp0002-mgmt has been successfully removed from aggregate 4
+----+------+-------------------+-------+-------------------------+
| Id | Name | Availability Zone | Hosts | Metadata                |
+----+------+-------------------+-------+-------------------------+
| 4  | AZ2  | AZ2               |       | 'availability_zone=AZ2' |
+----+------+-------------------+-------+-------------------------+</pre></div></li><li class="step"><p>
     You can confirm the last two steps completed successfully by running
     another <code class="literal">openstack compute service list</code>.
    </p><p>
     Here is an example which confirms that the node has been disabled and that
     it has been removed from the availability zone. I have highlighted these:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div></li></ol></div></div></section><section class="sect4" id="live-migration" data-id-title="Use Live Migration to Move Any Instances on this Host to Other Hosts"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.3 </span><span class="title-name">Use Live Migration to Move Any Instances on this Host to Other Hosts</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#live-migration">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     You will need to verify if the Compute node is currently hosting any
     instances on it. You can do this with the command below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host <em class="replaceable">nova hostname</em> --all_tenants=1</pre></div><p>
     Here is an example below which shows that we have a single running
     instance on this node currently:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host ardana-cp1-comp0002-mgmt --all-projects
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | ACTIVE | -          | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+</pre></div></li><li class="step"><p>
     You will likely want to migrate this instance off of this node before
     removing it. You can do this with the live migration functionality within
     nova. The command will look like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration --block-migrate <em class="replaceable">nova instance ID</em></pre></div><p>
     Here is an example using the instance in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration --block-migrate 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9</pre></div><p>
     You can check the status of the migration using the same command from the
     previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host ardana-cp1-comp0002-mgmt --all-projects
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status    | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | MIGRATING | migrating  | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+</pre></div></li><li class="step"><p>
     List the compute instances again to see that the running instance has been
     migrated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host ardana-cp1-comp0002-mgmt --all-projects
+----+------+-----------+--------+------------+-------------+----------+
| ID | Name | Tenant ID | Status | Task State | Power State | Networks |
+----+------+-----------+--------+------------+-------------+----------+
+----+------+-----------+--------+------------+-------------+----------+</pre></div></li></ol></div></div></section><section class="sect4" id="id-1.5.17.3.6.7.7" data-id-title="Disable Neutron Agents on Node to be Removed"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.4 </span><span class="title-name">Disable Neutron Agents on Node to be Removed</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.6.7.7">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You should also locate and disable or remove neutron agents. To see the
   neutron agents running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

<code class="prompt user">ardana &gt; </code>openstack network agent set --disable 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
<code class="prompt user">ardana &gt; </code>openstack network agent set --disable dbe4fe11-8f08-4306-8244-cc68e98bb770
<code class="prompt user">ardana &gt; </code>openstack network agent set --disable f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></section><section class="sect4" id="shutdown-node" data-id-title="Shut down or Stop the Nova and Neutron Services on the Compute Host"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.5 </span><span class="title-name">Shut down or Stop the Nova and Neutron Services on the Compute Host</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#shutdown-node">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To perform this step you have a few options. You can SSH into the Compute
   host and run the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop nova-compute</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop neutron-*</pre></div><p>
   Because the neutron agent self-registers against neutron server, you may
   want to prevent the following services from coming back online. Here is how
   you can get the list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl list-units neutron-* --all</pre></div><p>
   Here are the results:
  </p><div class="verbatim-wrap"><pre class="screen">UNIT                                  LOAD        ACTIVE     SUB      DESCRIPTION
neutron-common-rundir.service         loaded      inactive   dead     Create /var/run/neutron
•neutron-dhcp-agent.service         not-found     inactive   dead     neutron-dhcp-agent.service
neutron-l3-agent.service              loaded      inactive   dead     neutron-l3-agent Service
neutron-metadata-agent.service        loaded      inactive   dead     neutron-metadata-agent Service
•neutron-openvswitch-agent.service    loaded      failed     failed   neutron-openvswitch-agent Service
neutron-ovs-cleanup.service           loaded      inactive   dead     neutron OVS Cleanup Service

        LOAD   = Reflects whether the unit definition was properly loaded.
        ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
        SUB    = The low-level unit activation state, values depend on unit type.

        7 loaded units listed.
        To show all installed unit files use 'systemctl list-unit-files'.</pre></div><p>
   For each loaded service issue the command
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl disable <em class="replaceable">service-name</em></pre></div><p>
   In the above example that would be each service, <span class="emphasis"><em>except neutron-dhcp-agent.service
   </em></span>
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl disable neutron-common-rundir neutron-l3-agent neutron-metadata-agent neutron-openvswitch-agent</pre></div><p>
   Now you can shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo shutdown now</pre></div><p>
   OR
  </p><p>
   From the Cloud Lifecycle Manager you can use the
   <code class="literal">bm-power-down.yml</code> playbook to shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=<em class="replaceable">node name</em></pre></div><div id="id-1.5.17.3.6.7.8.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The <code class="literal"><em class="replaceable">node name</em></code> value will be the value
   corresponding to this node in Cobbler. You can run
   <code class="command">sudo cobbler system list</code> to retrieve these names.
  </p></div></section><section class="sect4" id="delete-node" data-id-title="Delete the Compute Host from Nova"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.6 </span><span class="title-name">Delete the Compute Host from Nova</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#delete-node">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Retrieve the list of nova services:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list</pre></div><p>
   Here is an example highlighting the Compute host we're going to remove:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div><p>
   Delete the host from nova using the command below:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service delete <em class="replaceable">service ID</em></pre></div><p>
   Following our example above, you would use:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service delete 37</pre></div><p>
   Use the command below to confirm that the Compute host has been completely
   removed from nova:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack hypervisor list</pre></div></section><section class="sect4" id="deletefromneutron" data-id-title="Delete the Compute Host from Neutron"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.7 </span><span class="title-name">Delete the Compute Host from Neutron</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#deletefromneutron">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Multiple neutron agents are running on the compute node. You have to remove
   all of the agents running on the node using the <code class="command">openstack network
   agent delete</code> command. In the example below, the l3-agent,
   openvswitch-agent and metadata-agent are running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ openstack network agent delete AGENT_ID

$ openstack network agent delete 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ openstack network agent delete dbe4fe11-8f08-4306-8244-cc68e98bb770
$ openstack network agent delete f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></section><section class="sect4" id="remove-node" data-id-title="Remove the Compute Host from the servers.yml File and Run the Configuration Processor"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.8 </span><span class="title-name">Remove the Compute Host from the servers.yml File and Run the Configuration Processor</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-node">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Complete these steps from the Cloud Lifecycle Manager to remove the Compute node:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="step"><p>
     Edit your <code class="literal">servers.yml</code> file in the location below to
     remove references to the Compute node(s) you want to remove:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>vi servers.yml</pre></div></li><li class="step"><p>
     You may also need to edit your <code class="literal">control_plane.yml</code> file
     to update the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code> if you used
     those to ensure they reflect the exact number of nodes you are using.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step"><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "Remove node <em class="replaceable">NODE_NAME</em>"</pre></div></li><li class="step"><p>
     To release the network capacity allocated to the deleted server(s), use
     the switches <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> when running the configuration
     processors. (For more information, see
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="step"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step"><p>
     Refresh the <code class="literal">/etc/hosts</code> file through the cloud to remove
     references to the old node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</pre></div></li></ol></div></div></section><section class="sect4" id="remove-cobbler" data-id-title="Remove the Compute Host from Cobbler"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.9 </span><span class="title-name">Remove the Compute Host from Cobbler</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-cobbler">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Complete these steps to remove the node from Cobbler:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Confirm the system name in Cobbler with this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo  cobbler system list</pre></div></li><li class="step"><p>
     Remove the system from Cobbler using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo  cobbler system remove --name=<em class="replaceable">node</em></pre></div></li><li class="step"><p>
     Run the <code class="literal">cobbler-deploy.yml</code> playbook to complete the
     process:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></section><section class="sect4" id="idg-all-operations-maintenance-compute-remove-compute-node-xml-14" data-id-title="Remove the Compute Host from Monitoring"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.3.5.10 </span><span class="title-name">Remove the Compute Host from Monitoring</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-remove-compute-node-xml-14">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have removed the Compute nodes, the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
    To find all monasca API servers
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cat /etc/haproxy/haproxy.cfg | grep MON
listen ardana-cp1-vip-public-MON-API-extapi-8070
    bind ardana-cp1-vip-public-MON-API-extapi:8070  ssl crt /etc/ssl/private//my-public-cert-entry-scale
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5
listen ardana-cp1-vip-MON-API-mgmt-8070
    bind ardana-cp1-vip-MON-API-mgmt:8070  ssl crt /etc/ssl/private//ardana-internal-cert
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5</pre></div><p>In above example <code class="literal">ardana-cp1-c1-m1-mgmt</code>,<code class="literal">ardana-cp1-c1-m2-mgmt</code>,
  <code class="literal">ardana-cp1-c1-m3-mgmt</code> are Monasa API servers</p><p>
   You will want to SSH to each of the monasca API servers and edit the
   <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to remove
   references to the Compute node you removed. This will require
   <code class="literal">sudo</code> access. The entries will look similar to the one
   below:
  </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: ardana-cp1-comp0001-mgmt
  name: ardana-cp1-comp0001-mgmt ping</pre></div><p>
   Once you have removed the references on each of your monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the Compute node references removed and the monasca-agent restarted,
   you can then delete the corresponding alarm to finish this process. To do so
   we recommend using the monasca CLI which should be installed on each of your
   monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-dimensions hostname=<em class="replaceable">compute node deleted</em></pre></div><p>
   For example, if your Compute node looked like the example above then you
   would use this command to get the alarm ID:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-delete <em class="replaceable">alarm ID</em></pre></div></section></section></section><section class="sect2" id="planned-maintenance-task-for-networking-nodes" data-id-title="Planned Network Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.1.4 </span><span class="title-name">Planned Network Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-maintenance-task-for-networking-nodes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking_nodes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Planned maintenance task for networking nodes.
 </p><section class="sect3" id="add-network-node" data-id-title="Adding a Network Node"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.4.1 </span><span class="title-name">Adding a Network Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#add-network-node">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Adding an additional neutron networking node allows you to increase the
  performance of your cloud.
 </p><p>
  You may have a need to add an additional neutron network node for increased
  performance or another purpose and these steps will help you achieve this.
 </p><section class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-6" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.4.1.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-networking-add-network-node-xml-6">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you are using the mid-scale model then your networking nodes are already
   separate and the roles are defined. If you are not already using this model
   and wish to add separate networking nodes then you need to ensure that those
   roles are defined. You can look in the <code class="literal">~/openstack/examples</code>
   folder on your Cloud Lifecycle Manager for the mid-scale example model files which
   show how to do this. We have also added the basic edits that need to be made
   below:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     In your <code class="literal">server_roles.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/server_roles.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-ROLE
  interface-model: NEUTRON-INTERFACES
  disk-model: NEUTRON-DISKS</pre></div></li><li class="listitem"><p>
     In your <code class="literal">net_interfaces.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-INTERFACES</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/net_interfaces.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-INTERFACES
  network-interfaces:
  - device:
      name: hed3
    name: hed3
    network-groups:
    - EXTERNAL-VM
    - GUEST
    - MANAGEMENT</pre></div></li><li class="listitem"><p>
     Create a <code class="literal">disks_neutron.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-DISKS</code> defined in it.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_neutron.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  product:
    version: 2

  disk-models:
  - name: NEUTRON-DISKS
    volume-groups:
      - name: ardana-vg
        physical-volumes:
         - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></li><li class="listitem"><p>
     Modify your <code class="literal">control_plane.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined as well as the neutron services
     added.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/control_plane.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  - allocation-policy: strict
    cluster-prefix: neut
    member-count: 1
    name: neut
    server-role: NEUTRON-ROLE
    service-components:
    - ntp-client
    - neutron-vpn-agent
    - neutron-dhcp-agent
    - neutron-metadata-agent
    - neutron-openvswitch-agent</pre></div></li></ol></div><p>
   You should also have one or more baremetal servers that meet the minimum
   hardware requirements for a network node which are documented in the
   <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span>.
  </p></section><section class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-7" data-id-title="Adding a network node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.4.1.2 </span><span class="title-name">Adding a network node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-networking-add-network-node-xml-7">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   These steps will show you how to add the new network node to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="listitem"><p>
     In the same directory, edit your <code class="literal">servers.yml</code> file to
     include the details about your new network node(s).
    </p><p>
     For example, if you already had a cluster of three network nodes and
     needed to add a fourth one you would add your details to the bottom of the
     file in this format:
    </p><div class="verbatim-wrap"><pre class="screen"># network nodes
- id: neut3
  ip-addr: 10.13.111.137
  role: NEUTRON-ROLE
  server-group: RACK2
  mac-addr: "5c:b9:01:89:b6:18"
  nic-mapping: HP-DL360-6PORT
  ip-addr: 10.243.140.22
  ilo-ip: 10.1.12.91
  ilo-password: password
  ilo-user: admin</pre></div><div id="id-1.5.17.3.7.3.5.3.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this node does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem"><p>
     In your <code class="literal">control_plane.yml</code> file you will need to check
     the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code>, if you
     specified them, to ensure that they match up with your new total node
     count. So for example, if you had previously specified
     <code class="literal">member-count: 3</code> and are adding a fourth network node,
     you will need to change that value to <code class="literal">member-count: 4</code>.
    </p></li><li class="listitem"><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "Add new networking node &lt;name&gt;"</pre></div></li><li class="listitem"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem"><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem"><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;hostname&gt;</pre></div><div id="id-1.5.17.3.7.3.5.3.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      If you do not know the <code class="literal">&lt;hostname&gt;</code>, you can
      get it by using <code class="command">sudo cobbler system list</code>.
     </p></div></li><li class="listitem"><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem"><p>
     Configure the operating system on the new networking node with this
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem"><p>
     Complete the networking node deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem"><p>
     Run the <code class="literal">site.yml</code> playbook with the required tag so that
     all other services become aware of the new node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li></ol></div></section><section class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-8" data-id-title="Adding a New Network Node to Monitoring"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.4.1.3 </span><span class="title-name">Adding a New Network Node to Monitoring</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-networking-add-network-node-xml-8">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you want to add a new networking node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></section></section></section><section class="sect2" id="storage-maintenance" data-id-title="Planned Storage Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.1.5 </span><span class="title-name">Planned Storage Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#storage-maintenance">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-storage_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Planned maintenance procedures for swift storage nodes.
 </p><section class="sect3" id="planned-maintenance-for-swift-nodes" data-id-title="Planned Maintenance Tasks for swift Nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1 </span><span class="title-name">Planned Maintenance Tasks for swift Nodes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#planned-maintenance-for-swift-nodes">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift_nodes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Planned maintenance tasks including recovering, adding, and removing swift
  nodes.
 </p><section class="sect4" id="sec-swift-add-object-node" data-id-title="Adding a Swift Object Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.1 </span><span class="title-name">Adding a Swift Object Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#sec-swift-add-object-node">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Adding additional object nodes allows you to increase capacity.
 </p><p>
  This topic describes how to add additional swift object server nodes to an
  existing system.
 </p><section class="sect5" id="id-1.5.17.3.8.3.3.4" data-id-title="To add a new node"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.1.1 </span><span class="title-name">To add a new node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.8.3.3.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="literal">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="literal">servers.yml
   file</code> by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager node.
    </p></li><li class="listitem"><p>
     Get the <code class="literal">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem"><p>
     If not already done, set the <code class="literal">weight-step</code> attribute. For
     instructions, see <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem"><p>
     Add the details of new nodes to the <code class="literal">servers.yml</code> file.
     In the following example only one new server
     <span class="bold"><strong>swobj4</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="literal">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swobj4
  role: SWOBJ_ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div></li><li class="listitem"><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.5.17.3.8.3.3.4.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 30 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem"><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem"><p>
     Configure Cobbler to include the new node, and then reimage the node (if
     you are adding several nodes, use a comma-separated list with the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swobj4</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj4</pre></div><div id="id-1.5.17.3.8.3.3.4.4.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">id</code>.
     </p></div></li><li class="listitem"><p>
     Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     The hostname of the newly added server can be found in the list generated
     from the output of the following command:
    </p><div class="verbatim-wrap"><pre class="screen">grep hostname ~/openstack/my_cloud/info/server_info.yml</pre></div><p>
     For example, for <span class="bold"><strong>swobj4</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-swobj0004-mgmt</strong></span>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-swobj0004-mgmt</pre></div></li><li class="listitem"><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem"><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem"><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swobj4</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem"><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml</pre></div></li></ol></div></section></section><section class="sect4" id="adding-proxy" data-id-title="Adding a Swift Proxy, Account, Container (PAC) Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.2 </span><span class="title-name">Adding a Swift Proxy, Account, Container (PAC) Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#adding-proxy">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Steps for adding additional PAC nodes to your swift system.
 </p><p>
  This topic describes how to add additional swift proxy, account, and
  container (PAC) servers to an existing system.
 </p><section class="sect5" id="id-1.5.17.3.8.3.4.4" data-id-title="Adding a new node"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.2.1 </span><span class="title-name">Adding a new node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.8.3.4.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="filename">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="filename">servers.yml</code>
   file by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Get the <code class="filename">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem"><p>
     If not already done, set the weight-step attribute. For instructions, see
     <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem"><p>
     Add details of new nodes to the <code class="filename">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swpac6
  role: SWPAC-ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div><p>
     In the above example, only one new server
     <span class="bold"><strong>swpac6</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="filename">servers.yml</code> file.
    </p><p>
     In the entry-scale configurations there is no dedicated swift PAC cluster.
     Instead, there is a cluster using servers that have a role of
     <code class="literal">CONTROLLER-ROLE</code>. You cannot add additional nodes
     dedicated exclusively to swift PAC because that would change the
     <code class="literal">member-count</code> of the entire cluster. In that case, to
     create a dedicated swift PAC cluster, you will need to add it to the
     configuration files. For details on how to do this, see
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.7 “Creating a Swift Proxy, Account, and Container (PAC) Cluster”</span>.
    </p><p>
     If using a new PAC nodes you must add the PAC node's configuration details
     in the following yaml files:
    </p><div class="verbatim-wrap"><pre class="screen">control_plane.yml
disks_pac.yml
net_interfaces.yml
servers.yml
server_roles.yml</pre></div><p>
     You can see a good example of this in the example configurations for the
     mid-scale model in the <code class="literal">~/openstack/examples/mid-scale-kvm</code>
     directory.
    </p><p>
     The following steps assume that you have already created a dedicated swift
     PAC cluster and that it has two members
     (<span class="bold"><strong>swpac4</strong></span> and
     <span class="bold"><strong>swpac5</strong></span>).
    </p></li><li class="listitem"><p>
     Set the member count of the swift PAC cluster to match the number of nodes.
     For example, if you are adding <span class="bold"><strong>swpac6</strong></span> as
     the 6th swift PAC node, the member count should be increased from 5 to 6
     as shown in the following example:
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1

  . . .
  clusters:
  . . .
     - name: swpac
       cluster-prefix: swpac
       server-role: SWPAC-ROLE
       member-count: 6
   . . .</pre></div></li><li class="listitem"><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.5.17.3.8.3.4.4.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 30 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem"><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem"><p>
     Configure Cobbler to include the new node and reimage the node (if you are
     adding several nodes, use a comma-separated list for the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swpac6</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swpac6</pre></div><div id="id-1.5.17.3.8.3.4.4.4.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">id</code>.
     </p></div></li><li class="listitem"><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     For example, for <span class="bold"><strong>swpac6</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-c2-m3-mgmt</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-c2-m3-mgmt</pre></div></li><li class="listitem"><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem"><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem"><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swpac6</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem"><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></section></section><section class="sect4" id="add-swift-disk" data-id-title="Adding Additional Disks to a Swift Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.3 </span><span class="title-name">Adding Additional Disks to a Swift Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#add-swift-disk">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Steps for adding additional disks to any nodes hosting swift services.
 </p><p>
  You may have a need to add additional disks to a node for swift usage and we
  can show you how. These steps work for adding additional disks to swift
  object or proxy, account, container (PAC) nodes. It can also apply to adding
  additional disks to a controller node that is hosting the swift service, like
  you would see if you are using one of the entry-scale example models.
 </p><p>
  Read through the notes below before beginning the process.
 </p><p>
  You can add multiple disks at the same time, there is no need to do it one at
  a time.
 </p><div id="id-1.5.17.3.8.3.5.6" data-id-title="Add the Same Number of Disks" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Add the Same Number of Disks</div><p>
   You must add the <span class="emphasis"><em>same</em></span> number of disks to each server
   that the disk model applies to. For example, if you have a single cluster of
   three swift servers and you want to increase capacity and decide to add two
   additional disks, you must add two to each of your three swift servers.
  </p></div><section class="sect5" id="id-1.5.17.3.8.3.5.7" data-id-title="Adding additional disks to your Swift servers"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.3.1 </span><span class="title-name">Adding additional disks to your Swift servers</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.8.3.5.7">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Verify the general health of the swift system and that it is safe to
     rebalance your rings. See <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details
     on how to do this.
    </p></li><li class="step"><p>
     Perform the disk maintenance.
    </p><ol type="a" class="substeps"><li class="step" id="st-swift-add-disk-shutdown"><p>
       Shut down the first swift server you wish to add disks to.
      </p></li><li class="step"><p>
       Add the additional disks to the physical server. The disk drives that
       are added should be clean. They should either contain no partitions or a
       single partition the size of the entire disk. It should not contain a
       file system or any volume groups. Failure to comply will cause errors
       and the disk will not be added.
      </p><p>
       For more details, see <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.
      </p></li><li class="step"><p>
       Power the server on.
      </p></li><li class="step"><p>
       While the server was shutdown, data that normally would have been placed
       on the server is placed elsewhere. When the server is rebooted, the
       swift replication process will move that data back onto the server.
       Monitor the replication process to determine when it is complete. See
       <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details on how to do this.
      </p></li><li class="step"><p>
       Repeat the steps from <a class="xref" href="system-maintenance.html#st-swift-add-disk-shutdown" title="Step 2.a">Step 2.a</a> for
       each of the swift servers you are adding the disks to, one at a time.
      </p><div id="id-1.5.17.3.8.3.5.7.2.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
         If the additional disks can be added to the swift servers online
         (for example, via hotplugging) then there is no need to perform the
         last two steps.
         
        </p></div></li></ol></li><li class="step"><p>
     On the Cloud Lifecycle Manager, update your cloud configuration with the details
     of your additional disks.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Edit the disk configuration file that correlates to the type of server
       you are adding your new disks to.
      </p><p>
       Path to the typical disk configuration files:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_swobj.yml
~/openstack/my_cloud/definition/data/disks_swpac.yml
~/openstack/my_cloud/definition/data/disks_controller_*.yml</pre></div><p>
       Example showing the addition of a single new disk, indicated by the
       <code class="literal">/dev/sdd</code>, in bold:
      </p><div class="verbatim-wrap"><pre class="screen">device-groups:
  - name: swiftObject
    devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"
      <span class="bold"><strong>- name: "/dev/sdd"</strong></span>
    consumer:
      name: swift
      ...</pre></div><div id="id-1.5.17.3.8.3.5.7.2.3.2.1.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
         For more details on how the disk model works, see
         <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”</span>.
        </p></div></li><li class="step"><p>
       Configure the swift weight-step value in the
       <code class="literal">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code>
       file. See <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for details on how to do
       this.
      </p></li><li class="step"><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "adding additional swift disks"</pre></div></li><li class="step"><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step"><p>
     Run the <code class="literal">osconfig-run.yml</code> playbook against the swift
     nodes you have added disks to. Use the <code class="literal">--limit</code> switch
     to target the specific nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostnames&gt;</pre></div><p>
     You can use a wildcard when specifying the hostnames with the
     <code class="literal">--limit</code> switch. If you added disks to all of the swift
     servers in your environment and they all have the same prefix (for
     example, <code class="literal">ardana-cp1-swobj...</code>) then you can use a
     wildcard like <code class="literal">ardana-cp1-swobj*</code>. If you only added
     disks to a set of nodes but not all of them, you can use a comma
     deliminated list and enter the hostnames of each of the nodes you added
     disks to.
    </p></li><li class="step"><p>
     Validate your swift configuration with this playbook which will also
     provide details of each drive being added:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div></li><li class="step"><p>
     Verify that swift services are running on all of your servers:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li><li class="step"><p>
     If everything looks okay with the swift status, then apply the changes to
     your swift rings with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="step"><p>
     At this point your swift rings will begin rebalancing. You should wait
     until replication has completed or min-part-hours has elapsed (whichever
     is longer), as described in <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> and then
     follow the "Weight Change Phase of Ring Rebalance" process as described in
     <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div></section></section><section class="sect4" id="remove-swift-node" data-id-title="Removing a Swift Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.4 </span><span class="title-name">Removing a Swift Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#remove-swift-node">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Removal process for both swift Object and PAC nodes.
 </p><p>
  You can use this process when you want to remove one or more swift nodes
  permanently. This process applies to both swift Proxy, Account, Container
  (PAC) nodes and swift Object nodes.
 </p><section class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-6" data-id-title="Setting the Pass-through Attributes"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.4.1 </span><span class="title-name">Setting the Pass-through Attributes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-swift-removing-swift-node-xml-6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This process will remove the swift node's drives from the rings and rebalance
   their responsibilities among the remaining nodes in your cluster. Note that
   removal will not succeed if it causes the number of remaining disks in the
   cluster to decrease below the replica count of its rings.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Ensure that the weight-step attribute is set. See
     <a class="xref" href="ops-managing-objectstorage.html#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for more details.
    </p></li><li class="listitem"><p>
     Add the pass-through definition to your input model, specifying the server
     ID (as opposed to the server name). It is easiest to include in your
     <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code> file
     since your server IDs are already listed in that file. For more
     information about pass-through, see <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.17 “Pass Through”</span>.
    </p><p>
     Here is the format required, which can be inserted at the topmost level
     of indentation in your file (typically 2 spaces):
    </p><div class="verbatim-wrap"><pre class="screen">pass-through:
  servers:
    - id: <em class="replaceable">server-id</em>
      data:
        <em class="replaceable">subsystem</em>:
          <em class="replaceable">subsystem-attributes</em></pre></div><p>
     Here is an example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  <span class="bold"><strong>pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            drain: yes</strong></span></pre></div><p>
     If a pass-through definition already exists in any of your input model
     data files, just include the additional data for the server which you are
     removing instead of defining an entirely new pass-through block.
    </p><p>
     By setting this pass-through attribute, you indicate that the system
     should reduce the weight of the server's drives. The weight reduction is
     determined by the weight-step attribute as described in the previous step.
     This process is known as "draining", where you remove the swift data from
     the node in preparation for removing the node.
    </p></li><li class="listitem"><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem"><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem"><p>
     Run the swift deploy playbook to perform the first ring rebuild. This will
     remove some of the partitions from all drives on the node you are
     removing:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem"><p>
     Wait until the replication has completed. For further details, see
     <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem"><p>
     Determine whether all of the partitions have been removed from all drives
     on the swift node you are removing. You can do this by SSH'ing into the
     first account server node and using these commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/swiftlm/cloud1/cp1/builder_dir/
sudo swift-ring-builder <em class="replaceable">ring_name</em>.builder</pre></div><p>
     For example, if the node you are removing was part of the object-o ring
     the command would be:
    </p><div class="verbatim-wrap"><pre class="screen">sudo swift-ring-builder object-0.builder</pre></div><p>
     Check the output. You will need to know the IP address of the server being
     drained. In the example below, the number of partitions of the drives on
     192.168.245.3 has reached zero for the object-0 ring:
    </p><div class="verbatim-wrap"><pre class="screen">$ cd /etc/swiftlm/cloud1/cp1/builder_dir/
$ sudo swift-ring-builder object-0.builder
account.builder, build version 6
4096 partitions, 3.000000 replicas, 1 regions, 1 zones, 6 devices, 0.00 balance, 0.00 dispersion
The minimum number of hours before a partition can be reassigned is 16
The overload factor is 0.00% (0.000000)
Devices:    id  region  zone      ip address  port  replication ip  replication port      name weight partitions balance meta
             0       1     1   192.168.245.3  6002   192.168.245.3              6002     disk0   0.00          0   -0.00 padawan-ccp-c1-m1:disk0:/dev/sdc
             1       1     1   192.168.245.3  6002   192.168.245.3              6002     disk1   0.00          0   -0.00 padawan-ccp-c1-m1:disk1:/dev/sdd
             2       1     1   192.168.245.4  6002   192.168.245.4              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m2:disk0:/dev/sdc
             3       1     1   192.168.245.4  6002   192.168.245.4              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m2:disk1:/dev/sdd
             4       1     1   192.168.245.5  6002   192.168.245.5              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m3:disk0:/dev/sdc
             5       1     1   192.168.245.5  6002   192.168.245.5              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m3:disk1:/dev/sdd</pre></div></li><li class="listitem"><p>
     If the number of partitions is zero for the server on all rings, you can
     move to the next step, otherwise continue the ring rebalance cycle by
     repeating steps 7-9 until the weight has reached zero.
    </p></li><li class="listitem"><p>
     If the number of partitions is zero for the server on all rings, you can
     remove the swift nodes' drives from all rings. Edit the pass-through data
     you created in step #3 and set the <code class="literal">remove</code> attribute as
     shown in this example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            <span class="bold"><strong>remove: yes</strong></span></pre></div></li><li class="listitem"><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem"><p>
     Run the swift deploy playbook to rebuild the rings by removing the server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem"><p>
     At this stage, the server has been removed from the rings and the data
     that was originally stored on the server has been replicated in a balanced
     way to the other servers in the system. You can proceed to the next phase.
    </p></li></ol></div></section><section class="sect5" id="sec-swift-disable-node" data-id-title="To Disable Swift on a Node"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.4.2 </span><span class="title-name">To Disable Swift on a Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#sec-swift-disable-node">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The next phase in this process will disable the swift service on the node.
   In this example, <span class="bold"><strong>swobj4</strong></span> is the node being
   removed from swift.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Stop swift services on the node using the
     <code class="literal">swift-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong><em class="replaceable">hostname</em></strong></span></pre></div><div id="id-1.5.17.3.8.3.6.5.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      When using the <code class="literal">--limit</code> argument, you must specify the
      full hostname (for example: <span class="emphasis"><em>ardana-cp1-swobj0004</em></span>) or
      use the wild card <code class="literal">*</code> (for example,
      <span class="emphasis"><em>*swobj4*</em></span>).
     </p></div><p>
     The following example uses the <code class="literal">swift-stop.yml</code> playbook
     to stop swift services on
     <span class="bold"><strong>ardana-cp1-swobj0004</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong>ardana-cp1-swobj0004</strong></span></pre></div></li><li class="listitem"><p>
     Remove the configuration files.
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-swobj4-mgmt sudo rm -R /etc/swift</pre></div><div id="id-1.5.17.3.8.3.6.5.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Do not run any other playbooks until you have finished the process
      described in <a class="xref" href="system-maintenance.html#sec-swift-remove-node-model" title="15.1.5.1.4.3. To Remove a Node from the Input Model">Section 15.1.5.1.4.3, “To Remove a Node from the Input Model”</a>. Otherwise,
      these playbooks may recreate <code class="filename">/etc/swift</code> and
      restart swift on <span class="emphasis"><em>swobj4</em></span>. If you accidentally run a
      playbook, repeat the process in <a class="xref" href="system-maintenance.html#sec-swift-disable-node" title="15.1.5.1.4.2. To Disable Swift on a Node">Section 15.1.5.1.4.2, “To Disable Swift on a Node”</a>.
     </p></div></li></ol></div></section><section class="sect5" id="sec-swift-remove-node-model" data-id-title="To Remove a Node from the Input Model"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.4.3 </span><span class="title-name">To Remove a Node from the Input Model</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#sec-swift-remove-node-model">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Use the following steps to finish the process of removing the swift node.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Edit the <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file and remove the entry for the node
     (<span class="bold"><strong>swobj4</strong></span> in this example). In addition,
     remove the related entry you created in the pass-through section earlier
     in this process.
    </p></li><li class="listitem"><p>
     If this was a SWPAC node, reduce the member-count attribute by 1 in the
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file. For SWOBJ nodes, no such action is needed.
    </p></li><li class="listitem"><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     Using the <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> switches is recommended
     to free up the resources associated with the removed node when
     running the configuration processor. For more information, see
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="listitem"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem"><p>
     Validate the changes you have made to the configuration files using the
     playbook below before proceeding further:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them in your configuration files and repeat
     steps 3-5 again until no more errors occur before going to the next step.
    </p><p>
     For more details on how to interpret and resolve errors, see
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>
    </p></li><li class="listitem"><p>
     Remove the node from Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name=swobj4</pre></div></li><li class="listitem"><p>
     Run the Cobbler deploy playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem"><p>
     The final step will depend on what type of swift node you are removing.
    </p><p>
     If the node was a SWPAC node, run the <code class="literal">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div><p>
     If the node was a SWOBJ node (and not a SWPAC node), run the
     <code class="literal">swift-deploy.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem"><p>
     Wait until replication has finished. For more details, see
     <a class="xref" href="ops-managing-objectstorage.html#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
    </p></li><li class="listitem"><p>
     You may need to continue to rebalance the rings. For instructions, see
     <span class="bold"><strong>Final Rebalance Phase </strong></span> at
     <a class="xref" href="ops-managing-objectstorage.html#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></section><section class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-9" data-id-title="Remove the Swift Node from Monitoring"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.4.4 </span><span class="title-name">Remove the Swift Node from Monitoring</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-swift-removing-swift-node-xml-9">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Once you have removed the swift node(s), the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
   Connect to each of the nodes in your cluster running the
   <code class="literal">monasca-api</code> service (as defined in
   <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>)
   and use <code class="command">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</code>
   to delete all references to the swift node(s) you removed.
  </p><p>
   Once you have removed the references on each of your monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the swift node references removed and the monasca-agent restarted, you
   can then delete the corresponding alarm to finish this process. To do so we
   recommend using the monasca CLI which should be installed on each of your
   monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=<em class="replaceable">swift node deleted</em></pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete <em class="replaceable">alarm ID</em></pre></div></section></section><section class="sect4" id="replace-swift-node" data-id-title="Replacing a swift Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.5 </span><span class="title-name">Replacing a swift Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace-swift-node">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Maintenance steps for replacing a failed swift node in your environment.
 </p><p>
  This process is used when you want to replace a failed swift node in your
  cloud.
 </p><div id="id-1.5.17.3.8.3.7.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
   If it applies to the server, do not skip step 10. If you do, the system will
   overwrite the existing rings with new rings. This will not cause data loss,
   but, potentially, will move most objects in your system to new locations and
   may make data unavailable until the replication process has completed.
  </p></div><section class="sect5" id="id-1.5.17.3.8.3.7.5" data-id-title="How to replace a swift node in your environment"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.5.1 </span><span class="title-name">How to replace a swift node in your environment</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.8.3.7.5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Power off the node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=<code class="literal">OLD_SWIFT_CONTROLLER_NODE</code></pre></div></li><li class="step"><p>
     Update your cloud configuration with the details of your replacement swift.
     node.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Edit your <code class="literal">servers.yml</code> file to include the details
       (MAC address, IPMI user, password, and IP address (IPME) if these
       have changed) about your replacement swift node.
      </p><div id="id-1.5.17.3.8.3.7.5.2.3.2.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Do not change the server's IP address (that is, <code class="literal">ip-addr</code>).
       </p></div><p>
       Path to file:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div><p>
       Example showing the fields to edit, in bold:
      </p><div class="verbatim-wrap"><pre class="screen"> - id: swobj5
   role: SWOBJ-ROLE
   server-group: rack2
   <span class="bold"><strong>mac-addr: 8c:dc:d4:b5:cb:bd</strong></span>
   nic-mapping: HP-DL360-6PORT
   ip-addr: 10.243.131.10
   <span class="bold"><strong>ilo-ip: 10.1.12.88
   ilo-user: iLOuser
   ilo-password: iLOpass</strong></span>
   ...</pre></div></li><li class="step"><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git commit -a -m "replacing a swift node"</pre></div></li><li class="step"><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step"><p>
     Prepare SLES:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-loader.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=<code class="literal">NEW REPLACEMENT NODE</code></pre></div></li><li class="step"><p>
     Update Cobbler and reimage your replacement swift node:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Obtain the name in Cobbler for your node you wish to remove. You will
       use this value to replace <code class="literal">&lt;node name&gt;</code> in future
       steps.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="step"><p>
       Remove the replaced swift node from Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name &lt;node name&gt;</pre></div></li><li class="step"><p>
       Re-run the <code class="literal">cobbler-deploy.yml</code> playbook to add the
       replaced node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
       Reimage the node using this playbook:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></li><li class="step"><p>
      Wipe the disks on the <code class="literal">NEW REPLACEMENT NODE</code>.
      This action will not affect the OS partitions on the server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit <code class="literal">NEW_REPLACEMENT_NODE</code></pre></div></li><li class="step"><p>
     Complete the deployment of your replacement swift node.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Obtain the hostname for your new swift node. Use this value to replace
       <code class="literal">&lt;hostname&gt;</code> in future steps.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/info/server_info.yml</pre></div></li><li class="step"><p>
       Configure the operating system on your replacement swift node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit &lt;hostname&gt;</pre></div></li><li class="step"><p>
       If this is the swift ring builder server, restore the swift ring builder
       files to the <code class="literal">/etc/swiftlm/<em class="replaceable">CLOUD-NAME</em>/<em class="replaceable">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For
       more information and instructions, see
       <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a> and
       <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a>.
      </p></li><li class="step"><p>
       Configure services on the node using the
       <code class="literal">ardana-deploy.yml</code> playbook. If you have used an
       encryption password when running the configuration processor, include
       the <code class="literal">--ask-vault-pass</code> argument.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit &lt;hostname&gt;</pre></div></li></ol></li></ol></div></div></section></section><section class="sect4" id="replacing-disks" data-id-title="Replacing Drives in a swift Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.6 </span><span class="title-name">Replacing Drives in a swift Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#replacing-disks">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Maintenance steps for replacing drives in a swift node.
 </p><p>
  This process is used when you want to remove a failed hard drive
  from swift node and replace it with a new one.
 </p><p>
  There are two different classes of drives in a swift node that needs to be
  replaced; the operating system disk drive (generally
  <span class="bold"><strong>/dev/sda</strong></span>) and storage disk drives. There are
  different procedures for the replacement of each class of drive to bring the
  node back to normal.
 </p><section class="sect5" id="id-1.5.17.3.8.3.8.5" data-id-title="To Replace the Operating System Disk Drive"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.6.1 </span><span class="title-name">To Replace the Operating System Disk Drive</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.8.3.8.5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After the operating system disk drive is replaced, the node must be
   reimaged.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Update your Cobbler profile:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem"><p>
     Reimage the node using this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server name&gt;</pre></div><p>
     In the example below <span class="bold"><strong>swobj2</strong></span> server is
     reimaged:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj2</pre></div></li><li class="listitem"><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     In the following example, for <span class="bold"><strong>swobj2</strong></span>, the
     hostname is <span class="bold"><strong>ardana-cp1-swobj0002</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit ardana-cp1-swobj0002*</pre></div></li><li class="listitem"><p>
     If this is the first server running the swift-proxy service, restore the
     swift Ring Builder files to the
     <code class="literal">/etc/swiftlm/<em class="replaceable">CLOUD-NAME</em>/<em class="replaceable">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For more
     information and instructions, see <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a> and
     <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a>.
    </p></li><li class="listitem"><p>
     Configure services on the node using the <code class="literal">ardana-deploy.yml</code>
     playbook. If you have used an encryption password when running the
     configuration processor include the <code class="literal">--ask-vault-pass</code>
     argument.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass \
  --limit &lt;hostname&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit ardana-cp1-swobj0002*</pre></div></li></ol></div></section><section class="sect5" id="id-1.5.17.3.8.3.8.6" data-id-title="To Replace a Storage Disk Drive"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.1.5.1.6.2 </span><span class="title-name">To Replace a Storage Disk Drive</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.3.8.3.8.6">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After a storage drive is replaced, there is no need to reimage the server.
   Instead, run the <code class="literal">swift-reconfigure.yml</code> playbook.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log onto the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit &lt;hostname&gt;</pre></div><p>
     In following example, the server used is
     <span class="bold"><strong>swobj2</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit ardana-cp1-swobj0002-mgmt</pre></div></li></ol></div></section></section></section></section><section class="sect2" id="mariadb-manual-update" data-id-title="Updating MariaDB with Galera"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.1.6 </span><span class="title-name">Updating MariaDB with Galera</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#mariadb-manual-update">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-mariadb-manual-update.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Updating MariaDB with Galera must be done manually. Updates are not
  installed automatically. This is particularly an issue with upgrades to
  MariaDB 10.2.17 or higher from MariaDB 10.2.16 or earlier. See
  <a class="link" href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/" target="_blank">MariaDB
  10.2.22 Release Notes - Notable Changes</a>.
 </p><p>
  Using the CLI, update MariaDB with the following procedure:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Mark Galera as unmanaged:
   </p><div class="verbatim-wrap"><pre class="screen">crm resource unmanage galera</pre></div><p>
    Or put the whole cluster into maintenance mode:
   </p><div class="verbatim-wrap"><pre class="screen">crm configure property maintenance-mode=true</pre></div></li><li class="step"><p>
    Pick a node other than the one currently targeted by the loadbalancer and
    stop MariaDB on that node:
   </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-demote -r galera -V</pre></div></li><li class="step"><p>
    Perform updates:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Uninstall the old versions of MariaDB and the Galera wsrep provider.
     </p></li><li class="step"><p>
      Install the new versions of MariaDB and the Galera wsrep provider.
      Select the appropriate instructions at
      <a class="link" href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/" target="_blank">Installing
      MariaDB with zypper</a>.
     </p></li><li class="step"><p>
      Change configuration options if necessary.
     </p></li></ol></li><li class="step"><p>
    Start MariaDB on the node.
   </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-promote -r galera -V</pre></div></li><li class="step"><p>
    Run <code class="command">mysql_upgrade</code> with the
    <code class="literal">--skip-write-binlog</code> option.
   </p></li><li class="step"><p>
    On the other nodes, repeat the process detailed above: stop MariaDB,
    perform updates, start MariaDB, run <code class="command">mysql_upgrade</code>.
   </p></li><li class="step"><p>
    Mark Galera as managed:
   </p><div class="verbatim-wrap"><pre class="screen">crm resource manage galera</pre></div><p>
    Or take the cluster out of maintenance mode.
   </p></li></ol></div></div></section></section><section class="sect1" id="unplanned-maintenance" data-id-title="Unplanned System Maintenance"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.2 </span><span class="title-name">Unplanned System Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#unplanned-maintenance">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-unplanned_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Unplanned maintenance tasks for your cloud.
 </p><section class="sect2" id="whole-unplanned" data-id-title="Whole Cloud Recovery Procedures"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.1 </span><span class="title-name">Whole Cloud Recovery Procedures</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#whole-unplanned">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-whole_unplanned.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Unplanned maintenance procedures for your whole cloud.
 </p><section class="sect3" id="full-recovery" data-id-title="Full Disaster Recovery"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1 </span><span class="title-name">Full Disaster Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#full-recovery">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In this disaster scenario, you have lost everything in your cloud. In other
  words, you have lost access to all data stored in the cloud that was not
  backed up to an external backup location, including:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Data in swift object storage
   </p></li><li class="listitem"><p>
    glance images
   </p></li><li class="listitem"><p>
    cinder volumes
   </p></li><li class="listitem"><p>
    Metering, Monitoring, and Logging (MML) data
   </p></li><li class="listitem"><p>
    Workloads running on compute resources
   </p></li></ul></div><p>
  In effect, the following recovery process creates a minimal new cloud with
  the existing identity information. Much of the operating state and data would
  have been lost, as would running workloads.
 </p><div id="id-1.5.17.4.3.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
   We recommend backups external to your cloud for your data, including as much
   as possible of the types of resources listed above. Most workloads that were
   running could possibly be recreated with sufficient external backups.
  </p></div><section class="sect4" id="id-1.5.17.4.3.3.6" data-id-title="Install and Set Up a Cloud Lifecycle Manager Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.1 </span><span class="title-name">Install and Set Up a Cloud Lifecycle Manager Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.6">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before beginning the process of a full cloud recovery, you need to install
   and set up a Cloud Lifecycle Manager node as though you are creating a new cloud. There are
   several steps in that process:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install the appropriate version of SUSE Linux Enterprise Server
    </p></li><li class="step"><p>
     Restore <code class="literal">passwd</code>, <code class="literal">shadow</code>, and
     <code class="literal">group</code> files. They have User ID (UID) and group ID (GID)
     content that will be used to set up the new cloud. If these are not
     restored immediately after installing the operating system, the cloud
     deployment will create new UIDs and GIDs, overwriting the existing
     content.
    </p></li><li class="step"><p>
     Install Cloud Lifecycle Manager software
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager, which includes installing the necessary packages
    </p></li><li class="step"><p>
     Initialize the Cloud Lifecycle Manager
    </p></li><li class="step"><p>
     Restore your OpenStack git repository
    </p></li><li class="step"><p>
     Adjust input model settings if the hardware setup has changed
    </p></li></ol></div></div><p>
   The following sections cover these steps in detail.
  </p></section><section class="sect4" id="id-1.5.17.4.3.3.7" data-id-title="Install the Operating System"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.2 </span><span class="title-name">Install the Operating System</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.7">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Follow the instructions for installing SUSE Linux Enterprise Server in
   <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”</span>.
  </p></section><section class="sect4" id="id-1.5.17.4.3.3.8" data-id-title="Restore files with UID and GID content"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.3 </span><span class="title-name">Restore files with UID and GID content</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.8">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.5.17.4.3.3.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    There is a risk that you may lose data completely. Restore the backups for
    <code class="filename">/etc/passwd</code>, <code class="filename">/etc/shadow</code>, and
    <code class="filename">/etc/group</code> immediately after installing SUSE Linux Enterprise Server.
   </p></div><p>
   Some backup files contain content that would no longer be valid if your
   cloud were to be freshly deployed in the next step of a whole cloud
   recovery. As a result, some of the backup must be restored before deploying
   a new cloud. Three kinds of backups are involved: passwd, shadow, and group.
   The following steps will restore those backups.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the server where the Cloud Lifecycle Manager will be installed.
    </p></li><li class="step"><p>
     Retrieve the Cloud Lifecycle Manager backups from the remote server, which were created and
     saved during <a class="xref" href="bura-overview.html#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp <em class="replaceable">USER@REMOTE_SERVER</em>:<em class="replaceable">TAR_ARCHIVE</em></pre></div></li><li class="step"><p>
     Untar the TAR archives to overwrite the three locations:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       passwd
      </p></li><li class="listitem"><p>
       shadow
      </p></li><li class="listitem"><p>
       group
      </p></li></ul></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory <em class="replaceable">RESTORE_TARGET</em> -f <em class="replaceable">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     The following are examples. Use the actual <code class="filename">tar.gz</code>
     file names of the backups.
    </p><p>
     BACKUP_TARGET=<code class="filename">/etc/passwd</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ -f passwd.tar.gz</pre></div><p>
     BACKUP_TARGET=<code class="filename">/etc/shadow</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ -f shadow.tar.gz</pre></div><p>
     BACKUP_TARGET=<code class="filename">/etc/group</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ -f group.tar.gz</pre></div></li></ol></div></div></section><section class="sect4" id="id-1.5.17.4.3.3.9" data-id-title="Install the Cloud Lifecycle Manager"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.4 </span><span class="title-name">Install the Cloud Lifecycle Manager</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.9">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that was previously
   loaded on your Cloud Lifecycle Manager, download and install the Cloud Lifecycle Manager software using the
   instructions from <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span>.
  </p></section><section class="sect4" id="id-1.5.17.4.3.3.10" data-id-title="Prepare to deploy your cloud"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.5 </span><span class="title-name">Prepare to deploy your cloud</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.10">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following is the general process for preparing to deploy a SUSE <span class="productname">OpenStack</span> Cloud. You
   may not need to perform all the steps, depending on your particular disaster
   recovery situation.
  </p><div id="id-1.5.17.4.3.3.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    When you install the <code class="literal">ardana cloud pattern</code> in the
    following process, the <code class="literal">ardana</code> user and
    <code class="literal">ardana</code> group will already exist in
    <code class="filename">/etc/passwd</code> and <code class="filename">/etc/group</code>. Do
    not re-create them.
   </p><p>
    When you run <code class="literal">ardana-init</code> in the following process,
    <code class="filename">/var/lib/ardana</code> is created as a deployer account using
    the account settings in <code class="filename">/etc/passwd</code> and
    <code class="filename">/etc/group</code> that were restored in the previous step.
   </p></div><section class="sect5" id="id-1.5.17.4.3.3.10.4" data-id-title="Prepare for Cloud Installation"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.5.1 </span><span class="title-name">Prepare for Cloud Installation</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.10.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Review the <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 14 “Pre-Installation Checklist”</span> about recommended
     pre-installation tasks.
    </p></li><li class="step"><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”</span>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step"><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 16 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”</span> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 17 “Software Repository Setup”</span>.
      </p></li><li class="step"><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.4 “Creating a User”</span>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable">CLOUD</em> with your user name
       choice.
      </p></li><li class="step"><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step"><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step"><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step"><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></section></section><section class="sect4" id="id-1.5.17.4.3.3.11" data-id-title="Restore the remaining Cloud Lifecycle Manager content from a remote backup"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.6 </span><span class="title-name">Restore the remaining Cloud Lifecycle Manager content from a remote backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.11">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Retrieve the Cloud Lifecycle Manager backups from the remote server, which were created and
     saved during <a class="xref" href="bura-overview.html#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp <em class="replaceable">USER@REMOTE_SERVER</em>:<em class="replaceable">TAR_ARCHIVE</em></pre></div></li><li class="step"><p>
     Untar the TAR archives to overwrite the remaining four required locations:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       home
      </p></li><li class="listitem"><p>
       ssh
      </p></li></ul></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory <em class="replaceable">RESTORE_TARGET</em> -f <em class="replaceable">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     The following are examples. Use the actual <code class="filename">tar.gz</code>
     file names of the backups.
    </p><p>
     BACKUP_TARGET=<code class="filename">/var/lib/ardana</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /var/lib/ -f home.tar.gz</pre></div><p>
     BACKUP_TARGET=/etc/ssh/
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</pre></div></li></ol></div></div></section><section class="sect4" id="id-1.5.17.4.3.3.12" data-id-title="Re-deployment of controllers 1, 2 and 3"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.7 </span><span class="title-name">Re-deployment of controllers 1, 2 and 3</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.12">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Change back to the default ardana user.
    </p></li><li class="step"><p>
     Run the <code class="filename">cobbler-deploy.yml</code> playbook.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
     Run the <code class="filename">bm-reimage.yml</code> playbook limited to the second
     and third controllers.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2,controller3</pre></div><p>
     The names of controller2 and controller3. Use the
     <code class="filename">bm-power-status.yml</code> playbook to check the cobbler
     names of these nodes.
    </p></li><li class="step"><p>
     Run the <code class="filename">site.yml</code> playbook limited to the three
     controllers and localhost—in this example,
     <code class="literal">doc-cp1-c1-m1-mgmt</code>,
     <code class="literal">doc-cp1-c1-m2-mgmt</code>,
     <code class="literal">doc-cp1-c1-m3-mgmt</code>, and <code class="literal">localhost</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step"><p>
     You can now perform the procedures to restore MariaDB and swift.
    </p></li></ol></div></div></section><section class="sect4" id="id-1.5.17.4.3.3.13" data-id-title="Restore MariaDB from a remote backup"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.8 </span><span class="title-name">Restore MariaDB from a remote backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.13">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the first node running the MariaDB service.
    </p></li><li class="step"><p>
     Retrieve the MariaDB backup that was created with
     <a class="xref" href="bura-overview.html#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>.
    </p></li><li class="step"><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">mydb.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</pre></div></li><li class="step"><p>
     Verify that the files have been restored on the controller.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li><li class="step"><p>
     Stop <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers (using the hostnames
     of the controllers in your configuration).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step"><p>
     Delete the files in the <code class="filename">mysql</code> directory and copy the
     restored backup to that directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cd /var/lib/mysql/
<code class="prompt user">root # </code>rm -rf ./*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* ./</pre></div></li><li class="step"><p>
     Switch back to the <code class="literal">ardana</code> user when the copy is
     finished.
    </p></li></ol></div></div></section><section class="sect4" id="id-1.5.17.4.3.3.14" data-id-title="Restore swift from a remote backup"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.9 </span><span class="title-name">Restore swift from a remote backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.14">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the first swift Proxy
     (<code class="literal">SWF-PRX--first-member</code>) node.
    </p><p>
     To find the first swift Proxy node:
    </p><ol type="a" class="substeps"><li class="step"><p>
       On the Cloud Lifecycle Manager
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd  ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit SWF-PRX--first-member</pre></div><p>
       At the end of the output, you will see something like the following
       example:
      </p><div class="verbatim-wrap"><pre class="screen">...
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:max-latency: 0.679254770279 (at 1529352109.66)'
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:avg-latency: 0.679254770279 (at 1529352109.66)'

PLAY RECAP ********************************************************************
ardana-qe102-cp1-c1-m1 : ok=12 changed=0 unreachable=0 failed=0```</pre></div></li><li class="step"><p>
       Find the first node name and its IP address. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana-qe102-cp1-c1-m1</pre></div></li></ol></li><li class="step"><p>
     Retrieve (<code class="command">scp</code>) the swift backup that was created with
     <a class="xref" href="bura-overview.html#swift-ring-backup" title="17.3.3. swift Ring Backup">Section 17.3.3, “swift Ring Backup”</a>.
    </p></li><li class="step"><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">swring.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/swift_builder_dir_restore; sudo tar -z \
--incremental --extract --ignore-zeros --warning=none --overwrite --directory \
/tmp/swift_builder_dir_restore/  -f swring.tar.gz</pre></div></li><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Stop the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml</pre></div></li><li class="step"><p>
     Log back in to the first swift Proxy
     (<code class="literal">SWF-PRX--first-member</code>) node, which was determined in
     <a class="xref" href="system-maintenance.html#swift-nodes" title="Step 1">Step 1</a>.
    </p></li><li class="step"><p>
     Copy the restored files.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>
     For example
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step"><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Reconfigure the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></section><section class="sect4" id="id-1.5.17.4.3.3.15" data-id-title="Restart SUSE OpenStack Cloud services"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.1.1.10 </span><span class="title-name">Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.3.3.15">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Restart the MariaDB database
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div><p>
     On the deployer node, execute the
     <code class="filename">galera-bootstrap.yml</code> playbook which will determine
     the log sequence number, bootstrap the main node, and start the database
     cluster.
    </p><p>
     If this process fails to recover the database cluster, refer to
     <a class="xref" href="system-maintenance.html#mysql" title="15.2.3.1.2. Recovering the MariaDB Database">Section 15.2.3.1.2, “Recovering the MariaDB Database”</a>.
    </p></li><li class="step"><p>
     Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers as in the
     following example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml \
--limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step"><p>
     Reconfigure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div></section></section></section><section class="sect2" id="recover-failed-boot-processes" data-id-title="Recover Start-up Processes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.2 </span><span class="title-name">Recover Start-up Processes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#recover-failed-boot-processes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-recover-boot-processes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In this scenario, processes do not start. If those processes are not running,
  ansible start-up scripts will fail. On the deployer, use
  <code class="literal">Ansible</code> to check status on the control plane servers. The
  following checks and remedies address common causes of this condition.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    If disk space is low, determine the cause and remove anything that is no
    longer needed. Check disk space with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -m shell -a 'df -h'</pre></div></li><li class="listitem"><p>
    Check that Network Time Protocol (NTP) is synchronizing clocks properly
    with the following command.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible resources -i hosts/verb_hosts \
-m shell -a "sudo ntpq -c peers"</pre></div></li><li class="listitem"><p>
    Check <code class="literal">keepalived</code>, the daemon that monitors services or
    systems and automatically fails over to a standby if problems occur.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status keepalived | head -8"</pre></div></li><li class="listitem"><p>
    Restart <code class="literal">keepalived</code> if necessary.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Check RabbitMQ status first:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo rabbitmqctl status | head -10"</pre></div></li><li class="step"><p>
      Restart RabbitMQ if necessary:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl start rabbitmq-server"</pre></div></li><li class="step"><p>
      If RabbitMQ is running, restart <code class="literal">keepalived</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl restart keepalived"</pre></div></li></ol></div></div></li><li class="listitem"><p>
    If RabbitMQ is up, is it clustered?
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo rabbitmqctl cluster_status"</pre></div><p>
    Restart RabbitMQ cluster if necessary:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible_playbook -i hosts/verb_hosts rabbitmq-start.yml</pre></div></li><li class="listitem"><p>
    Check <code class="literal">Kafka</code> messaging:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status kafka | head -5"</pre></div></li><li class="listitem"><p>
    Check the <code class="literal">Spark</code> framework:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status spark-worker | head -8"
<code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status spark-master | head -8"</pre></div></li><li class="listitem"><p>
    If necessary, start <code class="literal">Spark</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml
<code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts -m shell -a \
"sudo systemctl start spark-master | head -8"</pre></div></li><li class="listitem"><p>
    Check <code class="literal">Zookeeper</code> centralized service:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status zookeeper| head -8"</pre></div></li><li class="listitem"><p>
    Check MariaDB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts
-m shell -a "sudo mysql -e 'show status;' | grep -e wsrep_incoming_addresses \
-e wsrep_local_state_comment "</pre></div></li></ul></div></section><section class="sect2" id="cont-ungplanned" data-id-title="Unplanned Control Plane Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.3 </span><span class="title-name">Unplanned Control Plane Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#cont-ungplanned">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-cont_unplanned.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Unplanned maintenance tasks for controller nodes such as recovery from power
  failure.
 </p><section class="sect3" id="recover-downed-cluster" data-id-title="Restarting Controller Nodes After a Reboot"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.2.3.1 </span><span class="title-name">Restarting Controller Nodes After a Reboot</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#recover-downed-cluster">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Steps to follow if one or more of your controller nodes lose network
  connectivity or power, which includes if the node is either rebooted or needs
  hardware maintenance.
 </p><p>
  When a controller node is rebooted, needs hardware maintenance, loses
  network connectivity or loses power, these steps will help you recover the
  node.
 </p><p>
  These steps may also be used if the Host Status (ping) alarm is triggered
  for one or more of your controller nodes.
 </p><section class="sect4" id="idg-all-operations-maintenance-controller-restart-controller-xml-7" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.1.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-controller-restart-controller-xml-7">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following conditions must be true in order to perform these steps
   successfully:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Each of your controller nodes should be powered on.
    </p></li><li class="listitem"><p>
     Each of your controller nodes should have network connectivity, verified
     by SSH connectivity from the Cloud Lifecycle Manager to them.
    </p></li><li class="listitem"><p>
     The operator who performs these steps will need access to the Cloud Lifecycle Manager.
    </p></li></ul></div></section><section class="sect4" id="mysql" data-id-title="Recovering the MariaDB Database"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.1.2 </span><span class="title-name">Recovering the MariaDB Database</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#mysql">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The recovery process for your MariaDB database cluster will depend on how
   many of your controller nodes need to be recovered. We will cover two
   scenarios:
  </p><p>
   <span class="bold"><strong>Scenario 1: Recovering one or two of your controller
   nodes but not the entire cluster</strong></span>
  </p><p>
   Follow these steps to recover one or two of your controller nodes but not the
   entire cluster, then use these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Ensure the controller nodes have power and are booted to the command
     prompt.
    </p></li><li class="step"><p>
     If the MariaDB service is not started, start it with this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo service mysql start</pre></div></li><li class="step"><p>
     If MariaDB fails to start, proceed to the next section which covers the
     bootstrap process.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Scenario 2: Recovering the entire controller cluster
   with the bootstrap playbook</strong></span>
  </p><p>
   If the scenario above failed or if you need to recover your entire control
   plane cluster, use the process below to recover the MariaDB database.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure no <code class="literal">mysqld</code> daemon is running on any node in
     the cluster before you continue with the steps in this procedure. If there
     is a <code class="literal">mysqld</code> daemon running, then use the command below
     to shut down the daemon.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl stop mysql</pre></div><p>
     If the mysqld daemon does not go down following the service stop, then
     kill the daemon using <code class="literal">kill -9</code> before continuing.
    </p></li><li class="step"><p>
     On the deployer node, execute the
     <code class="filename">galera-bootstrap.yml</code> playbook which will
     automatically determine the log sequence number, bootstrap the main node,
     and start the database cluster.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li></ol></div></div></section><section class="sect4" id="hlm" data-id-title="Restarting Services on the Controller Nodes"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.1.3 </span><span class="title-name">Restarting Services on the Controller Nodes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#hlm">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   From the Cloud Lifecycle Manager you should execute the
   <code class="literal">ardana-start.yml</code> playbook for each node that was brought
   down so the services can be started back up.
  </p><p>
   If you have a dedicated (separate) Cloud Lifecycle Manager node you can use this
   syntax:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;</pre></div><p>
   If you have a shared Cloud Lifecycle Manager/controller setup and need to restart
   services on this shared node, you can use <code class="literal">localhost</code> to
   indicate the shared node, like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;,localhost</pre></div><div id="id-1.5.17.4.5.3.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    If you leave off the <code class="literal">--limit</code> switch, the playbook will
    be run against all nodes.
   </p></div></section><section class="sect4" id="monasca" data-id-title="Restart the Monitoring Agents"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.1.4 </span><span class="title-name">Restart the Monitoring Agents</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#monasca">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As part of the recovery process, you should also restart the
   <code class="literal">monasca-agent</code> and these steps will show you how:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Stop the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml</pre></div></li><li class="step"><p>
     Restart the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml</pre></div></li><li class="step"><p>
     You can then confirm the status of the <code class="literal">monasca-agent</code>
     with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div></section></section><section class="sect3" id="recovering-controller-nodes" data-id-title="Recovering the Control Plane"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2 </span><span class="title-name">Recovering the Control Plane</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#recovering-controller-nodes">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-recovering_controller_nodes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If one or more of your controller nodes has experienced data or disk
  corruption due to power loss or hardware failure and you need perform
  disaster recovery, there are several scenarios for recovering your cloud.
 </p><div id="id-1.5.17.4.5.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   If you backed up the Cloud Lifecycle Manager manually after installation (see <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 38 “Post Installation Tasks”</span>, you will have a backup copy of
   <code class="filename">/etc/group</code>. When recovering a Cloud Lifecycle Manager node, manually copy
   the <code class="filename">/etc/group</code> file from a backup of the old Cloud Lifecycle Manager.
  </p></div><section class="sect4" id="pit-database-recovery" data-id-title="Point-in-Time MariaDB Database Recovery"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.1 </span><span class="title-name">Point-in-Time MariaDB Database Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#pit-database-recovery">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, cloud controller nodes,
  and compute nodes) but you want to restore the MariaDB database to a
  previous state.
 </p><section class="sect5" id="id-1.5.17.4.5.4.4.3" data-id-title="Restore MariaDB manually"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.1.1 </span><span class="title-name">Restore MariaDB manually</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.5.4.4.3">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Follow this procedure to manually restore MariaDB:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Stop the MariaDB cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step"><p>
     On all of the nodes running the MariaDB service, which should be all of
     your controller nodes, run the following command to purge the old
     database:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -r /var/lib/mysql/*</pre></div></li><li class="step"><p>
     On the first node running the MariaDB service restore the backup with
     the command below. If you have already restored to a temporary directory,
     copy the files again.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</pre></div></li><li class="step"><p>
     If you need to restore the files manually from SSH, follow these steps:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Log in to the first node running the MariaDB service.
      </p></li><li class="step"><p>
       Retrieve the MariaDB backup that was created with <a class="xref" href="bura-overview.html#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>.
       </p></li><li class="step"><p>
        Create a temporary directory and extract the TAR archive (for example,
       <code class="filename">mydb.tar.gz</code>).
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</pre></div></li><li class="step"><p>
        Verify that the files have been restored on the controller.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li></ol></li><li class="step"><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Start the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step"><p>
     After approximately 10-15 minutes, the output of the
     <code class="literal">percona-status.yml</code> playbook should show all the
     MariaDB nodes in sync. MariaDB cluster status can be checked using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div><p>
     An example output is as follows:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] *************
  ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m2-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m3-mgmt] =&gt; {
  "msg": "mysql is synced."
  }</pre></div></li></ol></div></div></section><section class="sect5" id="id-1.5.17.4.5.4.4.4" data-id-title="Point-in-Time Cassandra Recovery"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.1.2 </span><span class="title-name">Point-in-Time Cassandra Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.5.4.4.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A node may have been removed either due to an intentional action in the
   Cloud Lifecycle Manager Admin UI or as a result of a fatal hardware event that requires a
   server to be replaced. In either case, the entry for the failed or deleted
   node should be removed from Cassandra before a new node is brought up.
  </p><p>
   The following steps should be taken before enabling and deploying the
   replacement node.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Determine the IP address of the node that was removed or is being replaced.
    </p></li><li class="step"><p>
     On one of the functional Cassandra control plane nodes, log in as the
     <code class="literal">ardana</code> user.
    </p></li><li class="step"><p>
     Run the command <code class="command">nodetool status</code> to display a list of
     Cassandra nodes.
    </p></li><li class="step"><p>
     If the node that has been removed (no IP address matches that of the
     removed node) is not in the list, skip the next step.
    </p></li><li class="step"><p>
     If the node that was removed is still in the list, copy its node
     <em class="replaceable">ID</em>.
    </p></li><li class="step"><p>
     Run the command <code class="command">nodetool removenode
     <em class="replaceable">ID</em></code>.
    </p></li></ol></div></div><p>
   After any obsolete node entries have been removed, the replacement node can
   be deployed as usual (for more information, see <a class="xref" href="system-maintenance.html#cont-planned" title="15.1.2. Planned Control Plane Maintenance">Section 15.1.2, “Planned Control Plane Maintenance”</a>). The new Cassandra node will be able to join the
   cluster and replicate data.
  </p><p>
   For more information, please consult <a class="link" href="http://cassandra.apache.org/doc/latest/operating/topo_changes.html" target="_blank">the
   Cassandra documentation</a>.
  </p></section></section><section class="sect4" id="pit-swiftrings-recovery" data-id-title="Point-in-Time swift Rings Recovery"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.2 </span><span class="title-name">Point-in-Time swift Rings Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#pit-swiftrings-recovery">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In this situation, everything is still running (Cloud Lifecycle Manager, control plane nodes,
  and compute nodes) but you want to restore your swift rings to a previous
  state.
 </p><div id="id-1.5.17.4.5.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   This process restores swift rings only, not swift data.
  </p></div><section class="sect5" id="restore-swift-bu" data-id-title="Restore from a swift backup"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.2.1 </span><span class="title-name">Restore from a swift backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#restore-swift-bu">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step" id="swift-nodes"><p>
     Log in to the first swift Proxy (<code class="literal">SWF-PRX--first-member</code>) node.
    </p><p>
     To find the first swift Proxy node:
    </p><ol type="a" class="substeps"><li class="step"><p>
       On the Cloud Lifecycle Manager
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd  ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit SWF-PRX--first-member</pre></div><p>
       At the end of the output, you will see something like the following
       example:
      </p><div class="verbatim-wrap"><pre class="screen">...
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:max-latency: 0.679254770279 (at 1529352109.66)'
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:avg-latency: 0.679254770279 (at 1529352109.66)'

PLAY RECAP ********************************************************************
ardana-qe102-cp1-c1-m1 : ok=12 changed=0 unreachable=0 failed=0```</pre></div></li><li class="step"><p>
       Find the first node name and its IP address. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana-qe102-cp1-c1-m1</pre></div></li></ol></li><li class="step"><p>
     Retrieve (<code class="command">scp</code>) the swift backup that was created with
     <a class="xref" href="bura-overview.html#swift-ring-backup" title="17.3.3. swift Ring Backup">Section 17.3.3, “swift Ring Backup”</a>.
    </p></li><li class="step"><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">swring.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/swift_builder_dir_restore; sudo tar -z \
--incremental --extract --ignore-zeros --warning=none --overwrite --directory \
/tmp/swift_builder_dir_restore/  -f swring.tar.gz</pre></div></li><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Stop the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml</pre></div></li><li class="step"><p>
     Log back in to the first swift Proxy (<code class="literal">SWF-PRX--first-member</code>)
     node, which was determined in <a class="xref" href="system-maintenance.html#swift-nodes" title="Step 1">Step 1</a>.
    </p></li><li class="step"><p>
     Copy the restored files.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step"><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Reconfigure the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></section></section><section class="sect4" id="pit-lifecyclemanager-recovery" data-id-title="Point-in-time Cloud Lifecycle Manager Recovery"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.3 </span><span class="title-name">Point-in-time Cloud Lifecycle Manager Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#pit-lifecyclemanager-recovery">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_lifecyclemanager_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, controller nodes, and
  compute nodes) but you want to restore the Cloud Lifecycle Manager to a previous state.
 </p><div class="procedure" id="restore-swift-ssh-bu" data-id-title="Restoring from a Swift or SSH Backup"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.1: </span><span class="title-name">Restoring from a Swift or SSH Backup </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#restore-swift-ssh-bu">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_lifecyclemanager_recovery.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step"><p>
    Retrieve the Cloud Lifecycle Manager backups that were created with <a class="xref" href="bura-overview.html#clm-data-backup" title="17.3.1. Cloud Lifecycle Manager Data Backup">Section 17.3.1, “Cloud Lifecycle Manager Data Backup”</a>. There are multiple backups; directories are
    handled differently than files.
    </p></li><li class="step"><p>
     Extract the TAR archives for each of the seven locations.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros
    --warning=none --overwrite --directory
    <em class="replaceable">RESTORE_TARGET</em> -f
    <em class="replaceable">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     For example, with a directory such as
     <em class="replaceable">BACKUP_TARGET</em>=<code class="filename">/etc/ssh/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</pre></div><p>
     With a file such as
     <em class="replaceable">BACKUP_TARGET</em>=<code class="filename">/etc/passwd</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros --warning=none --overwrite --directory /etc/ -f passwd.tar.gz</pre></div></li></ol></div></div></section><section class="sect4" id="lifecyclemanager-recovery" data-id-title="Cloud Lifecycle Manager Disaster Recovery"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.4 </span><span class="title-name">Cloud Lifecycle Manager Disaster Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#lifecyclemanager-recovery">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In this scenario everything is still running (controller nodes and compute
  nodes) but you have lost either a dedicated Cloud Lifecycle Manager or a shared
  Cloud Lifecycle Manager/controller node.
 </p><p>
  To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that was previously
  loaded on your Cloud Lifecycle Manager, download and install the Cloud Lifecycle Manager software using the
  instructions from <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span> before
  proceeding.
 </p><p>
  Prepare the Cloud Lifecycle Manager following the steps in the <code class="literal">Before You
  Start</code> section of <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 21 “Installing with the Install UI”</span>.
 </p><section class="sect5" id="id-1.5.17.4.5.4.7.5" data-id-title="Restore from a remote backup"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.4.1 </span><span class="title-name">Restore from a remote backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.5.4.7.5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
    Retrieve (with <code class="command">scp</code>) the Cloud Lifecycle Manager backups that were created
    with <a class="xref" href="bura-overview.html#clm-data-backup" title="17.3.1. Cloud Lifecycle Manager Data Backup">Section 17.3.1, “Cloud Lifecycle Manager Data Backup”</a>. There are multiple backups;
    directories are handled differently than files.
    </p></li><li class="step"><p>
     Extract the TAR archives for each of the seven locations.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros
    --warning=none --overwrite --directory
    <em class="replaceable">RESTORE_TARGET</em> -f
    <em class="replaceable">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     For example, with a directory such as
     <em class="replaceable">BACKUP_TARGET</em>=<code class="filename">/etc/ssh/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</pre></div><p>
     With a file such as
     <em class="replaceable">BACKUP_TARGET</em>=<code class="filename">/etc/passwd</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros --warning=none --overwrite --directory /etc/ -f passwd.tar.gz</pre></div></li><li class="step"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready_deployment.yml</pre></div></li><li class="step"><p>
     When the Cloud Lifecycle Manager is restored, re-run the deployment to ensure
     the Cloud Lifecycle Manager is in the correct state:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</pre></div></li></ol></div></div></section></section><section class="sect4" id="onetwo-controller-recovery" data-id-title="One or Two Controller Node Disaster Recovery"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.5 </span><span class="title-name">One or Two Controller Node Disaster Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#onetwo-controller-recovery">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This scenario makes the following assumptions:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Your Cloud Lifecycle Manager is still intact and working.
   </p></li><li class="listitem"><p>
    One or two of your controller nodes went down, but not the entire cluster.
   </p></li><li class="listitem"><p>
    The node needs to be rebuilt from scratch, not simply rebooted.
   </p></li></ul></div><section class="sect5" id="id-1.5.17.4.5.4.8.4" data-id-title="Steps to recovering one or two controller nodes"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.5.1 </span><span class="title-name">Steps to recovering one or two controller nodes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.5.4.8.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Ensure that your node has power and all of the hardware is functioning.
    </p></li><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Verify that all of the information in your
     <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file is
     correct for your controller node. You may need to replace the existing
     information if you had to either replacement your entire controller node
     or just pieces of it.
    </p></li><li class="listitem"><p>
     If you made changes to your <code class="literal">servers.yml</code> file then
     commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "editing controller information"</pre></div></li><li class="listitem"><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem"><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem"><p>
     Ensure that Cobbler has the correct system information:
    </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
       If you replaced your controller node with a completely new machine, you
       need to verify that Cobbler has the correct list of controller nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem"><p>
       Remove any controller nodes from Cobbler that no longer exist:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name=&lt;node&gt;</pre></div></li><li class="listitem"><p>
       Add the new node into Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></li><li class="listitem"><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.5.17.4.5.4.8.4.2.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      If you do not know the <code class="literal">&lt;node name&gt;</code> already,
      you can get it by using <code class="command">sudo cobbler system list</code>.
     </p></div><p>
     Before proceeding, look at <span class="bold"><strong>info/server_info.yml</strong></span> to see if the assignment of
     the node you have added is what you expect. It may not be, as nodes will
     not be numbered consecutively if any have previously been removed. To
     prevent loss of data, the configuration processor retains data about
     removed nodes and keeps their ID numbers from being reallocated. For more
     information about how this works, see <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span>.
    </p></li><li class="listitem"><p>
     Run the <code class="filename">wipe_disks.yml</code> playbook to ensure the
     non-OS partitions on your nodes are completely wiped prior to continuing with the
     installation.
    </p><div id="id-1.5.17.4.5.4.8.4.2.9.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      The <code class="filename">wipe_disks.yml</code> playbook is only meant to be run
      on systems immediately after running
      <code class="filename">bm-reimage.yml</code>. If used for any other situation, it
      may not wipe all of the expected partitions.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;controller_node_hostname&gt;</pre></div></li><li class="listitem"><p>
     Complete the rebuilding of your controller node with the two playbooks
     below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;</pre></div></li></ol></div></section></section><section class="sect4" id="three-controller-recovery" data-id-title="Three Control Plane Node Disaster Recovery"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.6 </span><span class="title-name">Three Control Plane Node Disaster Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#three-controller-recovery">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In this scenario, all control plane nodes are down and need to be rebuilt or
  replaced. Restoring from a swift backup is not possible because swift is
  gone.
 </p><section class="sect5" id="id-1.5.17.4.5.4.9.3" data-id-title="Restore from an SSH backup"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.6.1 </span><span class="title-name">Restore from an SSH backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.5.4.9.3">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Deploy the control plane nodes, using the values for your control plane
     node hostnames:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit \
  <em class="replaceable">CONTROL_PLANE_HOSTNAME1</em>,<em class="replaceable">CONTROL_PLANE_HOSTNAME2</em>, \
  <em class="replaceable">CONTROL_PLANE_HOSTNAME3</em> -e rebuild=True</pre></div><p>
     For example, if you were using the default values from the example model
     files, the command would look like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml \
--limit ardana-ccp-c1-m1-mgmt,ardana-ccp-c1-m2-mgmt,ardana-ccp-c1-m3-mgmt \
-e rebuild=True</pre></div><div id="id-1.5.17.4.5.4.9.3.2.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The <code class="literal">-e rebuild=True</code> is only used on a single control
      plane node when there are other controllers available to pull
      configuration data from. This causes the MariaDB database to be
      reinitialized, which is the only choice if there are no additional
      control nodes.
     </p></div></li><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Stop MariaDB:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step"><p>
     Retrieve the MariaDB backup that was created with <a class="xref" href="bura-overview.html#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>.
    </p></li><li class="step"><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">mydb.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</pre></div></li><li class="step"><p>
     Verify that the files have been restored on the controller.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li><li class="step"><p>
     Log back in to the first controller node and move the following files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh <em class="replaceable">FIRST_CONTROLLER_NODE</em>
<code class="prompt user">ardana &gt; </code>sudo su
<code class="prompt user">root # </code>rm -rf /var/lib/mysql/*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* /var/lib/mysql/</pre></div></li><li class="step"><p>
     Log back in to the Cloud Lifecycle Manager and bootstrap MariaDB:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step"><p>
     Verify the status of MariaDB:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div></li></ol></div></div></section></section><section class="sect4" id="swiftrings-recovery" data-id-title="swift Rings Recovery"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.7 </span><span class="title-name">swift Rings Recovery</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#swiftrings-recovery">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To recover the swift rings in the event of a disaster, follow the
  procedure that applies to your situation: either recover the rings with the
  manual swift backup and restore or use the SSH backup.
 </p><section class="sect5" id="id-1.5.17.4.5.4.10.3" data-id-title="Restore from the swift deployment backup"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.7.1 </span><span class="title-name">Restore from the swift deployment backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.5.4.10.3">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   See <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a>.
  </p></section><section class="sect5" id="id-1.5.17.4.5.4.10.4" data-id-title="Restore from the SSH backup"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.3.2.7.2 </span><span class="title-name">Restore from the SSH backup</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.4.5.4.10.4">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In case you have lost all system disks of all object nodes and swift proxy
   nodes are corrupted, you can recover the rings from a copy of the swift
   rings was backed up previously. swift data is still available (the disks
   used by swift still need to be accessible).
  </p><p>
   Recover the rings with these steps.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to a swift proxy node.
    </p></li><li class="step"><p>
     Become root:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo su</pre></div></li><li class="step"><p>
     Create the temporary directory for your restored files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /tmp/swift_builder_dir_restore/</pre></div></li><li class="step"><p>
     Retrieve (<code class="command">scp</code>) the swift backup that was created with
     <a class="xref" href="bura-overview.html#swift-ring-backup" title="17.3.3. swift Ring Backup">Section 17.3.3, “swift Ring Backup”</a>.
    </p></li><li class="step"><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">swring.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/swift_builder_dir_restore; sudo tar -z \
--incremental --extract --ignore-zeros --warning=none --overwrite --directory \
/tmp/swift_builder_dir_restore/  -f swring.tar.gz</pre></div><p>
     You now have the swift rings in
     <code class="literal">/tmp/swift_builder_dir_restore/</code>
    </p></li><li class="step"><p>
     If the SWF-PRX--first-member is already deployed, copy the contents of the
     restored directory
     (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</code>)
     to
     <code class="literal">/etc/swiftlm/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</code>
     on the SWF-PRX--first-member.
    </p></li><li class="step"><p>
      Then from the Cloud Lifecycle Manager run:
     </p></li><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/* \
/etc/swiftlm/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
/etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="step"><p>
     If the SWF-ACC--first-member is<span class="bold"><strong> not </strong></span>deployed, from
     the Cloud Lifecycle Manager run these playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;SWF-ACC[0]-hostname&gt;</pre></div></li><li class="step"><p>
     Copy the contents of the restored directory
     (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</code>) to
     <code class="literal">/etc/swiftlm/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</code>
     on the SWF-ACC[0].
    </p><p>
     Create the directories: <code class="literal">/etc/swiftlm/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/* \
/etc/swiftlm/<em class="replaceable">CLOUD_NAME</em>/<em class="replaceable">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
/etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step"><p>
     From the Cloud Lifecycle Manager, run the <code class="filename">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li></ol></div></div></section></section></section></section><section class="sect2" id="unplanned-compute-maintenance" data-id-title="Unplanned Compute Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.4 </span><span class="title-name">Unplanned Compute Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#unplanned-compute-maintenance">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Unplanned maintenance tasks including recovering compute nodes.
 </p><section class="sect3" id="recover-computenode" data-id-title="Recovering a Compute Node"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.2.4.1 </span><span class="title-name">Recovering a Compute Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#recover-computenode">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If one or more of your compute nodes has experienced an issue such as power
  loss or hardware failure, then you need to perform disaster recovery. Here we
  provide different scenarios and how to resolve them to get your cloud
  repaired.
 </p><p>
  Typical scenarios in which you will need to recover a compute node include
  the following:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The node has failed, either because it has shut down has a hardware
    failure, or for another reason.
   </p></li><li class="listitem"><p>
    The node is working but the <code class="literal">nova-compute</code> process is not
    responding, thus instances are working but you cannot manage them (for
    example to delete, reboot, and attach/detach volumes).
   </p></li><li class="listitem"><p>
    The node is fully operational but monitoring indicates a potential issue
    (such as disk errors) that require down time to fix.
   </p></li></ul></div><section class="sect4" id="idg-all-operations-maintenance-compute-recover-compute-node-xml-7" data-id-title="What to do if your compute node is down"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.4.1.1 </span><span class="title-name">What to do if your compute node is down</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-compute-recover-compute-node-xml-7">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   <span class="bold"><strong>Compute node has power but is not powered
   on</strong></span>
  </p><p>
   If your compute node has power but is not powered on, use these steps to
   restore the node:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Obtain the name for your compute node in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="step"><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Compute node is powered on but services are not
   running on it</strong></span>
  </p><p>
   If your compute node is powered on but you are unsure if services are
   running, you can use these steps to ensure that they are running:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Confirm the status of the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-status.yml --limit &lt;hostname&gt;</pre></div></li><li class="step"><p>
     You can start the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div></section><section class="sect4" id="unplanned" data-id-title="Scenarios involving disk failures on your compute nodes"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.4.1.2 </span><span class="title-name">Scenarios involving disk failures on your compute nodes</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#unplanned">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Your compute nodes should have a minimum of two disks, one that is used for
   the operating system and one that is used as the data disk. These are
   defined during the installation of your cloud, in the
   <code class="literal">~/openstack/my_cloud/definition/data/disks_compute.yml</code>
   file on the Cloud Lifecycle Manager. The data disk(s) are where the
   <code class="literal">nova-compute</code> service lives. Recovery scenarios will
   depend on whether one or the other, or both, of these disks experienced
   failures.
  </p><p>
   <span class="bold"><strong>If your operating system disk failed but the data
   disk(s) are okay</strong></span>
  </p><p>
   If you have had issues with the physical volume that nodes your operating
   system you need to ensure that your physical volume is restored and then you
   can use the following steps to restore the operating system:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step"><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="step"><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack host list | grep compute</pre></div></li><li class="step"><p>
     Obtain the status of the <code class="literal">nova-compute</code> service on that
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list --host &lt;hostname&gt;</pre></div></li><li class="step"><p>
     You will likely want to disable provisioning on that node to ensure that
     <code class="literal">nova-scheduler</code> does not attempt to place any additional
     instances on the node while you are repairing it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set –disable --reason "node is being rebuilt" &lt;hostname&gt;</pre></div></li><li class="step"><p>
     Obtain the status of the instances on the compute node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="step"><p>
     Before continuing, you should either evacuate all of the instances off
     your compute node or shut them down. If the instances are booted from
     volumes, then you can use the <code class="literal">nova evacuate</code> or
     <code class="literal">nova host-evacuate</code> commands to do this. See
     <a class="xref" href="system-maintenance.html#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a> for more details on how to do this.
    </p><p>
     If your instances are not booted from volumes, you will need to stop the
     instances using the <code class="literal">openstack server stop</code> command. Because the
     <code class="literal">nova-compute</code> service is not running on the node you
     will not see the instance status change, but the <code class="literal">Task
     State</code> for the instance should change to
     <code class="literal">powering-off</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server stop &lt;instance_uuid&gt;</pre></div><p>
     Verify the status of each of the instances using these commands, verifying
     the <code class="literal">Task State</code> states <code class="literal">powering-off</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server show &lt;instance_uuid&gt;</pre></div></li><li class="step"><p>
     At this point you should be ready with a functioning hard disk in the node
     that you can use for the operating system. Follow these steps:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Obtain the name for your compute node in Cobbler, which you will use in
       subsequent commands when <code class="literal">&lt;node_name&gt;</code> is
       requested:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="step"><p>
       Run the following playbook, ensuring that you specify only your UEFI
       SLES nodes using the nodelist. This playbook will reconfigure Cobbler
       for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step"><p>
       Reimage the compute node with this playbook:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></li><li class="step"><p>
     Once reimaging is complete, use the following playbook to configure the
     operating system and start up services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li><li class="step"><p>
     You should then ensure any instances on the recovered node are in an
     <code class="literal">ACTIVE</code> state. If they are not then use the
     <code class="literal">openstack server start</code> command to bring them to the
     <code class="literal">ACTIVE</code> state:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server start &lt;instance_uuid&gt;</pre></div></li><li class="step"><p>
     Re-enable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set –enable &lt;hostname&gt;</pre></div></li><li class="step"><p>
     Start any instances that you had stopped previously:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server start &lt;instance_uuid&gt;</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>If your data disk(s) failed but the operating system
   disk is okay OR if all drives failed</strong></span>
  </p><p>
   In this scenario your instances on the node are lost. First, follow steps 1
   to 5 and 8 to 9 in the previous scenario.
  </p><p>
   After that is complete, use the <code class="literal">openstack server rebuild</code>
   command to respawn your instances, which will also ensure that they receive
   the same IP address:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server rebuild &lt;instance_uuid&gt;</pre></div></section></section></section><section class="sect2" id="storage-unplanned" data-id-title="Unplanned Storage Maintenance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.2.5 </span><span class="title-name">Unplanned Storage Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#storage-unplanned">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-storage_unplanned.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Unplanned maintenance tasks for storage nodes.
 </p><section class="sect3" id="swift-storage-unplanned" data-id-title="Unplanned swift Storage Maintenance"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">15.2.5.1 </span><span class="title-name">Unplanned swift Storage Maintenance</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#swift-storage-unplanned">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-swift_storage_unplanned.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Unplanned maintenance tasks for swift storage nodes.
 </p><section class="sect4" id="recover-swiftnode" data-id-title="Recovering a Swift Node"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">15.2.5.1.1 </span><span class="title-name">Recovering a Swift Node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#recover-swiftnode">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If one or more of your swift Object or PAC nodes has experienced an issue,
  such as power loss or hardware failure, and you need to perform disaster
  recovery then we provide different scenarios and how to resolve them to get
  your cloud repaired.
 </p><p>
  Typical scenarios in which you will need to repair a swift object or PAC
  node include:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The node has either shut down or been rebooted.
   </p></li><li class="listitem"><p>
    The entire node has failed and needs to be replaced.
   </p></li><li class="listitem"><p>
    A disk drive has failed and must be replaced.
   </p></li></ul></div><section class="sect5" id="idg-all-operations-maintenance-swift-recover-swift-node-xml-5" data-id-title="What to do if your Swift host has shut down or rebooted"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.5.1.1.1 </span><span class="title-name">What to do if your Swift host has shut down or rebooted</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#idg-all-operations-maintenance-swift-recover-swift-node-xml-5">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If your swift host has power but is not powered on, from the lifecycle
   manager you can run this playbook:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem"><p>
     Obtain the name for your swift host in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem"><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div><p>
   Once the node is booted up, swift should start automatically. You can verify
   this with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div><p>
   Any alarms that have triggered due to the host going down should clear
   within 10 minutes. See <a class="xref" href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a> if further
   assistance is needed with the alarms.
  </p></section><section class="sect5" id="replace" data-id-title="How to replace your Swift node"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.5.1.1.2 </span><span class="title-name">How to replace your Swift node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#replace">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If your swift node has irreparable damage and you need to replace the entire
   node in your environment, see <a class="xref" href="system-maintenance.html#replace-swift-node" title="15.1.5.1.5. Replacing a swift Node">Section 15.1.5.1.5, “Replacing a swift Node”</a> for
   details on how to do this.
  </p></section><section class="sect5" id="disk-replacement" data-id-title="How to replace a hard disk in your Swift node"><div class="titlepage"><div><div><div class="title-container"><h6 class="title"><span class="title-number-name"><span class="title-number">15.2.5.1.1.3 </span><span class="title-name">How to replace a hard disk in your Swift node</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#disk-replacement">#</a></h6><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you need to do a hard drive replacement in your swift node, see
   <a class="xref" href="system-maintenance.html#replacing-disks" title="15.1.5.1.6. Replacing Drives in a swift Node">Section 15.1.5.1.6, “Replacing Drives in a swift Node”</a> for details on how to do this.
  </p></section></section></section></section></section><section class="sect1" id="maintenance-update" data-id-title="Cloud Lifecycle Manager Maintenance Update Procedure"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.3 </span><span class="title-name">Cloud Lifecycle Manager Maintenance Update Procedure</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#maintenance-update">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.5.17.5.2" data-id-title="Preparing for Update"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.2: </span><span class="title-name">Preparing for Update </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Ensure that the update repositories have been properly set up on all nodes.
    The easiest way to provide the required repositories on the Cloud Lifecycle Manager Server is
    to set up an SMT server as described in
    <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 16 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”</span>. Alternatives to setting up an
    SMT server are described in <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 17 “Software Repository Setup”</span>.
   </p></li><li class="step"><p>
    Read the Release Notes for the security and maintenance updates that will
    be installed.
   </p></li><li class="step"><p>
    Have a backup strategy in place. For further information, see
    <a class="xref" href="bura-overview.html" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
   </p></li><li class="step"><p>
    Ensure that you have a known starting state by resolving any unexpected
    alarms.
   </p></li><li class="step"><p>
    Determine if you need to reboot your cloud after updating the software.
    Rebooting is highly recommended to ensure that all affected services are
    restarted. Reboot may be required after installing Linux kernel updates,
    but it can be skipped if the impact on running services is non-existent or
    well understood.
   </p></li><li class="step"><p>
    Review steps in <a class="xref" href="system-maintenance.html#add-network-node" title="15.1.4.1. Adding a Network Node">Section 15.1.4.1, “Adding a Network Node”</a> and
    <a class="xref" href="system-maintenance.html#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a> to minimize the impact on existing
    workloads. These steps are critical when the neutron services are not
    provided via external SDN controllers.
   </p></li><li class="step"><p>
    Before the update, prepare your working loads by consolidating all of your
    instances to one or more Compute Nodes. After the update is complete on the
    evacuated Compute Nodes, reboot them and move the images from the remaining
    Compute Nodes to the newly booted ones. Then, update the remaining
    Compute Nodes.
   </p></li></ol></div></div><section class="sect2" id="perform-update" data-id-title="Performing the Update"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.3.1 </span><span class="title-name">Performing the Update</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#perform-update">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Before you proceed, get the status of all your services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div><p>
     If status check returns an error for a specific service, run the
     <code class="filename"><em class="replaceable">SERVICE</em>-reconfigure.yml</code>
     playbook. Then run the
     <code class="filename"><em class="replaceable">SERVICE</em>-status.yml</code>
     playbook to check that the issue has been resolved.
    </p><p>
     Update and reboot all nodes in the cloud one by one. Start with the
     deployer node, then follow the order recommended in
     <a class="xref" href="system-maintenance.html#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>.
    </p><div id="id-1.5.17.5.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The described workflow also covers cases in which the deployer node is
      also provisioned as an active cloud node.
     </p></div><p>
     To minimize the impact on the existing workloads, the node should first be
     prepared for an update and a subsequent reboot by following the steps
     leading up to stopping services listed in
     <a class="xref" href="system-maintenance.html#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>, such as migrating singleton agents on
     Control Nodes and evacuating Compute Nodes. Do not stop services running on
     the node, as they need to be running during the update.
    </p><div class="procedure" id="id-1.5.17.5.3.8" data-id-title="Update Instructions"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.3: </span><span class="title-name">Update Instructions </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.5.3.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Install all available security and maintenance updates on the deployer
       using the <code class="command">zypper patch</code> command.
      </p></li><li class="step"><p>
       Initialize the Cloud Lifecycle Manager and prepare the update playbooks.
      </p><ol type="a" class="substeps"><li class="step"><p>
         Run the <code class="systemitem">ardana-init</code> initialization script to
         update the deployer.
        </p></li><li class="step"><p>
         Redeploy cobbler:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step"><p>
         Run the configuration processor:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step"><p>
         Update your deployment directory:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step"><p>
       Installation and management of updates can be automated with the
       following playbooks:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="filename">ardana-update-pkgs.yml</code>
        </p></li><li class="listitem"><p>
         <code class="filename">ardana-update.yml</code>
        </p></li><li class="listitem"><p>
         <code class="filename">ardana-update-status.yml</code>
        </p></li><li class="listitem"><p>
         <code class="filename">ardana-reboot.yml</code>
        </p></li></ul></div></li><li class="step"><p>
       Confirm version changes by running <code class="literal">hostnamectl</code>
       before and after running the <code class="literal">ardana-update-pkgs</code>
       playbook on each node.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>hostnamectl</pre></div><p>
       Notice that <code class="literal">Boot ID:</code> and <code class="literal">Kernel:</code>
       have changed.
      </p></li><li class="step"><p>
       By default, the <code class="filename">ardana-update-pkgs.yml</code> playbook
       will install patches and updates that do not require a system
       reboot. Patches and updates that <span class="bold"><strong>do</strong></span>
       require a system reboot will be installed later in this process.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit <em class="replaceable">TARGET_NODE_NAME</em></pre></div><p>
       There may be a delay in the playbook output at the following task while
       updates are pulled from the deployer.
      </p><div class="verbatim-wrap"><pre class="screen">TASK: [ardana-upgrade-tools | pkg-update | Download and install
package updates] ***</pre></div></li><li class="step"><p>
       After running the <code class="filename">ardana-update-pkgs.yml</code> playbook
       to install patches and updates not requiring reboot, check the status of
       remaining tasks.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit <em class="replaceable">TARGET_NODE_NAME</em></pre></div></li><li class="step"><p>
       To install patches that require reboot, run the
       <code class="filename">ardana-update-pkgs.yml</code> playbook with the parameter
       <code class="literal">-e zypper_update_include_reboot_patches=true</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit  <em class="replaceable">TARGET_NODE_NAME</em> \
-e zypper_update_include_reboot_patches=true</pre></div><p>
	If the output of <code class="filename">ardana-update-pkgs.yml</code> indicates 
	that a reboot is required, run <code class="filename">ardana-reboot.yml</code> 
	<span class="emphasis"><em>after</em></span> completing the <code class="filename">ardana-update.yml</code> 
	step below. Running <code class="filename">ardana-reboot.yml</code>
	will cause cloud service interruption.
      </p><div id="id-1.5.17.5.3.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        To update a single package (for example, apply a PTF on a single node
        or on all nodes), run <code class="command">zypper update
        <em class="replaceable">PACKAGE</em></code>.
       </p><p>
        To install all package updates using <code class="command">zypper update</code>.
       </p></div></li><li class="step"><p>
       Update services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml \
--limit <em class="replaceable">TARGET_NODE_NAME</em></pre></div></li><li class="step"><p>
       If indicated by the <code class="filename">ardana-update-status.yml</code>
       playbook, reboot the node.
      </p><p>
       There may also be a warning to reboot after running the
       <code class="filename">ardana-update-pkgs.yml</code>.
      </p><p>
       This check can be overridden by setting the
       <code class="literal">SKIP_UPDATE_REBOOT_CHECKS</code> environment variable or the
       <code class="literal">skip_update_reboot_checks</code> Ansible variable.
        </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-reboot.yml \
--limit <em class="replaceable">TARGET_NODE_NAME</em></pre></div></li><li class="step"><p>
       To recheck pending system reboot status at a later time, run the
       following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2 -e update_status_var=system-reboot</pre></div></li><li class="step"><p>
       The pending system reboot status can be reset by running:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2 \
-e update_status_var=system-reboot \
-e update_status_reset=true</pre></div></li><li class="step"><p>
       Multiple servers can be patched at the same time with
       <code class="filename">ardana-update-pkgs.yml</code> by setting the option
       <code class="literal">-e skip_single_host_checks=true</code>.
      </p><div id="id-1.5.17.5.3.8.13.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
        When patching multiple servers at the same time, take care not to
        compromise HA capability by updating an entire cluster (controller,
        database, monitor, logging) at the same time.
       </p></div><p>
       If multiple nodes are specified on the command line (with
       <code class="literal">--limit</code>), services on those servers will experience
       outages as the packages are shutdown and updated.  On Compute Nodes (or
       group of Compute Nodes) migrate the workload off if you plan to update
       it. The same applies to Control Nodes: move singleton services off of the
       control plane node that will be updated.
      </p><div id="id-1.5.17.5.3.8.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        Do not reboot all of your controllers at the same time.
       </p></div></li><li class="step"><p>
       When the node comes up after the reboot, run the
       <code class="filename">spark-start.yml</code> file:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml</pre></div></li><li class="step"><p>
       Verify that Spark is running on all Control Nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-status.yml</pre></div></li><li class="step"><p>
       After all nodes have been updated, check the status of all services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.5.17.5.4" data-id-title="Summary of the Update Playbooks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.3.2 </span><span class="title-name">Summary of the Update Playbooks</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.5.4">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.17.5.4.2.1"><span class="term">ardana-update-pkgs.yml</span></dt><dd><p>
      Top-level playbook automates the installation of package updates on
      a single node. It also works for multiple nodes, if the single-node
      restriction is overridden by setting the SKIP_SINGLE_HOST_CHECKS
      environment variable <code class="literal">ardana-update-pkgs.yml -e
      skip_single_host_checks=true</code>.
     </p><p>
      Provide the following <code class="literal">-e</code> options to modify default
      behavior:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">zypper_update_method</code> (default: patch)
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">patch</code> installs all patches for the
          system. Patches are intended for specific bug and security fixes.
         </p></li><li class="listitem"><p>
          <code class="literal">update</code> installs all packages that have a
          higher version number than the installed packages.
         </p></li><li class="listitem"><p>
          <code class="literal">dist-upgrade</code> replaces each package installed with
          the version from the repository and deletes packages not available in
          the repositories.
         </p></li></ul></div></li><li class="listitem"><p>
        <code class="literal">zypper_update_repositories</code> (default: all) restricts
        the list of repositories used
       </p></li><li class="listitem"><p>
        <code class="literal">zypper_update_gpg_checks</code> (default: true) enables GPG
        checks. If set to <code class="literal">true</code>, checks if packages are
        correctly signed.
       </p></li><li class="listitem"><p>
        <code class="literal">zypper_update_licenses_agree</code> (default: false)
        automatically agrees with licenses. If set to <code class="literal">true</code>,
        zypper automatically accepts third party licenses.
       </p></li><li class="listitem"><p>
        <code class="literal">zypper_update_include_reboot_patches</code> (default:
        false) includes patches that require reboot. Setting this to
        <code class="literal">true</code> installs patches that require a reboot (such as
        kernel or glibc updates).
       </p></li></ul></div></dd><dt id="id-1.5.17.5.4.2.2"><span class="term">ardana-update.yml</span></dt><dd><p>
      Top level playbook that automates the update of all the services. Runs
      on all nodes by default, or can be limited to a single node by adding
      <code class="literal">--limit <em class="replaceable">nodename</em></code>.
     </p></dd><dt id="id-1.5.17.5.4.2.3"><span class="term">ardana-reboot.yml</span></dt><dd><p>
      Top-level playbook that automates the steps required to reboot a node. It
      includes pre-boot and post-boot phases, which can be extended to include
      additional checks.
     </p></dd><dt id="id-1.5.17.5.4.2.4"><span class="term">ardana-update-status.yml</span></dt><dd><p>
      This playbook can be used to check or reset the update-related status
      variables maintained by the update playbooks. The main reason for having
      this mechanism is to allow the update status to be checked at any point
      during the update procedure. It is also used heavily by the automation
      scripts to orchestrate installing maintenance updates on multiple nodes.
     </p></dd></dl></div></section></section><section class="sect1" id="upgrade-soc" data-id-title="Upgrading Cloud Lifecycle Manager 8 to Cloud Lifecycle Manager 9"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.4 </span><span class="title-name">Upgrading Cloud Lifecycle Manager 8 to Cloud Lifecycle Manager 9</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#upgrade-soc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before undertaking the upgrade from SUSE <span class="productname">OpenStack</span> Cloud (or HOS) 8 Cloud Lifecycle Manager to
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager, you need to ensure that your
   existing SUSE <span class="productname">OpenStack</span> Cloud 8 Cloud Lifecycle Manager installation is up to date by following the
   <a class="link" href="https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update" target="_blank">https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update</a>.
 </p><p>
   Ensure you review the following resources:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <a class="link" href="https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update/" target="_blank">https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update/</a>
     </p></li><li class="listitem"><p>
       <a class="link" href="https://documentation.suse.com/soc/8/html/suse-openstack-cloud-clm-all/system-maintenance.html#maintenance-update" target="_blank">https://documentation.suse.com/soc/8/html/suse-openstack-cloud-clm-all/system-maintenance.html#maintenance-update</a>
     </p></li></ul></div><p>
   To confirm that all nodes have been successfully updated with no pending
   actions, run the <code class="filename">ardana-update-status.yml</code> playbook on
   the Cloud Lifecycle Manager deployer node as follows:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml</pre></div><div id="id-1.5.17.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     Ensure that all nodes have been updated, and that there are no pending
     update actions remaining to be completed. In particular, ensure that
     any nodes that need to be rebooted have been, using the documented reboot procedure.
   </p></div><div class="procedure" id="id-1.5.17.6.8" data-id-title="Running the Pre-Upgrade Validation Checks to Ensure that your Cloud is Ready for Upgrade"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.4: </span><span class="title-name">Running the Pre-Upgrade Validation Checks to Ensure that your Cloud is Ready for Upgrade </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ul class="procedure"><li class="step"><p>
       Once all nodes have been successfully updated, and there are no pending
       update actions remaining, you should be able to run the
       <code class="filename">ardana-pre-upgrade-validations.sh</code> script, as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>./ardana-pre-upgrade-validations.sh
~/scratch/ansible/next/ardana/ansible ~/scratch/ansible/next/ardana/ansible

PLAY [Initialize an empty list of msgs] ***************************************

TASK: [set_fact ] *************************************************************
ok: [localhost]
...

PLAY RECAP ********************************************************************
...
localhost                  : ok=8    changed=5    unreachable=0    failed=0

msg: Please refer to /var/log/ardana-pre-upgrade-validations.log for the results of this run. Ensure that any messages in the file that have the words FAIL or WARN are resolved.</pre></div><p>
       The last line of output from the <code class="filename">ardana-pre-upgrade-validations.sh</code>
       script will tell you the name of its log file—in this case,
       <code class="filename">/var/log/ardana-pre-upgrade-validations.log</code>. If you
       look at the log file, you will see content similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cat /var/log/ardana-pre-upgrade-validations.log
ardana-cp-dbmqsw-m1*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-dbmqsw-m2*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-dbmqsw-m3*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-mml-m1****************************************************************
SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.
ardana-cp-mml-m2****************************************************************
SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.
ardana-cp-mml-m3****************************************************************
SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.
localhost***********************************************************************</pre></div><p>
       The report states the following:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.17.6.8.2.6.1"><span class="term">SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.</span></dt><dd><p>
             This check confirms that your cloud has been updated with the
             necessary changes such that all services will be using Keystone V3 API.
             This means that there should be minimal interruption of service during
             the upgrade. This is important because the Keystone V2 API has been
             removed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
           </p></dd><dt id="id-1.5.17.6.8.2.6.2"><span class="term">NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for
           /srv/node/disk0 is smaller than the SUSE Linux Enterprise 12 SP4 recommendation of 512. Some
           recommended XFS data integrity features may not be available after upgrade.</span></dt><dd><p>
             This check will only report something if you have local swift
             configured and it is formatted with the SUSE Linux Enterprise 12 SP3 default XFS
             inode size of 256. In SUSE Linux Enterprise 12 SP4, the default XFS inode size for a
             newly-formatted XFS file system has been increased to 512, to
             allow room for enabling some additional XFS data-integrity features by default.
           </p></dd></dl></div></li></ul></div></div><div id="id-1.5.17.6.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     There will be no loss of functionality as regards the swift
     solution after the upgrade. The difference is that some additional
     XFS features will not be available on file systems which were formatted
     under SUSE Linux Enterprise 12 SP3 or earlier. These XFS features aid in the detection of,
     and recovery from, data corruption. They are enabled by default for XFS
     file systems formatted under SUSE Linux Enterprise 12 SP 4.
   </p></div><div class="procedure" id="id-1.5.17.6.10" data-id-title="Additional Pre-Upgrade Checks That Should Be Performed"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.5: </span><span class="title-name">Additional Pre-Upgrade Checks That Should Be Performed </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.10">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     In addition to the automated upgrade checks above, there are some checks
     that should be performed manually.
   </p><ol class="procedure" type="1"><li class="step"><p>
       For each network interface device specified in the input model under
       <code class="filename">~/openstack/my_cloud/definition</code>, ensure that
       there is only one untagged VLAN. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager
       configuration processor will fail with an error if it detects this problem
       during the upgrade, so address this problem before starting the upgrade process.
     </p></li><li class="step"><p>
       If the deployer node is not a standalone system, but
       is instead co-located with the DB services, this can lead to
       potentially longer service disruptions during the upgrade process. To
       determine if this is the case, check if the deployer node (<code class="systemitem">OPS-LM--first-member</code>)
       is a member of the database nodes (<code class="literal">FND-MDB</code>). You can do this with the
       following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts 'FND-MDB:&amp;OPS-LM--first-member' --list-hosts</pre></div><p>
       If the output is:
     </p><div class="verbatim-wrap"><pre class="screen">       No hosts matched</pre></div><p>
       Then the deployer node is not co-located with the database nodes.
       Otherwise, if the command reports a hostname, then there may be
       additional interruptions to the database services during the upgrade.
     </p></li><li class="step"><p>
       Similarly, if the deployer is co-located with the database services,
       and you are also trying to run a local SMT service on the deployer
       node, you will run into issues trying to configure the SMT to enable and
       mirror the SUSE Linux Enterprise 12 SP4 and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 repositories.
     </p><p>
       In such cases, it is recommended that you run the SMT services on a
       different node, and NFS-import the <code class="filename">/srv/www/htdocs/repo</code> onto the deployer
       node, instead of trying to run the SMT services locally.
     </p></li></ol></div></div><div id="id-1.5.17.6.11" data-id-title="Backup the Cloud Lifecycle Manager Configuration Settings" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Backup the Cloud Lifecycle Manager Configuration Settings</div><p>
       The integrated backup solution in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager, freezer, is no
       longer available in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager. Therefore, we
       recommend doing a manual backup to a server that is not a member of the
       cloud, as per <a class="xref" href="bura-overview.html" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
     </p></div><section class="sect2" id="upgrade-overivew" data-id-title="Migrating the Deployer Node Packages"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.1 </span><span class="title-name">Migrating the Deployer Node Packages</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#upgrade-overivew">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The upgrade process first migrates the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager deployer node
     to SUSE Linux Enterprise 12 SP4 and the SOC 9 Cloud Lifecycle Manager packages.
   </p><div id="id-1.5.17.6.12.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       If the deployer node is not a dedicated node, but is instead a member
       of one of the cloud-control planes, then some services may restart with
       the SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 CLM versions of the software during the migration.
       This may mean that:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Some services fail to restart. This will be resolved when the
           appropriate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 configuration changes are applied
           by running the <code class="filename">ardana-upgrade.yml</code> playbook, later
           during the upgrade process.
         </p></li><li class="listitem"><p>
           Other services may log excessive warnings about connectivity issues
           and backwards-compatibility warnings. This will be resolved
           when the relevant services are upgraded during the <code class="filename">ardana-upgrade.yml</code>
           playbook run.
         </p></li></ul></div></div><p>
     In order to upgrade the deployer node to be based on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
     Cloud Lifecycle Manager, you first need to migrate the system to SUSE Linux Enterprise 12 SP4 with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
     Cloud Lifecycle Manager product installed.
   </p><p>
     The process for migrating the deployer node differs somewhat, depending
     on whether your deployer node is registered with the SUSE Customer Center
     (or an SMT mirror), versus using locally-maintained repositories available
     at the relevant locations.
   </p><p>
     If your deployer node is registered with the SUSE Customer Center or an SMT, the migration
     process requires the <code class="literal">zypper-migration-plugin</code> package to be installed.
   </p><div class="procedure" id="id-1.5.17.6.12.7" data-id-title="Migrating an SCC/SMT Registered Deployer Node"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.6: </span><span class="title-name">Migrating an SCC/SMT Registered Deployer Node </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.12.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         If you are using an SMT server to mirror the relevant repositories,
         then you need to enable mirroring of the relevant repositories. See
         <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 16 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”, Section 16.3 “Setting up Repository Mirroring on the SMT Server”</span> for more information.
       </p><p>
         Ensure that the mirroring process has completed before proceeding.
       </p></li><li class="step"><p>
         Ensure that the <code class="literal">zypper-migration-plugin</code> package is
         installed; if not, install it:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper install zypper-migration-plugin
Refreshing service 'SMT-http_smt_example_com'.
Loading repository data...
Reading installed packages...
'zypper-migration-plugin' is already installed.
No update candidate for 'zypper-migration-plugin-0.10-12.4.noarch'. The highest available version is already installed.
Resolving package dependencies...

Nothing to do.</pre></div></li><li class="step"><p>
         De-register the SUSE Linux Enterprise Server LTSS 12 SP3 x86_64
         extension (if enabled):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo SUSEConnect --status-text
Installed Products:
------------------------------------------

  SUSE Linux Enterprise Server 12 SP3 LTSS
  (SLES-LTSS/12.3/x86_64)

  Registered

------------------------------------------

  SUSE Linux Enterprise Server 12 SP3
  (SLES/12.3/x86_64)

  Registered

------------------------------------------

  SUSE OpenStack Cloud 8
  (suse-openstack-cloud/8/x86_64)

  Registered

------------------------------------------


ardana &gt; sudo SUSEConnect -d -p SLES-LTSS/12.3/x86_64
Deregistering system from registration proxy https://smt.example.com/

Deactivating SLES-LTSS 12.3 x86_64 ...
-&gt; Refreshing service ...
-&gt; Removing release package ...
ardana &gt; sudo SUSEConnect --status-text
Installed Products:
------------------------------------------

  SUSE Linux Enterprise Server 12 SP3
  (SLES/12.3/x86_64)

  Registered

------------------------------------------

  SUSE OpenStack Cloud 8
  (suse-openstack-cloud/8/x86_64)

  Registered

------------------------------------------</pre></div></li><li class="step"><p>
         Disable any other SUSE Linux Enterprise 12 SP3 or SUSE <span class="productname">OpenStack</span> Cloud (or HOS) 8 Cloud Lifecycle Manager-related repositories.
         The zypper migration process should detect and disable most of these
         automatically, but in some cases it may not catch all of them, which
         can lead to a minor disruption later during the upgrade procedure. For
         example, to disable any repositories served from the
         <code class="filename">/srv/www/suse-12.3</code> directory or the SUSE-12-4 alias
         under <code class="systemitem">http://localhost:79/</code>, you could use the following commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2
PTF
SLES12-SP3-LTSS-Updates
SLES12-SP3-Pool
SLES12-SP3-Updates
SUSE-OpenStack-Cloud-8-Pool
SUSE-OpenStack-Cloud-8-Updates
ardana &gt; for repo in $(zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2); do sudo zypper modifyrepo --disable "${repo}"; done
Repository 'PTF' has been successfully disabled.
Repository 'SLES12-SP3-LTSS-Updates' has been successfully disabled.
Repository 'SLES12-SP3-Pool' has been successfully disabled.
Repository 'SLES12-SP3-Updates' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Pool' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Updates' has been successfully disabled.</pre></div></li><li class="step"><p>
         Remove the PTF repository, which is based on SUSE Linux Enterprise 12 SP3 (a new one,
         based on SUSE Linux Enterprise 12 SP4, will be created during the upgrade process):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep PTF
 2 | PTF                                                               | PTF                                      | No      | (r ) Yes  | Yes
ardana &gt; sudo zypper removerepo PTF
Removing repository 'PTF' ..............................................................................................[done]
Repository 'PTF' has been removed.</pre></div></li><li class="step"><p>
         Remove the Cloud media repository (if defined):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep '[|] Cloud '
 1 | Cloud                          | SUSE OpenStack Cloud 8 DVD #1  | Yes     | (r ) Yes  | No
ardana &gt; sudo zypper removerepo Cloud
Removing repository 'SUSE OpenStack Cloud 8 DVD #1' ....................................................................[done]
Repository 'SUSE OpenStack Cloud 8 DVD #1' has been removed.</pre></div></li><li class="step"><p>
         Run the <code class="command">zypper migration</code> command, which should offer a single choice:
         namely, to upgrade to SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager. You need to accept the
         offered choice, then answer <code class="option">yes</code> to any prompts to disable obsoleted repositories.
         At that point, the <code class="command">zypper migration</code> command will run <code class="command">zypper dist-upgrade</code>,
         which will prompt you to agree with the proposed package changes. Finally,
         you will to agree with any new licenses. After this, the package upgrade
         of the deployer node will proceed. The output of the running <code class="command">zypper migration</code>
         should look something like the following:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper migration

Executing 'zypper  refresh'

Repository 'SLES12-SP3-Pool' is up to date.
Repository 'SLES12-SP3-Updates' is up to date.
Repository 'SLES12-SP3-Pool' is up to date.
Repository 'SLES12-SP3-Updates' is up to date.
Repository 'SUSE-OpenStack-Cloud-8-Pool' is up to date.
Repository 'SUSE-OpenStack-Cloud-8-Updates' is up to date.
Repository 'OpenStack-Cloud-8-Pool' is up to date.
Repository 'OpenStack-Cloud-8-Updates' is up to date.
All repositories have been refreshed.

Executing 'zypper  --no-refresh patch-check --updatestack-only'

Loading repository data...
Reading installed packages...

0 patches needed (0 security patches)

Available migrations:

    1 | SUSE Linux Enterprise Server 12 SP4 x86_64
        SUSE OpenStack Cloud 9 x86_64


[num/q]: 1

Executing 'snapper create --type pre --cleanup-algorithm=number --print-number --userdata important=yes --description 'before online migration''

The config 'root' does not exist. Likely snapper is not configured.
See 'man snapper' for further instructions.
Upgrading product SUSE Linux Enterprise Server 12 SP4 x86_64.
Found obsolete repository SLES12-SP3-Updates
Disable obsolete repository SLES12-SP3-Updates [y/n] (y): y
... disabling.
Found obsolete repository SLES12-SP3-Pool
Disable obsolete repository SLES12-SP3-Pool [y/n] (y): y
... disabling.
Upgrading product SUSE OpenStack Cloud 9 x86_64.
Found obsolete repository OpenStack-Cloud-8-Pool
Disable obsolete repository OpenStack-Cloud-8-Pool [y/n] (y): y
... disabling.

Executing 'zypper --releasever 12.4 ref -f'

Warning: Enforced setting: $releasever=12.4
Forcing raw metadata refresh
Retrieving repository 'SLES12-SP4-Pool' metadata .......................................................................[done]
Forcing building of repository cache
Building repository 'SLES12-SP4-Pool' cache ............................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SLES12-SP4-Updates' metadata ....................................................................[done]
Forcing building of repository cache
Building repository 'SLES12-SP4-Updates' cache .........................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SUSE-OpenStack-Cloud-9-Pool' metadata ...........................................................[done]
Forcing building of repository cache
Building repository 'SUSE-OpenStack-Cloud-9-Pool' cache ................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SUSE-OpenStack-Cloud-9-Updates' metadata ........................................................[done]
Forcing building of repository cache
Building repository 'SUSE-OpenStack-Cloud-9-Updates' cache .............................................................[done]
Forcing raw metadata refresh
Retrieving repository 'OpenStack-Cloud-8-Updates' metadata .............................................................[done]
Forcing building of repository cache
Building repository 'OpenStack-Cloud-8-Updates' cache ..................................................................[done]
All repositories have been refreshed.

Executing 'zypper --releasever 12.4  --no-refresh  dist-upgrade --no-allow-vendor-change '

Warning: Enforced setting: $releasever=12.4
Warning: You are about to do a distribution upgrade with all enabled repositories. Make sure these repositories are compatible before you continue. See 'man zypper' for more information about this command.
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

...

525 packages to upgrade, 14 to downgrade, 62 new, 5 to remove, 1 to change vendor, 1 to change arch.
Overall download size: 1.24 GiB. Already cached: 0 B. After the operation, additional 780.8 MiB will be used.
Continue? [y/n/...? shows all options] (y): y
...
    dracut: *** Generating early-microcode cpio image ***
    dracut: *** Constructing GenuineIntel.bin ****
    dracut: *** Store current command line parameters ***
    dracut: Stored kernel commandline:
    dracut:  rd.lvm.lv=ardana-vg/root
    dracut:  root=/dev/mapper/ardana--vg-root rootfstype=ext4 rootflags=rw,relatime,data=ordered
    dracut: *** Creating image file '/boot/initrd-4.4.180-94.127-default' ***
    dracut: *** Creating initramfs image file '/boot/initrd-4.4.180-94.127-default' done ***

Output of btrfsmaintenance-0.2-18.1.noarch.rpm %posttrans script:
    Refresh script btrfs-scrub.sh for monthly
    Refresh script btrfs-defrag.sh for none
    Refresh script btrfs-balance.sh for weekly
    Refresh script btrfs-trim.sh for none

There are some running programs that might use files deleted by recent upgrade. You may wish to check and restart some of them. Run 'zypper ps -s' to list these programs.</pre></div></li></ol></div></div><div class="procedure" id="id-1.5.17.6.12.8" data-id-title="Migrating a Deployer Node with Locally-Managed Repositories"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.7: </span><span class="title-name">Migrating a Deployer Node with Locally-Managed Repositories </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.12.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
       In this configuration, you need to manually migrate the system using
       <code class="command">zypper dist-upgrade</code>, according to the following steps:
     </p><ol class="procedure" type="1"><li class="step"><p>
         Disable any SUSE Linux Enterprise 12 SP3 or <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager-related repositories.
         Leaving the SUSE Linux Enterprise 12 SP3 and/or SUSE <span class="productname">OpenStack</span> Cloud (or HOS) 8 Cloud Lifecycle Manager-related repositories
         enabled can lead to a minor disruption later during the upgrade procedure.
         For example, to disable any repositories served from the
         <code class="filename">/srv/www/suse-12.3</code> directory, or the SUSE-12-4
         alias under <code class="systemitem">http://localhost:79/</code>,  use the following commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2
PTF
SLES12-SP3-LTSS-Updates
SLES12-SP3-Pool
SLES12-SP3-Updates
SUSE-OpenStack-Cloud-8-Pool
SUSE-OpenStack-Cloud-8-Updates
ardana &gt; for repo in $(zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2); do sudo zypper modifyrepo --disable "${repo}"; done
Repository 'PTF' has been successfully disabled.
Repository 'SLES12-SP3-LTSS-Updates' has been successfully disabled.
Repository 'SLES12-SP3-Pool' has been successfully disabled.
Repository 'SLES12-SP3-Updates' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Pool' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Updates' has been successfully disabled.</pre></div><div id="id-1.5.17.6.12.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
           The SLES12-SP3-LTSS-Updates repository should only be present if
           you have purchased the optional SUSE Linux Enterprise 12 SP3 LTSS support. Whether
           or not it is configured will not impact the upgrade process.
         </p></div></li><li class="step"><p>
         Remove the PTF repository, which is based on SUSE Linux Enterprise 12 SP3. A new one
         based on SUSE Linux Enterprise 12 SP4 will be created during the upgrade process.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep PTF
 2 | PTF                                                               | PTF                                      | Yes     | (r ) Yes  | Yes
ardana &gt; sudo zypper removerepo PTF
Removing repository 'PTF' ..............................................................................................[done]
Repository 'PTF' has been removed.</pre></div></li><li class="step"><p>
         Remove the Cloud media repository if defined.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep '[|] Cloud '
 1 | Cloud                          | SUSE OpenStack Cloud 8 DVD #1  | Yes     | (r ) Yes  | No
ardana &gt; sudo zypper removerepo Cloud
Removing repository 'SUSE OpenStack Cloud 8 DVD #1' ....................................................................[done]
Repository 'SUSE OpenStack Cloud 8 DVD #1' has been removed.</pre></div></li><li class="step"><p>
         Ensure the deployer node has access to the SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 CLM
         repositories as documented in <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 17 “Software Repository Setup”</span>
         paying attention to the non-SMT based repository setup. When you run
         <code class="command">zypper repos --show-enabled-only</code>, the output should look similar to the following:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos --show-enabled-only
#  | Alias                          | Name                           | Enabled | GPG Check | Refresh
---+--------------------------------+--------------------------------+---------+-----------+--------
 1 | Cloud                          | SUSE OpenStack Cloud 9 DVD #1  | Yes     | (r ) Yes  | No
 7 | SLES12-SP4-Pool                | SLES12-SP4-Pool                | Yes     | (r ) Yes  | No
 8 | SLES12-SP4-Updates             | SLES12-SP4-Updates             | Yes     | (r ) Yes  | Yes
 9 | SUSE-OpenStack-Cloud-9-Pool    | SUSE-OpenStack-Cloud-9-Pool    | Yes     | (r ) Yes  | No
10 | SUSE-OpenStack-Cloud-9-Updates | SUSE-OpenStack-Cloud-9-Updates | Yes     | (r ) Yes  | Yes</pre></div><div id="id-1.5.17.6.12.8.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
           The Cloud repository above is optional. Its content is equivalent to
           the SUSE-Openstack-Cloud-9-Pool repository.
         </p></div></li><li class="step"><p>
         Run the <code class="command">zypper dist-upgrade</code> command to upgrade the
         deployer node:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper dist-upgrade

Warning: You are about to do a distribution upgrade with all enabled repositories. Make sure these repositories are compatible before you continue. See 'man zypper' for more information about this command.
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

...

525 packages to upgrade, 14 to downgrade, 62 new, 5 to remove, 1 to change vendor, 1 to change arch.
Overall download size: 1.24 GiB. Already cached: 0 B. After the operation, additional 780.8 MiB will be used.
Continue? [y/n/...? shows all options] (y): y
...
    dracut: *** Generating early-microcode cpio image ***
    dracut: *** Constructing GenuineIntel.bin ****
    dracut: *** Store current command line parameters ***
    dracut: Stored kernel commandline:
    dracut:  rd.lvm.lv=ardana-vg/root
    dracut:  root=/dev/mapper/ardana--vg-root rootfstype=ext4 rootflags=rw,relatime,data=ordered
    dracut: *** Creating image file '/boot/initrd-4.4.180-94.127-default' ***
    dracut: *** Creating initramfs image file '/boot/initrd-4.4.180-94.127-default' done ***

Output of btrfsmaintenance-0.2-18.1.noarch.rpm %posttrans script:
    Refresh script btrfs-scrub.sh for monthly
    Refresh script btrfs-defrag.sh for none
    Refresh script btrfs-balance.sh for weekly
    Refresh script btrfs-trim.sh for none

There are some running programs that might use files deleted by recent upgrade. You may wish to check and restart some of them. Run 'zypper ps -s' to list these programs.</pre></div><div id="id-1.5.17.6.12.8.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
           You may need to run the <code class="command">zypper dist-upgrade</code> command more than
           once, if it determines that it needs to update the <code class="command">zypper</code>
           infrastructure on your system to be able to successfully
           <code class="literal">dist-upgrade</code> the node; the command will tell you if you need to run it again.
         </p></div></li></ol></div></div></section><section class="sect2" id="upgrade-deployer-node-config" data-id-title="Upgrading the Deployer Node Configuration Settings"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.2 </span><span class="title-name">Upgrading the Deployer Node Configuration Settings</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#upgrade-deployer-node-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Now that the deployer node packages have been migrated to SUSE Linux Enterprise 12 SP4 and
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager, we need to update the configuration
      settings to be <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager based.
    </p><p>
      The first step is to run the <code class="command">ardana-init</code> command. This will:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Add the PTF repository, creating it if needed.
        </p></li><li class="listitem"><p>
          Optionally add appropriate local repository references for any
          SMT-provided SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 repositories.
        </p></li><li class="listitem"><p>
          Upgrade the deployer account <code class="literal">~/openstack</code> area to be
          based upon <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager Ansible sources.
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              This will import the new <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager Ansible code into the Git
              repository on the Ardana branch, and then rebase the customer
              site branch on top of the updated Ardana branch.
            </p></li><li class="listitem"><p>
              Follow the directions to resolve any Git merge conflicts that may
              arise due to local changes that may have been made on the site branch:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ardana-init
...
 To continue installation copy your cloud layout to:
     /var/lib/ardana/openstack/my_cloud/definition

 Then execute the installation playbooks:
     cd /var/lib/ardana/openstack/ardana/ansible
     git add -A
     git commit -m 'My config'
     ansible-playbook -i hosts/localhost cobbler-deploy.yml
     ansible-playbook -i hosts/localhost bm-reimage.yml
     ansible-playbook -i hosts/localhost config-processor-run.yml
     ansible-playbook -i hosts/localhost ready-deployment.yml
     cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
     ansible-playbook -i hosts/verb_hosts site.yml

 If you prefer to use the UI to install the product, you can
 do either of the following:
     - If you are running a browser on this machine, you can point
       your browser to http://localhost:9085 to start the install
       via the UI.
     - If you are running the browser on a remote machine, you will
       need to create an ssh tunnel to access the UI.  Please refer
       to the Ardana installation documentation for further details.</pre></div></li></ul></div></li></ul></div><div id="id-1.5.17.6.13.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        As we are upgrading to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager, we do not need
        to run the suggested <code class="filename">bm-reimage.yml</code> playbook.
      </p></div><div class="procedure" id="id-1.5.17.6.13.6" data-id-title="Updating the Bare-Metal Provisioning Configuration"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.8: </span><span class="title-name">Updating the Bare-Metal Provisioning Configuration </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.13.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
        If you were previously using the <code class="literal">cobbler</code>-based integrated provisioning
        solution, then you will need to perform the following steps to import
        the SUSE Linux Enterprise 12 SP4 ISO and update the default provisioning distribution:
      </p><ol class="procedure" type="1"><li class="step"><p>
          Ensure there is a copy of the <code class="filename">SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso</code>,
          named <code class="filename">sles12sp4.iso</code>, available in the <code class="filename">/var/lib/ardana</code> directory.
        </p></li><li class="step"><p>
          Ensure that any distribution entries in <code class="filename">servers.yml</code>
          (or whichever file holds the server node definitions) under
          <code class="filename">~/openstack/my_cloud/definition</code> are updated to
          specify <code class="literal">sles12sp4</code> if they are currently using <code class="literal">sles12sp3</code>.
        </p><div id="id-1.5.17.6.13.6.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
            The default distribution will now be <code class="literal">sles12sp4</code>, so if there are no
            specific distribution entries specified for the servers, then no change
            will be required.
          </p></div><p>
          If you have made any changes to the <code class="filename">~/openstack/my_cloud/definition</code>
          files, you will need to commit those changes, as follows:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
 <code class="prompt user">ardana &gt; </code>git add -A
 <code class="prompt user">ardana &gt; </code>git commit -m "Update sles12sp3 distro entries to sles12sp4"</pre></div></li><li class="step"><p>
          Run the <code class="filename">cobbler-deploy.yml</code> playbook to import
          the SUSE Linux Enterprise 12 SP4 distribution as the new default distribution:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
 <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml
 Enter the password that will be used to access provisioned nodes:
 confirm Enter the password that will be used to access provisioned nodes:

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 TASK: [pbstart.yml pb_start_playbook] *****************************************
 ok: [localhost] =&gt; {
     "msg": "Playbook started - cobbler-deploy.yml"
 }

 msg: Playbook started - cobbler-deploy.yml

 ...

 PLAY [localhost] **************************************************************

 TASK: [pbfinish.yml pb_finish_playbook] ***************************************
 ok: [localhost] =&gt; {
     "msg": "Playbook finished - cobbler-deploy.yml"
 }

 msg: Playbook finished - cobbler-deploy.yml

 PLAY RECAP ********************************************************************
 localhost                  : ok=92   changed=45   unreachable=0    failed=0</pre></div></li></ol></div></div><p>
      You are now ready to upgrade the input model to be compatible.
    </p><div class="procedure" id="id-1.5.17.6.13.8" data-id-title="Upgrading the Cloud Input Model"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.9: </span><span class="title-name">Upgrading the Cloud Input Model </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.13.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
        At this point, there are some mandatory changes that will need to be made
        to the existing input model to permit the upgrade proceed. These mandatory
        changes represent:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            The removal of previously-deprecated service components;
          </p></li><li class="listitem"><p>
            The dropping of service components that are no longer supported;
          </p></li><li class="listitem"><p>
            That there can be only one untagged VLAN per network interface;
          </p></li><li class="listitem"><p>
            That there must be a <code class="literal">MANAGEMENT</code> network group.
          </p></li></ul></div><p>
        There are also some service components that have been made redundant
        and have no effect. These should be removed to quieten the associated
        <code class="filename">config-processor-run.yml</code> warnings.
      </p><p>
        For example, if you run the <code class="filename">configuration-processor-run.yml</code>
        playbook from the <code class="filename">~/openstack/ardana/ansible</code>
        directory before you made the necessary input model changes, you should
        see it fail with errors similar to those shown below—unless your input
        model doesn't deploy the problematic service component:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
 <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
 Enter encryption key (press return for none):
 confirm Enter encryption key (press return for none):
 To change encryption key enter new key (press return for none):
 confirm To change encryption key enter new key (press return for none):

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 ...

             "################################################################################",
             "# The configuration processor failed.  ",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'designate-pool-manager' has been deprecated and will be replaced by 'designate-worker'. The replacement component will be automatically deployed in a future release. You will then need to update the input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'manila-share' service component is deprecated. The 'manila-share' service component can be removed as manila share service will be deployed where manila-api is specified. This is not a deprecation for openstack-manila-share but just an entry deprecation in input model.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'designate-zone-manager' has been deprecated and will be replaced by 'designate-producer'. The replacement component will be automatically deployed in a future release. You will then need to update the input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'glance-registry' has been deprectated and is no longer deployed. Please update you input model to remove any 'glance-registry' service component specifications to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:mml: 'ceilometer-api' is no longer used by Ardana and will not be deployed. Please update your input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:sles-compute: 'neutron-lbaasv2-agent' has been deprecated and replaced by 'octavia' and will not be deployed in a future release. Please update your input model to remove this warning.",
             "",
             "#   control-planes-2.0        ERR: cp:common-service-components: Undefined component 'freezer-agent'",
             "#   control-planes-2.0        ERR: cp:openstack-core: Undefined component 'nova-console-auth'",
             "#   control-planes-2.0        ERR: cp:openstack-core: Undefined component 'heat-api-cloudwatch'",
             "#   control-planes-2.0        ERR: cp:mml: Undefined component 'freezer-api'",
             "################################################################################"
         ]
     }
 }

 TASK: [debug var=config_processor_result.stderr] ******************************
 ok: [localhost] =&gt; {
     "var": {
         "config_processor_result.stderr": "/usr/lib/python2.7/site-packages/ardana_configurationprocessor/cp/model/YamlConfigFile.py:95: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n  self._contents = yaml.load(''.join(lines))"
     }
 }

 TASK: [fail msg="Configuration processor run failed, see log output above for details"] ***
 failed: [localhost] =&gt; {"failed": true}
 msg: Configuration processor run failed, see log output above for details

 msg: Configuration processor run failed, see log output above for details

 FATAL: all hosts have already failed -- aborting

 PLAY RECAP ********************************************************************
            to retry, use: --limit @/var/lib/ardana/config-processor-run.retry

 localhost                  : ok=8    changed=5    unreachable=0    failed=1</pre></div><p>
        To resolve any errors and warnings like those shown above, you will
        need to perform the following actions:
      </p><ol class="procedure" type="1"><li class="step"><p>
          Remove any service component entries that are no longer valid from the
          <code class="filename">control_plane.yml</code> (or whichever file holds the
          control-plane definitions) under <code class="filename">~/openstack/my_cloud/definition</code>.
          This means that you have to comment out (or delete) any lines for the
          following service components, which are no longer available:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <code class="literal">freezer-agent</code>
            </p></li><li class="listitem"><p>
              <code class="literal">freezer-api</code>
            </p></li><li class="listitem"><p>
              <code class="literal">heat-api-cloudwatch</code>
            </p></li><li class="listitem"><p>
              <code class="literal">nova-console-auth</code>
            </p></li></ul></div><div id="id-1.5.17.6.13.8.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
            This should resolve the errors that cause the <code class="filename">config-processor-run.yml</code>
            playbook to fail.
          </p></div></li><li class="step"><p>
          Similarly, remove any service components that are redundant and no
          longer required. This means that you should comment out (or delete)
          any lines for the following service components:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              <code class="literal">ceilometer-api</code>
            </p></li><li class="listitem"><p>
              <code class="literal">glance-registry</code>
            </p></li><li class="listitem"><p>
              <code class="literal">manila-share</code>
            </p></li><li class="listitem"><p>
              <code class="literal">neutron-lbaasv2-agent</code>
            </p></li></ul></div><div id="id-1.5.17.6.13.8.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
            This should resolve most of the warnings reported by the
            <code class="filename">config-processor-run.yml</code> playbook.
          </p></div><div id="id-1.5.17.6.13.8.9.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
            If you have deployed the <code class="systemitem">designate</code> service components
            (<code class="systemitem">designate-pool-manager</code> and <code class="systemitem">designate-zone-manager</code>) in your cloud,
            you will see warnings like those shown above, indicating that these
            service components have been deprecated.
          </p><p>
            You can switch to using the newer <code class="systemitem">designate-worker</code> and <code class="systemitem">designate-producer</code>
            service components, which will quieten these deprecation warnings
            produced by the <code class="filename">config-processor-run.yml</code> playbook run.
          </p><p>
            However, this is a procedure that should be perfomed after the
            upgrade has completed, as outlined in the <a class="xref" href="system-maintenance.html#post-upgrade-tasks" title="15.4.5. Post-Upgrade Tasks">Section 15.4.5, “Post-Upgrade Tasks”</a>
            section below.
          </p></div></li><li class="step"><p>
        Once you have made the necessary changes to your input model, if
        you run <code class="command">git diff</code> under the <code class="filename">~/openstack/my_cloud/definition</code>
        directory, you should see output similar to the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
 <code class="prompt user">ardana &gt; </code>git diff
 diff --git a/my_cloud/definition/data/control_plane.yml b/my_cloud/definition/data/control_plane.yml
 index f7cfd84..2c1a73c 100644
 --- a/my_cloud/definition/data/control_plane.yml
 +++ b/my_cloud/definition/data/control_plane.yml
 @@ -32,7 +32,6 @@
          - NEUTRON-CONFIG-CP1
        common-service-components:
          - lifecycle-manager-target
 -        - freezer-agent
          - stunnel
          - monasca-agent
          - logging-rotate
 @@ -118,12 +117,10 @@
              - cinder-volume
              - cinder-backup
              - glance-api
 -            - glance-registry
              - nova-api
              - nova-placement-api
              - nova-scheduler
              - nova-conductor
 -            - nova-console-auth
              - nova-novncproxy
              - neutron-server
              - neutron-ml2-plugin
 @@ -137,7 +134,6 @@
              - horizon
              - heat-api
              - heat-api-cfn
 -            - heat-api-cloudwatch
              - heat-engine
              - ops-console-web
              - barbican-api
 @@ -151,7 +147,6 @@
              - magnum-api
              - magnum-conductor
              - manila-api
 -            - manila-share

          - name: mml
            cluster-prefix: mml
 @@ -164,9 +159,7 @@

              # freezer-api shares elastic-search with logging-server
              # so must be co-located with it
 -            - freezer-api

 -            - ceilometer-api
              - ceilometer-polling
              - ceilometer-agent-notification
              - ceilometer-common
 @@ -194,4 +187,3 @@
              - neutron-l3-agent
              - neutron-metadata-agent
              - neutron-openvswitch-agent
 -            - neutron-lbaasv2-agent</pre></div></li><li class="step"><p>
        If you are happy with these changes, commit them into
        the Git repository as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
 <code class="prompt user">ardana &gt; </code>git add -A
 <code class="prompt user">ardana &gt; </code>git commit -m "SOC 9 CLM Upgrade input model migration"</pre></div></li><li class="step"><p>
        Now you are ready to run the <code class="filename">config-processor-run.yml</code> playbook. If the
        necessary input model changes have been made, it will complete
        sucessfully:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
 <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
 Enter encryption key (press return for none):
 confirm Enter encryption key (press return for none):
 To change encryption key enter new key (press return for none):
 confirm To change encryption key enter new key (press return for none):

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 ...
 PLAY RECAP ********************************************************************
 localhost                  : ok=24   changed=20   unreachable=0    failed=0</pre></div></li></ol></div></div></section><section class="sect2" id="upgrade-cloud-services" data-id-title="Upgrading Cloud Services"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.3 </span><span class="title-name">Upgrading Cloud Services</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#upgrade-cloud-services">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The deployer node is now ready to be used to upgrade the remaining
     cloud nodes and running services.
   </p><div id="id-1.5.17.6.14.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
      If upgrading from Helion OpenStack 8, there is a manual file update that must
      be applied before continuing the upgrade process. In the file
      <code class="literal">/usr/share/ardana/ansible/roles/osconfig/tasks/check-product-status.yml</code>
      replace `command` with `shell` in the first ansible entry. The correct version
      of the file appears below.
     </p><div class="verbatim-wrap"><pre class="screen">- name: deployer-setup | check-product-status | Check HOS product installed
  shell: |-
    zypper info hpe-helion-openstack-release | grep "^Installed *: *Yes"
  ignore_errors: yes
  register: product_flavor_hos

- name: deployer-setup | check-product-status | Check SOC product availability
  become: yes
  zypper:
    name: "suse-openstack-cloud-release&gt;=8"
    state: present
  ignore_errors: yes
  register: product_flavor_soc

- name: deployer-setup | check-product-status | Provide help
  fail:
    msg: &gt;
      The deployer node does not have a Cloud Add-On product installed.
      In YaST select Software/Add-On Products to see an overview of installed
       add-on products and use "Add" to add the Cloud product.
  when:
    - product_flavor_soc|failed
    - product_flavor_hos|failed</pre></div><p>
      Changes to the <code class="literal">check-product-status.yml</code> file must be staged
      and committed via git.
   </p><div class="verbatim-wrap"><pre class="screen">git add -u
git commit -m "applying osconfig fix prior to HOS8 to SOC9 upgrade"</pre></div></div><div id="id-1.5.17.6.14.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       The <code class="filename">ardana-upgrade.yml</code> playbook runs the upgrade process
       against all nodes in parallel, though some of the steps are serialised
       to run on only one node at a time to avoid triggering potentially
       problematic race conditions. As such, the playbook can take a long time to run.
     </p></div><div class="procedure" id="id-1.5.17.6.14.5" data-id-title="Generate the SUSE OpenStack Cloud 9 Cloud Lifecycle Manager Based Scratch Area"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.10: </span><span class="title-name">Generate the SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager Based Scratch Area </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.14.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Generate the updated scratch area using the SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager Ansible sources:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml

PLAY [localhost] **************************************************************

GATHERING FACTS ***************************************************************
ok: [localhost]

...

PLAY RECAP ********************************************************************
localhost                  : ok=31   changed=16   unreachable=0    failed=0</pre></div></li><li class="step"><p>
         Confirm that there are no pending updates for the deployer node.
         This could happen if you are using an SMT to manage the repositories,
         and updates have been released through the official channels since
         the deployer node was migrated. To check for any pending Cloud Lifecycle Manager package
         updates, you can run the <code class="filename">ardana-update-pkgs.yml</code> playbook as follows:
       </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml --limit OPS-LM--first-member

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-dplyr-m1]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-update-pkgs.yml"
}

...

TASK: [_ardana-update-status | Report update status] **************************
ok: [ardana-cp-dplyr-m1] =&gt; {
    "msg": "=====================================================================\nUpdate status for node ardana-cp-dplyr-m1:\n=====================================================================\nNo pending update actions on the ardana-cp-dplyr-m1 host\nwere collected or reset during this update run or persisted during\nprevious unsuccessful or incomplete update runs.\n\n====================================================================="
}

msg: =====================================================================
Update status for node ardana-cp-dplyr-m1:
=====================================================================
No pending update actions on the ardana-cp-dplyr-m1 host
were collected or reset during this update run or persisted during
previous unsuccessful or incomplete update runs.

=====================================================================

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-update-pkgs.yml"
}

msg: Playbook finished - ardana-update-pkgs.yml

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=98   changed=12   unreachable=0    failed=0
localhost                  : ok=6    changed=2    unreachable=0    failed=0</pre></div><div id="id-1.5.17.6.14.5.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
           If running the <code class="filename">ardana-update-pkgs.yml</code> playbook identifies that there
           were updates that needed to be installed on your deployer node, then you
           need to go back to running the <code class="command">ardana-init</code> command, followed by the
           <code class="filename">cobbler-deploy.yml</code> playbook, then the <code class="filename">config-processor-run.yml</code>
           playbook, and finally the <code class="filename">ready-deployment.yml</code> playbook, addressing
           any additional input model changes that may be needed. Then,
           repeat this step to check for any pending updates before
           continuing with the upgrade.
         </p></div></li><li class="step"><p>
         Double-check that there are no pending actions needed for the deployer node
         by running the <code class="filename">ardana-update-status.yml</code> playbook, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml --limit OPS-LM--first-member

PLAY [resources] **************************************************************

...

TASK: [_ardana-update-status | Report update status] **************************
ok: [ardana-cp-dplyr-m1] =&gt; {
    "msg": "=====================================================================\nUpdate status for node ardana-cp-dplyr-m1:\n=====================================================================\nNo pending update actions on the ardana-cp-dplyr-m1 host\nwere collected or reset during this update run or persisted during\nprevious unsuccessful or incomplete update runs.\n\n====================================================================="
}

msg: =====================================================================
Update status for node ardana-cp-dplyr-m1:
=====================================================================
No pending update actions on the ardana-cp-dplyr-m1 host
were collected or reset during this update run or persisted during
previous unsuccessful or incomplete update runs.

=====================================================================

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=12   changed=0    unreachable=0    failed=0</pre></div></li><li class="step"><p>
         Having verified that there are no pending actions detected, it is
         safe to proceed with running the <code class="filename">ardana-upgrade.yml</code>
         playbook to upgrade the entire cloud:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-upgrade.yml
PLAY [all] ********************************************************************

...

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-upgrade.yml"
}

msg: Playbook started - ardana-upgrade.yml

...
...
...
...
...

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-upgrade.yml"
}

msg: Playbook finished - ardana-upgrade.yml</pre></div></li></ol></div></div><p>
       The <code class="filename">ardana-upgrade.yml</code> playbook run will take a long time. The
       <code class="command">zypper dist-upgrade</code> phase is serialised across all of
       the nodes and usually takes between five and 10 minutes for each node. This
       is followed by the cloud service upgrade phase, which will take
       approximately the same amount of time as a full cloud deploy. During
       this time, the cloud should remain basically functional, though there
       may be brief interruptions to some services. However, it is recommended
       that any workload management tasks are avoided during this period.
     </p><div id="id-1.5.17.6.14.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
         Until the <code class="filename">ardana-upgrade.yml</code> playbook run has
         ompleted successfully, other playbooks such as the <code class="filename">ardana-status.yml</code>,
         may report status problems. This is because some services that are
         expected to be running may not be installed, enabled, or migrated yet.
       </p></div><p>
       The <code class="filename">ardana-upgrade.yml</code> playbook run may sometimes
       fail during the whole cloud upgrade phase, if a service (for example,
       the <code class="systemitem">monasca-thresh</code> service) is slow to restart. In such cases, it is
       safe to run the <code class="filename">ardana-upgrade.yml</code> playbook again,
       and in most cases it should continue past the stage that failed
       previously. However, if the same problem persists across multiple runs,
       contact your support team for assistance.
     </p><div id="id-1.5.17.6.14.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
         It is important to disable all SUSE Linux Enterprise 12 SP3 <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager
         repositories before migrating the deployer to SUSE Linux Enterprise 12 SP4 <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
         9 Cloud Lifecycle Manager. If you did not do this, then the first time you
         run the <code class="filename">ardana-upgrade.yml</code> playbook, it may
         complain that there are pending updates for the deployer node.
         This will require you to repeat the earlier steps to upgrade the
          deployer node, starting with running the <code class="command">ardana-init</code>
          command. If this happens, repeat the steps as requested. Note that this
          does not represent a serious problem.
       </p></div><p>
     In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager the LBaaS V2 legacy driver has been
     deprecated and removed. As part of the <code class="filename">ardana-upgrade.yml</code> playbook run,
     all existing LBaaS V2 load-balancers will be automatically migrated to being
     based on the Octavia Amphora provider. To enable creation of any new Octavia-
     based load-balancer instances, you need to ensure that an appropriate Amphora
     image is registered for use when creating instances, by following
     <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span>.
   </p><div id="id-1.5.17.6.14.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       While running the <code class="filename">ardana-upgrade.yml</code> playbook, a point will be
       reached when the Neutron services are upgraded. As part of this upgrade,
       any existing LBaaS V2 load-balancer definitions will be migrated to
       Octavia Amphora-based load-balancer definitions.
     </p><p>
       After this migration of load-balancer definitions has completed,
       if a load-balancer failover is triggered, then the replacement load-
       balancer may fail to start, as an appropriate Octavia Amphora image
       for SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager will not yet be available.
     </p><p>
       However, once the Octavia Amphora image has been uploaded using the
       above instructions, then it will be possible to recover any failed
       load-balancers by re-triggering the failover: follow the instructions at
       <a class="link" href="https://docs.openstack.org/python-octaviaclient/latest/cli/index.html#loadbalancer-failover" target="_blank">https://docs.openstack.org/python-octaviaclient/latest/cli/index.html#loadbalancer-failover</a>.
     </p></div></section><section class="sect2" id="upgrade-reboot-nodes-kernel" data-id-title="Rebooting the Nodes into the SUSE Linux Enterprise 12 SP4 Kernel"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.4 </span><span class="title-name">Rebooting the Nodes into the SUSE Linux Enterprise 12 SP4 Kernel</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#upgrade-reboot-nodes-kernel">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     At this point, all of the cloud services have been upgraded, but
     the nodes are still running the SUSE Linux Enterprise 12 SP3 kernel. The final step in
     the upgrade workflow is to reboot all of the nodes in the
     cloud in a controlled fashion, to ensure that active services failover
     appropriately.
   </p><p>
     The recommended order for rebooting nodes is to start with the deployer.
     This requires special handling, since the Ansible-based automation cannot
     fully manage the reboot of the node that it is running on.
   </p><p>
     After that, we recommend rebooting the rest of the nodes in the control
     planes in a rolling-reboot fashion, ensuring that high-availability services
     remain available.
   </p><p>
     Finally, the compute nodes can be rebooted, either individually or
     in groups, as is appropriate to avoid interruptions to running workloads.
   </p><div id="id-1.5.17.6.15.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
       Do not reboot all your control plane nodes at the same time.
     </p></div><div class="procedure" id="id-1.5.17.6.15.7" data-id-title="Rebooting the Deployer Node"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.11: </span><span class="title-name">Rebooting the Deployer Node </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.15.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
       The reboot of the deployer node requires additional steps, as the
       Ansible-based automation framework cannot fully automate the reboot of
       the node that runs the ansible-playbook commands.
     </p><ol class="procedure" type="1"><li class="step"><p>
         Run the <code class="filename">ardana-reboot.yml</code> playbook limited to the
         deployer node, either by name, or using the logical node identified
         <code class="literal">OPS-LM--first-member</code>, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit OPS-LM--first-member

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-dplyr-m1]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml

...

TASK: [ardana-reboot | Deployer node has to be rebooted manually] *************
failed: [ardana-cp-dplyr-m1] =&gt; {"failed": true}
msg: The deployer node needs to be rebooted manually. After reboot, resume by running the post-reboot playbook:
cd ~/scratch/ansible/next/ardana/ansible ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-dplyr-m1

msg: The deployer node needs to be rebooted manually. After reboot, resume by running the post-reboot playbook:
cd ~/scratch/ansible/next/ardana/ansible ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-dplyr-m1

FATAL: all hosts have already failed -- aborting

PLAY RECAP ********************************************************************
           to retry, use: --limit @/var/lib/ardana/ardana-reboot.retry

ardana-cp-dplyr-m1         : ok=8    changed=3    unreachable=0    failed=1
localhost                  : ok=7    changed=0    unreachable=0    failed=0</pre></div><p>
         The <code class="filename">ardana-reboot.yml</code> playbook will fail when run
         on a deployer node; this is expected. The reported failure message
         tells you what you need to do to complete the remaining steps of the
         reboot manually: namely, rebooting the node, then logging back in again
         to run the <code class="filename">_ardana-post-reboot.yml</code> playbook, to start any services
         that need to be running on the node.
       </p></li><li class="step"><p>
         Manually reboot the deployer node, for example with <code class="command">shutdown -r now</code>.
       </p></li><li class="step"><p>
         Once the deployer node has rebooted, you need to log in again and run
         the <code class="filename">_ardana-post-reboot.yml</code> playbook to complete
         the startup of any services that should be running on the deployer node, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook _ardana-post-reboot.yml --limit OPS-LM--first-member

PLAY [resources] **************************************************************

TASK: [Set pending_clm_update] ************************************************
skipping: [ardana-cp-dplyr-m1]

...

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=26   changed=0    unreachable=0    failed=0
localhost                  : ok=19   changed=1    unreachable=0    failed=0</pre></div></li></ol></div></div><div class="procedure" id="id-1.5.17.6.15.8" data-id-title="Rebooting the Remaining Control Plane Nodes"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.12: </span><span class="title-name">Rebooting the Remaining Control Plane Nodes </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.15.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
       For the remaining nodes, you can use <code class="filename">ardana-reboot.yml</code>
       to fully automate the reboot process. However, it is recommended that you
       reboot the nodes in a rolling-reboot fashion, such that high-availability
       services continue to run without interruption. Similarly, to avoid
       interruption of service for any singleton services (such as the <code class="systemitem">cinder-volume</code>
       and <code class="systemitem">cinder-backup</code> services), they should be migrated off the intended
       node before it is rebooted, and then migrated back again afterwards.
     </p><ol class="procedure" type="1"><li class="step"><p>
         Use the <code class="command">ansible</code> command's <code class="option">--list-hosts</code> option to
         list the remaining nodes in the cloud that are neither the deployer
         nor a compute node:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts --list-hosts 'resources:!OPS-LM--first-member:!NOV-CMP'
    ardana-cp-dbmqsw-m1
    ardana-cp-dbmqsw-m2
    ardana-cp-dbmqsw-m3
    ardana-cp-osc-m1
    ardana-cp-osc-m2
    ardana-cp-mml-m1
    ardana-cp-mml-m2
    ardana-cp-mml-m3</pre></div></li><li class="step"><p>
         Use the following command to generate the set of <code class="command">ansible-playbook</code>
         commands that need to be run to reboot all the nodes sequentially:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>for node in $(ansible -i hosts/verb_hosts --list-hosts 'resources:!OPS-LM--first-member:!NOV-CMP'); do echo ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ${node} || break; done
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m3
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-osc-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-osc-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m3</pre></div><div id="id-1.5.17.6.15.8.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
          Do not reboot all your control-plane nodes at the same time.
        </p></div></li><li class="step"><p>
         To reboot a specific control-plane node, you can use the above
         <code class="command">ansible-playbook</code> commands as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m3

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-mml-m3]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml



...

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-reboot.yml"
}

msg: Playbook finished - ardana-reboot.yml

PLAY RECAP ********************************************************************
ardana-cp-mml-m3           : ok=389  changed=105  unreachable=0    failed=0
localhost                  : ok=27   changed=1    unreachable=0    failed=0</pre></div></li></ol></div></div><div id="id-1.5.17.6.15.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
       You can reboot more than one control-plane node at a time, but only if
       they are members of different control-plane clusters. For example, you
       could reboot one node out of each of the Openstack controller, database,
       swift, monitoring or logging clusters, so long as doing do only reboots
       one node out of each cluster at the same time.
     </p></div><p>
     When rebooting the first member of the control-plane cluster where
     monitoring services run, the <code class="systemitem">monasca-thresh</code> service can sometimes fail
     to start up in a timely fashion when the node is coming back up after
     being rebooted. This can cause <code class="filename">ardana-reboot.yml</code> to fail.
     See below for suggestions on how to handle this problem.
   </p><div class="procedure" id="id-1.5.17.6.15.11" data-id-title="Getting monasca-thresh Running After an ardana-reboot.yml Failure"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.13: </span><span class="title-name">Getting <code class="systemitem">monasca-thresh</code> Running After an <code class="filename">ardana-reboot.yml</code> Failure </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.15.11">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
         If the <code class="filename">ardana-reboot.yml</code> playbook failed because
         <code class="systemitem">monasca-thresh</code> didn't start up in a timely fashion
         after a reboot, you can retry starting the services on the node using
         the <code class="filename">_ardana-post-reboot.yml</code> playbook for the node.
         This is similar to the manual handling of the deployer reboot, since
         the node has already successfully rebooted onto the new kernel, and
         you just need to get the required services running again on the node.
       </p><p>
         It can sometimes take up to 15 minutes for the <code class="systemitem">monasca-thresh</code>
         service to successfully start in such cases.
       </p><ul class="procedure"><li class="step"><p>
         However, if the service still fails to start after that time, then
         you may need to force a restart of the <code class="systemitem">storm-nimbus</code> and
         <code class="systemitem">storm-supervisor</code> services on all nodes in the
         <code class="literal">MON-THR</code> node group, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible MON-THR -b -m shell -a "systemctl restart storm-nimbus"
ardana-cp-mml-m1 | success | rc=0 &gt;&gt;


ardana-cp-mml-m2 | success | rc=0 &gt;&gt;


ardana-cp-mml-m3 | success | rc=0 &gt;&gt;


ardana &gt; ansible MON-THR -b -m shell -a "systemctl restart storm-supervisor"
ardana-cp-mml-m1 | success | rc=0 &gt;&gt;


ardana-cp-mml-m2 | success | rc=0 &gt;&gt;


ardana-cp-mml-m3 | success | rc=0 &gt;&gt;


ardana &gt; ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-mml-m1</pre></div></li></ul></div></div><p>
     If the <code class="literal">monasca-thresh</code> service still fails to start up,
     contact your support team.
   </p><p>
     To check which control plane nodes have not yet been rebooted onto
     the new kernel, you can use an Ansible command to run the command <code class="command">uname -r</code>
     on the target nodes, as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts 'resources:!OPS-LM--first-member:!NOV-CMP' -m command -a 'uname -r'
ardana-cp-dbmqsw-m1 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-dbmqsw-m3 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-osc-m1 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-dbmqsw-m2 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-mml-m2 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-osc-m2 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-mml-m1 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-mml-m3 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana &gt; uname -r
4.12.14-95.57-default</pre></div><p>
     If any node's <code class="command">uname -r</code> value does not match the kernel
     that the deployer is running, you probably have not yet rebooted that node.
   </p><div class="procedure" id="id-1.5.17.6.15.16" data-id-title="Rebooting the Compute Nodes"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.14: </span><span class="title-name">Rebooting the Compute Nodes </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.15.16">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
       Finally, you need to reboot the compute nodes. Rebooting multiple
       compute nodes at the same time is possible, so long as doing so does
       not compromise the integrity of running workloads. We recommended
       that you migrate workloads off groups of compute nodes in a controlled fashion,
       enabling them to be rebooted together.
     </p><div id="id-1.5.17.6.15.16.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
         Do not reboot all of your compute nodes at the same time.
       </p></div><ol class="procedure" type="1"><li class="step"><p>
         To see all the compute nodes that are available to be rebooted, you
         can run the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
    <code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts --list-hosts NOV-CMP
    ardana-cp-slcomp0001
    ardana-cp-slcomp0002
...
    ardana-cp-slcomp0080</pre></div></li><li class="step"><p>
         Reboot the compute nodes, individually or in groups, using the
         <code class="filename">ardana-reboot.yml</code> playbook as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
    <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-slcomp0001,ardana-cp-slcomp0002

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-slcomp0001]
ok: [ardana-cp-slcomp0002]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml

...

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-reboot.yml"
}

msg: Playbook finished - ardana-reboot.yml

PLAY RECAP ********************************************************************
ardana-cp-slcomp0001       : ok=120  changed=11   unreachable=0    failed=0
ardana-cp-slcomp0002       : ok=120  changed=11   unreachable=0    failed=0
localhost                  : ok=27   changed=1    unreachable=0    failed=0</pre></div></li></ol></div></div><div id="id-1.5.17.6.15.17" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       You must ensure that there is sufficient unused workload capacity to
       host any migrated workload or Amphora instances that may be running on
       the targeted compute nodes.
     </p><p>
       When rebooting multiple compute nodes at the same time, consider
       manually migrating any running workloads and Amphora instances off the
       target nodes in advance, to avoid any potential risk of workload or
       service interruption.
     </p></div></section><section class="sect2" id="post-upgrade-tasks" data-id-title="Post-Upgrade Tasks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.5 </span><span class="title-name">Post-Upgrade Tasks</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#post-upgrade-tasks">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     After the cloud has been upgraded to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager,
     if <code class="systemitem">designate</code> was previously configured, then the deprecated service
     components, <code class="literal">designate-zone-manager</code> and
     <code class="literal">designate-pool-manager</code>, were being used.
   </p><p>
     They will continue to operate correctly under <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager,
     but we recommend that you migrate to using the newer <code class="systemitem">designate-worker</code>
     <code class="systemitem">designate-producer</code> service components instead by
     following the procedure documented in <span class="intraxref">Book “<em class="citetitle">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 25 “DNS Service Installation Overview”, Section 25.4 “Migrate Zone/Pool to Worker/Producer after Upgrade”</span>.
   </p><div class="procedure" id="id-1.5.17.6.16.4" data-id-title="Cleanup Orphaned Packages"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.15: </span><span class="title-name">Cleanup Orphaned Packages </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.16.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ul class="procedure"><li class="step"><p>
         After migrating the deployer node, there are a small number of
         packages that were installed that are no longer required—such as
         the <code class="systemitem">ceilometer</code> and <code class="systemitem">freezer</code> <code class="literal">virtualenv</code> (venv) packages.
       </p><p>
         You can safely remove these packages with the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper packages --orphaned
Loading repository data...
Reading installed packages...
S | Repository | Name                             | Version                           | Arch
--+------------+----------------------------------+-----------------------------------+-------
i | @System    | python-flup                      | 1.0.3.dev_20110405-2.10.52        | noarch
i | @System    | python-happybase                 | 0.9-1.64                          | noarch
i | @System    | venv-openstack-ceilometer-x86_64 | 9.0.8~dev7-12.24.2                | noarch
i | @System    | venv-openstack-freezer-x86_64    | 5.0.0.0~xrc2~dev2-10.22.1         | noarch
ardana&gt; sudo zypper remove venv-openstack-ceilometer-x86_64 venv-openstack-freezer-x86_64
Loading repository data...
Reading installed packages...
Resolving package dependencies...

The following 2 packages are going to be REMOVED:
  venv-openstack-ceilometer-x86_64 venv-openstack-freezer-x86_64

2 packages to remove.
After the operation, 79.0 MiB will be freed.
Continue? [y/n/...? shows all options] (y): y
(1/2) Removing venv-openstack-ceilometer-x86_64-9.0.8~dev7-12.24.2.noarch ..................................................................[done]
Additional rpm output:
/usr/lib/python2.7/site-packages/ardana_packager/indexer.py:148: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  return yaml.load(f)


(2/2) Removing venv-openstack-freezer-x86_64-5.0.0.0~xrc2~dev2-10.22.1.noarch ..............................................................[done]
Additional rpm output:
/usr/lib/python2.7/site-packages/ardana_packager/indexer.py:148: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  return yaml.load(f)</pre></div></li></ul></div></div><div class="procedure" id="id-1.5.17.6.16.5" data-id-title="Delete freezer Containers from swift"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.16: </span><span class="title-name">Delete freezer Containers from swift </span></span><a title="Permalink" class="permalink" href="system-maintenance.html#id-1.5.17.6.16.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
       The freezer service has been deprecated and removed from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager,
       but the backups that the freezer service created before you upgraded
       will still be consuming space in your Swift Object store.
     </p><p>
       Therefore, once you have completed the upgrade successfully, you can
       safely delete the containers that freezer used to hold the database and
       ring backups, freeing up that space.
     </p><ul class="procedure"><li class="step"><p>
         Using the credentials in the <code class="filename">backup.osrc</code> file,
         found on the deployer node in the Ardana account's home directory,
         run the following commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>. ~/backup.osrc
<code class="prompt user">ardana &gt; </code>swift list
freezer_database_backups
freezer_rings_backups
ardana&gt; swift delete --all
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/1_1598548599/segments/000000021
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/2_1598605266/data1
...
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/0_1598505404/segments/000000001
freezer_database_backups
freezer_rings_backups/metadata/tar/ardana-cp-dbmqsw-m1-host_freezer_swift_builder_dir_backup/1598548636/0_1598548636/metadata
...
freezer_rings_backups/data/tar/ardana-cp-dbmqsw-m1-host_freezer_swift_builder_dir_backup/1598548636/0_1598548636/data
freezer_rings_backups</pre></div></li></ul></div></div></section></section><section class="sect1" id="deploy-ptf" data-id-title="Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.5 </span><span class="title-name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#deploy-ptf">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-deploy-ptf.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Occasionally, in order to fix a given issue, SUSE will provide a set of
  packages known as a Program Temporary Fix (PTF). Such a PTF is fully
  supported by SUSE until the Maintenance Update containing a permanent fix has
  been released via the regular Update repositories. Customers running PTF
  fixes will be notified through the related Service Request when a permanent
  patch for a PTF has been released.
 </p><p>
  Use the following steps to deploy a PTF:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    When SUSE has developed a PTF, you will receive a URL for that PTF. You
    should download the packages from the location provided by SUSE Support
    to a temporary location on the Cloud Lifecycle Manager. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>tmpdir=`mktemp -d`
<code class="prompt user">ardana &gt; </code>cd $tmpdir
<code class="prompt user">ardana &gt; </code>wget --no-directories --recursive --reject "index.html*"\
--user=<em class="replaceable">USER_NAME</em> \
--ask-password \
--no-parent https://ptf.suse.com/54321aaaa...dddd12345/cloud8/042171/x86_64/20181030/</pre></div></li><li class="step"><p>
    Remove any old data from the PTF repository, such as a listing for a PTF
    repository from a migration or when previous product patches were
    installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -rf /srv/www/suse-12.4/x86_64/repos/PTF/*</pre></div></li><li class="step"><p>
    Move packages from the temporary download location to the PTF repository
    directory on the CLM Server. This example is for a neutron PTF.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo mkdir -p /srv/www/suse-12.4/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo mv $tmpdir/*
   /srv/www/suse-12.4/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo chown --recursive root:root /srv/www/suse-12.4/x86_64/repos/PTF/*
<code class="prompt user">ardana &gt; </code>rmdir $tmpdir</pre></div></li><li class="step"><p>
    Create or update the repository metadata:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo /usr/local/sbin/createrepo-cloud-ptf
Spawning worker 0 with 2 pkgs
Workers Finished
Saving Primary metadata
Saving file lists metadata
Saving other metadata</pre></div></li><li class="step"><p>
    Refresh the PTF repository before installing package updates on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper refresh --force --repo PTF
Forcing raw metadata refresh
Retrieving repository 'PTF' metadata
..........................................[d
one]
Forcing building of repository cache
Building repository 'PTF' cache ..........................................[done]
Specified repositories have been refreshed.</pre></div></li><li class="step"><p>
    The PTF shows as available on the deployer.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --repo PTF
Loading repository data...
Reading installed packages...

S | Name                          | Summary                                 | Type
--+-------------------------------+-----------------------------------------+--------
  | python-neutronclient          | Python API and CLI for OpenStack neutron | package
i | venv-openstack-neutron-x86_64 | Python virtualenv for OpenStack neutron | package</pre></div></li><li class="step"><p>
    Install the PTF venv packages on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper dup  --from PTF
Refreshing service
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

The following package is going to be upgraded:
  venv-openstack-neutron-x86_64

The following package has no support information from its vendor:
  venv-openstack-neutron-x86_64

1 package to upgrade.
Overall download size: 64.2 MiB. Already cached: 0 B. After the operation, additional 6.9 KiB will be used.
Continue? [y/n/...? shows all options] (y): y
Retrieving package venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ... (1/1),  64.2 MiB ( 64.6 MiB unpacked)
Retrieving: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm ....[done]
Checking for file conflicts: ..............................................................[done]
(1/1) Installing: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ....[done]
Additional rpm output:
warning
warning: /var/cache/zypp/packages/PTF/noarch/venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm: Header V3 DSA/SHA1 Signature, key ID b37b98a9: NOKEY</pre></div></li><li class="step"><p>
    Validate the venv tarball has been installed into the deployment directory:(note:the packages file under that dir shows the registered tarballs that will be used for the services, which should align with the installed venv RPM)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/ardana_packager/ardana-9/sles_venv/x86_64
total 898952
drwxr-xr-x 2 root root     4096 Oct 30 16:10 .
...
-rw-r--r-- 1 root root 67688160 Oct 30 12:44 neutron-20181030T124310Z.tgz &lt;&lt;&lt;
-rw-r--r-- 1 root root 64674087 Aug 14 16:14 nova-20180814T161306Z.tgz
-rw-r--r-- 1 root root 45378897 Aug 14 16:09 octavia-20180814T160839Z.tgz
-rw-r--r-- 1 root root     1879 Oct 30 16:10 packages
-rw-r--r-- 1 root root 27186008 Apr 26  2018 swift-20180426T230541Z.tgz</pre></div></li><li class="step"><p>
    Install the non-venv PTF packages on the Compute Node
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml --extra-vars '{"zypper_update_method": "update", "zypper_update_repositories": ["PTF"]}' --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that the upgraded package
    has been installed on <code class="literal">comp0001-mgmt</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --detail python-neutronclient
Loading repository data...
Reading installed packages...

S | Name                 | Type     | Version                         | Arch   | Repository
--+----------------------+----------+---------------------------------+--------+--------------------------------------
i | python-neutronclient | package  | 6.5.1-4.361.042171.0.PTF.102473 | noarch | PTF
  | python-neutronclient | package  | 6.5.0-4.361                     | noarch | SUSE-OPENSTACK-CLOUD-x86_64-GM-DVD1</pre></div></li><li class="step"><p>
    Running the ardana update playbook will distribute the PTF venv packages to
    the cloud server. Then you can find them loaded in the virtual environment
    directory with the other venvs.
   </p><p>
    The Compute Node before running the update playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 24
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step"><p>
    Run the update.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that an additional virtual environment
    has been installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 28
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x  9 root root 4096 Oct 30 12:43 neutron-20181030T124310Z &lt;&lt;&lt; New venv installed
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step"><p>
    The PTF may also have <code class="literal">RPM</code> package updates in addition to
    venv updates. To complete the update, follow the instructions at <a class="xref" href="system-maintenance.html#perform-update" title="15.3.1. Performing the Update">Section 15.3.1, “Performing the Update”</a>
   </p></li></ol></div></div></section><section class="sect1" id="database-maintenance" data-id-title="Periodic OpenStack Maintenance Tasks"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.6 </span><span class="title-name">Periodic OpenStack Maintenance Tasks</span></span> <a title="Permalink" class="permalink" href="system-maintenance.html#database-maintenance">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-database_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Heat-manage helps manage Heat specific database operations. The associated
    database should be periodically purged to save space. The following should
    be setup as a cron job on the servers where the heat service is running at
    <code class="literal">/etc/cron.weekly/local-cleanup-heat</code>
    with the following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su heat -s /bin/bash -c "/usr/bin/heat-manage purge_deleted -g days 14" || :</pre></div><p>
     nova-manage db archive_deleted_rows command will move deleted rows
     from production tables to shadow tables. Including
     <code class="literal">--until-complete</code> will make the command run continuously
     until all deleted rows are archived. It is recommended to setup this task
     as <code class="literal">/etc/cron.weekly/local-cleanup-nova</code>
     on the servers where the nova service is running, with the
     following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su nova -s /bin/bash -c "/usr/bin/nova-manage db archive_deleted_rows --until-complete" || :</pre></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="using-container-as-a-service-overview.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 14 </span>Managing Container as a Service (Magnum)</span></a> </div><div><a class="pagination-link next" href="manage-ops-console.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 16 </span>Operations Console</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="system-maintenance.html#planned-maintenance"><span class="title-number">15.1 </span><span class="title-name">Planned System Maintenance</span></a></span></li><li><span class="section"><a href="system-maintenance.html#unplanned-maintenance"><span class="title-number">15.2 </span><span class="title-name">Unplanned System Maintenance</span></a></span></li><li><span class="section"><a href="system-maintenance.html#maintenance-update"><span class="title-number">15.3 </span><span class="title-name">Cloud Lifecycle Manager Maintenance Update Procedure</span></a></span></li><li><span class="section"><a href="system-maintenance.html#upgrade-soc"><span class="title-number">15.4 </span><span class="title-name">Upgrading Cloud Lifecycle Manager 8 to Cloud Lifecycle Manager 9</span></a></span></li><li><span class="section"><a href="system-maintenance.html#deploy-ptf"><span class="title-number">15.5 </span><span class="title-name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span></a></span></li><li><span class="section"><a href="system-maintenance.html#database-maintenance"><span class="title-number">15.6 </span><span class="title-name">Periodic OpenStack Maintenance Tasks</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>